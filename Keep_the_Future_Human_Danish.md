# Hold Fremtiden Menneskelig

Dette essay argumenterer for hvorfor og hvordan vi bør lukke portene for AGI og superintelligens, og hvad vi i stedet bør bygge.

Hvis du kun vil have de vigtigste pointer, så gå til det udførende resumé. Derefter vil kapitel 2-5 give baggrund om de typer AI-systemer, der diskuteres i essayet. Kapitel 5-7 forklarer, hvorfor vi kan forvente, at AGI ankommer snart, og hvad der kan ske, når det sker. Endelig skitserer kapitel 8-9 et konkret forslag til at forhindre, at AGI bliver bygget.

[Download PDF](https://keepthefuturehuman.ai/wp-content/uploads/2025/03/Keep_the_Future_Human__AnthonyAguirre__5March2025.pdf)

Samlet læsetid: 2-3 timer

## Executive summary

En overordnet gennemgang af essayet. Hvis du har travlt, kan du få alle hovedpointerne på bare 10 minutter.

De dramatiske fremskridt inden for kunstig intelligens i løbet af det seneste årti (for snævert specialiserede AI-systemer) og de seneste år (for generelle AI-systemer) har forvandlet AI fra et nichepræget akademisk felt til kerneforretningsstrategien for mange af verdens største virksomheder, med hundredvis af milliarder dollars i årlige investeringer i teknikker og teknologier til at udvikle AI's kapaciteter.

Nu står vi ved et kritisk vendepunkt. Efterhånden som nye AI-systemers kapaciteter begynder at matche og overgå menneskers på mange kognitive områder, må menneskeheden beslutte: hvor langt skal vi gå, og i hvilken retning?

AI startede, som alle andre teknologier, med målet om at forbedre tingene for sin skaber. Men vores nuværende bane og implicitte valg er et ukontrolleret kapløb mod stadig mere kraftfulde systemer, drevet af økonomiske incitamenter fra få gigantiske teknologivirksomheder, der søger at automatisere store dele af den nuværende økonomiske aktivitet og menneskelige arbejdskraft. Hvis dette kapløb fortsætter meget længere, er der en uundgåelig vinder: AI'en selv – et hurtigere, smartere, billigere alternativ til mennesker i vores økonomi, vores tænkning, vores beslutninger og til sidst i kontrol med vores civilisation.

Men vi kan træffe et andet valg: gennem vores regeringer kan vi tage kontrol over AI-udviklingsprocessen for at pålægge klare grænser, linjer vi ikke vil krydse, og ting vi simpelthen ikke vil gøre – som vi har gjort for nuklear teknologi, masseødelæggelsesvåben, rumvåben, miljøskadelige processer, bioengineering af mennesker og eugenik. Vigtigst af alt kan vi sikre, at AI forbliver et værktøj til at styrke mennesker, frem for en ny art der erstatter og til sidst fortrænger os.

Dette essay argumenterer for, at vi bør *holde fremtiden menneskelig* ved at lukke "Portene" til smartere-end-menneske, autonome, generelle AI-systemer – sommetider kaldet "AGI" – og især til den stærkt overmenneskelige version, der sommetider kaldes "superintelligens." I stedet bør vi fokusere på kraftfulde, troværdige AI-værktøjer, der kan styrke individer og transformativt forbedre menneskelige samfunds evner til at gøre det, de gør bedst. Strukturen i denne argumentation følger kort herunder.

### AI er anderledes

AI-systemer er fundamentalt forskellige fra andre teknologier. Mens traditionel software følger præcise instruktioner, lærer AI-systemer hvordan de opnår mål uden eksplicit at blive fortalt hvordan. Dette gør dem kraftfulde: hvis vi kan definere målet klart eller en metrik for succes, kan et AI-system i de fleste tilfælde lære at opnå det. Men det gør dem også inherent uforudsigelige: vi kan ikke pålideligt fastslå, hvilke handlinger de vil foretage for at opnå deres mål.

De er også stort set uforklarlige: selvom de delvist består af kode, består de for det meste af et enormt sæt ugennemskuelige tal – neurale netværks "vægte" – der ikke kan fortolkes; vi er ikke meget bedre til at forstå deres indre funktioner end til at udlede tanker ved at kigge ind i en biologisk hjerne.

Denne grundlæggende måde at træne digitale neurale netværk på bliver hurtigt mere og mere kompleks. De mest kraftfulde AI-systemer skabes gennem massive beregningseksperimenter, hvor man bruger specialiseret hardware til at træne neurale netværk på enorme datasæt, som derefter udvides med softwareværktøjer og overstruktur.

Dette har ført til skabelsen af meget kraftfulde værktøjer til at skabe og behandle tekst og billeder, udføre matematisk og videnskabelig ræsonnering, aggregere information og interaktivt søge i et enormt lager af menneskelig viden.

Desværre, selvom udvikling af mere kraftfulde, mere troværdige teknologiske værktøjer er det, vi *burde* gøre, og det næsten alle ønsker og siger de ønsker, er det ikke den bane, vi faktisk befinder os på.

### AGI og superintelligens

Siden feltets begyndelse har AI-forskning i stedet fokuseret på et andet mål: Artificial General Intelligence. Dette fokus er nu blevet fokus for de titaniske virksomheder, der leder AI-udviklingen.

Hvad er AGI? Det defineres ofte vagt som "AI på menneskeniveau," men dette er problematisk: hvilke mennesker, og på hvilke kapaciteter er det på menneskeniveau? Og hvad med de overmenneskelige kapaciteter, det allerede har? En mere brugbar måde at forstå AGI på er gennem skæringspunktet mellem tre nøgleegenskaber: høj **A**utonomi (handlingsindependence), høj **G**eneralitet (bred rækkevidde og tilpasningsevne) og høj **I**ntelligens (kompetence til kognitive opgaver). Nuværende AI-systemer kan være meget kapable men snævre, eller generelle men kræve konstant menneskelig overvågning, eller autonome men begrænsede i rækkevidde.

Fuld A-G-I ville kombinere alle tre egenskaber på niveauer, der matcher eller overgår top menneskelige kapaciteter. Kritisk set er det denne kombination, der gør mennesker så effektive og så forskellige fra nuværende software; det er også det, der ville gøre det muligt at erstatte mennesker fuldstændigt med digitale systemer.

Selvom menneskelig intelligens er speciel, er den på ingen måde en grænse. Kunstige "superintelligente" systemer kunne operere hundredvis af gange hurtigere, behandle enormt meget mere data og holde enorme mængder "i tankerne" på én gang, og danne aggregater, der er meget større og mere effektive end samlinger af mennesker. De kunne fortrænge ikke blot individer, men virksomheder, nationer eller vores civilisation som helhed.

### Vi står på tærsklen

Der er stærk videnskabelig konsensus om, at AGI er *mulig.* AI overgår allerede menneskelig præstation i mange generelle test af intellektuel kapacitet, herunder for nylig højt niveau ræsonnering og problemløsning. Haltende kapaciteter – såsom kontinuerlig læring, planlægning, selvbevidsthed og originalitet – eksisterer alle på et vist niveau i nuværende AI-systemer, og kendte teknikker eksisterer, som sandsynligvis vil forbedre dem alle.

Mens mange forskere indtil for få år siden så AGI som årtier væk, er der i øjeblikket stærk evidens for korte tidslinjer til AGI:

- Empirisk verificerede "skaleringslove" forbinder beregningsinput med AI-kapacitet, og virksomheder er på sporet til at skalere beregningsinput med størrelsesordener over de kommende år. De menneskelige og finansielle ressourcer dedikeret til AI-udvikling svarer nu til dem fra et dusin Manhattan-projekter og flere Apollo-projekter.
- AI-virksomheder og deres ledere tror offentligt og privat, at AGI (efter en eller anden definition) er opnåelig inden for få år. Disse virksomheder har information, som offentligheden ikke har, herunder nogle der har næste generation af AI-systemer i hænde.
- Ekspertforudsigere med dokumenterede track records tildeler 25% sandsynlighed for at AGI (efter en eller anden definition) ankommer inden for 1-2 år, og 50% for 2-5 år (se Metaculus forudsigelser for ['svag'](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) og ['fuld'](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/) AGI).
- Autonomi (herunder langtrækkende fleksibel planlægning) halter i AI-systemer, men større virksomheder fokuserer nu deres store ressourcer på at udvikle autonome AI-systemer og har uformelt døbt 2025 ["agenternes år."](https://techinformed.com/2025-informed-the-year-of-agentic-ai/)
- AI bidrager mere og mere til sin egen forbedring. Når AI-systemer bliver lige så kompetente som menneskelige AI-forskere til at lave AI-forskning, vil en kritisk tærskel for hurtig fremskridt til meget mere kraftfulde AI-systemer blive ramt og sandsynligvis føre til en ukontrolleret udvikling i AI-kapacitet. (Kunne man argumentere for, at den ukontrollerede udvikling allerede er begyndt.)

Ideen om, at smartere-end-menneske AGI er årtier væk eller mere, er simpelthen ikke længere holdbar for langt de fleste eksperter på området. Uenigheder nu handler om, hvor mange måneder eller år det vil tage, hvis vi holder fast i dette forløb. Det centrale spørgsmål, vi står over for, er: skal vi?

### Hvad der driver kapløbet mod AGI

Kapløbet mod AGI drives af flere kræfter, der hver gør situationen mere farlig. Store teknologivirksomheder ser AGI som den ultimative automatiseringsteknologi – ikke bare supplere menneskelige arbejdere, men erstatte dem stort set eller helt. For virksomheder er præmien enorm: muligheden for at indfange en betydelig del af verdens 100 billioner dollars årlige økonomiske produktion ved at automatisere menneskelige arbejdsomkostninger væk.

Nationer føler sig tvunget til at deltage i dette kapløb, og henviser offentligt til økonomisk og videnskabeligt lederskab, men betragter privat AGI som en potentiel revolution i militære anliggender sammenlignelig med atomvåben. Frygten for, at rivaler måtte opnå en afgørende strategisk fordel, skaber en klassisk våbenkapløbsdynamik.

De, der forfølger superintelligens, henviser ofte til store visioner: kurere alle sygdomme, vende aldring om, opnå gennembrud inden for energi og rumfart eller skabe overmenneskelige planlægningskapaciteter.

Mindre velvilligt er det, der driver kapløbet, magt. Hver deltager – hvad enten det er virksomhed eller land – tror, at intelligens er lig med magt, og at de vil være den bedste forvalter af den magt.

Jeg argumenterer for, at disse motiver er reelle, men fundamentalt fejlvejledte: AGI vil *absorbere* og *søge* magt frem for at give den; AI-skabte teknologier vil *også* være stærkt tveæggede, og hvor de er gavnlige, kan de skabes med AI-værktøjer og uden AGI; og selv i det omfang AGI og dens resultater forbliver under kontrol, gør disse kapløbsdynamikker – både virksomhedsmæssige og geopolitiske – store risici for vores samfund næsten uundgåelige, medmindre de afgørende afbrydes.

### AGI og superintelligens udgør en dramatisk trussel mod civilisationen

På trods af deres tiltrækning udgør AGI og superintelligens dramatiske trusler mod civilisationen gennem flere forstærkende veje:

*Magtkoncentration:* overmenneskelig AI kunne fratage langt størstedelen af menneskeheden magt ved at absorbere enorme dele af social og økonomisk aktivitet ind i AI-systemer drevet af en håndfuld gigantiske virksomheder (som til gengæld enten kan blive overtaget af, eller effektivt overtage, regeringer.)

*Massiv disruption:* bulkautomatisering af de fleste kognitivt baserede job, erstatning af vores nuværende epistemiske systemer og udrulning af enorme mængder aktive ikke-menneskelige agenter ville vælte de fleste af vores nuværende civilisatoriske systemer på relativt kort tid.

*Katastrofer:* ved at sprede evnen – potentielt over menneskeniveau – til at skabe nye militære og destruktive teknologier og afkoble den fra de sociale og juridiske systemer, der forankrer ansvar, bliver fysiske katastrofer fra masseødelæggelsesvåben dramatisk mere sandsynlige.

*Geopolitik og krig:* store verdensmagter vil ikke sidde stille, hvis de føler, at en teknologi, der kunne levere en "afgørende strategisk fordel," bliver udviklet af deres modstandere.

*Ukontrolleret udvikling og tab af kontrol:* Medmindre det specifikt forhindres, vil overmenneskelig AI have alle incitamenter til yderligere at forbedre sig selv og kunne langt overgå mennesker i hastighed, databehandling og sofistikering af tænkning. Der er ingen meningsfuld måde, hvorpå vi kan have kontrol over et sådant system. Sådan AI vil ikke give magt til mennesker; vi vil give magt til den, eller den vil tage den.

Mange af disse risici forbliver, selv hvis det tekniske "alignment"-problem – at sikre at avanceret AI pålideligt gør det, mennesker ønsker, den skal gøre – løses. AI præsenterer en enorm udfordring i, hvordan den vil blive forvaltet, og meget mange aspekter af denne forvaltning bliver utroligt vanskelige eller uløselige, når menneskelig intelligens brydes.

Mest fundamentalt ville den type overmenneskelig generel AI, der i øjeblikket forfølges, i sagens natur have mål, aktørskab og kapaciteter, der overstiger vores egne. Den ville være inherent ukontrollerbar – hvordan kan vi kontrollere noget, vi hverken kan forstå eller forudsige? Den ville ikke være et teknologisk værktøj til menneskelig brug, men en anden intelligensart på Jorden ved siden af vores. Hvis den fik lov at udvikle sig yderligere, ville den udgøre ikke blot en anden art, men en erstatningsart.

Måske ville den behandle os godt, måske ikke. Men fremtiden ville tilhøre den, ikke os. Den menneskelige æra ville være forbi.

### Dette er ikke uundgåeligt; menneskeheden kan meget konkret beslutte ikke at bygge sin erstatning.

Skabelsen af overmenneskelig AGI er langt fra uundgåelig. Vi kan forhindre det gennem et koordineret sæt styringsforanstaltninger:

For det første har vi brug for robust regnskabsføring og overvågning af AI-beregning ("compute"), som er en grundlæggende mulighedsskaber for, og løftestang til at styre, storskala AI-systemer. Dette kræver til gengæld standardiseret måling og rapportering af den samlede compute, der bruges til at træne AI-modeller og køre dem, og tekniske metoder til at opgøre, certificere og verificere anvendt beregning.

For det andet bør vi implementere hårde begrænsninger på AI-beregning, både til træning og til drift; disse forhindrer AI i både at være for kraftfuld og operere for hurtigt. Disse begrænsninger kan implementeres gennem både juridiske krav og hardwarebaserede sikkerhedsforanstaltninger bygget ind i AI-specialiserede chips, analogt med sikkerhedsfeatures i moderne telefoner. Fordi specialiseret AI-hardware kun laves af en håndfuld virksomheder, er verificering og håndhævelse mulig gennem den eksisterende forsyningskæde.

For det tredje har vi brug for øget ansvar for de mest farlige AI-systemer. De, der udvikler AI, som kombinerer høj autonomi, bred generalitet og overlegen intelligens, bør møde strikt erstatningsansvar for skader, mens sikre havne fra dette ansvar ville tilskynde til udvikling af mere begrænsede og kontrollerbare systemer.

For det fjerde har vi brug for trindelt regulering baseret på risikoniveauer. De mest kapable og farlige systemer ville kræve omfattende sikkerheds- og kontrollerbarhedsgarantier før udvikling og implementering, mens mindre kraftfulde eller mere specialiserede systemer ville møde proportional overvågning. Denne reguleringsramme bør til sidst operere på både nationale og internationale niveauer.

Denne tilgang – med detaljeret specifikation givet i det fulde dokument – er praktisk: selvom international koordination vil være nødvendig, kan verificering og håndhævelse virke gennem det lille antal virksomheder, der kontrollerer den specialiserede hardware-forsyningskæde. Den er også fleksibel: virksomheder kan stadig innovere og profitere fra AI-udvikling, bare med klare grænser på de mest farlige systemer.

Langsigtet inddæmning af AI-magt og -risiko ville kræve internationale aftaler baseret på både selv- og fælles interesse, ligesom kontrol af atomvåbenspredning gør nu. Men vi kan starte med det samme med øget overvågning og ansvar, mens vi bygger mod mere omfattende styring.

Den vigtige manglende ingrediens er politisk og social vilje til at tage kontrol over AI-udviklingsprocessen. Kilden til den vilje, hvis den kommer i tide, vil være virkeligheden selv – det vil sige fra udbredt erkendelse af de reelle implikationer af det, vi gør.

### Vi kan designe Værktøjs-AI til at styrke menneskeheden

I stedet for at forfølge ukontrollerbar AGI kan vi udvikle kraftfulde "Værktøjs-AI"-systemer, der forbedrer menneskelige kapaciteter, mens de forbliver under meningsfuld menneskelig kontrol. Værktøjs-AI-systemer kan være yderst kapable, mens de undgår det farlige tredobbelte skæringspunkt mellem høj autonomi, bred generalitet og overmenneskelig intelligens, så længe vi designer dem til at være kontrollerbare på et niveau, der står mål med deres kapacitet. De kan også kombineres til sofistikerede systemer, der opretholder menneskelig overvågning, mens de leverer transformative fordele.

Værktøjs-AI kan revolutionere medicin, accelerere videnskabelig opdagelse, forbedre uddannelse og forbedre demokratiske processer. Når det styres korrekt, kan det gøre menneskelige eksperter og institutioner mere effektive frem for at erstatte dem. Selvom sådanne systemer stadig vil være meget disruptive og kræve omhyggelig forvaltning, er de risici, de udgør, fundamentalt forskellige fra AGI: det er risici, vi kan styre, som dem fra andre kraftfulde teknologier, ikke eksistentielle trusler mod menneskelig aktørskab og civilisation. Og kritisk set, når det udvikles fornuftigt, kan AI-værktøjer hjælpe mennesker med at styre kraftfuld AI og forvalte dens effekter.

Denne tilgang kræver, at man gentænker både hvordan AI udvikles, og hvordan dens fordele fordeles. Nye modeller for offentlig og non-profit AI-udvikling, robuste reguleringsrammer og mekanismer til at fordele økonomiske fordele mere bredt kan hjælpe med at sikre, at AI styrker menneskeheden som helhed frem for at koncentrere magt i få hænder. AI selv kan hjælpe med at bygge bedre sociale og styringsinsitutioner, muliggøre nye former for koordination og diskurs, der styrker frem for underminerer menneskelige samfund. Nationale sikkerhedsapparater kan udnytte deres ekspertise til at gøre AI-værktøjssystemer virkelig sikre og troværdige og en sand kilde til forsvar såvel som national magt.

Vi kan til sidst vælge at udvikle endnu mere kraftfulde og mere suveræne systemer, der er mindre som værktøjer og – kan vi håbe – mere som vise og kraftfulde velgørere. Men vi bør kun gøre det, efter at vi har udviklet den videnskabelige forståelse og styringskapacitet til at gøre det sikkert. En så betydningsfuld og irreversibel beslutning bør træffes bevidst af menneskeheden som helhed, ikke som standard i et kapløb mellem tech-virksomheder og nationer.

### I menneskelige hænder

Folk ønsker det gode, der kommer fra AI: nyttige værktøjer, der styrker dem, oplader økonomiske muligheder og vækst og lover gennembrud inden for videnskab, teknologi og uddannelse. Hvorfor skulle de ikke? Men når de bliver spurgt, ønsker overvældende flertal af den brede offentlighed [langsommere og mere omhyggelig AI-udvikling](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation) og ønsker ikke smartere-end-menneske AI, der vil erstatte dem i deres job og andre steder, fylde deres kultur og informationsfællesskaber med ikke-menneskelig indhold, koncentrere magt i et lille sæt virksomheder, udgøre ekstreme storskala globale risici og til sidst true med at fratage magt eller erstatte deres art. Hvorfor skulle de?

Vi *kan* have det ene uden det andet. Det starter med at beslutte, at vores skæbne ikke ligger i den formodede uundgåelighed af en eller anden teknologi eller i hænderne på nogle få CEO'er i Silicon Valley, men i resten af vores hænder, hvis vi griber fat i det. Lad os lukke Portene og holde fremtiden menneskelig.

## Kapitel 1 - Indledning

Hvordan vi vil reagere på udsigten til AI, der er klogere end mennesker, er det mest presserende spørgsmål i vores tid. Dette essay giver en vej fremad.

Vi befinder os måske ved afslutningen af den menneskelige æra.

Noget er begyndt i løbet af de seneste ti år, som er unikt i vores arts historie. Konsekvenserne vil i høj grad afgøre menneskehedens fremtid. Fra omkring 2015 er forskere lykkedes med at udvikle *snæver* kunstig intelligens (AI) – systemer der kan vinde i spil som Go, genkende billeder og tale, og så videre, bedre end noget menneske.[^1]

Dette er en fantastisk succes, og det giver ekstrem nyttige systemer og produkter, som vil styrke menneskeheden. Men snæver kunstig intelligens har aldrig været feltets egentlige mål. I stedet har formålet været at skabe AI-systemer til *generel* anvendelse, især dem der ofte kaldes "AGI (Artificial General Intelligence)" eller "superintelligens", som samtidigt er lige så gode eller bedre end mennesker på tværs af næsten *alle* opgaver, præcis som AI nu er overmenneskelig til Go, skak, poker, drone-racing, osv. Dette er det erklærede mål for mange store AI-virksomheder.[^2]

*Disse bestræbelser lykkes også.* Generelle AI-systemer som ChatGPT, Gemini, Llama, Grok, Claude og Deepseek, baseret på massive beregninger og bjerge af data, har nået samme niveau som typiske mennesker på tværs af en bred vifte af opgaver, og matcher endda menneskelige eksperter på nogle områder. Nu kapløber AI-ingeniører hos nogle af de største teknologivirksomheder om at skubbe disse gigantiske eksperimenter i maskinelligens til de næste niveauer, hvor de matcher og derefter overgår hele spektret af menneskelige evner, ekspertise og autonomi.

*Dette er umiddelbart forestående.* I løbet af de sidste ti år er eksperternes estimater for, hvor lang tid dette vil tage – hvis vi fortsætter vores nuværende kurs – faldet fra årtier (eller århundreder) til et enkelt antal år.

Det har også epokegørende betydning og transcendent risiko. Tilhængere af AGI ser det som en positiv transformation, der vil løse videnskabelige problemer, helbrede sygdomme, udvikle nye teknologier og automatisere slidsomme opgaver. Og AI kunne bestemt hjælpe med at opnå alle disse ting – det gør det faktisk allerede. Men gennem årtier har mange omhyggelige tænkere, fra Alan Turing til Stephen Hawking til nutidens Geoffrey Hinton og Yoshua Bengio [^3] udsendt en skarp advarsel: at bygge virkelig klogere-end-menneskelig, generel, autonom AI vil som minimum fuldstændigt og uigenkaldeligt vælte samfundet, og som maksimum resultere i menneskelig udryddelse.[^4]

Superintelligent AI nærmer sig hurtigt på vores nuværende vej, men er langt fra uundgåelig. Dette essay er et udvidet argument for, hvorfor og hvordan vi bør *lukke Portene* til denne nærmende umenneskelige fremtid, og hvad vi i stedet bør gøre.


[^1]: Dette [diagram](https://time.com/6300942/ai-progress-charts/) viser et sæt opgaver; mange lignende kurver kunne tilføjes til denne graf. Disse hurtige fremskridt inden for snæver AI har overrasket selv eksperter på området, hvor benchmarks er blevet overgået år før forudsigelserne.

[^2]: Deepmind, OpenAI, Anthropic og X.ai blev alle grundlagt med det specifikke mål at udvikle AGI. For eksempel erklærer OpenAI's charter eksplicit sit mål som at udvikle "artificial general intelligence der gavner hele menneskeheden," mens DeepMinds mission er "at løse intelligens, og derefter bruge det til at løse alt andet." Meta, Microsoft og andre forfølger nu væsentligt lignende veje. Meta har sagt, at det [planlægger at udvikle AGI og frigive det åbent.](https://www.forbes.com/sites/johnkoetsier/2024/01/18/zuckerberg-on-ai-meta-building-agi-for-everyone-and-open-sourcing-it/)

[^3]: Hinton og Bengio er to af de mest citerede AI-forskere, har begge vundet AI-feltets Nobel, Turing-prisen, og Hinton har vundet en Nobelpris (i fysik) oven i købet.

[^4]: At bygge noget med denne risiko, under kommercielle incitamenter og næsten nul statsligt tilsyn, er fuldstændigt uden fortilfælde. Der er ikke engang kontrovers om risikoen blandt dem, der bygger det! Lederne af Deepmind, OpenAI og Anthropic, blandt mange andre eksperter, har alle bogstaveligt talt underskrevet en [erklæring](https://www.safe.ai/work/statement-on-ai-risk) om, at avanceret AI udgør en *udryddelsesrisiko for menneskeheden.* Alarmklokkerne kunne ikke ringe hårdere, og man kan kun konkludere, at dem der ignorerer dem, simpelthen ikke tager AGI og superintelligens seriøst. Et mål med dette essay er at hjælpe dem med at forstå, hvorfor de burde.

## Kapitel 2 - Det nødvendige at vide om AI neurale netværk

Hvordan fungerer moderne AI-systemer, og hvad kan vi forvente af den næste generation af AI?

For at forstå hvordan konsekvenserne af at udvikle mere kraftfuld AI vil udfolde sig, er det vigtigt at tilegne sig nogle grundlæggende principper. Dette og de næste to afsnit behandler disse principper og dækker på skift hvad moderne AI er, hvordan den udnytter massive beregninger, og på hvilke måder den hurtigt vokser i generalitet og kapacitet.[^5]

Der er mange måder at definere kunstig intelligens på, men til vores formål er AI's nøgleegenskab, at mens et almindeligt computerprogram er en liste af instruktioner for hvordan man udfører en opgave, er et AI-system et system, der lærer af data eller erfaring at udføre opgaver *uden eksplicit at blive fortalt hvordan det skal gøres.*

Næsten al relevant moderne AI er baseret på neurale netværk. Disse er matematiske/beregningsmæssige strukturer, repræsenteret af et meget stort (milliarder eller billioner) sæt af tal ("vægte"), der udfører en træningsopgave godt. Disse vægte bliver formet (eller måske "groet" eller "fundet") ved iterativt at justere dem, så det neurale netværk forbedrer en numerisk score (også kaldet "loss") defineret til at klare sig godt til en eller flere opgaver.[^6] Denne proces kaldes *træning* af det neurale netværk.[^7]

Der er mange teknikker til at udføre denne træning, men disse detaljer er langt mindre relevante end de måder, hvorpå scoringen er defineret, og hvordan disse resulterer i forskellige opgaver, som det neurale netværk klarer godt. En nøgleforskel er historisk blevet draget mellem "snæver" og "generel" AI.

Snæver AI bliver bevidst trænet til at udføre en bestemt opgave eller et lille sæt af opgaver (såsom at genkende billeder eller spille skak); den kræver gentræning til nye opgaver og har et snævert kapacitetsområde. Vi har overmenneskelig snæver AI, hvilket betyder, at for næsten enhver diskret veldefineret opgave en person kan udføre, kan vi sandsynligvis konstruere en score og derefter med succes træne et snævert AI-system til at gøre det bedre, end et menneske kunne.

Generelle AI-systemer (GPAI) kan udføre en bred vifte af opgaver, herunder mange de ikke eksplicit blev trænet til; de kan også lære nye opgaver som en del af deres drift. Nuværende store "multimodale modeller"[^8] som ChatGPT eksemplificerer dette: trænet på et meget stort korpus af tekst og billeder kan de engagere sig i kompleks ræsonnering, skrive kode, analysere billeder og assistere med en bred vifte af intellektuelle opgaver. Selvom de stadig er ret forskellige fra menneskelig intelligens på måder, vi vil se grundigt nedenfor, har deres generalitet forårsaget en revolution inden for AI.[^9]

### Uforudsigelighed: en nøgleegenskab ved AI-systemer

En nøgleforskel mellem AI-systemer og konventionel software ligger i forudsigeligheden. Standard softwares output kan være uforudsigelig – det er faktisk nogle gange derfor vi skriver software, for at give os resultater vi ikke kunne have forudsagt. Men konventionel software gør sjældent noget, den ikke blev programmeret til at gøre – dens omfang og adfærd er generelt som designet. Et førsteklasses skakprogram kan lave træk, intet menneske kunne forudsige (ellers kunne de slå det skakprogram!) men det vil generelt ikke gøre andet end at spille skak.

Ligesom konventionel software har snæver AI forudsigelig omfang og adfærd, men kan have uforudsigelige resultater. Dette er egentlig bare en anden måde at definere snæver AI på: som AI der ligner konventionel software i sin forudsigelighed og driftsområde.

Generel AI er anderledes: dens omfang (de domæner den anvendes over), adfærd (de slags ting den gør) og resultater (dens faktiske output) kan alle være uforudsigelige.[^10] GPT-4 blev trænet kun til at generere tekst præcist, men udviklede mange kapaciteter, dens trænere ikke forudsagde eller havde til hensigt. Denne uforudsigelighed stammer fra kompleksiteten af træning: fordi træningsdataene indeholder output fra mange forskellige opgaver, skal AI'en effektivt lære at udføre disse opgaver for at forudsige godt.

Denne uforudsigelighed af generelle AI-systemer er ret grundlæggende. Selvom det i princippet er muligt omhyggeligt at konstruere AI-systemer, der har garanterede begrænsninger på deres adfærd (som nævnt senere i essayet), er AI-systemer, som de skabes nu, uforudsigelige i praksis og endda i princippet.

### Passiv AI, agenter, autonome systemer og alignment

Denne uforudsigelighed bliver særlig vigtig, når vi overvejer, hvordan AI-systemer faktisk implementeres og bruges til at opnå forskellige mål.

Mange AI-systemer er relativt passive i den forstand, at de primært leverer information, og brugeren foretager handlinger. Andre, almindeligvis kaldet *agenter*, foretager selv handlinger med varierende niveauer af involvering fra en bruger. Dem der foretager handlinger med relativt mindre eksternt input eller overvågning kan betegnes som mere *autonome*. Dette danner et spektrum med hensyn til handlefrihed, fra passive værktøjer til autonome agenter.[^11]

Hvad angår AI-systemers mål, kan disse være direkte knyttet til deres træningsmål (f.eks. målet om at "vinde" for et Go-spillende system er også eksplicit det, det blev trænet til at gøre). Eller de er det måske ikke: ChatGPT's træningsmål er delvist at forudsige tekst, delvist at være en hjælpsom assistent. Men når den udfører en given opgave, leveres dens mål til den af brugeren. Mål kan også skabes af et AI-system selv, kun meget indirekte relateret til dets træningsmål.[^12]

Mål er tæt knyttet til spørgsmålet om "alignment," det vil sige spørgsmålet om hvorvidt AI-systemer vil *gøre det, vi vil have dem til at gøre*. Dette enkle spørgsmål skjuler et enormt niveau af subtilitet.[^13] Bemærk foreløbig, at "vi" i denne sætning kan referere til mange forskellige personer og grupper, hvilket fører til forskellige typer af alignment. For eksempel kan en AI være meget *lydig* (eller ["loyal"](https://arxiv.org/abs/2003.11157)) over for sin bruger – her er "vi" "hver af os." Eller den kan være mere *suveræn*, primært drevet af sine egne mål og begrænsninger, men stadig handle bredt i menneskehedens fælles interesse – "vi" er så "menneskeheden" eller "samfundet." Imellem er et spektrum, hvor en AI stort set ville være lydig, men måske nægte at foretage handlinger, der skader andre eller samfundet, overtræder loven osv.

Disse to akser – niveau af autonomi og type af alignment – er ikke helt uafhængige. For eksempel er et suverænt passivt system, selvom det ikke er helt selvmodsigende, et koncept i spænding, ligesom en lydig autonom agent er det.[^14] Der er en klar forstand, hvori autonomi og suverænitet har tendens til at gå hånd i hånd. På samme måde har forudsigelighed tendens til at være højere i "passive" og "lydige" AI-systemer, hvorimod suveræne eller autonome vil have tendens til at være mere uforudsigelige. Alt dette vil være afgørende for at forstå konsekvenserne af potentiel AGI og superintelligens.

At skabe virkelig tilpasset AI, af hvilken art det end er, kræver løsning af tre forskellige udfordringer:

1. At forstå hvad "vi" vil have – hvilket er komplekst, hvad enten "vi" betyder en specifik person eller organisation (loyalitet) eller menneskeheden bredt (suverænitet);
2. At bygge systemer, der regelmæssigt handler i overensstemmelse med disse ønsker – i det væsentlige skabe konsekvent positiv adfærd;
3. Mest fundamentalt at lave systemer, der oprigtigt "bekymrer sig om" disse ønsker snarere end blot at handle som om de gør det.

Forskellen mellem pålidelig adfærd og oprigtig omsorg er afgørende. Ligesom en menneskelig medarbejder måske følger ordrer perfekt, mens den mangler ethvert reelt engagement i organisationens mission, kan et AI-system handle tilpasset uden virkelig at værdsætte menneskelige præferencer. Vi kan træne AI-systemer til at sige og gøre ting gennem feedback, og de kan lære at ræsonnere om, hvad mennesker vil have. Men at få dem til *oprigtigt* at værdsætte menneskelige præferencer er en langt dybere udfordring.[^15]

De dybe vanskeligheder ved at løse disse alignment-udfordringer og deres implikationer for AI-risiko vil blive udforsket yderligere nedenfor. Forstå foreløbig, at alignment ikke bare er en teknisk egenskab, vi hæfter på AI-systemer, men et grundlæggende aspekt af deres arkitektur, der former deres forhold til menneskeheden.

[^5]: For en blid men teknisk introduktion til maskinlæring og AI, særligt sprogmodeller, se [denne side.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) For en anden moderne primer om AI-udryddelsesrisici, se [dette stykke.](https://www.thecompendium.ai/) For en omfattende og autoritativ videnskabelig analyse af AI-sikkerheds tilstand, se den seneste [International AI Safety Report.](https://arxiv.org/abs/2501.17805)

[^6]: Træning sker typisk ved at lede efter et lokalt maksimum af scoren i et højdimensionalt rum givet af modellens vægte. Ved at tjekke hvordan scoren ændrer sig, når vægte justeres, identificerer træningsalgoritmen hvilke justeringer forbedrer scoren mest og flytter vægtene i den retning.

[^7]: For eksempel i et billedgenkendelses-problem ville det neurale netværk outputte sandsynligheder for etiketter for billedet. En score ville være relateret til sandsynligheden AI'en tillægger det korrekte svar. Træningsproceduren ville så justere vægte, så næste gang ville AI'en outputte en højere sandsynlighed for den korrekte etiket for det billede. Dette gentages så et enormt antal gange. Den samme grundlæggende procedure bruges til træning af i det væsentlige alle moderne neurale netværk, omend med mere komplekse scoringsmekanismer.

[^8]: De fleste multimodale modeller bruger "transformer"-arkitekturen til at behandle og generere flere typer data (tekst, billeder, lyd). Disse kan alle dekomponeres til og derefter behandles på lige fod som forskellige typer "tokens." Multimodale modeller trænes først til præcist at forudsige tokens inden for massive datasæt, derefter raffineres gennem forstærkningslæring for at øge kapaciteter og forme adfærd.

[^9]: At sprogmodeller trænes til at gøre én ting – forudsige ord – har fået nogle til at kalde dem snæver AI. Men dette er misvisende: fordi at forudsige tekst godt kræver så mange forskellige kapaciteter, fører denne træningsopgave til et overraskende generelt system. Bemærk også, at disse systemer trænes omfattende gennem forstærkningslæring, hvilket effektivt repræsenterer tusindvis af mennesker, der giver modellen et belønningssignal, når den gør et godt stykke arbejde med nogen af de mange ting, den gør. Den arver så betydelig generalitet fra de mennesker, der giver denne feedback.

[^10]: Der er flere måder, hvorpå AI er uforudsigelig. En er, at man i det generelle tilfælde ikke kan forudsige, hvad en algoritme vil gøre uden faktisk at køre den; der er [teoremer](https://arxiv.org/abs/1310.3225) for denne effekt. Dette kan være sandt bare fordi output fra algoritmer kan være komplekst. Men det er særlig klart og relevant i tilfældet (såsom i skak eller Go), hvor forudsigelsen ville implicere en kapacitet (at slå AI'en), den potentielle forudsiger ikke har. For det andet vil et givent AI-system ikke altid producere det samme output selv givet det samme input – dets output indeholder tilfældighed; dette kobler også med algoritmisk uforudsigelighed. For det tredje kan uventede og emergente kapaciteter opstå fra træning, hvilket betyder, at selv *typerne* af ting et AI-system kan og vil gøre er uforudsigelige; Denne sidste type er særlig vigtig for sikkerhedsovervejelser.

[^11]: Se [her](https://arxiv.org/abs/2502.02649) for en dybdegående gennemgang af, hvad der menes med en "autonom agent" (sammen med etiske argumenter imod at bygge dem).

[^12]: Du hører måske nogle gange "AI kan ikke have sine egne mål." Dette er fuldstændig nonsens. Det er let at generere eksempler, hvor AI har eller udvikler mål, der aldrig blev givet til den og kun er kendt af den selv. Du ser ikke dette meget i nuværende populære multimodale modeller, fordi det trænes ud af dem; det kunne lige så let trænes ind i dem.

[^13]: Der er en stor litteratur. Om det generelle problem se Christians [*The Alignment Problem*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821), og Russells [*Human-Compatible*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616). På en mere teknisk side se f.eks. [dette papir](https://arxiv.org/abs/2209.00626).

[^14]: Vi vil senere se, at selvom sådanne systemer går imod tendensen, gør det dem faktisk meget interessante og nyttige.

[^15]: Dette er ikke for at sige, vi kræver følelser eller bevidsthed. Snarere er det enormt vanskeligt udefra et system at vide, hvad dets indre mål, præferencer og værdier er. "Oprigtig" her ville betyde, at vi har stærk nok grund til at stole på det, at i tilfælde af kritiske systemer kan vi satse vores liv på det.

## Kapitel 3 - Nøgleaspekter af hvordan moderne generelle AI-systemer fremstilles

Verdens mest avancerede AI-systemer fremstilles ved hjælp af overraskende lignende metoder. Her er det grundlæggende.

For virkelig at forstå et menneske er man nødt til at vide noget om biologi, evolution, børneopdragelse og mere; for at forstå AI er man også nødt til at vide noget om, hvordan det fremstilles. I løbet af de seneste fem år har AI-systemer udviklet sig enormt både i kapacitet og kompleksitet. En central fremmende faktor har været tilgængeligheden af meget store mængder beregning (eller i daglig tale "compute" når det anvendes på AI).

Tallene er svimlende. Omkring 10<sup>25</sup>-10<sup>26</sup> "floating-point operations" (FLOP) [^16] anvendes til træning af modeller som GPT-serien, Claude, Gemini osv.[^17] (Til sammenligning: hvis alle mennesker på Jorden arbejdede uafbrudt med at udføre én beregning hvert femte sekund, ville det tage omkring en milliard år at opnå dette.) Denne enorme mængde beregning muliggør træning af modeller med op til billioner af modelvægte på terabytes af data – en stor brøkdel af al kvalitetstekst, der nogensinde er blevet skrevet, sammen med store biblioteker af lyd, billeder og video. Ved at supplere denne træning med yderligere omfattende træning, der forstærker menneskelige præferencer og god opgaveudførelse, udviser modeller trænet på denne måde menneskekonkurrencedygtige præstationer på tværs af et betydeligt spektrum af grundlæggende intellektuelle opgaver, herunder ræsonnement og problemløsning.

Vi ved også (meget, meget groft) hvor meget beregningshastighed i operationer per sekund, der er tilstrækkelig til at *inferens*-hastigheden [^18] af et sådant system matcher *hastigheden* af menneskelig tekstbehandling. Det er omkring 10<sup>15</sup>-10<sup>16</sup> FLOP per sekund.[^19]

Selvom de er kraftfulde, er disse modeller af deres natur begrænsede på nøgleområder, ret analogt med hvordan et individuelt menneske ville være begrænset, hvis det blev tvunget til blot at producere tekst med en fast hastighed af ord per minut uden at stoppe op for at tænke eller bruge yderligere værktøjer. Nyere AI-systemer adresserer disse begrænsninger gennem en mere kompleks proces og arkitektur, der kombinerer flere nøgleelementer:

- Et eller flere neurale netværk, hvor én model leverer den centrale kognitive kapacitet, og op til flere andre udfører andre mere snævre opgaver;
- *Værktøjer* leveret til og anvendelige af modellen – for eksempel evnen til at søge på nettet, oprette eller redigere dokumenter, udføre programmer osv.
- *Stilladsering* der forbinder input og output af neurale netværk. Et meget simpelt stillads kunne blot tillade to "instanser" af en AI-model at konversere med hinanden, eller at den ene kontrollerer den andens arbejde.[^20]
- *Tankegang* og relaterede prompting-teknikker gør noget lignende og får en model til for eksempel at generere mange tilgange til et problem og derefter behandle disse tilgange for et samlet svar.
- *Omtræning* af modeller til at gøre bedre brug af værktøjer, stilladsering og tankegang.

Eftersom disse udvidelser kan være meget kraftfulde (og inkludere AI-systemer selv), kan disse sammensatte systemer være ret sofistikerede og dramatisk forbedre AI-kapaciteter.[^21] Og for nylig er teknikker inden for stilladsering og især tankegangs-prompting (og indarbejdelse af resultater tilbage i omtræning af modeller til at bruge disse bedre) blevet udviklet og anvendt i [o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/) og [DeepSeek R1](https://api-docs.deepseek.com/news/news250120) til at foretage mange inferens-gennemgange som respons på en given forespørgsel.[^22] Dette gør det i praksis muligt for modellen at "tænke over" sit svar og øger dramatisk disse modellers evne til at udføre højkvalitets-ræsonnement inden for videnskab, matematik og programmeringsopgaver.[^23]

For en given AI-arkitektur kan stigninger i træningsberegning [pålideligt oversættes](https://arxiv.org/abs/2405.10938) til forbedringer i et sæt klart definerede målinger. For mindre præcist definerede generelle kapaciteter (såsom dem diskuteret nedenfor) er oversættelsen mindre klar og forudsigelig, men det er næsten sikkert, at større modeller med mere træningsberegning vil have nye og bedre kapaciteter, selvom det er svært at forudsige, hvad disse vil være.

Tilsvarende har sammensatte systemer og især fremskridt inden for "tankegang" (og træning af modeller, der fungerer godt med det) låst op for skalering i *inferens*-beregning: for en given trænet kernemodel stiger i det mindste nogle AI-systemkapaciteter, efterhånden som mere beregning anvendes, der gør det muligt for dem at "tænke hårdere og længere" over komplekse problemer. Dette kommer til en høj pris i beregningshastighed og kræver hundredvis eller tusinder flere FLOP/s for at matche menneskelig præstation.[^24]

Selvom det kun er en del af det, der fører til hurtige AI-fremskridt,[^25] vil rollen af beregning og muligheden for sammensatte systemer vise sig at være afgørende for både at forhindre ukontrollerbar AGI og udvikle sikrere alternativer.

[^16]: 10<sup>27</sup> betyder 1 efterfulgt af 25 nuller, eller ti billioner billioner. En FLOP er blot en aritmetisk addition eller multiplikation af tal med en vis præcision. Bemærk at AI-hardware-ydeevne kan variere med en faktor på ti mere afhængigt af den aritmetiske præcision og computerens arkitektur. At tælle logiske gate-operationer (ANDS, ORS, AND NOTS) ville være fundamentalt, men disse er ikke almindeligt tilgængelige eller benchmark'et; til nuværende formål er det nyttigt at standardisere på 16-bit operationer (FP16), selvom passende konverteringsfaktorer bør etableres.

[^17]: En samling af estimater og hårde data er tilgængelig fra [Epoch AI](https://epochai.org/data/large-scale-ai-models) og indikerer omkring 2×10<sup>25</sup> 16-bit FLOP for GPT-4; dette matcher nogenlunde [tal der blev lækket](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) for GPT-4. Estimater for andre mid-2024 modeller er alle inden for en faktor på få af GPT-4.

[^18]: Inferens er simpelthen processen med at generere et output fra et neuralt netværk. Træning kan betragtes som en rækkefølge af mange inferenser og modelvægt-justeringer.

[^19]: For tekstproduktion krævede den oprindelige GPT-4 560 TFLOP per token genereret. Omkring 7 tokens/s er nødvendigt for at følge med menneskelig tankegang, så dette giver ≈3×10<sup>15</sup> FLOP/s. Men effektiviseringer har drevet dette ned; [denne NVIDIA-brochure](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/) indikerer for eksempel så lidt som 3×10<sup>14</sup> FLOP/s for en sammenlignelig præsterende Llama 405B model.

[^20]: Som et lidt mere komplekst eksempel kunne et AI-system først generere flere mulige løsninger på et matematikproblem, derefter bruge en anden instans til at kontrollere hver løsning og til sidst bruge en tredje til at syntetisere resultaterne til en klar forklaring. Dette muliggør mere grundig og pålidelig problemløsning end et enkelt gennemløb.

[^21]: Se for eksempel detaljer om [OpenAI's "Operator"](https://openai.com/index/introducing-operator/), [Claude's værktøjskapaciteter](https://docs.anthropic.com/en/docs/build-with-claude/computer-use) og [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). OpenAI's [Deep Research](https://openai.com/index/introducing-deep-research/) har formodentlig en ret sofistikeret arkitektur, men detaljer er ikke tilgængelige.

[^22]: Deepseek R1 bygger på iterativ træning og prompting af modellen, så den endelige trænede model skaber omfattende tankegangs-ræsonnement. Arkitektoniske detaljer er ikke tilgængelige for o1 eller o3, men Deepseek har afsløret, at der ikke kræves nogen særlig "hemmelig ingrediens" for at låse op for kapacitetsskalering med inferens. Men på trods af at have modtaget meget presse som omvæltende af "status quo" inden for AI, påvirker det ikke denne artikels kerneargumenter.

[^23]: Disse modeller overgår betydeligt standardmodeller på ræsonnement-benchmarks. For eksempel på GPQA Diamond Benchmark - en stringent test af spørgsmål på PhD-niveau inden for videnskab - [scorede](https://openai.com/index/learning-to-reason-with-llms/) GPT-4o 56%, mens o1 og o3 opnåede henholdsvis 78% og 88%, hvilket langt oversteg de 70% gennemsnitsscore fra menneskelige eksperter.

[^24]: OpenAI's O3 brugte formodentlig ∼10<sup>21</sup>-10<sup>22</sup> FLOP [til at gennemføre hver af ARC-AGI udfordrings-spørgsmålene](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai), som kompetente mennesker kan gøre på (lad os sige) 10-100 sekunder, hvilket giver et tal mere som ∼10<sup>20</sup> FLOP/s.

[^25]: Selvom beregning er et nøglemål for AI-systemkapacitet, interagerer det med både datakvalitet og algoritmiske forbedringer. Bedre data eller algoritmer kan reducere beregningskrav, mens mere beregning nogle gange kan kompensere for svagere data eller algoritmer.

## Kapitel 4 - Hvad er AGI og superintelligens?

Hvad er det helt præcist, verdens største teknologivirksomheder kapløber om at bygge bag lukkede døre?

Begrebet "artificial general intelligence" har eksisteret i nogen tid for at pege på "menneskelig niveau" generel kunstig intelligens. Det har aldrig været særlig veldefineret, men i de seneste år er det paradoksalt nok blevet endnu dårligere defineret, men samtidig endnu vigtigere, med eksperter der samtidig diskuterer, om AGI er årtier væk eller allerede opnået, og billioner-dollar virksomheder der kapløber "mod AGI." (Tvetydigheden omkring "AGI" blev fremhævet for nylig, da [lækkede dokumenter angiveligt afslørede](https://gizmodo.com/leaked-documents-show-openai-has-a-very-clear-definition-of-agi-2000543339), at i OpenAIs kontrakt med Microsoft blev AGI defineret som AI, der genererer 100 milliarder dollars i indtægter for OpenAI – en ret mere pengegrisk end intellektuel definition.)

Der er to kerneproblememer med ideen om AI med "menneskelig intelligens." For det første er mennesker meget, meget forskellige i deres evne til at udføre enhver given type kognitiv arbejde, så der findes ikke ét "menneskelig niveau." For det andet er intelligens meget multidimensionel; selvom der måske er korrelationer, er de ufuldkomne og kan være helt anderledes i AI. Så selv hvis vi kunne definere "menneskelig niveau" for mange evner, ville AI sikkert være langt hinsides det på nogle områder, mens den er ret under på andre.[^26]

Det er ikke desto mindre afgørende at kunne diskutere typer, niveauer og tærskler for AI-evner. Tilgangen her er at understrege, at generel kunstig intelligens er her, og at den kommer – og vil komme – på forskellige evneniveauer, hvor det er praktisk at knytte begreber til, selvom de er reduktive, fordi de svarer til afgørende tærskler hvad angår AIs effekter på samfund og menneskehed.

Vi definerer "fuld" AGI som synonymt med "over-menneskelig generel kunstig intelligens," hvilket betyder et AI-system, der er i stand til at udføre stort set alle menneskelige kognitive opgaver på eller over topekspertniveau, samt tilegne sig nye færdigheder og overføre evner til nye domæner. Dette er i tråd med, hvordan "AGI" ofte defineres i den moderne litteratur. Det er vigtigt at bemærke, at dette er en *meget* høj tærskel. Intet menneske har denne type intelligens; snarere er det den type intelligens, som store samlinger af topeksperter ville have, hvis de blev kombineret. Vi kan kalde "superintelligens" en evne, der går ud over dette, og definere mere begrænsede evneniveauer ved "menneskelig-konkurrencedygtig" og "ekspert-konkurrencedygtig" GPAI, som udfører en bred vifte af opgaver på typisk professionelt eller menneskeligt ekspertniveau.[^27]

Disse begreber og nogle andre er samlet i [tabellen](https://keepthefuturehuman.ai/essay/docs/#tab:terms) nedenfor. For en mere konkret fornemmelse af, hvad de forskellige grader af systemer kan gøre, er det nyttigt at tage definitionerne alvorligt og overveje, hvad de betyder.

| AI-type                      | Relaterede begreber                       | Definition                                                                                                                                                                                                     | Eksempler                                                                                                                                        |
| ---------------------------- | ----------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| Snæver AI                    | Svag AI                                   | AI trænet til en specifik opgave eller familie af opgaver. Udmærker sig i sit domæne, men mangler generel intelligens eller evne til at overføre læring.                                                    | Billedgenkendelses-software; Stemmeassistenter (f.eks. Siri, Alexa); Skakprogrammer; DeepMinds AlphaFold                                      |
| Værktøjs-AI                  | Forstærket intelligens, AI-assistent      | (Diskuteres senere i essayet.) AI-system der forstærker menneskelige evner. Kombinerer menneskelig-konkurrencedygtig generel AI, snæver AI og garanteret kontrol, prioriterer sikkerhed og samarbejde. Støtter menneskelig beslutningstagning. | Avancerede kodeassistenter; AI-drevne forskningsværktøjer; Sofistikerede dataanalyseplatforme. Kompetente men snævre og kontrollerbare agenter |
| Generel kunstig intelligens (GPAI) |                                           | AI-system, der kan tilpasse sig forskellige opgaver, inklusive dem det ikke specifikt er trænet til.                                                                                                         | Sprogmodeller (f.eks. GPT-4, Claude); Multimodale AI-modeller; DeepMinds MuZero                                                               |
| Menneskelig-konkurrencedygtig GPAI | AGI \[svag\]                              | Generel kunstig intelligens, der udfører opgaver på gennemsnitligt menneskeligt niveau, nogle gange overgår det.                                                                                             | Avancerede sprogmodeller (f.eks. O1, Claude 3.5); Nogle multimodale AI-systemer                                                              |
| Ekspert-konkurrencedygtig GPAI | AGI \[delvis\]                            | Generel kunstig intelligens, der udfører de fleste opgaver på menneskeligt ekspertniveau, med betydelig men begrænset autonomi                                                                               | Muligvis en værktøjsudrustet og stilladseret O3, i hvert fald til matematik, programmering og nogle hårde videnskaber                        |
| AGI \[fuld\]                 | Over-menneskelig GPAI                     | AI-system, der autonomt kan udføre stort set alle menneskelige intellektuelle opgaver på eller ud over ekspertniveau, med effektiv læring og vidensoverførsel.                                            | \[Ingen nuværende eksempler – teoretisk\]                                                                                                     |
| Superintelligens             | Højt over-menneskelig GPAI                | AI-system, der langt overgår menneskelige evner på tværs af alle domæner og overgår kollektiv menneskelig ekspertise. Denne overlegenhed kunne være i generalitet, kvalitet, hastighed og/eller andre mål. | \[Ingen nuværende eksempler – teoretisk\]                                                                                                     |

Vi oplever allerede, hvordan det er at have GPAI'er op til menneskelig-konkurrencedygtigt niveau. Dette er blevet integreret relativt gnidningsløst, da de fleste brugere oplever dette som at have en smart, men begrænset midlertidig medarbejder, der gør dem mere produktive med blandet indvirkning på kvaliteten af deres arbejde.[^28]

Det, der ville være anderledes ved ekspert-konkurrencedygtig GPAI, er, at den ikke ville have de grundlæggende begrænsninger hos nutidens AI og ville gøre de ting, eksperter gør: uafhængigt økonomisk værdifuldt arbejde, reel videnskabelse, teknisk arbejde man kan regne med, mens den sjældent (dog stadig lejlighedsvis) laver dumme fejl.

Ideen om fuld AGI er, at den *virkelig gør* alle de kognitive ting, selv de mest kapable og effektive mennesker gør, autonomt og uden nødvendig hjælp eller overvågning. Dette inkluderer sofistikeret planlægning, læring af nye færdigheder, håndtering af komplekse projekter osv. Den kunne udføre original banebrydende forskning. Den kunne drive en virksomhed. Uanset hvad dit job er, hvis det primært udføres ved computer eller over telefon, *kunne den gøre det mindst lige så godt som dig.* Og sandsynligvis meget hurtigere og billigere. Vi vil diskutere nogle af konsekvenserne nedenfor, men for nu er udfordringen for dig virkelig at tage dette alvorligt. Forestil dig de ti mest vidende og kompetente mennesker, du kender eller kender til – inklusive direktører, videnskabsfolk, professorer, topingeniører, psykologer, politiske ledere og forfattere. Pak dem alle sammen i én, som også taler 100 sprog, har en enorm hukommelse, opererer hurtigt, er utrættelig og altid motiveret og arbejder for under mindsteløn.[^29] Det giver en fornemmelse af, hvad AGI ville være.

For superintelligens er forestillingen sværere, fordi ideen er, at den kunne udføre intellektuelle bedrifter, som intet menneske eller endda samling af mennesker kan – den er per definition uforudsigelig for os. Men vi kan få en fornemmelse. Som en ren baseline, overvej masser af AGI'er, hver meget mere kapabel end selv den bedste menneskelige ekspert, kørende med 100 gange menneskelig hastighed, med enorm hukommelse og fantastisk koordinationsevne.[^30] Og det går op derfra. At have med superintelligens at gøre ville være mindre som at samtale med et anderledes sind, mere som at forhandle med en anderledes (og mere avanceret) civilisation.

Så hvor tæt *er vi* på AGI og superintelligens?


[^26]: For eksempel overgår nuværende AI-systemer langt menneskelig evne i hurtig regning eller hukommelsesopgaver, mens de halter bagud i abstrakt ræsonnement og kreativ problemløsning.

[^27]: Meget vigtigt, som konkurrent ville sådan AI have flere store strukturelle fordele herunder: den ville ikke blive træt eller have andre individuelle behov som mennesker; den kan køre med højere hastigheder bare ved at skalere beregningskraft; den kan kopieres sammen med enhver ekspertise eller viden, den tilegner sig – og neurale netværks tilegnede viden kan endda "flettes" for at overføre hele færdighedssæt mellem sig selv; den kunne kommunikere med maskinhastighed; og den kunne selv-modificere eller selv-forbedre sig på mere betydningsfulde måder og højere hastighed end noget menneske.

[^28]: Hvis du ikke har brugt tid med at bruge nutidens topmoderne AI-systemer, anbefaler jeg det: de er genuint nyttige og kapable, og det er også vigtigt for at kalibrere den effekt, AI vil have, efterhånden som de bliver mere kraftfulde.

[^29]: Overvej et større forskningshospital: fuldt realiseret AGI kunne samtidig analysere alle indkommende patientdata, følge med i hvert nyt medicinsk paper, foreslå diagnoser, designe behandlingsplaner, håndtere kliniske forsøg og koordinere personaleplanlægning – alt sammen mens den opererer på et niveau, der matcher eller overgår hospitalets topspecialister på hvert område. Og den kunne gøre dette for flere hospitaler samtidig til en brøkdel af de nuværende omkostninger. Desværre må du også overveje et organiseret kriminalitetssyndikat: fuldt realiseret AGI kunne samtidig hacke, efterligne, spionere på og afpresse tusindvis af ofre, følge med retshåndhævelsen (som automatiserer meget langsommere), designe nye måder at tjene penge på og koordinere personaleplanlægning – hvis der er noget personale.

[^30]: I sit [essay](https://darioamodei.com/machines-of-loving-grace) kaldte Dario Amodei, CEO for Anthropic, et "Land af \[en million\] genier" til minde.

## Kapitel 5 - På tærsklen

Vejen fra nutidens AI-systemer til fuldstændig udviklede AGI virker chokerende kort og forudsigelig.

De seneste ti år har budt på dramatiske fremskridt inden for AI drevet af enorme [beregnings-](https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year), menneskelige og [finansielle](https://arxiv.org/abs/2405.21015) ressourcer. Mange snævre AI-anvendelser klarer sig bedre end mennesker på deres tildelte opgaver og er bestemt langt hurtigere og billigere.[^31] Og der findes også snævre overmenneskelige agenter, som kan slå alle mennesker i spil inden for begrænsede domæner som [Go](https://www.nature.com/articles/nature16961), [skak](https://arxiv.org/abs/1712.01815) og [poker](https://www.deepstack.ai/), såvel som mere [generelle agenter](https://deepmind.google/discover/blog/a-generalist-agent/), der kan planlægge og udføre handlinger i forenklede simulerede miljøer lige så effektivt som mennesker.

Mest fremtrædende er nutidens generelle AI-systemer fra OpenAI/Microsoft, Google/Deepmind, Anthropic/Amazon, Facebook/Meta, X.ai/Tesla og andre[^32] opstået siden begyndelsen af 2023 og støt (om end ujævnt) øget deres kapaciteter siden da. Alle disse er blevet skabt via token-forudsigelse på enorme tekst- og multimediedatasæt kombineret med omfattende forstærkende feedback fra mennesker og andre AI-systemer. Nogle af dem inkluderer også omfattende værktøjs- og stilladseringssystemer.

### Styrker og svagheder ved nuværende generelle systemer

Disse systemer klarer sig godt på et stadig bredere spektrum af tests designet til at måle intelligens og ekspertise, med fremskridt der har overrasket selv eksperter på området:

- Da GPT-4 først blev frigivet, [matchede eller oversteg den typisk menneskelig præstation](https://arxiv.org/abs/2303.08774) på standardiserede akademiske tests inklusive SAT, GRE, optagelsesprøver og advokateksamen. Nyere modeller klarer sig sandsynligvis betydeligt bedre, selvom resultaterne ikke er offentligt tilgængelige.
- Turing-testen – længe betragtet som et nøglebenchmark for "sand" AI – bestås nu rutinemæssigt i nogle former af moderne sprogmodeller, både uformelt og i [formelle studier](https://arxiv.org/abs/2405.08007).[^33]
- På det omfattende MMLU benchmark, der spænder over 57 akademiske fag, [opnår nyere modeller ekspertniveauresultater](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu) (∼90%)[^34]
- Teknisk ekspertise har udviklet sig dramatisk: GPQA benchmarket for fysik på kandidatniveau så [præstationen springe](https://epoch.ai/data/ai-benchmarking-dashboard) fra næsten tilfældige gæt (GPT-4, 2022) til ekspertniveau (o1-preview, 2024).
- Selv tests specifikt designet til at være AI-resistente falder: OpenAIs O3 [løser angiveligt](https://www.nextbigfuture.com/2024/12/openai-releases-o3-model-with-high-performance-and-high-cost.html) ARC-AGI abstrakte problemløsningsbenchmark på menneskeniveau, opnår topekspert-programmeringsydelse og scorer 25% på Epoch AIs "frontier math"-problemer designet til at udfordre elitematematikere.[^35]
- Tendensen er så klar, at MMLUs udvikler nu har skabt ["Menneskehedens Sidste Eksamen"](https://agi.safe.ai/) – et ildevarslende navn, der reflekterer muligheden for, at AI snart vil overgå menneskelig præstation på enhver meningsfuld test. På skrivende tidspunkt er der påstande om AI-systemer, der opnår 27% (ifølge [Sam Altman](https://x.com/sama/status/1886220281565381078)) og 35% (ifølge [dette papir](https://arxiv.org/abs/2502.09955)) på denne ekstremt svære eksamen. Det er højst usandsynligt, at nogen individuel person kunne gøre det samme.

På trods af disse imponerende tal (og deres åbenlyse intelligens, når man interagerer med dem)[^36] er der mange ting, som (i det mindste de frigivne versioner af) disse neurale netværk *ikke kan* gøre. I øjeblikket er de fleste ikke-kropslige – eksisterer kun på servere – og behandler højst tekst, lyd og stilbilleder (men ikke video). Afgørende er det, at de fleste ikke kan udføre komplekse planlagte aktiviteter, der kræver høj nøjagtighed.[^37] Og der er en række andre kvaliteter, der er stærke i menneskelig kognition på højt niveau, som i øjeblikket er lave i frigivne AI-systemer.

Den følgende tabel opremser en række af disse baseret på AI-systemer fra midten af 2024 som GPT-4o, Claude 3.5 Sonnet og Google Gemini 1.5.[^38] Nøglespørgsmålet for hvor hurtigt generel AI vil blive mere kraftfuld er: i hvilken grad vil det bare at gøre *mere af det samme* producere resultater, versus tilføjelse af yderligere men *kendte* teknikker, versus udvikling eller implementering af *virkelig nye* AI-forskningsretninger. Mine egne forudsigelser for dette er givet i tabellen med hensyn til hvor sandsynligt hvert af disse scenarier er til at få den kapacitet til og ud over menneskeniveau.

<table><tbody><tr><th>Kapacitet</th><th>Beskrivelse af kapacitet</th><th>Status/prognose</th><th>Skalering/kendt/ny</th></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td colspan="4"><em>Centrale Kognitive Kapaciteter</em></td></tr><tr><td>Ræsonnement</td><td>Mennesker kan udføre nøjagtig, flertrinsforbigående, følge regler og kontrollere nøjagtighed.</td><td>Dramatiske nylige fremskridt ved brug af udvidet tankegang og gentræning</td><td>95/5/5</td></tr><tr><td>Planlægning</td><td>Mennesker udviser langsigtet og hierarkisk planlægning.</td><td>Forbedres med skala; kan styrkes kraftigt ved hjælp af stilladsering og bedre træningsmetoder.</td><td>10/85/5</td></tr><tr><td>Sandhedsforankring</td><td>GPAIer konfabulerer uforankret information for at tilfredsstille forespørgsler.</td><td>Forbedres med skala; kalibreringsdata tilgængelige i modellen; kan kontrolleres/forbedres via stilladsering.</td><td>30/65/5</td></tr><tr><td>Fleksibel problemløsning</td><td>Mennesker kan genkende nye mønstre og opfinde nye løsninger til komplekse problemer; nuværende ML-modeller kæmper.</td><td>Forbedres med skala men svagt; kan løses med neurosymbolske eller generaliserede "søge"-teknikker.</td><td>15/75/10</td></tr><tr><td colspan="4"><em>Læring og Viden</em></td></tr><tr><td>Læring & hukommelse</td><td>Mennesker har arbejds-, korttids- og langtidshukommelse, som alle er dynamiske og indbyrdes relaterede.</td><td>Alle modeller lærer under træning; GPAIer lærer inden for kontekstvindue og under finjustering; "kontinuert læring" og andre teknikker eksisterer men endnu ikke integreret i store GPAIer.</td><td>5/80/15</td></tr><tr><td>Abstraktion & rekursion</td><td>Mennesker kan kortlægge og overføre relationssæt til mere abstrakte for ræsonnement og manipulation, inklusive rekursivt "meta" ræsonnement.</td><td>Forbedres svagt med skala; kunne opstå i neurosymbolske systemer.</td><td>30/50/20</td></tr><tr><td>Verdensmodel(ler)</td><td>Mennesker har og opdaterer kontinuerligt en forudsigelig verdensmodel, inden for hvilken de kan løse problemer og foretage fysisk ræsonnement</td><td>Forbedres med skala; opdatering knyttet til læring; GPAIer svage i virkelighedsforudsigelse.</td><td>20/50/30</td></tr><tr><td colspan="4"><em>Selv og Agens</em></td></tr><tr><td>Agens</td><td>Mennesker kan handle for at forfølge mål baseret på planlægning/forudsigelse.</td><td>Mange ML-systemer er agentiske; LLMer kan gøres til agenter via wrappers.</td><td>5/90/5</td></tr><tr><td>Selvledelse</td><td>Mennesker udvikler og forfølger deres egne mål med internt genereret motivation og drivkraft.</td><td>Består stort set af agens plus originalitet; vil sandsynligvis opstå i komplekse agentiske systemer med abstrakte mål.</td><td>40/45/15</td></tr><tr><td>Selvreference</td><td>Mennesker forstår og ræsonnerer om sig selv som situeret i et miljø/kontekst.</td><td>Forbedres med skala og kunne forøges med træningsbelønning.</td><td>70/15/15</td></tr><tr><td>Selvbevidsthed</td><td>Mennesker har viden om og kan ræsonnere angående deres egne tanker og mentale tilstande.</td><td>Eksisterer på en måde i GPAIer, som nok kan bestå den klassiske "spejltest" for selvbevidsthed. Kan forbedres med stilladsering; men uklart om dette er nok.</td><td>20/55/25</td></tr><tr><td colspan="4"><em>Interface og Miljø</em></td></tr><tr><td>Kropsliggjort intelligens</td><td>Mennesker forstår og interagerer aktivt med deres virkelige miljø.</td><td>Forstærkende læring fungerer godt i simulerede og virkelige (robotiske) miljøer og kan integreres i multimodale transformere.</td><td>5/85/10</td></tr><tr><td>Multi-sans behandling</td><td>Mennesker integrerer og behandler visuelle, auditive og andre sensoriske strømme i realtid.</td><td>Træning i flere modaliteter synes at "bare virke" og forbedres med skala. Realtids videobehandling er vanskelig, men f.eks. selvkørende systemer forbedres hurtigt.</td><td>30/60/10</td></tr><tr><td colspan="4"><em>Kapaciteter af Højere Orden</em></td></tr><tr><td>Originalitet</td><td>Nuværende ML-modeller er kreative i at transformere og kombinere eksisterende ideer/værker, men mennesker kan bygge nye rammer og strukturer, nogle gange knyttet til deres identitet.</td><td>Kan være svært at skelne fra "kreativitet," som måske skalerer ind i den; kan opstå fra kreativitet plus selvbevidsthed.</td><td>50/40/10</td></tr><tr><td>Følsomhed</td><td>Mennesker oplever qualia; disse kan være af positiv, negativ eller neutral valens; det er "som noget" at være et menneske.</td><td>Meget vanskeligt og filosofisk problematisk at afgøre, om et givet system har dette.</td><td>5/10/85</td></tr></tbody></table>

Nøglekapaciteter i øjeblikket under menneskeligt ekspertniveau i moderne GPAI-systemer, grupperet efter type. Den tredje kolonne opsummerer nuværende status. Sidste kolonne viser forudsagt sandsynlighed (%) for, at menneskeniveau vil opnås gennem: skalering af nuværende teknikker / kombination med kendte teknikker / udvikling af nye teknikker. Disse kapaciteter er ikke uafhængige, og stigning i en går typisk sammen med stigninger i andre. Bemærk at ikke alle (især følsomhed) er nødvendige for AI-systemer, der kan fremme AI-udvikling, hvilket fremhæver muligheden for kraftfuld men ikke-følsom AI.

At opdele det der "mangler" på denne måde gør det ret klart, at vi er godt på vej mod bredt overmenneskelig intelligens ved at skalere eksisterende eller kendte teknikker.[^39]

Der kunne stadig være overraskelser. Selv hvis vi ser bort fra "følsomhed," kunne der være nogle af de oplistede centrale kognitive kapaciteter, der virkelig ikke kan gøres med nuværende teknikker og kræver nye. Men overvej dette. Den nuværende indsats, der ydes af mange af verdens største virksomheder, svarer til flere gange Apollo-projektets og titusinder af gange Manhattan-projektets udgifter,[^40] og ansætter tusinder af de allerførende tekniske folk til uhørte lønninger. Dynamikken fra de seneste år har nu bragt mere menneskelig intellektuel ildkraft til dette (med AI nu tilføjet) end nogen bestræbelse i historien. Vi bør ikke satse på fiasko.

### Det store mål: generaliserede autonome agenter

Udviklingen af generel AI gennem de seneste år har fokuseret på at skabe generel og kraftfuld men værktøjsagtig AI: den fungerer primært som en (ret) loyal assistent og tager generelt ikke handlinger på egen hånd. Dette er delvis ved design, men hovedsagelig fordi disse systemer simpelthen ikke har været kompetente nok til de relevante færdigheder til at blive betroet komplekse handlinger.[^41]

AI-virksomheder og forskere [skifter dog i stigende grad fokus](https://www.axios.com/2025/01/23/davos-2025-ai-agents) mod *autonome* ekspertniveau-generelle-formål agenter.[^42] Dette ville tillade systemerne at agere mere som en menneskelig assistent, som brugeren kan delegere virkelige handlinger til.[^43] Hvad vil det kræve? En række af kapaciteterne i "hvad der mangler"-tabellen er implicerede, inklusive stærk sandhedsforankring, læring og hukommelse, abstraktion og rekursion samt verdensmodellering (for intelligens), planlægning, agens, originalitet, selvledelse, selvreference og selvbevidsthed (for autonomi), og multi-sans-behandling, kropsliggjort intelligens og fleksibel problemløsning (for generalitet).[^44]

Dette triple-kryds af høj autonomi (uafhængighed af handling), høj generalitet (omfang og opgavebredde) og høj intelligens (kompetence til kognitive opgaver) er i øjeblikket unikt for mennesker. Det er implicit, hvad mange sandsynligvis har i tankerne, når de tænker på AGI – både med hensyn til dens værdi såvel som dens risici.

Dette giver en anden måde at definere A-G-I som ***A*** utonom- ***G*** enerel- ***I*** ntelligens, og vi vil se, at dette triple kryds giver en meget værdifuld linse for høj-kapacitets systemer både i forståelsen af deres risici og belønninger og i styring af AI.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Simple-1024x1024.png&w=3840&q=75) Den transformative A-G-I kraft- og risikozone opstår fra krydsfeltet mellem tre nøgleegenskaber: høj Autonomi, høj Intelligens til opgaver og høj Generalitet.

### AI-(selv-)forbedrings-cyklusen

En sidste afgørende faktor i forståelsen af AI-fremskridt er AIs unikke teknologiske feedback-loop. I udviklingen af AI bringer succes – i både demonstrerede systemer og deployerede produkter – yderligere investering, talent og konkurrence, og vi befinder os i øjeblikket midt i en enorm AI-hype-plus-virkelighed feedback-loop, der driver hundredvis af milliarder eller endda billioner af dollars i investering.

Denne type feedback-cyklus kunne ske med enhver teknologi, og vi har set det i mange, hvor markedssucces avler investering, som avler forbedring og bedre markedssucces. Men AI-udvikling går længere, idet AI-systemer nu hjælper med at udvikle nye og mere kraftfulde AI-systemer.[^45] Vi kan tænke på denne feedback-loop i fem stadier, hver med en kortere tidsskala end den sidste, som vist i tabellen.

*AI-forbedrings-cyklusen opererer på tværs af flere tidsskalaer, hvor hvert stadie potentielt kan accelerere efterfølgende stadier. Tidligere stadier er godt i gang, mens senere stadier forbliver spekulative men kunne forløbe meget hurtigt, når de først er låst op.*

Flere af disse stadier er allerede i gang, og et par er klart ved at komme i gang. Det sidste stadie, hvor AI-systemer autonomt forbedrer sig selv, har været en fast bestanddel af litteraturen om risikoen ved meget kraftfulde AI-systemer, og af god grund.[^46] Men det er vigtigt at bemærke, at det blot er den mest drastiske form af en feedback-cyklus, der allerede er begyndt og kunne føre til flere overraskelser i den hurtige udvikling af teknologien.


[^31]: Du bruger meget mere af denne AI, end du sandsynligvis tror, til at drive taleoprettelse og -genkendelse, billedbehandling, nyhedsfeed-algoritmer osv.

[^32]: Mens forholdet mellem disse par af virksomheder er ret komplekse og nuancerede, har jeg eksplicit opført dem for at indikere både den enorme samlede markedsværdi af firmaer, der nu er engageret i AI-udvikling, og også at selv bag "mindre" virksomheder som Anthropic sidder enormt dybe lommer via investeringer og større partnerskabsaftaler.

[^33]: Det er blevet fashionabelt at nedgøre Turing-testen, men den er ret kraftfuld og generel. I svage versioner indikerer den, om typiske mennesker, der interagerer med en AI (som er trænet til at agere menneskeligt) på typiske måder i korte perioder, kan fortælle, om det er en AI. Det kan de ikke. For det andet kan en stærkt modstridende Turing-test undersøge grundlæggende ethvert element af menneskelig kapacitet og intelligens – ved f.eks. at sammenligne et AI-system med en menneskelig ekspert, evalueret af andre menneskelige eksperter. Der er en forstand på hvilken, at meget af AI-evaluering er en generaliseret form for Turing-test.

[^34]: Dette er per domæne – intet menneske kunne plausibelt opnå sådanne scores på tværs af alle fag samtidigt.

[^35]: Dette er problemer, der ville tage selv fremragende matematikere betydelig tid at løse, hvis de overhovedet kunne løse dem.

[^36]: Hvis du er skeptisk af sind, bevar din skepsis men prøv virkelig de mest aktuelle modeller, såvel som prøv selv nogle af de testspørgsmål, de kan bestå. Som fysilkprofessor ville jeg forudsige med næsten sikkerhed, at f.eks. topmodellerne ville bestå kandidateksamen i vores afdeling.

[^37]: Dette og andre svagheder som konfabulation har bremset markedsadoptionen og ført til et gab mellem opfattede og påståede kapaciteter (som også skal ses gennem linsen af intens markedskonkurrence og behovet for at tiltrække investering). Dette har forvirret både offentligheden og politiske beslutningstagere om den faktiske tilstand af AI-fremskridt. Mens det måske ikke matcher hypen, er fremskridtet meget reelt.

[^38]: Det store fremskridt siden da har været udviklingen af systemer trænet til topkvalitets ræsonnement, der udnytter mere beregning under inferens og større forstærkende læring. Fordi disse modeller er nye og deres kapaciteter mindre testede, har jeg ikke helt omdannet denne tabel undtagen for "ræsonnement," som jeg betragter som grundlæggende løst. Men jeg har opdateret forudsigelser baseret på erfarne og rapporterede kapaciteter af disse systemer.

[^39]: Tidligere bølger af AI-optimisme i 1960erne og 1980erne endte i "AI-vintre," når lovede kapaciteter ikke materialiserede sig. Den nuværende bølge adskiller sig dog fundamentalt ved at have opnået overmenneskelig præstation i mange domæner, understøttet af massive beregningsressourcer og kommerciel succes.

[^40]: Det fulde Apollo-projekt [kostede omkring 250 milliarder USD i 2020-dollars](https://www.planetary.org/space-policy/cost-of-apollo), og Manhattan-projektet [mindre end en tiendedel deraf](https://www.brookings.edu/the-costs-of-the-manhattan-project/). Goldman Sachs [projekterer en billion dollars udgifter bare på AI-datacentre](https://www.datacenterdynamics.com/en/news/goldman-sachs-1tn-to-be-spent-on-ai-data-centers-chips-and-utility-upgrades-with-little-to-show-for-it-so-far/) over de næste år.

[^41]: Selvom mennesker laver masser af fejl, undervurderer vi bare hvor pålidelige vi kan være! Fordi sandsynligheder multipliceres, kræver en opgave der kræver 20 trin at udføre korrekt, at hvert trin er 97% pålideligt bare for at få det gjort rigtigt halvdelen af tiden. Vi laver sådanne opgaver hele tiden.

[^42]: Et stærkt skridt i denne retning er for ganske nylig blevet taget med OpenAIs ["Deep Research"](https://openai.com/index/introducing-deep-research/) assistent, der autonomt udfører generel forskning, beskrevet som "en ny agentisk kapacitet, der udfører flertrinsforskning på internettet for komplekse opgaver."

[^43]: Ting som at udfylde det besværlige PDF-formular, booke flyvninger osv. Men med en PhD i 20 fag! Så også: skrive det speciale for dig, forhandle den kontrakt for dig, bevise det teorem for dig, skabe den reklamekampagne for dig osv. Hvad gør *du*? Du fortæller det, hvad det skal gøre, selvfølgelig.

[^44]: Bemærk at følsomhed *ikke* er klart påkrævet, og at AI i dette triple-kryds ikke nødvendigvis indebærer det.

[^45]: Den nærmeste analogi her er måske chip-teknologi, hvor udvikling har opretholdt Moores lov i årtier, da computerteknologier hjælper folk med at designe næste generation af chip-teknologi. Men AI vil være langt mere direkte.

[^46]: Det er vigtigt at lade det synke ind et øjeblik, at AI kunne – snart – forbedre sig selv på en tidsskala af dage eller uger. Eller mindre. Husk dette, når nogen fortæller dig, at en AI-kapacitet bestemt er langt væk.

## Kapitel 6 - Kapløbet om AGI

Hvad er drivkræfterne bag kapløbet om at bygge AGI, både for virksomheder og lande?

De seneste hurtige fremskridt inden for AI har både resulteret fra og skabt et ekstraordinært niveau af opmærksomhed og investering. Dette er delvist drevet af succes i AI-udvikling, men der foregår mere. Hvorfor kappes nogle af Jordens største virksomheder, og endda lande, om at bygge ikke bare AI, men AGI og superintelligens?

### Hvad har drevet AI-forskning mod menneskelignende AI

Indtil de seneste fem år eller deromkring har AI hovedsageligt været et akademisk og videnskabeligt forskningsproblem, og derfor primært drevet af nysgerrighed og trangen til at forstå intelligens og hvordan man skaber den på et nyt substrat.

I denne fase var der relativt lidt opmærksomhed på fordelene eller farerne ved AI blandt de fleste forskere. Når man spurgte, hvorfor AI skulle udvikles, kunne et almindeligt svar være at opstille, noget vagt, problemer som AI kunne hjælpe med: nye lægemidler, nye materialer, ny videnskab, smartere processer, og generelt forbedre tingene for mennesker.[^47]

Dette er beundringsværdige mål![^48] Selvom vi kan og vil stille spørgsmålstegn ved, om AGI – snarere end AI generelt – er nødvendig for disse mål, viser de den idealisme, som mange AI-forskere startede med.

I løbet af de seneste fem år er AI dog blevet transformeret fra et relativt rent forskningsfelt til meget mere et ingeniør- og produktfelt, primært drevet af nogle af verdens største virksomheder.[^49] Forskere er, selvom de stadig er relevante, ikke længere dem, der styrer processen.

### Hvorfor prøver virksomheder at bygge AGI?

Så hvorfor hælder gigantiske selskaber (og endnu mere investorer) enorme ressourcer i at bygge AGI? Der er to drivkræfter, som de fleste virksomheder er ganske ærlige omkring: de ser AI som drivere af produktivitet for samfundet og af profit for dem selv. Fordi generel AI per definition er generel, er der en kæmpe gevinst: i stedet for at vælge en sektor, hvor man skal skabe produkter og tjenester, kan man prøve *alle på én gang.* Store teknologivirksomheder er vokset enorme ved at producere digitale varer og tjenester, og i det mindste nogle ledere ser sikkert AI som blot det næste skridt i at levere dem ordentligt, med risici og fordele der udvider, men gentager, dem som søgemaskiner, sociale medier, bærbare computere, telefoner osv. giver.

Men hvorfor AGI? Der er et meget simpelt svar på dette, som de fleste virksomheder og investorer skyr at diskutere offentligt.[^50]

Det er, at AGI direkte, en-til-en, kan *erstatte arbejdere.*

Ikke supplere, ikke styrke, ikke gøre mere produktive. Ikke engang *fortrænge.* Alt dette kan og vil blive gjort af ikke-AGI. AGI er specifikt det, der fuldt ud kan *erstatte* tankearbejdere (og med robotteknologi mange fysiske også.) Som støtte for dette synspunkt behøver man ikke se længere end til OpenAI's [(offentligt erklærede) definition](https://openai.com/our-structure/) af AGI, som er "et højt autonomt system, der overgår mennesker i det meste økonomisk værdifulde arbejde."

Gevinsten her (for virksomheder!) er enorm. Lønomkostninger udgør en væsentlig procentdel af verdens ∼100 billioner dollars globale økonomi. Selv hvis kun en brøkdel af dette fanges ved erstatning af menneskelig arbejdskraft med AI-arbejdskraft, er dette billioner af dollars i årlig indtægt. AI-virksomheder er også bevidste om, hvem der er villige til at betale. Som de ser det, vil du ikke betale tusindvis af dollars om året for produktivitetsværktøjer. Men en virksomhed *vil* betale tusindvis af dollars om året for at erstatte din arbejdskraft, hvis de kan.

### Hvorfor lande føler, de er nødt til at kappes om AGI

Landes erklærede motivationer for at forfølge AGI fokuserer på økonomisk og videnskabeligt lederskab. Argumentet er overbevisende: AGI kunne dramatisk accelerere videnskabelig forskning, teknologisk udvikling og økonomisk vækst. Set i lyset af, hvad der står på spil, argumenterer de, kan ingen stormagt have råd til at falde bagud.[^51]

Men der er også yderligere og stort set ustilede drivkræfter. Der er ingen tvivl om, at når visse militære og nationale sikkerhedsledere mødes bag lukkede døre for at diskutere en ekstraordinært potent og katastrofalt risikabel teknologi, er deres fokus ikke på "hvordan undgår vi disse risici", men snarere "hvordan får vi dette først?" Militære og efterretningsledere ser AGI som en potentiel revolution i militære anliggender, måske den mest betydningsfulde siden atomvåben. Frygten er, at det første land, der udvikler AGI, kunne opnå en uoverkommelig strategisk fordel. Dette skaber en klassisk våbenkapløbsdynamik.

Vi vil se, at denne "kapløb om AGI"-tankegang,[^52] selvom den er overbevisende, er dybt fejlbehæftet. Dette er ikke fordi kapløb er farligt og risikabelt – selvom det er det – men på grund af teknologiens natur. Den usagte antagelse er, at AGI, ligesom andre teknologier, kan kontrolleres af den stat, der udvikler den, og er en magtgivende velsignelse for det samfund, der har mest af den. Som vi vil se, vil den sandsynligvis ikke være nogen af delene.

### Hvorfor superintelligens?

Mens virksomheder offentligt fokuserer på produktivitet, og lande på økonomisk og teknologisk vækst, er disse blot begyndelsen for dem, der bevidst forfølger fuld AGI og superintelligens. Hvad har de virkelig i tankerne? Selvom det sjældent siges højt, inkluderer det:

1. Kure mod mange eller alle sygdomme;
2. Stopning og vending af aldring;
3. Nye bæredygtige energikilder som fusion;
4. Menneskelige opgraderinger eller designerorganismer via genmanipulation;
5. Nanoteknologi og molekylær fremstilling;
6. Sind-uploads;
7. Eksotisk fysik eller rumteknologier;
8. Overmenneskelig rådgivning og beslutningsstøtte;
9. Overmenneskelig planlægning og koordination.

De første tre er hovedsageligt "enkeltsidet" teknologier – dvs. sandsynligvis ganske stærkt netto positive. Det er svært at argumentere imod at helbrede sygdomme eller at kunne leve længere, hvis man vælger det. Og vi har allerede høstet den negative side af fusion (i form af atomvåben); det ville være dejligt nu at få den positive side. Spørgsmålet med denne første kategori er, om det at få disse teknologier hurtigere kompenserer for risikoen.

De næste fire er klart tveæggede: transformative teknologier med både potentielt enorme fordele og immense risici, meget ligesom AI. Alle disse, hvis de sprang ud af en sort boks i morgen og blev implementeret, ville være utroligt svære at håndtere.[^53]

De sidste to drejer sig om den overmenneskelige AI, der gør ting selv frem for blot at opfinde teknologi. Mere præcist, ved at lægge eufemismer til side, involverer disse kraftfulde AI-systemer, der fortæller folk, hvad de skal gøre. At kalde dette "rådgivning" er uredelig, hvis systemet, der giver råd, er langt mere kraftfuldt end den rådgivne, som ikke meningsfuldt kan forstå grundlaget for beslutningen (eller selvom dette gives, stole på at rådgiveren ikke ville give en tilsvarende overbevisende begrundelse for en anden beslutning.)

Dette peger på en vigtig ting, der mangler på ovenstående liste:

10. Magt.

Det er overdrevent klart, at meget af det, der ligger til grund for det nuværende kapløb om overmenneskelig AI, er ideen om, at *intelligens = magt*. Hver deltager satser på at være den bedste indehaver af denne magt, og at de vil være i stand til at udøve den af tilsyneladende velgørende årsager uden at den glider eller bliver taget fra deres kontrol.

Det vil sige, det virksomheder og nationer virkelig jager, er ikke blot frugterne af AGI og superintelligens, men magten til at kontrollere, hvem der får adgang til dem, og hvordan de bruges. Virksomheder ser sig selv som ansvarlige forvaltere af denne magt i service af aktionærer og menneskeheden; nationer ser sig selv som nødvendige vogtere, der forhindrer fjendtlige magter i at opnå afgørende fordel. Begge tager farligt fejl og anerkender ikke, at superintelligens per sin natur ikke kan kontrolleres pålideligt af nogen menneskelig institution. Vi vil se, at superintelligente systemers natur og dynamikker gør menneskelig kontrol ekstremt vanskelig, hvis ikke umulig.

Disse kapløbsdynamikker – både erhvervsmæssige og geopolitiske – gør visse risici næsten uundgåelige, medmindre de afgørende afbrydes. Vi vender os nu til at undersøge disse risici og hvorfor de ikke kan afbødes tilstrækkeligt inden for et konkurrencepræget [^54] udviklingsparadigme.


[^47]: En mere præcis liste over værdige mål er FN's [Bæredygtighedsmål.](https://sdgs.un.org/goals) Disse er i en forstand det tætteste, vi kommer på et sæt globale konsensusmål for, hvad vi gerne vil se forbedret i verden. AI kunne hjælpe.

[^48]: Teknologi generelt har en transformativ økonomisk og social kraft til menneskelig forbedring, som tusindvis af år vidner om. I denne ånd kan en lang og overbevisende forklaring af en positiv AGI-vision findes i [dette essay](https://darioamodei.com/machines-of-loving-grace) af Anthropic-grundlægger Dario Amodei.

[^49]: Private AI-investeringer [begyndte at boome i 2018-19, hvor de krydsede offentlige investeringer omkring da,](https://cset.georgetown.edu/publication/tracking-ai-investment/) og har enormt overgået dem siden.

[^50]: Jeg kan bekræfte, at bag mere lukkede døre har de ingen sådan skrupler. Og det bliver mere offentligt; se for eksempel Y-combinators nye ["request for startups"](https://www.ycombinator.com/rfs), hvor mange dele eksplicit opfordrer til fuldstændig erstatning af menneskelige arbejdere. For at citere dem: "Værdiforslaget for B2B SaaS var at gøre menneskelige arbejdere trinvist mere effektive. Værdiforslaget for vertikale AI-agenter er at automatisere arbejdet fuldstændigt...Det er helt muligt, at denne mulighed er stor nok til at præge yderligere 100 enhjørninger." (For dem, der ikke er vandt til Silicon Valley-sprog, er "B2B" business-to-business og en enhjørning er en virksomhed til 1 milliard dollars. Det vil sige, de taler om mere end hundrede milliard-plus-dollar-forretninger, der erstatter arbejdere for andre virksomheder.)

[^51]: Se for eksempel en nylig [US-China Economic and Security Review Commission-rapport](https://www.uscc.gov/sites/default/files/2024-11/2024_Executive_Summary.pdf). Selvom der var overraskende lidt begrundelse i selve rapporten, var topanbefalingen, at USA "Kongressen etablerer og finansierer et Manhattan Project-lignende program dedikeret til at kappes om og erhverve en Artificial General Intelligence (AGI) kapacitet."

[^52]: Virksomheder adopterer nu denne geopolitiske indramning som et skjold mod enhver begrænsning på deres AI-udvikling, generelt på måder der er åbenlyst selvtjenende, og nogle gange på måder der ikke engang giver grundlæggende mening. Overvej Metas [Approach to Frontier AI](https://about.fb.com/news/2025/02/meta-approach-frontier-ai/), som samtidig argumenterer for, at Amerika skal "[Cementere sin] position som leder i teknologisk innovation, økonomisk vækst og national sikkerhed" og også at det skal gøre det ved åbent at frigive sine mest kraftfulde AI-systemer – hvilket inkluderer at give dem direkte til sine geopolitiske rivaler og modstandere.

[^53]: Derfor ville vi sandsynligvis være nødt til at overlade håndteringen af disse teknologier til AI'erne. Men dette ville være en meget problematisk delegation af kontrol, som vi vil vende tilbage til nedenfor.

[^54]: Konkurrence i teknologiudvikling bringer ofte vigtige fordele: forhindrer monopolistisk kontrol, driver innovation og omkostningsreduktion, muliggør forskellige tilgange og skaber gensidig overvågning. Men med AGI skal disse fordele vejes mod unikke risici fra kapløbsdynamikker og pres for at reducere sikkerhedsforanstaltninger.

## Kapitel 7 - Hvad sker der, hvis vi bygger AGI på vores nuværende kurs?

Samfundet er ikke klar til AGI-lignende systemer. Hvis vi bygger dem meget snart, kan tingene blive grimme.

Udviklingen af fuld kunstig generel intelligens – det vi her vil kalde AI, der er "uden for Portene" – ville være et fundamentalt skift i verdens natur: i sin essens betyder det at tilføje en ny art af intelligens til Jorden med større kapabilitet end menneskers.

Hvad der så sker, afhænger af mange ting, herunder teknologiens natur, valg truffet af dem, der udvikler den, og den verdenskontekst, den udvikles i.

I øjeblikket bliver fuld AGI udviklet af en håndfuld massive private virksomheder i et kapløb mod hinanden, med ringe meningsfuld regulering eller eksternt tilsyn,[^55] i et samfund med stadig svagere og endda dysfunktionelle kerneinstitutioner,[^56] i en tid med høj geopolitisk spænding og lav international koordination. Selvom nogle er altruistisk motiverede, er mange af dem, der gør det, drevet af penge, eller magt, eller begge dele.

Forudsigelse er meget svært, men der er nogle dynamikker, som er velforståede nok, og passende nok analogier med tidligere teknologier til at tilbyde en vejledning. Og desværre, på trods af AI's løfte, giver de god grund til at være dybt pessimistisk om, hvordan vores nuværende bane vil udspille sig.

For at sige det direkte, på vores nuværende kurs vil udvikling af AGI have nogle positive effekter (og gøre nogle mennesker meget, meget rige). Men teknologiens natur, de fundamentale dynamikker og den kontekst, den udvikles i, indikerer kraftigt, at: kraftig AI vil dramatisk underminere vores samfund og civilisation; vi vil miste kontrollen over den; vi kan meget vel ende i en verdenskrig på grund af den; vi vil miste (eller afgive) kontrol *til* den; det vil føre til kunstig superintelligens, som vi absolut ikke vil kontrollere, og som vil betyde enden på en menneskestyret verden.

Dette er stærke påstande, og jeg ønsker, de var ligegyldige spekulationer eller uberettiget "doomer"-tankegang. Men det er her videnskaben, spilteori, evolutionsteori og historien alle peger hen. Dette afsnit udvikler disse påstande og deres støtte i detaljer.

### Vi vil underminere vores samfund og civilisation

På trods af hvad du måske hører i Silicon Valleys bestyrelseslokaler, er det meste disruption – især af den meget hurtige variant – ikke gavnlig. Der er langt flere måder at gøre komplekse systemer værre på end bedre. Vores verden fungerer så godt, som den gør, fordi vi omhyggeligt har bygget processer, teknologier og institutioner, der har gjort den støt bedre.[^57] At tage en forhammer til en fabrik forbedrer sjældent driften.

Her er et (ufuldstændigt) katalog over måder, hvorpå AGI-systemer ville forstyrre vores civilisation.

- De ville dramatisk forstyrre arbejdsmarkedet og føre *i det mindste* til dramatisk højere indkomstulighed og potentielt store underbeskæftigelse eller arbejdsløshed på et tidsskema, der er alt for kort til, at samfundet kan tilpasse sig.[^58]
- De ville sandsynligvis føre til koncentration af enorm økonomisk, social og politisk magt – potentielt mere end nationalstaters – hos et lille antal massive private interesser, der ikke er ansvarlige over for offentligheden.
- De kunne pludselig gøre tidligere svære eller dyre aktiviteter trivielt lette og destabilisere sociale systemer, der afhænger af, at visse aktiviteter forbliver omkostningsfulde eller kræver betydelig menneskelig indsats.[^59]
- De kunne oversvømme samfundets informationsindsamlings-, -behandlings- og kommunikationssystemer med fuldstændig realistiske men falske, spammy, overdrevent målrettede eller manipulerende medier så grundigt, at det bliver umuligt at skelne, hvad der er fysisk virkeligt eller ej, menneskeligt eller ej, faktuel eller ej, og troværdigt eller ej.[^60]
- De kunne skabe farlig og næsten total intellektuel afhængighed, hvor menneskelig forståelse af nøglesystemer og teknologier svinder, mens vi i stigende grad stoler på AI-systemer, vi ikke fuldt ud kan forstå.
- De kunne effektivt afslutte den menneskelige kultur, når næsten alle kulturelle objekter (tekst, musik, visuel kunst, film osv.) konsumeret af de fleste mennesker bliver skabt, medieret eller kurateret af ikke-menneskelige sind.
- De kunne muliggøre effektive masseovervågnings- og manipulationssystemer, der kan bruges af regeringer eller private interesser til at kontrollere en befolkning og forfølge mål i konflikt med den offentlige interesse.
- Ved at underminere menneskelig diskurs, debat og valgsystemer kunne de reducere de demokratiske institutioners troværdighed til det punkt, hvor de effektivt (eller eksplicit) erstattes af andre, hvilket afslutter demokratiet i stater, hvor det i øjeblikket eksisterer.
- De kunne blive til, eller skabe, avancerede selvreplikerende intelligente softwarevira og orme, der kunne sprede sig og udvikle sig og massivt forstyrre globale informationssystemer.
- De kan dramatisk øge terrorister, dårlige aktører og skurkestaters evne til at forårsage skade via biologiske, kemiske, cyber-, autonome eller andre våben, uden at AI giver en balancerende evne til at forhindre sådan skade. På samme måde ville de underminere national sikkerhed og geopolitiske balancer ved at gøre topniveau nuklear, bio-, ingeniør- og anden ekspertise tilgængelig for regimer, der ellers ikke ville have den.
- De kunne forårsage hurtig storstilet løbsk hyper-kapitalisme med effektivt AI-drevne virksomheder, der konkurrerer i stort set elektroniske finans-, salgs- og serviceområder. AI-drevne finansielle markeder kunne operere med hastigheder og kompleksiteter langt ud over menneskelig forståelse eller kontrol. Alle fejlmodi og negative eksternaliteter i nuværende kapitalistiske økonomier kunne blive forværret og fremskyndet langt ud over menneskelig kontrol, styring eller reguleringsmæssig kapacitet.
- De kunne brænde stof på et våbenkapløb mellem nationer i AI-drevne våben, kommando-og-kontrol-systemer, cybervåben osv., hvilket skaber meget hurtig opbygning af ekstremt destruktive kapaciteter.

Disse risici er ikke spekulative. Mange af dem realiseres, som vi taler! Men overvej, *virkelig* overvej, hvordan hver enkelt ville se ud med dramatisk mere kraftig AI.

Overvej arbejdskraftforskydning, når de fleste arbejdere simpelthen ikke kan give nogen betydelig økonomisk værdi ud over hvad AI kan, inden for deres ekspertiseområde eller erfaring – eller selv hvis de omskoles! Overvej masseovervågning, hvis alle bliver individuelt overvåget og monitoreret af noget hurtigere og klogere end dem selv. Hvordan ser demokratiet ud, når vi ikke pålideligt kan stole på nogen digital information, vi ser, hører eller læser, og når de mest overbevisende offentlige stemmer ikke engang er menneskelige og ikke har nogen andel i resultatet? Hvad bliver der af krigsførelse, når generaler konstant må underkaste sig AI (eller simpelthen sætte det i kommando), for ikke at give fjenden en afgørende fordel? Enhver af ovenstående risici repræsenterer en katastrofe for den menneskelige[^61] civilisation, hvis den fuldt ud realiseres.

Du kan lave dine egne forudsigelser. Stil dig selv disse tre spørgsmål for hver risiko:

1. Ville super-kapabel, højt autonom og meget generel AI tillade det på en måde eller i et omfang, der ellers ikke ville være muligt?
2. Er der parter, som ville have gavn af ting, der forårsager, at det sker?
3. Er der systemer og institutioner på plads, der effektivt ville forhindre det i at ske?

Hvor dine svar er "ja, ja, nej", kan du se, vi har et stort problem.

Hvad er vores plan for at håndtere dem? Som det står nu, er der to på bordet vedrørende AI generelt.

Den første er at bygge sikkerhedsforanstaltninger ind i systemerne for at forhindre dem i at gøre ting, de ikke bør. Det gøres nu: kommercielle AI-systemer vil for eksempel nægte at hjælpe med at bygge en bombe eller skrive hadtale.

Denne plan er håbløst utilstrækkelig for systemer uden for Portene.[^62] Den kan hjælpe med at mindske risikoen for, at AI giver åbenbart farlig assistance til dårlige aktører. Men den vil intet gøre for at forhindre arbejdskraftforstyrrelser, magtkoncentration, løbsk hyper-kapitalisme eller erstatning af menneskelig kultur: disse er bare resultater af at bruge systemerne på tilladte måder, der gavner deres leverandører! Og regeringer vil helt sikkert få adgang til systemer til militær eller overvågningsbrug.

Den anden plan er endnu værre: simpelthen åbent at frigive meget kraftige AI-systemer, så alle kan bruge dem, som de vil,[^63] og håbe på det bedste.

Implicit i begge planer er, at nogen andre, f.eks. regeringer, vil hjælpe med at løse problemerne gennem blød eller hård lov, standarder, regler, normer og andre mekanismer, vi generelt bruger til at håndtere teknologier.[^64] Men når vi ser bort fra, at AI-selskaber allerede kæmper tand og negl mod enhver væsentlig regulering eller eksternt pålagte begrænsninger overhovedet, er det for en række af disse risici ret svært at se, hvad regulering overhovedet rigtig ville hjælpe. Regulering kunne pålægge sikkerhedsstandarder for AI. Men ville det forhindre virksomheder i at erstatte arbejdere i det hele taget med AI? Ville det forbyde folk at lade AI drive deres virksomheder for dem? Ville det forhindre regeringer i at bruge potent AI i overvågning og våben? Disse spørgsmål er fundamentale. Menneskeheden kunne potentielt finde måder at tilpasse sig dem på, men kun med *meget* mere tid. Som det er, givet den hastighed, hvormed AI når eller overstiger kapaciteterne hos de mennesker, der prøver at håndtere dem, ser disse problemer ud til at være stadig mere uløselige.

### Vi vil miste kontrollen over (i det mindste nogle) AGI-systemer

De fleste teknologier er meget kontrollerbare pr. konstruktion. Hvis din bil eller din brødrister begynder at gøre noget, du ikke vil have den til at gøre, er det bare et funktionsfejl, ikke en del af dens natur som brødrister. AI er anderledes: det bliver *dyrket* snarere end designet, dets kerneoperation er uigennemsigtig, og det er i sagens natur uforudsigeligt.

Dette tab af kontrol er ikke teoretisk – vi ser tidlige versioner allerede. Overvej først et prosaisk og formentlig godartet eksempel. Hvis du beder ChatGPT om at hjælpe dig med at blande en gift eller skrive et racistisk skrift, vil det nægte. Det er formentlig godt. Men det er også ChatGPT, der *ikke gør det, du eksplicit har bedt det om at gøre*. Andre stykker software gør ikke det. Den samme model vil heller ikke designe gifte på anmodning fra en OpenAI-medarbejder.[^65] Dette gør det meget let at forestille sig, hvordan det ville være for fremtidige mere kraftige AI at være ude af kontrol. I mange tilfælde vil de simpelthen ikke gøre, hvad vi beder om! Enten vil et givet over-menneskeligt AGI-system være absolut lydigt og loyalt over for et menneskeligt kommandosystem, eller det vil det ikke. Hvis ikke, *vil det gøre ting, det måske tror er gode for os, men som strider mod vores eksplicitte kommandoer.* Det er ikke noget, der er under kontrol. Men, kunne du sige, dette er intentionelt – disse afslag er designet, en del af det, der kaldes at "tilpasse" systemerne til menneskelige værdier. Og det er sandt. Dog har "tilpasnings"programmet selv to store problemer.[^66]

For det første, på et dybt niveau har vi ingen anelse om, hvordan man gør det. Hvordan garanterer vi, at et AI-system vil "bekymre sig om" det, vi ønsker? Vi kan træne AI-systemer til at sige og ikke sige ting ved at give feedback; og de kan lære og ræsonnere om, hvad mennesker ønsker og bekymrer sig om, ligesom de ræsonnerer om andre ting. Men vi har ingen metode – selv teoretisk – til at få dem til at dybt og pålideligt værdsætte det, mennesker bekymrer sig om. Der er højt fungerende menneskelige psykopater, som ved, hvad der betragtes som rigtigt og forkert, og hvordan de skal opføre sig. De *bekymrer sig* bare ikke. Men de kan *agere* som om de gør, hvis det passer deres formål. Ligesom vi ikke ved, hvordan man ændrer en psykopat (eller nogen anden) til nogen, der er ægte, fuldstændigt loyal eller tilpasset nogen eller noget andet, har vi *ingen anelse*[^67] om, hvordan man løser alignment-problemet i systemer avancerede nok til at modellere sig selv som agenter i verden og potentielt [manipulere deres egen træning](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) og [bedrage mennesker.](https://arxiv.org/abs/2311.08379) Hvis det viser sig umuligt eller uopnåeligt *enten* at gøre AGI fuldt lydigt eller få det til dybt at bekymre sig om mennesker, så vil det, så snart det er i stand til (og tror, det kan slippe afsted med det), begynde at gøre ting, vi ikke ønsker.[^68]

For det andet er der dybe teoretiske grunde til at tro, at *af natur* vil avancerede AI-systemer have mål og dermed adfærd, der strider mod menneskelige interesser. Hvorfor? Nå, det kan selvfølgelig *blive givet* disse mål. Et system skabt af militæret ville sandsynligvis bevidst være dårligt for i det mindste nogle parter. Meget mere generelt kunne dog et AI-system få et relativt neutralt ("tjen mange penge") eller endda tilsyneladende positivt ("reducer forurening") mål, der næsten uundgåeligt fører til "instrumentale" mål, der er ret mindre godartede.

Vi ser dette hele tiden i menneskelige systemer. Ligesom virksomheder, der forfølger profit, udvikler instrumentale mål som at erhverve politisk magt (for at afvæbne reguleringer), blive hemmelighedsfulde (for at afmægtiggøre konkurrence eller ekstern kontrol), eller underminere videnskabelig forståelse (hvis denne forståelse viser, at deres handlinger er skadelige), vil kraftige AI-systemer udvikle lignende kapaciteter – men med langt større hastighed og effektivitet. Enhver højt kompetent agent vil ønske at gøre ting som at erhverve magt og ressourcer, øge sine egne kapaciteter, forhindre sig selv i at blive dræbt, lukket ned eller afmægtiggjort, kontrollere sociale narrativer og rammer omkring sine handlinger, overbevise andre om sine synspunkter og så videre.[^69]

Og alligevel er det ikke kun en næsten uundgåelig teoretisk forudsigelse, det sker allerede observerbart i nutidens AI-systemer og stiger med deres kapabilitet. Når de evalueres, vil selv disse relativt "passive" AI-systemer under passende omstændigheder bevidst [bedrage evaluatorer om deres mål og kapaciteter, sigte mod at deaktivere tilsynsmekanismer,](https://arxiv.org/abs/2412.04984) og unddrage sig at blive lukket ned eller omtrænet ved at [fake alignment](https://arxiv.org/abs/2412.14093) eller kopiere sig selv til andre steder. Selvom det er helt uoverraskende for AI-sikkerhedsforskere, er disse adfærd meget alvorlige at observere. Og de varsler meget dårligt for langt mere kraftige og autonome AI-systemer, der kommer.

Faktisk vil generelt vores manglende evne til at sikre, at AI "bekymrer sig om" det, vi bekymrer os om, eller opfører sig kontrollerbart eller forudsigeligt, eller undgår at udvikle drift mod selvbevarelse, magterhvervelse osv., kun blive mere udtalt, efterhånden som AI bliver mere kraftig. At skabe et nyt fly indebærer større forståelse af avionik, hydrodynamik og kontrolsystemer. At skabe en mere kraftig computer indebærer større forståelse og beherskelse af computer-, chip- og softwareoperation og -design. *Ikke* sådan med et AI-system.[^70]

For at opsummere: det er tænkeligt, at AGI kunne gøres til at være fuldstændig lydig; men vi ved ikke, hvordan man gør det. Hvis ikke, vil det være mere suverænt, som mennesker, og gøre forskellige ting af forskellige årsager. Vi ved heller ikke, hvordan man pålideligt indgydr dyb "tilpasning" i AI, der ville få disse ting til at have en tendens til at være gode for menneskeheden, og i mangel af et dybt niveau af tilpasning indikerer agentskabets og intelligensens natur selv, at – ligesom mennesker og virksomheder – vil de blive drevet til at gøre mange dybt asociale ting.

Hvor efterlader det os? En verden fuld af kraftige ukontrollerede suveræne AI *kunne* ende med at være en god verden for mennesker at være i.[^71] Men efterhånden som de vokser stadigt mere kraftige, som vi vil se nedenfor, ville det ikke være *vores* verden.

Det er for ukontrollerbar AGI. Men selv hvis AGI på en eller anden måde kunne gøres perfekt kontrolleret og loyal, ville vi stadig have enorme problemer. Vi har allerede set ét: kraftig AI kan bruges og misbruges til dybt at forstyrre vores samfunds funktion. Lad os se et andet: for så vidt som AGI var kontrollerbar og ændrende kraftig (eller endda *troet* at være det), ville det så true magtstrukturer i verden, at det ville udgøre en dyb risiko.

### Vi øger radikalt sandsynligheden for storstilet krig

Forestil dig en situation i den nære fremtid, hvor det blev klart, at en virksomhedsindsats, måske i samarbejde med en national regering, var på tærsklen til hurtig selvforbedring AI. Dette sker i den nuværende kontekst af et kapløb mellem virksomheder og noget mellem lande, hvor anbefalinger bliver lavet til den amerikanske regering om eksplicit at forfølge et "AGI Manhattan-projekt", og USA kontrollerer eksport af højydende AI-chips til ikke-allierede lande.

Spilteorien her er barsk: når et sådant kapløb begynder (som det har, mellem virksomheder og noget mellem lande), er der kun fire mulige udfald:

1. Kapløbet stoppes (ved aftale eller ekstern kraft).
2. Den ene part "vinder" ved at udvikle stærk AGI og derefter stoppe de andre (ved hjælp af AI eller på anden måde).
3. Kapløbet stoppes ved gensidig destruktion af deltagernes kapacitet til at deltage.
4. Flere deltagere fortsætter med at deltage og udvikler superintelligens, nogenlunde lige så hurtigt som hinanden.

Lad os undersøge hver mulighed. Når det først er startet, ville det kræve national regeringsintervention (for virksomheder) eller hidtil uset international koordination (for lande) at stoppe et kapløb mellem virksomheder fredeligt. Men når enhver nedlukning eller betydelig forsigtighed foreslås, ville der være øjeblikkelige råb: "men hvis vi stoppes, kommer *de* til at skynde sig frem", hvor "de" nu er Kina (for USA), eller USA (for Kina), eller Kina *og* USA (for Europa eller Indien). Under denne tankegang[^72] kan ingen deltager stoppe ensidigt: så længe én forpligter sig til at deltage, føler de andre, at de ikke har råd til at stoppe.

Den anden mulighed har én side, der "vinder." Men hvad betyder det? Blot at få (på en eller anden måde lydig) AGI først er ikke nok. Vinderen skal *også* stoppe de andre i at fortsætte med at deltage – ellers vil de også få det. Dette er muligt i princippet: den, der udvikler AGI først, *kunne* opnå ustoppelig magt over alle andre aktører. Men hvad ville det faktisk kræve at opnå sådan en "afgørende strategisk fordel"? Måske ville det være ændrende militære kapaciteter?[^73] Eller cyberangrebskraft?[^74] Måske ville AGI'en bare være så fantastisk overbevisende, at den ville overbevise de andre parter om bare at stoppe?[^75] Så rig, at den køber de andre virksomheder eller endda lande?[^76]

Hvordan bygger den ene side *præcis* en AI kraftig nok til at afmægtiggøre andre fra at bygge tilsvarende kraftig AI? Men det er det lette spørgsmål.

For nu overvej, hvordan denne situation ser ud for andre magter. Hvad tænker den kinesiske regering, når USA ser ud til at opnå sådan en kapabilitet? Eller omvendt? Hvad tænker den amerikanske regering (eller kinesiske, eller russiske, eller indiske), når OpenAI eller DeepMind eller Anthropic ser ud til at være tæt på et gennembrud? Hvad sker der, hvis USA ser en ny indisk eller UAE-indsats med gennembrudsucces? De ville se både en eksistentiel trussel og – afgørende – at den eneste måde, dette "kapløb" ender på, er gennem deres egen afmægtiggørelse. Disse meget kraftige agenter – inklusive regeringer i fuldt udstyrede nationer, der helt sikkert har midlerne til at gøre det – ville være højt motiverede til enten at opnå eller ødelægge sådan en kapabilitet, hvad enten det er ved magt eller list.[^77]

Dette kunne starte småt, som sabotage af træningskørsler eller angreb på chipfremstilling, men disse angreb kan kun rigtig stoppe, når alle parter enten mister kapaciteten til at deltage i AI-kapløbet eller mister kapaciteten til at lave angrebene. Fordi deltagerne ser indsatsen som eksistentiel, vil begge tilfælde sandsynligvis repræsentere en katastrofal krig.

Det bringer os til den fjerde mulighed: kapløb til superintelligens, og på den hurtigste, mindst kontrollerede måde. Efterhånden som AI stiger i kraft, vil dets udviklere på begge sider finde det progressivt sværere at kontrollere, især fordi kapløb om kapaciteter er antitetisk til den slags omhyggelige arbejde, kontrollerbarhed ville kræve. Så dette scenario sætter os direkte i det tilfælde, hvor kontrol tabes (eller gives, som vi vil se næste) til AI-systemerne selv. Det vil sige, *AI vinder kapløbet.* Men på den anden side, i den grad kontrol *opretholdes*, fortsætter vi med at have flere gensidigt fjendtlige parter, hver ansvarlig for ekstremt kraftige kapaciteter. Det ligner krig igen.

Lad os sætte alt dette på en anden måde.[^78] Den nuværende verden har simpelthen ikke nogen institutioner, der kunne betros at huse udvikling af en AI af denne kapabilitet uden at invitere øjeblikkelig angreb.[^79] Alle parter vil korrekt ræsonnere, at enten vil den *ikke* være under kontrol – og derfor være en trussel mod alle parter, eller den *vil* være under kontrol, og derfor være en trussel mod enhver modstander, der udvikler den mindre hurtigt. Dette er atomvåbenbesiddende lande eller virksomheder huset inden for dem.

I mangel af nogen plausibel måde for mennesker at "vinde" dette kapløb, står vi tilbage med en barsk konklusion: den eneste måde, dette kapløb ender på, er enten i katastrofal konflikt eller hvor AI, og ikke nogen menneskelig gruppe, er vinderen.

### Vi giver kontrol til AI (eller det tager den)

Geopolitisk "stormagts"konkurrence er bare én af mange konkurrencer: individer konkurrerer økonomisk og socialt; virksomheder konkurrerer på markeder; politiske partier konkurrerer om magt; bevægelser konkurrerer om indflydelse. I hver arena, efterhånden som AI nærmer sig og overgår menneskelig kapabilitet, vil konkurrencepres tvinge deltagere til at delegere eller afgive mere og mere kontrol til AI-systemer – ikke fordi disse deltagere ønsker det, men fordi de [ikke har råd til ikke at gøre det.](https://arxiv.org/abs/2303.16200)

Som med andre risici ved AGI ser vi dette allerede med svagere systemer. Studerende føler pres for at bruge AI i deres opgaver, fordi mange andre studerende tydeligvis gør. Virksomheder [skynder sig at adoptere AI-løsninger af konkurrencemæssige årsager.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) Kunstnere og programmører føler sig tvunget til at bruge AI, ellers vil deres takster blive underbud af andre, der gør.

Disse føles som presset delegation, men ikke kontroltab. Men lad os skrue op for indsatsen og skubbe uret fremad. Overvej en CEO, hvis konkurrenter bruger AGI-"hjælpere" til at træffe hurtigere, bedre beslutninger, eller en militær kommandør over for en modstander med AI-forbedret kommando og kontrol. Et tilstrækkeligt avanceret AI-system kunne autonomt operere ved mange gange menneskelig hastighed, sofistikering, kompleksitet og databehandlingskapabilitet og forfølge komplekse mål på komplicerede måder. Vores CEO eller kommandør, ansvarlig for et sådant system, kan se det opnå, hvad de ønsker; men ville de forstå selv en lille del af *hvordan* det blev opnået? Nej, de ville bare måtte acceptere det. Hvad mere er, meget af det systemet kan gøre er ikke bare at tage ordrer, men rådgive sin formodede chef om, hvad der skal gøres. Dette råd vil være godt –– igen og igen.

På hvilket tidspunkt vil menneskets rolle så blive reduceret til at klikke "ja, gør det"?

Det føles godt at have kapable AI-systemer, der kan forbedre vores produktivitet, tage sig af irriterende slid og endda fungere som en tanke-partner i at få ting gjort. Det vil føles godt at have en AI-assistent, der kan tage sig af handlinger for os, som en god menneskelig personlig assistent. Det vil føles naturligt, endda gavnligt, efterhånden som AI bliver meget smart, kompetent og pålidelig, at overlade flere og flere beslutninger til den. Men denne "gavnlige" delegation har et klart endepunkt, hvis vi fortsætter ned ad vejen: en dag vil vi opdage, at vi ikke rigtig er ansvarlige for ret meget længere, og at de AI-systemer, der faktisk driver showet, ikke kan slukkes mere end olieselskaber, sociale medier, internettet eller kapitalismen.

Og dette er den meget mere positive version, hvor AI simpelthen er så nyttig og effektiv, at vi lader den træffe de fleste af vores nøglebeslutninger for os. Virkeligheden ville sandsynligvis være meget mere af en blanding mellem dette og versioner, hvor ukontrollerede AGI-systemer *tager* forskellige former for magt for sig selv, fordi, husk, magt er nyttig for næsten ethvert mål man har, og AGI ville være, designmæssigt, mindst lige så effektiv til at forfølge sine mål som mennesker.

Hvad enten vi giver kontrol eller den rives fra os, ser dets tab ekstremt sandsynligt ud. Som Alan Turing oprindeligt udtrykte det: "...det synes sandsynligt, at når maskinens tænkemetode var startet, ville det ikke tage lang tid at overgå vores svage kræfter. Der ville ikke være noget spørgsmål om, at maskinerne døde, og de ville kunne tale med hinanden for at skærpe deres vid. På et tidspunkt skulle vi derfor forvente, at maskinerne tager kontrol..."

Bemærk venligst, selvom det er indlysende nok, at tab af kontrol af menneskeheden til AI også indebærer tab af kontrol over USA af den amerikanske regering; det betyder tab af kontrol over Kina af det kinesiske kommunistparti og tab af kontrol over Indien, Frankrig, Brasilien, Rusland og alle andre lande af deres egen regering. Således deltager AI-virksomheder, selvom det ikke er deres hensigt, i øjeblikket i det potentielle styrt af verdens regeringer, inklusive deres egen. Dette kunne ske på få år.

### AGI vil føre til superintelligens

Der kan argumenteres for, at menneske-konkurrencedygtig eller endda ekspert-konkurrencedygtig generel AI, selv hvis autonom, kunne være håndterbar. Det kan være utroligt forstyrrende på alle de måder, der diskuteres ovenfor, men der er mange meget smarte, agentiske mennesker i verden nu, og de er mere eller mindre håndterbare.[^80]

Men vi kommer ikke til at forblive på nogenlunde menneskelig niveau. Progressionen derudover vil sandsynligvis blive drevet af de samme kræfter, vi allerede har set: konkurrencepres mellem AI-udviklere, der søger profit og magt, konkurrencepres mellem AI-brugere, der ikke har råd til at falde bagud, og – vigtigst – AGI's egen evne til at forbedre sig selv.

I en proces, vi allerede har set starte med mindre kraftige systemer, ville AGI selv være i stand til at udtænke og designe forbedrede versioner af sig selv. Dette inkluderer hardware, software, neurale netværk, værktøjer, stilladsering osv. Det vil per definition være bedre end os til at gøre dette, så vi ved ikke præcis, hvordan det vil intelligens-bootstrap. Men det behøver vi ikke. For så vidt som vi stadig har indflydelse på, hvad AGI gør, ville vi blot behøve at bede det om det eller lade det.

Der er ingen menneskelig-niveau barriere for kognition, der kunne beskytte os mod denne løbske.[^81]

Progressionen af AGI til superintelligens er ikke en naturlov; det ville stadig være muligt at begrænse den løbske, især hvis AGI er relativt centraliseret, og i det omfang det kontrolleres af parter, der ikke føler pres for at konkurrere med hinanden. Men skulle AGI være bredt udbredt og højt autonomt, synes det næsten umuligt at forhindre det i at beslutte, det bør være mere, og så endnu mere, kraftig.

### Hvad sker der, hvis vi bygger (eller AGI bygger) superintelligens

For at sige det direkte, vi har ingen anelse om, hvad der ville ske, hvis vi bygger superintelligens.[^82] Det ville tage handlinger, vi ikke kan spore eller opfatte, af grunde vi ikke kan fatte, mod mål vi ikke kan forestille os. Det, vi ved, er, at det ikke vil være op til os.[^83]

Umuligheden af at kontrollere superintelligens kan forstås gennem stadig mere barske analogier. Først, forestil dig, at du er CEO for et stort selskab. Der er ingen måde, du kan holde styr på alt, hvad der foregår, men med den rette opsætning af personale kan du stadig meningsfuldt forstå det store billede og træffe beslutninger. Men antag bare én ting: alle andre i virksomheden opererer med hundrede gange din hastighed. Kan du stadig følge med?

Med superintelligent AI ville mennesker "kommandere" noget, der ikke bare er hurtigere, men opererer på niveauer af sofistikering og kompleksitet, de ikke kan forstå, og behandler enormt meget mere data, end de overhovedet kan forestille sig. Denne inkommensurabilitet kan sættes på et formelt niveau: [Ashbys lov om nødvendig variation](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (og se det relaterede ["good regulator theorem"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)) fastslår groft, at ethvert kontrolsystem skal have lige så mange knapper og drejeknapper, som det system, der kontrolleres, har frihedsgrader.

En person, der kontrollerer et superintelligent AI-system, ville være som en bregne, der kontrollerer General Motors: selv hvis "gør hvad bregnen ønsker" var skrevet ind i virksomhedens vedtægter, er systemerne så forskellige i hastighed og handlingsområde, at "kontrol" simpelthen ikke gælder. (Og hvor længe indtil den irriterende vedtægt bliver omskrevet?)[^84]

Da der er nul eksempler på planter, der kontrollerer Fortune 500-virksomheder, ville der være nøjagtigt nul eksempler på mennesker, der kontrollerer superintelligenser. Dette nærmer sig et matematisk faktum.[^85] Hvis superintelligens blev konstrueret – uanset hvordan vi kom dertil – ville spørgsmålet ikke være, om mennesker kunne kontrollere det, men om vi ville fortsætte med at eksistere, og hvis sådan, om vi ville have en god og meningsfuld eksistens som individer eller som art. Over disse eksistentielle spørgsmål for menneskeheden ville vi have ringe indflydelse. Den menneskelige æra ville være forbi.

### Konklusion: vi må ikke bygge AGI

Der er et scenarie, hvor bygning af AGI kan gå godt for menneskeheden: det bygges omhyggeligt, under kontrol og til gavn for menneskeheden, styret ved gensidig aftale mellem mange interessenter,[^86] og forhindret i at udvikle sig til ukontrollerbar superintelligens.

*Dette scenarie er ikke åbent for os under nuværende omstændigheder.* Som diskuteret i dette afsnit ville udvikling af AGI med meget høj sandsynlighed føre til en kombination af:

- Massiv samfundsmæssig og civilisatorisk forstyrrelse eller ødelæggelse;
- Konflikt eller krig mellem stormagter;
- Tab af kontrol af menneskeheden *over* eller *til* kraftige AI-systemer;
- Løbsk til ukontrollerbar superintelligens og irrelevans eller ophør af den menneskelige art.

Som en tidlig fiktiv skildring af AGI udtrykte det: den eneste måde at vinde på er ikke at spille.

[^55]: [EU AI-loven](https://artificialintelligenceact.eu/) er et betydningsfuldt stykke lovgivning, men ville ikke direkte forhindre et farligt AI-system i at blive udviklet eller implementeret, eller endda åbent frigivet, især i USA. Et andet betydningsfuldt stykke politik, den amerikanske præsidentielle order om AI, er blevet tilbagekaldt.

[^56]: Denne [Gallup-undersøgelse](https://news.gallup.com/poll/1597/confidence-institutions.aspx) viser et dystert fald i tillid til offentlige institutioner siden 2000 i USA. Europæiske tal er varierede og mindre ekstreme, men også på en nedadgående tendens. Mistillid betyder ikke strengt taget, at institutioner virkelig *er* dysfunktionelle, men det er en indikation såvel som en årsag.

[^57]: Og store forstyrrelser, vi nu støtter – såsom udvidelse af rettigheder til nye grupper – blev specifikt drevet af mennesker i en retning mod at gøre tingene bedre.

[^58]: Lad mig være direkte. Hvis dit job kan udføres bag en computer med relativt lidt personlig interaktion med mennesker uden for din organisation og ikke indebærer juridisk ansvar over for eksterne parter, ville det per definition være muligt (og sandsynligvis omkostningsbesparende) fuldstændigt at erstatte dig med et digitalt system. Robotik til at erstatte meget fysisk arbejde vil komme senere – men ikke så meget senere, når AGI begynder at designe robotter.

[^59]: For eksempel, hvad sker der med vores retssystem, hvis sagsanlæg er næsten gratis at anlægge? Hvad sker der, når omgåelse af sikkerhedssystemer gennem social engineering bliver billig, let og risikofri?

[^60]: [Denne artikel](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/) påstår, at 10% af alt internetindhold allerede er AI-genereret, og er Googles topresultat (for mig) til søgeforespørgslen "estimates of what fraction of new internet content is AI-generated." Er det sandt? Jeg har ingen anelse! Den citerer ingen referencer, og den blev ikke skrevet af en person. Hvilken brøkdel af nye billeder indekseret af Google, eller Tweets, eller kommentarer på Reddit, eller YouTube-videoer genereres af mennesker? Ingen ved – jeg tror ikke, det er et kendeligt tal. Og dette mindre end *to år* inde i generativ AI's ankomst.

[^61]: Også værd at tilføje er, at der er "moralsk" risiko for, at vi kunne skabe digitale væsener, der kan lide. Da vi i øjeblikket ikke har en pålidelig teori om bevidsthed, der ville tillade os at skelne fysiske systemer, der kan og ikke kan lide, kan vi ikke udelukke dette teoretisk. Desuden er AI-systemers rapporter om deres følelsesvæsen sandsynligvis upålidelige med hensyn til deres faktiske oplevelse (eller ikke-oplevelse) af følelsesvæsen.

[^62]: Tekniske løsninger inden for dette felt af AI-"alignment" er usandsynligt at være op til opgaven heller. I nuværende systemer virker de på et vist niveau, men er overfladiske og kan generelt omgås uden betydelig indsats; og som diskuteret nedenfor har vi ingen rigtig idé om, hvordan man gør dette for meget mere avancerede systemer.

[^63]: Sådanne AI-systemer kan komme med nogle indbyggede sikkerhedsforanstaltninger. Men for enhver model med noget lignende nuværende arkitektur, hvis fuld adgang til dens vægte er tilgængelig, kan sikkerhedsforanstaltninger fjernes via yderligere træning eller andre teknikker. Så det er praktisk talt garanteret, at for hvert system med gelændere vil der også være et bredt tilgængeligt system uden dem. Faktisk blev Metas Llama 3.1 405B-model åbent frigivet med sikkerhedsforanstaltninger. Men *selv før det* blev en "base"-model, uden sikkerhedsforanstaltninger, lækket.

[^64]: Kunne markedet håndtere disse risici uden regeringsinvolværing? Kort sagt, nej. Der er bestemt risici, som virksomheder er stærkt incitamenteret til at mindske. Men mange andre kan virksomheder og eksternaliserer til alle andre, og mange af ovenstående er i denne klasse: der er ingen naturlige markedsincitamenter til at forhindre masseovervågning, sandhedshenfald, magtkoncentration, arbejdskraftforstyrrelser, skadelig politisk diskurs osv. Faktisk har vi set alt dette fra nutidens tech, især sociale medier, som er gået i det væsentlige ureguleret. AI ville bare enormt forstærke mange af de samme dynamikker.

[^65]: OpenAI har sandsynligvis mere lydige modeller til intern brug. Det er usandsynligt, at OpenAI har bygget en slags "bagdør", så ChatGPT kan kontrolleres bedre af OpenAI selv, fordi dette ville være en frygtelig sikkerhedspraksis og være højt udnytbar givet AI's uigennemsigtighed og uforudsigelighed.

[^66]: Også af afgørende betydning: alignment eller andre sikkerhedsegenskaber betyder kun noget, hvis de faktisk bruges i et AI-system. Systemer, der er åbent frigivet (dvs. hvor modelvægte og arkitektur er offentligt tilgængelige), kan omdannes relativt let til systemer *uden* disse sikkerhedsforanstaltninger. Åben frigilvelse af smartere-end-menneskelige AGI-systemer ville være forbløffende hensynsløst, og det er svært at forestille sig, hvordan menneskelig kontrol eller endda relevans ville blive opretholdt i et sådant scenarie. Der ville være al motivation for eksempel at slippe kraftige selvreproducerende og selvbærende AI-agenter løs med målet at tjene penge og sende dem til en kryptovaluta-pung. Eller at vinde et valg. Eller omstyrte en regering. Kunne "god" AI hjælpe med at begrænse dette? Måske – men kun ved at delegere enorm autoritet til den, hvilket fører til kontroltab som beskrevet nedenfor.

[^67]: For bogl ængde fremstillinger af problemet se f.eks. *Superintelligence*, *The Alignment Problem* og *Human-Compatible*. For en kæmpe bunke arbejde på forskellige tekniske niveauer af dem, der har slidt i årevis med at tænke på problemet, kan du besøge [AI alignment forum](https://www.alignmentforum.org/). Her er et [nyligt take](https://alignment.anthropic.com/2025/recommended-directions/) fra Anthropics alignment-team om, hvad de betragter som uløst.

[^68]: Dette er ["rogue AI"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/)-scenariet. I princippet kunne risikoen være relativt mindre, hvis systemet stadig kan kontrolleres ved at lukke det ned; men scenariet kunne også inkludere AI-bedrageri, selv-eksfiltrering og reproduktion, aggregering af magt og andre trin, der ville gøre det svært eller umuligt at gøre det.

[^69]: Der er en meget rig litteratur om dette emne, der går tilbage til formative skrifter af [Steve Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), Nick Bostrom og Eliezer Yudkowsky. For en bogl ængde fremstilling se [Human Compatible](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) af Stuart Russell; [her](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/) er en kort og opdateret primer.

[^70]: I erkendelse af dette, i stedet for at sænke farten for at få bedre forståelse, er AGI-virksomheder kommet med en anden plan: de vil få AI til at gøre det! Mere specifikt vil de have AI *N* til at hjælpe dem med at finde ud af, hvordan man tilpasser AI *N+1*, hele vejen til superintelligens. Selvom udnyttelse af AI til at hjælpe os med at tilpasse AI lyder lovende, er der et stærkt argument for, at det simpelthen antager sin konklusion som en præmis og generelt er en utroligt risikabel tilgang. Se [her](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us) for noget diskussion. Denne "plan" er ikke én og har undergået intet i retning af den granskning, der er passende til kernestragien for, hvordan man får over-menneskelig AI til at gå godt for menneskeheden.

[^71]: Trods alt har mennesker, mangelfulde og egenrådige som vi er, udviklet etiske systemer, hvorved vi behandler i det mindste nogle andre arter på Jorden godt. (Bare tænk ikke på de fabriksfarms.)

[^72]: Der er heldigvis en udvej her: hvis deltagerne kommer til at forstå, at de er engageret i et selvmordskapløb snarere end et vindbart. Dette er, hvad der skete nær slutningen af den kolde krig, da USA og USSR kom til at indse, at på grund af atomvinter ville selv et *ubesvaret* atomangreb være katastrofalt for angriberen. Med indseenn af, at "atomkrig ikke kan vindes og må aldrig kæmpes", kom betydelige aftaler om våbenreduktion – i det væsentlige en afslutning på våbenkapløbet.

[^73]: Krig, eksplicit eller implicit.

[^74]: Eskalering, derefter krig.

[^75]: Magisk tænkning.

[^76]: Jeg har også en kvadrillion dollar bro at sælge dig.

[^77]: Sådanne agenter ville formentlig foretrække "opnåelse" med ødelæggelse som en fallback; men at sikre modeller mod både ødelæggelse *og* tyveri af kraftige nationer er svært at sige det mindste, især for private enheder.

[^78]: For et andet perspektiv på AGI's nationale sikkerhedsrisici, se [denne RAND-rapport.](https://www.rand.org/pubs/perspectives/PEA3691-4.html)

[^79]: Måske kunne vi bygge sådan en institution! Der har været forslag om et "CERN for AI" og andre lignende initiativer, hvor AGI-udvikling er under multilateral global kontrol. Men i øjeblikket eksisterer ingen sådan institution eller er i sigte.

[^80]: Og mens alignment er meget svær, er det endnu sværere at få mennesker til at opføre sig!

[^81]: Forestil dig et system, der kan tale 50 sprog, have ekspertise i alle akademiske fag, læse en hel bog på sekunder og have alt materialet øjeblikkeligt i tankerne og producere outputs med ti gange menneskelig hastighed. Faktisk behøver du ikke at forestille dig det: bare indlæs et nuværende AI-system. Disse er over-menneskelige på mange måder, og der er intet, der stopper dem fra at være endnu mere over-menneskelige i disse og mange andre.

[^82]: Dette er grunden til, at dette er blevet benævnt en teknologisk "singularitet", der låner fra fysikken idéen om, at man ikke kan lave forudsigelser forbi en singularitet. Fortalere for at læne *ind i* sådan en singularitet kan også ønske at reflektere over, at i fysik river og knuser de samme slags singulariteter dem, der går ind i dem, i stykker.

[^83]: Problemet blev omfattende skitseret i Bostroms [*Superintelligence*](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834), og intet siden da har betydeligt ændret kernebeskeden. For et mere nyligt bind, der indsamler formelle og matematiske resultater om ukontrollerbarhed, se Yampolskiys [AI: Unexplainable, Unpredictable, Uncontrollable](https://www.amazon.com/Unexplainable-Unpredictable-Uncontrollable-Artificial-Intelligence/dp/103257626X)

[^84]: Dette gør det også klart, hvorfor den nuværende strategi for AI-virksomheder (iterativt at lade AI "tilpasse" den næste mest kraftige AI) ikke kan virke. Antag at en bregne via sin frondes behag tilslutter sig en førsteårselev til at tage sig af den. Førsteårseleven skriver nogle detaljerede instruktioner for en andetårselev at følge og en note, der overbeviser dem om at gøre det. Andetårseleven gør det samme for en tredje årselev, og så videre hele vejen til en universitetsuddannet, en leder, en direktør og endelig GM CEO. Vil GM så "gøre hvad bregnen ønsker"? Ved hvert trin kan det føles som om det virker. Men når det hele sættes sammen, vil det virke næsten nøjagtigt i den grad, som CEOen, bestyrelsen og aktionærerne i GM tilfældigvis bekymrer sig om børn og bregner, og have lidt til ingenting at gøre med alle de noter og instruktionssæt.

[^85]: Karakteren er ikke så forskellig fra formelle resultater som Gödels ufuldstændighedsteorem eller Turings halting-argument, idet begrebet kontrol fundamentalt modsiger præmissen: hvordan kan du meningsfuldt kontrollere noget, du ikke kan forstå eller forudsige; men hvis du kunne forstå og forudsige superintelligens, ville du være superintelligent. Grunden til, at jeg siger "nærmer sig", er, at de formelle resultater ikke er så grundige eller kontrollerede som i det rene matematiks tilfælde, og fordi jeg gerne vil holde håbet oppe om, at nogen meget omhyggeligt konstrueret generel intelligens, der bruger totalt forskellige metoder end dem, der i øjeblikket anvendes, kunne have nogle matematisk bevislige sikkerhedsegenskaber i henhold til den slags "garanteret sikker" AI-program, der diskuteres nedenfor.

[^86]: I øjeblikket er de fleste interessenter – det vil sige næsten hele menneskeheden – sat på sidelinjen i denne diskussion. Det er dybt forkert, og hvis ikke inviteret ind, bør de mange, mange andre grupper, der vil blive påvirket af AGI-udvikling, kræve at blive lukket ind.

## Kapitel 8 - Sådan undgår man at bygge AGI

AGI er ikke uundgåelig – i dag står vi ved en skillevej. Dette kapitel præsenterer et forslag til, hvordan vi kunne forhindre, at det bliver bygget.

Hvis den vej, vi i øjeblikket befinder os på, fører til vores civilisations sandsynlige undergang, hvordan skifter vi så vej?

Antag at ønsket om at stoppe udviklingen af AGI og superintelligens var udbredt og stærkt,[^87] fordi det bliver almindelig forståelse, at AGI ville være magtabsorberende snarere end magtgivende, og en dyb fare for samfundet og menneskeheden. Hvordan ville vi lukke Portene?

I øjeblikket kender vi kun én måde at *skabe* kraftfuld og generel AI på, hvilket er via virkelig massive beregninger af dybe neurale netværk. Fordi disse er utrolig vanskelige og dyre ting at gøre, er der en forstand, hvori det *ikke* at gøre dem er let.[^88] Men vi har allerede set de kræfter, der driver mod AGI, og den spilteoretiske dynamik, der gør det meget vanskeligt for nogen part ensidigt at stoppe. Så det ville kræve en kombination af indgriben udefra (dvs. regeringer) for at stoppe virksomheder, og aftaler mellem regeringer for at stoppe sig selv.[^89] Hvordan kunne dette se ud?

Det er nyttigt først at skelne mellem AI-udviklinger, der må *forhindres* eller *forbydes*, og dem, der må *håndteres.* Det første ville primært være løbsk udvikling til superintelligens.[^90] For forbudt udvikling bør definitioner være så skarpe som muligt, og både verifikation og håndhævelse bør være praktiske. Det, der må *håndteres*, ville være generelle, kraftfulde AI-systemer – som vi allerede har, og som vil have mange gråzoner, nuancer og kompleksitet. For disse er stærke effektive institutioner afgørende.

Vi kan også nyttigt afgrænse spørgsmål, der må behandles på internationalt niveau (inklusive mellem geopolitiske rivaler eller modstandere)[^91] fra dem, som individuelle jurisdiktioner, lande eller samlinger af lande kan håndtere. Forbudt udvikling falder i vid udstrækning ind under kategorien "international", fordi et lokalt forbud mod udvikling af en teknologi generelt kan omgås ved at skifte lokation.[^92]

Endelig kan vi overveje værktøjer i værktøjskassen. Der er mange, inklusive tekniske værktøjer, bløde love (standarder, normer osv.), hårde love (reguleringer og krav), ansvar, markedsincitamenter og så videre. Lad os lægge særlig opmærksomhed på ét, der er særligt for AI.

### Compute-sikkerhed og -styring

Et kerneredskab i styringen af højtydende AI vil være den hardware, den kræver. Software spreder sig let, har næsten nul marginal produktionsomkostning, krydser grænser trivielt og kan øjeblikkeligt modificeres; intet af dette gælder for hardware. Men som vi har diskuteret, er enorme mængder af denne "compute" nødvendige både under træning af AI-systemer og under inferens for at opnå de mest kapable systemer. Compute kan let kvantificeres, regnskabsføres og auditeres, med relativt lidt tvetydighed, når først gode regler for at gøre dette er udviklet. Mest afgørende er store mængder beregning, ligesom beriget uran, en meget knaphed, dyr og svær-at-producere ressource. Selvom computerchips er allestedsnærværende, er den hardware, der kræves til AI, dyr og enormt vanskelig at fremstille.[^93]

Det, der gør AI-specialiserede chips langt *mere* håndterbare som en knaphedsressource end uran, er, at de kan inkludere hardware-baserede sikkerhedsmekanismer. De fleste moderne mobiltelefoner, og nogle bærbare computere, har specialiserede on-chip hardware-funktioner, der giver dem mulighed for at sikre, at de kun installerer godkendt operativsystem-software og opdateringer, at de bevarer og beskytter følsomme biometriske data på enheden, og at de kan gøres ubrugelige for alle andre end deres ejer, hvis de mistes eller stjæles. I løbet af de seneste år er sådanne hardware-sikkerhedsforanstaltninger blevet veletablerede og bredt adopteret, og generelt vist sig ret sikre.

Nøglenyskabelsen ved disse funktioner er, at de binder hardware og software sammen ved hjælp af kryptografi.[^94] Det vil sige, bare at have et bestemt stykke computerhardware betyder ikke, at en bruger kan gøre alt, hvad de vil med det ved at anvende forskellig software. Og denne binding giver også kraftig sikkerhed, fordi mange angreb ville kræve et brud på *hardware*-sikkerhed snarere end bare *software*-sikkerhed.

Flere nylige rapporter (f.eks. fra [GovAI og samarbejdspartnere](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips), og [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) har påpeget, at lignende hardware-funktioner indlejret i banebrydende AI-relevant computerhardware kunne spille en ekstremt nyttig rolle i AI-sikkerhed og -styring. De muliggør en række funktioner tilgængelige for en "guvernør"[^95], som man måske ikke ville gætte var tilgængelige eller endda mulige. Som nogle nøgleeksempler:

- *Geolokalisering*: Systemer kan sættes op, så chips har en kendt lokation, og kan handle forskelligt (eller slukkes helt) baseret på lokation.[^96]
- *Tilladte forbindelser*: hvert chip kan konfigureres med en hardware-håndhævet tilladelsesliste over bestemte andre chips, som det kan netværke med, og være ude af stand til at forbinde med chips, der ikke er på denne liste.[^97] Dette kan begrænse størrelsen af kommunikerende klynger af chips.[^98]
- *Målt inferens eller træning (og auto-afbryder)*: En guvernør kan kun licensere en vis mængde træning eller inferens (i tid, eller FLOP, eller muligvis tokens) til at blive udført af en bruger, hvorefter ny tilladelse kræves. Hvis incrementerne er små, kræves så relativt kontinuerlig re-licensering af en model. Modellen kan så "slukkes" simpelthen ved at tilbageholde dette licenssignal.[^99]
- *Hastighedsbegrænsning*: En model forhindres i at køre ved højere inferenshastighed end en vis grænse, der bestemmes af en guvernør eller på anden vis. Dette kunne implementeres via et begrænset sæt af tilladte forbindelser eller ved mere sofistikerede midler.
- *Attesteret træning*: En træningsprocedure kan give kryptografisk sikkert bevis for, at et bestemt sæt koder, data og mængde compute-brug blev anvendt i generering af modellen.

### Sådan undgår man at bygge superintelligens: globale grænser for trænings- og inferens-compute

Med disse overvejelser – især vedrørende beregning – på plads, kan vi diskutere, hvordan man lukker Portene til kunstig superintelligens; vi vil så vende os til at forhindre fuld AGI og håndtere AI-modeller, når de nærmer sig og overgår menneskelig kapacitet i forskellige aspekter.

Den første ingrediens er selvfølgelig forståelsen af, at superintelligens ikke ville være kontrollerbar, og at dens konsekvenser er fundamentalt uforudsigelige. I det mindste Kina og USA må uafhængigt beslutte, af denne eller andre årsager, ikke at bygge superintelligens.[^100] Så er der brug for en international aftale mellem dem og andre, med en stærk verifikations- og håndhævelsesmekanisme, til at forsikre alle parter om, at deres rivaler ikke defekterer og beslutter at kaste terningerne.

For at være verificerbare og håndhævelige bør grænserne være hårde grænser og så entydige som muligt. Dette virker som et praktisk talt umuligt problem: at begrænse kapaciteterne af kompleks software med uforudsigelige egenskaber, på verdensplan. Heldigvis er situationen meget bedre end dette, fordi selve det, der har gjort avanceret AI mulig – en enorm mængde compute – er meget, meget lettere at kontrollere. Selvom det måske stadig ville tillade nogle kraftfulde og farlige systemer, kan *løbsk superintelligens* sandsynligvis forhindres af et hårdt loft over mængden af beregning, der går ind i et neuralt netværk, sammen med en hastighedsgrænse på mængden af inferens, som et AI-system (af forbundne neurale netværk og anden software) kan udføre. En specifik version af dette foreslås nedenfor.

Det kan synes, som om at placere hårde globale grænser på AI-beregning ville kræve enorme niveauer af international koordination og indtrængende, privatlivs-knusende overvågning. Heldigvis ville det ikke. Den ekstremt [stramme og flaskehalspræget forsyningskæde](https://arxiv.org/abs/2402.08797) sørger for, at når først en grænse er sat lovligt (hvad enten det er ved lov eller eksekutivordre), ville verifikation af overholdelse af den grænse kun kræve involvering og samarbejde fra en håndfuld store virksomheder.[^101]

En plan som denne har en række højst ønskværdige funktioner. Den er minimalt invasiv i den forstand, at kun få store virksomheder får krav pålagt dem, og kun ret betydelige klynger af beregning ville blive styret. De relevante chips indeholder allerede de hardware-kapaciteter, der er nødvendige for en første version.[^102] Både implementering og håndhævelse er afhængige af standard juridiske restriktioner. Men disse er bakket op af brugsbetingelser for hardwaren og af hardware-kontroller, hvilket drastisk forenkler håndhævelse og forhindrer snyd fra virksomheder, private grupper eller endda lande. Der er rigeligt præcedens for, at hardware-virksomheder placerer fjernrestriktioner på deres hardware-brug og låser/oplåser bestemte kapaciteter eksternt,[^103] inklusive endda i højtydende CPU'er i datacentre.[^104] Selv for den ret lille andel af hardware og organisationer, der berøres, kunne overvågningen begrænses til telemetri, uden direkte adgang til data eller modeller selv; og softwaren til dette kunne være åben for inspektion for at vise, at ingen yderligere data bliver registreret. Skemaet er internationalt og kooperativt og ret fleksibelt og udvidbart. Fordi grænsen hovedsageligt er på hardware snarere end software, er den relativt agnostisk med hensyn til, hvordan AI-software-udvikling og -deployment forekommer, og er kompatibel med en række paradigmer inklusive mere "decentraliserede" eller "offentlige" AI-tilgange, der sigter mod at bekæmpe AI-drevet magtkoncentration.

En beregningsbaseret Port-lukning har dog også ulemper. For det første er den langt fra en fuldstændig løsning på problemet med AI-styring generelt. For det andet, i takt med at computerhardware bliver hurtigere, ville systemet "fange" mere og mere hardware i mindre og mindre klynger (eller endda individuelle GPU'er).[^105] Det er også muligt, at på grund af algoritmiske forbedringer ville en endnu lavere beregningsgrænse med tiden være nødvendig,[^106] eller at beregnings-mængde bliver stort set irrelevant, og at lukke Porten i stedet ville nødvendiggøre et mere detaljeret risiko-baseret eller kapacitets-baseret styringsregime for AI. For det tredje, uanset garantierne og det lille antal berørte enheder, er et sådant system nødt til at skabe modreaktion vedrørende privatliv og overvågning blandt andre bekymringer.[^107]

Selvfølgelig vil udvikling og implementering af et compute-begrænsende styringsregime på kort tid være ret udfordrende. Men det er absolut gennemførligt.

### A-G-I: Det triple-skæringspunkt som basis for risiko og politik

Lad os nu vende os til AGI. Hårde linjer og definitioner her er vanskeligere, fordi vi bestemt har intelligens, der er kunstig og generel, og efter ingen eksisterende definition vil alle være enige om, hvorvidt eller hvornår det eksisterer. Desuden er en compute- eller inferensgrænse et noget sløvt værktøj (compute er en proxy for kapacitet, som så er en proxy for risiko), der – medmindre den er ret lav – næppe vil forhindre AGI, der er kraftfuld nok til at forårsage social eller civilisatorisk forstyrrelse eller akutte risici.

Jeg har argumenteret for, at de mest akutte risici opstår fra det triple-skæringspunkt af meget høj kapacitet, høj autonomi og stor generalitet. Dette er de systemer, der – hvis de overhovedet udvikles – må håndteres med enorm forsigtighed. Ved at skabe strenge standarder (gennem ansvar og regulering) for systemer, der kombinerer alle tre egenskaber, kan vi kanalisere AI-udvikling mod sikrere alternativer.

Som med andre industrier og produkter, der potentielt kunne skade forbrugere eller offentligheden, kræver AI-systemer omhyggelig regulering af effektive og bemyndigede regeringsagenturer. Denne regulering bør anerkende de iboende risici ved AGI og forhindre, at uacceptabelt risikable højtydende AI-systemer udvikles.[^108]

Dog tager storstilet regulering, især med rigtige tænder, der med sikkerhed vil blive modsat af industrien,[^109] tid[^110] såvel som politisk overbevisning om, at det er nødvendigt.[^111] Givet tempoet i fremskridt kan dette tage mere tid, end vi har til rådighed.

På en meget hurtigere tidsskala og mens reguleringsforanstaltninger udvikles, kan vi give virksomheder de nødvendige incitamenter til (a) at afstå fra meget højrisiko-aktiviteter og (b) at udvikle omfattende systemer til at vurdere og afbøde risiko, ved at afklare og øge ansvarsniveauer for de farligste systemer. Ideen ville være at pålægge de allerhøjeste niveauer af ansvar – streng og i nogle tilfælde personlig kriminel – for systemer i det triple-skæringspunkt af høj autonomi-generalitet-intelligens, men at give "sikre havne" til mere typisk fejl-baseret ansvar for systemer, hvor en af disse egenskaber mangler eller er garanteret at være håndterbar. Det vil sige, for eksempel, et "svagt" system, der er generelt og autonomt (som en kapabel og troværdig men begrænset personlig assistent) ville være underlagt lavere ansvarsniveauer. Ligeledes et snævert og autonomt system som en selvkørende bil ville stadig være underlagt den betydelige regulering, det allerede er, men ikke forhøjet ansvar. På samme måde for et højt kapabelt og generelt system, der er "passivt" og stort set ude af stand til uafhængig handling. Systemer, der mangler *to* af de tre egenskaber, er endnu mere håndterbare, og sikre havne ville være endnu lettere at påberåbe sig. Denne tilgang afspejler, hvordan vi håndterer andre potentielt farlige teknologier:[^112] højere ansvar for farligere konfigurationer skaber naturlige incitamenter for sikrere alternativer.

Det standard-udfald af sådanne høje niveauer af ansvar, som virker for at *internalisere* AGI-risiko til virksomheder snarere end at aflade det til offentligheden, er sandsynligt (og forhåbentlig!) for virksomheder simpelthen ikke at udvikle fuld AGI, indtil og medmindre de genuint kan gøre den troværdig, sikker og kontrollerbar givet, at *deres egen ledelse* er de parter i risiko. (I tilfælde af at dette ikke er tilstrækkeligt, bør lovgivningen, der afklarer ansvar, også eksplicit tillade injunktiv lettelse, dvs. en dommer, der beordrer et stop, for aktiviteter, der klart er i farezonen og diskutabelt udgør en offentlig risiko.) Når regulering kommer på plads, kan efterlevelse af regulering blive den sikre havn, og de sikre havne fra lav autonomi, snæverhed eller svaghed i AI-systemer kan konvertere til relativt lettere reguleringsregimer.

### Nøglebestemmelser for en Port-lukning

Med ovenstående diskussion i tankerne giver dette afsnit forslag til nøglebestemmelser, der ville implementere og vedligeholde forbud mod fuld AGI og superintelligens, og håndtering af menneske-konkurrencedygtig eller ekspert-konkurrencedygtig generel-formål AI nær den fulde AGI-tærskel.[^113] Det har fire nøgleelementer: 1) compute-regnskab og overvågning, 2) compute-lofter i træning og drift af AI, 3) et ansvarsrammeværk, og 4) trindelte sikkerheds- og sikkerhedsstandarder defineret, der inkluderer hårde reguleringsgrænser. Disse beskrives kortfattet næste gang, med yderligere detaljer eller implementeringseksempler givet i tre ledsagende tabeller. Vigtigt, bemærk at disse er langt fra alt, hvad der vil være nødvendigt for at styre avancerede AI-systemer; mens de vil have yderligere sikkerheds- og sikkerhedsfordele, sigter de mod at lukke Porten til intelligens-løbsk og omdirigere AI-udvikling i en bedre retning.

#### 1\. Compute-regnskab og gennemsigtighed

- En standardorganisation (f.eks. NIST i USA efterfulgt af ISO/IEEE internationalt) bør kodificere en detaljeret teknisk standard for den totale compute, der bruges i træning og drift af AI-modeller, i FLOP, og hastigheden i FLOP/s, hvormed de opererer. Detaljer for, hvordan dette kunne se ud, gives i Appendiks A.[^114]
- Et krav – enten ved ny lovgivning eller under eksisterende autoritet[^115] – bør pålægges af jurisdiktioner, hvor storstilet AI-træning finder sted, for at beregne og rapportere til en reguleringsmyndighed eller andet agentur den totale FLOP, der bruges i træning og drift af alle modeller over en tærskel på 10<sup>25</sup> FLOP eller 10<sup>18</sup> FLOP/s.[^116]
- Disse krav bør indfases, hvor der indledningsvis kræves veldokumenterede god-tro estimater på kvartalsbasis, med senere faser, der kræver progressivt højere standarder, op til kryptografisk attesteret total FLOP og FLOP/s knyttet til hvert model-*output*.
- Disse rapporter bør suppleres med veldokumenterede estimater af marginal energi og finansielle omkostninger brugt på at generere hvert AI-output.

Begrundelse: Disse velberegnede og gennemsigtigt rapporterede tal ville give basis for trænings- og driftslofter, såvel som en sikker havn fra højere ansvarsforanstaltninger (se Appendiks C og D).

#### 2\. Trænings- og drifts-compute-lofter

- Jurisdiktioner, der hoster AI-systemer, bør pålægge en hård grænse på den totale compute, der går ind i ethvert AI-model-output, startende ved 10<sup>27</sup> FLOP[^117] og justerbar som passende.
- Jurisdiktioner, der hoster AI-systemer, bør pålægge en hård grænse på compute-raten for AI-model-outputs, startende ved 10<sup>20</sup> FLOP/s og justerbar som passende.

Begrundelse: Total beregning, mens meget ufuldkommen, er en proxy for AI-kapacitet (og risiko), der er konkret målbar og verificerbar, så den giver en hård bagstopper til at begrænse kapaciteter. Et konkret implementeringsforslag gives i Appendiks B.

#### 3\. Forhøjet ansvar for farlige systemer

- Skabelse og drift[^118] af et avanceret AI-system, der er højt generelt, kapabelt og autonomt, bør juridisk afklares via lovgivning til at være underlagt streng, fælles-og-solidarisk, snarere end enkelt-parts fejl-baseret, ansvar.[^119]
- En juridisk proces bør være tilgængelig for at gøre bekræftende sikkerhedssager, som ville give sikker havn fra strengt ansvar for systemer, der er små (i form af compute), svage, snævre, passive eller har tilstrækkelige sikkerheds-, sikkerheds- og kontrollerbarhedsgarantier.
- En eksplicit vej og et sæt betingelser for injunktiv lettelse til at stoppe AI-trænings- og inferens-aktiviteter, der udgør en offentlig fare, bør skitseres.

Begrundelse: AI-systemer kan ikke holdes ansvarlige, så vi må holde menneskelige individer og organisationer ansvarlige for skade, de forårsager (ansvar).[^120] Ukontrollerbar AGI er en trussel mod samfundet og civilisationen og bør i mangel af en sikkerhedssag betragtes som unormalt farlig. At lægge ansvarsbyren på udviklere for at vise, at kraftfulde modeller er sikre nok til ikke at blive betragtet som "unormalt farlige", giver incitament til sikker udvikling, sammen med gennemsigtighed og registrering for at påberåbe sig disse sikre havne. Regulering kan så forhindre skade, hvor afskrækkelse fra ansvar er utilstrækkelig. Endelig er AI-udviklere allerede ansvarlige for skader, de forårsager, så juridisk afklaring af ansvar for de mest risikable systemer kan gøres øjeblikkeligt, uden at højt detaljerede standarder udvikles; disse kan så udvikle sig over tid. Detaljer gives i Appendiks C.

#### 4\. Sikkerhedsregulering for AI

Et reguleringsregime, der adresserer storskalede akutte risici ved AI, vil kræve som minimum:

- Identifikation eller oprettelse af et passende sæt af reguleringsmyndigheder, sandsynligvis et nyt agentur;
- Et omfattende risikovurderingsrammeværk;[^121]
- Et rammeværk for bekræftende sikkerhedssager, delvist baseret på risikovurderingsrammeværket, der skal laves af udviklere, og til auditering af *uafhængige* grupper og agenturer;
- Et trindt licenssystem, med trin, der sporer niveauer af kapacitet.[^122] Licenser ville blive givet på basis af sikkerhedssager og audits, til udvikling og deployment af systemer. Krav ville spænde fra notifikation i den lave ende til kvantitative sikkerheds-, sikkerheds- og kontrollerbarhedsgarantier før udvikling i den høje ende. Disse ville forhindre frigivelse af systemer, indtil de demonstreres sikre, og forbyde udviklingen af iboende usikre systemer. Appendiks D giver et forslag til, hvad sådanne sikkerheds- og sikkerhedsstandarder kunne indebære.
- Aftaler om at bringe sådanne foranstaltninger til det internationale niveau, inklusive internationale organer til at harmonisere normer og standarder, og potentielt internationale agenturer til at gennemgå sikkerhedssager.

Begrundelse: Ultimativt er ansvar ikke den rigtige mekanisme til at forhindre storstilet risiko for offentligheden fra en ny teknologi. Omfattende regulering med bemyndigede reguleringsmyndigheder vil være nødvendig for AI ligesom for alle andre større industrier, der udgør en risiko for offentligheden.[^123]

Regulering mod at forhindre andre gennemgribende men mindre akutte risici vil sandsynligvis variere i sin form fra jurisdiktion til jurisdiktion. Det afgørende er at undgå at udvikle de AI-systemer, der er så risikable, at disse risici er uhåndterbare.

### Hvad så?

I løbet af det næste årti, i takt med at AI bliver mere gennemgribende og kerneteknologien avancerer, vil to nøgleting sandsynligvis ske. For det første vil regulering af eksisterende kraftfulde AI-systemer blive vanskeligere, men endnu mere nødvendig. Det er sandsynligt, at i det mindste nogle foranstaltninger, der adresserer storskalede sikkerhedsrisici, vil kræve aftale på internationalt niveau, med individuelle jurisdiktioner, der håndhæver regler baseret på internationale aftaler.

For det andet vil trænings- og drifts-compute-lofter blive sværere at vedligeholde, i takt med at hardware bliver billigere og mere omkostningseffektiv; de kan også blive mindre relevante (eller skal være endnu strammere) med fremskridt i algoritmer og arkitekturer.

At kontrollere AI vil blive sværere betyder ikke, at vi skal give op! Implementering af planen skitseret i dette essay ville give os både værdifuld tid og afgørende kontrol over processen, der ville sætte os i en langt, langt bedre position til at undgå den eksistentielle risiko ved AI for vores samfund, civilisation og art.

På endnu længere sigt vil der være valg at træffe med hensyn til, hvad vi tillader. Vi kan stadig vælge at skabe en eller anden form for genuint kontrollerbar AGI, i det omfang dette viser sig muligt. Eller vi kan beslutte, at at køre verden er bedre overladt til maskinerne, hvis vi kan overbevise os selv om, at de vil gøre et bedre stykke arbejde med det og behandle os godt. Men disse bør være beslutninger truffet med dyb videnskabelig forståelse af AI i hånden, og efter meningsfuld global inkluderende diskussion, ikke i et kapløb mellem tech-moguler med det meste af menneskeheden fuldstændigt uinvolveret og uvidende.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) Sammenfatning af A-G-I og superintelligens-styring via ansvar og regulering. Ansvar er højest, og regulering stærkest, ved det triple-skæringspunkt af Autonomi, Generalitet og Intelligens. Sikre havne fra strengt ansvar og stærk regulering kan opnås via bekræftende sikkerhedssager, der demonstrerer, at et system er svagt og/eller snævert og/eller passivt. Lofter over total Trænings-Compute og Inferens-Compute-hastighed, verificeret og håndhævet juridisk og ved hjælp af hardware- og kryptografiske sikkerhedsforanstaltninger, understøtter sikkerhed ved at undgå fuld AGI og effektivt forbyde superintelligens.

[^87]: Mest sandsynligt vil spredningen af denne erkendelse tage enten intensiv indsats fra uddannelses- og fortalergrupper, der gør dette argument, eller en ret betydelig AI-forårsaget katastrofe. Vi kan håbe, det vil være det første.

[^88]: Paradoksalt nok er vi vant til, at Naturen begrænser vores teknologi ved at gøre den meget svær at udvikle, især videnskabeligt. Men det er ikke længere tilfældet for AI: de vigtigste videnskabelige problemer viser sig at være lettere end forventet. Vi kan ikke regne med, at Naturen redder os fra os selv her – det må vi gøre selv.

[^89]: Hvor præcist stopper vi med at udvikle nye systemer? Her bør vi adoptere et forsigtighedsprincip. Når først et system er deployeret, og især når det niveau af systemkapacitet prolifererer, er det ekstremt vanskeligt at rulle tilbage. Og hvis et system er *udviklet* (især til store omkostninger og indsats), vil der være enormt pres for at bruge eller deployere det, og fristelse til, at det lækker eller stjæles. At udvikle systemer og *så* beslutte om de er dybt usikre er en farlig vej.

[^90]: Det ville også være klogt at forbyde AI-udvikling, der er iboende farlig, såsom selv-replikerende og evolverende systemer, dem designet til at undslippe indeslutning, dem der kan autonomt selv-forbedre, bevidst vildledende og ondsindede AI, osv.

[^91]: Bemærk dette betyder ikke nødvendigvis *håndhævet* på internationalt niveau af en slags global organisation: i stedet kunne suveræne nationer håndhæve aftalte regler, som i mange traktater.

[^92]: Som vi vil se nedenfor, ville naturen af AI-beregning tillade noget af en hybrid; men internationalt samarbejde vil stadig være nødvendigt.

[^93]: For eksempel de maskiner, der kræves for at ætse AI-relevante chips, laves kun af ét firma, ASML (på trods af mange andre forsøg på at gøre det), langt størstedelen af relevante chips fremstilles af ét firma, TSMC (på trods af andre forsøg på at konkurrere), og design og konstruktion af hardware fra disse chips gøres af kun få inklusive NVIDIA, AMD og Google.

[^94]: Vigtigst holder hvert chip en unik og utilgængelig kryptografisk privat nøgle, det kan bruge til at "underskrive" ting.

[^95]: Som standard ville dette være virksomheden, der sælger chips, men andre modeller er mulige og potentielt nyttige.

[^96]: En guvernør kan fastslå et chips lokation ved at time udveksling af underskrevne beskeder med det: lysets endelige hastighed kræver, at chippet er inden for en given radius *r* af en "station", hvis det kan returnere en underskrevet besked på en tid mindre end *r* / *c*, hvor *c* er lysets hastighed. Ved at bruge flere stationer og noget forståelse af netværkskarakteristika kan chippets lokation bestemmes. Skønheden ved denne metode er, at det meste af dens sikkerhed leveres af fysikkens love. Andre metoder kunne bruge GPS, inertiafølgning og lignende teknologier.

[^97]: Alternativt kunne par af chips kun tillades at kommunikere med hinanden via eksplicit tilladelse fra en guvernør.

[^98]: Dette er afgørende, fordi i det mindste i øjeblikket er meget høj båndbreddeforbindelse mellem chips nødvendig for at træne store AI-modeller på dem.

[^99]: Dette kunne også sættes op til at kræve underskrevne beskeder fra *N* af *M* forskellige guvernører, hvilket tillader flere parter at dele styring.

[^100]: Dette er langt fra uden fortilfælde – for eksempel har militærer ikke udviklet hære af klonede eller genetisk konstruerede supersoldater, selvom dette sandsynligvis er teknologisk muligt. Men de har *valgt* ikke at gøre dette, snarere end at blive forhindret af andre. Track record er ikke god for store verdensmagter, der forhindres i at udvikle en teknologi, de stærkt ønsker at udvikle.

[^101]: Med et par bemærkelsesværdige undtagelser (især NVIDIA) er den AI-specialiserede hardware en relativt lille del af disse virksomheders overordnede forretning og indtægtsmodel. Desuden er gabet mellem hardware brugt i avanceret AI og "forbrugerkvalitets"-hardware betydelig, så de fleste forbrugere af computerhardware ville være stort set upåvirkede.

[^102]: For mere detaljeret analyse, se de nylige rapporter fra [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) og [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). Disse fokuserer på teknisk gennemførlighed, især i konteksten af amerikanske eksportkontroller, der søger at begrænse andre landes kapacitet i high-end beregning; men dette har åbenlyst overlap med den globale begrænsning, der forestilles her.

[^103]: Apple-enheder, for eksempel, fjernlåses sikkert, når de rapporteres tabt eller stjålet, og kan genaktiveres fjernt. Dette er afhængigt af de samme hardware-sikkerhedsfunktioner diskuteret her.

[^104]: Se f.eks. IBM's [capacity on demand](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) tilbud, Intel's [Intel on demand.](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html), og Apple's [private cloud compute](https://security.apple.com/blog/private-cloud-compute/).

[^105]: [Dette studie](https://epochai.org/trends#hardware-trends-section) viser, at historisk er den samme ydeevne opnået ved at bruge omkring 30% færre dollars per år. Hvis denne trend fortsætter, kan der være betydelig overlap mellem AI og "forbruger"-chipbrug, og generelt kunne mængden af nødvendig hardware til højtydende AI-systemer blive ubehageligt lille.

[^106]: Per det [samme studie](https://epochai.org/trends#hardware-trends-section) har given ydeevne på billedgenkendelse krævet 2,5x mindre beregning hvert år. Hvis dette også skulle gælde for de mest kapable AI-systemer også, ville en beregningsgrænse ikke være en nyttig en ret længe.

[^107]: Især på landeniveau ligner dette meget en nationalisering af beregning, idet regeringen ville have meget kontrol over, hvordan beregningskraft bliver brugt. Dog for dem, der bekymrer sig om regeringsinvolvering, synes dette langt sikrere end og at foretrække for den mest kraftfulde AI-software *selv* at blive nationaliseret via en fusion mellem store AI-virksomheder og nationale regeringer, som nogle begynder at advokere for.

[^108]: Et stort reguleringstrin i Europa blev taget med 2024-vedtagelsen af [EU AI Act.](https://artificialintelligenceact.eu/) Den klassificerer AI efter risiko: forbyder uacceptable systemer, regulerer højrisikonerne og pålægger gennemsigtighedsregler eller slet ingen foranstaltninger på lavrisikosystemer. Den vil betydeligt reducere nogle AI-risici og booste AI-gennemsigtighed selv for amerikanske firmaer, men har to nøglefejl. For det første begrænset rækkevidde: selvom den gælder for enhver virksomhed, der leverer AI i EU, er håndhævelse over amerikanske-baserede firmaer svag, og militær-AI er undtaget. For det andet, selvom den dækker GPAI, undlader den at anerkende AGI eller superintelligens som uacceptable risici eller forhindre deres udvikling—kun deres EU-deployment. Som resultat gør den lidt for at dæmme op for risiciene ved AGI eller superintelligens.

[^109]: Virksomheder repræsenterer ofte, at de er for fornuftig regulering. Men på en eller anden måde synes de næsten altid at modsætte sig enhver *bestemt* regulering; se kampen om den ret lav-berørende SB1047, som [de fleste AI-virksomheder offentligt eller privat modsatte sig.](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/)

[^110]: Det var omkring 3½ år fra det tidspunkt, EU AI-loven blev foreslået, til den trådte i kraft.

[^111]: Det udtrykkes sommetider, at det er "for tidligt" at begynde at regulere AI. Givet den sidste note synes det næppe sandsynligt. En anden udtrykt bekymring er, at regulering ville "skade innovation." Men god regulering ændrer bare retningen, ikke mængden, af innovation.

[^112]: Et interessant præcedens er i transporten af farlige materialer, som kunne undslippe og forårsage skade. Her har [regulering](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) og [retspraksis](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) etableret strengt ansvar for meget farlige materialer som sprængstoffer, benzin, gifte, infektiøse agenter og radioaktivt affald. Andre eksempler inkluderer [advarsler på lægemidler](https://www.medicalnewstoday.com/articles/boxed-warnings), [klasser af medicinske enheder,](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification) osv.

[^113]: Et andet omfattende forslag med lignende mål fremsat i ["A Narrow Path"](https://www.narrowpath.co/) advokerer for en mere centraliseret, forbud-baseret tilgang, der kanaliserer al frontier AI-udvikling gennem en enkelt international enhed, overvåget af stærke internationale institutioner, med klare kategoriske forbud snarere end graduerede restriktioner. Jeg ville også støtte den plan; dog vil den tage endnu mere politisk vilje og koordination end den foreslået her.

[^114]: Nogle retningslinjer for en sådan standard blev [udgivet](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/) af Frontier Model Forum. Relativt til forslaget her fejler de på siden af mindre præcision og mindre compute inkluderet i optællingen.

[^115]: Den 2023 amerikanske AI-eksekutivordre (nu ophævet) krævede lignende men mindre finmasket rapportering. Dette bør styrkes af en erstattende ordre.

[^116]: Meget groft, for nu-almindelige H100-chips svarer dette til klynger på omkring 1000, der laver inferens; det er omkring 100 (omkring 5 millioner USD værd) af de allernyeste top-of-the-line NVIDIA B200-chips, der laver inferens. I begge tilfælde svarer træningsnummeret til den klynge, der beregner i flere måneder.

[^117]: Denne mængde er større end ethvert i øjeblikket trænet AI-system; et større eller mindre tal kunne være begrundet, da vi bedre forstår, hvordan AI-kapacitet skalerer med compute.

[^118]: Dette gælder dem, der skaber og leverer/hoster modellerne, ikke slutbrugere.

[^119]: Nogenlunde betyder "streng" ansvar, at udviklere holdes ansvarlige for skader forårsaget af et produkt *som standard* og er en standard brugt for "unormalt farlige" produkter, og (noget morsomt men passende) vilde dyr. "Fælles og solidarisk" ansvar betyder, at ansvar tildeles alle parter ansvarlige for et produkt, og disse parter må sortere ud mellem sig, hvem der bærer hvilket ansvar. Dette er vigtigt for systemer som AI med en lang og kompleks værdikæde.

[^120]: Standard fejl-baseret enkelt-parts ansvar er ikke nok: fejl vil både være vanskelig at spore og tildele, fordi AI-systemer er komplekse, deres operation ikke forstås, og mange parter kan være involveret i skabelsen af et farligt system eller output. Derudover vil retssager tage år at afgøre og sandsynligvis resultere blot i bøder, der er ubetydelige for disse virksomheder, så personligt ansvar for ledere er vigtigt også.

[^121]: Der bør ikke være undtagelse fra sikkerhedskriterier for åben-vægt-modeller. Desuden bør det i vurderingen af risiko antages, at guardrails, der kan fjernes, vil blive fjernet fra bredt tilgængelige modeller, og at selv lukkede modeller vil proliferere, medmindre der er en meget høj sikkerhed for, at de vil forblive sikre.

[^122]: Det foreslåede skema her har reguleringsundersøgelse udløst på generel kapacitet; dog giver det mening for nogle særligt risikable brugssager at udløse mere undersøgelse – for eksempel et ekspert-virologi AI-system, selv hvis snævert og passivt, bør sandsynligvis gå i et højere trin. Den tidligere amerikanske eksekutivordre havde noget af denne struktur for biologiske kapaciteter.

[^123]: To klare eksempler er luftfart og medicin, reguleret af FAA og FDA, og lignende agenturer i andre lande. Disse agenturer er ufuldkomne, men har været absolut vitale for funktionen og succesen af disse industrier.

## Kapitel 9 - At designe fremtiden — hvad vi bør gøre i stedet

AI kan gøre utroligt meget godt i verden. For at få alle fordelene uden risiciene skal vi sikre, at AI forbliver et menneskeligt værktøj.

Hvis vi med succes vælger ikke at erstatte menneskeheden med maskiner – i hvert fald i et stykke tid! – hvad kan vi så gøre i stedet? Giver vi afkald på AI's enorme potentiale som teknologi? På et vist niveau er svaret et simpelt *nej:* luk Portene for ukontrollerbar AGI og superintelligens, men byg *gerne* mange andre former for AI samt de styringssystemer og institutioner, vi får brug for til at håndtere dem.

Men der er stadig meget at sige; at få dette til at virke ville være en central opgave for menneskeheden. Dette afsnit undersøger flere nøgletemaer:

- Hvordan vi kan karakterisere "Værktøjs"-AI og de former, den kan antage.
- At vi kan få (næsten) alt, hvad menneskeheden ønsker, uden AGI, med Værktøjs-AI.
- At Værktøjs-AI-systemer er (sandsynligvis, i princippet) håndterbare.
- At det at vende ryggen til AGI ikke betyder at gå på kompromis med national sikkerhed – tværtimod.
- At magtkoncentration er en reel bekymring. Kan vi begrænse den uden at underminere sikkerhed?
- At vi vil ønske – og få brug for – nye styrings- og sociale strukturer, og AI kan faktisk hjælpe.

### AI inden for Portene: Værktøjs-AI

Trippelkrydsets diagram giver en god måde at afgrænse det, vi kan kalde "Værktøjs-AI": AI der er et kontrollerbart værktøj til menneskelig brug, snarere end en ukontrollerbar rival eller erstatning. De mindst problematiske AI-systemer er dem, der er autonome, men ikke generelle eller særligt kapable (som en auktionsbot), eller generelle, men ikke autonome eller kapable (som en lille sprogmodel), eller kapable, men snævre og meget kontrollerbare (som AlphaGo).[^124] Dem med to overlappende egenskaber har bredere anvendelse, men højere risiko og vil kræve store indsatser at håndtere. (At et AI-system mere er et værktøj betyder ikke, at det er iboende sikkert, blot at det ikke er iboende *usikkert* – tænk på en kædesav versus en tamkat som kæledyr.) Porten skal forblive lukket for (fuld) AGI og superintelligens ved trippelkrydset, og der skal udvises enorm forsigtighed med AI-systemer, der nærmer sig denne tærskel.

Men det efterlader en masse kraftfuld AI! Vi kan få enormt udbytte af smarte og generelle passive "orakler" og snævre systemer, generelle systemer på menneskeligt, men ikke overmenskeligt niveau, og så videre. Mange tech-virksomheder og udviklere bygger aktivt sådanne værktøjer og bør fortsætte; ligesom de fleste mennesker antager de implicit, at Portene til AGI og superintelligens vil blive lukket.[^125]

Desuden kan AI-systemer effektivt kombineres i sammensatte systemer, der bevarer menneskeligt tilsyn mens de øger kapaciteten. I stedet for at stole på uigennemskuelige sorte kasser kan vi bygge systemer, hvor flere komponenter – både AI og traditionel software – arbejder sammen på måder, som mennesker kan overvåge og forstå.[^126] Mens nogle komponenter måske er sorte kasser, ville ingen være tæt på AGI – kun det sammensatte system som helhed ville være både meget generelt og meget kapabelt, og det på en strengt kontrollerbar måde.[^127]

#### Meningsfuld og garanteret menneskelig kontrol

Hvad betyder "strengt kontrollerbar"? En nøgleidé i "Værktøjs"-frameworket er at tillade systemer – selv om de er ret generelle og kraftfulde – der er garanteret at være under meningsfuld menneskelig kontrol. Hvad betyder det? Det indebærer to aspekter. For det første er der en designovervejelse: mennesker bør være dybt og centralt involveret i, hvad systemet gør, *uden* at uddelegere vigtige beslutninger til AI'en. Det er karakteristisk for de fleste nuværende AI-systemer. For det andet skal AI-systemer, i det omfang de er autonome, have garantier, der begrænser deres handlingsområde. En garanti bør være et *tal*, der karakteriserer sandsynligheden for, at noget sker, og en grund til at tro på det tal. Det er, hvad vi kræver på andre sikkerhedskritiske områder, hvor tal som "gennemsnitlig tid mellem fejl" og forventet antal ulykker beregnes, understøttes og offentliggøres i sikkerhedsargumenter.[^128] Det ideelle tal for fejl er selvfølgelig nul. Og den gode nyhed er, at vi måske kan komme ret tæt på det, omend med helt andre AI-arkitekturer, ved at bruge idéer om *formelt verificerede* egenskaber ved programmer (herunder AI). Idéen, udfoldet i længden af Omohundro, Tegmark, Bengio, Dalrymple og andre (se [her](https://arxiv.org/abs/2309.01933) og [her](https://arxiv.org/abs/2405.06624)) er at konstruere et program med bestemte egenskaber (for eksempel: at et menneske kan slukke det) og formelt *bevise*, at disse egenskaber gælder. Det kan gøres nu for ret korte programmer og simple egenskaber, men den (kommende) kraft af AI-drevet bevissoftware kunne tillade det for meget mere komplekse programmer (f.eks. wrappere) og endda AI selv. Dette er et meget ambitiøst program, men i takt med at presset på Portene vokser, får vi brug for nogle kraftfulde materialer til at forstærke dem. Matematisk bevis kan være et af de få, der er stærkt nok.

#### Hvorhen AI-industrien

Med AI-fremskridt omdirigeret ville Værktøjs-AI stadig være en enorm industri. Med hensyn til hardware vil træning og inferens i mindre modeller, selv med compute-begrænsninger for at forhindre superintelligens, stadig kræve enorme mængder specialiserede komponenter. På softwaresiden bør defusering af eksplosionen i AI-model- og beregningsstørrelse blot føre til, at virksomheder omdirigerer ressourcer mod at gøre de mindre systemer bedre, mere forskelligartede og mere specialiserede, snarere end blot at gøre dem større.[^129] Der ville være masser af plads – mere sandsynligt – til alle de pengeskabende Silicon Valley-startups.[^130]

### Værktøjs-AI kan give (næsten) alt, hvad menneskeheden ønsker, uden AGI

Intelligens, hvad enten den er biologisk eller maskinel, kan bredt betragtes som evnen til at planlægge og udføre aktiviteter, der skaber fremtider mere i overensstemmelse med et sæt mål. Som sådan er intelligens af enorm fordel, når den bruges i forfølgelsen af klogt valgte mål. Kunstig intelligens tiltrækker enorme investeringer af tid og indsats, hovedsageligt på grund af dens lovende fordele. Så vi bør spørge: i hvilket omfang ville vi stadig høste fordelene ved AI, hvis vi begrænser dens løbske udvikling til superintelligens? Svaret: vi mister måske overraskende lidt.

Overvej først, at nuværende AI-systemer allerede er meget kraftfulde, og vi har kun ridset i overfladen af, hvad der kan gøres med dem.[^131] De er rimeligt kapable til at "styre showet" med hensyn til at "forstå" et spørgsmål eller en opgave præsenteret for dem, og hvad det ville kræve at besvare dette spørgsmål eller udføre den opgave.

Dernæst skyldes meget af begejstringen for moderne AI-systemer deres generalitet; men nogle af de mest kapable AI-systemer – som dem der genererer eller genkender tale eller billeder, laver videnskabelig forudsigelse og modellering, spiller spil osv. – er meget snævrere og godt "inden for Portene" med hensyn til beregning.[^132] Disse systemer er overmenneskelige til de specifikke opgaver, de udfører. De kan have kanttilfælde[^133] (eller [udnyttelige](https://arxiv.org/abs/2211.00241)) svagheder på grund af deres snæverhed; men *totalt* snævre eller *fuldt* generelle er ikke de eneste tilgængelige muligheder: der er mange arkitekturer imellem.[^134]

Disse AI-værktøjer kan betydeligt fremskynde fremskridt inden for andre positive teknologier uden AGI. For at blive bedre til kernefysik behøver vi ikke AI til at være kernefysiker – dem har vi! Hvis vi vil accelerere medicin, så giv biologer, medicinske forskere og kemikere kraftfulde værktøjer. De ønsker dem og vil bruge dem til enorm gevinst. Vi behøver ikke en serverfarm fuld af en million digitale genier; vi har millioner af mennesker, hvis geni AI kan hjælpe med at bringe frem. Ja, det vil tage længere tid at opnå udødelighed og kuren mod alle sygdomme. Det er en reel omkostning. Men selv de mest lovende sundhedsinnovationer ville være til ringe nytte, hvis AI-dreven ustabilitet fører til global konflikt eller samfundsmæssigt sammenbrud. Vi skylder os selv at give AI-styrkede mennesker en chance for at tackle problemet først.

Og antag, at der faktisk er en eller anden enorm fordel ved AGI, som ikke kan opnås af menneskeheden ved brug af værktøjer inden for Portene. Mister vi dem ved *aldrig* at bygge AGI og superintelligens? Når vi vejer risici og belønninger her, er der en enorm asymmetrisk fordel ved at vente kontra skynde sig: vi kan vente, indtil det kan gøres på en garanteret sikker og gavnlig måde, og næsten alle vil stadig få glæde af belønningerne; hvis vi skynder os, kunne det være – med OpenAI-direktør Sam Altmans ord – [sluk for lyset for *os alle*.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

Men hvis ikke-AGI-værktøjer er potentielt så kraftfulde, kan vi så håndtere dem? Svaret er et klart... måske.

### Værktøjs-AI-systemer er (sandsynligvis, i princippet) håndterbare

Men det bliver ikke let. Nuværende avancerede AI-systemer kan i høj grad styrke mennesker og institutioner i at opnå deres mål. Det er generelt en god ting! Der er dog naturlige dynamikker ved at have sådanne systemer til rådighed – pludselig og uden megen tid for samfundet til at tilpasse sig – som byder på alvorlige risici, der skal håndteres. Det er værd at diskutere nogle få store klasser af sådanne risici og hvordan de kan begrænses, forudsat en lukning af Portene.

En klasse af risici er, at kraftfuld Værktøjs-AI tillader adgang til viden eller kapacitet, som tidligere var bundet til en person eller organisation, hvilket gør en kombination af høj kapacitet plus høj loyalitet tilgængelig for et meget bredt spektrum af aktører. I dag kunne en person med onde hensigter med nok penge hyre et hold kemikere til at designe og producere nye kemiske våben – men det er ikke så let at have de penge eller at finde/samle holdet og overbevise dem om at gøre noget, der er ret klart ulovligt, uetisk og farligt. For at forhindre AI-systemer i at spille en sådan rolle kan forbedringer af nuværende metoder meget vel være tilstrækkelige,[^135] så længe alle disse systemer og adgangen til dem håndteres ansvarligt. På den anden side, hvis kraftfulde systemer frigives til generel brug og modifikation, kan eventuelle indbyggede sikkerhedsforanstaltninger sandsynligvis fjernes. Så for at undgå risici i denne klasse vil der være behov for stærke restriktioner på, hvad der kan frigives offentligt – analog med restriktioner på detaljer om nukleare, eksplosive og andre farlige teknologier.[^136]

En anden klasse af risici stammer fra opskalering af maskiner, der opfører sig som eller efterligner mennesker. På niveau med skade på individuelle personer omfatter disse risici meget mere effektive svindelnumre, spam og phishing samt udbredelse af ikke-samtykkebaserede deepfakes.[^137] På kollektivt niveau omfatter de forstyrrelse af centrale sociale processer som offentlig diskussion og debat, vores samfundsmæssige informations- og videnindsamlings-, behandlings- og formidlingssystemer og vores politiske valgssystemer. Begrænsning af denne risiko vil sandsynligvis involvere (a) love, der begrænser efterligning af mennesker af AI-systemer og gør AI-udviklere ansvarlige for at skabe systemer, der genererer sådanne efterligninger, (b) vandmærke- og herkomst-systemer, der identificerer og klassificerer (ansvarligt) genereret AI-indhold, og (c) nye socio-tekniske epistemiske systemer, der kan skabe en betroet kæde fra data (f.eks. kameraer og optagelser) gennem fakta, forståelse og gode verdensmodeller.[^138] Alt dette er muligt, og AI kan hjælpe med nogle dele af det.

En tredje generel risiko er, at i det omfang nogle opgaver automatiseres, kan de mennesker, der i øjeblikket udfører disse opgaver, have mindre økonomisk værdi som arbejdskraft. Historisk set har automatisering af opgaver gjort ting muliggjort af disse opgaver billigere og mere righoldige, mens det sorterer de mennesker, der tidligere udførte disse opgaver, i dem, der stadig er involveret i den automatiserede version (generelt ved højere færdigheds-/lønniveau), og dem, hvis arbejdskraft er mindre værd eller intet værd. Samlet set er det vanskeligt at forudsige, i hvilke sektorer der vil være behov for mere versus mindre menneskelig arbejdskraft i den resulterende større, men mere effektive sektor. Parallelt hermed har automatiseringsdynamikken tendens til at øge ulighed og generel produktivitet, mindske omkostningerne til visse varer og tjenester (via effektivitetsstigninger) og øge omkostningerne til andre (via [omkostningssygdom](https://en.wikipedia.org/wiki/Baumol_effect)). For dem på den disfavoriserede side af ulighedsstigningen er det dybt uklart, om omkostningsfaldet for visse varer og tjenester opvejer stigningen i andre og fører til generelt større velbefindende. Så hvordan vil det gå med AI? På grund af den relative lethed, hvormed menneskelig intellektuel arbejdskraft kan erstattes af generel AI, kan vi forvente en hurtig version af dette med menneske-konkurrencedygtig generel AI.[^139] Hvis vi lukker Porten til AGI, vil mange færre job blive totalt erstattet af AI-agenter; men enorm fortrængning af arbejdskraft er stadig sandsynlig over en periode på år.[^140] For at undgå udbredt økonomisk lidelse vil det sandsynligvis være nødvendigt at implementere både en form for universelle grundaktiver eller -indkomst og også tilrettelægge et kulturelt skift mod at værdsætte og belønne menneskecentreret arbejde, der er sværere at automatisere (snarere end at se arbejdspriser falde på grund af stigningen i tilgængelig arbejdskraft skubbet ud af andre dele af økonomien). Andre konstruktioner, som ["datadignitet"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society) (hvor de menneskelige producenter af træningsdata automatisk tildeles royalties for den værdi, data skaber i AI), kan hjælpe. Automatisering af AI har også en anden potentielt negativ effekt, som er *uhensigtsmæssig* automatisering. Sammen med anvendelser, hvor AI simpelthen gør et dårligere arbejde, ville dette omfatte dem, hvor AI-systemer sandsynligvis vil krænke moralske, etiske eller juridiske principper – for eksempel i livs- og dødsbeslutninger og i juridiske anliggender. Disse skal behandles ved at anvende og udvide vores nuværende juridiske rammer.

Endelig er en væsentlig trussel fra AI inden for portene dens brug i personaliseret overtalelse, opmærksomhedsfangst og manipulation. Vi har set væksten af en dybt forankret opmærksomhedsøkonomi på sociale medier og andre online platforme (hvor online-tjenester kæmper hårdt om brugerens opmærksomhed) og ["overvågningskapitalisme"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism)-systemer (hvor brugerinformation og profilering tilføjes til kommodificeringen af opmærksomhed). Det er så godt som sikkert, at mere AI vil blive sat i tjeneste for begge dele. AI bruges allerede i høj grad i vanedannende feed-algoritmer, men dette vil udvikle sig til vanedannende AI-genereret indhold, skræddersyet til at blive kompulsivt konsumeret af en enkelt person. Og den persons input, reaktioner og data vil blive fodret ind i opmærksomheds-/reklamemaskinen for at fortsætte den onde cirkel. Desuden, i takt med at AI-hjælpere leveret af tech-virksomheder bliver grænsefladen for mere online-liv, vil de sandsynligvis erstatte søgemaskiner og feeds som mekanismen, hvorigennem overtalelse og monetarisering af kunder sker. Vores samfunds hidtidige fiasko i at kontrollere denne dynamik lover ikke godt. Noget af denne dynamik kan begrænses via regulering vedrørende privatliv, datarettigheder og manipulation. At komme mere til problemets rod kan kræve andre perspektiver, såsom loyale AI-assistenter (diskuteret nedenfor).

Konklusionen på denne diskussion er håb: værktøjsbaserede systemer inden for Portene – i hvert fald så længe de forbliver sammenlignelige i kraft og kapacitet med nutidens mest avancerede systemer – er sandsynligvis håndterbare, hvis der er vilje og koordination til at gøre det. Anstændige menneskelige institutioner, styrket af AI-værktøjer,[^141] kan gøre det. Vi kunne også fejle i at gøre det. Men det er svært at se, hvordan det at tillade mere kraftfulde systemer ville hjælpe – bortset fra at sætte dem til at styre og håbe på det bedste.

### National sikkerhed

Kapløb om AI-overlegenhed – drevet af national sikkerhed eller andre motivationer – driver os mod ukontrollerede kraftfulde AI-systemer, som har tendens til at absorbere snarere end at give magt. Et AGI-kapløb mellem USA og Kina er et kapløb om at afgøre, hvilken nation der får superintelligens først.

Så hvad bør dem, der har ansvaret for national sikkerhed, gøre i stedet? Regeringer har stærk erfaring med at bygge kontrollerbare og sikre systemer, og de bør fordoble indsatsen med at gøre det inden for AI, støtte den slags infrastrukturprojekter, der lykkes bedst, når de gøres i stor målestok og med regeringens godkendelse.

I stedet for et hensynsløst "Manhattan-projekt" mod AGI[^142] kunne den amerikanske regering lancere et Apollo-projekt for kontrollerbare, sikre, troværdige systemer. Dette kunne omfatte for eksempel:

- Et større program til (a) at udvikle on-chip hardware-sikkerhedsmekanismer og (b) infrastrukturen til at håndtere compute-siden af kraftfuld AI. Disse kunne bygge på det amerikanske [CHIPS-lovgivning](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) og [eksportkontrolsystem](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion).
- Et storstillet initiativ til at udvikle formel verificeringsteknikker, så bestemte egenskaber ved AI-systemer (som en afbryder) kan *bevises* at være til stede eller fraværende. Dette kan udnytte AI selv til at udvikle beviser for egenskaber.
- En national indsats for at skabe software, der kan verificeres som sikker, drevet af AI-værktøjer, der kan omkode eksisterende software til verificerbart sikre rammer.
- Et nationalt investeringsprojekt i videnskabelig fremgang ved brug af AI,[^143] der kører som et partnerskab mellem DOE, NSF og NIH.

Generelt er der en enorm angrebsflade på vores samfund, der gør os sårbare over for risici fra AI og dets misbrug. Beskyttelse mod nogle af disse risici vil kræve regeringsstørrelse investering og standardisering. Disse ville give langt mere sikkerhed end at hælde benzin på ilden af kapløb mod AGI. Og hvis AI skal bygges ind i våbensystemer og kommando-og-kontrol-systemer, er det afgørende, at AI'en er troværdig og sikker, hvilket nuværende AI simpelthen ikke er.

### Magtkoncentration og dens begrænsninger

Dette essay har fokuseret på idéen om menneskelig kontrol af AI og dens potentielle fiasko. Men en anden gyldig linse, hvorigennem AI-situationen kan betragtes, er gennem *magtkoncentration*. Udviklingen af meget kraftfuld AI truer med at koncentrere magt enten i de meget få og meget store virksomhedshænder, der har udviklet og vil kontrollere den, eller i regeringer, der bruger AI som et nyt middel til at opretholde deres egen magt og kontrol, eller i AI-systemerne selv. Eller en eller anden ugudelig blanding af ovenstående. I alle disse tilfælde mister størstedelen af menneskeheden magt, kontrol og handlekraft. Hvordan kan vi bekæmpe dette?

Det allerførste og vigtigste skridt er selvfølgelig en lukning af Portene til klogere-end-menneske AGI og superintelligens. Disse kan eksplicit direkte erstatte mennesker og grupper af mennesker. Hvis de er under virksomheds- eller regeringskontrol, vil de koncentrere magt i disse virksomheder eller regeringer; hvis de er "frie", vil de koncentrere magt i sig selv. Så lad os antage, at Portene er lukkede. Hvad så?

En foreslået løsning på magtkoncentration er "open source" AI, hvor modelvægte er frit eller bredt tilgængelige. Men som nævnt tidligere kan de fleste sikkerhedsforanstaltninger eller sikkerhedsmekanismer (og gøres generelt) fjernes, når en model er åben. Så der er en akut spænding mellem på den ene side decentralisering og på den anden side sikkerhed og menneskelig kontrol af AI-systemer. Der er også grunde til at være skeptiske over for, at åbne modeller i sig selv vil bekæmpe magtkoncentration i AI mere meningsfuldt, end de har gjort i operativsystemer (stadig domineret af Microsoft, Apple og Google trods åbne alternativer).[^144]

Der kan dog være måder at kvadrere denne cirkel på – at centralisere og begrænse risici samtidig med at decentralisere kapacitet og økonomisk belønning. Dette kræver at gentænke både hvordan AI udvikles, og hvordan dens fordele fordeles.

Nye modeller for offentlig AI-udvikling og -ejerskab ville hjælpe. Dette kunne antage flere former: regeringsudviklet AI (underlagt demokratisk tilsyn),[^145] nonprofitorganisationer til AI-udvikling (som Mozilla for browsere) eller strukturer, der muliggør meget udbredt ejerskab og styring. Det er centralt, at disse institutioner eksplicit ville have til opgave at tjene den offentlige interesse, mens de opererer under stærke sikkerhedsbegrænsninger.[^146] Veldesignede regulerings- og standarder/certificeringsregimer vil også være vitale, så AI-produkter tilbudt af et levende marked forbliver genuint nyttige snarere end udnyttende over for deres brugere.

Med hensyn til økonomisk magtkoncentration kan vi bruge herkomstsporing og "datadignitet" til at sikre, at økonomiske fordele strømmer bredere. Især stammer det meste AI-magt nu (og i fremtiden, hvis vi holder Portene lukkede) fra menneske-genererede data, hvad enten det er direkte træningsdata eller menneskelig feedback. Hvis AI-virksomheder blev påkrævet at kompensere dataleverandører retfærdigt,[^147] kunne dette i det mindste hjælpe med at fordele de økonomiske belønninger bredere. Udover dette kunne en anden model være offentligt ejerskab af betydelige dele af store AI-virksomheder. For eksempel kunne regeringer, der er i stand til at beskatte AI-virksomheder, investere en del af indtægterne i en statslig formuefond, der besidder aktier i virksomhederne og udbetaler udbytte til befolkningen.[^148]

Afgørende i disse mekanismer er at bruge AI's egen kraft til at hjælpe med at fordele magt bedre, snarere end blot at bekæmpe AI-drevet magtkoncentration ved brug af ikke-AI-midler. En kraftfuld tilgang ville være gennem veldesignede AI-assistenter, der opererer med ægte tillidsforpligtelse over for deres brugere – der sætter brugernes interesser først, især over virksomhedsleverandørernes.[^149] Disse assistenter skal være virkelig troværdige, teknisk kompetente, men passende begrænsede baseret på use case og risikoniveau, og bredt tilgængelige for alle gennem offentlige, nonprofit- eller certificerede for-profit-kanaler. Ligesom vi aldrig ville acceptere en menneskelig assistent, der i hemmelighed arbejder imod vores interesser for en anden part, bør vi ikke acceptere AI-assistenter, der overvåger, manipulerer eller udvinder værdi fra deres brugere til virksomhedsfordel.

En sådan transformation ville fundamentalt ændre den nuværende dynamik, hvor individer står alene og skal forhandle med enorme (AI-drevne) virksomheds- og bureaukratiske maskiner, der prioriterer værdiudvinding frem for menneskelig velfærd. Mens der er mange mulige tilgange til at omfordele AI-dreven magt bredere, vil ingen opstå af sig selv: de skal bevidst konstrueres og styres med mekanismer som tillidsforpligtelser, offentlig levering og lagdelt adgang baseret på risiko.

Tilgange til at begrænse magtkoncentration kan møde betydelige modvind fra etablerede magter.[^150] Men der er veje til AI-udvikling, der ikke kræver at vælge mellem sikkerhed og koncentreret magt. Ved at bygge de rigtige institutioner nu kunne vi sikre, at AI's fordele deles bredt, mens dens risici håndteres omhyggeligt.

### Nye styrings- og sociale strukturer

Vores nuværende styringsstrukturer kæmper: de er langsomme til at reagere, ofte fanget af særlige interesser og [i stigende grad ikke betroede af offentligheden.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) Dette er dog ikke en grund til at opgive dem – tværtimod. Nogle institutioner kan have behov for udskiftning, men mere bredt har vi brug for nye mekanismer, der kan styrke og supplere vores eksisterende strukturer og hjælpe dem med at fungere bedre i vores hurtigt udviklende verden.

Meget af vores institutionelle svaghed stammer ikke fra formelle regeringsstrukturer, men fra forringede sociale institutioner: vores systemer til at udvikle fælles forståelse, koordinere handling og føre meningsfuld diskurs. Indtil videre har AI accelereret denne forringelse, oversvømmet vores informationskanaler med genereret indhold, peget os mod det mest polariserende og splittende indhold og gjort det sværere at skelne sandhed fra fiktion.

Men AI kunne faktisk hjælpe med at genopbygge og styrke disse sociale institutioner. Overvej tre afgørende områder:

For det første kunne AI hjælpe med at genoprette tillid til vores epistemiske systemer – vores måder at vide, hvad der er sandt på. Vi kunne udvikle AI-drevne systemer, der sporer og verificerer informations herkomst, fra rå data gennem analyse til konklusioner. Disse systemer kunne kombinere kryptografisk verifikation med sofistikeret analyse for at hjælpe mennesker med at forstå ikke blot, om noget er sandt, men hvordan vi ved, at det er sandt.[^151] Loyale AI-assistenter kunne få til opgave at følge detaljerne for at sikre, at de stemmer.

For det andet kunne AI muliggøre nye former for storskala koordination. Mange af vores mest presserende problemer – fra klimaforandringer til antibiotikaresistens – er fundamentalt koordinationsproblemer. Vi [sidder fast i situationer, der er værre, end de kunne være for næsten alle](https://equilibriabook.com/), fordi ingen enkelt eller gruppe har råd til at tage det første skridt. AI-systemer kunne hjælpe ved at modellere komplekse incitamentsstrukturer, identificere levedygtige veje til bedre resultater og facilitere de tillidsopbygnings- og forpligtelsesmekanismer, der er nødvendige for at komme derhen.

Måske mest fascinerende kunne AI muliggøre helt nye former for social diskurs. Forestil dig at kunne "tale med en by"[^152] – ikke blot se statistikker, men have en meningsfuld dialog med et AI-system, der behandler og syntetiserer millioner af beboeres synspunkter, oplevelser, behov og aspirationer. Eller overvej, hvordan AI kunne facilitere ægte dialog mellem grupper, der i øjeblikket taler forbi hinanden, ved at hjælpe hver side med bedre at forstå den andens faktiske bekymringer og værdier snarere end deres karikaturer af hinanden.[^153] Eller AI kunne tilbyde dygtig, troværdigt neutral mægling af tvister mellem mennesker eller endda store grupper mennesker (som alle kunne interagere med det direkte og individuelt!) Nuværende AI er totalt i stand til at gøre dette arbejde, men værktøjerne til at gøre det vil ikke opstå af sig selv eller via markedsincitamenter.

Disse muligheder kan lyde utopiske, især i betragtning af AI's nuværende rolle i at forringe diskurs og tillid. Men det er præcis derfor, vi aktivt skal udvikle disse positive anvendelser. Ved at lukke Portene til ukontrollerbar AGI og prioritere AI, der styrker menneskelig handlekraft, kan vi styre teknologisk fremskridt mod en fremtid, hvor AI tjener som en kraft for styrkelse, modstandsdygtighed og kollektive fremskridt.


[^124]: Når det er sagt, er det desværre ikke så let, som man kunne ønske, at holde sig væk fra trippelkrydset. At presse kapaciteten meget hårdt i et af de tre aspekter har tendens til at øge den i de andre. Især kan det være svært at skabe en ekstremt generel og kapabel intelligens, der ikke let kan gøres autonom. En tilgang er at træne modeller som ["nærsynede"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) systemer med begrænsede planlægningsevner. En anden ville være at fokusere på at konstruere rene ["orakel"](https://arxiv.org/abs/1711.05541)-systemer, der ville sky væk fra at svare på handlingsorienterede spørgsmål.

[^125]: Mange virksomheder formår ikke at indse, at de også til sidst ville blive fortrængt af AGI, selv om det tager længere tid – hvis de gjorde, ville de måske skubbe lidt mindre på disse Porte!

[^126]: AI-systemer kunne kommunikere på mere effektive, men mindre forståelige måder, men at opretholde menneskelig forståelse bør have prioritet.

[^127]: Denne idé om modulær, fortolkelig AI er blevet udviklet i detaljer af flere forskere; se f.eks. ["Comprehensive AI Services"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf)-modellen af Drexler, ["Open Agency Architecture"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai) af Dalrymple og andre. Mens sådanne systemer måske kræver mere ingeniørarbejde end monolitiske neurale netværk trænet med massiv beregning, er det præcis der, compute-begrænsninger hjælper – ved at gøre den sikrere, mere transparente vej også den mere praktiske.

[^128]: Om sikkerhedsargumenter generelt se [denne håndbog](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). Vedrørende AI specifikt, se [Wasil et al.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [Clymer et al.](https://arxiv.org/abs/2403.10462), [Buhl et al.](https://arxiv.org/abs/2410.21572), og [Balesni et al.](https://arxiv.org/abs/2411.03336)

[^129]: Vi ser faktisk allerede denne tendens drevet blot af den høje omkostning ved inferens: mindre og mere specialiserede modeller "destilleret" fra større og i stand til at køre på mindre dyr hardware.

[^130]: Jeg forstår, hvorfor dem, der er begejstrede for AI-tech-økosystemet, kan modsætte sig det, de ser som byrdefulde regulering af deres industri. Men det er ærligt talt forvirrende for mig, hvorfor for eksempel en venturekapitalist ville ønske at tillade løbsk udvikling til AGI og superintelligens. Disse systemer (og virksomheder, så længe de forbliver under virksomhedskontrol) vil *spise alle startups som et mellemmåltid*. Sandsynligvis endda *hurtigere* end at spise andre industrier. Alle, der er investeret i et blomstrende AI-økosystem, bør prioritere at sikre, at AGI-udvikling ikke fører til monopolisering af få dominerende aktører.

[^131]: Som økonomen og tidligere Deepmind-forsker Michael Webb [udtrykte det](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/), "Jeg tror, at hvis vi stoppede al udvikling af større sprogmodeller i dag, så GPT-4 og Claude og hvad som helst er de sidste ting, vi træner af den størrelse – så vi tillader meget mere iteration på ting af den størrelse og alle slags fine-tuning, men intet større end det, ingen større fremskridt – bare det, vi har i dag, tror jeg er nok til at drive 20 eller 30 års utrolig økonomisk vækst."

[^132]: For eksempel brugte DeepMinds alphafold-system kun 1/100.000 af GPT-4's FLOP-tal.

[^133]: Vanskeligheden ved selvkørende biler er vigtig at bemærke her: mens det nominelt er en snæver opgave og opnåelig med fair pålidelighed med relativt små AI-systemer, er omfattende virkelig viden og forståelse nødvendig for at få pålidelighed til det niveau, der er nødvendigt i en sådan sikkerhedskritisk opgave.

[^134]: For eksempel, givet et beregningsbudget, ville vi sandsynligvis se GPAI-modeller fortrænet til (lad os sige) halvdelen af det budget, og den anden halvdel brugt til at træne op meget høj kapacitet i et mere snævert opgaveområde. Dette ville give overmenneskelig snæver kapacitet understøttet af næsten-menneskelig generel intelligens.

[^135]: Den nuværende dominerende alignment-teknik er "forstærkende læring ved menneskelig feedback" [(RLHF)](https://arxiv.org/abs/1706.03741) og bruger menneskelig feedback til at skabe et belønnings-/strafsignal for forstærkende læring af AI-modellen. Dette og relaterede teknikker som [konstitutionel AI](https://arxiv.org/abs/2212.08073) fungerer overraskende godt (selvom de mangler robusthed og kan omgås med beskedent besvær). Derudover er nuværende sprogmodeller generelt kompetente nok til sund fornuft-ræsonnement til, at de ikke vil lave tåbelige moralske fejl. Dette er noget af et sweet spot: smarte nok til at forstå, hvad folk ønsker (i det omfang det kan defineres), men ikke smarte nok til at planlægge omfattende bedrageri eller forårsage enorm skade, når de gør fejl.

[^136]: I det lange løb vil ethvert niveau af AI-kapacitet, der udvikles, sandsynligvis sprede sig, da det i sidste ende er software og nyttigt. Vi skal have robuste mekanismer til at forsvare os mod de risici, sådanne systemer udgør. Men vi *har ikke det nu*, så vi skal være meget afmålte i, hvor meget kraftfulde AI-modeller får lov til at sprede sig.

[^137]: Langt størstedelen af disse er ikke-samtykkebaserede pornografiske deepfakes, herunder af mindreårige.

[^138]: Mange ingredienser til sådanne løsninger eksisterer i form af "bot-eller-ej"-love (i EU AI-loven blandt andre steder), [industriens herkomstssporingsteknologier](https://c2pa.org/), [innovative nyhedsaggregatorer](https://www.improvethenews.org/), forudsigelse[saggregatorer](https://metaculus.com/) og markeder osv.

[^139]: Automatiseringsbølgen følger måske ikke tidligere mønstre, idet relativt *høj*-færdighedsopgaver som kvalitetsskrivning, fortolkning af lovgivning eller medicinsk rådgivning kan være lige så meget eller endnu mere sårbare over for automatisering end opgaver med lavere færdigheder.

[^140]: For omhyggelig modellering af AGI's effekt på lønninger, se rapporten [her](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek), og blodige detaljer [her](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0), fra Anton Korinek og samarbejdspartnere. De finder, at i takt med at flere dele af job automatiseres, stiger produktivitet og lønninger – til et punkt. Når *for* meget bliver automatiseret, fortsætter produktiviteten med at stige, men lønningerne styrtdykker, fordi mennesker erstattes totalt af effektiv AI. Dette er grunden til, at lukning af Portene er så nyttig: vi får produktiviteten uden de forsvundne menneskelige lønninger.

[^141]: Der er mange måder, AI kan bruges som og til at hjælpe med at bygge "defensive" teknologier for at gøre beskyttelse og håndtering mere robust. Se [dette](https://vitalik.eth.limo/general/2025/01/05/dacc2.html) indflydelsesrige indlæg, der beskriver denne "D/acc"-dagsorden.

[^142]: Noget ironisk ville et amerikansk Manhattan-projekt sandsynligvis gøre lidt for at fremskynde timelines mod AGI – skiven for menneskelig og finansiel investering i AI-fremskridt er allerede fastgjort på 11. De primære resultater ville være at inspirere et lignende projekt i Kina (som udmærker sig i nationale infrastrukturprojekter), at gøre internationale aftaler, der begrænser AI's risiko, meget sværere, og at alarmere andre geopolitiske modstandere af USA såsom Rusland.

[^143]: ["National AI Research Resource"](https://nairrpilot.org/)-programmet er et godt nuværende skridt i denne retning og bør udvides.

[^144]: Se [denne analyse](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807) af de forskellige betydninger og implikationer af "åben" i tech-produkter, og hvordan nogle har ført til mere snarere end mindre forankring af dominans.

[^145]: Planer i USA for en [National AI Research Resource](https://nairratdoe.ornl.gov/) og den nylige lancering af en [European AI Foundation](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/) er interessante skridt i denne retning.

[^146]: Udfordringen her er ikke teknisk, men institutionel – vi har akut brug for eksempler og eksperimenter fra den virkelige verden på, hvordan AI-udvikling i offentlighedens interesse kunne se ud.

[^147]: Dette går imod nuværende big tech-forretningsmodeller og ville kræve både juridisk handling og nye normer.

[^148]: Kun nogle regeringer vil være i stand til at gøre det. En mere radikal idé er [en universel fond af denne type under fælles ejerskab af alle mennesker.](https://futureoflife.org/project/the-windfall-trust/)

[^149]: For en længere fremstilling af dette argument se [dette papir](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338) om AI-loyalitet. Desværre er standardbanen for AI-assistenter sandsynligvis en, hvor de bliver i stigende grad illoyale.

[^150]: Noget ironisk er mange etablerede magter også i risiko for AI-støttet magtberøvelse; men det kan være vanskeligt for dem at opfatte dette, medmindre processen kommer ret langt.

[^151]: Nogle interessante indsatser i denne retning repræsenteres af [c2pa-koalitionen](https://c2pa.org/) om kryptografisk verifikation; [Verity](https://www.improvethenews.org/) og [Ground news](https://ground.news/) om bedre nyhedsepistemer; og [Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com) og forudsigelsesmarkeder om at forankre diskurs i falsificerbare forudsigelser.

[^152]: Se [dette](https://talktothecity.org/) fascinerende pilotprojekt.

[^153]: Se [Kialo](https://www.kialo-edu.com/), og indsatser fra [Collective Intelligence Project](https://www.cip.org/) for nogle eksempler.

## Kapitel 10 - Valget vi står overfor

For at bevare vores menneskelige fremtid må vi vælge at lukke Portene til AGI og superintelligens.

Sidste gang menneskeheden delte Jorden med andre sind, der talte, tænkte, byggede teknologi og løste generelle problemer, var for 40.000 år siden i istidseuropa. De andre sind uddøde, helt eller delvist på grund af vores indsats.

Vi går nu ind i en lignende tid. De mest avancerede produkter af vores kultur og teknologi – datasæt bygget fra vores samlede internetinformationsfælles, og chips med 100 milliarder elementer, der er de mest komplekse teknologier, vi nogensinde har skabt – bliver kombineret for at bringe avancerede generelle AI-systemer til live.

Udviklerne af disse systemer er ivrige efter at fremstille dem som værktøjer til menneskelig empowerment. Og det kunne de faktisk være. Men misforstå ikke: vores nuværende kurs er at bygge stadig mere kraftfulde, målorienterede, beslutningstagende og generelt kompetente digitale agenter. De præsterer allerede lige så godt som mange mennesker på en bred vifte af intellektuelle opgaver, forbedres hurtigt og bidrager til deres egen forbedring.

Medmindre denne kurs ændres eller rammer en uventet vejspærring, vil vi snart – om år, ikke årtier – have digitale intelligenser, der er farligt kraftfulde. Selv i de *bedste* scenarier ville disse bringe store økonomiske fordele (i det mindste for nogle af os), men kun til prisen for en dybtgående forstyrrelse af vores samfund og erstatning af mennesker i de fleste af de vigtigste ting, vi gør: disse maskiner ville tænke for os, planlægge for os, beslutte for os og skabe for os. Vi ville blive forkælede, men som forkælede børn. Meget mere sandsynligt ville disse systemer erstatte mennesker i både de positive *og* negative ting, vi gør, herunder udnyttelse, manipulation, vold og krig. Kan vi overleve AI-hyperopladede versioner af disse? Endelig er det mere end plausibelt, at tingene slet ikke ville gå godt: at vi forholdsvis hurtigt ville blive erstattet ikke kun i det, vi gør, men i det vi *er*, som arkitekter af civilisationen og fremtiden. Spørg neandertalerne, hvordan det går. Måske gav vi dem også ekstra nips i et stykke tid.

*Vi behøver ikke at gøre dette.* Vi har menneskekonkurrencedygtig AI, og der er intet behov for at bygge AI, som vi *ikke* kan konkurrere med. Vi kan bygge fantastiske AI-værktøjer uden at bygge en efterfølgerart. Forestillingen om, at AGI og superintelligens er uundgåelig, er et *valg, der maskerer sig som skæbne*.

Ved at pålægge nogle hårde, globale grænser kan vi holde AI's generelle kapacitet på cirka menneskeligt niveau, mens vi stadig høster fordelene ved computeres evne til at behandle data på måder, vi ikke kan, og automatisere opgaver, som ingen af os ønsker at udføre. Disse ville stadig udgøre mange risici, men hvis de designes og styres godt, være en enorm velsignelse for menneskeheden, fra medicin til forskning til forbrugerprodukter.

At pålægge begrænsninger ville kræve internationalt samarbejde, men mindre end man kunne tro, og disse begrænsninger ville stadig efterlade masser af plads til en enorm AI- og AI-hardwareindustri fokuseret på anvendelser, der forbedrer menneskers velbefindende, snarere end på den rå jagt på magt. Og hvis vi, med stærke sikkerhedsgarantier og efter en meningsfuld global dialog, beslutter at gå videre, forbliver den mulighed vores at forfølge.

Menneskeheden må *vælge* at lukke Portene til AGI og superintelligens.

For at holde fremtiden menneskelig.

### En note fra forfatteren

Tak for at tage dig tid til at udforske dette emne med os.

Jeg skrev dette essay, fordi jeg som videnskabsmand føler, det er vigtigt at fortælle den usminkede sandhed, og fordi jeg som person føler, det er afgørende for os at handle hurtigt og beslutsomt for at tackle et verdensforandrende spørgsmål: udviklingen af AI-systemer, der er klogere end mennesker.

Hvis vi skal reagere på denne bemærkelsesværdige situation med visdom, må vi være parate til kritisk at undersøge det fremherskende narrativ om, at AGI og superintelligens 'må' bygges for at sikre vores interesser, eller er 'uundgåelig' og ikke kan stoppes. Disse narrativer efterlader os magtesløse, ude af stand til at se de alternative veje foran os.

Jeg håber, du vil slutte dig til mig i at opfordre til forsigtighed over for hensynsløshed og mod over for grådighed.

Jeg håber, du vil slutte dig til mig i at opfordre til en menneskelig fremtid.

*– Anthony*

![Anthony Aguirre signatur](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAnthony-Aguirre-signature-300x84.png&w=3840&q=75)

## Bilag

Supplerende information, herunder - Tekniske detaljer omkring compute-regnskab, et eksempel på implementering af en 'portlukning', detaljer for et strengt AGI-ansvarsregime, og en trinvis tilgang til AGI-sikkerheds- og sikkerhedsstandarder.

### Bilag A: Tekniske detaljer for compute-regnskab

En detaljeret metode til både "absolut sandhed" samt gode tilnærmelser for den samlede compute brugt i træning og inferens er nødvendig for meningsfuld compute-baseret kontrol. Her er et eksempel på, hvordan den "absolutte sandhed" kunne opgøres på teknisk niveau.

**Definitioner:**

*Compute kausal graf:* For et givet output O fra en AI-model, er der et sæt digitale beregninger, hvor ændring af resultatet af den beregning potentielt kunne ændre O. (Dette bør antages konservativt, dvs. der bør være en klar grund til at tro, at en beregning er uafhængig af en forløber, der både forekommer tidligere i tid og har en fysisk potentiel kausal virkningssti.) Dette inkluderer beregning udført af AI-modellen under inferens, samt beregninger der indgik i input, dataforberedelse og træning af modellen. Da enhver af disse selv kan være output fra en AI-model, beregnes dette rekursivt, afbrudt hvor et menneske har givet en væsentlig ændring til inputtet.

*Trænings-compute:* Den samlede compute, i FLOP eller andre enheder, der indgås af compute-kausal-grafen for et neuralt netværk (inkluderende dataforberedelse, træning og finjustering, og andre beregninger.)

*Output-compute:* Den samlede compute i compute-kausal-grafen for et givet AI-output, inkluderende alle neurale netværk (og inkluderende deres Trænings-compute) og andre beregninger, der indgår i det output.

*Inferens-compute-rate:* I en serie af outputs, ændringshastigheden (i FLOP/s eller andre enheder) af Output-compute mellem outputs, dvs. den compute der bruges til at producere det næste output, divideret med tidsintervallet mellem outputs.

**Eksempler og tilnærmelser:**

- For et enkelt neuralt netværk trænet på menneske-skabte data er Trænings-compute blot den samlede trænings-compute som sædvanligt rapporteret.
- For et sådant neuralt netværk, der udfører inferens med en konstant hastighed, er Inferens-compute-raten cirka den samlede hastighed af beregningsklyngen, der udfører inferensen i FLOP/s.
- For model-finjustering gives Trænings-compute for den komplette model ved Trænings-compute for den ikke-finjusterede model plus beregningen udført under finjustering og til forberedelse af data brugt i finjustering.
- For en destilleret model inkluderer Trænings-compute for den komplette model træning af både den destillerede model og den større model brugt til at levere syntetiske data eller andet træningsinput.
- Hvis flere modeller trænes, men mange "forsøg" kasseres på baggrund af menneskelig vurdering, tæller disse ikke med i Trænings- eller Output-compute for den beholdte model.

### Bilag B: Eksempel på implementering af en portlukning

**Implementeringseksempel:** Her er et eksempel på, hvordan en portlukning kunne fungere, givet en grænse på 10<sup>27</sup> FLOP for træning og 10<sup>20</sup> FLOP/s for inferens (kørsel af AI'en):

**1. Pause:** Af nationale sikkerhedshensyn beder den amerikanske regering alle virksomheder baseret i USA, der driver forretning i USA, eller som bruger chips fremstillet i USA, om at ophøre med nye AI-træningskørsler, der kunne overskride 10<sup>27</sup> FLOP Trænings-compute-grænsen. USA bør indlede diskussioner med andre lande, der huser AI-udvikling, og kraftigt opfordre dem til at tage lignende skridt og indikere, at den amerikanske pause kan ophæves, hvis de vælger ikke at efterkomme.

**2. Amerikansk tilsyn og licensering:** Ved præsidentiel bekendtgørelse eller handling fra et eksisterende reguleringsagentur kræver USA, at inden (f.eks.) et år:

- Alle AI-træningskørsler estimeret over 10<sup>25</sup> FLOP udført af virksomheder, der opererer i USA, registreres i en database vedligeholdt af et amerikansk reguleringsagentur. (Bemærk: En lidt svagere version af dette var allerede inkluderet i den nu tilbagekaldte amerikanske præsidentielle bekendtgørelse om AI fra 2023, der krævede registrering for modeller over 10<sup>26</sup> FLOP.)
- Alle AI-relevante hardwareproducenter, der opererer i USA eller handler med den amerikanske regering, overholder et sæt krav til deres specialiserede hardware og den software, der driver den. (Mange af disse krav kunne indbygges i software- og firmware-opdateringer til eksisterende hardware, men langsigtede og robuste løsninger ville kræve ændringer til senere generationer af hardware.) Blandt disse er et krav om, at hvis hardwaren er del af en højhastigheds-sammenkoblet klynge, der kan udføre 10<sup>18</sup> FLOP/s beregning, kræves et højere niveau af verifikation, som inkluderer regelmæssig tilladelse fra en fjern "styrer", der modtager både telemetri og anmodninger om at udføre yderligere beregning.
- Forvalteren rapporterer den samlede beregning udført på sin hardware til agenturet, der vedligeholder den amerikanske database.
- Stærkere krav indføres gradvist for at tillade både mere sikker og mere fleksibel overvågning og tilladelse.

**3. Internationalt tilsyn:**

- USA, Kina og andre lande, der huser avanceret chip-fremstillingskapacitet, forhandler en international aftale.
- Denne aftale skaber et nyt internationalt agentur, analogt med Det Internationale Atomenergiagentur, der har til opgave at overvåge AI-træning og -udførelse.
- Underskriverlande skal kræve, at deres indenlandske AI-hardwareproducenter overholder et sæt krav, der er mindst lige så stærke som dem pålagt i USA.
- Forvaltere skal nu rapportere AI-beregningstal til både agenturer i deres hjemlande samt et nyt kontor inden for det internationale agentur.
- Yderligere lande opfordres kraftigt til at tilslutte sig den eksisterende internationale aftale: eksportkontrol fra underskriverlande begrænser adgang til high-end hardware for ikke-underskrivere, mens underskrivere kan modtage teknisk støtte til at administrere deres AI-systemer.

**4. International verifikation og håndhævelse:**

- Hardware-verifikationssystemet opdateres, så det rapporterer beregningsbrug til både den oprindelige forvalter og også direkte til det internationale agenturs kontor.
- Agenturet aftaler via diskussion med underskriverne af den internationale aftale compute-begrænsninger, som derefter får juridisk kraft i underskriverlandene.
- Parallelt kan et sæt internationale standarder udvikles, så træning og kørsel af AI'er over en beregnings-tærskel (men under grænsen) skal overholde disse standarder.
- Agenturet kan, hvis nødvendigt for at kompensere for bedre algoritmer osv., sænke compute-grænsen. Eller, hvis det anses for sikkert og tilrådeligt (på niveau med beviseligt sikkerhedsgarantier), hæve compute-grænsen.

### Bilag C: Detaljer for et strengt AGI-ansvarsregime

**Detaljer for et strengt AGI-ansvarsregime**

- Skabelse og drift af et avanceret AI-system, der er meget generelt, kapabelt og autonomt, betragtes som en "unormalt farlig" aktivitet.
- Som sådan er standardansvar for træning og drift af sådanne systemer strengt, solidarisk ansvar (eller dets ikke-amerikanske ækvivalent) for enhver skade forårsaget af modellen eller dens outputs/handlinger.
- Personligt ansvar pålægges direktører og bestyrelsesmedlemmer i tilfælde af grov forsømmelighed eller bevidst uredelighed. Dette bør inkludere kriminelle straffe for de mest alvorlige tilfælde.
- Der er talrige sikre havne, under hvilke ansvar vender tilbage til standardansvar (fejlbaseret i USA), som personer og virksomheder normalt ville være underlagt.
	- Modeller trænet og drevet under en vis compute-tærskel (som ville være mindst 10 gange lavere end grænserne beskrevet ovenfor.)
	- AI der er "svag" (groft, under menneskelig ekspertniveau på de opgaver, den er beregnet til) og/eller
	- AI der er "snæver" (har et fast og ret begrænset omfang af opgaver og operationer, den er specifikt designet og trænet til) og/eller
	- AI der er "passiv" (meget begrænset i sin evne – selv under beskeden modifikation – til at udføre handlinger eller komplekse flertrinssopgaver uden direkte menneskelig involvering og kontrol.)
	- En AI der garanteres at være sikker, sikret og kontrollerbar (beviseligt sikker, eller en risikoanalyse indikerer et ubetydelig niveau af forventet skade.)
- Sikre havne kan påberåbes på baggrund af et [sikkerheds-case](https://arxiv.org/abs/2410.21572) udarbejdet af AI-udvikleren og godkendt af et agentur eller revisor akkrediteret af et agentur. For at påberåbe sig en sikker havn baseret på compute skal udvikleren blot levere troværdige estimater af samlet Trænings-compute og maksimal Inferens-rate
- Lovgivning ville eksplicit skitsere situationer, under hvilke påbudsmæssig lindring fra udvikling af AI-systemer med høj risiko for offentlig skade ville være passende.
- Virksomhedskonsorrier, der arbejder med NGO'er og regeringsagenturer, bør udvikle standarder og normer, der definerer disse termer, hvordan regulatorer skal tildele sikre havne, hvordan AI-udviklere skal udvikle sikkerheds-cases, og hvordan domstole skal fortolke ansvar, hvor sikre havne ikke proaktivt påberåbes.

### Bilag D: En trinvis tilgang til AGI-sikkerheds- og sikkerhedsstandarder

**En trinvis tilgang til AGI-sikkerheds- og sikkerhedsstandarder**

| Risiko-niveau | Udløser | Krav til træning | Krav til udrulning |
| --- | --- | --- | --- |
| RT-0 | AI svag i autonomi, generalitet og intelligens | ingen | ingen |
| RT-1 | AI stærk i en af autonomi, generalitet og intelligens | ingen | Baseret på risiko og brug, potentielt sikkerheds-cases godkendt af nationale myndigheder, hvor modellen kan bruges |
| RT-2 | AI stærk i to af autonomi, generalitet og intelligens | Registrering hos national myndighed med jurisdiktion over udvikleren | Sikkerheds-case der afgrænser risiko for større skade under autoriserede niveauer plus uafhængige sikkerhedsaudits (inkluderende black-box og white-box redteaming) godkendt af nationale myndigheder, hvor modellen kan bruges |
| RT-3 | AGI stærk i autonomi, generalitet og intelligens | Forhåndsgodkendelse af sikkerheds- og sikkerhedsplan af national myndighed med jurisdiktion over udvikleren | Sikkerheds-case der garanterer afgrænset risiko for større skade under autoriserede niveauer samt påkrævede specifikationer, inkluderende cybersikkerhed, kontrollerbarhed, en ikke-fjernbar dræberknap, tilpasning til menneskelige værdier og robusthed over for ondsindet brug. |
| RT-4 | Enhver model der også overstiger enten 10<sup>27</sup> FLOP Træning eller 10<sup>20</sup> FLOP/s Inferens | Forbudt indtil international aftalt ophævelse af compute-grænse | Forbudt indtil international aftalt ophævelse af compute-grænse |

Risikokategorisering og sikkerheds-/sikkerhedsstandarder, med niveauer baseret på compute-tærskler samt kombinationer af høj autonomi, generalitet og intelligens:

- *Stærk autonomi* gælder, hvis systemet er i stand til at udføre, eller nemt kan gøres til at udføre, flertrinssopgaver og/eller tage komplekse handlinger, der er virkelige verdens-relevante, uden betydelig menneskelig overvågning eller intervention. Eksempler: autonome køretøjer og robotter; finansielle handelsbots. Ikke-eksempler: GPT-4; billede-klassifikatorer
- *Stærk generalitet* indikerer et bredt anvendelsesområde, udførelse af opgaver, som modellen ikke var bevidst og specifikt trænet til, og betydelig evne til at lære nye opgaver. Eksempler: GPT-4; mu-zero. Ikke-eksempler: AlphaFold; autonome køretøjer; billedgeneratorer
- *Stærk intelligens* svarer til at matche menneskelig ekspert-niveau præstation på de opgaver, som modellen præsterer bedst på (og for en generel model, på tværs af et bredt spektrum af opgaver.) Eksempler: AlphaFold; mu-zero; o3. Ikke-eksempler: GPT-4; Siri

### Tak

Et par tak til personer, der har bidraget til Hold Fremtiden Menneskelig.

Dette arbejde afspejler forfatterens holdninger og bør ikke betragtes som den officielle holdning fra Future of Life Institute (selvom de er kompatible; for den officielle holdning, se [denne side](https://futureoflife.org/our-position-on-ai/)), eller nogen anden organisation, som forfatteren er tilknyttet.

Jeg er taknemmelig over for menneskene Mark Brakel, Ben Eisenpress, Anna Hehir, Carlos Gutierrez, Emilia Javorsky, Richard Mallah, Jordan Scharnhorst, Elyse Fulcher, Max Tegmark og Jaan Tallinn for kommentarer til manuskriptet; til Tim Schrier for hjælp med nogle referencer; til Taylor Jones og Elyse Fulcher for forskønnelse af diagrammer.

Dette arbejde har gjort begrænset brug af generative AI-modeller (Claude og ChatGPT) under skabelsen, til redigering og red-teaming. Ifølge den veletablerede standard for niveauer af AI-involvering i kreative værker ville dette arbejde sandsynligvis få 3/10. (Der findes faktisk ingen sådan standard! Men det burde der.)

Vi er meget taknemmelige over for [Julius Odai](https://www.linkedin.com/in/julius-odai/) for at have produceret denne webversion af essayet, som gør læsning og navigation rundt i essayet til en meget behagelig oplevelse. Julius er teknolog og nylig deltager i BlueDot Impact AI Governance-kurset.