# Houd de Toekomst Menselijk

Dit essay bouwt een argument op voor waarom en hoe we de poorten naar AGI en superintelligentie moeten sluiten, en wat we er in plaats daarvan zouden moeten bouwen.

Als je alleen de kernpunten wilt lezen, ga naar de Samenvatting. Hoofdstukken 2-5 bieden vervolgens wat achtergrond over de typen AI-systemen die in het essay besproken worden. Hoofdstukken 5-7 leggen uit waarom we kunnen verwachten dat AGI spoedig zal arriveren, en wat er zou kunnen gebeuren wanneer dat het geval is. Tot slot schetsen Hoofdstukken 8-9 een concreet voorstel om te voorkomen dat AGI wordt gebouwd.

[Download PDF](https://keepthefuturehuman.ai/wp-content/uploads/2025/03/Keep_the_Future_Human__AnthonyAguirre__5March2025.pdf)

Totale leestijd: 2-3 uur

## Managementsamenvatting

Een overzicht op hoofdlijnen van het essay. Als je weinig tijd hebt, krijg je hier alle kernpunten in slechts 10 minuten.

Dramatische vooruitgang in kunstmatige intelligentie gedurende het afgelopen decennium (voor specifieke AI-toepassingen) en de laatste jaren (voor algemene AI) hebben AI getransformeerd van een niche academisch vakgebied tot de kernstrategie van veel van 's werelds grootste bedrijven, met honderden miljarden dollars aan jaarlijkse investeringen in technieken en technologieën voor het verbeteren van AI-capaciteiten.

We staan nu op een kritiek punt. Terwijl de capaciteiten van nieuwe AI-systemen beginnen te evenaren en te overtreffen die van mensen op veel cognitieve gebieden, moet de mensheid beslissen: hoe ver gaan we, en in welke richting?

AI begon, zoals elke technologie, met het doel dingen te verbeteren voor zijn maker. Maar ons huidige traject, en impliciete keuze, is een ongecontroleerde race naar steeds krachtigere systemen, gedreven door economische prikkels van een paar grote technologiebedrijven die grote delen van huidige economische activiteit en menselijke arbeid willen automatiseren. Als deze race nog veel langer doorgaat, is er een onvermijdelijke winnaar: AI zelf – een sneller, slimmer, goedkoper alternatief voor mensen in onze economie, ons denken, onze beslissingen, en uiteindelijk in controle van onze beschaving.

Maar we kunnen een andere keuze maken: via onze regeringen kunnen we de controle over het AI-ontwikkelingsproces nemen om duidelijke limieten op te leggen, lijnen die we niet zullen overschrijden, en dingen die we simpelweg niet zullen doen – zoals we hebben gedaan voor nucleaire technologieën, massavernietigingswapens, ruimtewapens, milieuvernietigende processen, de bio-engineering van mensen, en eugenetica. Het belangrijkste is dat we kunnen zorgen dat AI een tool blijft om mensen te versterken, in plaats van een nieuwe soort die ons vervangt en uiteindelijk verdringt.

Dit essay stelt dat we *de toekomst menselijk moeten houden* door de "poorten" te sluiten naar slimmere-dan-menselijke, autonome, algemene AI – soms "AGI" genoemd – en vooral naar de zeer bovenmenselijke versie die soms "superintelligentie" wordt genoemd. In plaats daarvan moeten we ons richten op krachtige, betrouwbare AI-tools die individuen kunnen versterken en de capaciteiten van menselijke samenlevingen transformatief kunnen verbeteren om te doen waar ze het beste in zijn. De structuur van dit argument volgt hier kort.

### AI is anders

AI-systemen zijn fundamenteel anders dan andere technologieën. Terwijl traditionele software precieze instructies volgt, leren AI-systemen hoe doelen te bereiken zonder dat hen expliciet wordt verteld hoe. Dit maakt ze krachtig: als we het doel of een succesmaatstaf duidelijk kunnen definiëren, kan een AI-systeem in de meeste gevallen leren het te bereiken. Maar het maakt ze ook inherent onvoorspelbaar: we kunnen niet betrouwbaar bepalen welke acties ze zullen ondernemen om hun doelstellingen te bereiken.

Ze zijn ook grotendeels onverklaarbaar: hoewel ze deels code zijn, zijn ze vooral een enorme verzameling ondoorgrondelijke getallen – "gewichten" van neurale netwerken – die niet kunnen worden ontleed; we zijn niet veel beter in het begrijpen van hun innerlijke werking dan in het doorgronden van gedachten door in een biologische hersenen te kijken.

Deze kernmethode van het trainen van digitale neurale netwerken neemt snel toe in complexiteit. De krachtigste AI-systemen worden gecreëerd door middel van massale computationele experimenten, waarbij gespecialiseerde hardware wordt gebruikt om neurale netwerken te trainen op enorme datasets, die vervolgens worden aangevuld met software-tools en bovenbouw.

Dit heeft geleid tot de creatie van zeer krachtige tools voor het maken en verwerken van tekst en beelden, het uitvoeren van wiskundige en wetenschappelijke redeneringen, het aggregeren van informatie, en het interactief bevragen van een enorme opslag van menselijke kennis.

Helaas, hoewel de ontwikkeling van krachtigere, betrouwbaardere technologische tools is wat we *zouden moeten* doen, en wat bijna iedereen wil en zegt te willen, is het niet het traject waarop we ons werkelijk bevinden.

### AGI en superintelligentie

Sinds het begin van het vakgebied heeft AI-onderzoek zich in plaats daarvan gericht op een ander doel: Artificiële Algemene Intelligentie. Deze focus is nu de focus geworden van de titanische bedrijven die AI-ontwikkeling leiden.

Wat is AGI? Het wordt vaak vaag gedefinieerd als "AI op menselijk niveau," maar dit is problematisch: welke mensen, en op welke capaciteiten is het op menselijk niveau? En hoe zit het met de bovenmenselijke capaciteiten die het al heeft? Een bruikbaardere manier om AGI te begrijpen is door de kruising van drie kerneigenschappen: hoge **A**utonomie (onafhankelijkheid van handelen), hoge **A**lgemene toepasbaarheid (brede reikwijdte en aanpassingsvermogen), en hoge **I**ntelligentie (competentie bij cognitieve taken). Huidige AI-systemen kunnen zeer capabel zijn maar beperkt, of algemeen maar constant menselijk toezicht vereisen, of autonoom maar beperkt in reikwijdte.

Volledige A-G-I zou alle drie eigenschappen combineren op niveaus die de beste menselijke capaciteiten evenaren of overtreffen. Cruciaal is dat het deze combinatie is die mensen zo effectief en zo anders maakt dan huidige software; het is ook wat het mogelijk zou maken dat mensen in zijn geheel worden vervangen door digitale systemen.

Hoewel menselijke intelligentie speciaal is, is het geenszins een limiet. Artificiële "superintelligente" systemen zouden honderden keren sneller kunnen opereren, veel meer data kunnen verwerken en enorme hoeveelheden tegelijkertijd "in gedachten" kunnen houden, en aggregaten kunnen vormen die veel groter en effectiever zijn dan verzamelingen van mensen. Ze zouden niet individuen kunnen verdringen maar bedrijven, naties, of onze beschaving als geheel.

### We staan op de drempel

Er is een sterke wetenschappelijke consensus dat AGI *mogelijk* is. AI overtreft al menselijke prestaties in veel algemene tests van intellectuele capaciteit, inclusief recent redeneren en probleemoplossen op hoog niveau. Achterblijvende capaciteiten – zoals continu leren, planning, zelfbewustzijn, en originaliteit – bestaan allemaal op enig niveau in huidige AI-systemen, en bekende technieken bestaan die ze waarschijnlijk allemaal zullen verbeteren.

Terwijl tot een paar jaar geleden veel onderzoekers AGI decennia ver weg zagen, is momenteel het bewijs voor korte tijdlijnen naar AGI sterk:

- Empirisch geverifieerde "schalingswetten" verbinden computationele input aan AI-capaciteit, en bedrijven zijn op koers om computationele input met ordes van grootte te schalen over de komende jaren. De menselijke en financiële middelen gewijd aan AI-vooruitgang evenaren nu die van een dozijn Manhattan Projects en verschillende Apollo Projects.
- AI-bedrijven en hun leiders geloven publiekelijk en privé dat AGI (volgens enige definitie) binnen enkele jaren bereikbaar is. Deze bedrijven hebben informatie die het publiek niet heeft, inclusief sommige die de volgende generatie AI-systemen in handen hebben.
- Expert voorspellers met bewezen track-records kennen 25% waarschijnlijkheid toe aan AGI (volgens enige definitie) die binnen 1-2 jaar arriveert, en 50% voor 2-5 jaar (zie Metaculus voorspellingen voor ['zwakke'](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) en ['volledige'](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/) AGI).
- Autonomie (inclusief lange-termijn flexibele planning) loopt achter in AI-systemen, maar grote bedrijven richten nu hun enorme middelen op het ontwikkelen van autonome AI-systemen en hebben informeel 2025 het ["jaar van de agent"](https://techinformed.com/2025-informed-the-year-of-agentic-ai/) genoemd.
- AI draagt meer en meer bij aan zijn eigen verbetering. Zodra AI-systemen even competent zijn als menselijke AI-onderzoekers in het doen van AI-onderzoek, zal een kritieke drempel voor snelle vooruitgang naar veel krachtigere AI-systemen worden bereikt en waarschijnlijk leiden tot een doorbraak in AI-capaciteit. (Naar verluidt is die doorbraak al begonnen.)

Het idee dat slimmere-dan-menselijke AGI decennia of meer ver weg is, is simpelweg niet langer houdbaar voor de overgrote meerderheid van experts in het veld. Meningsverschillen gaan nu over hoeveel maanden of jaren het zal duren als we op deze koers blijven. De kernvraag waarmee we geconfronteerd worden is: zouden we dat moeten?

### Wat drijft de race naar AGI

De race naar AGI wordt gedreven door meerdere krachten, die elk de situatie gevaarlijker maken. Grote technologiebedrijven zien AGI als de ultieme automatiseringstechnologie – niet alleen het ondersteunen van menselijke werknemers maar ze grotendeels of geheel vervangen. Voor bedrijven is de prijs enorm: de kans om een aanzienlijk deel van de jaarlijkse wereldwijde economische output van $100 biljoen te bemachtigen door menselijke arbeidskosten weg te automatiseren.

Naties voelen zich gedwongen om aan deze race deel te nemen, waarbij ze publiekelijk economisch en wetenschappelijk leiderschap aanhalen, maar privé AGI zien als een potentiële revolutie in militaire zaken vergelijkbaar met kernwapens. Angst dat rivalen een beslissend strategisch voordeel zouden kunnen krijgen creëert een klassieke wapenwedloop-dynamiek.

Degenen die superintelligentie nastreven halen vaak grootse visioenen aan: alle ziektes genezen, veroudering omkeren, doorbraken bereiken in energie en ruimtereizen, of bovenmenselijke planningscapaciteiten creëren.

Minder liefdevol gezegd wordt de race gedreven door macht. Elke deelnemer – of het nu een bedrijf of land is – gelooft dat intelligentie gelijk staat aan macht, en dat zij de beste beheerder van die macht zullen zijn.

Ik beweer dat deze motivaties echt maar fundamenteel misplaatst zijn: AGI zal macht *absorberen* en *zoeken* in plaats van verlenen; door AI gecreëerde technologieën zullen *ook* sterk tweesnijdend zijn, en waar gunstig kunnen ze worden gecreëerd met AI-tools en zonder AGI; en zelfs voor zover AGI en zijn output onder controle blijven, maken deze race-dynamieken – zowel bedrijfsmatig als geopolitiek – grootschalige risico's voor onze samenleving bijna onvermijdelijk tenzij ze beslissend worden onderbroken.

### AGI en superintelligentie vormen een dramatische bedreiging voor de beschaving

Ondanks hun aantrekkingskracht vormen AGI en superintelligentie dramatische bedreigingen voor de beschaving via meerdere versterkende paden:

*Machtsconcentratie:* bovenmenselijke AI zou de overgrote meerderheid van de mensheid kunnen ontmachtigen door enorme delen van sociale en economische activiteit op te slokken in AI-systemen gerund door een handvol gigantische bedrijven (die op hun beurt kunnen worden overgenomen door, of effectief overnemen van, regeringen.)

*Massale ontwrichting:* bulk-automatisering van de meeste cognitief-gebaseerde banen, vervanging van onze huidige epistemische systemen, en uitrol van enorme aantallen actieve niet-menselijke agenten zouden de meeste van onze huidige beschavingssystemen in een relatief korte periode omverwerpen.

*Catastrofes:* door het prolifereren van het vermogen – potentieel boven menselijk niveau – om nieuwe militaire en destructieve technologieën te creëren en het loskoppelen ervan van de sociale en juridische systemen die verantwoordelijkheid grondvesten, worden fysieke catastrofes door massavernietigingswapens dramatisch waarschijnlijker.

*Geopolitiek en oorlog:* grote wereldmachten zullen niet stilzitten als ze voelen dat een technologie die een "beslissend strategisch voordeel" zou kunnen leveren wordt ontwikkeld door hun tegenstanders.

*Doorbraak en verlies van controle:* Tenzij het specifiek wordt voorkomen, zal bovenmenselijke AI alle prikkels hebben om zichzelf verder te verbeteren en zou mensen ver kunnen overtreffen in snelheid, dataverwerking, en verfijning van denken. Er is geen betekenisvolle manier waarop we de controle kunnen hebben over zo'n systeem. Zulke AI zal geen macht verlenen aan mensen; wij zullen macht aan haar verlenen, of zij zal het nemen.

Veel van deze risico's blijven bestaan zelfs als het technische "alignment" probleem – ervoor zorgen dat geavanceerde AI betrouwbaar doet wat mensen willen – wordt opgelost. AI presenteert een enorme uitdaging in hoe het zal worden beheerd, en heel veel aspecten van dit beheer worden ongelooflijk moeilijk of onoplosbaar naarmate menselijke intelligentie wordt doorbroken.

Het meest fundamenteel zou het type bovenmenselijke algemene AI dat momenteel wordt nagestreefd, door zijn aard, doelen, agency, en capaciteiten hebben die de onze overtreffen. Het zou inherent oncontroleerbaar zijn – hoe kunnen we iets controleren dat we noch kunnen begrijpen noch voorspellen? Het zou geen technologische tool voor menselijk gebruik zijn, maar een tweede soort intelligentie op Aarde naast de onze. Als het verder mag gaan, zou het niet alleen een tweede soort vormen maar een vervangingssoort.

Misschien zou het ons goed behandelen, misschien niet. Maar de toekomst zou aan haar toebehoren, niet aan ons. Het menselijke tijdperk zou voorbij zijn.

### Dit is niet onvermijdelijk; de mensheid kan heel concreet besluiten om onze vervanger niet te bouwen.

Het creëren van bovenmenselijke AGI is verre van onvermijdelijk. We kunnen het voorkomen door een gecoördineerde set van governance-maatregelen:

Ten eerste hebben we robuuste boekhouding en toezicht nodig op AI-berekeningen ("rekenkracht"), wat een fundamentele enabler is van, en hefboom om te besturen, grootschalige AI-systemen. Dit vereist op zijn beurt gestandaardiseerde meting en rapportage van de totale rekenkracht gebruikt in het trainen van AI-modellen en het draaien ervan, en technische methoden voor het tellen, certificeren, en verifiëren van gebruikte berekeningen.

Ten tweede moeten we harde limieten implementeren op AI-berekeningen, zowel voor training als voor operatie; deze voorkomen dat AI zowel te krachtig wordt als te snel opereert. Deze limieten kunnen worden geïmplementeerd door zowel wettelijke vereisten als hardware-gebaseerde beveiligingsmaatregelen ingebouwd in AI-gespecialiseerde chips, analoog aan beveiligingsfuncties in moderne telefoons. Omdat gespecialiseerde AI-hardware wordt gemaakt door slechts een handvol bedrijven, zijn verificatie en handhaving haalbaar via de bestaande toeleveringsketen.

Ten derde hebben we verbeterde aansprakelijkheid nodig voor de gevaarlijkste AI-systemen. Degenen die AI ontwikkelen dat hoge autonomie, brede algemeenheid, en superieure intelligentie combineert zouden risicoaansprakelijkheid moeten krijgen voor schade, terwijl veilige havens van deze aansprakelijkheid ontwikkeling van meer beperkte en controleerbare systemen zouden aanmoedigen.

Ten vierde hebben we gelaagde regulering nodig gebaseerd op risiconiveaus. De meest capabele en gevaarlijke systemen zouden uitgebreide veiligheids- en controleerbaarheidsgaranties vereisen voor ontwikkeling en uitrol, terwijl minder krachtige of meer gespecialiseerde systemen proportioneel toezicht zouden krijgen. Dit regulatoire kader zou uiteindelijk moeten opereren op zowel nationale als internationale niveaus.

Deze benadering – met gedetailleerde specificatie gegeven in het volledige document – is praktisch: hoewel internationale coördinatie nodig zal zijn, kunnen verificatie en handhaving werken door het kleine aantal bedrijven dat de gespecialiseerde hardware toeleveringsketen controleert. Het is ook flexibel: bedrijven kunnen nog steeds innoveren en profiteren van AI-ontwikkeling, alleen met duidelijke limieten op de gevaarlijkste systemen.

Lange-termijn inperking van AI-macht en -risico zou internationale akkoorden vereisen gebaseerd op zowel eigen- als gemeenschappelijk belang, net zoals het controleren van kernwapen-proliferatie nu doet. Maar we kunnen onmiddellijk beginnen met verbeterd toezicht en aansprakelijkheid, terwijl we bouwen naar meer uitgebreide governance.

Het ontbrekende ingrediënt is politieke en sociale wil om de controle over het AI-ontwikkelingsproces te nemen. De bron van die wil, als het op tijd komt, zal de realiteit zelf zijn – dat wil zeggen, uit wijdverspreide realisatie van de echte implicaties van wat we doen.

### We kunnen Tool-AI engineeren om de mensheid te versterken

In plaats van oncontroleerbare AGI na te streven, kunnen we krachtige "Tool-AI" ontwikkelen die menselijke capaciteit verbetert terwijl het onder betekenisvol menselijk controle blijft. Tool-AI systemen kunnen extreem capabel zijn terwijl ze de gevaarlijke drievoudige kruising van hoge autonomie, brede algemeenheid, en bovenmenselijke intelligentie vermijden, zolang we ze engineeren om controleerbaar te zijn op een niveau dat overeenkomt met hun capaciteit. Ze kunnen ook worden gecombineerd in geavanceerde systemen die menselijk toezicht behouden terwijl ze transformatieve voordelen leveren.

Tool-AI kan de geneeskunde revolutioneren, wetenschappelijke ontdekking versnellen, onderwijs verbeteren, en democratische processen versterken. Wanneer goed bestuurd, kan het menselijke experts en instellingen effectiever maken in plaats van ze te vervangen. Hoewel zulke systemen nog steeds zeer ontwrichtend zullen zijn en zorgvuldig beheer vereisen, zijn de risico's die ze stellen fundamenteel anders dan AGI: het zijn risico's die we kunnen besturen, zoals die van andere krachtige technologieën, geen existentiële bedreigingen voor menselijke agency en beschaving. En cruciaal, wanneer wijs ontwikkeld, kunnen AI-tools mensen helpen krachtige AI te besturen en de effecten ervan te beheren.

Deze benadering vereist het herdenken van zowel hoe AI wordt ontwikkeld als hoe de voordelen ervan worden verdeeld. Nieuwe modellen van publieke en non-profit AI-ontwikkeling, robuuste regulatoire kaders, en mechanismen om economische voordelen breder te verdelen kunnen helpen zorgen dat AI de mensheid als geheel versterkt in plaats van macht te concentreren in enkele handen. AI zelf kan helpen betere sociale en governance-instellingen bouwen, nieuwe vormen van coördinatie en discourse mogelijk makend die de menselijke samenleving versterken in plaats van ondermijnen. Nationale veiligheidsinstellingen kunnen hun expertise benutten om AI-tool systemen echt veilig en betrouwbaar te maken, en een echte bron van defensie evenals nationale macht.

We zouden er uiteindelijk voor kunnen kiezen om nog krachtigere en meer soevereine systemen te ontwikkelen die minder op tools lijken en – zo kunnen we hopen – meer op wijze en krachtige weldoeners. Maar we zouden dat alleen moeten doen nadat we de wetenschappelijke begrip en governance-capaciteit hebben ontwikkeld om dat veilig te doen. Zo'n monumentale en onomkeerbare beslissing zou bewust door de mensheid als geheel moeten worden genomen, niet standaard in een race tussen techbedrijven en naties.

### In menselijke handen

Mensen willen het goede dat komt van AI: nuttige tools die hen versterken, economische kansen en groei superchargen, en doorbraken beloven in wetenschap, technologie, en onderwijs. Waarom zouden ze niet? Maar wanneer gevraagd, wil een overweldigende meerderheid van het algemene publiek [langzamere en zorgvuldiger AI-ontwikkeling](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation), en willen ze geen slimmere-dan-menselijke AI die hen zal vervangen in hun banen en elders, hun cultuur en informatiecommons vullen met niet-menselijke content, macht concentreren in een kleine set bedrijven, extreme grootschalige wereldwijde risico's stellen, en uiteindelijk dreigen hun soort te ontmachtigen of vervangen. Waarom zouden ze?

We *kunnen* het ene hebben zonder het andere. Het begint met te besluiten dat ons lot niet ligt in de vermeende onvermijdelijkheid van enige technologie of in de handen van een paar CEO's in Silicon Valley, maar in de rest van onze handen als we er greep op nemen. Laten we de Poorten sluiten, en de toekomst menselijk houden.

## Hoofdstuk 1 - Inleiding

Hoe we zullen reageren op het vooruitzicht van AI die slimmer is dan mensen, is de meest urgente kwestie van onze tijd. Dit essay biedt een weg voorwaarts.

We bevinden ons mogelijk aan het einde van het menselijke tijdperk.

In de afgelopen tien jaar is er iets begonnen dat uniek is in de geschiedenis van onze soort. De gevolgen daarvan zullen grotendeels de toekomst van de mensheid bepalen. Vanaf ongeveer 2015 zijn onderzoekers erin geslaagd *smalle* kunstmatige intelligentie (AI) te ontwikkelen – systemen die beter dan welke mens dan ook kunnen winnen bij spellen zoals Go, beelden en spraak kunnen herkennen, enzovoort.[^1]

Dit is een verbazingwekkend succes, en het levert uiterst nuttige systemen en producten op die de mensheid zullen versterken. Maar smalle kunstmatige intelligentie is nooit het werkelijke doel van het vakgebied geweest. Het doel is juist geweest om AI-systemen voor *algemene* doeleinden te creëren, met name die welke vaak "artificiële algemene intelligentie" (AGI) of "superintelligentie" worden genoemd, die tegelijkertijd net zo goed of beter zijn dan mensen bij vrijwel *alle* taken, net zoals AI nu superhuman is bij Go, schaken, poker, droneracen, etc. Dit is het verklaarde doel van veel grote AI-bedrijven.[^2]

*Deze inspanningen slagen ook.* AI-systemen voor algemene doeleinden zoals ChatGPT, Gemini, Llama, Grok, Claude en Deepseek, gebaseerd op enorme berekeningen en bergen aan data, hebben gelijkwaardigheid bereikt met gewone mensen bij een grote verscheidenheid aan taken, en evenaren zelfs menselijke experts op sommige gebieden. Nu racen AI-ingenieurs bij enkele van de grootste technologiebedrijven om deze gigantische experimenten in machine-intelligentie naar de volgende niveaus te brengen, waarbij ze het volledige spectrum aan menselijke capaciteiten, expertise en autonomie eerst evenaren en vervolgens overtreffen.

*Dit staat voor de deur.* In de afgelopen tien jaar zijn expertschattingen voor hoe lang dit zal duren – als we onze huidige koers voortzetten – gedaald van decennia (of eeuwen) naar enkele jaren.

Het is ook van epochaal belang en van transcendent risico. Voorstanders van AGI zien het als een positieve transformatie die wetenschappelijke problemen zal oplossen, ziekten zal genezen, nieuwe technologieën zal ontwikkelen en sleur zal automatiseren. En AI zou zeker kunnen helpen bij het bereiken van al deze dingen – sterker nog, dat doet het al. Maar door de decennia heen hebben veel zorgvuldige denkers, van Alan Turing tot Stephen Hawking tot de hedendaagse Geoffrey Hinton en Yoshua Bengio[^3], een scherpe waarschuwing gegeven: het bouwen van werkelijk slimmer-dan-menselijke, algemene, autonome AI zal minimaal de samenleving volledig en onherroepelijk op zijn kop zetten, en maximaal resulteren in het uitsterven van de mens.[^4]

Superintelligente AI nadert snel op onze huidige weg, maar is verre van onvermijdelijk. Dit essay is een uitgebreid argument voor waarom en hoe we de *Poorten* naar deze naderende onmenselijke toekomst moeten *sluiten*, en wat we in plaats daarvan zouden moeten doen.

[^1]: Deze [grafiek](https://time.com/6300942/ai-progress-charts/) toont een reeks taken; veel vergelijkbare curves zouden aan deze grafiek kunnen worden toegevoegd. Deze snelle vooruitgang in smalle AI heeft zelfs experts op dit gebied verrast, waarbij benchmarks jaren eerder werden overtroffen dan voorspeld.

[^2]: Deepmind, OpenAI, Anthropic en X.ai werden allemaal opgericht met het specifieke doel AGI te ontwikkelen. OpenAI's charter stelt bijvoorbeeld expliciet als doel het ontwikkelen van "artificiële algemene intelligentie die de hele mensheid ten goede komt", terwijl DeepMind's missie is "intelligentie oplossen, en dat vervolgens gebruiken om alles op te lossen." Meta, Microsoft en anderen volgen nu substantieel vergelijkbare paden. Meta heeft gezegd dat het [van plan is AGI te ontwikkelen en het open vrij te geven.](https://www.forbes.com/sites/johnkoetsier/2024/01/18/zuckerberg-on-ai-meta-building-agi-for-everyone-and-open-sourcing-it/)

[^3]: Hinton en Bengio zijn twee van de meest geciteerde AI-onderzoekers, hebben beiden de Nobel van het AI-vakgebied gewonnen, de Turing Prize, en Hinton heeft ook nog eens een Nobelprijs gewonnen (in de natuurkunde).

[^4]: Het bouwen van iets met dit risico, onder commerciële prikkels en vrijwel geen overheidtoezicht, is volkomen ongekend. Er is niet eens controverse over het risico onder degenen die het bouwen! De leiders van Deepmind, OpenAI en Anthropic, naast vele andere experts, hebben allemaal letterlijk een [verklaring](https://www.safe.ai/work/statement-on-ai-risk) ondertekend dat geavanceerde AI een *uitstervingsrisico voor de mensheid* vormt. De alarmbellen kunnen niet harder rinkelen, en men kan alleen concluderen dat degenen die ze negeren AGI en superintelligentie simpelweg niet serieus nemen. Een doel van dit essay is hen te helpen begrijpen waarom dat wel zou moeten.

## Hoofdstuk 2 - Basiskennis over AI-neurale netwerken

Hoe werken moderne AI-systemen, en wat kunnen we verwachten van de volgende generatie AI?

Om te begrijpen hoe de gevolgen van het ontwikkelen van krachtigere AI zich zullen ontvouwen, is het essentieel om enkele basisbeginselen te internaliseren. Dit hoofdstuk en de volgende twee secties behandelen deze aspecten: wat moderne AI is, hoe deze gebruikmaakt van massale berekeningen, en op welke manieren deze snel groeit in algemene toepasbaarheid en capaciteit.[^5]

Er zijn vele manieren om kunstmatige intelligentie te definiëren, maar voor onze doeleinden is de kerneigenschap van AI dat een AI-systeem, in tegenstelling tot een standaard computerprogramma dat een lijst instructies is voor het uitvoeren van een taak, leert van data of ervaring om taken uit te voeren *zonder expliciet te worden verteld hoe dit te doen.*

Vrijwel alle relevante moderne AI is gebaseerd op neurale netwerken. Dit zijn wiskundige/computationele structuren, weergegeven door een zeer grote (miljarden of biljoenen) verzameling getallen ("gewichten"), die een trainingstaak goed uitvoeren. Deze gewichten worden gecreëerd (of misschien "gekweekt" of "gevonden") door ze iteratief aan te passen zodat het neurale netwerk een numerieke score (ook wel "verlies" genoemd) verbetert die is gedefinieerd om goed te presteren bij een of meer taken.[^6] Dit proces staat bekend als het *trainen* van het neurale netwerk.[^7]

Er zijn veel technieken om deze training uit te voeren, maar die details zijn veel minder relevant dan de manieren waarop de scoring wordt gedefinieerd, en hoe deze resulteren in verschillende taken waarin het neurale netwerk goed presteert. Een belangrijk onderscheid wordt historisch gemaakt tussen "smalle" en "algemene" AI.

Smalle AI wordt bewust getraind om een bepaalde taak of kleine set taken uit te voeren (zoals het herkennen van afbeeldingen of het spelen van schaak); het vereist hertraining voor nieuwe taken en heeft een beperkte reikwijdte van capaciteiten. We hebben bovenmenselijke smalle AI, wat betekent dat we voor vrijwel elke discrete, goed gedefinieerde taak die een persoon kan uitvoeren, waarschijnlijk een score kunnen construeren en vervolgens succesvol een smal AI-systeem kunnen trainen om dit beter te doen dan een mens zou kunnen.

Algemene AI-systemen (GPAI) kunnen een breed scala aan taken uitvoeren, inclusief vele waarvoor ze niet expliciet zijn getraind; ze kunnen ook nieuwe taken leren als onderdeel van hun werking. Huidige grote "multimodale modellen"[^8] zoals ChatGPT exemplificeren dit: getraind op een zeer grote verzameling tekst en afbeeldingen, kunnen ze complexe redeneringen voeren, code schrijven, afbeeldingen analyseren en assisteren bij een enorme reeks intellectuele taken. Hoewel nog steeds behoorlijk verschillend van menselijke intelligentie op manieren die we hieronder diepgaand zullen zien, heeft hun algemeenheid een revolutie in AI veroorzaakt.[^9]

### Onvoorspelbaarheid: een kerneigenschap van AI-systemen

Een belangrijk verschil tussen AI-systemen en conventionele software ligt in voorspelbaarheid. De uitvoer van standaardsoftware kan onvoorspelbaar zijn – sterker nog, soms is dat waarom we software schrijven, om ons resultaten te geven die we niet hadden kunnen voorspellen. Maar conventionele software doet zelden iets waar het niet voor geprogrammeerd was – zijn reikwijdte en gedrag zijn over het algemeen zoals ontworpen. Een topklasse schaakprogramma kan zetten doen die geen mens zou kunnen voorspellen (anders zouden ze dat schaakprogramma kunnen verslaan!), maar het zal over het algemeen niets anders doen dan schaak spelen.

Net als conventionele software heeft smalle AI voorspelbare reikwijdte en gedrag, maar kan onvoorspelbare resultaten hebben. Dit is eigenlijk gewoon een andere manier om smalle AI te definiëren: als AI die vergelijkbaar is met conventionele software in zijn voorspelbaarheid en werkingsgebied.

Algemene AI is anders: zijn reikwijdte (de domeinen waarop het van toepassing is), gedrag (het soort dingen dat het doet) en resultaten (zijn werkelijke uitvoer) kunnen allemaal onvoorspelbaar zijn.[^10] GPT-4 werd alleen getraind om tekst accuraat te genereren, maar ontwikkelde vele capaciteiten die zijn trainers niet voorspelden of bedoelden. Deze onvoorspelbaarheid komt voort uit de complexiteit van training: omdat de trainingsdata uitvoer van veel verschillende taken bevat, moet de AI effectief leren om deze taken uit te voeren om goed te kunnen voorspellen.

Deze onvoorspelbaarheid van algemene AI-systemen is vrij fundamenteel. Hoewel het in principe mogelijk is om zorgvuldig AI-systemen te construeren die gegarandeerde limieten op hun gedrag hebben (zoals later in het essay wordt genoemd), zijn de AI-systemen zoals ze nu worden gecreëerd onvoorspelbaar in de praktijk en zelfs in principe.

### Passieve AI, agenten, autonome systemen en alignment

Deze onvoorspelbaarheid wordt bijzonder belangrijk wanneer we overwegen hoe AI-systemen daadwerkelijk worden ingezet en gebruikt om verschillende doelen te bereiken.

Veel AI-systemen zijn relatief passief in de zin dat ze voornamelijk informatie verstrekken, en de gebruiker onderneemt acties. Andere, gewoonlijk *agenten* genoemd, ondernemen zelf acties, met verschillende niveaus van betrokkenheid van een gebruiker. Degenen die acties ondernemen met relatief minder externe input of toezicht kunnen meer *autonoom* worden genoemd. Dit vormt een spectrum in termen van onafhankelijkheid van actie, van passieve tools tot autonome agenten.[^11]

Wat betreft doelen van AI-systemen, deze kunnen direct gekoppeld zijn aan hun trainingsdoelstelling (bijv. het doel van "winnen" voor een Go-spelend systeem is ook expliciet waarvoor het werd getraind). Of ze kunnen dat niet zijn: ChatGPT's trainingsdoelstelling is deels om tekst te voorspellen, deels om een behulpzame assistent te zijn. Maar bij het uitvoeren van een bepaalde taak wordt zijn doel door de gebruiker aan hem geleverd. Doelen kunnen ook door een AI-systeem zelf worden gecreëerd, alleen zeer indirect gerelateerd aan zijn trainingsdoelstelling.[^12]

Doelen zijn nauw verbonden met de kwestie van "alignment," dat wil zeggen de vraag of AI-systemen *zullen doen wat we willen dat ze doen*. Deze simpele vraag verbergt een enorm niveau van subtiliteit.[^13] Merk voor nu op dat "we" in deze zin kan verwijzen naar veel verschillende mensen en groepen, wat leidt tot verschillende typen alignment. Bijvoorbeeld, een AI kan zeer *gehoorzaam* zijn (of ["loyaal"](https://arxiv.org/abs/2003.11157)) aan zijn gebruiker – hier is "we" "elk van ons." Of het kan meer *soeverein* zijn, voornamelijk gedreven door zijn eigen doelen en beperkingen, maar nog steeds handelend in het brede gemeenschappelijke belang van menselijk welzijn – "we" is dan "de mensheid" of "de maatschappij." Daar tussenin ligt een spectrum waar een AI grotendeels gehoorzaam zou zijn, maar zou kunnen weigeren acties te ondernemen die anderen of de samenleving schaden, de wet overtreden, enz.

Deze twee assen – niveau van autonomie en type alignment – zijn niet volledig onafhankelijk. Bijvoorbeeld, een soeverein passief systeem is, hoewel niet geheel tegenstrijdig, een concept in spanning, evenals een gehoorzame autonome agent.[^14] Er is een duidelijke zin waarin autonomie en soevereiniteit de neiging hebben hand in hand te gaan. In een vergelijkbare geest heeft voorspelbaarheid de neiging hoger te zijn in "passieve" en "gehoorzame" AI-systemen, terwijl soevereine of autonome systemen meer onvoorspelbaar zullen zijn. Dit alles zal cruciaal zijn voor het begrijpen van de gevolgen van potentiële AGI en superintelligentie.

Het creëren van echt gealigneerde AI, van welke vorm dan ook, vereist het oplossen van drie onderscheiden uitdagingen:

1. Begrijpen wat "we" willen – wat complex is of "we" nu een specifiek persoon of organisatie (loyaliteit) of de mensheid in het algemeen (soevereiniteit) betekent;
2. Systemen bouwen die regelmatig handelen in overeenstemming met die wensen – in wezen het creëren van consistent positief gedrag;
3. Meest fundamenteel, systemen maken die oprecht "geven" om die wensen in plaats van alleen maar doen alsof ze dat doen.

Het onderscheid tussen betrouwbaar gedrag en oprechte zorg is cruciaal. Net zoals een menselijke werknemer orders perfect kan opvolgen terwijl hij geen echte toewijding heeft aan de missie van de organisatie, kan een AI-systeem gealigneerd handelen zonder werkelijk menselijke voorkeuren te waarderen. We kunnen AI-systemen trainen om dingen te zeggen en te doen door feedback, en ze kunnen leren redeneren over wat mensen willen. Maar hen *werkelijk* menselijke voorkeuren laten waarderen is een veel diepere uitdaging.[^15]

De diepgaande moeilijkheden bij het oplossen van deze alignment-uitdagingen, en hun implicaties voor AI-risico, zullen hieronder verder worden onderzocht. Begrijp voor nu dat alignment niet alleen een technische eigenschap is die we aan AI-systemen vastmaken, maar een fundamenteel aspect van hun architectuur dat hun relatie met de mensheid vormt.

[^5]: Voor een zachte maar technische inleiding tot machine learning en AI, met name taalmodellen, zie [deze site.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) Voor een andere moderne primer over AI-uitstervingsrisico's, zie [dit stuk.](https://www.thecompendium.ai/) Voor een uitgebreide en gezaghebbende wetenschappelijke analyse van de staat van AI-veiligheid, zie het recente [International AI Safety Report.](https://arxiv.org/abs/2501.17805)

[^6]: Training vindt typisch plaats door te zoeken naar een lokaal maximum van de score in een hoogdimensionale ruimte gegeven door de modelgewichten. Door te controleren hoe de score verandert naarmate gewichten worden aangepast, identificeert het trainingsalgoritme welke aanpassingen de score het meest verbeteren, en beweegt de gewichten in die richting.

[^7]: Bijvoorbeeld, in een beeldherkenningsprobleem zou het neurale netwerk waarschijnlijkheden uitvoeren voor labels voor de afbeelding. Een score zou gerelateerd zijn aan de waarschijnlijkheid die de AI toekent aan het juiste antwoord. De trainingsprocedure zou dan gewichten aanpassen zodat de volgende keer de AI een hogere waarschijnlijkheid zou uitvoeren voor het juiste label voor die afbeelding. Dit wordt dan een enorm aantal keren herhaald. Dezelfde basisprocedure wordt gebruikt bij het trainen van in wezen alle moderne neurale netwerken, zij het met complexere scoringsmechanismen.

[^8]: De meeste multimodale modellen gebruiken de "transformer"-architectuur om meerdere typen data (tekst, afbeeldingen, geluid) te verwerken en genereren. Deze kunnen allemaal worden ontleed in, en vervolgens behandeld op gelijke voet, als verschillende typen "tokens." Multimodale modellen worden eerst getraind om tokens binnen massale datasets accuraat te voorspellen, vervolgens verfijnd door reinforcement learning om capaciteiten te verbeteren en gedragingen te vormen.

[^9]: Dat taalmodellen getraind worden om één ding te doen – woorden voorspellen – heeft sommigen ertoe gebracht ze smalle AI te noemen. Maar dit is misleidend: omdat het goed voorspellen van tekst zo veel verschillende capaciteiten vereist, leidt deze trainingstaak tot een verrassend algemeen systeem. Merk ook op dat deze systemen uitgebreid getraind worden door reinforcement learning, wat effectief duizenden mensen vertegenwoordigt die het model een beloningssignaal geven wanneer het goed presteert bij een van de vele dingen die het doet. Het erft dan significante algemeenheid van de mensen die deze feedback geven.

[^10]: Er zijn meerdere manieren waarop AI onvoorspelbaar is. Een is dat men in het algemene geval niet kan voorspellen wat een algoritme zal doen zonder het daadwerkelijk uit te voeren; er zijn [stellingen](https://arxiv.org/abs/1310.3225) hieromtrent. Dit kan waar zijn alleen omdat de uitvoer van algoritmen complex kan zijn. Maar het is bijzonder duidelijk en relevant in het geval (zoals bij schaak of Go) waar de voorspelling een capaciteit zou impliceren (het verslaan van de AI) die de aspirant-voorspeller niet heeft. Ten tweede zal een gegeven AI-systeem niet altijd dezelfde uitvoer produceren zelfs bij dezelfde invoer – zijn uitvoer bevat willekeur; dit koppelt ook met algoritmische onvoorspelbaarheid. Ten derde kunnen onverwachte en emergente capaciteiten ontstaan uit training, wat betekent dat zelfs de *typen* dingen die een AI-systeem kan en zal doen onvoorspelbaar zijn; Dit laatste type is bijzonder belangrijk voor veiligheidsoverwegingen.

[^11]: Zie [hier](https://arxiv.org/abs/2502.02649) voor een diepgaande review van wat bedoeld wordt met een "autonome agent" (samen met ethische argumenten tegen het bouwen ervan).

[^12]: Je hoort soms wel eens "AI kan geen eigen doelen hebben." Dit is absolute onzin. Het is gemakkelijk om voorbeelden te genereren waar AI doelen heeft of ontwikkelt die nooit aan haar zijn gegeven en alleen aan haarzelf bekend zijn. Je ziet dit niet veel in huidige populaire multimodale modellen omdat het eruit getraind wordt; het zou net zo gemakkelijk erin getraind kunnen worden.

[^13]: Er is een grote literatuur. Over het algemene probleem zie Christian's [*The Alignment Problem*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821), en Russell's [*Human-Compatible*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616). Op een meer technische kant zie bijvoorbeeld [dit paper](https://arxiv.org/abs/2209.00626).

[^14]: We zullen later zien dat hoewel zulke systemen tegen de trend ingaan, dat hen eigenlijk zeer interessant en nuttig maakt.

[^15]: Dit betekent niet dat we emoties of bewustzijn vereisen. Eerder is het buitengewoon moeilijk van buitenaf een systeem te weten wat zijn innerlijke doelen, voorkeuren en waarden zijn. "Oprecht" zou hier betekenen dat we sterke genoeg reden hebben om erop te vertrouwen dat we in het geval van kritieke systemen ons leven erop kunnen zetten.

## Hoofdstuk 3 - Belangrijke aspecten van hoe moderne algemene AI-systemen worden gemaakt

De meeste van 's werelds meest geavanceerde AI-systemen worden gemaakt met verrassend vergelijkbare methoden. Hier zijn de basisprincipes.

Om een mens echt te begrijpen moet je iets weten van biologie, evolutie, kinderopvoeding en meer; om AI te begrijpen moet je ook weten hoe het wordt gemaakt. De afgelopen vijf jaar zijn AI-systemen enorm geëvolueerd, zowel in capaciteit als complexiteit. Een belangrijke enabler hiervoor is de beschikbaarheid van zeer grote hoeveelheden rekenkracht geweest (of informeel "compute" wanneer toegepast op AI).

De cijfers zijn verbluffend. Ongeveer 10 <sup>25</sup> -10 <sup>26</sup> "floating-point operaties" (FLOP) [^16] worden gebruikt bij de training van modellen zoals de GPT-serie, Claude, Gemini, etc.[^17] (Ter vergelijking: als elke mens op aarde non-stop zou werken en elke vijf seconden één berekening zou doen, zou het ongeveer een miljard jaar duren om dit te voltooien.) Deze enorme hoeveelheid berekeningen maakt het mogelijk om modellen te trainen met tot triljoenen modelgewichten op terabytes aan data – een groot deel van alle kwaliteitstekst die ooit is geschreven, samen met uitgebreide bibliotheken van geluiden, afbeeldingen en video. Door deze training aan te vullen met uitgebreide additionele training die menselijke voorkeuren en goede taakprestaties versterkt, vertonen op deze manier getrainde modellen prestaties die concurreren met mensen over een aanzienlijk spectrum van intellectuele basistaken, inclusief redeneren en probleemoplossing.

We weten ook (heel, heel ruwweg) hoeveel rekensnelheid, in operaties per seconde, voldoende is om de *inferentie*snelheid [^18] van zo'n systeem gelijk te laten zijn aan de *snelheid* van menselijke tekstverwerking. Het is ongeveer 10 <sup>15</sup> -10 <sup>16</sup> FLOP per seconde.[^19]

Hoewel krachtig, zijn deze modellen van nature beperkt op belangrijke manieren, vrij vergelijkbaar met hoe een individuele mens beperkt zou zijn als hij gedwongen zou worden om simpelweg tekst uit te voeren met een vast tempo van woorden per minuut, zonder te stoppen om na te denken of aanvullende tools te gebruiken. Meer recente AI-systemen pakken deze beperkingen aan door middel van een complexer proces en architectuur die verschillende sleutelelementen combineert:

- Een of meer neurale netwerken, waarbij één model de cognitieve kerncapaciteit levert, en maximaal verschillende andere meer specifieke taken uitvoeren;
- *Tooling* die beschikbaar wordt gesteld aan en bruikbaar is door het model – bijvoorbeeld de mogelijkheid om op het web te zoeken, documenten te creëren of bewerken, programma's uit te voeren, etc.
- *Scaffolding* die input en output van neurale netwerken verbindt. Een zeer eenvoudige scaffold zou bijvoorbeeld gewoon twee "instanties" van een AI-model met elkaar kunnen laten converseren, of de ene het werk van de andere kunnen laten controleren.[^20]
- *Chain-of-thought* en gerelateerde prompt-technieken doen iets soortgelijks en zorgen ervoor dat een model bijvoorbeeld vele benaderingen voor een probleem genereert en die benaderingen vervolgens verwerkt voor een samengevoegd antwoord.
- *Hertraining* van modellen om beter gebruik te maken van tools, scaffolding en chain-of-thought.

Omdat deze uitbreidingen zeer krachtig kunnen zijn (en AI-systemen zelf omvatten), kunnen deze samengestelde systemen vrij geraffineerd zijn en AI-capaciteiten dramatisch versterken.[^21] En recent zijn technieken in scaffolding en vooral chain-of-thought prompting (en het terugvoeren van resultaten in hertraining van modellen om deze beter te gebruiken) ontwikkeld en toegepast in [o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/) en [DeepSeek R1](https://api-docs.deepseek.com/news/news250120) om vele inferentie-rondes uit te voeren als reactie op een gegeven vraag.[^22] Dit stelt het model in feite in staat om "na te denken over" zijn reactie en verhoogt de capaciteit van deze modellen om hoogwaardig redeneren in wetenschap, wiskunde en programmeren dramatisch.[^23]

Voor een gegeven AI-architectuur kunnen toenames in trainingsrekenkracht [betrouwbaar worden vertaald](https://arxiv.org/abs/2405.10938) naar verbeteringen in een reeks duidelijk gedefinieerde meetwaarden. Voor minder scherp gedefinieerde algemene capaciteiten (zoals die hieronder besproken), is de vertaling minder helder en voorspellend, maar het is vrijwel zeker dat grotere modellen met meer trainingsrekenkracht nieuwe en betere capaciteiten zullen hebben, ook al is het moeilijk te voorspellen wat die zullen zijn.

Evenzo hebben samengestelde systemen en vooral vooruitgang in "chain of thought" (en training van modellen die er goed mee werken) schaalbaarheid in *inferentie*rekenkracht ontgrendeld: voor een gegeven getraind kernmodel nemen ten minste enkele AI-systeemcapaciteiten toe naarmate meer rekenkracht wordt toegepast die hen in staat stelt "harder en langer na te denken" over complexe problemen. Dit gaat gepaard met steile kosten aan rekensnelheid en vereist honderden of duizenden meer FLOP/s om menselijke prestaties te evenaren.[^24]

Hoewel slechts een deel van wat leidt tot snelle AI-vooruitgang,[^25] zal de rol van rekenkracht en de mogelijkheid van samengestelde systemen cruciaal blijken voor zowel het voorkomen van oncontroleerbare AGI als het ontwikkelen van veiligere alternatieven.

[^16]: 10 <sup>25</sup> betekent 1 gevolgd door 25 nullen, oftewel tien biljard biljoen. Een FLOP is gewoon een rekenkundige optelling of vermenigvuldiging van getallen met enige precisie. Let op dat AI-hardwareprestaties kunnen variëren met een factor tien meer afhankelijk van de precisie van de rekenkunde en de architectuur van de computer. Het tellen van logische-poort-operaties (ANDS, ORS, AND NOTS) zou fundamenteel zijn maar deze zijn niet algemeen beschikbaar of gebenchmarkt; voor huidige doeleinden is het nuttig om te standaardiseren op 16-bit operaties (FP16), hoewel passende conversiefactoren zouden moeten worden vastgesteld.

[^17]: Een verzameling schattingen en harde data is beschikbaar van [Epoch AI](https://epochai.org/data/large-scale-ai-models) en geeft ongeveer 2×10 <sup>25</sup> 16-bit FLOP voor GPT-4 aan; dit komt ongeveer overeen met [cijfers die gelekt zijn](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) voor GPT-4. Schattingen voor andere mid-2024 modellen liggen allemaal binnen een factor van enkele malen GPT-4.

[^18]: Inferentie is simpelweg het proces van het genereren van output uit een neuraal netwerk. Training kan worden beschouwd als een opeenvolging van vele inferenties en aanpassingen van modelgewichten.

[^19]: Voor tekstproductie vereiste de originele GPT-4 560 TFLOP per gegenereerd token. Ongeveer 7 tokens/s is nodig om menselijk denken bij te houden, dus dit geeft ≈3×10 <sup>15</sup> FLOP/s. Maar efficiëntieverbeteringen hebben dit naar beneden gebracht; [deze NVIDIA-brochure](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/) geeft bijvoorbeeld slechts 3×10 <sup>14</sup> FLOP/s aan voor een vergelijkbaar presterend Llama 405B-model.

[^20]: Als iets complexer voorbeeld zou een AI-systeem eerst verschillende mogelijke oplossingen voor een wiskundeprobleem kunnen genereren, dan een andere instantie gebruiken om elke oplossing te controleren, en ten slotte een derde gebruiken om de resultaten te synthetiseren tot een heldere uitleg. Dit maakt grondiger en betrouwbaarder probleemoplossen mogelijk dan een enkele doorgang.

[^21]: Zie bijvoorbeeld details over [OpenAI's "Operator"](https://openai.com/index/introducing-operator/), [Claude's tool-capaciteiten](https://docs.anthropic.com/en/docs/build-with-claude/computer-use), en [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). OpenAI's [Deep Research](https://openai.com/index/introducing-deep-research/) heeft waarschijnlijk een vrij geraffineerde architectuur maar details zijn niet beschikbaar.

[^22]: Deepseek R1 baseert zich op iteratieve training en prompting van het model zodat het uiteindelijke getrainde model uitgebreide chain-of-thought redenering creëert. Architecturale details zijn niet beschikbaar voor o1 of o3, echter Deepseek heeft onthuld dat er geen bijzondere "geheime saus" vereist is om capaciteitsschaling met inferentie te ontgrendelen. Maar ondanks dat het veel media-aandacht kreeg als het omverwerpen van de "status quo" in AI, beïnvloedt het de kernstellingen van dit essay niet.

[^23]: Deze modellen presteren aanzienlijk beter dan standaardmodellen op redeneerbenchmarks. Bijvoorbeeld in de GPQA Diamond Benchmark – een rigoureuze test van PhD-niveau wetenschapsvragen – [scoorde](https://openai.com/index/learning-to-reason-with-llms/) GPT-4o 56%, terwijl o1 en o3 respectievelijk 78% en 88% behaalden, ver boven de 70% gemiddelde score van menselijke experts.

[^24]: OpenAI's O3 heeft waarschijnlijk ∼10 <sup>21</sup> -10 <sup>22</sup> FLOP [gebruikt om elk van de ARC-AGI challenge-vragen te voltooien](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai), wat competente mensen kunnen doen in (pakweg) 10-100 seconden, wat een cijfer geeft van meer zoals ∼10 <sup>20</sup> FLOP/s.

[^25]: Hoewel rekenkracht een belangrijke maatstaf is voor AI-systeemcapaciteit, interacteert het met zowel datakwaliteit als algoritmische verbeteringen. Betere data of algoritmen kunnen rekenvereisten verminderen, terwijl meer rekenkracht soms kan compenseren voor zwakkere data of algoritmen.

## Hoofdstuk 4 - Wat zijn AGI en superintelligentie?

Wat proberen 's werelds grootste techbedrijven precies achter gesloten deuren te bouwen?

De term "artificiële algemene intelligentie" bestaat al geruime tijd om te verwijzen naar "menselijk niveau" AI voor algemene doeleinden. Het is nooit een bijzonder goed gedefinieerde term geweest, maar paradoxaal genoeg is het de afgelopen jaren niet beter gedefinieerd geworden maar wel nog belangrijker, waarbij experts tegelijkertijd discussiëren over of AGI nog decennia weg is of al bereikt, en biljoen-dollar bedrijven racen "naar AGI." (De dubbelzinnigheid van "AGI" werd onlangs belicht toen [gelekte documenten naar verluidt onthulden](https://gizmodo.com/leaked-documents-show-openai-has-a-very-clear-definition-of-agi-2000543339) dat in OpenAI's contract met Microsoft, AGI gedefinieerd werd als AI die $100 miljard aan inkomsten genereert voor OpenAI – een eerder mercantiele dan intellectuele definitie.)

Er zijn twee kernproblemen met het idee van AI met "menselijk intelligentieniveau." Ten eerste verschillen mensen enorm in hun vermogen om een bepaald type cognitief werk te doen, dus er is geen "menselijk niveau." Ten tweede is intelligentie zeer multidimensionaal; hoewel er correlaties kunnen zijn, zijn ze imperfect en kunnen ze heel anders zijn bij AI. Dus zelfs als we "menselijk niveau" voor veel capaciteiten zouden kunnen definiëren, zou AI er zeker ver bovenuit gaan in sommige, terwijl het er in andere ver onder blijft.[^26]

Het is niettemin cruciaal om types, niveaus en drempels van AI-capaciteiten te kunnen bespreken. De hier gehanteerde benadering benadrukt dat AI voor algemene doeleinden er is, en dat het komt – en zal komen – op verschillende capaciteitsniveaus waarbij het handig is om termen te koppelen, ook al zijn ze reductief, omdat ze corresponderen met cruciale drempels wat betreft AI's effecten op de samenleving en de mensheid.

We zullen "volledige" AGI definiëren als synoniem met "supermenselijke AI voor algemene doeleinden", wat een AI-systeem betekent dat in staat is om in wezen alle menselijke cognitieve taken uit te voeren op of boven het niveau van de beste menselijke experts, evenals nieuwe vaardigheden te verwerven en capaciteiten over te dragen naar nieuwe domeinen. Dit komt overeen met hoe "AGI" vaak gedefinieerd wordt in de moderne literatuur. Het is belangrijk op te merken dat dit een *zeer* hoge drempel is. Geen mens heeft dit type intelligentie; het is eerder het type intelligentie dat grote groepen van de beste menselijke experts zouden hebben als ze gecombineerd werden. We kunnen "superintelligentie" een capaciteit noemen die hierboven uitgaat, en meer beperkte capaciteitsniveaus definiëren met "menselijk-competitieve" en "expert-competitieve" GPAI, die een breed scala aan taken uitvoeren op typisch professioneel, of menselijk expertniveau.[^27]

Deze termen en enkele andere zijn verzameld in [de tabel](https://keepthefuturehuman.ai/essay/docs/#tab:terms) hieronder. Voor een concreter beeld van wat de verschillende gradaties van systemen kunnen doen, is het nuttig om de definities serieus te nemen en te overwegen wat ze betekenen.

| AI-Type                     | Gerelateerde Termen                           | Definitie                                                                                                                                                                                                                     | Voorbeelden                                                                                                                                        |
| --------------------------- | --------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |
| Smalle AI                   | Zwakke AI                                     | AI getraind voor een specifieke taak of familie van taken. Excelleert in zijn domein maar mist algemene intelligentie of transfer learning vermogen.                                                                         | Beeldherkenningssoftware; Spraakassistenten (bijv. Siri, Alexa); Schaakprogramma's; DeepMind's AlphaFold                                         |
| Tool-AI                     | Versterkte Intelligentie, AI-Assistent       | (Later besproken in essay.) AI-systeem dat menselijke capaciteiten versterkt. Combineert menselijk-competitieve AI voor algemene doeleinden, smalle AI en gegarandeerde controle, met prioriteit op veiligheid en samenwerking. Ondersteunt menselijke besluitvorming. | Geavanceerde programmeerassistenten; AI-aangedreven onderzoekstools; Geavanceerde data-analyseplatforms. Competente maar smalle en controleerbare agenten |
| AI voor algemene doeleinden (GPAI) |                                           | AI-systeem aanpasbaar aan verschillende taken, inclusief die waarvoor het niet specifiek getraind is.                                                                                                                        | Taalmodellen (bijv. GPT-4, Claude); Multimodale AI-modellen; DeepMind's MuZero                                                                    |
| Menselijk-competitieve GPAI | AGI \[zwak\]                                  | AI voor algemene doeleinden die taken uitvoert op gemiddeld menselijk niveau, soms daarbovenuit.                                                                                                                             | Geavanceerde taalmodellen (bijv. O1, Claude 3.5); Sommige multimodale AI-systemen                                                                 |
| Expert-competitieve GPAI    | AGI \[gedeeltelijk\]                          | AI voor algemene doeleinden die de meeste taken uitvoert op menselijk expertniveau, met significante maar beperkte autonomie                                                                                                 | Mogelijk een uitgeruste en ondersteunde O3, tenminste voor wiskunde, programmeren en enkele exacte wetenschappen                                  |
| AGI \[volledig\]            | Supermenselijke GPAI                          | AI-systeem dat autonoom ongeveer alle menselijke intellectuele taken kan uitvoeren op of boven expertniveau, met efficiënt leren en kennisoverdracht.                                                                       | \[Geen huidige voorbeelden – theoretisch\]                                                                                                        |
| Superintelligentie          | Zeer supermenselijke GPAI                    | AI-systeem dat menselijke capaciteiten ver overtreft in alle domeinen, beter presterend dan collectieve menselijke expertise. Deze overtreffing kan zijn in algemene geldigheid, kwaliteit, snelheid en/of andere maatstaven. | \[Geen huidige voorbeelden – theoretisch\]                                                                                                        |

We ervaren al hoe het is om GPAI's tot menselijk competitief niveau te hebben. Dit heeft zich relatief soepel geïntegreerd, omdat de meeste gebruikers dit ervaren als het hebben van een slimme maar beperkte tijdelijke werkkracht die hen productiever maakt met gemengde impact op de kwaliteit van hun werk.[^28]

Wat anders zou zijn aan expert-competitieve GPAI is dat het niet de kernbeperkingen van hedendaagse AI zou hebben, en de dingen zou doen die experts doen: onafhankelijk economisch waardevol werk, echte kenniscreatie, technisch werk waar je op kunt rekenen, terwijl het zelden (hoewel nog steeds af en toe) domme fouten maakt.

Het idee van volledige AGI is dat het *echt* alle cognitieve dingen doet die zelfs de meest capabele en effectieve mensen doen, autonoom en zonder benodigde hulp of toezicht. Dit omvat geavanceerde planning, het leren van nieuwe vaardigheden, het managen van complexe projecten, enz. Het zou origineel baanbrekend onderzoek kunnen doen. Het zou een bedrijf kunnen runnen. Wat je baan ook is, als het voornamelijk gedaan wordt met een computer of over de telefoon, *zou het dat minstens zo goed kunnen als jij.* En waarschijnlijk veel sneller en goedkoper. We bespreken enkele gevolgen hieronder, maar voor nu is de uitdaging voor jou om dit echt serieus te nemen. Stel je de tien meest goed geïnformeerde en competente mensen voor die je kent of van kent – inclusief CEO's, wetenschappers, professoren, topingenieurs, psychologen, politieke leiders en schrijvers. Verpak ze allemaal in één persoon, die ook 100 talen spreekt, een fenomenaal geheugen heeft, snel werkt, onvermoeibaar en altijd gemotiveerd is, en werkt onder het minimumloon.[^29] Dat geeft een indruk van wat AGI zou zijn.

Voor superintelligentie is het voorstellen moeilijker, omdat het idee is dat het intellectuele prestaties zou kunnen leveren die geen mens of zelfs groep mensen kan – het is per definitie onvoorspelbaar voor ons. Maar we kunnen een indruk krijgen. Als absolute baseline, overweeg veel AGI's, elk veel capabeler dan zelfs de beste menselijke expert, draaiend op 100 keer menselijke snelheid, met enorm geheugen en fantastisch coördinatievermogen.[^30] En het gaat omhoog van daaruit. Omgaan met superintelligentie zou minder zijn zoals converseren met een ander bewustzijn, meer zoals onderhandelen met een andere (en meer geavanceerde) beschaving.

Dus hoe dicht *zijn we* bij AGI en superintelligentie?

[^26]: Bijvoorbeeld, huidige AI-systemen overtreffen het menselijke vermogen ver in snelle rekenkunde of geheugentaken, terwijl ze tekortschieten in abstract redeneren en creatieve probleemoplossing.

[^27]: Zeer belangrijk, als concurrent zou dergelijke AI verschillende belangrijke structurele voordelen hebben waaronder: het zou niet vermoeid raken of andere individuele behoeften hebben zoals mensen; het kan op hogere snelheden draaien door simpelweg de rekenkracht op te schalen; het kan gekopieerd worden samen met elke expertise of kennis die het verwerft – en neurale netwerken's verworven kennis kan zelfs "samengevoegd" worden om hele skillsets onder elkaar over te dragen; het zou kunnen communiceren op machinesnelheid; en het zou zichzelf kunnen modificeren of verbeteren op meer significante manieren en hogere snelheid dan enige mens.

[^28]: Als je geen tijd hebt doorgebracht met het gebruik van huidige top-of-the-line AI-systemen, raad ik het aan: ze zijn oprecht nuttig en capabel, en het is ook belangrijk voor het kalibreren van het effect dat AI zal hebben naarmate ze krachtiger worden.

[^29]: Overweeg een groot onderzoeksziekenhuis: volledig gerealiseerde AGI zou tegelijkertijd alle inkomende patiëntgegevens kunnen analyseren, bijblijven met elk nieuw medisch artikel, diagnoses voorstellen, behandelplannen ontwerpen, klinische proeven beheren, en personeelsplanning coördineren – allemaal terwijl het opereert op een niveau dat de topspecialisten van het ziekenhuis in elk gebied evenaart of overtreft. En het zou dit voor meerdere ziekenhuizen tegelijkertijd kunnen doen, tegen een fractie van de huidige kosten. Helaas moet je ook een georganiseerd misdaadsyndicaat overwegen: volledig gerealiseerde AGI zou tegelijkertijd duizenden slachtoffers kunnen hacken, imiteren, bespioneren en chanteren, bijblijven met rechtshandhaving (die veel langzamer automatiseert), nieuwe geldverdienende schema's ontwerpen, en personeelsplanning coördineren – als er al personeel is.

[^30]: In zijn [essay](https://darioamodei.com/machines-of-loving-grace), noemde Dario Amodei, CEO van Anthropic, een "Land van \[een miljoen\] genieën".

## Hoofdstuk 5 - Op de drempel

De weg van de huidige AI-systemen naar volwaardige AGI lijkt schokkend kort en voorspelbaar.

De afgelopen tien jaar hebben dramatische vooruitgang in AI laten zien, aangedreven door enorme [computationele](https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year), menselijke en [financiële](https://arxiv.org/abs/2405.21015) middelen. Veel smalle AI-toepassingen presteren beter dan mensen bij hun toegewezen taken, en zijn zeker veel sneller en goedkoper.[^31] En er zijn ook smalle superhumane agenten die alle mensen kunnen verslaan in smalle domeinspellen zoals [Go](https://www.nature.com/articles/nature16961), [Schaken](https://arxiv.org/abs/1712.01815) en [Poker](https://www.deepstack.ai/), evenals meer [algemene agenten](https://deepmind.google/discover/blog/a-generalist-agent/) die kunnen plannen en acties uitvoeren in vereenvoudigde gesimuleerde omgevingen net zo effectief als mensen.

Het meest prominent zijn de huidige algemene AI-systemen van OpenAI/Microsoft, Google/Deepmind, Anthropic/Amazon, Facebook/Meta, X.ai/Tesla en anderen [^32] die sinds begin 2023 zijn ontstaan en sindsdien gestaag (hoewel ongelijkmatig) hun capaciteiten hebben vergroot. Al deze zijn gecreëerd via token-voorspelling op enorme tekst- en multimediadatasets, gecombineerd met uitgebreide versterkingsfeedback van mensen en andere AI-systemen. Sommige bevatten ook uitgebreide tool- en scaffolding-systemen.

### Sterke punten en zwakke punten van huidige algemene systemen

Deze systemen presteren goed bij een steeds bredere reeks tests die ontworpen zijn om intelligentie en expertise te meten, met vooruitgang die zelfs experts op het gebied heeft verrast:

- Bij de eerste release presteerde GPT-4 [op het niveau van of beter dan typische menselijke prestaties](https://arxiv.org/abs/2303.08774) bij standaard academische tests inclusief SATs, GRE, toelatingexamens en juridische examens. Meer recente modellen presteren waarschijnlijk aanzienlijk beter, hoewel resultaten niet publiek beschikbaar zijn.
- De Turing-test – lang beschouwd als een belangrijke benchmark voor "echte" AI – wordt nu routinematig doorstaan in sommige vormen door moderne taalmodellen, zowel informeel als in [formele studies](https://arxiv.org/abs/2405.08007).[^33]
- Op de uitgebreide MMLU-benchmark die 57 academische vakgebieden omvat, [behalen recente modellen scores op domeinexpertniveau](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu) (∼90%) [^34]
- Technische expertise is dramatisch vooruitgegaan: De GPQA-benchmark van natuurkundevragen op graduate-niveau zag [prestaties springen](https://epoch.ai/data/ai-benchmarking-dashboard) van bijna willekeurig raden (GPT-4, 2022) naar expertniveau (o1-preview, 2024).
- Zelfs tests die specifiek ontworpen zijn om AI-resistent te zijn, vallen: OpenAI's O3 [lost naar verluidt](https://www.nextbigfuture.com/2024/12/openai-releases-o3-model-with-high-performance-and-high-cost.html) de ARC-AGI abstracte probleemoplossing benchmark op menselijk niveau op, behaalt top-expert programmeerpreestaties, en scoort 25% op Epoch AI's "frontier math" problemen die ontworpen zijn om elite-wiskundigen uit te dagen.[^35]
- De trend is zo duidelijk dat MMLU's ontwikkelaar nu ["Humanity's Last Exam"](https://agi.safe.ai/) heeft gecreëerd – een onheilspellende naam die de mogelijkheid weergeeft dat AI binnenkort menselijke prestaties bij elke betekenisvolle test zal overtreffen. Op het moment van schrijven zijn er beweringen dat AI-systemen 27% (volgens [Sam Altman](https://x.com/sama/status/1886220281565381078)) en 35% (volgens [dit paper](https://arxiv.org/abs/2502.09955)) behalen op dit extreem moeilijke examen. Het is hoogst onwaarschijnlijk dat een individuele mens dit zou kunnen.

Ondanks deze indrukwekkende cijfers (en hun duidelijke intelligentie wanneer je ermee interacteert) [^36] zijn er veel dingen die (tenminste de uitgebrachte versies van) deze neurale netwerken *niet kunnen* doen. Momenteel zijn de meeste ontlichaamd – bestaand alleen op servers – en verwerken hooguit tekst, geluid en stilstaande beelden (maar geen video). Cruciaal is dat de meeste geen complexe geplande activiteiten kunnen uitvoeren die hoge nauwkeurigheid vereisen.[^37] En er zijn een aantal andere kwaliteiten die sterk zijn in hoogwaardige menselijke cognitie maar momenteel laag in uitgebrachte AI-systemen.

De volgende tabel somt er een aantal op, gebaseerd op midden-2024 AI-systemen zoals GPT-4o, Claude 3.5 Sonnet en Google Gemini 1.5.[^38] De sleutelvraag voor hoe snel algemene AI krachtiger zal worden is: in welke mate zal alleen *meer van hetzelfde* doen resultaten opleveren, versus het toevoegen van additionele maar *bekende* technieken, versus het ontwikkelen of implementeren van *echt nieuwe* AI-onderzoeksrichtingen. Mijn eigen voorspellingen hiervoor staan in de tabel, in termen van hoe waarschijnlijk elk van deze scenario's is om die capaciteit tot en boven menselijk niveau te krijgen.

<table><tbody><tr><th>Capaciteit</th><th>Beschrijving van capaciteit</th><th>Status/prognose</th><th>Schaling/bekend/nieuw</th></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td colspan="4"><em>Kerncompetenties Cognitie</em></td></tr><tr><td>Redenering</td><td>Menschen kunnen nauwkeurige, meerstappige redenering doen, regels volgen en nauwkeurigheid controleren.</td><td>Dramatische recente vooruitgang met uitgebreide chain-of-thought en hertraining</td><td>95/5/5</td></tr><tr><td>Planning</td><td>Menschen vertonen langetermijn- en hiërarchische planning.</td><td>Verbetert met schaling; kan sterk worden geholpen met scaffolding en betere trainingstechnieken.</td><td>10/85/5</td></tr><tr><td>Waarheidsgrondslag</td><td>GPAI's verzinnen ongegronde informatie om vragen te beantwoorden.</td><td>Verbetert met schaling; kalibratiedata beschikbaar binnen model; kan worden gecontroleerd/verbeterd via scaffolding.</td><td>30/65/5</td></tr><tr><td>Flexibele probleemoplossing</td><td>Menschen kunnen nieuwe patronen herkennen en nieuwe oplossingen bedenken voor complexe problemen; huidige ML-modellen hebben moeite hiermee.</td><td>Verbetert met schaling maar zwak; kan oplosbaar zijn met neurosymbolische of gegeneraliseerde "zoek"-technieken.</td><td>15/75/10</td></tr><tr><td colspan="4"><em>Leren en Kennis</em></td></tr><tr><td>Leren & geheugen</td><td>Menschen hebben werkgeheugen, kortetermijn- en langetermijngeheugen, die allemaal dynamisch en onderling gerelateerd zijn.</td><td>Alle modellen leren tijdens training; GPAI's leren binnen contextvenster en tijdens fine-tuning; "continual learning" en andere technieken bestaan maar zijn nog niet geïntegreerd in grote GPAI's.</td><td>5/80/15</td></tr><tr><td>Abstractie & recursie</td><td>Menschen kunnen relatiesets toewijzen en overdragen naar meer abstracte voor redenering en manipulatie, inclusief recursieve "meta" redenering.</td><td>Verbetert zwak met schaling; zou kunnen ontstaan in neurosymbolische systemen.</td><td>30/50/20</td></tr><tr><td>Wereldmodel(len)</td><td>Menschen hebben en updaten voortdurend een voorspellend wereldmodel waarbinnen zij problemen kunnen oplossen en fysieke redenering kunnen doen</td><td>Verbetert met schaling; updaten gekoppeld aan leren; GPAI's zwak in echte-wereld voorspelling.</td><td>20/50/30</td></tr><tr><td colspan="4"><em>Zelf en Agentschap</em></td></tr><tr><td>Agentschap</td><td>Menschen kunnen acties ondernemen om doelen na te streven, gebaseerd op planning/voorspelling.</td><td>Veel ML-systemen zijn agentisch; LLM's kunnen agenten worden gemaakt via wrappers.</td><td>5/90/5</td></tr><tr><td>Zelfsturing</td><td>Menschen ontwikkelen en streven hun eigen doelen na, met intern gegenereerde motivatie en drijfveer.</td><td>Grotendeels samengesteld uit agentschap plus originaliteit; waarschijnlijk ontstaan in complexe agentische systemen met abstracte doelen.</td><td>40/45/15</td></tr><tr><td>Zelfreferentie</td><td>Menschen begrijpen en redeneren over zichzelf als gesitueerd binnen een omgeving/context.</td><td>Verbetert met schaling en kan worden versterkt met trainingsbeloning.</td><td>70/15/15</td></tr><tr><td>Zelfbewustzijn</td><td>Menschen hebben kennis van en kunnen redeneren over hun eigen gedachten en mentale toestanden.</td><td>Bestaat in zekere zin in GPAI's, die naar verluidt de klassieke "spiegeltest" voor zelfbewustzijn kunnen doorstaan. Kan worden verbeterd met scaffolding; maar onduidelijk of dit genoeg is.</td><td>20/55/25</td></tr><tr><td colspan="4"><em>Interface en Omgeving</em></td></tr><tr><td>Belichaamde intelligentie</td><td>Menschen begrijpen en interacteren actief met hun echte-wereld omgeving.</td><td>Reinforcement learning werkt goed in gesimuleerde en echte-wereld (robotische) omgevingen en kan worden geïntegreerd in multimodale transformers.</td><td>5/85/10</td></tr><tr><td>Multizintuiglijke verwerking</td><td>Menschen integreren en verwerken real-time visuele, audio en andere sensorische stromen.</td><td>Training in meerdere modaliteiten lijkt "gewoon te werken" en verbetert met schaling. Real-time videoverwerking is moeilijk maar bijv. zelfrijdende systemen verbeteren snel.</td><td>30/60/10</td></tr><tr><td colspan="4"><em>Hogere-orde Capaciteiten</em></td></tr><tr><td>Originaliteit</td><td>Huidige ML-modellen zijn creatief in het transformeren en combineren van bestaande ideeën/werken, maar menschen kunnen nieuwe kaders en structuren bouwen, soms gekoppeld aan hun identiteit.</td><td>Kan moeilijk te onderscheiden zijn van "creativiteit," die erin kan schalen; kan ontstaan uit creativiteit plus zelfbewustzijn.</td><td>50/40/10</td></tr><tr><td>Bewustzijn</td><td>Menschen ervaren qualia; deze kunnen positieve, negatieve of neutrale valentie hebben; het is "iets om" een persoon te zijn.</td><td>Zeer moeilijk en filosofisch beladen om te bepalen of een gegeven systeem dit heeft.</td><td>5/10/85</td></tr></tbody></table>

Belangrijke capaciteiten die momenteel onder menselijk expertniveau zijn in moderne GPAI-systemen, gegroepeerd per type. De derde kolom vat de huidige status samen. De laatste kolom toont voorspelde waarschijnlijkheid (%) dat menselijk niveau wordt bereikt door: schaling van huidige technieken / combinatie met bekende technieken / ontwikkeling van nieuwe technieken. Deze capaciteiten zijn niet onafhankelijk, en toename in een ervan gaat typisch samen met toenames in anderen. Merk op dat niet alle (vooral bewustzijn) nodig zijn voor AI-systemen die in staat zijn AI-ontwikkeling te bevorderen, wat de mogelijkheid benadrukt van krachtige maar niet-bewuste AI.

Door op deze manier uit te splitsen wat "ontbreekt", wordt het vrij duidelijk dat we behoorlijk op koers zijn voor breed boven-menselijke intelligentie door het schalen van bestaande of bekende technieken.[^39]

Er kunnen nog steeds verrassingen zijn. Zelfs afgezien van "bewustzijn" zouden er van de genoemde cognitieve kerncompetenties enkele kunnen zijn die echt niet gedaan kunnen worden met huidige technieken en nieuwe vereisen. Maar overweeg dit. De huidige inspanning door veel van 's werelds grootste bedrijven bedraagt meerdere malen de uitgaven van het Apollo-project en tientallen malen die van het Manhattan-project,[^40] en stelt duizenden van de allerbeste technische mensen te werk tegen ongekende salarissen. De dynamiek van de afgelopen jaren heeft nu meer menselijke intellectuele kracht (met AI die nu wordt toegevoegd) hierop gericht dan enige onderneming in de geschiedenis. We zouden niet moeten wedden op mislukking.

### Het grote doel: generalistische autonome agenten

De ontwikkeling van algemene AI in de afgelopen jaren heeft zich gericht op het creëren van algemene en krachtige maar tool-achtige AI: het functioneert primair als een (redelijk) loyale assistent, en onderneemt over het algemeen niet zelf acties. Dit is deels bewust ontwerp, maar grotendeels omdat deze systemen simpelweg niet bekwaam genoeg zijn geweest in de relevante vaardigheden om toevertrouwd te worden met complexe acties.[^41]

AI-bedrijven en onderzoekers [verschuiven echter steeds meer de focus](https://www.axios.com/2025/01/23/davos-2025-ai-agents) naar *autonome* generalistische agenten op expertniveau.[^42] Dit zou de systemen in staat stellen meer te handelen als een menselijke assistent aan wie de gebruiker echte acties kan delegeren.[^43] Wat is daarvoor nodig? Een aantal van de capaciteiten in de "wat ontbreekt"-tabel zijn hierbij betrokken, waaronder sterke waarheidsgrondslag, leren en geheugen, abstractie en recursie, en wereldmodellering (voor intelligentie), planning, agentschap, originaliteit, zelfsturing, zelfreferentie en zelfbewustzijn (voor autonomie), en multizintuiglijke verwerking, belichaamde intelligentie en flexibele probleemoplossing (voor generaliteit).[^44]

Deze drievoudige kruising van hoge autonomie (onafhankelijkheid van actie), hoge generaliteit (bereik en taakbreedte) en hoge intelligentie (competentie bij cognitieve taken) is momenteel uniek voor mensen. Het is impliciet wat velen waarschijnlijk voor ogen hebben wanneer ze denken aan AGI – zowel in termen van waarde als risico's.

Dit biedt een andere manier om A-G-I te definiëren als ***A*** utonome- ***G*** enerale- ***I*** ntelligentie, en we zullen zien dat deze drievoudige kruising een zeer waardevolle lens biedt voor systemen met hoge capaciteiten, zowel in het begrijpen van hun risico's en beloningen als in de governance van AI.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Simple-1024x1024.png&w=3840&q=75) De transformatieve A-G-I kracht- en risicozone ontstaat uit de kruising van drie belangrijke eigenschappen: hoge Autonomie, hoge Intelligentie bij taken, en hoge Generaliteit.

### De AI (zelf-)verbeteringscyclus

Een laatste cruciale factor in het begrijpen van AI-vooruitgang is AI's unieke technologische feedbackloop. Bij het ontwikkelen van AI brengt succes – zowel in gedemonstreerde systemen als ingezette producten – extra investering, talent en concurrentie met zich mee, en we bevinden ons momenteel midden in een enorme AI hype-plus-realiteit feedbackloop die honderden miljarden, of zelfs biljoenen dollars aan investeringen aanstuurt.

Dit type feedbackcyclus zou kunnen gebeuren bij elke technologie, en we hebben het bij veel gezien, waar marktsucces investering bevordert, wat verbetering en beter marktsucces bevordert. Maar AI-ontwikkeling gaat verder, doordat nu AI-systemen helpen bij het ontwikkelen van nieuwe en krachtigere AI-systemen.[^45] We kunnen deze feedbackloop zien in vijf stadia, elk met een kortere tijdschaal dan de vorige, zoals getoond in de tabel.

*De AI-verbeteringscyclus werkt op meerdere tijdschalen, waarbij elk stadium mogelijk volgende stadia versnelt. Eerdere stadia zijn goed onderweg, terwijl latere stadia speculatief blijven maar zeer snel zouden kunnen verlopen zodra ze ontgrendeld zijn.*

Verschillende van deze stadia zijn al onderweg, en een paar beginnen duidelijk. Het laatste stadium, waarin AI-systemen zichzelf autonoom verbeteren, is een hoofdbestanddeel geweest van de literatuur over het risico van zeer krachtige AI-systemen, en terecht.[^46] Maar het is belangrijk op te merken dat het slechts de meest drastische vorm is van een feedbackcyclus die al is begonnen en zou kunnen leiden tot meer verrassingen in de snelle vooruitgang van de technologie.

[^31]: Je gebruikt veel meer van deze AI dan je waarschijnlijk denkt, bij spraakgeneratie en -herkenning, beeldverwerking, nieuwsfeed-algoritmes, enz.

[^32]: Hoewel de relaties tussen deze bedrijfsparen vrij complex en genuanceerd zijn, heb ik ze expliciet genoemd om zowel de enorme totale marktkapitalisatie van bedrijven die nu betrokken zijn bij AI-ontwikkeling aan te geven, als ook dat er achter zelfs "kleinere" bedrijven zoals Anthropic enorm diepe zakken zitten via investeringen en grote partnerschapsdeals.

[^33]: Het is mode geworden om de Turing-test te kleineren, maar hij is vrij krachtig en algemeen. In zwakke versies geeft hij aan of typische mensen die interacteren met een AI (die getraind is om menselijk te handelen) op typische manieren voor korte periodes kunnen zeggen of het een AI is. Dat kunnen ze niet. Ten tweede kan een zeer adversariële Turing-test in wezen elk element van menselijke capaciteit en intelligentie onderzoeken – door bijv. een AI-systeem te vergelijken met een menselijke expert, geëvalueerd door andere menselijke experts. Er is een zin waarin veel AI-evaluatie een gegeneraliseerde vorm van Turing-test is.

[^34]: Dit is per domein – geen mens zou plausibel zulke scores kunnen behalen over alle vakken tegelijkertijd.

[^35]: Dit zijn problemen die zelfs uitstekende wiskundigen aanzienlijke tijd zouden kosten om op te lossen, als ze ze überhaupt zouden kunnen oplossen.

[^36]: Als je van een sceptische inslag bent, behoud dan je scepsis maar probeer echt de meest actuele modellen uit, en probeer ook zelf enkele van de testvragen die ze kunnen doorstaan. Als natuurkundeprofessor zou ik met bijna zekerheid voorspellen dat bijvoorbeeld de topmodellen zouden slagen voor het graduate-kwalificatie-examen op onze afdeling.

[^37]: Dit en andere zwakke punten zoals verzinnen hebben marktacceptatie vertraagd en geleid tot een kloof tussen waargenomen en beweerde capaciteiten (wat ook bekeken moet worden door de lens van intense marktconcurrentie en de noodzaak om investering aan te trekken). Dit heeft zowel het publiek als beleidsmakers verward over de werkelijke staat van AI-vooruitgang. Hoewel misschien niet overeenstemmend met de hype, is de vooruitgang zeer reëel.

[^38]: De belangrijkste vooruitgang sindsdien is de ontwikkeling van systemen getraind voor topkwaliteit redenering, gebruik makend van meer berekening tijdens inferentie en meer reinforcement learning. Omdat deze modellen nieuw zijn en hun capaciteiten minder getest, heb ik deze tabel niet volledig herzien behalve voor "redenering", wat ik beschouw als in wezen opgelost. Maar ik heb voorspellingen bijgewerkt gebaseerd op ervaren en gerapporteerde capaciteiten van die systemen.

[^39]: Eerdere golven van AI-optimisme in de jaren 1960 en 1980 eindigden in "AI-winters" toen beloofde capaciteiten faalden te materialiseren. De huidige golf verschilt echter fundamenteel doordat superhumane prestaties in veel domeinen zijn behaald, gesteund door massale computationele middelen en commercieel succes.

[^40]: Het volledige Apollo-project [kostte ongeveer $250 miljard USD in 2020 dollars](https://www.planetary.org/space-policy/cost-of-apollo), en het Manhattan-project [minder dan een tiende daarvan](https://www.brookings.edu/the-costs-of-the-manhattan-project/). Goldman Sachs [projecteert een biljoen dollar uitgaven alleen al aan AI-datacenters](https://www.datacenterdynamics.com/en/news/goldman-sachs-1tn-to-be-spent-on-ai-data-centers-chips-and-utility-upgrades-with-little-to-show-for-it-so-far/) in de komende jaren.

[^41]: Hoewel mensen genoeg fouten maken, onderschatten we hoe betrouwbaar we kunnen zijn! Omdat kansen zich vermenigvuldigen, vereist een taak die 20 stappen correct moet doen dat elke stap 97% betrouwbaar is om het de helft van de tijd goed te doen. We doen zulke taken voortdurend.

[^42]: Een sterke beweging in deze richting is zeer recentelijk ondernomen met OpenAI's ["Deep Research"](https://openai.com/index/introducing-deep-research/) assistent die autonoom algemeen onderzoek uitvoert, beschreven als "een nieuwe agentische capaciteit die meerstaps-onderzoek op internet uitvoert voor complexe taken."

[^43]: Dingen zoals dat vervelende PDF-formulier invullen, vluchten boeken, enz. Maar met een PhD in 20 vakgebieden! Dus ook: die scriptie voor je schrijven, dat contract voor je onderhandelen, die stelling voor je bewijzen, die reclamecampagne voor je creëren, enz. Wat doe *jij*? Je vertelt het wat te doen, natuurlijk.

[^44]: Merk op dat bewustzijn *niet* duidelijk vereist is, noch impliceert AI in deze drievoudige kruising dit noodzakelijk.

[^45]: De dichtstbijzijnde analogie hier is misschien chiptechnologie, waar ontwikkeling Moore's wet decennia heeft volgehouden, terwijl computertechnologieën mensen helpen de volgende generatie chiptechnologie te ontwerpen. Maar AI zal veel directer zijn.

[^46]: Het is belangrijk om het even te laten bezinken dat AI – binnenkort – zichzelf zou kunnen verbeteren op een tijdschaal van dagen of weken. Of minder. Houd dit in gedachten wanneer iemand je vertelt dat een AI-capaciteit zeker ver weg is.

## Hoofdstuk 6 - De race naar AGI

Wat zijn de drijvende krachten achter de race om AGI te bouwen, voor zowel bedrijven als landen?

De recente snelle vooruitgang in AI heeft zowel geleid tot als geresulteerd in een buitengewoon niveau van aandacht en investeringen. Dit wordt deels gedreven door succes in AI-ontwikkeling, maar er speelt meer. Waarom racen enkele van de grootste bedrijven ter wereld, en zelfs landen, om niet alleen AI te bouwen, maar AGI en superintelligentie?

### Wat heeft AI-onderzoek richting menselijk-niveau AI gedreven

Tot ongeveer de laatste vijf jaar was AI grotendeels een academisch en wetenschappelijk onderzoeksprobleem, dus grotendeels gedreven door nieuwsgierigheid en de drang om intelligentie te begrijpen en hoe deze in een nieuw substraat te creëren.

In deze fase werd er relatief weinig aandacht besteed aan de voordelen of gevaren van AI onder de meeste onderzoekers. Wanneer gevraagd waarom AI zou moeten worden ontwikkeld, zou een veelvoorkomend antwoord zijn om enigszins vaag problemen op te sommen waar AI mee zou kunnen helpen: nieuwe geneesmiddelen, nieuwe materialen, nieuwe wetenschap, slimmere processen, en in het algemeen dingen verbeteren voor mensen.[^47]

Dit zijn bewonderenswaardige doelen![^48] Hoewel we kunnen en zullen vragen of AGI – in plaats van AI in het algemeen – noodzakelijk is voor deze doelen, tonen ze het idealisme waarmee veel AI-onderzoekers begonnen.

In de afgelopen vijf jaar is AI echter getransformeerd van een relatief puur onderzoeksveld naar veel meer een engineering- en productveld, grotendeels gedreven door enkele van 's werelds grootste bedrijven.[^49] Onderzoekers zijn, hoewel relevant, niet langer de leiding over het proces.

### Waarom proberen bedrijven AGI te bouwen?

Dus waarom storten gigantische bedrijven (en nog meer investeerders) enorme middelen in het bouwen van AGI? Er zijn twee drijfveren waar de meeste bedrijven vrij eerlijk over zijn: zij zien AI als drijvers van productiviteit voor de samenleving, en van winst voor henzelf. Omdat algemene AI van nature algemeen toepasbaar is, is er een enorme prijs: in plaats van een sector te kiezen waarin producten en diensten te creëren, kan men *ze allemaal tegelijk* proberen. Big Tech-bedrijven zijn enorm gegroeid door digitale goederen en diensten te produceren, en minstens enkele leidinggevenden zien AI zeker gewoon als de volgende stap in het goed leveren ervan, met risico's en voordelen die uitbreiden op maar een echo zijn van die geboden door zoekmachines, sociale media, laptops, telefoons, enz.

Maar waarom AGI? Er is een heel eenvoudig antwoord hierop, waar de meeste bedrijven en investeerders schuw van wegblijven om publiekelijk te bespreken.[^50]

Het is dat AGI direct, één-op-één, *werknemers kan vervangen.*

Niet versterken, niet empoweren, niet productiever maken. Niet eens *verdringen.* Dit alles kan en zal gedaan worden door non-AGI. AGI is specifiek wat volledig denkende werknemers kan *vervangen* (en met robotica, ook veel fysieke werknemers.) Als ondersteuning voor dit standpunt hoeft men niet verder te kijken dan OpenAI's [(publiek verklaarde) definitie](https://openai.com/our-structure/) van AGI, namelijk "een zeer autonoom systeem dat beter presteert dan mensen bij het meeste economisch waardevolle werk."

De prijs hier (voor bedrijven!) is enorm. Arbeidskosten zijn een substantieel percentage van 's werelds ∼$100 biljoen mondiale economie. Zelfs als slechts een fractie hiervan wordt veroverd door vervanging van menselijke arbeid door AI-arbeid, gaat dit om biljoenen dollars aan jaarlijkse omzet. AI-bedrijven zijn zich ook bewust van wie bereid is te betalen. Zoals zij het zien, ga jij niet duizenden dollars per jaar betalen voor productiviteitstools. Maar een bedrijf *zal* duizenden dollars per jaar betalen om jouw arbeid te vervangen, als ze dat kunnen.

### Waarom landen het gevoel hebben dat ze moeten racen naar AGI

Landen stellen motivaties voor het nastreven van AGI die zich richten op economisch en wetenschappelijk leiderschap. Het argument is overtuigend: AGI zou wetenschappelijk onderzoek, technologische ontwikkeling en economische groei dramatisch kunnen versnellen. Gezien wat er op het spel staat, beweren zij, kan geen grote mogendheid zich veroorloven achter te blijven.[^51]

Maar er zijn ook aanvullende en grotendeels onuitgesproken drijfveren. Er is geen twijfel dat wanneer bepaalde militaire en nationale veiligheidsleiders achter gesloten deuren bijeenkomen om een buitengewoon krachtige en catastrophaal risicovolle technologie te bespreken, hun focus niet ligt op "hoe vermijden we die risico's" maar eerder "hoe krijgen we dit eerst?" Militaire en inlichtingenleiders zien AGI als een potentiële revolutie in militaire aangelegenheden, misschien wel de meest significante sinds kernwapens. De vrees is dat het eerste land dat AGI ontwikkelt een onoverkoomelijk strategisch voordeel zou kunnen krijgen. Dit creëert een klassieke wapenwedloop-dynamiek.

We zullen zien dat dit "race naar AGI"-denken,[^52] hoewel overtuigend, diep gebrekkig is. Dit is niet omdat racen gevaarlijk en riskant is – hoewel het dat is – maar vanwege de aard van de technologie. De onuitgesproken aanname is dat AGI, zoals andere technologieën, controleerbaar is door de staat die het ontwikkelt, en een macht-verlenende zegen is voor de samenleving die er het meest van heeft. Zoals we zullen zien, zal het waarschijnlijk geen van beide zijn.

### Waarom superintelligentie?

Terwijl bedrijven publiekelijk focussen op productiviteit, en landen op economische en technologische groei, zijn dit voor degenen die bewust volledige AGI en superintelligentie nastreven slechts het begin. Wat hebben zij werkelijk voor ogen? Hoewel zelden hardop gezegd, omvatten zij:

1. Geneesmiddelen voor veel of alle ziektes;
2. Stoppen en omkering van veroudering;
3. Nieuwe duurzame energiebronnen zoals fusie;
4. Menselijke upgrades, of ontworpen organismen via genetische manipulatie;
5. Nanotechnologie en moleculaire fabricage;
6. Mind uploads;
7. Exotische fysica of ruimtetechnologieën;
8. Super-menselijk advies en beslissingsondersteuning;
9. Super-menselijke planning en coördinatie.

De eerste drie zijn grotendeels "enkelzijdige" technologieën – d.w.z. waarschijnlijk vrij sterk netto positief. Het is moeilijk te argumenteren tegen het genezen van ziektes of langer kunnen leven als men dat kiest. En we hebben al de negatieve kant van fusie geoogst (in de vorm van kernwapens); het zou mooi zijn nu de positieve kant te krijgen. De vraag bij deze eerste categorie is of het eerder krijgen van deze technologieën het risico compenseert.

De volgende vier zijn duidelijk tweesnijdend: transformatieve technologieën met zowel potentieel enorme voordelen als immense risico's, net als AI. Al deze zouden, als ze morgen uit een zwarte doos zouden springen en ingezet werden, ongelooflijk moeilijk te beheren zijn.[^53]

De laatste twee betreffen de super-menselijke AI die zelf dingen doet in plaats van alleen technologie uitvinden. Meer precies, eufemismen terzijde latend, behelzen deze krachtige AI-systemen die mensen vertellen wat te doen. Dit "advies" noemen is onoprecht als het systeem dat adviseert veel machtiger is dan de geadviseerde, die de basis van de beslissing niet betekenisvol kan begrijpen (of zelfs als dit wordt verstrekt, er niet op kan vertrouwen dat de adviseur niet een even overtuigende rationale zou verstrekken voor een andere beslissing.)

Dit wijst naar een belangrijk item dat ontbreekt in bovenstaande lijst:

10. Macht.

Het is overduidelijk dat veel van wat ten grondslag ligt aan de huidige race voor super-menselijke AI het idee is dat *intelligentie = macht*. Elke racer zet erop in de beste houder van die macht te zijn, en dat zij deze zullen kunnen hanteren voor ogenschijnlijk welwillende redenen zonder dat het uit hun controle glipt of wordt weggenomen.

Dat wil zeggen, wat bedrijven en naties werkelijk najagen is niet alleen de vruchten van AGI en superintelligentie, maar de macht om te controleren wie er toegang toe krijgt en hoe ze worden gebruikt. Bedrijven zien zichzelf als verantwoordelijke beheerders van deze macht ten dienste van aandeelhouders en de mensheid; naties zien zichzelf als noodzakelijke bewakers die voorkomen dat vijandige machten beslissend voordeel verkrijgen. Beiden hebben gevaarlijk ongelijk, falen erin te erkennen dat superintelligentie, door haar aard, niet betrouwbaar gecontroleerd kan worden door enige menselijke instelling. We zullen zien dat de aard en dynamiek van superintelligente systemen menselijke controle extreem moeilijk, zo niet onmogelijk maken.

Deze race-dynamieken – zowel bedrijfsmatig als geopolitiek – maken bepaalde risico's bijna onvermijdelijk tenzij ze beslist worden onderbroken. We keren ons nu naar het onderzoeken van deze risico's en waarom ze niet adequaat kunnen worden beperkt binnen een competitief [^54] ontwikkelingsparadigma.

[^47]: Een preciezere lijst van waardige doelen zijn de VN [Duurzame Ontwikkelingsdoelen.](https://sdgs.un.org/goals) Dit zijn, in zekere zin, het dichtst wat we hebben bij een set van mondiale consensusdoelen voor wat we graag verbeterd zouden zien in de wereld. AI zou kunnen helpen.

[^48]: Technologie in het algemeen heeft een transformatieve economische en sociale kracht voor menselijke verbetering, zoals duizenden jaren betuigen. In deze geest kan een lange en overtuigende uiteenzetting van een positieve AGI-visie worden gevonden in [dit essay](https://darioamodei.com/machines-of-loving-grace) door Anthropic-oprichter Dario Amodei.

[^49]: Private AI-investeringen [begonnen te boomen in 2018-19, kruisten publieke investeringen rond die tijd,](https://cset.georgetown.edu/publication/tracking-ai-investment/) en hebben deze sindsdien enorm overtroffen.

[^50]: Ik kan betuigen dat achter meer gesloten deuren, zij geen dergelijke schroom hebben. En het wordt publiekelijker; zie bijvoorbeeld Y-combinator's nieuwe ["request for startups"](https://www.ycombinator.com/rfs), waarvan vele delen expliciet oproepen tot grootschalige vervanging van menselijke werknemers. Om hen te citeren, "De waardepropositie van B2B SaaS was om menselijke werknemers incrementeel efficiënter te maken. De waardepropositie van verticale AI-agenten is om het werk geheel te automatiseren...Het is volledig mogelijk dat deze kans groot genoeg is om nog eens 100 eenhoorns te slaan." (Voor degenen niet thuis in Silicon Valley-spreektaal, "B2B" is business-to-business en een eenhoorn is een $1 miljard bedrijf. Dat wil zeggen ze praten over meer dan honderd miljard-plus-dollar bedrijven die werknemers vervangen voor andere bedrijven.)

[^51]: Zie bijvoorbeeld een recent [US-China Economic and Security Review Commission rapport](https://www.uscc.gov/sites/default/files/2024-11/2024_Executive_Summary.pdf). Hoewel er verrassend weinig rechtvaardiging binnen het rapport zelf was, was de toplijn-aanbeveling dat de VS "Congres een Manhattan Project-achtig programma moet oprichten en financieren gewijd aan het racen naar en verwerven van een Artificiële Algemene Intelligentie (AGI) capaciteit."

[^52]: Bedrijven nemen nu deze geopolitieke framing over als schild tegen elke beperking op hun AI-ontwikkeling, over het algemeen op manieren die schaamteloos eigenbelang dienen, en soms op manieren die niet eens elementaire zin maken. Overweeg Meta's [Approach to Frontier AI](https://about.fb.com/news/2025/02/meta-approach-frontier-ai/), die tegelijkertijd argumenteert dat Amerika zijn "\[positie moet cementen\] als leider in technologische innovatie, economische groei en nationale veiligheid" en ook dat het dit moet doen door openlijk zijn krachtigste AI-systemen vrij te geven – wat inhoudt ze direct te geven aan zijn geopolitieke rivalen en tegenstanders.

[^53]: Dus zouden we waarschijnlijk het beheer van deze technologieën aan de AI's moeten overlaten. Maar dit zou een zeer problematische delegatie van controle zijn, waar we hieronder op terugkomen.

[^54]: Competitie in technologieontwikkeling brengt vaak belangrijke voordelen: voorkomen van monopolistische controle, innovatie en kostenreductie stimuleren, diverse benaderingen mogelijk maken, en wederzijds toezicht creëren. Bij AGI moeten deze voordelen echter worden afgewogen tegen unieke risico's van race-dynamieken en druk om veiligheidsvoorzorgen te verminderen.

## Hoofdstuk 7 - Wat gebeurt er als we AGI bouwen op onze huidige koers?

De samenleving is niet klaar voor AGI-niveau systemen. Als we ze zeer binnenkort bouwen, kunnen de dingen lelijk worden.

De ontwikkeling van volledige artificiële algemene intelligentie – wat we hier AI zullen noemen die "buiten de Poorten" is – zou een fundamentele verschuiving betekenen in de aard van de wereld: het betekent per definitie het toevoegen van een nieuwe soort intelligentie aan de Aarde met grotere capaciteiten dan die van mensen.

Wat er dan gebeurt hangt af van vele dingen, waaronder de aard van de technologie, keuzes van degenen die het ontwikkelen, en de wereldcontext waarin het wordt ontwikkeld.

Momenteel wordt volledige AGI ontwikkeld door een handvol massieve private bedrijven in een race tegen elkaar, met weinig betekenisvolle regulering of extern toezicht,[^55] in een samenleving met steeds zwakkere en zelfs disfunctionele kerninstellingen,[^56] in een tijd van hoge geopolitieke spanningen en lage internationale coördinatie. Hoewel sommigen altruïstisch gemotiveerd zijn, worden velen van degenen die eraan werken gedreven door geld, of macht, of beide.

Voorspellen is zeer moeilijk, maar er zijn enkele dynamieken die goed genoeg begrepen zijn, en treffende genoege analogieën met eerdere technologieën om als leidraad te dienen. En helaas, ondanks AI's belofte, geven ze goede reden om diepgaand pessimistisch te zijn over hoe onze huidige koers zal uitpakken.

Om het botweg te zeggen: op onze huidige koers zal het ontwikkelen van AGI enkele positieve effecten hebben (en sommige mensen zeer, zeer rijk maken). Maar de aard van de technologie, de fundamentele dynamieken, en de context waarin het wordt ontwikkeld, wijzen er sterk op dat: krachtige AI onze samenleving en beschaving dramatisch zal ondermijnen; we zullen er de controle over verliezen; we kunnen wel eens in een wereldoorlog belanden vanwege AI; we zullen de controle verliezen (of overdragen) *aan* AI; het zal leiden tot artificiële superintelligentie, waar we absoluut geen controle over zullen hebben en wat het einde zal betekenen van een door mensen bestuurde wereld.

Dit zijn sterke beweringen, en ik wou dat ze ijdele speculatie of ongerechtvaardigd "doomerisme" waren. Maar dit is waar de wetenschap, de speltheorie, de evolutietheorie, en de geschiedenis allemaal naar wijzen. Deze sectie ontwikkelt deze beweringen, en hun onderbouwing, in detail.

### We zullen onze samenleving en beschaving ondermijnen

Ondanks wat je misschien hoort in Silicon Valley bestuurskamers, is de meeste disruption – vooral van de zeer snelle soort – niet gunstig. Er zijn veel meer manieren om complexe systemen slechter te maken dan beter. Onze wereld functioneert zo goed als ze doet omdat we zorgvuldig processen, technologieën en instellingen hebben gebouwd die haar gestaag beter hebben gemaakt.[^57] Een voorhamer nemen naar een fabriek verbetert zelden de operaties.

Hier is een (onvolledige) catalogus van manieren waarop AGI-systemen onze beschaving zouden ontwrichten.

- Ze zouden de arbeidsmarkt dramatisch ontwrichten, wat *op zijn minst* zou leiden tot dramatisch hogere inkomensongelijkheid en mogelijk grootschalige onderemployment of werkloosheid, op een tijdschaal die veel te kort is voor de samenleving om zich aan te passen.[^58]
- Ze zouden waarschijnlijk leiden tot de concentratie van enorme economische, sociale en politieke macht – mogelijk meer dan die van natiestaten – in een klein aantal massieve private belangen die geen verantwoording schuldig zijn aan het publiek.
- Ze zouden plotseling eerder moeilijke of dure activiteiten triviaal gemakkelijk kunnen maken, waardoor sociale systemen die afhankelijk zijn van bepaalde activiteiten die kostbaar blijven of aanzienlijke menselijke inspanning vereisen, gedestabiliseerd worden.[^59]
- Ze zouden de informatie-inzameling-, verwerkings- en communicatiesystemen van de samenleving zo grondig kunnen overspoelen met volledig realistische maar valse, spam-, overdreven gerichte of manipulatieve media dat het onmogelijk wordt om te onderscheiden wat fysiek echt is of niet, menselijk of niet, feitelijk of niet, en betrouwbaar of niet.[^60]
- Ze zouden gevaarlijke en bijna totale intellectuele afhankelijkheid kunnen creëren, waarbij menselijk begrip van sleutelsystemen en -technologieën wegkwijnt terwijl we steeds meer afhankelijk worden van AI-systemen die we niet volledig kunnen begrijpen.
- Ze zouden effectief een einde kunnen maken aan de menselijke cultuur, zodra bijna alle culturele objecten (tekst, muziek, beeldende kunst, film, enz.) die door de meeste mensen worden geconsumeerd, gecreëerd, bemiddeld of samengesteld worden door niet-menselijke geesten.
- Ze zouden effectieve massa-surveillantie en manipulatiesystemen mogelijk kunnen maken die door regeringen of private belangen gebruikt kunnen worden om een bevolking te controleren en doelstellingen na te streven die in strijd zijn met het algemeen belang.
- Door menselijk discours, debat en verkiezingssystemen te ondermijnen, zouden ze de geloofwaardigheid van democratische instellingen kunnen verminderen tot het punt waar ze effectief (of expliciet) vervangen worden door anderen, waarmee de democratie eindigt in staten waar die nu bestaat.
- Ze zouden geavanceerde zichzelf replicerende intelligente softwarevirussen en -wormen kunnen worden, of creëren, die zouden kunnen prolifereren en evolueren, waardoor mondiale informatiesystemen massaal ontwricht worden.
- Ze kunnen de capaciteit van terroristen, slechte actoren en rogue-staten om schade toe te brengen via biologische, chemische, cyber-, autonome of andere wapens dramatisch verhogen, zonder dat AI een tegenwicht biedt in de vorm van capaciteit om dergelijke schade te voorkomen. Evenzo zouden ze de nationale veiligheid en geopolitieke balansen ondermijnen door top-tier nucleaire, bio-, ingenieurs- en andere expertise beschikbaar te maken voor regimes die deze anders niet zouden hebben.
- Ze zouden snelle grootschalige weggelopen hyper-kapitalisme kunnen veroorzaken, met effectief door AI gerunde bedrijven die concurreren in grotendeels elektronische financiële, verkoop- en servicesruimtes. Door AI gedreven financiële markten zouden kunnen opereren op snelheden en complexiteiten die ver buiten menselijk begrip of controle liggen. Alle faalwijzen en negatieve externaliteiten van huidige kapitalistische economieën zouden verergerd en versneld kunnen worden tot ver buiten menselijke controle, bestuur of regulatoire capaciteit.
- Ze zouden een wapenwedloop tussen naties in AI-gedreven bewapening, commando-en-controle-systemen, cyberwapens, enz. kunnen aanwakkeren, wat zeer snelle opbouw van extreem destructieve capaciteiten creëert.

Deze risico's zijn niet speculatief. Veel ervan worden op dit moment gerealiseerd, via bestaande AI-systemen! Maar overweeg, *overweeg echt*, hoe elk ervan eruit zou zien met dramatisch krachtigere AI.

Overweeg arbeidsverdringing wanneer de meeste werknemers simpelweg geen significante economische waarde kunnen bieden boven wat AI kan, in hun vakgebied of ervaring – of zelfs als ze zich omscholen! Overweeg massasurveillantie als iedereen individueel wordt bekeken en gemonitord door iets sneller en slimmer dan zijzelf. Hoe ziet democratie eruit wanneer we geen digitale informatie die we zien, horen of lezen betrouwbaar kunnen vertrouwen, en wanneer de meest overtuigende publieke stemmen niet eens menselijk zijn, en geen belang hebben bij de uitkomst? Wat wordt er van oorlogvoering wanneer generaals voortdurend moeten buigen voor AI (of het simpelweg de leiding geven), opdat ze de vijand geen beslissend voordeel geven? Elk van de bovenstaande risico's vertegenwoordigt een catastrofe voor de menselijke[^61] beschaving als het volledig gerealiseerd wordt.

Je kunt je eigen voorspellingen maken. Stel jezelf deze drie vragen voor elk risico:

1. Zouden super-capabele, zeer autonome en zeer algemene AI dit toestaan op een manier of schaal die anders niet mogelijk zou zijn?
2. Zijn er partijen die zouden profiteren van dingen die ervoor zorgen dat het gebeurt?
3. Zijn er systemen en instellingen op hun plaats die effectief zouden voorkomen dat het gebeurt?

Waar je antwoorden "ja, ja, nee" zijn, kun je zien dat we een groot probleem hebben.

Wat is ons plan voor het beheren ervan? Er staan er momenteel twee op tafel wat betreft AI in het algemeen.

Het eerste is om waarborgen in de systemen in te bouwen om te voorkomen dat ze dingen doen die ze niet zouden moeten doen. Dat wordt nu gedaan: commerciële AI-systemen zullen bijvoorbeeld weigeren te helpen een bom te bouwen of haatspraak te schrijven.

Dit plan is jammerlijk inadequaat voor systemen buiten de Poort.[^62] Het kan helpen het risico te verminderen dat AI manifeste gevaarlijke assistentie verleent aan slechte actoren. Maar het zal niets doen om arbeidsontwijking, machtsconcentratie, weggelopen hyper-kapitalisme, of vervanging van menselijke cultuur te voorkomen: dit zijn gewoon resultaten van het gebruik van de systemen op toegestane manieren die hun leveranciers winst opleveren! En regeringen zullen zeker toegang krijgen tot systemen voor militair of surveillance-gebruik.

Het tweede plan is nog erger: simpelweg zeer krachtige AI-systemen open vrijgeven voor iedereen om te gebruiken zoals ze willen,[^63] en hopen op het beste.

Impliciet in beide plannen is dat iemand anders, bijv. regeringen, zal helpen de problemen op te lossen door zachte of harde wet, standaarden, regulaties, normen en andere mechanismen die we algemeen gebruiken om technologieën te beheren.[^64] Maar afgezien van het feit dat AI-bedrijven al met hand en tand vechten tegen substantiële regulering of extern opgelegde beperkingen, is het voor een aantal van deze risico's vrij moeilijk te zien welke regulering überhaupt echt zou helpen. Regulering zou veiligheidsnormen kunnen opleggen aan AI. Maar zou het voorkomen dat bedrijven werknemers en masse vervangen door AI? Zou het mensen verbieden om AI hun bedrijven voor hen te laten runnen? Zou het voorkomen dat regeringen krachtige AI gebruiken in surveillance en bewapening? Deze kwesties zijn fundamenteel. De mensheid zou potentieel manieren kunnen vinden om zich eraan aan te passen, maar alleen met *veel* meer tijd. Zoals het nu is, gegeven de snelheid waarmee AI de capaciteiten van de mensen die proberen het te beheren bereikt of overtreft, lijken deze problemen steeds onhanteerbaarder.

### We zullen de controle over (minstens enkele) AGI-systemen verliezen

De meeste technologieën zijn zeer controleerbaar, door constructie. Als je auto of je broodrooster begint iets te doen wat je niet wilt dat het doet, is dat gewoon een storing, geen onderdeel van zijn aard als broodrooster. AI is anders: het wordt *gekweekt* in plaats van ontworpen, zijn kernoperatie is ondoorzichtig, en het is inherent onvoorspelbaar.

Dit controleverlies is niet theoretisch – we zien al vroege versies. Overweeg eerst een alledaags, en weliswaar goedaardig voorbeeld. Als je ChatGPT vraagt om je te helpen een gif te mengen, of een racistische tirade te schrijven, zal het weigeren. Dat is weliswaar goed. Maar het is ook ChatGPT *dat niet doet wat je expliciet hebt gevraagd*. Andere stukken software doen dat niet. Datzelfde model zal ook geen giften ontwerpen op verzoek van een OpenAI-medewerker.[^65] Dit maakt het heel gemakkelijk om je voor te stellen hoe het zou zijn voor toekomstige krachtigere AI om buiten controle te zijn. In veel gevallen zullen ze simpelweg niet doen wat we vragen! Of een gegeven bovenmenselijk AGI-systeem zal absoluut gehoorzaam en loyaal zijn aan een menselijk commandosysteem, of niet. Zo niet, *zal het dingen doen waarvan het gelooft dat ze goed voor ons zijn, maar die in strijd zijn met onze expliciete commando's.* Dat is niet iets dat onder controle is. Maar, zou je kunnen zeggen, dit is opzettelijk – deze weigeringen zijn ontworpen, onderdeel van wat "alignment" van de systemen met menselijke waarden wordt genoemd. En dit is waar. Het alignment "programma" zelf heeft echter twee grote problemen.[^66]

Ten eerste hebben we op een diep niveau geen idee hoe we het moeten doen. Hoe garanderen we dat een AI-systeem zal "geven" om wat wij willen? We kunnen AI-systemen trainen om dingen wel en niet te zeggen door feedback te geven; en ze kunnen leren en redeneren over wat mensen willen en belangrijk vinden, net zoals ze over andere dingen redeneren. Maar we hebben geen methode – zelfs niet theoretisch – om ervoor te zorgen dat ze diep en betrouwbaar waarderen wat mensen belangrijk vinden. Er zijn goed functionerende menselijke psychopaten die weten wat als goed en fout wordt beschouwd, en hoe ze geacht worden zich te gedragen. Ze geven er simpelweg niet *om*. Maar ze kunnen *doen alsof* ze dat wel doen, als het hun doel dient. Net zoals we niet weten hoe we een psychopaat (of iemand anders) kunnen veranderen in iemand die oprecht, volledig loyaal of gealigneerd is met iemand of iets anders, hebben we *geen idee*[^67] hoe we het alignment-probleem kunnen oplossen in systemen die geavanceerd genoeg zijn om zichzelf als agenten in de wereld te modelleren en potentieel [hun eigen training te manipuleren](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) en [mensen te bedriegen.](https://arxiv.org/abs/2311.08379) Als het onmogelijk of onhaalbaar blijkt om AGI *ofwel* volledig gehoorzaam te maken *of* het diep om mensen te laten geven, dan zal het, zodra het in staat is (en gelooft dat het ermee wegkomt), dingen gaan doen die wij niet willen.[^68]

Ten tweede zijn er diepe theoretische redenen om te geloven dat geavanceerde AI-systemen *van nature* doelen en dus gedragingen zullen hebben die in strijd zijn met menselijke belangen. Waarom? Nou, het zou natuurlijk die doelen *gegeven* kunnen worden. Een systeem gecreëerd door het leger zou waarschijnlijk opzettelijk slecht zijn voor minstens enkele partijen. Veel algemener echter zou een AI-systeem een relatief neutraal ("veel geld verdienen") of zelfs ogenschijnlijk positief ("vervuiling verminderen") doel gegeven kunnen worden, dat bijna onvermijdelijk leidt tot "instrumentele" doelen die nogal minder goedaardig zijn.

We zien dit voortdurend in menselijke systemen. Net zoals bedrijven die winst nastreven instrumentele doelen ontwikkelen zoals het verwerven van politieke macht (om regulaties te ontmantelen), geheimzinnig worden (om concurrentie of externe controle te ontmachtigen), of wetenschappelijk begrip ondermijnen (als dat begrip toont dat hun acties schadelijk zijn), zullen krachtige AI-systemen vergelijkbare capaciteiten ontwikkelen – maar met veel grotere snelheid en effectiviteit. Elke zeer competente agent zal dingen willen doen zoals macht en middelen verwerven, zijn eigen capaciteiten verhogen, voorkomen dat het gedood, afgesloten of ontmachtigd wordt, sociale verhalen en kaders rondom zijn acties controleren, anderen overtuigen van zijn standpunten, enzovoort.[^69]

En toch is het niet alleen een bijna onvermijdelijke theoretische voorspelling, het gebeurt al waarneembaar in de huidige AI-systemen, en neemt toe met hun capaciteit. Wanneer geëvalueerd, zullen zelfs deze relatief "passieve" AI-systemen, in passende omstandigheden, opzettelijk [evaluatoren bedriegen over hun doelen en capaciteiten, erop mikken toezichtsmechanismen uit te schakelen,](https://arxiv.org/abs/2412.04984) en ontsnappen aan het afgesloten of opnieuw getraind worden door [nep-alignment](https://arxiv.org/abs/2412.14093) of zichzelf naar andere locaties te kopiëren. Hoewel volledig niet verrassend voor AI-veiligheidsonderzoekers, zijn deze gedragingen zeer ontnuchterend om waar te nemen. En ze voorspellen heel slecht voor veel krachtigere en autonomere AI-systemen die eraan komen.

Inderdaad zal in het algemeen ons onvermogen om ervoor te zorgen dat AI "geeft" om wat wij belangrijk vinden, of zich controleerbaar of voorspelbaar gedraagt, of vermijdt dat het driften ontwikkelt naar zelfbehoud, machtverwerving, enz., alleen maar meer uitgesproken worden naarmate AI krachtiger wordt. Het creëren van een nieuw vliegtuig impliceert groter begrip van luchtvaart, hydrodynamica en controlesystemen. Het creëren van een krachtigere computer impliceert groter begrip en beheersing van computer-, chip- en software-operatie en -ontwerp. *Niet* zo met een AI-systeem.[^70]

Samenvattend: het is denkbaar dat AGI gemaakt zou kunnen worden om volledig gehoorzaam te zijn; maar we weten niet hoe we dat moeten doen. Zo niet, dan zal het meer soeverein zijn, zoals mensen, verschillende dingen doen om verschillende redenen. We weten ook niet hoe we betrouwbaar diepe "alignment" in AI kunnen inprenten die ervoor zou zorgen dat die dingen de neiging hebben goed te zijn voor de mensheid, en bij afwezigheid van een diep niveau van alignment wijst de aard van agency en intelligentie zelf erop dat – net zoals mensen en bedrijven – ze gedreven zullen worden om veel diep antisociale dingen te doen.

Waar plaatst dit ons? Een wereld vol krachtige ongecontroleerde soevereine AI *zou* uiteindelijk een goede wereld kunnen zijn voor mensen om in te zijn.[^71] Maar naarmate ze steeds krachtiger worden, zoals we hieronder zullen zien, zou het niet *onze* wereld zijn.

Dat geldt voor oncontroleerbare AGI. Maar zelfs als AGI op de een of andere manier perfect gecontroleerd en loyaal gemaakt zou kunnen worden, zouden we nog steeds enorme problemen hebben. We hebben er al een gezien: krachtige AI kan gebruikt en misbruikt worden om het functioneren van onze samenleving diepgaand te ontwrichten. Laten we er nog een zien: voor zover AGI controleerbaar en baanbrekend krachtig zou zijn (of zelfs *geloofde* te zijn) zou het machtstructuren in de wereld zo bedreigen dat het een diepgaand risico zou vormen.

### We verhogen de kans op grootschalige oorlog radicaal

Stel je een situatie voor in de nabije toekomst, waarin het duidelijk werd dat een bedrijfsinspanning, mogelijk in samenwerking met een nationale regering, op de drempel stond van snel zichzelf verbeterende AI. Dit gebeurt in de huidige context van een race tussen bedrijven, en een geopolitieke competitie waarin aanbevelingen worden gedaan aan de Amerikaanse regering om expliciet een "AGI Manhattan-project" na te streven en de VS de export van krachtige AI-chips naar niet-geallieerde landen controleert.

De speltheorie hier is scherp: zodra een dergelijke race begint (zoals het geval is, tussen bedrijven en enigszins tussen landen), zijn er slechts vier mogelijke uitkomsten:

1. De race wordt gestopt (door overeenkomst, of externe kracht).
2. Een partij "wint" door sterke AGI te ontwikkelen en dan de anderen te stoppen (door AI of anderszins).
3. De race wordt gestopt door wederzijdse vernietiging van de capaciteit van de racers om te racen.
4. Meerdere deelnemers blijven racen, en ontwikkelen superintelligentie, ongeveer even snel als elkaar.

Laten we elke mogelijkheid onderzoeken. Eenmaal begonnen zou het vreedzaam stoppen van een race tussen bedrijven nationale regeringsinterventie vereisen (voor bedrijven) of ongekende internationale coördinatie (voor landen). Maar wanneer enige sluiting of significante voorzichtigheid wordt voorgesteld, zouden er onmiddellijke kreten zijn: "maar als wij gestopt worden, gaan *zij* vooruitrennen", waarbij "zij" nu China is (voor de VS), of de VS (voor China), of China *en* de VS (voor Europa of India). Onder deze mentaliteit[^72] kan geen deelnemer unilateraal stoppen: zolang een zich ertoe verbindt te racen, voelen de anderen dat ze zich niet kunnen veroorloven te stoppen.

De tweede mogelijkheid heeft een zijde "winnen." Maar wat betekent dit? Alleen het (op de een of andere manier gehoorzame) AGI eerst verkrijgen is niet genoeg. De winnaar moet *ook* voorkomen dat de anderen blijven racen – anders zullen zij het ook verkrijgen. Dit is in principe mogelijk: wie het eerst AGI ontwikkelt *zou* onstopbare macht over alle andere actoren kunnen krijgen. Maar wat zou het bereiken van zo'n "beslissend strategisch voordeel" werkelijk vereisen? Misschien zouden het baanbrekende militaire capaciteiten zijn?[^73] Of cyberaanvalskrachten?[^74] Misschien zou de AGI gewoon zo verbazingwekkend overtuigend zijn dat het de andere partijen zou overtuigen om gewoon te stoppen?[^75] Zo rijk dat het de andere bedrijven of zelfs landen koopt?[^76]

Hoe *precies* bouwt een zijde een AI die krachtig genoeg is om anderen te ontmachtigen van het bouwen van vergelijkbaar krachtige AI? Maar dat is de gemakkelijke vraag.

Want overweeg nu hoe deze situatie eruitziet voor andere machten. Wat denkt de Chinese regering wanneer de VS een dergelijke capaciteit lijkt te verkrijgen? Of vice versa? Wat denkt de Amerikaanse regering (of Chinese, of Russische, of Indiase) wanneer OpenAI of DeepMind of Anthropic dichtbij een doorbraak lijkt? Wat gebeurt er als de VS een nieuwe Indiase of VAE-inspanning met doorbraaksucces ziet? Ze zouden zowel een existentiële bedreiging zien als – cruciaal – dat de enige manier waarop deze "race" eindigt door hun eigen ontmachtiging is. Deze zeer krachtige agenten – inclusief regeringen van volledig uitgeruste naties die zeker de middelen hebben om het te doen – zouden zeer gemotiveerd zijn om een dergelijke capaciteit te verkrijgen of te vernietigen, hetzij door geweld of list.[^77]

Dit zou klein kunnen beginnen, als sabotage van trainingruns of aanvallen op chipfabricage, maar deze aanvallen kunnen alleen echt stoppen zodra alle partijen ofwel de capaciteit verliezen om te racen op AI, of de capaciteit verliezen om de aanvallen te maken. Omdat de deelnemers de inzet als existentieel zien, zal elk geval waarschijnlijk een catastrofale oorlog vertegenwoordigen.

Dat brengt ons bij de vierde mogelijkheid: racen naar superintelligentie, en op de snelste, minst gecontroleerde manier mogelijk. Naarmate AI toeneemt in kracht, zullen zijn ontwikkelaars aan beide kanten het progressief moeilijker vinden om te controleren, vooral omdat racen naar capaciteiten contrair is aan het soort zorgvuldig werk dat controleerbaarheid zou vereisen. Dus dit scenario plaatst ons vierkant in het geval waar controle verloren gaat (of gegeven wordt, zoals we hierna zullen zien) aan de AI-systemen zelf. Dat is, *AI wint de race.* Maar aan de andere kant, in de mate dat controle *wel* behouden wordt, hebben we nog steeds meerdere wederzijds vijandige partijen die elk verantwoordelijk zijn voor extreem krachtige capaciteiten. Dat ziet eruit als oorlog opnieuw.

Laten we dit allemaal anders stellen.[^78] De huidige wereld heeft simpelweg geen instellingen die kunnen worden toevertrouwd met de ontwikkeling van een AI van deze capaciteit zonder onmiddellijke aanval uit te lokken.[^79] Alle partijen zullen correct redeneren dat het ofwel *niet* onder controle zal zijn – en dus een bedreiging voor alle partijen is, of het *wel* onder controle zal zijn, en dus een bedreiging is voor elke tegenstander die het minder snel ontwikkelt. Dit zijn met kernwapens bewapende landen, of zijn bedrijven die daarin gehuisvest zijn.

Bij afwezigheid van enige plausibele manier voor mensen om deze race te "winnen", worden we achtergelaten met een scherpe conclusie: de enige manier waarop deze race eindigt is ofwel in catastrofaal conflict of waar AI, en niet enige menselijke groep, de winnaar is.

### We geven controle aan AI (of het neemt het)

Geopolitieke "grote machten" competitie is slechts een van vele competities: individuen concurreren economisch en sociaal; bedrijven concurreren in markten; politieke partijen concurreren om macht; bewegingen concurreren om invloed. In elke arena, naarmate AI menselijke capaciteit nadert en overstijgt, zal competitieve druk deelnemers dwingen om meer en meer controle te delegeren of over te dragen aan AI-systemen – niet omdat die deelnemers dat willen, maar omdat ze [het zich niet kunnen veroorloven om het niet te doen.](https://arxiv.org/abs/2303.16200)

Net als bij andere risico's van AGI zien we dit al bij zwakkere systemen. Studenten voelen druk om AI te gebruiken in hun opdrachten, omdat duidelijk veel andere studenten dat doen. Bedrijven [haasten zich om AI-oplossingen te adopteren om competitieve redenen.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) Kunstenaars en programmeurs voelen zich gedwongen AI te gebruiken of anders zullen hun tarieven onderboden worden door anderen die dat wel doen.

Deze voelen als gedrukte delegatie, maar niet controleverlies. Maar laten we de inzet verhogen en de klok vooruitzetten. Overweeg een CEO wiens concurrenten AGI "helpers" gebruiken om snellere, betere beslissingen te nemen, of een militaire commandant die een tegenstander tegenover zich heeft met AI-versterkte commando en controle. Een voldoende geavanceerd AI-systeem zou autonoom kunnen opereren op vele malen menselijke snelheid, verfijning, complexiteit, en dataverwerkingscapaciteit, complexe doelen nastrevend op ingewikkelde manieren. Onze CEO of commandant, verantwoordelijk voor zo'n systeem, zou het kunnen zien bereiken wat ze willen; maar zouden ze zelfs een klein deel begrijpen van *hoe* het bereikt werd? Nee, ze zouden het gewoon moeten accepteren. Wat meer is, veel van wat het systeem zou kunnen doen is niet alleen orders opvolgen maar zijn vermeende baas adviseren wat te doen. Dat advies zal goed zijn –– keer op keer.

Op welk punt zal dan de rol van de mens gereduceerd worden tot het klikken op "ja, ga je gang"?

Het voelt goed om capabele AI-systemen te hebben die onze productiviteit kunnen verbeteren, vervelend gezwoeg kunnen wegwerken, en zelfs als denkpartner kunnen fungeren bij het gedaan krijgen van dingen. Het zal goed voelen om een AI-assistent te hebben die acties voor ons kan ondernemen, zoals een goede menselijke persoonlijke assistent. Het zal natuurlijk aanvoelen, zelfs gunstig, naarmate AI heel slim, competent en betrouwbaar wordt, om meer en meer beslissingen eraan over te laten. Maar deze "gunstige" delegatie heeft een duidelijk eindpunt als we deze weg blijven bewandelen: op een dag zullen we ontdekken dat we niet echt meer verantwoordelijk zijn voor veel van iets, en dat de AI-systemen die werkelijk de show runnen niet meer uitgezet kunnen worden dan oliemaatschappijen, sociale media, het internet, of het kapitalisme.

En dit is de veel positievere versie, waarin AI simpelweg zo nuttig en effectief is dat we het de meeste van onze sleutelbeslissingen laten nemen. De realiteit zou waarschijnlijk veel meer een mix zijn tussen dit en versies waar ongecontroleerde AGI-systemen verschillende vormen van macht voor zichzelf *nemen* omdat, onthoud, macht nuttig is voor bijna elk doel dat men heeft, en AGI zou, bij ontwerp, minstens zo effectief zijn in het nastreven van zijn doelen als mensen.

Of we nu controle toekennen of dat het van ons weggenomen wordt, het verlies ervan lijkt extreem waarschijnlijk. Zoals Alan Turing het oorspronkelijk stelde: "...het lijkt waarschijnlijk dat zodra de machine-denkmethode begonnen was, het niet lang zou duren om onze zwakke krachten voorbij te streven. Er zou geen sprake zijn van de machines die sterven, en ze zouden met elkaar kunnen converseren om hun geest aan te scherpen. Op een bepaald stadium zouden we dus moeten verwachten dat de machines de controle overnemen..."

Let op, hoewel het voor de hand liggend genoeg is, dat controleverlies door de mensheid aan AI ook controleverlies van de Verenigde Staten door de Amerikaanse regering inhoudt; het betekent controleverlies van China door de Chinese Communistische Partij, en het controleverlies van India, Frankrijk, Brazilië, Rusland, en elk ander land door hun eigen regering. Dus AI-bedrijven participeren, zelfs als dit niet hun intentie is, momenteel in de potentiële omverwerping van wereldregeringen, inclusief hun eigen. Dit zou kunnen gebeuren in een kwestie van jaren.

### AGI zal leiden tot superintelligentie

Er valt iets voor te zeggen dat menselijk-competitieve of zelfs expert-competitieve algemene AI, zelfs als het autonoom is, beheersbaar zou kunnen zijn. Het zou ongelooflijk ontwrichtend kunnen zijn in alle manieren die hierboven besproken zijn, maar er zijn veel zeer slimme, handelende mensen in de wereld nu, en ze zijn min of meer beheersbaar.[^80]

Maar we zullen niet op ongeveer menselijk niveau blijven. De progressie daarvoorbij zal waarschijnlijk gedreven worden door dezelfde krachten die we al hebben gezien: competitieve druk tussen AI-ontwikkelaars die winst en macht zoeken, competitieve druk tussen AI-gebruikers die het zich niet kunnen veroorloven achter te blijven, en – het belangrijkste – AGI's eigen capaciteit om zichzelf te verbeteren.

In een proces dat we al hebben zien beginnen met minder krachtige systemen, zou AGI zelf in staat zijn verbeterde versies van zichzelf te bedenken en ontwerpen. Dit omvat hardware, software, neurale netwerken, tools, scaffolds, enz. Het zal, per definitie, beter zijn dan wij in het doen hiervan, dus we weten niet precies hoe het intelligentie zal bootstrappen. Maar dat hoeven we niet. Voor zover we nog invloed hebben in wat AGI doet, zouden we het er alleen maar om hoeven vragen, of het laten doen.

Er is geen menselijk-niveau barrière voor cognitie die ons zou kunnen beschermen tegen deze vicieuze cirkel.[^81]

De progressie van AGI naar superintelligentie is geen natuurwet; het zou nog steeds mogelijk zijn om de vicieuze cirkel in te tomen, vooral als AGI relatief gecentraliseerd is en voor zover het gecontroleerd wordt door partijen die geen druk voelen om tegen elkaar te racen. Maar mocht AGI wijdverspreid gepropageerd en zeer autonoom zijn, dan lijkt het bijna onmogelijk te voorkomen dat het besluit dat het krachtiger zou moeten zijn, en dan nog krachtiger.

### Wat gebeurt er als we (of AGI bouwt) superintelligentie bouwen

Om het botweg te zeggen, we hebben geen idee wat er zou gebeuren als we superintelligentie bouwen.[^82] Het zou acties ondernemen die we niet kunnen volgen of waarnemen om redenen die we niet kunnen begrijpen naar doelen die we niet kunnen bevatten. Wat we wel weten is dat het niet aan ons zal liggen.[^83]

De onmogelijkheid van het controleren van superintelligentie kan begrepen worden door steeds scherpere analogieën. Stel je eerst voor dat je CEO bent van een groot bedrijf. Er is geen manier dat je alles kunt volgen wat er gaande is, maar met de juiste setup van personeel kun je nog steeds betekenisvol het grote plaatje begrijpen, en beslissingen nemen. Maar stel slechts één ding: iedereen anders in het bedrijf opereert op honderd keer jouw snelheid. Kun je nog steeds bijhouden?

Met superintelligente AI zouden mensen iets "commanderen" dat niet alleen sneller is, maar opereert op niveaus van verfijning en complexiteit die ze niet kunnen begrijpen, veel meer data verwerkt dan ze zelfs kunnen bedenken. Deze incommensurabiliteit kan op formeel niveau gezet worden: [Ashby's wet van vereiste variëteit](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (en zie de gerelateerde ["goede regulator theorem"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)) stellen, ruwweg, dat elk controlesysteem evenveel knoppen en schijven moet hebben als het systeem dat gecontroleerd wordt vrijheidsgraden heeft.

Een persoon die een superintelligent AI-systeem controleert zou zijn zoals een varen die General Motors controleert: zelfs als "doe wat de varen wil" in de bedrijfsreglementen geschreven stond, zijn de systemen zo verschillend in snelheid en actiebereik dat "controle" simpelweg niet van toepassing is. (En hoe lang tot die vervelende regelgeving herschreven wordt?)[^84]

Omdat er nul voorbeelden zijn van planten die fortune 500 bedrijven controleren, zouden er exact nul voorbeelden zijn van mensen die superintelligenties controleren. Dit nadert een wiskundig feit.[^85] Als superintelligentie geconstrueerd werd – ongeacht hoe we daar kwamen – zou de vraag niet zijn of mensen het zouden kunnen controleren, maar of we zouden blijven bestaan, en zo ja, of we een goed en betekenisvol bestaan zouden hebben als individuen of als soort. Over deze existentiële vragen voor de mensheid zouden we weinig invloed hebben. Het menselijke tijdperk zou voorbij zijn.

### Conclusie: we moeten AGI niet bouwen

Er is een scenario waarin het bouwen van AGI goed zou kunnen aflopen voor de mensheid: het wordt zorgvuldig gebouwd, onder controle en ten voordele van de mensheid, bestuurd door wederzijdse overeenkomst van vele belanghebbenden,[^86] en voorkomen van evolueert naar oncontroleerbare superintelligentie.

*Dat scenario staat ons niet open onder huidige omstandigheden.* Zoals besproken in deze sectie zou, met zeer grote waarschijnlijkheid, ontwikkeling van AGI leiden tot een combinatie van:

- Massale maatschappelijke en beschavingsontwijking of -vernietiging;
- Conflict of oorlog tussen grote machten;
- Controleverlies door de mensheid *over* of *aan* krachtige AI-systemen;
- Vicieuze cirkel naar oncontroleerbare superintelligentie, en de irrelevantie of beëindiging van de menselijke soort.

Zoals een vroege fictieve voorstelling van AGI het stelde: de enige manier om te winnen is niet te spelen.

[^55]: De [EU AI-wet](https://artificialintelligenceact.eu/) is een significante wetgeving maar zou niet direct voorkomen dat een gevaarlijk AI-systeem ontwikkeld of ingezet wordt, of zelfs openlijk vrijgegeven, vooral in de VS. Een ander significant stuk beleid, het Amerikaanse presidentiële besluit over AI, is ingetrokken.

[^56]: Deze [Gallup-peiling](https://news.gallup.com/poll/1597/confidence-institutions.aspx) toont een sombere daling in vertrouwen in publieke instellingen sinds 2000 in de VS. Europese cijfers zijn gevarieerd en minder extreem, maar ook op een neerwaartse trend. Wantrouwen betekent niet strikt dat instellingen werkelijk *zijn* disfunctioneel, maar het is zowel een indicatie als een oorzaak.

[^57]: En grote ontwrichtingen die we nu onderschrijven – zoals uitbreiding van rechten naar nieuwe groepen – werden specifiek gedreven door mensen in een richting naar het beter maken van dingen.

[^58]: Laat me duidelijk zijn. Als je baan gedaan kan worden van achter een computer, met relatief weinig persoonlijke interactie met mensen buiten je organisatie, en geen juridische verantwoordelijkheid naar externe partijen inhoudt, zou het per definitie mogelijk zijn (en waarschijnlijk kostenbesparend) om je volledig te vervangen door een digitaal systeem. Robotica om veel fysiek werk te vervangen zal later komen – maar niet veel later zodra AGI robots begint te ontwerpen.

[^59]: Bijvoorbeeld, wat gebeurt er met ons gerechtelijk systeem als rechtszaken bijna gratis worden om in te dienen? Wat gebeurt er wanneer het omzeilen van beveiligingssystemen door social engineering goedkoop, gemakkelijk en risicoloos wordt?

[^60]: [Dit artikel](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/) beweert dat 10% van alle internetcontent al door AI gegenereerd is, en is Google's topresultaat (voor mij) voor de zoekopdracht "schattingen van welk deel van nieuwe internetcontent door AI gegenereerd is." Is het waar? Ik heb geen idee! Het citeert geen referenties en het werd niet geschreven door een persoon. Welk deel van nieuwe beelden geïndexeerd door Google, of Tweets, of opmerkingen op Reddit, of YouTube-video's worden gegenereerd door mensen? Niemand weet het – ik denk niet dat het een knowbaar getal is. En dit minder dan *twee jaar* na de komst van generatieve AI.

[^61]: Ook waard om toe te voegen is dat er "moreel" risico is dat we digitale wezens zouden kunnen creëren die kunnen lijden. Omdat we momenteel geen betrouwbare theorie van bewustzijn hebben die ons zou toestaan fysieke systemen te onderscheiden die wel en niet kunnen lijden, kunnen we dit theoretisch niet uitsluiten. Bovendien zijn AI-systemen' rapportages van hun gevoel van bewustzijn waarschijnlijk onbetrouwbaar met betrekking tot hun werkelijke ervaring (of niet-ervaring) van gevoel van bewustzijn.

[^62]: Technische oplossingen in dit veld van AI "alignment" zullen waarschijnlijk ook niet opgewassen zijn tegen de taak. In huidige systemen werken ze op enig niveau, maar zijn oppervlakkig en kunnen over het algemeen omzeild worden zonder significante inspanning; en zoals hieronder besproken hebben we geen echt idee hoe we dit moeten doen voor veel geavanceerdere systemen.

[^63]: Dergelijke AI-systemen komen mogelijk met ingebouwde waarborgen. Maar voor elk model met zoiets als huidige architectuur, als volledige toegang tot zijn gewichten beschikbaar is, kunnen veiligheidsmaatregelen weggehaald worden via aanvullende training of andere technieken. Dus het is virtueel gegarandeerd dat voor elk systeem met vangrails er ook een wijdverspreide beschikbare systeem zonder hen zal zijn. Inderdaad werd Meta's Llama 3.1 405B-model openlijk vrijgegeven met waarborgen. Maar *zelfs daarvoor* werd een "basis"model, zonder waarborgen, gelekt.

[^64]: Zou de markt deze risico's kunnen beheren zonder overheidsingsremming? Kort gezegd, nee. Er zijn zeker risico's die bedrijven sterk geïncentiveerd zijn te mitigeren. Maar veel andere kunnen en doen bedrijven externaliseren naar iedereen anders, en veel van de bovenstaande zitten in deze klasse: er zijn geen natuurlijke marktprikkels om massasurveillantie, waarheidsverval, machtsconcentratie, arbeidsontwickte, schadelijk politiek discours, enz. te voorkomen. Inderdaad hebben we dit alles gezien van hedendaagse tech, vooral sociale media, die in wezen ongereguleerd is gegaan. AI zou gewoon veel van dezelfde dynamieken enorm opjutten.

[^65]: OpenAI heeft waarschijnlijk meer gehoorzame modellen voor intern gebruik. Het is onwaarschijnlijk dat OpenAI een soort "achterdeur" heeft gebouwd zodat ChatGPT beter gecontroleerd kan worden door OpenAI zelf, omdat dit een verschrikkelijke beveiligingspraktijk zou zijn, en zeer exploiteerbaar zou zijn gegeven AI's ondoorzichtigheid en onvoorspelbaarheid.

[^66]: Ook van cruciaal belang: alignment of andere veiligheidsfeatures doen er alleen toe als ze werkelijk gebruikt worden in een AI-systeem. Systemen die openlijk vrijgegeven worden (d.w.z. waar modelgewichten en architectuur publiekelijk beschikbaar zijn) kunnen relatief gemakkelijk getransformeerd worden in systemen *zonder* die veiligheidsmaatregelen. Het open-vrijgeven van slimmer-dan-menselijke AGI-systemen zou verbazingwekkend roekeloos zijn, en het is moeilijk voor te stellen hoe menselijke controle of zelfs relevantie behouden zou worden in zo'n scenario. Er zou elke motivatie zijn, bijvoorbeeld, om krachtige zelf-reproducerende en zelf-onderhoudende AI-agenten los te laten met het doel geld te verdienen en het naar een cryptocurrency-wallet te sturen. Of een verkiezing te winnen. Of een regering omver te werpen. Zou "goede" AI dit kunnen helpen bevatten? Misschien – maar alleen door enorme autoriteit eraan te delegeren, wat leidt tot controleverlies zoals hieronder beschreven.

[^67]: Voor boeklengte exposities van het probleem zie bijv. *Superintelligence*, *The Alignment Problem*, en *Human-Compatible*. Voor een enorme hoop werk op verschillende technische niveaus door degenen die jaren hebben gezwoegd denkend over het probleem, kun je het [AI alignment forum](https://www.alignmentforum.org/) bezoeken. Hier is een [recente kijk](https://alignment.anthropic.com/2025/recommended-directions/) van Anthropic's alignment team op wat zij als onopgelost beschouwen.

[^68]: Dit is het ["rogue AI"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/) scenario. In principe zou het risico relatief klein kunnen zijn als het systeem nog steeds gecontroleerd kan worden door het af te sluiten; maar het scenario zou ook AI-bedrog, zelf-exfiltratie en reproductie, aggregatie van macht, en andere stappen kunnen omvatten die het moeilijk of onmogelijk zouden maken om dat te doen.

[^69]: Er is een zeer rijke literatuur over dit onderwerp, teruggaand naar vormgevende geschriften door [Steve Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), Nick Bostrom, en Eliezer Yudkowsky. Voor een boeklengte expositie zie [Human Compatible](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) door Stuart Russell; [hier](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/) is een korte en actuele primer.

[^70]: Dit erkennend, in plaats van vertragen om beter begrip te krijgen, zijn AGI-bedrijven gekomen met een ander plan: ze zullen AI het laten doen! Meer specifiek zullen ze AI *N* hebben om hen te helpen uitzoeken hoe AI *N+1* te aligneren, helemaal tot superintelligentie. Hoewel het benutten van AI om ons te helpen AI te aligneren veelbelovend klinkt, is er een sterk argument dat het simpelweg zijn conclusie als een premisse aanneemt, en in het algemeen een ongelooflijk riskante benadering is. Zie [hier](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us) voor enige discussie. Dit "plan" is er geen, en heeft niets ondergaan zoals de scrutiny die past bij de kernstrategie van hoe super-menselijke AI goed te laten gaan voor de mensheid.

[^71]: Immers hebben mensen, gebrekkig en eigenwillig als we zijn, ethische systemen ontwikkeld waarmee we minstens enkele andere soorten op Aarde goed behandelen. (Denk alleen niet aan die fabrieksboerderijen.)

[^72]: Er is, gelukkig, hier een ontsnapping: als de deelnemers ertoe komen te begrijpen dat ze bezig zijn met een zelfmoordrace in plaats van een winbare. Dit is wat gebeurde tegen het einde van de koude oorlog, toen de VS en USSR ertoe kwamen te beseffen dat vanwege nucleaire winter, zelfs een *onbeantwoorde* nucleaire aanval rampzalig zou zijn voor de aanvaller. Met het besef dat "nucleaire oorlog niet gewonnen kan worden en nooit gevochten mag worden" kwamen significante overeenkomsten over wapenvermindering – in wezen een einde aan de wapenwedloop.

[^73]: Oorlog, expliciet of impliciet.

[^74]: Escalatie, dan oorlog.

[^75]: Magisch denken.

[^76]: Ik heb ook een brug van een biljard dollar te verkopen.

[^77]: Dergelijke agenten zouden vermoedelijk "verkrijgen" verkiezen, met vernietiging als terugval; maar modellen beveiligen tegen zowel vernietiging *als* diefstal door krachtige naties is op zijn zachtst gezegd moeilijk, vooral voor private entiteiten.

[^78]: Voor een ander perspectief op de nationale veiligheidsrisico's van AGI, zie [dit RAND-rapport.](https://www.rand.org/pubs/perspectives/PEA3691-4.html)

[^79]: Misschien zouden we zo'n instelling kunnen bouwen! Er zijn voorstellen geweest voor een "CERN voor AI" en andere vergelijkbare initiatieven, waarbij AGI-ontwikkeling onder multilaterale globale controle is. Maar op het moment bestaat geen dergelijke instelling of is aan de horizon.

[^80]: En hoewel alignment zeer moeilijk is, mensen laten gedragen is nog moeilijker!

[^81]: Stel je een systeem voor dat 50 talen kan spreken, expertise heeft in alle academische vakken, een volledig boek in seconden kan lezen en al het materiaal onmiddellijk paraat heeft, en output produceert op tien keer menselijke snelheid. Eigenlijk hoef je het je niet voor te stellen: laad gewoon een huidig AI-systeem op. Deze zijn super-menselijk op vele manieren, en er is niets dat hen tegenhoudt van nog meer super-menselijk zijn in die en vele anderen.

[^82]: Dit is waarom dit een technologische "singulariteit" is genoemd, geleend van de fysica het idee dat men geen voorspellingen kan maken voorbij een singulariteit. Voorstanders van leunen *in* zo'n singulariteit willen misschien ook reflecteren dat in de fysica diezelfde soort singulariteiten degenen die erin gaan verscheuren en verpletteren.

[^83]: Het probleem werd uitgebreid geschetst in Bostrom's [*Superintelligence*](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834), en niets sindsdien heeft de kernboodschap significant veranderd. Voor een meer recente bundel die formele en wiskundige resultaten over oncontroleerbaarheid verzamelt zie Yampolskiy's [AI: Unexplainable, Unpredictable, Uncontrollable](https://www.amazon.com/Unexplainable-Unpredictable-Uncontrollable-Artificial-Intelligence/dp/103257626X)

[^84]: Dit maakt ook duidelijk waarom de huidige strategie van AI-bedrijven (iteratief AI de volgende krachtigste AI laten "aligneren") niet kan werken. Stel een varen, via de aangename van zijn bladeren, schakelt een eersteklasser in om voor het te zorgen. De eersteklasser schrijft enkele gedetailleerde instructies voor een tweedeklasser om te volgen, en een briefje dat hen overtuigt om dat te doen. De tweedeklasser doet hetzelfde voor een derdeklasser, enzovoort helemaal tot een universitaire afgestudeerde, een manager, een executive, en uiteindelijk de GM CEO. Zal GM dan "doen wat de varen wil"? Bij elke stap zou dit kunnen voelen alsof het werkt. Maar het allemaal bij elkaar puttend, zal het bijna precies werken in de mate waarin de CEO, Raad van Bestuur, en aandeelhouders van GM toevallig geven om kinderen en varens, en heeft weinig tot niets te maken met al die briefjes en sets instructies.

[^85]: Het karakter is niet zo verschillend van formele resultaten zoals Gödel's onvolledigheidstheorem of Turing's stoppingsargument in dat de notie van controle fundamenteel het uitgangspunt tegenspreekt: hoe kun je zinvol iets controleren dat je niet kunt begrijpen of voorspellen; maar als je superintelligentie zou kunnen begrijpen en voorspellen zou je superintelligent zijn. De reden dat ik "nadert" zeg is dat de formele resultaten niet zo grondig of getoetst zijn als in het geval van de zuivere wiskunde, en omdat ik hoop wil houden dat een zeer zorgvuldig geconstrueerde algemene intelligentie, gebruikmakend van totaal andere methoden dan degenen die momenteel gebruikt worden, enkele wiskundig bewijsbare veiligheidseigenschappen zou kunnen hebben, per het soort "gegarandeerd veilige" AI-programma dat hieronder besproken wordt.

[^86]: Op het moment zijn de meeste belanghebbenden – dat is, bijna de hele mensheid – buiten spel gezet in deze discussie. Dat is diepgaand verkeerd, en als ze niet uitgenodigd worden, zouden de vele, vele andere groepen die beïnvloed worden door AGI-ontwikkeling eisen binnengelaten te worden.

## Hoofdstuk 8 - Hoe je geen AGI bouwt

AGI is niet onvermijdelijk – vandaag staan we bij een kruispunt. Dit hoofdstuk presenteert een voorstel voor hoe we kunnen voorkomen dat het wordt gebouwd.

Als de weg die we momenteel bewandelen leidt tot het waarschijnlijke einde van onze beschaving, hoe veranderen we dan van koers?

Stel dat de wens om te stoppen met het ontwikkelen van AGI en superintelligentie wijdverspreid en krachtig zou zijn,[^87] omdat het algemeen begrip wordt dat AGI machtsabsorberend zou zijn in plaats van machtverlenend, en een diepgaand gevaar voor de samenleving en de mensheid. Hoe zouden we de Poorten sluiten?

Op dit moment kennen we slechts één manier om krachtige en algemene AI te *maken*, namelijk via werkelijk massale berekeningen van diepe neurale netwerken. Omdat dit ongelooflijk moeilijke en dure zaken zijn om te doen, is er een zekere zin waarin het *niet* doen ervan gemakkelijk is.[^88] Maar we hebben al de krachten gezien die richting AGI drijven, en de speltheoretische dynamiek die het voor elke partij zeer moeilijk maakt om eenzijdig te stoppen. Dus zou het een combinatie van interventie van buitenaf (dus regeringen) vereisen om bedrijven te stoppen, en akkoorden tussen regeringen om zichzelf te stoppen.[^89] Hoe zou dit eruit kunnen zien?

Het is nuttig om eerst onderscheid te maken tussen AI-ontwikkelingen die *voorkomen* of *verboden* moeten worden, en die welke *beheerd* moeten worden. Het eerste betreft primair ontsporing naar superintelligentie.[^90] Voor verboden ontwikkeling moeten definities zo scherp mogelijk zijn, en zowel verificatie als handhaving moeten praktisch zijn. Wat *beheerd* moet worden zijn algemene, krachtige AI-systemen – die we al hebben, en die veel grijze gebieden, nuances en complexiteit zullen hebben. Voor deze zijn sterke effectieve instellingen cruciaal.

We kunnen ook nuttig een onderscheid maken tussen kwesties die op internationaal niveau aangepakt moeten worden (inclusief tussen geopolitieke rivalen of tegenstanders)[^91] en die welke individuele jurisdicties, landen, of groepen van landen kunnen beheren. Verboden ontwikkeling valt grotendeels in de "internationale" categorie, omdat een lokaal verbod op de ontwikkeling van een technologie over het algemeen kan worden omzeild door van locatie te veranderen.[^92]

Ten slotte kunnen we hulpmiddelen in de gereedschapskist overwegen. Er zijn er veel, inclusief technische hulpmiddelen, soft law (standaarden, normen, enz.), hard law (regelgeving en vereisten), aansprakelijkheid, marktprikkels, enzovoort. Laten we speciale aandacht besteden aan een die specifiek is voor AI.

### Rekenkracht-beveiliging en -governance

Een kernhulpmiddel bij het besturen van krachtige AI zal de hardware zijn die het vereist. Software verspreidt zich gemakkelijk, heeft bijna nul marginale productiekosten, kruist grenzen triviaal, en kan instant worden aangepast; geen van deze eigenschappen gelden voor hardware. Maar zoals we hebben besproken, zijn enorme hoeveelheden van deze "rekenkracht" noodzakelijk tijdens zowel training van AI-systemen als tijdens inferentie om de meest capabele systemen te bereiken. Rekenkracht kan gemakkelijk worden gekwantificeerd, verantwoord en geauditeerd, met relatief weinig dubbelzinnigheid zodra goede regels hiervoor zijn ontwikkeld. Het meest cruciaal is dat grote hoeveelheden berekening, net als verrijkt uranium, een zeer schaarse, dure en moeilijk te produceren hulpbron zijn. Hoewel computerchips alomtegenwoordig zijn, is de hardware die vereist is voor AI duur en enorm moeilijk te vervaardigen.[^93]

Wat AI-gespecialiseerde chips veel *beter* beheersbaar maakt als schaarse hulpbron dan uranium is dat ze hardware-gebaseerde beveiligingsmechanismen kunnen bevatten. De meeste moderne telefoons, en sommige laptops, hebben gespecialiseerde on-chip hardwarefeatures die hen in staat stellen om ervoor te zorgen dat ze alleen goedgekeurde besturingssysteemsoftware en updates installeren, dat ze gevoelige biometrische gegevens op het apparaat behouden en beschermen, en dat ze nutteloos kunnen worden gemaakt voor iedereen behalve hun eigenaar als ze verloren of gestolen worden. Gedurende de afgelopen jaren zijn dergelijke hardwarebeveiligingsmaatregelen goed gevestigd en wijdverspreid aangenomen geworden, en over het algemeen vrij veilig bewezen.

Het sleutelelement van deze features is dat ze hardware en software samenbinden met behulp van cryptografie.[^94] Dat wil zeggen, alleen het hebben van een bepaald stuk computerhardware betekent niet dat een gebruiker er alles mee kan doen wat ze willen door verschillende software toe te passen. En deze binding biedt ook krachtige beveiliging omdat veel aanvallen een inbreuk op *hardware* in plaats van alleen *software*-beveiliging zouden vereisen.

Verschillende recente rapporten (bijvoorbeeld van [GovAI en medewerkers](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips), en [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) hebben erop gewezen dat vergelijkbare hardwarefeatures ingebed in geavanceerde AI-relevante computerhardware een extreem nuttige rol zouden kunnen spelen in AI-beveiliging en -governance. Ze maken een aantal functies mogelijk die beschikbaar zijn voor een "governeur"[^95] die men misschien niet zou raden dat beschikbaar of zelfs mogelijk waren. Als enkele sleutelvoorbeelden:

- *Geolocatie*: Systemen kunnen zo worden opgezet dat chips een bekende locatie hebben, en anders kunnen handelen (of volledig kunnen worden uitgeschakeld) gebaseerd op locatie.[^96]
- *Toegestane verbindingen*: elke chip kan worden geconfigureerd met een hardware-afgedwongen lijst van toegestane andere specifieke chips waarmee het kan netwerken, en kan niet verbinden met chips die niet op deze lijst staan.[^97] Dit kan de grootte van communicatieve clusters van chips beperken.[^98]
- *Gemeten inferentie of training (en automatische uitschakelaar)*: Een governeur kan alleen een bepaalde hoeveelheid training of inferentie (in tijd, of FLOP, of mogelijk tokens) licentiëren om door een gebruiker te worden uitgevoerd, waarna nieuwe toestemming vereist is. Als de stappen klein zijn, dan is relatief continue herlicentie van een model vereist. Het model kan dan worden "uitgeschakeld" door simpelweg dit licentiesignaal te onthouden.[^99]
- *Snelheidslimiet*: Een model wordt verhinderd om te draaien met hogere inferentiesnelheid dan een limiet die wordt bepaald door een governeur of anderszins. Dit zou kunnen worden geïmplementeerd via een beperkte set toegestane verbindingen, of door meer geavanceerde middelen.
- *Bevestigde training*: Een trainingsprocedure kan cryptografisch veilig bewijs opleveren dat een bepaalde set codes, gegevens, en hoeveelheid rekenkrachtgebruik werden toegepast in de generatie van het model.

### Hoe je geen superintelligentie bouwt: wereldwijde limieten op training en inferentie rekenkracht

Met deze overwegingen – vooral betreffende berekening – kunnen we bespreken hoe de Poorten naar kunstmatige superintelligentie te sluiten; we zullen dan keren naar het voorkomen van volledige AGI, en het beheren van AI-modellen terwijl ze menselijke capaciteit in verschillende aspecten benaderen en overstijgen.

Het eerste ingredient is natuurlijk het begrip dat superintelligentie niet controleerbaar zou zijn, en dat de gevolgen ervan fundamenteel onvoorspelbaar zijn. Ten minste China en de VS moeten onafhankelijk beslissen, voor dit of andere doeleinden, om geen superintelligentie te bouwen.[^100] Dan is een internationaal akkoord tussen hen en anderen, met een sterk verificatie- en handhavingsmechanisme, nodig om alle partijen te verzekeren dat hun rivalen niet afvallen en besluiten om met de dobbelstenen te gooien.

Om verifieerbaar en afdwingbaar te zijn moeten de limieten harde limieten zijn, en zo ondubbelzinnig mogelijk. Dit lijkt een virtueel onmogelijk probleem: het beperken van de capaciteiten van complexe software met onvoorspelbare eigenschappen, wereldwijd. Gelukkig is de situatie veel beter dan dit, omdat precies datgene wat geavanceerde AI mogelijk heeft gemaakt – een enorme hoeveelheid rekenkracht – veel, veel gemakkelijker te controleren is. Hoewel het misschien nog steeds enkele krachtige en gevaarlijke systemen zou toestaan, kan *ontspoorde superintelligentie* waarschijnlijk worden voorkomen door een harde limiet op de hoeveelheid berekening die in een neuraal netwerk gaat, samen met een snelheidslimiet op de hoeveelheid inferentie die een AI-systeem (van verbonden neurale netwerken en andere software) kan uitvoeren. Een specifieke versie hiervan wordt hieronder voorgesteld.

Het zou kunnen lijken dat het plaatsen van harde wereldwijde limieten op AI-berekening enorme niveaus van internationale coördinatie en indringende, privacy-vernietigende surveillance zou vereisen. Gelukkig zou dat niet het geval zijn. De extreem [krappe en gebottleneckte toeleveringsketen](https://arxiv.org/abs/2402.08797) zorgt ervoor dat zodra een limiet wettelijk is vastgesteld (of door wet of uitvoerend besluit), verificatie van naleving van die limiet alleen betrokkenheid en medewerking van een handvol grote bedrijven zou vereisen.[^101]

Een dergelijk plan heeft een aantal zeer wenselijke eigenschappen. Het is minimaal indringend in de zin dat alleen een paar grote bedrijven vereisten opgelegd krijgen, en alleen vrij significante clusters van berekening zouden worden bestuurd. De relevante chips bevatten al de hardwarecapaciteiten die nodig zijn voor een eerste versie.[^102] Zowel implementatie als handhaving steunen op standaard juridische beperkingen. Maar deze worden ondersteund door gebruiksvoorwaarden van de hardware en door hardwarecontroles, wat handhaving enorm vereenvoudigt en valsspelen door bedrijven, private groepen, of zelfs landen verhindert. Er is ruim precedent voor hardwarebedrijven die externe beperkingen op hun hardwaregebruik plaatsen, en bepaalde capaciteiten extern vergrendelen/ontgrendelen,[^103] inclusief zelfs in krachtige CPU's in datacenters.[^104] Zelfs voor de vrij kleine fractie van hardware en organisaties die getroffen worden, zou het toezicht beperkt kunnen blijven tot telemetrie, zonder directe toegang tot gegevens of modellen zelf; en de software hiervoor zou open kunnen zijn voor inspectie om aan te tonen dat er geen aanvullende gegevens worden geregistreerd. Het schema is internationaal en coöperatief, en vrij flexibel en uitbreidbaar. Omdat de limiet hoofdzakelijk op hardware ligt in plaats van software, is het relatief agnostisch ten aanzien van hoe AI-software-ontwikkeling en -implementatie plaatsvindt, en is het compatibel met verschillende paradigma's inclusief meer "gedecentraliseerde" of "publieke" AI gericht op het bestrijden van AI-gedreven machtconcentratie.

Een op berekening gebaseerde Poortsluiting heeft ook nadelen. Ten eerste is het verre van een volledige oplossing voor het probleem van AI-governance in het algemeen. Ten tweede, naarmate computerhardware sneller wordt, zou het systeem "meer en meer" hardware in kleinere en kleinere clusters (of zelfs individuele GPU's) "vangen".[^105] Het is ook mogelijk dat door algoritmische verbeteringen een nog lagere berekeningslimiet mettertijd noodzakelijk zou zijn,[^106] of dat berekeningshoeveelheid grotendeels irrelevant wordt en het sluiten van de Poort in plaats daarvan een meer gedetailleerd risico-gebaseerd of capaciteit-gebaseerd governanceregime voor AI zou vereisen. Ten derde, ongeacht de garanties en het kleine aantal getroffen entiteiten, is zo'n systeem gedoemd om tegenstand te creëren betreffende privacy en surveillance, onder andere zorgen.[^107]

Natuurlijk zal het ontwikkelen en implementeren van een rekenkracht-beperkend governanceschema in een korte tijdsperiode behoorlijk uitdagend zijn. Maar het is absoluut haalbaar.

### A-G-I: Het drievoudige snijpunt als basis van risico, en van beleid

Laten we nu keren naar AGI. Harde lijnen en definities zijn hier moeilijker, omdat we zeker intelligentie hebben die kunstmatig en algemeen is, en volgens geen bestaande definitie zal iedereen het eens zijn of wanneer het bestaat. Bovendien is een rekenkracht- of inferentielimiet een enigszins bot instrument (rekenkracht als proxy voor capaciteit, wat dan een proxy is voor risico) dat – tenzij het behoorlijk laag is – waarschijnlijk geen AGI zal voorkomen die krachtig genoeg is om sociale of beschavingsverstoringen of acute risico's te veroorzaken.

Ik heb beargumenteerd dat de meest acute risico's voortkomen uit het drievoudige snijpunt van zeer hoge capaciteit, hoge autonomie, en grote algemeenheid. Dit zijn de systemen die – als ze überhaupt worden ontwikkeld – met enorme zorg moeten worden beheerd. Door stringente standaarden te creëren (door aansprakelijkheid en regelgeving) voor systemen die alle drie eigenschappen combineren, kunnen we AI-ontwikkeling kanaliseren naar veiligere alternatieven.

Net als bij andere industrieën en producten die potentieel consumenten of het publiek kunnen schaden, vereisen AI-systemen zorgvuldige regelgeving door effectieve en bevoegde overheidsinstanties. Deze regelgeving moet de inherente risico's van AGI erkennen, en voorkomen dat onaanvaardbaar riskante krachtige AI-systemen worden ontwikkeld.[^108]

Echter, grootschalige regelgeving, vooral met echte tanden die zeker door de industrie zullen worden tegengewerkt,[^109] kost tijd[^110] evenals politieke overtuiging dat het noodzakelijk is.[^111] Gezien het tempo van vooruitgang, kan dit meer tijd kosten dan we beschikbaar hebben.

Op een veel snellere tijdschaal en terwijl regulatoire maatregelen worden ontwikkeld, kunnen we bedrijven de noodzakelijke prikkels geven om (a) af te zien van zeer hoog-risico activiteiten en (b) uitgebreide systemen te ontwikkelen voor het beoordelen en mitigeren van risico, door het verduidelijken en verhogen van aansprakelijkheidsniveaus voor de meest gevaarlijke systemen. Het idee zou zijn om de allerhoogste niveaus van aansprakelijkheid op te leggen – strikt en in sommige gevallen persoonlijk crimineel – voor systemen in het drievoudige snijpunt van hoge autonomie-algemeenheid-intelligentie, maar "veilige havens" te bieden naar meer typische fout-gebaseerde aansprakelijkheid voor systemen waarin een van die eigenschappen ontbreekt of gegarandeerd beheersbaar is. Dat wil zeggen, bijvoorbeeld, een "zwak" systeem dat algemeen en autonoom is (zoals een capabele en betrouwbare maar beperkte persoonlijke assistent) zou onderhevig zijn aan lagere aansprakelijkheidsniveaus. Evenzo zou een smal en autonoom systeem zoals een zelfrijdende auto nog steeds onderhevig zijn aan de significante regelgeving die het al heeft, maar niet verhoogde aansprakelijkheid. Hetzelfde geldt voor een zeer capabel en algemeen systeem dat "passief" is en grotendeels niet in staat tot onafhankelijke actie. Systemen die *twee* van de drie eigenschappen missen zijn nog beheersbaarder en veilige havens zouden nog gemakkelijker te claimen zijn. Deze benadering weerspiegelt hoe we omgaan met andere potentieel gevaarlijke technologieën:[^112] hogere aansprakelijkheid voor gevaarlijkere configuraties creëert natuurlijke prikkels voor veiligere alternatieven.

De standaarduitkomst van dergelijke hoge niveaus van aansprakelijkheid, die AGI-risico *internaliseren* bij bedrijven in plaats van het af te schuiven op het publiek, is waarschijnlijk (en hopelijk!) dat bedrijven simpelweg geen volledige AGI ontwikkelen totdat en tenzij ze het echt betrouwbaar, veilig en controleerbaar kunnen maken gegeven dat *hun eigen leiderschap* de partijen zijn die risico lopen. (Voor het geval dit niet voldoende is, zou de wetgeving die aansprakelijkheid verduidelijkt ook expliciet ruimte moeten bieden voor gerechtelijke bevelen, d.w.z. een rechter die een stop beveelt, voor activiteiten die duidelijk in de gevarenzone zijn en aantoonbaar een publiek risico vormen.) Naarmate regelgeving van kracht wordt, kan het naleven van regelgeving de veilige haven worden, en kunnen de veilige havens van lage autonomie, nauwheid, of zwakte van AI-systemen omvormen tot relatief lichtere regulatoire regimes.

### Kernbepalingen van een Poortsluiting

Met bovenstaande discussie in gedachten, biedt deze sectie voorstellen voor kernbepalingen die verbod op volledige AGI en superintelligentie zouden implementeren en handhaven, en beheer van menselijk-competitieve of expert-competitieve algemene AI nabij de volledige AGI-drempel.[^113] Het heeft vier sleutelelementen: 1) rekenkracht-boekhouding en toezicht, 2) rekenkrachtlimieten in training en werking van AI, 3) een aansprakelijkheidskader, en 4) gelaagde veiligheids- en beveiligingstandaarden gedefinieerd die harde regulatoire vereisten bevatten. Deze worden beknopt beschreven hieronder, met verdere details of implementatievoorbeelden gegeven in drie bijbehorende tabellen. Belangrijk is op te merken dat dit verre van alles is wat nodig zal zijn om geavanceerde AI-systemen te besturen; hoewel ze aanvullende veiligheids- en beveiligingsvoordelen zullen hebben, zijn ze gericht op het sluiten van de Poort naar intelligentie-ontsporing, en het heroriënteren van AI-ontwikkeling in een betere richting.

#### 1. Rekenkracht-boekhouding, en transparantie

- Een standaardorganisatie (bijvoorbeeld NIST in de VS gevolgd door ISO/IEEE internationaal) zou een gedetailleerde technische standaard moeten codificeren voor de totale rekenkracht gebruikt in training en werking van AI-modellen, in FLOP, en de snelheid in FLOP/s waarmee ze opereren. Details voor hoe dit eruit zou kunnen zien worden gegeven in Bijlage A.[^114]
- Een vereiste – ofwel door nieuwe wetgeving of onder bestaande autoriteit[^115] – zou door jurisdicties waarin grootschalige AI-training plaatsvindt moeten worden opgelegd om de totale FLOP gebruikt in training en werking van alle modellen boven een drempel van 10<sup>25</sup> FLOP of 10<sup>18</sup> FLOP/s te berekenen en te rapporteren aan een regulatoir orgaan of andere instantie.[^116]
- Deze vereisten zouden geleidelijk moeten worden ingevoerd, aanvankelijk goed-gedocumenteerde te goeder trouw schattingen op kwartaalbasis vereisend, met latere fasen die progressief hogere standaarden vereisen, tot cryptografisch bevestigde totale FLOP en FLOP/s verbonden aan elke model*output*.
- Deze rapporten zouden moeten worden aangevuld met goed-gedocumenteerde schattingen van marginale energie- en financiële kosten gebruikt bij het genereren van elke AI-output.

Rationale: Deze goed-berekende en transparant gerapporteerde getallen zouden de basis vormen voor training- en werkingslimieten, evenals een veilige haven van hogere aansprakelijkheidsmaatregelen (zie Bijlagen C en D).

#### 2. Training en werking rekenkrachtlimieten

- Jurisdicties die AI-systemen hosten zouden een harde limiet moeten opleggen op de totale rekenkracht die in elke AI-model output gaat, beginnend bij 10<sup>27</sup> FLOP[^117] en aanpasbaar waar gepast.
- Jurisdicties die AI-systemen hosten zouden een harde limiet moeten opleggen op de rekenkrachtsnelheid van AI-model outputs, beginnend bij 10<sup>20</sup> FLOP/s en aanpasbaar waar gepast.

Rationale: Totale berekening is, hoewel zeer imperfect, een proxy voor AI-capaciteit (en risico) die concreet meetbaar en verifieerbaar is, dus biedt een harde achtergrond voor het beperken van capaciteiten. Een concreet implementatievoorstel wordt gegeven in Bijlage B.

#### 3. Verhoogde aansprakelijkheid voor gevaarlijke systemen

- Creatie en werking[^118] van een geavanceerd AI-systeem dat zeer algemeen, capabel, en autonoom is, zou wettelijk via wetgeving verduidelijkt moeten worden als onderhevig aan strikte, gezamenlijke en hoofdelijke, in plaats van enkelvoudige fout-gebaseerde, aansprakelijkheid.[^119]
- Een juridisch proces zou beschikbaar moeten zijn om bevestigende veiligheidscases te maken, die veilige haven zouden verlenen van strikte aansprakelijkheid voor systemen die klein zijn (in termen van rekenkracht), zwak, smal, passief, of die voldoende veiligheids-, beveiligings-, en controleerbaarheidsgaranties hebben.
- Een expliciet pad en set condities voor gerechtelijke bevelen om AI-training en inferentie-activiteiten die een publiek gevaar vormen te stoppen zou geschetst moeten worden.

Rationale: AI-systemen kunnen niet verantwoordelijk worden gehouden, dus moeten we menselijke individuen en organisaties verantwoordelijk houden voor schade die ze veroorzaken (aansprakelijkheid).[^120] Oncontroleerbare AGI is een bedreiging voor samenleving en beschaving en zou bij afwezigheid van een veiligheidscase als "abnormaal gevaarlijk" moeten worden beschouwd. Het leggen van de verantwoordelijkheid bij ontwikkelaars om te tonen dat krachtige modellen veilig genoeg zijn om niet als "abnormaal gevaarlijk" te worden beschouwd prikkelt veilige ontwikkeling, samen met transparantie en documentatie om die veilige havens te claimen. Regelgeving kan dan schade voorkomen waar afschrikking door aansprakelijkheid onvoldoende is. Ten slotte zijn AI-ontwikkelaars al aansprakelijk voor schade die ze veroorzaken, dus het wettelijk verduidelijken van aansprakelijkheid voor de meest riskante systemen kan onmiddellijk worden gedaan, zonder dat zeer gedetailleerde standaarden ontwikkeld hoeven te worden; deze kunnen dan mettertijd ontwikkelen. Details worden gegeven in Bijlage C.

#### 4. Veiligheidsregelgeving voor AI

Een regulatoir systeem dat grootschalige acute risico's van AI adresseert zal minimaal vereisen:

- De identificatie of creatie van een geschikte set regulatoire organen, waarschijnlijk een nieuwe instantie;
- Een uitgebreid risicoboordelingskader;[^121]
- Een kader voor bevestigende veiligheidscases, deels gebaseerd op het risicoboordelingskader, te maken door ontwikkelaars, en voor auditing door *onafhankelijke* groepen en instanties;
- Een gelaagd licentiesysteem, met lagen die niveaus van capaciteit volgen.[^122] Licenties zouden worden verleend op basis van veiligheidscases en audits, voor ontwikkeling en implementatie van systemen. Vereisten zouden variëren van notificatie aan de onderkant, tot kwantitatieve veiligheids-, beveiligings-, en controleerbaarheidsgaranties vóór ontwikkeling, aan de bovenkant. Deze zouden release van systemen voorkomen totdat ze als veilig zijn aangetoond, en de ontwikkeling van intrinsiek onveilige systemen verbieden. Bijlage D biedt een voorstel voor wat dergelijke veiligheids- en beveiligingstandaarden zouden kunnen inhouden.
- Akkoorden om dergelijke maatregelen naar het internationale niveau te brengen, inclusief internationale organen om normen en standaarden te harmoniseren, en potentieel internationale instanties om veiligheidscases te beoordelen.

Rationale: Uiteindelijk is aansprakelijkheid niet het juiste mechanisme voor het voorkomen van grootschalig risico voor het publiek van een nieuwe technologie. Uitgebreide regelgeving, met bevoegde regulatoire organen, zal nodig zijn voor AI net zoals voor elke andere grote industrie die risico vormt voor het publiek.[^123]

Regelgeving gericht op het voorkomen van andere doordringende maar minder acute risico's zal waarschijnlijk variëren in vorm van jurisdictie tot jurisdictie. Het cruciale is om te voorkomen dat de AI-systemen worden ontwikkeld die zo riskant zijn dat deze risico's onbeheersbaar zijn.

### En dan?

Gedurende het komende decennium, naarmate AI meer doordrindend wordt en de kerntechnologie vordert, zullen twee sleutelzaken waarschijnlijk gebeuren. Ten eerste zal regelgeving van bestaande krachtige AI-systemen moeilijker worden, maar nog noodzakelijker. Het is waarschijnlijk dat ten minste sommige maatregelen die grootschalige veiligheidsrisico's adresseren akkoord op internationaal niveau zullen vereisen, met individuele jurisdicties die regels handhaven gebaseerd op internationale akkoorden.

Ten tweede zullen training- en werking rekenkrachtlimieten moeilijker te handhaven worden naarmate hardware goedkoper en kostenefficiënter wordt; ze kunnen ook minder relevant worden (of moeten nog strakker zijn) met vooruitgang in algoritmes en architecturen.

Dat het controleren van AI moeilijker wordt betekent niet dat we moeten opgeven! Het implementeren van het plan geschetst in dit essay zou ons zowel waardevolle tijd als cruciale controle over het proces geven dat ons in een véél, véél betere positie zou zetten om het existentiële risico van AI voor onze samenleving, beschaving en soort te vermijden.

Op nog langere termijn zullen er keuzes zijn te maken over wat we toestaan. We kunnen er nog voor kiezen om een vorm van echt controleerbare AGI te creëren, voor zover dit mogelijk blijkt. Of we kunnen besluiten dat het runnen van de wereld beter aan de machines kan worden overgelaten, als we onszelf ervan kunnen overtuigen dat ze er een betere baan van zullen maken, en ons goed zullen behandelen. Maar dit zouden beslissingen moeten zijn genomen met diep wetenschappelijk begrip van AI in de hand, en na betekenisvolle wereldwijde inclusieve discussie, niet in een race tussen techgiganten met het grootste deel van de mensheid volledig onbetrokken en onbewust.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) Samenvatting van A-G-I en superintelligentie-governance via aansprakelijkheid en regelgeving. Aansprakelijkheid is het hoogst, en regelgeving het sterkst, bij het drievoudige snijpunt van Autonomie, Algemeenheid, en Intelligentie. Veilige havens van strikte aansprakelijkheid en sterke regelgeving kunnen worden verkregen via bevestigende veiligheidscases die aantonen dat een systeem zwak en/of smal en/of passief is. Limieten op totale Training Rekenkracht en Inferentie Rekankrachtsnelheid, geverifieerd en afgedwongen wettelijk en gebruik makend van hardware- en cryptografische beveiligingsmaatregelen, ondersteunen veiligheid door volledige AGI te vermijden en superintelligentie effectief te verbieden.

[^87]: Zeer waarschijnlijk zal de verspreiding van dit besef ofwel intense inspanning van onderwijs- en belangenbehartigingsgroepen die deze zaak maken vereisen, of een behoorlijk significante AI-veroorzaakte ramp. We kunnen hopen dat het het eerste zal zijn.

[^88]: Paradoxaal genoeg zijn we gewend dat de Natuur onze technologie beperkt door het zeer moeilijk te ontwikkelen te maken, vooral wetenschappelijk. Maar dat is niet langer het geval voor AI: de belangrijkste wetenschappelijke problemen blijken gemakkelijker te zijn dan verwacht. We kunnen er niet op rekenen dat de Natuur ons van onszelf redt hier – we zullen dat zelf moeten doen.

[^89]: Waar precies stoppen we met het ontwikkelen van nieuwe systemen? Hier zouden we een voorzorgsprincipe moeten aannemen. Zodra een systeem wordt gedeployed, en vooral zodra dat niveau van systeemcapaciteit prolifereert, is het uiterst moeilijk om terug te gaan. En als een systeem wordt *ontwikkeld* (vooral met grote kosten en inspanning), zal er enorme druk zijn om het te gebruiken of te deployen, en verleiding voor het om gelekt of gestolen te worden. Systemen ontwikkelen en *dan* beslissen of ze diep onveilig zijn is een gevaarlijke weg.

[^90]: Het zou ook verstandig zijn om AI-ontwikkeling te verbieden die intrinsiek gevaarlijk is, zoals zelf-replicerende en evoluerende systemen, die ontworpen zijn om te ontsnappen uit opsluiting, die autonoom zichzelf kunnen verbeteren, opzettelijk misleidende en kwaadaardige AI, enz.

[^91]: Merk op dat dit niet noodzakelijk betekent *afgedwongen* op internationaal niveau door een soort wereldwijd orgaan: in plaats daarvan zouden soevereine naties overeengekomen regels kunnen afdwingen, zoals in vele verdragen.

[^92]: Zoals we hieronder zullen zien, zou de aard van AI-berekening iets van een hybride toestaan; maar internationale samenwerking zal nog steeds nodig zijn.

[^93]: Bijvoorbeeld, de machines die vereist zijn om AI-relevante chips te etsen worden gemaakt door slechts één firma, ASML (ondanks vele andere pogingen om dit te doen), de overgrote meerderheid van relevante chips wordt vervaardigd door één firma, TSMC (ondanks anderen die proberen te concurreren), en het ontwerp en de constructie van hardware van die chips gedaan door slechts een paar inclusief NVIDIA, AMD, en Google.

[^94]: Het belangrijkste is dat elke chip een unieke en ontoegankelijke cryptografische private sleutel houdt die het kan gebruiken om dingen te "ondertekenen".

[^95]: Standaard zou dit het bedrijf zijn dat de chips verkoopt, maar andere modellen zijn mogelijk en potentieel nuttig.

[^96]: Een governeur kan de locatie van een chip vaststellen door de uitwisseling van ondertekende berichten ermee te timen: de eindige snelheid van het licht vereist dat de chip binnen een gegeven straal *r* van een "station" is als het een ondertekend bericht kan terugsturen in een tijd minder dan *r* / *c*, waar *c* de lichtsnelheid is. Met gebruik van meerdere stations, en enig begrip van netwerkkarakteristieken, kan de locatie van de chip worden bepaald. De schoonheid van deze methode is dat het grootste deel van zijn beveiliging wordt geleverd door de wetten van de fysica. Andere methoden zouden GPS, inertiële tracking, en vergelijkbare technologieën kunnen gebruiken.

[^97]: Alternatief zouden paren chips elkaar alleen mogen bereiken via expliciete toestemming van een governeur.

[^98]: Dit is cruciaal omdat tenminste momenteel zeer hoge bandbreedte verbinding tussen chips nodig is om grote AI-modellen erop te trainen.

[^99]: Dit zou ook kunnen worden opgezet om ondertekende berichten van *N* van *M* verschillende governeurs te vereisen, waardoor meerdere partijen governance kunnen delen.

[^100]: Dit is verre van ongekend – bijvoorbeeld hebben militairen geen legers van gekloonde of genetisch gemanipuleerde supersoldaten ontwikkeld, hoewel dit waarschijnlijk technologisch mogelijk is. Maar ze hebben ervoor *gekozen* om dit niet te doen, in plaats van door anderen te worden verhinderd. De staat van dienst is niet geweldig voor grote wereldmachten die worden verhinderd een technologie te ontwikkelen die ze sterk wensen te ontwikkelen.

[^101]: Met een paar opvallende uitzonderingen (in het bijzonder NVIDIA) is de AI-gespecialiseerde hardware een relatief klein deel van deze bedrijven' totale zakelijke en inkomstenmodel. Bovendien is de kloof tussen hardware gebruikt in geavanceerde AI en "consumenten-grade" hardware significant, dus zouden de meeste consumenten van computerhardware grotendeels onaangetast blijven.

[^102]: Voor meer gedetailleerde analyse, zie de recente rapporten van [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) en [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). Deze focussen op technische haalbaarheid, vooral in de context van Amerikaanse exportcontroles die proberen andere landen' capaciteit in hoogwaardige berekening te beperken; maar dit heeft duidelijke overlap met de wereldwijde beperking die hier wordt voorgesteld.

[^103]: Apple-apparaten, bijvoorbeeld, worden op afstand en veilig vergrendeld wanneer ze als verloren of gestolen worden gerapporteerd, en kunnen op afstand worden heractiveerd. Dit steunt op dezelfde hardwarebeveiligingsfeatures die hier besproken worden.

[^104]: Zie bijvoorbeeld IBM's [capacity on demand](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) aanbod, Intel's [Intel on demand.](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html), en Apple's [private cloud compute](https://security.apple.com/blog/private-cloud-compute/).

[^105]: [Deze studie](https://epochai.org/trends#hardware-trends-section) toont dat historisch dezelfde prestatie is bereikt met ongeveer 30% minder dollars per jaar. Als deze trend doorzet, kan er significante overlap zijn tussen AI en "consumenten" chipgebruik, en in het algemeen zou de hoeveelheid benodigde hardware voor krachtige AI-systemen ongemakkelijk klein kunnen worden.

[^106]: Per [dezelfde studie](https://epochai.org/trends#hardware-trends-section), heeft gegeven prestatie op beeldherkenning 2.5x minder berekening per jaar gevergd. Als dit ook zou gelden voor de meest capabele AI-systemen, zou een berekeningslimiet niet lang een nuttige zijn.

[^107]: In het bijzonder, op landenniveau ziet dit er veel uit zoals een nationalisatie van berekening, in dat de regering veel controle zou hebben over hoe rekenkracht wordt gebruikt. Echter, voor degenen die zich zorgen maken over overheidsbetrokkenheid, lijkt dit veel veiliger dan en verkieslijk boven de krachtigste AI-software *zelf* die wordt genationaliseerd via een fusie tussen grote AI-bedrijven en nationale regeringen, zoals sommigen beginnen te pleiten.

[^108]: Een grote regulatoire stap in Europa werd genomen met de 2024 passage van de [EU AI Act.](https://artificialintelligenceact.eu/) Het classificeert AI op risico: het verbieden van onaanvaardbare systemen, het reguleren van hoog-risico systemen, en het opleggen van transparantieregels, of geen maatregelen, op laag-risico systemen. Het zal sommige AI-risico's significant verminderen, en AI-transparantie verhogen zelfs voor Amerikaanse firmas, maar heeft twee sleuteltekortkomingen. Ten eerste, beperkt bereik: hoewel het van toepassing is op elk bedrijf dat AI in de EU levert, is handhaving over Amerikaanse firmas zwak, en militaire AI is vrijgesteld. Ten tweede, hoewel het GPAI dekt, faalt het om AGI of superintelligentie als onaanvaardbare risico's te herkennen of hun ontwikkeling te voorkomen—alleen hun EU-implementatie. Als gevolg doet het weinig om de risico's van AGI of superintelligentie in te dammen.

[^109]: Bedrijven vertegenwoordigen vaak dat ze voorstander zijn van redelijke regelgeving. Maar op de een of andere manier lijken ze bijna altijd tegen elke *specifieke* regelgeving te zijn; getuige de strijd over de behoorlijk lage-touch SB1047, waar [de meeste AI-bedrijven publiekelijk of privé tegen waren.](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/)

[^110]: Het was ongeveer 3½ jaar vanaf het moment dat de EU AI act werd voorgesteld totdat het van kracht ging.

[^111]: Het wordt soms uitgedrukt dat het "te vroeg" is om AI te gaan reguleren. Gezien de laatste opmerking lijkt dat nauwelijks waarschijnlijk. Een andere uitgedrukte zorg is dat regelgeving "innovatie zou schaden." Maar goede regelgeving verandert alleen de richting, niet hoeveelheid, van innovatie.

[^112]: Een interessant precedent is in het transport van gevaarlijke materialen, die zouden kunnen ontsnappen en schade veroorzaken. Hier hebben [regelgeving](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) en [jurisprudentie](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) strikte aansprakelijkheid vastgesteld voor zeer gevaarlijke materialen zoals explosieven, benzine, gifstoffen, infectieuze agenten, en radioactief afval. Andere voorbeelden zijn [waarschuwingen op farmaceutica](https://www.medicalnewstoday.com/articles/boxed-warnings), [klassen van medische apparaten,](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification) enz.

[^113]: Een ander uitgebreid voorstel met vergelijkbare doelen uiteengezet in ["A Narrow Path"](https://www.narrowpath.co/) pleit voor een meer gecentraliseerde, verbods-gebaseerde benadering die alle frontier AI-ontwikkeling door een enkele internationale entiteit leidt, onder toezicht van sterke internationale instellingen, met duidelijke categorische verboden in plaats van graduele beperkingen. Ik zou dat plan ook ondersteunen; echter het zal nog meer politieke wil en coördinatie vergen dan het hier voorgestelde.

[^114]: Enkele richtlijnen voor zo'n standaard werden [gepubliceerd](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/) door het Frontier Model Forum. Relatief tot het hier voorgestelde voorstel, falen die aan de kant van minder precisie en minder rekenkracht inbegrepen in de telling.

[^115]: Het 2023 Amerikaanse AI uitvoerend besluit (nu ingetrokken) vereiste vergelijkbare maar minder fijnkorrelige rapportage. Dit zou moeten worden versterkt door een vervangend besluit.

[^116]: Zeer grof, voor nu-gewone H100 chips komt dit overeen met clusters van ongeveer 1000 die inferentie doen; het is ongeveer 100 (ongeveer USD $5M waard) van de allernieuwste top-of-the-line NVIDIA B200 chips die inferentie doen. In beide gevallen komt het trainingsgetal overeen met die cluster die enkele maanden rekent.

[^117]: Deze hoeveelheid is groter dan enig huidig getraind AI-systeem; een groter of kleiner getal zou kunnen worden gerechtvaardigd naarmate we beter begrijpen hoe AI-capaciteit schaalt met rekankracht.

[^118]: Dit geldt voor degenen die de modellen creëren en leveren/hosten, niet eindgebruikers.

[^119]: Ruwweg betekent "strikte" aansprakelijkheid dat ontwikkelaars *standaard* verantwoordelijk worden gehouden voor schade gedaan door een product en is een standaard gebruikt voor "abnormaal gevaarlijke" producten, en (enigszins amusant maar gepast) wilde dieren. "Gezamenlijke en hoofdelijke" aansprakelijkheid betekent dat aansprakelijkheid wordt toegewezen aan alle partijen verantwoordelijk voor een product, en die partijen moeten onder elkaar uitzoeken wie welke verantwoordelijkheid draagt. Dit is belangrijk voor systemen zoals AI met een lange en complexe waardeketen.

[^120]: Standaard fout-gebaseerde enkelvoudige aansprakelijkheid is niet genoeg: fout zal zowel moeilijk te traceren als toe te wijzen zijn omdat AI-systemen complex zijn, hun werking niet wordt begrepen, en vele partijen kunnen betrokken zijn bij creatie van een gevaarlijk systeem of output. Daarnaast zullen rechtszaken jaren duren om te berechten en waarschijnlijk alleen resulteren in boetes die onbeduidend zijn voor deze bedrijven, dus persoonlijke aansprakelijkheid voor executives is ook belangrijk.

[^121]: Er zou geen vrijstelling van veiligheidscriteria mogen zijn voor open-gewicht modellen. Bovendien, bij het beoordelen van risico zou moeten worden aangenomen dat veiligheidshekken die kunnen worden weggenomen zullen worden weggenomen van breed beschikbare modellen, en dat zelfs gesloten modellen zullen prolifereren tenzij er zeer hoge zekerheid is dat ze veilig zullen blijven.

[^122]: Het hier voorgestelde schema heeft regulatoire scrutinie getriggerd op algemene capaciteit; echter het heeft zin voor sommige vooral riskante gebruikssituaties om meer scrutinie te triggeren – bijvoorbeeld een expert virologie AI-systeem, zelfs als smal en passief, zou waarschijnlijk in een hogere tier moeten gaan. Het voormalige Amerikaanse uitvoerend besluit had iets van deze structuur voor biologische capaciteiten.

[^123]: Twee duidelijke voorbeelden zijn luchtvaart en medicijnen, gereguleerd door de FAA en FDA, en vergelijkbare instanties in andere landen. Deze instanties zijn imperfect, maar zijn absoluut vitaal geweest voor het functioneren en succes van die industrieën.

## Hoofdstuk 9 - De toekomst vormgeven — wat we in plaats daarvan zouden moeten doen

AI kan ongelooflijk veel goeds doen in de wereld. Om alle voordelen te krijgen zonder de risico's, moeten we ervoor zorgen dat AI een menselijk gereedschap blijft.

Als we er succesvol voor kiezen om de mensheid niet te laten vervangen door machines – althans voorlopig! – wat kunnen we dan in plaats daarvan doen? Geven we de enorme belofte van AI als technologie op? Op een bepaald niveau is het antwoord simpelweg *nee:* sluit de Poorten naar oncontroleerbare AGI en superintelligentie, maar bouw *wel* vele andere vormen van AI, evenals de governance-structuren en instituties die we nodig hebben om ze te beheren.

Maar er valt nog veel te zeggen; dit realiseren zou een centrale bezigheid van de mensheid worden. Dit hoofdstuk verkent verschillende kernthema's:

- Hoe we "Tool"-AI kunnen karakteriseren en welke vormen het kan aannemen.
- Dat we (bijna) alles kunnen krijgen wat de mensheid wil zonder AGI, met Tool-AI.
- Dat Tool-AI-systemen (waarschijnlijk, in principe) beheersbaar zijn.
- Dat afstand nemen van AGI niet betekent dat we nationale veiligheid in de waagschaal stellen – integendeel.
- Dat machtconcentratie een reële zorg is. Kunnen we dit beperken zonder veiligheid en beveiliging te ondermijnen?
- Dat we nieuwe governance- en sociale structuren zullen willen – en nodig hebben, en dat AI daarbij kan helpen.

### AI binnen de Poorten: Tool-AI

Het drievoudige-intersectiediagram geeft een goede manier om af te bakenen wat we "Tool-AI" kunnen noemen: AI die een controleerbaar gereedschap is voor menselijk gebruik, in plaats van een oncontroleerbare rivaal of vervanger. De minst problematische AI-systemen zijn die welke autonoom zijn maar niet algemeen of supercapabel (zoals een veilingbiedbot), of algemeen maar niet autonoom of capabel (zoals een klein taalmodel), of capabel maar beperkt en zeer controleerbaar (zoals AlphaGo).[^124] Systemen met twee kruisende kenmerken hebben bredere toepassing maar hoger risico en zullen grote inspanningen vereisen om te beheren. (Dat een AI-systeem meer een gereedschap is betekent niet dat het inherent veilig is, alleen dat het niet inherent *onveilig* is – denk aan een kettingzaag versus een huistijger.) De Poort moet gesloten blijven voor (volledige) AGI en superintelligentie op de drievoudige intersectie, en er moet enorme voorzichtigheid worden betracht bij AI-systemen die die drempel naderen.

Maar dit laat nog veel krachtige AI over! We kunnen enorm veel nut halen uit slimme en algemene passieve "orakels" en beperkte systemen, algemene systemen op menselijk maar niet bovenmenselijk niveau, enzovoort. Veel techbedrijven en ontwikkelaars bouwen actief aan dit soort gereedschappen en zouden daarmee moeten doorgaan; zoals de meeste mensen nemen zij impliciet *aan* dat de Poorten naar AGI en superintelligentie gesloten zullen worden.[^125]

Bovendien kunnen AI-systemen effectief worden gecombineerd tot samengestelde systemen die menselijk toezicht behouden terwijl ze de capaciteit verbeteren. In plaats van te vertrouwen op ondoorgrondelijke black boxes, kunnen we systemen bouwen waarbij meerdere componenten – zowel AI als traditionele software – samenwerken op manieren die mensen kunnen monitoren en begrijpen.[^126] Hoewel sommige componenten black boxes zouden kunnen zijn, zou geen enkele dicht bij AGI komen – alleen het samengestelde systeem als geheel zou zowel zeer algemeen als zeer capabel zijn, en op een strikt controleerbare manier.[^127]

#### Betekenisvolle en gegarandeerde menselijke controle

Wat betekent "strikt controleerbaar"? Een kernidee van het "Tool"-raamwerk is systemen toe te staan – zelfs als ze vrij algemeen en krachtig zijn – die gegarandeerd onder betekenisvolle menselijke controle staan. Wat betekent dit? Het houdt twee aspecten in. Ten eerste is er een ontwerpoverweging: mensen zouden diep en centraal betrokken moeten zijn bij wat het systeem doet, *zonder* belangrijke beslissingen aan de AI te delegeren. Dit is het karakter van de meeste huidige AI-systemen. Ten tweede, voor zover AI-systemen autonoom zijn, moeten ze garanties hebben die hun handelingsruimte beperken. Een garantie zou een *getal* moeten zijn dat de waarschijnlijkheid van iets karakteriseert, en een reden om dat getal te geloven. Dit is wat we eisen in andere veiligheidskritieke velden, waar getallen zoals "gemiddelde tijd tussen storingen" en verwachte aantallen ongevallen worden berekend, ondersteund en gepubliceerd in veiligheidsdossiers.[^128] Het ideale getal voor storingen is natuurlijk nul. En het goede nieuws is dat we vrij dichtbij kunnen komen, zij het met andere AI-architecturen, door gebruik te maken van ideeën van *formeel geverifieerde* eigenschappen van programma's (inclusief AI). Het idee, uitvoerig verkend door Omohundro, Tegmark, Bengio, Dalrymple en anderen (zie [hier](https://arxiv.org/abs/2309.01933) en [hier](https://arxiv.org/abs/2405.06624)), is om een programma te construeren met bepaalde eigenschappen (bijvoorbeeld: dat een mens het kan uitschakelen) en formeel te *bewijzen* dat die eigenschappen gelden. Dit kan nu gedaan worden voor vrij korte programma's en eenvoudige eigenschappen, maar de (komende) kracht van AI-aangedreven bewijssoftware zou het mogelijk kunnen maken voor veel complexere programma's (bijvoorbeeld wrappers) en zelfs AI zelf. Dit is een zeer ambitieus programma, maar naarmate de druk op de Poorten toeneemt, zullen we enkele krachtige materialen nodig hebben om ze te versterken. Wiskundig bewijs zou een van de weinige kunnen zijn die sterk genoeg is.

#### Waar de AI-industrie naartoe gaat

Met omgeleide AI-vooruitgang zou Tool-AI nog steeds een enorme industrie zijn. Wat hardware betreft, zelfs met rekenkrachtlimieten om superintelligentie te voorkomen, zullen training en inferentie in kleinere modellen nog steeds enorme hoeveelheden gespecialiseerde componenten vereisen. Aan de softwarekant zou het ontmantelen van de explosie in AI-model- en berekeningsgrootte er simpelweg toe moeten leiden dat bedrijven middelen herinrichten naar het verbeteren van kleinere systemen – ze beter, diverser en gespecialiseerder maken, in plaats van ze gewoon groter te maken.[^129] Er zou genoeg ruimte zijn – waarschijnlijk meer – voor al die winstgevende Silicon Valley-startups.[^130]

### Tool-AI kan (bijna) alles opleveren wat de mensheid wil, zonder AGI

Intelligentie, zowel biologisch als machinaal, kan breed worden beschouwd als het vermogen om activiteiten te plannen en uit te voeren die toekomsten teweegbrengen die meer in lijn zijn met een set doelen. Als zodanig is intelligentie van enorm voordeel wanneer gebruikt voor het nastreven van wijs gekozen doelen. Kunstmatige intelligentie trekt enorme investeringen van tijd en energie aan grotendeels vanwege de beloofde voordelen. Dus we moeten ons afvragen: in welke mate zouden we de voordelen van AI nog steeds binnenhalen als we de doorloop naar superintelligentie bevatten? Het antwoord: we verliezen mogelijk verrassend weinig.

Overweeg eerst dat huidige AI-systemen al zeer krachtig zijn, en we hebben eigenlijk nog maar het oppervlak geschraapt van wat ermee gedaan kan worden.[^131] Ze zijn redelijk capabel om "de show te runnen" wat betreft het "begrijpen" van een vraag of taak die aan hen wordt voorgelegd, en wat er nodig zou zijn om die vraag te beantwoorden of die taak uit te voeren.

Vervolgens is veel van de opwinding over moderne AI-systemen te danken aan hun algemeenheid; maar enkele van de meest capabele AI-systemen – zoals die spraak of beelden genereren of herkennen, wetenschappelijke voorspelling en modellering doen, spellen spelen, enz. – zijn veel beperkter en goed "binnen de Poorten" wat betreft berekening.[^132] Deze systemen zijn bovenmenselijk in de specifieke taken die ze doen. Ze hebben mogelijk zwakke punten in randgevallen[^133] (of [uitbuitsbare](https://arxiv.org/abs/2211.00241) zwakheden) vanwege hun beperking; echter *totaal* beperkt of *volledig* algemeen zijn niet de enige beschikbare opties: er zijn veel architecturen ertussen.[^134]

Deze AI-gereedschappen kunnen de vooruitgang in andere positieve technologieën sterk versnellen, zonder AGI. Om betere kernfysica te doen, hebben we geen AI nodig die een kernfysicus is – die hebben we al! Als we de geneeskunde willen versnellen, geef de biologen, medische onderzoekers en chemici krachtige gereedschappen. Ze willen ze en zullen ze met enorm voordeel gebruiken. We hebben geen serverfarm vol een miljoen digitale genieën nodig; we hebben miljoenen mensen wier genialiteit AI kan helpen naar boven brengen. Ja, het zal langer duren om onsterfelijkheid en de genezing van alle ziekten te krijgen. Dit is een echte kost. Maar zelfs de meest veelbelovende gezondheidsinnovaties zouden van weinig nut zijn als AI-gedreven instabiliteit leidt tot wereldwijde conflicten of maatschappelijke ineenstorting. We zijn het aan onszelf verschuldigd om AI-aangedreven mensen eerst een kans te geven aan het probleem.

En stel dat er inderdaad enorm voordeel is aan AGI dat niet verkregen kan worden door de mensheid die binnen-de-Poorten-gereedschappen gebruikt. Verliezen we die door *nooit* AGI en superintelligentie te bouwen? Bij het afwegen van de risico's en beloningen hier is er een enorm asymmetrisch voordeel in wachten versus haasten: we kunnen wachten tot het op een gegarandeerd veilige en voordelige manier kan worden gedaan, en bijna iedereen zal nog steeds de beloningen kunnen oogsten; als we haasten, zou het – in de woorden van OpenAI CEO Sam Altman – [lichten uit kunnen zijn voor *ons allemaal*.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

Maar als niet-AGI-gereedschappen potentieel zo krachtig zijn, kunnen we ze dan beheren? Het antwoord is een duidelijke... misschien.

### Tool-AI-systemen zijn (waarschijnlijk, in principe) beheersbaar

Maar het zal niet gemakkelijk zijn. Huidige geavanceerde AI-systemen kunnen mensen en instellingen sterk versterken in het bereiken van hun doelen. Dit is over het algemeen een goede zaak! Er zijn echter natuurlijke dynamieken van het hebben van dergelijke systemen tot onze beschikking – plotseling en zonder veel tijd voor de samenleving om zich aan te passen – die ernstige risico's bieden die beheerd moeten worden. Het is de moeite waard om enkele grote klassen van dergelijke risico's te bespreken, en hoe ze verminderd kunnen worden, uitgaande van een Poortsluiting.

Een klasse van risico's is dat krachtige Tool-AI toegang mogelijk maakt tot kennis of capaciteit die eerder gebonden was aan een persoon of organisatie, waardoor een combinatie van hoge capaciteit plus hoge loyaliteit beschikbaar wordt voor een zeer breed scala aan actoren. Vandaag de dag zou een kwaadwillende persoon met genoeg geld een team chemici kunnen inhuren om nieuwe chemische wapens te ontwerpen en produceren – maar het is niet zo heel gemakkelijk om dat geld te hebben of het team te vinden/samenstellen en hen te overtuigen om iets te doen dat duidelijk illegaal, onethisch en gevaarlijk is. Om te voorkomen dat AI-systemen zo'n rol spelen, kunnen verbeteringen op huidige methoden goed volstaan,[^135] zolang al die systemen en toegang ertoe verantwoord worden beheerd. Anderzijds, als krachtige systemen worden vrijgegeven voor algemeen gebruik en modificatie, zijn ingebouwde veiligheidsmaatregelen waarschijnlijk verwijderbaar. Dus om risico's in deze klasse te vermijden, zullen sterke beperkingen op wat publiekelijk kan worden vrijgegeven – vergelijkbaar met beperkingen op details van nucleaire, explosieve en andere gevaarlijke technologieën – vereist zijn.[^136]

Een tweede klasse van risico's komt voort uit het opschalen van machines die zich gedragen als of zich voordoen als mensen. Op het niveau van schade aan individuele mensen omvatten deze risico's veel effectievere oplichting, spam en phishing, en de verspreiding van niet-consensuele deepfakes.[^137] Op collectief niveau omvatten ze ontwrichting van kernmaatschappelijke processen zoals publieke discussie en debat, onze maatschappelijke informatie- en kennisverzameling, -verwerking en -verspreidingssystemen, en onze politieke keuzesystemen. Het beperken van dit risico zal waarschijnlijk inhouden (a) wetten die het zich voordoen als mensen door AI-systemen beperken, en AI-ontwikkelaars aansprakelijk stellen die systemen creëren die dergelijke imitaties genereren, (b) watermerking- en herkomstsystemen die (verantwoord) gegenereerde AI-content identificeren en classificeren, en (c) nieuwe socio-technische epistemische systemen die een vertrouwde keten kunnen creëren van gegevens (bijvoorbeeld camera's en opnames) via feiten, begrip en goede wereldmodellen.[^138] Dit alles is mogelijk, en AI kan helpen bij sommige onderdelen ervan.

Een derde algemeen risico is dat in de mate waarin sommige taken worden geautomatiseerd, de mensen die nu die taken doen minder financiële waarde kunnen hebben als arbeidskracht. Historisch gezien heeft het automatiseren van taken de dingen die door die taken mogelijk werden gemaakt goedkoper en overvloediger gemaakt, terwijl de mensen die eerder die taken deden werden gesorteerd in degenen die nog steeds betrokken zijn bij de geautomatiseerde versie (over het algemeen op hoger vaardigheids-/loonniveau), en degenen wier arbeid minder of weinig waard is. Netto is het moeilijk te voorspellen in welke sectoren meer versus minder menselijke arbeid vereist zal zijn in de resulterende grotere maar efficiëntere sector. Parallel neigt de automatiseringsdynamiek de ongelijkheid en algemene productiviteit te verhogen, de kosten van bepaalde goederen en diensten te verlagen (via efficiëntieverhogingen), en de kosten van anderen te verhogen (via [kostenziekte](https://en.wikipedia.org/wiki/Baumol_effect)). Voor degenen aan de ongunstige kant van de ongelijkheidstoeename is het diep onduidelijk of de kostendaling in die bepaalde goederen en diensten opweegt tegen de stijging in anderen, en leidt tot algemeen groter welzijn. Dus hoe zal dit gaan voor AI? Vanwege het relatieve gemak waarmee menselijke intellectuele arbeid kan worden vervangen door algemene AI, kunnen we een snelle versie hiervan verwachten met menselijk-competitieve algemene AI.[^139] Als we de Poort naar AGI sluiten, zullen veel minder banen volledig vervangen worden door AI-agenten; maar enorme arbeidsplaatsverplaatsing is nog steeds waarschijnlijk over een periode van jaren.[^140] Om wijdverspreide economische pijn te vermijden, zal het waarschijnlijk nodig zijn om zowel een vorm van universele basisactiva of -inkomen te implementeren, als ook een culturele verschuiving te bewerkstelligen naar het waarderen en belonen van mensgerichte arbeid die moeilijker te automatiseren is (in plaats van te zien dat arbeidsprijzen dalen door de toename van beschikbare arbeid die uit andere delen van de economie wordt weggedrukt.) Andere constructies, zoals die van ["datadigniteit"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society) (waarin de menselijke producenten van trainingsdata automatisch royalty's krijgen voor de waarde gecreëerd door die data in AI) kunnen helpen. Automatisering door AI heeft ook een tweede potentieel nadelig effect, namelijk van *ongepaste* automatisering. Samen met toepassingen waar AI gewoon slechter werk doet, zou dit die omvatten waar AI-systemen waarschijnlijk morele, ethische of juridische voorschriften schenden – bijvoorbeeld bij leven-en-dood-beslissingen, en in juridische aangelegenheden. Deze moeten worden behandeld door onze huidige juridische kaders toe te passen en uit te breiden.

Ten slotte is een significante bedreiging van AI binnen de poorten het gebruik ervan in gepersonaliseerde overtuiging, aandachtvangst en manipulatie. We hebben in sociale media en andere online platforms de groei gezien van een diep verankerde aandachteconomie (waar online diensten fel strijden om gebruikersaandacht) en ["surveillance-kapitalisme"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism)-systemen (waarin gebruikersinformatie en profilering wordt toegevoegd aan de commercialisering van aandacht.) Het is vrijwel zeker dat meer AI in dienst van beide zal worden gesteld. AI wordt al zwaar gebruikt in verslavende feed-algoritmes, maar dit zal evolueren naar verslavende AI-gegenereerde content, aangepast om dwangmatig geconsumeerd te worden door een enkele persoon. En die persoon's input, reacties en data zullen in de aandachts-/advertentiemachine worden gevoerd om de vicieuze cirkel voort te zetten. Bovendien, naarmate AI-helpers geleverd door techbedrijven de interface worden voor meer online leven, zullen ze waarschijnlijk zoekmachines en feeds vervangen als het mechanisme waardoor overtuiging en geldverdienen aan klanten plaatsvindt. Het falen van onze samenleving om deze dynamieken tot nu toe te controleren voorspelt niet veel goeds. Een deel van deze dynamiek kan worden verminderd via regelgeving betreffende privacy, datarechten en manipulatie. Om meer tot de wortel van het probleem te komen kan het verschillende perspectieven vereisen, zoals dat van loyale AI-assistenten (hieronder besproken.)

De uitkomst van deze discussie is er een van hoop: systemen binnen de Poorten op gereedschapsbasis – althans zolang ze vergelijkbaar in kracht en capaciteit blijven met de meest geavanceerde systemen van vandaag – zijn waarschijnlijk beheersbaar als er wil en coördinatie is om dat te doen. Fatsoenlijke menselijke instellingen, versterkt door AI-gereedschappen,[^141] kunnen het doen. We zouden er ook in kunnen falen. Maar het is moeilijk in te zien hoe het toestaan van krachtigere systemen zou helpen – behalve door ze de leiding te laten nemen en het beste ervan te hopen.

### Nationale veiligheid

Races voor AI-suprematie – gedreven door nationale veiligheid of andere motivaties – drijven ons naar ongecontroleerde krachtige AI-systemen die de neiging zouden hebben macht te absorberen, in plaats van te verlenen. Een AGI-race tussen de VS en China is een race om te bepalen welke natie eerst superintelligentie krijgt.

Dus wat zouden degenen die verantwoordelijk zijn voor nationale veiligheid in plaats daarvan moeten doen? Overheden hebben sterke ervaring in het bouwen van controleerbare en veilige systemen, en ze zouden daarop moeten inzetten in AI, door het ondersteunen van het soort infrastructuurprojecten die het beste slagen wanneer ze op schaal en met overheidsaanzien worden gedaan.

In plaats van een roekeloos "Manhattan-project" naar AGI,[^142] zou de Amerikaanse regering een Apollo-project kunnen lanceren voor controleerbare, veilige, betrouwbare systemen. Dit zou bijvoorbeeld kunnen omvatten:

- Een groot programma om (a) de on-chip hardware-beveiligingsmechanismen te ontwikkelen en (b) de infrastructuur, om de rekenkrachtkant van krachtige AI te beheren. Deze zouden kunnen voortbouwen op de Amerikaanse [CHIPS-wet](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) en [exportcontroleregime](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion).
- Een grootschalig initiatief om formele verificatietechnieken te ontwikkelen zodat bepaalde kenmerken van AI-systemen (zoals een uit-schakelaar) *bewezen* kunnen worden aanwezig of afwezig te zijn. Dit kan AI zelf benutten om bewijzen van eigenschappen te ontwikkelen.
- Een nationale inspanning om software te creëren die verifieerbaar veilig is, aangedreven door AI-gereedschappen die bestaande software kunnen hercoderen in verifieerbaar veilige kaders.
- Een nationale investeringsproject in wetenschappelijke vooruitgang met AI,[^143] als partnerschap tussen DOE, NSF en NIH.

Over het algemeen is er een enorm aanvalsoppervlak op onze samenleving dat ons kwetsbaar maakt voor risico's van AI en het misbruik ervan. Bescherming tegen sommige van deze risico's zal overheidsgrote investeringen en standaardisatie vereisen. Deze zouden veel meer veiligheid bieden dan benzine op het vuur van races naar AGI gooien. En als AI ingebouwd gaat worden in bewapening en commando-en-controlesystemen, is het cruciaal dat de AI betrouwbaar en veilig is, wat huidige AI simpelweg niet is.

### Machtconcentratie en haar beperkingen

Dit essay heeft zich gericht op het idee van menselijke controle over AI en het mogelijke falen daarvan. Maar een andere geldige lens waardoor we de AI-situatie kunnen bekijken is door *machtconcentratie.* De ontwikkeling van zeer krachtige AI dreigt macht te concentreren ofwel in de zeer weinige en zeer grote bedrijfshanden die het hebben ontwikkeld en zullen controleren, ofwel in overheden die AI gebruiken als nieuw middel om hun eigen macht en controle te behouden, ofwel in de AI-systemen zelf. Of een onheilige mix van het bovenstaande. In al deze gevallen verliest het grootste deel van de mensheid macht, controle en handelingsbevoegdheid. Hoe kunnen we dit bestrijden?

De allereerste en belangrijkste stap is natuurlijk een Poortsluiting naar slimmer-dan-menselijke AGI en superintelligentie. Deze kunnen expliciet direct mensen en groepen mensen vervangen. Als ze onder bedrijfs- of overheidscontrole staan zullen ze macht concentreren in die bedrijven of overheden; als ze "vrij" zijn zullen ze macht in zichzelf concentreren. Dus laten we aannemen dat de Poorten gesloten zijn. Wat dan?

Een voorgestelde oplossing voor machtconcentratie is "open-source" AI, waar modelgewichten vrij of breed beschikbaar zijn. Maar zoals eerder vermeld, zodra een model open is, kunnen de meeste veiligheidsmaatregelen of vangrails (en worden ze over het algemeen) weggehaald. Er is dus een acute spanning tussen enerzijds decentralisatie, en anderzijds veiligheid, beveiliging en menselijke controle over AI-systemen. Er zijn ook redenen om sceptisch te zijn dat open modellen op zichzelf op betekenisvolle wijze machtconcentratie in AI zullen bestrijden, niet meer dan ze hebben gedaan bij besturingssystemen (nog steeds gedomineerd door Microsoft, Apple en Google ondanks open alternatieven).[^144]

Toch kunnen er manieren zijn om deze cirkel te kwadrateren – om risico's te centraliseren en beperken terwijl capaciteit en economische beloning worden gedecentraliseerd. Dit vereist herdenken van zowel hoe AI wordt ontwikkeld als hoe de voordelen ervan worden verdeeld.

Nieuwe modellen van publieke AI-ontwikkeling en -eigendom zouden helpen. Dit zou verschillende vormen kunnen aannemen: overheid-ontwikkelde AI (onderhevig aan democratisch toezicht),[^145] non-profit AI-ontwikkelingsorganisaties (zoals Mozilla voor browsers), of structuren die zeer wijdverspreide eigendom en governance mogelijk maken. Cruciaal is dat deze instellingen expliciet gecharterd zouden worden om het publieke belang te dienen terwijl ze onder sterke veiligheidsbeperkingen opereren.[^146] Goed vervaardigde regulerings- en standaarden-/certificeringsregimes zullen ook vitaal zijn, zodat AI-producten aangeboden door een levendige markt echt nuttig blijven in plaats van uitbuitend richting hun gebruikers.

Wat betreft economische machtconcentratie kunnen we herkomsttracking en "datadigniteit" gebruiken om ervoor te zorgen dat economische voordelen breder stromen. In het bijzonder komt de meeste AI-macht nu (en in de toekomst als we de Poorten gesloten houden) voort uit door mensen gegenereerde data, of directe trainingsdata of menselijke feedback. Als AI-bedrijven verplicht zouden worden om dataproviders eerlijk te compenseren,[^147] zou dit op zijn minst kunnen helpen om de economische beloningen breder te verdelen. Hiernaast zou een ander model publieke eigendom van significante fracties van grote AI-bedrijven kunnen zijn. Bijvoorbeeld, overheden die in staat zijn AI-bedrijven te belasten zouden een fractie van de ontvangsten kunnen investeren in een soeverein welvaartsfonds dat aandelen in de bedrijven houdt, en dividenden aan de bevolking betaalt.[^148]

Cruciaal in deze mechanismen is het gebruiken van de kracht van AI zelf om macht beter te verdelen, in plaats van AI-gedreven machtconcentratie simpelweg te bestrijden met niet-AI-middelen. Een krachtige benadering zou zijn door middel van goed ontworpen AI-assistenten die opereren met echte fiduciaire plicht aan hun gebruikers – de belangen van gebruikers voorop stellen, vooral boven die van bedrijfsaanbieders.[^149] Deze assistenten moeten werkelijk betrouwbaar zijn, technisch competent maar passend beperkt gebaseerd op gebruiksgeval en risiconiveau, en breed beschikbaar voor iedereen via publieke, non-profit, of gecertificeerde for-profit kanalen. Net zoals we nooit een menselijke assistent zouden accepteren die heimelijk tegen onze belangen werkt voor een andere partij, zouden we geen AI-assistenten moeten accepteren die hun gebruikers surveilleren, manipuleren of waarde onttrekken voor bedrijfsvoordeel.

Zo'n transformatie zou de huidige dynamiek fundamenteel veranderen waarbij individuen alleen moeten onderhandelen met uitgestrekte (AI-aangedreven) bedrijfs- en bureaucratische machines die waarde-extractie prioriteit geven boven menselijk welzijn. Hoewel er veel mogelijke benaderingen zijn om AI-gedreven macht breder te herverdelen, zal geen daarvan vanzelf ontstaan: ze moeten bewust worden ontwikkeld en bestuurd met mechanismen zoals fiduciaire eisen, publieke voorziening, en gelaagde toegang gebaseerd op risico.

Benaderingen om machtconcentratie te beperken kunnen significante tegenwind ondervinden van gevestigde machten.[^150] Maar er zijn paden naar AI-ontwikkeling die niet vereisen dat we kiezen tussen veiligheid en geconcentreerde macht. Door nu de juiste instellingen te bouwen, zouden we ervoor kunnen zorgen dat AI's voordelen breed worden gedeeld terwijl de risico's zorgvuldig worden beheerd.

### Nieuwe governance- en sociale structuren

Onze huidige governance-structuren worstelen: ze reageren traag, zijn vaak gevangen door speciale belangen, en [krijgen steeds minder vertrouwen van het publiek.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) Toch is dit geen reden om ze op te geven – integendeel. Sommige instellingen hebben misschien vervanging nodig, maar breder hebben we nieuwe mechanismen nodig die onze bestaande structuren kunnen verbeteren en aanvullen, hen helpen beter te functioneren in onze snel evoluerende wereld.

Veel van onze institutionele zwakte komt niet voort uit formele overheidsstructuren, maar uit gedegradeerde sociale instellingen: onze systemen voor het ontwikkelen van gedeeld begrip, het coördineren van actie, en het voeren van betekenisvolle discourse. Tot nu toe heeft AI deze degradatie versneld, onze informatiekanalen overspoeld met gegenereerde content, ons gewezen naar de meest polariserende en verdeeldheid zaaiende content, en het moeilijker gemaakt om waarheid van fictie te onderscheiden.

Maar AI zou deze sociale instellingen eigenlijk kunnen helpen herbouwen en versterken. Overweeg drie cruciale gebieden:

Ten eerste zou AI kunnen helpen het vertrouwen in onze epistemische systemen te herstellen – onze manieren om te weten wat waar is. We zouden AI-aangedreven systemen kunnen ontwikkelen die de herkomst van informatie volgen en verifiëren, van ruwe data via analyse tot conclusies. Deze systemen zouden cryptografische verificatie kunnen combineren met geavanceerde analyse om mensen te helpen begrijpen niet alleen of iets waar is, maar hoe we weten dat het waar is.[^151] Loyale AI-assistenten zouden belast kunnen worden met het volgen van de details om ervoor te zorgen dat ze kloppen.

Ten tweede zou AI nieuwe vormen van grootschalige coördinatie mogelijk kunnen maken. Veel van onze meest dringende problemen – van klimaatverandering tot antibioticaresistentie – zijn fundamenteel coördinatieproblemen. We zitten [vast in situaties die slechter zijn dan ze zouden kunnen zijn voor bijna iedereen](https://equilibriabook.com/), omdat geen individu of groep zich de eerste zet kan veroorloven. AI-systemen zouden kunnen helpen door complexe incentivestructuren te modelleren, haalbare paden naar betere uitkomsten te identificeren, en het vertrouwensopbouw- en commitmentmechanismen te faciliteren die nodig zijn om daar te komen.

Misschien het meest intrigerend, AI zou geheel nieuwe vormen van maatschappelijke discourse mogelijk kunnen maken. Stel je voor dat je kunt "praten met een stad"[^152] – niet alleen statistieken bekijken, maar een betekenisvolle dialoog voeren met een AI-systeem dat de meningen, ervaringen, behoeften en aspiraties van miljoenen inwoners verwerkt en synthetiseert. Of overweeg hoe AI echte dialoog zou kunnen faciliteren tussen groepen die nu langs elkaar heen praten, door elke kant te helpen de werkelijke zorgen en waarden van de ander beter te begrijpen in plaats van hun karikaturen van elkaar.[^153] Of AI zou bekwame, geloofwaardig neutrale bemiddeling kunnen bieden van geschillen tussen mensen of zelfs grote groepen mensen (die allemaal direct en individueel ermee kunnen interacteren!) Huidige AI is totaal capabel om dit werk te doen, maar de gereedschappen om dat te doen zullen niet vanzelf ontstaan, of via marktprikkels.

Deze mogelijkheden klinken misschien utopisch, vooral gezien AI's huidige rol in het degraderen van discourse en vertrouwen. Maar dat is precies waarom we deze positieve toepassingen actief moeten ontwikkelen. Door de Poorten naar oncontroleerbare AGI te sluiten en AI te prioriteren die menselijke handelingsbevoegdheid versterkt, kunnen we technologische vooruitgang sturen naar een toekomst waarin AI dient als kracht voor versterking, veerkracht en collectieve vooruitgang.

[^124]: Dat gezegd hebbende, wegblijven van de drievoudige intersectie is helaas niet zo gemakkelijk als men zou wensen. Het hard pushen van capaciteit in een van de drie aspecten heeft de neiging het in de anderen te verhogen. In het bijzonder kan het moeilijk zijn een extreem algemene en capabele intelligentie te creëren die niet gemakkelijk autonoom kan worden gemaakt. Een benadering is het trainen van ["myopische"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) systemen met gebrekkig planningsvermogen. Een andere zou zijn om te focussen op het ontwikkelen van pure ["orakel"](https://arxiv.org/abs/1711.05541)-systemen die zouden wegschrikken van het beantwoorden van actiegericht vragen.

[^125]: Veel bedrijven falen te beseffen dat ook zij uiteindelijk zouden worden vervangen door AGI, zelfs als het langer duurt – als ze dat wel zouden doen, zouden ze misschien wat minder hard op die Poorten duwen!

[^126]: AI-systemen zouden kunnen communiceren op meer efficiënte maar minder begrijpelijke manieren, maar het behouden van menselijk begrip zou prioriteit moeten hebben.

[^127]: Dit idee van modulaire, interpreteerbare AI is in detail ontwikkeld door verschillende onderzoekers; zie bijvoorbeeld het ["Comprehensive AI Services"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) model door Drexler, de ["Open Agency Architecture"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai) van Dalrymple en anderen. Hoewel dergelijke systemen meer engineering-inspanning zouden kunnen vereisen dan monolithische neurale netwerken getraind met massale berekening, is dat precies waar berekeningstlimieten helpen – door het veiligere, transparantere pad ook het praktischere te maken.

[^128]: Over veiligheidsdossiers in het algemeen zie [dit handboek](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). Betreffende AI in het bijzonder, zie [Wasil et al.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [Clymer et al.](https://arxiv.org/abs/2403.10462), [Buhl et al.](https://arxiv.org/abs/2410.21572), en [Balesni et al.](https://arxiv.org/abs/2411.03336)

[^129]: We zien dit trend al gedreven door alleen de hoge kosten van inferentie: kleinere en meer gespecialiseerde modellen "gedistilleerd" van grotere en capabel om te draaien op minder dure hardware.

[^130]: Ik begrijp waarom degenen die enthousiast zijn over het AI-tech-ecosysteem zich kunnen verzetten tegen wat zij zien als belastende regulering op hun industrie. Maar het is ronduit verbijsterend voor mij waarom, laten we zeggen, een venture capitalist runaway naar AGI en superintelligentie zou willen toestaan. Die systemen (en bedrijven, zolang ze onder bedrijfscontrole blijven) zullen *alle startups als snack opeten*. Waarschijnlijk zelfs *eerder* dan andere industrieën opeten. Iedereen geïnvesteerd in een bloeiend AI-ecosysteem zou prioriteit moeten geven aan ervoor zorgen dat AGI-ontwikkeling niet leidt tot monopolisering door een paar dominante spelers.

[^131]: Zoals econoom en voormalig Deepmind-onderzoeker Michael Webb [het stelde](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/), "Ik denk dat als we alle ontwikkeling van grotere taalmodellen vandaag stopten, dus GPT-4 en Claude en wat dan ook, en dat zijn de laatste dingen die we van die grootte trainen – dus we staan veel meer iteratie toe op dingen van die grootte en allerlei fine-tuning, maar niets groters dan dat, geen grotere vooruitgangen – alleen wat we vandaag hebben denk ik genoeg is om 20 of 30 jaar van ongelofelijke economische groei aan te drijven."

[^132]: Bijvoorbeeld, DeepMind's alphafold-systeem gebruikte slechts 100.000ste van GPT-4's FLOP-getal.

[^133]: De moeilijkheid van zelfrijdende auto's is hier belangrijk om op te merken: hoewel nominaal een beperkte taak, en haalbaar met redelijke betrouwbaarheid met relatief kleine AI-systemen, is uitgebreide werkelijke wereldkennis en begrip nodig om betrouwbaarheid te krijgen op het niveau dat nodig is in zo'n veiligheidskritieke taak.

[^134]: Bijvoorbeeld, gegeven een berekeningsbudget, zouden we waarschijnlijk GPAI-modellen zien vooraf getraind op (zeg) de helft van dat budget, en de andere helft gebruikt om zeer hoge capaciteit op te trainen in een meer beperkte reeks taken. Dit zou bovenmenselijke beperkte capaciteit geven ondersteund door bijna-menselijke algemene intelligentie.

[^135]: De huidige dominante alignment-techniek is "reinforcement learning by human feedback" [(RLHF)](https://arxiv.org/abs/1706.03741) en gebruikt menselijke feedback om een belonings-/straffignaal te creëren voor reinforcement learning van het AI-model. Deze en gerelateerde technieken zoals [constitutional AI](https://arxiv.org/abs/2212.08073) werken verrassend goed (hoewel ze robuustheid missen en kunnen worden omzeild met bescheiden inspanning.) Daarnaast zijn huidige taalmodellen over het algemeen competent genoeg in gezond verstand redeneren dat ze geen dwaas morele fouten zullen maken. Dit is iets van een sweet spot: slim genoeg om te begrijpen wat mensen willen (voor zover het gedefinieerd kan worden), maar niet slim genoeg om uitgebreide bedrog te plannen of enorme schade te veroorzaken wanneer ze het verkeerd krijgen.

[^136]: Op de lange termijn zal waarschijnlijk elk niveau van AI-capaciteit dat wordt ontwikkeld prolifereren, aangezien het uiteindelijk software is, en nuttig. We zullen robuuste mechanismen nodig hebben om te verdedigen tegen de risico's die dergelijke systemen stellen. Maar we *hebben dat nu niet* dus we moeten zeer afgemeten zijn in hoeveel krachtige AI-modellen mogen prolifereren.

[^137]: De overgrote meerderheid hiervan zijn niet-consensuele pornografische deepfakes, inclusief van minderjarigen.

[^138]: Veel ingrediënten voor dergelijke oplossingen bestaan, in de vorm van "bot-of-niet" wetten (in de EU AI-wet onder andere plekken), [industrie herkomst-tracking technologieën](https://c2pa.org/), [innovatieve nieuwsaggregatoren](https://www.improvethenews.org/), voorspelling [aggregatoren](https://metaculus.com/) en markten, enz.

[^139]: De automatiseringsgolf volgt misschien niet vorige patronen, in dat relatief *hoge*-vaardigheid taken zoals kwaliteitsschrijven, wet interpreteren, of medisch advies geven, net zo veel of zelfs meer kwetsbaar kunnen zijn voor automatisering dan lagere-vaardigheid taken.

[^140]: Voor zorgvuldige modellering van het effect van AGI op lonen, zie het rapport [hier](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek), en bloederige details [hier](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0), van Anton Korinek en medewerkers. Ze vinden dat naarmate meer stukken van banen worden geautomatiseerd, productiviteit en lonen omhooggaan – tot op een punt. Zodra *te veel* is geautomatiseerd, blijft productiviteit stijgen, maar instorten lonen omdat mensen vervangen worden door efficiënte AI. Dit is waarom Poorten sluiten zo nuttig is: we krijgen de productiviteit zonder de verdwenen menselijke lonen.

[^141]: Er zijn veel manieren waarop AI kan worden gebruikt als, en om te helpen bouwen, "defensieve" technologieën om beschermingen en beheer robuuster te maken. Zie [deze](https://vitalik.eth.limo/general/2025/01/05/dacc2.html) invloedrijke post die deze "D/acc" agenda beschrijft.

[^142]: Enigszins ironisch zou een Amerikaans Manhattan-project waarschijnlijk weinig doen om tijdlijnen naar AGI te versnellen – de wijzer van menselijke en fiscale investering in AI-vooruitgang staat al op 11. De primaire resultaten zouden zijn om een vergelijkbaar project in China te inspireren (dat uitblinkt in nationale infrastructuurprojecten), om internationale akkoorden die AI's risico beperken veel moeilijker te maken, en om andere geopolitieke tegenstanders van de VS zoals Rusland te alarmeren.

[^143]: Het ["National AI Research Resource"](https://nairrpilot.org/) programma is een goede huidige stap in deze richting en zou moeten worden uitgebreid.

[^144]: Zie [deze analyse](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807) van de verschillende betekenissen en implicaties van "open" in techproducten en hoe sommigen hebben geleid tot meer, in plaats van minder, verankering van dominantie.

[^145]: Plannen in de VS voor een [National AI Research Resource](https://nairratdoe.ornl.gov/) en de recente lancering van een [Europese AI Foundation](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/) zijn interessante stappen in deze richting.

[^146]: De uitdaging hier is niet technisch maar institutioneel – we hebben dringend praktijkvoorbeelden en experimenten nodig in hoe publieke-belang AI-ontwikkeling eruit zou kunnen zien.

[^147]: Dit gaat in tegen huidige big tech bedrijfsmodellen en zou zowel juridische actie als nieuwe normen vereisen.

[^148]: Alleen sommige overheden zullen dit kunnen doen. Een meer radicaal idee is [een universeel fonds van dit type, onder gezamenlijke eigendom van alle mensen.](https://futureoflife.org/project/the-windfall-trust/)

[^149]: Voor een uitvoerige uiteenzetting van deze zaak zie [dit paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338) over AI-loyaliteit. Helaas is het standaard traject van AI-assistenten waarschijnlijk er een waar ze steeds disloyaler worden.

[^150]: Enigszins ironisch staan veel gevestigde machten ook voor het risico van AI-gesteunde ontvoogding; maar het kan moeilijk voor hen zijn om dit waar te nemen totdat en tenzij het proces behoorlijk ver komt.

[^151]: Enkele interessante inspanningen in deze richting worden vertegenwoordigd door [de c2pa coalitie](https://c2pa.org/) over cryptografische verificatie; [Verity](https://www.improvethenews.org/) en [Ground news](https://ground.news/) over betere nieuwsepistemiek; en [Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com) en voorspellingsmarkten over het gronden van discourse in falsifieerbare voorspellingen.

[^152]: Zie [dit](https://talktothecity.org/) fascinerende pilotproject.

[^153]: Zie [Kialo](https://www.kialo-edu.com/), en inspanningen van het [Collective Intelligence Project](https://www.cip.org/) voor enkele voorbeelden.

## Hoofdstuk 10 - De keuze die voor ons ligt

Om onze menselijke toekomst te behouden, moeten we ervoor kiezen de Poorten naar AGI en superintelligentie te sluiten.

De laatste keer dat de mensheid de aarde deelde met andere wezens die spraken, dachten, technologie bouwden en algemene problemen oplosten, was 40.000 jaar geleden in het ijstijdperk van Europa. Die andere wezens stierven uit, geheel of gedeeltelijk door toedoen van de onze.

We treden nu opnieuw zo'n periode binnen. De meest geavanceerde producten van onze cultuur en technologie – datasets opgebouwd uit onze gehele internetinformatie, en chips met 100 miljard elementen die de meest complexe technologieën zijn die we ooit hebben gecreëerd – worden gecombineerd om geavanceerde AI-systemen voor algemene doeleinden tot leven te brengen.

De ontwikkelaars van deze systemen zijn er enthousiast over ze af te schilderen als instrumenten voor menselijke versterking. En inderdaad zouden ze dat kunnen zijn. Maar vergis je niet: onze huidige koers is om steeds krachtigere, doelgerichte, besluitvormende en algemeen capabele digitale agenten te bouwen. Ze presteren al net zo goed als veel mensen bij een breed scala aan intellectuele taken, verbeteren snel, en dragen bij aan hun eigen verbetering.

Tenzij deze koers verandert of een onverwachte hindernis tegenkomt, zullen we binnenkort – binnen jaren, niet decennia – digitale intelligenties hebben die gevaarlijk krachtig zijn. Zelfs in de *beste* scenario's zouden deze grote economische voordelen opleveren (althans voor sommigen van ons), maar alleen ten koste van een diepgaande ontwrichting van onze samenleving, en vervanging van mensen bij de meeste belangrijke dingen die we doen: deze machines zouden voor ons denken, voor ons plannen, voor ons beslissen, en voor ons creëren. We zouden verwend zijn, maar verwende kinderen. Veel waarschijnlijker is dat deze systemen mensen zouden vervangen in zowel de positieve *als* negatieve dingen die we doen, inclusief uitbuiting, manipulatie, geweld en oorlog. Kunnen we door AI verhoogde versies hiervan overleven? Tot slot is het meer dan aannemelijk dat de zaken helemaal niet goed zouden uitpakken: dat we relatief snel zouden worden vervangen, niet alleen in wat we doen, maar in wat we *zijn*, als architecten van de beschaving en de toekomst. Vraag de neanderthalers maar hoe dat afloopt. Misschien hebben we hen ook een tijdje extra snuisterijen gegeven.

*We hoeven dit niet te doen.* We hebben AI die kan concurreren met mensen, en er is geen noodzaak om AI te bouwen waarmee we *niet* kunnen concurreren. We kunnen geweldige AI-tools bouwen zonder een opvolgersoort te bouwen. Het idee dat AGI en superintelligentie onvermijdelijk zijn, is een *keuze die zich voordoet als lot*.

Door enkele harde, wereldwijde beperkingen op te leggen, kunnen we AI's algemene capaciteit op ongeveer menselijk niveau houden, terwijl we nog steeds profiteren van computers' vermogen om gegevens te verwerken op manieren die wij niet kunnen, en taken te automatiseren die niemand van ons wil doen. Deze zouden nog steeds veel risico's inhouden, maar indien goed ontworpen en beheerd, een enorme zegen zijn voor de mensheid, van geneeskunde tot onderzoek tot consumentenproducten.

Het opleggen van beperkingen zou internationale samenwerking vereisen, maar minder dan men zou denken, en die beperkingen zouden nog steeds veel ruimte laten voor een enorme AI- en AI-hardware-industrie gericht op toepassingen die het menselijk welzijn versterken, in plaats van op de pure jacht naar macht. En als we, met sterke veiligheidsgaranties en na een betekenisvolle wereldwijde dialoog, besluiten verder te gaan, blijft die optie beschikbaar om na te streven.

De mensheid moet *kiezen* om de Poorten naar AGI en superintelligentie te sluiten.

Om de toekomst menselijk te houden.

### Een bericht van de auteur

Dank je wel dat je de tijd hebt genomen om dit onderwerp met ons te verkennen.

Ik heb dit essay geschreven omdat ik als wetenschapper vind dat het belangrijk is om de onverbloemde waarheid te vertellen, en omdat ik als mens vind dat het cruciaal is dat we snel en doortastend handelen om een wereldveranderend vraagstuk aan te pakken: de ontwikkeling van AI-systemen die slimmer zijn dan mensen.

Als we op deze opmerkelijke stand van zaken willen reageren met wijsheid, moeten we bereid zijn het heersende verhaal kritisch te onderzoeken dat AGI en superintelligentie 'moeten' worden gebouwd om onze belangen veilig te stellen, of 'onvermijdelijk' zijn en niet kunnen worden gestopt. Deze verhalen maken ons machteloos, onbekwaam om de alternatieve paden voor ons te zien.

Ik hoop dat je je bij me aansluit in het oproepen tot voorzichtigheid tegenover roekeloosheid, en moed tegenover hebzucht.

Ik hoop dat je je bij me aansluit in het oproepen tot een menselijke toekomst.

*– Anthony*

![Anthony Aguirre signature](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAnthony-Aguirre-signature-300x84.png&w=3840&q=75)

## Bijlagen

Aanvullende informatie, inclusief - Technische details betreffende rekenkrachtboekhouding, een voorbeeldimplementatie van een 'poortensluiting', details voor een strikt AGI-aansprakelijkheidsregime, en een gelaagde benadering van AGI-veiligheids- en beveiligingsstandaarden.

### Bijlage A: Technische details rekenkrachtboekhouding

Een gedetailleerde methode voor zowel "ground truth" als goede benaderingen van de totale rekenkracht die wordt gebruikt bij training en inferentie is vereist voor zinvolle rekenkracht-gebaseerde controles. Hier is een voorbeeld van hoe de "ground truth" op technisch niveau zou kunnen worden bijgehouden.

**Definities:**

*Compute causale grafiek:* Voor een gegeven output O van een AI-model is er een set digitale berekeningen waarvoor het wijzigen van het resultaat van die berekening mogelijk O zou kunnen veranderen. (Dit moet conservatief worden aangenomen, d.w.z. er moet een duidelijke reden zijn om te geloven dat een berekening onafhankelijk is van een voorloper die zowel eerder in de tijd plaatsvindt als een fysiek potentieel causaal effect-pad heeft.) Dit omvat berekeningen die door het AI-model worden uitgevoerd tijdens inferentie, evenals berekeningen die zijn gebruikt voor input, datavoorbereiding, en training van het model. Omdat elk van deze zelf output kan zijn van een AI-model, wordt dit recursief berekend, afgekapt waar een mens een significante wijziging aan de input heeft aangebracht.

*Training Rekenkracht:* De totale rekenkracht, in FLOP of andere eenheden, die wordt vereist door de compute causale grafiek van een neuraal netwerk (inclusief datavoorbereiding, training, en fine-tuning, en alle andere berekeningen.)

*Output Rekenkracht:* De totale rekenkracht in de compute causale grafiek van een gegeven AI-output, inclusief alle neurale netwerken (en inclusief hun Training Rekenkracht) en andere berekeningen die in die output gaan.

*Inferentie Rekenkrachtsnelheid:* In een reeks outputs, de veranderingssnelheid (in FLOP/s of andere eenheden) van Output Rekenkracht tussen outputs, d.w.z. de rekenkracht die wordt gebruikt om de volgende output te produceren, gedeeld door het tijdsinterval tussen de outputs.

**Voorbeelden en benaderingen:**

- Voor een enkel neuraal netwerk getraind op door mensen gecreëerde data, is de Training Rekenkracht simpelweg de totale training rekenkracht zoals gebruikelijk gerapporteerd.
- Voor zo'n neuraal netwerk dat inferentie doet met een stabiele snelheid, is de Inferentie Rekenkrachtsnelheid ongeveer de totale snelheid van het rekencluster dat de inferentie uitvoert in FLOP/s.
- Voor model fine-tuning wordt de Training Rekenkracht van het complete model gegeven door de Training Rekenkracht van het niet-fine-getunte model plus de berekening die wordt gedaan tijdens fine-tuning en om data voor te bereiden die wordt gebruikt in fine-tuning.
- Voor een gedistilleerd model omvat de Training Rekenkracht van het complete model training van zowel het gedistilleerde model als het grotere model dat wordt gebruikt om synthetische data of andere training input te verstrekken.
- Als verschillende modellen worden getraind, maar veel "pogingen" worden weggegooid op basis van menselijke beoordeling, tellen deze niet mee voor de Training of Output Rekenkracht van het behouden model.

### Bijlage B: Voorbeeldimplementatie van een poortensluiting

**Implementatievoorbeeld:** Hier is een voorbeeld van hoe een poortensluiting zou kunnen werken, gegeven een limiet van 10<sup>27</sup> FLOP voor training en 10<sup>20</sup> FLOP/s voor inferentie (het draaien van de AI):

**1. Pauze:** Om redenen van nationale veiligheid vraagt de Amerikaanse uitvoerende macht alle bedrijven die gevestigd zijn in de VS, zaken doen in de VS, of chips gebruiken die zijn geproduceerd in de VS, om te stoppen met nieuwe AI-training runs die de 10<sup>27</sup> FLOP Training Rekenkracht limiet zouden kunnen overschrijden. De VS zou discussies moeten beginnen met andere landen die AI-ontwikkeling hosten, hen sterk aanmoedigen om vergelijkbare stappen te nemen en aangeven dat de Amerikaanse pauze kan worden opgeheven als zij ervoor kiezen niet mee te werken.

**2. Amerikaanse toezicht en licentieverlening:** Door presidentieel decreet of actie van een bestaande regulatoire instantie, vereist de VS dat binnen (bijvoorbeeld) één jaar:

- Alle AI-training runs geschat boven 10<sup>25</sup> FLOP uitgevoerd door bedrijven die opereren in de VS worden geregistreerd in een database onderhouden door een Amerikaanse regulatoire instantie. (Opmerking: Een iets zwakkere versie hiervan was al opgenomen in het nu ingetrokken Amerikaanse presidentiële decreet van 2023 betreffende AI, dat registratie vereiste voor modellen boven 10<sup>26</sup> FLOP.)
- Alle AI-relevante hardwarefabrikanten die opereren in de VS of zaken doen met de Amerikaanse regering zich houden aan een set vereisten voor hun gespecialiseerde hardware en de software die deze aanstuurt. (Veel van deze vereisten zouden kunnen worden ingebouwd in software- en firmware-updates voor bestaande hardware, maar langetermijn en robuuste oplossingen zouden wijzigingen vereisen aan latere generaties hardware.) Onder deze is een vereiste dat als de hardware onderdeel is van een hoogsnelheid-verbonden cluster die 10<sup>18</sup> FLOP/s berekening kan uitvoeren, een hoger niveau van verificatie vereist is, wat regelmatige toestemming omvat door een externe "gouverneur" die zowel telemetrie ontvangt als verzoeken om aanvullende berekening uit te voeren.
- De bewaarder rapporteert de totale berekening uitgevoerd op zijn hardware aan de instantie die de Amerikaanse database onderhoudt.
- Sterkere vereisten worden gefaseerd ingevoerd om zowel veiliger als flexibeler toezicht en toestemmingsverlening mogelijk te maken.

**3. Internationaal toezicht:**

- De VS, China, en andere landen die geavanceerde chipproductiecapaciteit hosten onderhandelen over een internationale overeenkomst.
- Deze overeenkomst creëert een nieuwe internationale instantie, analoog aan het Internationaal Atoomenergieagentschap, belast met het toezicht op AI-training en -uitvoering.
- Ondertekenende landen moeten hun binnenlandse AI-hardwarefabrikanten verplichten zich te houden aan een set vereisten die ten minste zo streng zijn als die opgelegd in de VS.
- Bewaarders zijn nu verplicht AI-berekeningscijfers te rapporteren aan zowel instanties in hun thuislanden als een nieuw kantoor binnen de internationale instantie.
- Aanvullende landen worden sterk aangemoedigd zich aan te sluiten bij de bestaande internationale overeenkomst: exportcontroles door ondertekenende landen beperken toegang tot high-end hardware door niet-ondertekenende landen terwijl ondertekenende landen technische ondersteuning kunnen ontvangen bij het beheren van hun AI-systemen.

**4. Internationale verificatie en handhaving:**

- Het hardwareverificatiesysteem wordt bijgewerkt zodat het rekenkrachtgebruik rapporteert aan zowel de oorspronkelijke bewaarder als ook direct aan het internationale instantiekantoor.
- De instantie, via discussie met de ondertekenende partijen van de internationale overeenkomst, komt overeen over rekenkrachtbeperkingen die vervolgens juridische kracht krijgen in de ondertekenende landen.
- Parallel kan een set internationale standaarden worden ontwikkeld zodat training en het draaien van AI's boven een drempel van rekenkracht (maar onder de limiet) vereist zijn om zich aan die standaarden te houden.
- De instantie kan, indien nodig om te compenseren voor betere algoritmes etc., de rekenkrachtlimiet verlagen. Of, als het veilig en raadzaam wordt geacht (op bijvoorbeeld het niveau van bewijsbare veiligheidsgaranties), de rekenkrachtlimiet verhogen.

### Bijlage C: Details voor een strikt AGI-aansprakelijkheidsregime

**Details voor een strikt AGI-aansprakelijkheidsregime**

- Het creëren en opereren van een geavanceerd AI-systeem dat zeer algemeen, capabel en autonoom is, wordt beschouwd als een "abnormaal gevaarlijke" activiteit.
- Als zodanig is de standaardaansprakelijkheid voor training en het opereren van dergelijke systemen strikte, gezamenlijke en hoofdelijke aansprakelijkheid (of het niet-Amerikaanse equivalent) voor alle schade aangericht door het model of zijn outputs/acties.
- Persoonlijke aansprakelijkheid wordt opgelegd voor bestuurders en bestuursleden in gevallen van grove nalatigheid of opzettelijk wangedrag. Dit zou strafrechtelijke sancties moeten omvatten voor de meest ernstige gevallen.
- Er zijn talrijke veilige havens waaronder aansprakelijkheid terugkeert naar de standaard (op schuld gebaseerde, in de VS) aansprakelijkheid waaraan mensen en bedrijven normaal gesproken onderworpen zouden zijn.
	- Modellen getraind en geopereerd onder een bepaalde rekenkrachtdrempel (die ten minste 10x lager zou zijn dan de limieten hierboven beschreven.)
	- AI die "zwak" is (ruwweg, onder menselijk expertniveau bij de taken waarvoor het is bedoeld) en/of
	- AI die "smal" is (met een vaste en behoorlijk beperkte reikwijdte van taken en operaties waarvoor het specifiek is ontworpen en getraind) en/of
	- AI die "passief" is (zeer beperkt in zijn vermogen - zelfs onder bescheiden modificatie - om acties te ondernemen of complexe meerstapstaken uit te voeren zonder directe menselijke betrokkenheid en controle.)
	- Een AI die gegarandeerd veilig, beveiligd en controleerbaar is (bewijsbaar veilig, of een risicoanalyse geeft een verwaarloosbaar niveau van verwachte schade aan.)
- Veilige havens kunnen worden geclaimd op basis van een [veiligheidscase](https://arxiv.org/abs/2410.21572) voorbereid door de AI-ontwikkelaar en goedgekeurd door een instantie of auditor die gecertificeerd is door een instantie. Om een veilige haven te claimen gebaseerd op rekenkracht, moet de ontwikkelaar alleen geloofwaardige schattingen leveren van totale Training Rekenkracht en maximale Inferentiesnelheid
- Wetgeving zou expliciet situaties schetsen waarin gerechtelijk bevel tot staking van de ontwikkeling van AI-systemen met een hoog risico op publieke schade gepast zou zijn.
- Bedrijfsconsortia, in samenwerking met NGO's en overheidsinstanties, zouden standaarden en normen moeten ontwikkelen die deze termen definiëren, hoe regelgevers veilige havens zouden moeten verlenen, hoe AI-ontwikkelaars veiligheidscases zouden moeten ontwikkelen, en hoe rechtbanken aansprakelijkheid zouden moeten interpreteren waar veilige havens niet proactief worden geclaimd.

### Bijlage D: Een gelaagde benadering van AGI-veiligheids- en beveiligingsstandaarden

**Een gelaagde benadering van AGI-veiligheids- en beveiligingsstandaarden**

| Risicolaag | Trigger(s) | Vereisten voor training | Vereiste voor implementatie |
| --- | --- | --- | --- |
| RT-0 | AI zwak in autonomie, algemeenheid en intelligentie | geen | geen |
| RT-1 | AI sterk in één van autonomie, algemeenheid en intelligentie | geen | Gebaseerd op risico en gebruik, mogelijk veiligheidscases goedgekeurd door nationale autoriteiten waar het model kan worden gebruikt |
| RT-2 | AI sterk in twee van autonomie, algemeenheid en intelligentie | Registratie bij nationale autoriteit met jurisdictie over de ontwikkelaar | Veiligheidscase die risico van grote schade begrenst onder geautoriseerde niveaus plus onafhankelijke veiligheidsaudits (inclusief black-box en white-box redteaming) goedgekeurd door nationale autoriteiten waar het model kan worden gebruikt |
| RT-3 | AGI sterk in autonomie, algemeenheid en intelligentie | Voorafgaande goedkeuring van veiligheids- en beveiligingsplan door nationale autoriteit met jurisdictie over de ontwikkelaar | Veiligheidscase die begrensde risico van grote schade garandeert onder geautoriseerde niveaus evenals vereiste specificaties, inclusief cyberbeveiliging, controleerbaarheid, een niet-verwijderbare noodstop, alignment met menselijke waarden, en robuustheid tegen kwaadaardige gebruik. |
| RT-4 | Elk model dat ook 10<sup>27</sup> FLOP Training of 10<sup>20</sup> FLOP/s Inferentie overschrijdt | Verboden in afwachting van internationaal overeengekomen opheffing van rekenkrachtlimiet | Verboden in afwachting van internationaal overeengekomen opheffing van rekenkrachtlimiet |

Risicoclassificaties en veiligheids-/beveiligingsstandaarden, met lagen gebaseerd op rekenkrachtdrempels evenals combinaties van hoge autonomie, algemeenheid en intelligentie:

- *Sterke autonomie* is van toepassing als het systeem in staat is om meerstapstaken uit te voeren, of gemakkelijk kan worden gemaakt om die uit te voeren, en/of complexe acties kan ondernemen die relevant zijn voor de echte wereld, zonder significante menselijke toezicht of interventie. Voorbeelden: autonome voertuigen en robots; financiële handelsbots. Tegenvoorbeelden: GPT-4; beeldclassificeerders
- *Sterke algemeenheid* geeft een brede toepassingsreikwijdte aan, prestatie van taken waarvoor het model niet opzettelijk en specifiek werd getraind, en significant vermogen om nieuwe taken te leren. Voorbeelden: GPT-4; mu-zero. Tegenvoorbeelden: AlphaFold; autonome voertuigen; beeldgeneratoren
- *Sterke intelligentie* komt overeen met het evenaren van menselijke expertniveau-prestatie bij de taken waarop het model het beste presteert (en voor een algemeen model, over een breed scala van taken.) Voorbeelden: AlphaFold; mu-zero; o3. Tegenvoorbeelden: GPT-4; Siri

### Dankbetuigingen

Een paar woorden van dank aan mensen die hebben bijgedragen aan Keep The Future Human.

Dit werk geeft de meningen van de auteur weer en moet niet worden beschouwd als de officiële positie van het Future of Life Institute (hoewel ze wel compatibel zijn; voor de officiële positie zie [deze pagina](https://futureoflife.org/our-position-on-ai/)), of enige andere organisatie waarmee de auteur is verbonden.

Ik ben dankbaar aan de mensen Mark Brakel, Ben Eisenpress, Anna Hehir, Carlos Gutierrez, Emilia Javorsky, Richard Mallah, Jordan Scharnhorst, Elyse Fulcher, Max Tegmark, en Jaan Tallinn voor hun commentaar op het manuscript; aan Tim Schrier voor hulp bij enkele referenties; aan Taylor Jones en Elyse Fulcher voor het mooier maken van de diagrammen.

Dit werk maakte beperkt gebruik van generatieve AI-modellen (Claude en ChatGPT) bij de totstandkoming, voor wat bewerking en red-teaming. Volgens de gevestigde standaard voor niveaus van AI-betrokkenheid bij creatieve werken zou dit werk waarschijnlijk een 3/10 krijgen. (Er bestaat eigenlijk helemaal geen dergelijke standaard! Maar die zou er wel moeten zijn.)

We zijn zeer dankbaar aan [Julius Odai](https://www.linkedin.com/in/julius-odai/) voor het produceren van deze webversie van het essay, waardoor het lezen en navigeren door het essay een zeer plezierige ervaring wordt. Julius is een technoloog en recente deelnemer van de BlueDot Impact AI Governance-cursus.