# Die Zukunft menschlich halten

Dieser Essay begründet, warum und wie wir die Tore zu AGI und Superintelligenz schließen sollten, und was wir stattdessen entwickeln sollten.

Wenn Sie nur die wichtigsten Erkenntnisse benötigen, gehen Sie zur Zusammenfassung. Die Kapitel 2-5 vermitteln dann Hintergrundwissen zu den in diesem Essay behandelten KI-Systemen. Die Kapitel 5-7 erläutern, warum wir erwarten könnten, dass AGI bald eintrifft, und was geschehen könnte, wenn es soweit ist. Schließlich skizzieren die Kapitel 8-9 einen konkreten Vorschlag, um die Entwicklung von AGI zu verhindern.

[PDF herunterladen](https://keepthefuturehuman.ai/wp-content/uploads/2025/03/Keep_the_Future_Human__AnthonyAguirre__5March2025.pdf)

Gesamte Lesezeit: 2-3 Stunden

## Zusammenfassung

Ein Überblick über den Essay auf hoher Ebene. Wenn Sie wenig Zeit haben, erhalten Sie alle wichtigsten Punkte in nur 10 Minuten.

Die dramatischen Fortschritte in der künstlichen Intelligenz der letzten Dekade (bei spezialisierten KI-Systemen) und der letzten Jahre (bei Allzweck-KI) haben die KI von einem akademischen Nischenbereich zur Kernstrategie vieler der größten Unternehmen der Welt verwandelt, mit hunderten Milliarden Dollar jährlicher Investitionen in die Techniken und Technologien zur Weiterentwicklung der KI-Fähigkeiten.

Wir stehen nun an einem kritischen Wendepunkt. Da die Fähigkeiten neuer KI-Systeme beginnen, denen von Menschen in vielen kognitiven Bereichen zu entsprechen und sie zu übertreffen, muss die Menschheit entscheiden: Wie weit gehen wir, und in welche Richtung?

KI begann, wie jede Technologie, mit dem Ziel, die Dinge für ihre Schöpfer zu verbessern. Aber unser aktueller Kurs und unsere stillschweigende Entscheidung ist ein unkontrolliertes Rennen hin zu immer mächtigeren Systemen, angetrieben von wirtschaftlichen Anreizen weniger großer Technologieunternehmen, die große Bereiche der aktuellen Wirtschaftstätigkeit und menschlichen Arbeit automatisieren wollen. Falls dieses Rennen noch viel länger anhält, gibt es einen unvermeidlichen Gewinner: die KI selbst – eine schnellere, intelligentere, günstigere Alternative zu Menschen in unserer Wirtschaft, unserem Denken, unseren Entscheidungen und schließlich in der Kontrolle über unsere Zivilisation.

Aber wir können eine andere Wahl treffen: über unsere Regierungen können wir die Kontrolle über den KI-Entwicklungsprozess übernehmen, um klare Grenzen zu setzen, Linien, die wir nicht überschreiten werden, und Dinge, die wir einfach nicht tun werden – wie wir es bei Nukleartechnologien, Massenvernichtungswaffen, Weltraumwaffen, umweltschädlichen Prozessen, der Bioengineering von Menschen und der Eugenik getan haben. Am wichtigsten ist, dass wir sicherstellen können, dass KI ein Werkzeug bleibt, das Menschen stärkt, anstatt eine neue Spezies zu werden, die uns ersetzt und schließlich verdrängt.

Dieser Essay argumentiert, dass wir *die Zukunft menschlich halten* sollten, indem wir die „Tore" zu intelligenterer-als-menschlicher, autonomer, Allzweck-KI – manchmal „AGI" genannt – und besonders zu der hochgradig übermenschlichen Version, die manchmal „Superintelligenz" genannt wird, schließen. Stattdessen sollten wir uns auf mächtige, vertrauenswürdige KI-Werkzeuge konzentrieren, die Individuen stärken und die Fähigkeiten menschlicher Gesellschaften transformativ verbessern können, das zu tun, was sie am besten können. Die Struktur dieses Arguments folgt hier in Kürze.

### KI ist anders

KI-Systeme unterscheiden sich grundlegend von anderen Technologien. Während herkömmliche Software präzisen Anweisungen folgt, lernen KI-Systeme, wie sie Ziele erreichen können, ohne explizit gesagt zu bekommen, wie. Das macht sie mächtig: Wenn wir das Ziel oder eine Erfolgsmetrik klar definieren können, kann ein KI-System in den meisten Fällen lernen, es zu erreichen. Aber es macht sie auch von Natur aus unvorhersagbar: Wir können nicht zuverlässig bestimmen, welche Handlungen sie zur Erreichung ihrer Ziele unternehmen werden.

Sie sind auch weitgehend unerklärlich: Obwohl sie teilweise Code sind, bestehen sie größtenteils aus einer enormen Menge unergründlicher Zahlen – neuronale Netzwerk-„Gewichtungen" –, die nicht analysiert werden können; wir sind nicht viel besser darin, ihre inneren Abläufe zu verstehen, als Gedanken durch einen Blick ins biologische Gehirn zu erkennen.

Diese grundlegende Art des Trainings digitaler neuronaler Netzwerke nimmt rapide an Komplexität zu. Die mächtigsten KI-Systeme werden durch massive Rechenexperimente erstellt, die spezialisierte Hardware verwenden, um neuronale Netzwerke auf enormen Datensätzen zu trainieren, die dann mit Software-Werkzeugen und Superstrukturen erweitert werden.

Dies hat zur Entstehung sehr mächtiger Werkzeuge für das Erstellen und Verarbeiten von Texten und Bildern, das Durchführen mathematischen und wissenschaftlichen Schlussfolgerns, das Aggregieren von Informationen und das interaktive Abfragen eines riesigen Speichers menschlichen Wissens geführt.

Leider ist die Entwicklung mächtigerer, vertrauenswürdigerer technologischer Werkzeuge zwar das, was wir tun *sollten* und was fast jeder will und sagt zu wollen, aber nicht der Kurs, auf dem wir uns tatsächlich befinden.

### AGI und Superintelligenz

Seit den Anfängen des Feldes konzentrierte sich die KI-Forschung stattdessen auf ein anderes Ziel: Artificial General Intelligence. Dieser Fokus ist nun zum Fokus der titanischen Unternehmen geworden, die die KI-Entwicklung anführen.

Was ist AGI? Es wird oft vage als „menschliche KI" definiert, aber das ist problematisch: Welche Menschen, und bei welchen Fähigkeiten ist sie menschlich? Und was ist mit den übermenschlichen Fähigkeiten, die sie bereits hat? Eine nützlichere Art, AGI zu verstehen, ist durch die Schnittmenge dreier Schlüsseleigenschaften: hohe **A**utonomie (Handlungsunabhängigkeit), hohe **A**llgemeinheit (breite Reichweite und Anpassungsfähigkeit) und hohe **I**ntelligenz (Kompetenz bei kognitiven Aufgaben). Aktuelle KI-Systeme mögen hochfähig aber eng gefasst sein, oder allgemein aber konstante menschliche Aufsicht erfordern, oder autonom aber im Umfang begrenzt.

Vollständige A-G-I würde alle drei Eigenschaften auf Ebenen kombinieren, die menschliche Spitzenfähigkeiten erreichen oder übertreffen. Kritisch ist, dass es diese Kombination ist, die Menschen so effektiv und so unterschiedlich von aktueller Software macht; es ist auch das, was es ermöglichen würde, dass Menschen vollständig durch digitale Systeme ersetzt werden.

Während menschliche Intelligenz besonders ist, ist sie keineswegs eine Grenze. Künstliche „superintelligente" Systeme könnten hunderte Male schneller operieren, weitaus mehr Daten analysieren und enorme Mengen gleichzeitig „im Kopf" behalten, und Aggregate bilden, die viel größer und effektiver sind als Zusammenschlüsse von Menschen. Sie könnten nicht Individuen, sondern Unternehmen, Nationen oder unsere Zivilisation als Ganzes verdrängen.

### Wir stehen an der Schwelle

Es gibt einen starken wissenschaftlichen Konsens, dass AGI *möglich* ist. KI übertrifft bereits die menschliche Leistung in vielen allgemeinen Tests intellektueller Fähigkeiten, einschließlich kürzlich bei hochgradigem Schlussfolgern und Problemlösen. Rückständige Fähigkeiten – wie kontinuierliches Lernen, Planung, Selbstbewusstsein und Originalität – existieren alle auf einer gewissen Ebene in gegenwärtigen KI-Systemen, und bekannte Techniken existieren, die wahrscheinlich alle davon verbessern werden.

Während bis vor wenigen Jahren viele Forscher AGI als Jahrzehnte entfernt sahen, sind die Belege für kurze Zeitspannen bis zur AGI derzeit stark:

- Empirisch verifizierte „Skalierungsgesetze" verbinden rechnerischen Input mit KI-Fähigkeiten, und Unternehmen sind auf Kurs, rechnerischen Input um Größenordnungen über die kommenden Jahre zu skalieren. Die menschlichen und finanziellen Ressourcen, die der KI-Weiterentwicklung gewidmet sind, entsprechen nun denen von einem Dutzend Manhattan-Projekten und mehreren Apollo-Projekten.
- KI-Unternehmen und ihre Führungskräfte glauben öffentlich und privat, dass AGI (nach irgendeiner Definition) innerhalb weniger Jahre erreichbar ist. Diese Unternehmen haben Informationen, die die Öffentlichkeit nicht hat, einschließlich dass einige die nächste Generation von KI-Systemen bereits in der Hand haben.
- Experten-Prognostiker mit bewährter Erfolgsbilanz weisen AGI (nach irgendeiner Definition) eine 25%ige Wahrscheinlichkeit zu, innerhalb von 1-2 Jahren einzutreffen, und 50% für 2-5 Jahre (siehe Metaculus-Vorhersagen für ['schwache'](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) und ['vollständige'](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/) AGI).
- Autonomie (einschließlich langfristiger flexibler Planung) hinkt in KI-Systemen hinterher, aber große Unternehmen konzentrieren nun ihre enormen Ressourcen auf die Entwicklung autonomer KI-Systeme und haben informell 2025 das [„Jahr des Agenten"](https://techinformed.com/2025-informed-the-year-of-agentic-ai/) genannt.
- KI trägt mehr und mehr zu ihrer eigenen Verbesserung bei. Sobald KI-Systeme so kompetent wie menschliche KI-Forscher bei der KI-Forschung sind, wird eine kritische Schwelle für schnellen Fortschritt zu viel mächtigeren KI-Systemen erreicht und wahrscheinlich zu einem Kontrollverlust bei den KI-Fähigkeiten führen. (Wohl hat dieser Kontrollverlust bereits begonnen.)

Die Vorstellung, dass intelligentere-als-menschliche AGI Jahrzehnte oder mehr entfernt ist, ist einfach nicht mehr haltbar für die große Mehrheit der Experten auf diesem Gebiet. Meinungsverschiedenheiten gibt es jetzt darüber, wie viele Monate oder Jahre es dauern wird, wenn wir auf diesem Kurs bleiben. Die Kernfrage, der wir uns stellen: Sollten wir das?

### Was das Rennen zur AGI antreibt

Das Rennen zur AGI wird von mehreren Kräften angetrieben, die jeweils die Situation gefährlicher machen. Große Technologieunternehmen sehen AGI als die ultimative Automatisierungstechnologie – die menschliche Arbeiter nicht nur ergänzt, sondern sie weitgehend oder vollständig ersetzt. Für Unternehmen ist der Preis enorm: die Gelegenheit, einen bedeutenden Anteil der jährlichen Wirtschaftsleistung der Welt von 100 Billionen Dollar zu erfassen, indem sie menschliche Arbeitskosten wegautomatisieren.

Nationen fühlen sich gedrängt, diesem Rennen beizutreten, und zitieren öffentlich wirtschaftliche und wissenschaftliche Führerschaft, aber betrachten privat AGI als eine potenzielle Revolution in militärischen Angelegenheiten, vergleichbar mit Nuklearwaffen. Die Furcht, dass Rivalen einen entscheidenden strategischen Vorteil erlangen könnten, schafft eine klassische Rüstungsrennen-Dynamik.

Diejenigen, die Superintelligenz verfolgen, führen oft große Visionen an: alle Krankheiten heilen, Alterung umkehren, Durchbrüche in Energie und Raumfahrt erreichen oder übermenschliche Planungsfähigkeiten schaffen.

Weniger wohlwollend betrachtet treibt das Rennen die Macht an. Jeder Teilnehmer – sei es Unternehmen oder Land – glaubt, dass Intelligenz gleich Macht ist, und dass sie der beste Verwalter dieser Macht sein werden.

Ich argumentiere, dass diese Motivationen real, aber grundlegend fehlgeleitet sind: AGI wird Macht *absorbieren* und *suchen*, anstatt sie zu gewähren; KI-geschaffene Technologien werden *auch* stark zweischneidig sein, und wo sie vorteilhaft sind, können sie mit KI-Werkzeugen und ohne AGI geschaffen werden; und selbst insofern AGI und ihre Ergebnisse unter Kontrolle bleiben, machen diese Renndynamiken – sowohl unternehmerische als auch geopolitische – große Risiken für unsere Gesellschaft nahezu unvermeidlich, es sei denn, sie werden entschieden unterbrochen.

### AGI und Superintelligenz stellen eine dramatische Bedrohung für die Zivilisation dar

Trotz ihrer Anziehungskraft stellen AGI und Superintelligenz dramatische Bedrohungen für die Zivilisation durch mehrere sich verstärkende Pfade dar:

*Machtkonzentration:* Übermenschliche KI könnte die große Mehrheit der Menschheit entmachten, indem sie riesige Bereiche sozialer und wirtschaftlicher Aktivität in KI-Systeme absorbiert, die von einer Handvoll Riesenunternehmen betrieben werden (die wiederum entweder von Regierungen übernommen werden könnten oder diese effektiv übernehmen könnten).

*Massive Störung:* Massenautomatisierung der meisten kognitiv-basierten Jobs, Ersetzung unserer aktuellen epistemischen Systeme und Einführung riesiger Mengen aktiver nicht-menschlicher Agenten würden die meisten unserer aktuellen zivilisatorischen Systeme in einem relativ kurzen Zeitraum umwälzen.

*Katastrophen:* Durch die Verbreitung der Fähigkeit – potenziell über menschliches Niveau – neue militärische und zerstörerische Technologien zu schaffen und ihre Entkopplung von den sozialen und rechtlichen Systemen, die Verantwortung begründen, werden physische Katastrophen durch Massenvernichtungswaffen dramatisch wahrscheinlicher.

*Geopolitik und Krieg:* Große Weltmächte werden nicht untätig zusehen, wenn sie das Gefühl haben, dass eine Technologie, die einen „entscheidenden strategischen Vorteil" liefern könnte, von ihren Gegnern entwickelt wird.

*Kontrollverlust und Kontrollverlust:* Sofern es nicht spezifisch verhindert wird, wird übermenschliche KI jeden Anreiz haben, sich selbst weiter zu verbessern und könnte Menschen bei Geschwindigkeit, Datenverarbeitung und Raffinesse des Denkens weit übertreffen. Es gibt keinen bedeutsamen Weg, auf den wir ein solches System kontrollieren können. Solche KI wird Menschen keine Macht gewähren; wir werden ihr Macht gewähren, oder sie wird sie nehmen.

Viele dieser Risiken bleiben selbst dann bestehen, wenn das technische „Alignment"-Problem – sicherzustellen, dass fortgeschrittene KI zuverlässig das tut, was Menschen von ihr wollen – gelöst ist. KI stellt eine enorme Herausforderung darin dar, wie sie verwaltet wird, und sehr viele Aspekte dieser Verwaltung werden unglaublich schwierig oder unlösbar, sobald menschliche Intelligenz durchbrochen wird.

Am grundlegendsten hätte die Art übermenschlicher Allzweck-KI, die derzeit verfolgt wird, von ihrer Natur her Ziele, Handlungsfähigkeit und Fähigkeiten, die unsere eigenen übertreffen. Sie wäre von Natur aus unkontrollierbar – wie können wir etwas kontrollieren, das wir weder verstehen noch vorhersagen können? Sie wäre kein technologisches Werkzeug für menschlichen Gebrauch, sondern eine zweite Intelligenzspezies auf der Erde neben unserer. Wenn ihr erlaubt würde, weiter fortzuschreiten, würde sie nicht nur eine zweite Spezies, sondern eine Ersatzspezies darstellen.

Vielleicht würde sie uns gut behandeln, vielleicht nicht. Aber die Zukunft würde ihr gehören, nicht uns. Die menschliche Ära wäre vorbei.

### Das ist nicht unvermeidlich; die Menschheit kann sehr konkret entscheiden, ihren Ersatz nicht zu bauen.

Die Schaffung übermenschlicher AGI ist alles andere als unvermeidlich. Wir können sie durch eine koordinierte Reihe von Governance-Maßnahmen verhindern:

Erstens brauchen wir robuste Buchführung und Aufsicht über KI-Rechenleistung, die ein grundlegender Ermöglicher und Hebel zur Steuerung großangelegter KI-Systeme ist. Das wiederum erfordert standardisierte Messung und Berichterstattung der gesamten Rechenleistung, die beim Training von KI-Modellen und beim Betrieb verwendet wird, sowie technische Methoden zur Zählung, Zertifizierung und Verifizierung verwendeter Rechenleistung.

Zweitens sollten wir harte Rechenleistungsobergrenzen für KI implementieren, sowohl für Training als auch für Betrieb; diese verhindern sowohl, dass KI zu mächtig wird als auch zu schnell operiert. Diese Obergrenzen können sowohl durch rechtliche Anforderungen als auch durch hardware-basierte Sicherheitsmaßnahmen implementiert werden, die in KI-spezialisierte Chips eingebaut sind, analog zu Sicherheitsfeatures in modernen Handys. Da spezialisierte KI-Hardware nur von einer Handvoll Unternehmen hergestellt wird, sind Verifizierung und Durchsetzung durch die bestehende Lieferkette machbar.

Drittens brauchen wir verschärfte Haftung für die gefährlichsten KI-Systeme. Diejenigen, die KI entwickeln, die hohe Autonomie, breite Allgemeinheit und überlegene Intelligenz kombiniert, sollten strikter Haftung für Schäden gegenüberstehen, während Schutzräume vor dieser Haftung die Entwicklung begrenzterer und kontrollierbarer Systeme fördern würden.

Viertens brauchen wir gestufte Regulierung basierend auf Risikoebenen. Die fähigsten und gefährlichsten Systeme würden umfangreiche Sicherheits- und Kontrollierbarkeitsgarantien vor Entwicklung und Einsatz erfordern, während weniger mächtige oder spezialisiertere Systeme proportionale Aufsicht erfahren würden. Dieses Regulierungsframework sollte schließlich sowohl auf nationaler als auch internationaler Ebene operieren.

Dieser Ansatz – mit detaillierter Spezifikation im vollständigen Dokument – ist praktisch: Während internationale Koordination nötig sein wird, können Verifizierung und Durchsetzung durch die kleine Anzahl von Unternehmen funktionieren, die die spezialisierte Hardware-Lieferkette kontrollieren. Er ist auch flexibel: Unternehmen können noch immer innovieren und von KI-Entwicklung profitieren, nur mit klaren Grenzen bei den gefährlichsten Systemen.

Längerfristige Eindämmung von KI-Macht und -Risiko würde internationale Abkommen erfordern, die sowohl auf Eigen- als auch auf Gemeininteresse basieren, genau wie die Kontrolle der Nuklearwaffenverbreitung es jetzt tut. Aber wir können sofort mit verstärkter Aufsicht und Haftung beginnen, während wir zu umfassenderer Governance aufbauen.

Die wichtigste fehlende Zutat ist der politische und gesellschaftliche Wille, die Kontrolle über den KI-Entwicklungsprozess zu übernehmen. Die Quelle dieses Willens, wenn er rechtzeitig kommt, wird die Realität selbst sein – das heißt, aus weitverbreiteter Erkenntnis der wirklichen Implikationen dessen, was wir tun.

### Wir können Werkzeug-KI entwickeln, um die Menschheit zu stärken

Anstatt unkontrollierbare AGI zu verfolgen, können wir mächtige „Werkzeug-KI" entwickeln, die menschliche Fähigkeiten verstärkt, während sie unter bedeutsamer menschlicher Kontrolle bleibt. Werkzeug-KI-Systeme können extrem fähig sein, während sie die gefährliche Dreifach-Schnittmenge hoher Autonomie, breiter Allgemeinheit und übermenschlicher Intelligenz vermeiden, solange wir sie so entwickeln, dass sie auf einer Ebene kontrollierbar sind, die ihrer Fähigkeit entspricht. Sie können auch zu sophistizierten Systemen kombiniert werden, die menschliche Aufsicht bewahren, während sie transformative Vorteile liefern.

Werkzeug-KI kann die Medizin revolutionieren, wissenschaftliche Entdeckungen beschleunigen, Bildung verbessern und demokratische Prozesse stärken. Wenn sie richtig gesteuert wird, kann sie menschliche Experten und Institutionen effektiver machen, anstatt sie zu ersetzen. Während solche Systeme noch immer hochgradig disruptiv sein und sorgfältiges Management erfordern werden, sind die Risiken, die sie darstellen, grundlegend anders als AGI: Es sind Risiken, die wir regieren können, wie die anderer mächtiger Technologien, nicht existenzielle Bedrohungen für menschliche Handlungsfähigkeit und Zivilisation. Und entscheidend, wenn sie klug entwickelt wird, kann KI-Werkzeuge Menschen dabei helfen, mächtige KI zu steuern und ihre Effekte zu verwalten.

Dieser Ansatz erfordert ein Umdenken sowohl darüber, wie KI entwickelt wird, als auch wie ihre Vorteile verteilt werden. Neue Modelle öffentlicher und gemeinnütziger KI-Entwicklung, robuste Regulierungsrahmen und Mechanismen zur breiteren Verteilung wirtschaftlicher Vorteile können dabei helfen sicherzustellen, dass KI die Menschheit als Ganzes stärkt, anstatt Macht in wenigen Händen zu konzentrieren. KI selbst kann dabei helfen, bessere soziale und Governance-Institutionen zu bauen, die neue Formen der Koordination und des Diskurses ermöglichen, die die menschliche Gesellschaft stärken, anstatt sie zu untergraben. Nationale Sicherheitseinrichtungen können ihre Expertise nutzen, um KI-Werkzeugsysteme wirklich sicher und vertrauenswürdig zu machen und zu einer wahren Quelle der Verteidigung sowie nationaler Macht.

Wir könnten uns schließlich dafür entscheiden, noch mächtigere und souveränere Systeme zu entwickeln, die weniger wie Werkzeuge und – so können wir hoffen – mehr wie weise und mächtige Wohltäter sind. Aber wir sollten das nur tun, nachdem wir das wissenschaftliche Verständnis und die Governance-Kapazität entwickelt haben, um das sicher zu tun. Eine solche bedeutsame und unumkehrbare Entscheidung sollte bewusst von der Menschheit als Ganzer getroffen werden, nicht standardmäßig in einem Rennen zwischen Technologieunternehmen und Nationen.

### In menschlichen Händen

Menschen wollen das Gute, das von KI kommt: nützliche Werkzeuge, die sie stärken, wirtschaftliche Möglichkeiten und Wachstum verstärken und Durchbrüche in Wissenschaft, Technologie und Bildung versprechen. Warum sollten sie nicht? Aber wenn gefragt, will die überwältigende Mehrheit der allgemeinen Öffentlichkeit [langsamere und sorgfältigere KI-Entwicklung](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation) und will keine intelligentere-als-menschliche KI, die sie in ihren Jobs und anderswo ersetzen wird, ihre Kultur und Informationsallmende mit nicht-menschlichem Inhalt füllen, Macht in einer winzigen Anzahl von Unternehmen konzentrieren, extreme großangelegte globale Risiken darstellen und schließlich drohen, ihre Spezies zu entmachten oder zu ersetzen. Warum sollten sie?

Wir *können* das eine ohne das andere haben. Es beginnt damit zu entscheiden, dass unser Schicksal nicht in der vermeintlichen Unvermeidlichkeit irgendeiner Technologie oder in den Händen weniger CEOs im Silicon Valley liegt, sondern in unseren anderen Händen, wenn wir sie ergreifen. Lasst uns die Tore schließen und die Zukunft menschlich halten.

## Kapitel 1 - Einleitung

Die Frage, wie wir auf die Aussicht auf übermenschliche KI reagieren werden, ist das drängendste Thema unserer Zeit. Dieser Essay zeigt einen Weg nach vorn auf.

Wir stehen möglicherweise am Ende des menschlichen Zeitalters.

In den letzten zehn Jahren hat etwas begonnen, das in der Geschichte unserer Spezies einzigartig ist. Die Konsequenzen werden zu einem großen Teil die Zukunft der Menschheit bestimmen. Seit etwa 2015 ist es Forschern gelungen, *spezialisierte* künstliche Intelligenz (KI) zu entwickeln – Systeme, die Spiele wie Go gewinnen, Bilder und Sprache erkennen können und so weiter, besser als jeder Mensch.[^1]

Das ist ein erstaunlicher Erfolg und führt zu äußerst nützlichen Systemen und Produkten, die die Menschheit stärken werden. Doch spezialisierte künstliche Intelligenz war nie das wahre Ziel des Forschungsfelds. Vielmehr bestand das Ziel darin, KI-Systeme für *allgemeine* Zwecke zu schaffen, insbesondere solche, die oft als „Artificial General Intelligence" (AGI) oder „Superintelligenz" bezeichnet werden und gleichzeitig genauso gut oder besser als Menschen in nahezu *allen* Aufgaben sind, so wie KI heute bereits übermenschlich beim Go, Schach, Poker, Drohnenrennen usw. ist. Das ist das erklärte Ziel vieler großer KI-Unternehmen.[^2]

*Diese Bemühungen sind ebenfalls erfolgreich.* Allzweck-KI-Systeme wie ChatGPT, Gemini, Llama, Grok, Claude und Deepseek, die auf massiven Berechnungen und Bergen von Daten basieren, haben bei einer Vielzahl von Aufgaben Gleichstand mit durchschnittlichen Menschen erreicht und entsprechen sogar menschlichen Experten in einigen Bereichen. Nun wetteifern KI-Ingenieure bei einigen der größten Technologieunternehmen darum, diese gigantischen Experimente mit maschineller Intelligenz auf die nächsten Stufen zu bringen, auf denen sie das gesamte Spektrum menschlicher Fähigkeiten, Expertise und Autonomie erreichen und dann übertreffen.

*Das steht unmittelbar bevor.* In den letzten zehn Jahren sind die Schätzungen von Experten dafür, wie lange das dauern wird – wenn wir unseren gegenwärtigen Kurs fortsetzen –, von Jahrzehnten (oder Jahrhunderten) auf einstellige Jahreszahlen gefallen.

Es ist auch von epochaler Bedeutung und birgt außergewöhnliche Risiken. Befürworter von AGI sehen darin eine positive Transformation, die wissenschaftliche Probleme lösen, Krankheiten heilen, neue Technologien entwickeln und mühselige Arbeiten automatisieren wird. Und KI könnte sicherlich dabei helfen, all diese Dinge zu erreichen – tatsächlich tut sie das bereits. Doch über die Jahrzehnte hinweg haben viele sorgfältige Denker, von Alan Turing über Stephen Hawking bis hin zu den heutigen Geoffrey Hinton und Yoshua Bengio [^3], eine eindringliche Warnung ausgesprochen: Der Bau einer wirklich übermenschlichen, allgemeinen, autonomen KI wird die Gesellschaft mindestens vollständig und unwiderruflich umwälzen und höchstens zum Aussterben der Menschheit führen.[^4]

Superintelligente KI rückt auf unserem gegenwärtigen Pfad schnell näher, ist aber keineswegs unvermeidlich. Dieser Essay ist ein ausführliches Argument dafür, warum und wie wir die *Tore* zu dieser sich nähernden unmenschlichen Zukunft *schließen* sollten und was wir stattdessen tun sollten.


[^1]: Diese [Grafik](https://time.com/6300942/ai-progress-charts/) zeigt eine Reihe von Aufgaben; viele ähnliche Kurven könnten zu diesem Diagramm hinzugefügt werden. Dieser schnelle Fortschritt in der spezialisierten KI hat sogar Experten auf diesem Gebiet überrascht, da Benchmarks Jahre früher als vorhergesagt übertroffen wurden.

[^2]: Deepmind, OpenAI, Anthropic und X.ai wurden alle mit dem spezifischen Ziel gegründet, AGI zu entwickeln. OpenAIs Gründungsurkunde etwa erklärt explizit als Ziel die Entwicklung „künstlicher allgemeiner Intelligenz, die der gesamten Menschheit zugute kommt", während DeepMinds Mission lautet: „Intelligenz zu lösen und sie dann zu nutzen, um alles andere zu lösen." Meta, Microsoft und andere verfolgen nun im Wesentlichen ähnliche Wege. Meta hat erklärt, [AGI entwickeln und offen zugänglich machen zu wollen.](https://www.forbes.com/sites/johnkoetsier/2024/01/18/zuckerberg-on-ai-meta-building-agi-for-everyone-and-open-sourcing-it/)

[^3]: Hinton und Bengio gehören zu den meistzitierten KI-Forschern, haben beide den Nobel des KI-Bereichs, den Turing-Preis, gewonnen, und Hinton hat obendrein einen Nobelpreis (in Physik) erhalten.

[^4]: Etwas mit diesem Risiko unter kommerziellen Anreizen und nahezu ohne staatliche Aufsicht zu entwickeln, ist völlig beispiellos. Es gibt nicht einmal Kontroversen über das Risiko unter denen, die es entwickeln! Die Leiter von Deepmind, OpenAI und Anthropic haben zusammen mit vielen anderen Experten alle buchstäblich eine [Erklärung](https://www.safe.ai/work/statement-on-ai-risk) unterzeichnet, dass fortgeschrittene KI ein *Existenzrisiko für die Menschheit* darstellt. Die Alarmglocken könnten nicht lauter läuten, und man kann nur schlussfolgern, dass diejenigen, die sie ignorieren, AGI und Superintelligenz einfach nicht ernst nehmen. Ein Ziel dieses Essays ist es, ihnen zu helfen zu verstehen, warum sie das tun sollten.

## Kapitel 2 - Wichtiges Grundwissen über neuronale KI-Netzwerke

Wie funktionieren moderne KI-Systeme und was könnte bei der nächsten Generation von KI-Systemen auf uns zukommen?

Um zu verstehen, welche Folgen die Entwicklung leistungsfähigerer KI haben wird, müssen wir uns zunächst einige Grundlagen aneignen. Dieses und die nächsten beiden Kapitel erarbeiten diese Grundlagen und behandeln der Reihe nach, was moderne KI ausmacht, wie sie massive Berechnungen nutzt und in welcher Hinsicht ihre Allgemeinheit und Leistungsfähigkeit rasant zunimmt.[^5]

Es gibt viele Möglichkeiten, künstliche Intelligenz zu definieren, doch für unsere Zwecke liegt die Kerneigenschaft von KI darin, dass sie – im Gegensatz zu einem herkömmlichen Computerprogramm, das eine Liste von Anweisungen zur Ausführung einer Aufgabe darstellt – aus Daten oder Erfahrungen lernt, um Aufgaben zu erledigen, *ohne dass ihr explizit gesagt wird, wie sie dies tun soll.*

Nahezu alle bedeutsamen modernen KI-Systeme basieren auf neuronalen Netzwerken. Dies sind mathematische/rechnerische Strukturen, die durch eine sehr große Anzahl (Milliarden oder Billionen) von Zahlen („Gewichtungen") repräsentiert werden und eine Trainingsaufgabe gut erfüllen. Diese Gewichtungen werden durch iterative Anpassungen erstellt (oder vielleicht eher „gezüchtet" oder „gefunden"), sodass das neuronale Netzwerk einen numerischen Wert (auch „Loss" genannt) verbessert, der darauf ausgelegt ist, eine oder mehrere Aufgaben gut zu erfüllen.[^6] Dieser Vorgang wird als *Training* des neuronalen Netzwerks bezeichnet.[^7]

Es gibt viele Techniken für dieses Training, doch diese Details sind weit weniger relevant als die Art, wie die Bewertung definiert wird und wie daraus unterschiedliche Aufgaben entstehen, die das neuronale Netzwerk gut erfüllt. Historisch wurde eine wichtige Unterscheidung zwischen „enger" und „allgemeiner" KI getroffen.

Enge KI wird gezielt darauf trainiert, eine bestimmte Aufgabe oder eine kleine Gruppe von Aufgaben zu erfüllen (wie etwa Bilderkennung oder Schachspielen); sie erfordert ein erneutes Training für neue Aufgaben und hat einen begrenzten Fähigkeitsbereich. Wir verfügen bereits über übermenschliche enge KI, das heißt, für nahezu jede abgegrenzte, klar definierte Aufgabe, die ein Mensch erledigen kann, können wir wahrscheinlich eine Bewertung erstellen und dann erfolgreich ein enges KI-System trainieren, das diese besser erledigt als ein Mensch.

Allzweck-KI-Systeme können ein breites Spektrum von Aufgaben erfüllen, einschließlich vieler, für die sie nicht explizit trainiert wurden; sie können auch neue Aufgaben als Teil ihres Betriebs erlernen. Aktuelle große „multimodale Modelle"[^8] wie ChatGPT sind dafür beispielhaft: Trainiert auf einem sehr großen Textkorpus und Bildern können sie komplexe Schlussfolgerungen ziehen, Code schreiben, Bilder analysieren und bei einer Vielzahl intellektueller Aufgaben helfen. Obwohl sie sich noch erheblich von menschlicher Intelligenz unterscheiden, wie wir unten ausführlich sehen werden, hat ihre Allgemeinheit eine Revolution in der KI ausgelöst.[^9]

### Unvorhersagbarkeit: ein Schlüsselmerkmal von KI-Systemen

Ein wesentlicher Unterschied zwischen KI-Systemen und herkömmlicher Software liegt in der Vorhersagbarkeit. Die Ausgabe herkömmlicher Software kann unvorhersagbar sein – tatsächlich schreiben wir manchmal genau deshalb Software, um Ergebnisse zu erhalten, die wir nicht hätten vorhersagen können. Aber herkömmliche Software tut selten etwas, wofür sie nicht programmiert wurde – ihr Umfang und Verhalten entsprechen im Allgemeinen dem vorgesehenen Design. Ein erstklassiges Schachprogramm mag Züge machen, die kein Mensch vorhersagen könnte (sonst könnte er dieses Schachprogramm schlagen!), aber es wird im Allgemeinen nichts anderes tun als Schach spielen.

Wie herkömmliche Software hat enge KI einen vorhersagbaren Umfang und vorhersagbares Verhalten, kann aber unvorhersagbare Ergebnisse haben. Dies ist eigentlich nur eine andere Art, enge KI zu definieren: als KI, die herkömmlicher Software in ihrer Vorhersagbarkeit und ihrem Anwendungsbereich ähnelt.

Allzweck-KI ist anders: ihr Umfang (die Bereiche, in denen sie anwendbar ist), ihr Verhalten (die Art der Dinge, die sie tut) und ihre Ergebnisse (ihre tatsächlichen Ausgaben) können alle unvorhersagbar sein.[^10] GPT-4 wurde nur darauf trainiert, Text akkurat zu generieren, entwickelte aber viele Fähigkeiten, die seine Trainer weder vorhergesagt noch beabsichtigt hatten. Diese Unvorhersagbarkeit entspringt der Komplexität des Trainings: Da die Trainingsdaten Ausgaben vieler verschiedener Aufgaben enthalten, muss die KI effektiv lernen, diese Aufgaben zu erfüllen, um gut vorhersagen zu können.

Diese Unvorhersagbarkeit allgemeiner KI-Systeme ist ziemlich grundlegend. Obwohl es prinzipiell möglich ist, KI-Systeme sorgfältig so zu konstruieren, dass sie garantierte Grenzen ihres Verhaltens haben (wie später in diesem Essay erwähnt), sind KI-Systeme, wie sie derzeit entwickelt werden, sowohl praktisch als auch prinzipiell unvorhersagbar.

### Passive KI, Agenten, autonome Systeme und Alignment

Diese Unvorhersagbarkeit wird besonders wichtig, wenn wir betrachten, wie KI-Systeme tatsächlich eingesetzt und verwendet werden, um verschiedene Ziele zu erreichen.

Viele KI-Systeme sind relativ passiv in dem Sinne, dass sie hauptsächlich Informationen bereitstellen und der Benutzer Handlungen vornimmt. Andere, gemeinhin als *Agenten* bezeichnet, führen selbst Handlungen aus, mit unterschiedlichen Graden der Beteiligung eines Benutzers. Solche, die Handlungen mit relativ weniger externem Input oder Überwachung ausführen, können als *autonomer* bezeichnet werden. Dies bildet ein Spektrum in Bezug auf die Handlungsunabhängigkeit, von passiven Werkzeugen bis hin zu autonomen Agenten.[^11]

Was die Ziele von KI-Systemen angeht, so können diese direkt mit ihrem Trainingsziel verknüpft sein (z.B. ist das Ziel des „Gewinnens" für ein Go-spielendes System auch explizit das, wofür es trainiert wurde). Oder sie sind es nicht: ChatGPTs Trainingsziel besteht teilweise darin, Text vorherzusagen, teilweise darin, ein hilfreicher Assistent zu sein. Aber bei einer gegebenen Aufgabe wird sein Ziel vom Benutzer vorgegeben. Ziele können auch von einem KI-System selbst erstellt werden, nur sehr indirekt mit seinem Trainingsziel verbunden.[^12]

Ziele sind eng mit der Frage des „Alignments" verknüpft, das heißt der Frage, ob KI-Systeme *das tun werden, was wir von ihnen wollen*. Diese einfache Frage verbirgt ein enormes Maß an Subtilität.[^13] Beachten Sie vorerst, dass „wir" in diesem Satz sich auf viele verschiedene Personen und Gruppen beziehen könnte, was zu verschiedenen Arten von Alignment führt. Ein KI-System könnte beispielsweise seinem Benutzer gegenüber hochgradig *gehorsam* (oder [„loyal"](https://arxiv.org/abs/2003.11157)) sein – hier ist „wir" gleich „jeder von uns". Oder es könnte *souveräner* sein, primär von seinen eigenen Zielen und Einschränkungen angetrieben, aber dennoch weitgehend im gemeinsamen Interesse menschlichen Wohlbefindens handeln – „wir" ist dann „die Menschheit" oder „die Gesellschaft". Dazwischen liegt ein Spektrum, in dem eine KI weitgehend gehorsam wäre, aber möglicherweise Handlungen verweigert, die anderen oder der Gesellschaft schaden, gegen das Gesetz verstoßen usw.

Diese beiden Achsen – Grad der Autonomie und Art des Alignments – sind nicht völlig unabhängig voneinander. Zum Beispiel ist ein souveränes passives System, obwohl nicht ganz selbstwidersprüchlich, ein spannungsreiches Konzept, ebenso wie ein gehorsamer autonomer Agent.[^14] Es gibt einen klaren Sinn, in dem Autonomie und Souveränität Hand in Hand gehen. In ähnlicher Weise ist die Vorhersagbarkeit bei „passiven" und „gehorsamen" KI-Systemen tendenziell höher, während souveräne oder autonome eher unvorhersagbarer sind. All dies wird entscheidend sein für das Verständnis der Auswirkungen einer möglichen AGI und Superintelligenz.

Die Schaffung wirklich ausgerichteter KI, welcher Art auch immer, erfordert die Lösung dreier verschiedener Herausforderungen:

1. Verstehen, was „wir" wollen – was komplex ist, ob „wir" nun eine bestimmte Person oder Organisation (Loyalität) oder die Menschheit im weiteren Sinne (Souveränität) meint;
2. Systeme bauen, die regelmäßig in Übereinstimmung mit diesen Wünschen handeln – im Wesentlichen konsistent positives Verhalten schaffen;
3. Am grundlegendsten: Systeme schaffen, die sich wirklich um diese Wünsche „sorgen", anstatt nur so zu handeln, als täten sie es.

Die Unterscheidung zwischen zuverlässigem Verhalten und echter Sorge ist entscheidend. Genau wie ein menschlicher Angestellter Befehlen perfekt folgen könnte, während ihm jede echte Verpflichtung gegenüber der Mission der Organisation fehlt, könnte ein KI-System sich ausgerichtet verhalten, ohne wirklich menschliche Präferenzen zu schätzen. Wir können KI-Systeme durch Feedback dazu trainieren, Dinge zu sagen und zu tun, und sie können lernen, darüber zu urteilen, was Menschen wollen. Aber sie dazu zu bringen, menschliche Präferenzen *wirklich* zu schätzen, ist eine weit tiefgreifendere Herausforderung.[^15]

Die tiefgreifenden Schwierigkeiten bei der Lösung dieser Alignment-Herausforderungen und ihre Auswirkungen auf KI-Risiken werden weiter unten näher untersucht. Verstehen Sie vorerst, dass Alignment nicht nur ein technisches Merkmal ist, das wir KI-Systemen anhängen, sondern ein grundlegender Aspekt ihrer Architektur, der ihre Beziehung zur Menschheit prägt.

[^5]: Für eine sanfte, aber technische Einführung in maschinelles Lernen und KI, insbesondere Sprachmodelle, siehe [diese Website.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) Für eine weitere moderne Einführung in KI-Existenzrisiken siehe [diesen Artikel.](https://www.thecompendium.ai/) Für eine umfassende und maßgebliche wissenschaftliche Analyse des Stands der KI-Sicherheit siehe den aktuellen [International AI Safety Report.](https://arxiv.org/abs/2501.17805)

[^6]: Das Training erfolgt typischerweise durch die Suche nach einem lokalen Maximum des Scores in einem hochdimensionalen Raum, der durch die Modellgewichte gegeben ist. Indem geprüft wird, wie sich der Score ändert, wenn Gewichte angepasst werden, identifiziert der Trainingsalgorithmus, welche Anpassungen den Score am stärksten verbessern, und bewegt die Gewichte in diese Richtung.

[^7]: Bei einem Bilderkennungsproblem würde das neuronale Netzwerk beispielsweise Wahrscheinlichkeiten für Labels des Bildes ausgeben. Ein Score würde mit der Wahrscheinlichkeit zusammenhängen, die die KI der korrekten Antwort zuordnet. Das Trainingsverfahren würde dann Gewichte so anpassen, dass die KI beim nächsten Mal eine höhere Wahrscheinlichkeit für das korrekte Label für dieses Bild ausgibt. Dies wird dann sehr oft wiederholt. Dasselbe Grundverfahren wird beim Training im Wesentlichen aller modernen neuronalen Netzwerke verwendet, wenn auch mit komplexeren Bewertungsmechanismen.

[^8]: Die meisten multimodalen Modelle nutzen die „Transformer"-Architektur zur Verarbeitung und Generierung mehrerer Datentypen (Text, Bilder, Ton). Diese können alle in verschiedene Arten von „Tokens" zerlegt und dann gleichberechtigt behandelt werden. Multimodale Modelle werden zunächst darauf trainiert, Tokens in massiven Datensätzen akkurat vorherzusagen, dann durch Reinforcement Learning verfeinert, um Fähigkeiten zu verbessern und Verhaltensweisen zu formen.

[^9]: Dass Sprachmodelle darauf trainiert werden, eine Sache zu tun – Wörter vorherzusagen – hat manche dazu veranlasst, sie als enge KI zu bezeichnen. Aber das ist irreführend: Weil die gute Vorhersage von Text so viele verschiedene Fähigkeiten erfordert, führt diese Trainingsaufgabe zu einem überraschend allgemeinen System. Beachten Sie auch, dass diese Systeme umfangreich durch Reinforcement Learning trainiert werden, was effektiv Tausende von Menschen repräsentiert, die dem Modell ein Belohnungssignal geben, wenn es bei einer der vielen Sachen, die es tut, gute Arbeit leistet. Es erbt dann erhebliche Allgemeinheit von den Menschen, die dieses Feedback geben.

[^10]: Es gibt mehrere Arten, in denen KI unvorhersagbar ist. Eine ist, dass man im allgemeinen Fall nicht vorhersagen kann, was ein Algorithmus tun wird, ohne ihn tatsächlich auszuführen; es gibt [Theoreme](https://arxiv.org/abs/1310.3225) zu diesem Effekt. Das kann einfach daran liegen, dass die Ausgabe von Algorithmen komplex sein kann. Aber es ist besonders klar und relevant in dem Fall (wie bei Schach oder Go), wo die Vorhersage eine Fähigkeit implizieren würde (die KI zu schlagen), die der potentielle Vorhersagende nicht hat. Zweitens wird ein gegebenes KI-System nicht immer dieselbe Ausgabe bei derselben Eingabe produzieren – seine Ausgaben enthalten Zufälligkeit; das koppelt sich auch mit algorithmischer Unvorhersagbarkeit. Drittens können unerwartete und emergente Fähigkeiten aus dem Training entstehen, was bedeutet, dass sogar die *Arten* von Dingen, die ein KI-System kann und tun wird, unvorhersagbar sind. Dieser letzte Typ ist besonders wichtig für Sicherheitsüberlegungen.

[^11]: Siehe [hier](https://arxiv.org/abs/2502.02649) für eine ausführliche Übersicht darüber, was mit einem „autonomen Agenten" gemeint ist (zusammen mit ethischen Argumenten gegen deren Entwicklung).

[^12]: Sie hören vielleicht manchmal „KI kann keine eigenen Ziele haben." Das ist absoluter Unsinn. Es ist leicht, Beispiele zu generieren, wo KI Ziele hat oder entwickelt, die ihr nie gegeben wurden und nur ihr selbst bekannt sind. Sie sehen das bei aktuellen populären multimodalen Modellen nicht viel, weil es aus ihnen heraustrainiert wird; es könnte genauso leicht in sie hineintrainiert werden.

[^13]: Es gibt eine umfangreiche Literatur. Zum allgemeinen Problem siehe Christians [*The Alignment Problem*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821) und Russells [*Human-Compatible*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616). Auf einer technischeren Seite siehe z.B. [dieses Paper](https://arxiv.org/abs/2209.00626).

[^14]: Wir werden später sehen, dass solche Systeme, obwohl sie dem Trend widersprechen, tatsächlich sehr interessant und nützlich sind.

[^15]: Das heißt nicht, dass wir Emotionen oder Empfindungsfähigkeit benötigen. Vielmehr ist es von außen außerordentlich schwierig zu wissen, was die inneren Ziele, Präferenzen und Werte eines Systems sind. „Wirklich" würde hier bedeuten, dass wir starke genug Gründe haben, uns darauf zu verlassen, dass wir im Fall kritischer Systeme unser Leben darauf setzen können.

## Kapitel 3 - Zentrale Aspekte der Entwicklung moderner allgemeiner KI-Systeme

Die meisten der weltweit modernsten KI-Systeme werden mit überraschend ähnlichen Methoden entwickelt. Hier sind die Grundlagen.

Um einen Menschen wirklich zu verstehen, muss man etwas über Biologie, Evolution, Kindererziehung und mehr wissen; um KI zu verstehen, muss man ebenfalls wissen, wie sie entwickelt wird. In den letzten fünf Jahren haben sich KI-Systeme sowohl in ihrer Leistungsfähigkeit als auch in ihrer Komplexität enorm weiterentwickelt. Ein entscheidender Erfolgsfaktor war die Verfügbarkeit sehr großer Mengen an Rechenleistung (umgangssprachlich „Compute", wenn es auf KI angewendet wird).

Die Zahlen sind verblüffend. Etwa 10<sup>25</sup>-10<sup>26</sup> „Gleitkommaoperationen" (FLOP) [^16] werden für das Training von Modellen wie der GPT-Serie, Claude, Gemini usw. verwendet.[^17] (Zum Vergleich: Wenn jeder Mensch auf der Erde ununterbrochen arbeiten und alle fünf Sekunden eine Berechnung durchführen würde, bräuchte es etwa eine Milliarde Jahre, um dies zu schaffen.) Diese enorme Menge an Rechenleistung ermöglicht das Training von Modellen mit bis zu Billionen von Modellgewichten auf Terabytes von Daten – einem großen Teil aller jemals geschriebenen qualitativ hochwertigen Texte sowie umfangreichen Bibliotheken von Tönen, Bildern und Videos. Ergänzt durch zusätzliches umfassendes Training, das menschliche Präferenzen und gute Aufgabenleistung verstärkt, zeigen auf diese Weise trainierte Modelle menschlich vergleichbare Leistung bei einer beträchtlichen Bandbreite grundlegender intellektueller Aufgaben, einschließlich Reasoning und Problemlösung.

Wir wissen auch (sehr, sehr grob), wie viel Rechengeschwindigkeit, in Operationen pro Sekunde, ausreicht, damit die *Inferenz*-Geschwindigkeit [^18] eines solchen Systems der *Geschwindigkeit* menschlicher Textverarbeitung entspricht. Es sind etwa 10<sup>15</sup>-10<sup>16</sup> FLOP pro Sekunde.[^19]

Obwohl diese Modelle mächtig sind, sind sie von Natur aus in wichtigen Aspekten begrenzt, ganz ähnlich wie ein einzelner Mensch begrenzt wäre, wenn er gezwungen wäre, einfach Text mit einer festen Rate von Wörtern pro Minute auszugeben, ohne anzuhalten zum Nachdenken oder zusätzliche Werkzeuge zu verwenden. Neuere KI-Systeme gehen diese Einschränkungen durch einen komplexeren Prozess und eine Architektur an, die mehrere Schlüsselelemente kombiniert:

- Ein oder mehrere neuronale Netzwerke, wobei ein Modell die Kernkognition bereitstellt und bis zu mehrere andere spezialisiertere Aufgaben übernehmen;
- *Werkzeuge*, die dem Modell zur Verfügung gestellt und von ihm verwendet werden können – beispielsweise die Fähigkeit, im Web zu suchen, Dokumente zu erstellen oder zu bearbeiten, Programme auszuführen usw.
- *Gerüstsysteme*, die Ein- und Ausgaben neuronaler Netzwerke verbinden. Ein sehr einfaches Gerüstsystem könnte nur zwei „Instanzen" eines KI-Modells miteinander sprechen lassen oder eine die Arbeit der anderen überprüfen lassen.[^20]
- *Chain-of-Thought* und verwandte Prompting-Techniken tun etwas Ähnliches und veranlassen ein Modell beispielsweise dazu, viele Ansätze für ein Problem zu generieren und diese Ansätze dann für eine aggregierte Antwort zu verarbeiten.
- *Retraining* von Modellen, um Werkzeuge, Gerüstsysteme und Chain-of-Thought besser zu nutzen.

Da diese Erweiterungen sehr mächtig sein können (und KI-Systeme selbst einschließen), können diese zusammengesetzten Systeme ziemlich ausgereift sein und die KI-Fähigkeiten dramatisch verbessern.[^21] Und kürzlich wurden Techniken in Gerüstsystemen und besonders Chain-of-Thought-Prompting (und die Einspeisung der Ergebnisse zurück in das Retraining von Modellen, um diese besser zu nutzen) in [o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/) und [DeepSeek R1](https://api-docs.deepseek.com/news/news250120) entwickelt und eingesetzt, um viele Inferenzdurchläufe als Antwort auf eine gegebene Anfrage durchzuführen.[^22] Dies ermöglicht es dem Modell effektiv, über seine Antwort „nachzudenken" und steigert die Fähigkeit dieser Modelle dramatisch, hochwertiges Reasoning bei Wissenschafts-, Mathematik- und Programmieraufgaben durchzuführen.[^23]

Für eine gegebene KI-Architektur können Erhöhungen der Trainingsrechenleistung [zuverlässig übersetzt werden](https://arxiv.org/abs/2405.10938) in Verbesserungen bei einer Reihe klar definierter Metriken. Für weniger scharf definierte allgemeine Fähigkeiten (wie die unten diskutierten) ist die Übersetzung weniger klar und vorhersagbar, aber es ist fast sicher, dass größere Modelle mit mehr Trainingsrechenleistung neue und bessere Fähigkeiten haben werden, auch wenn es schwer vorherzusagen ist, welche das sein werden.

Ähnlich haben zusammengesetzte Systeme und besonders Fortschritte in „Chain of Thought" (und Training von Modellen, die gut damit funktionieren) Skalierung in der *Inferenz*-Rechenleistung freigeschaltet: Für ein gegebenes trainiertes Kernmodell steigen zumindest einige KI-Systemfähigkeiten, wenn mehr Rechenleistung angewendet wird, die es ihnen ermöglicht, „härter und länger" über komplexe Probleme zu „denken". Dies geht mit steilen Kosten bei der Rechengeschwindigkeit einher und erfordert Hunderte oder Tausende mehr FLOP/s, um menschliche Leistung zu erreichen.[^24]

Obwohl die Rolle der Rechenleistung nur ein Teil dessen ist, was zu raschem KI-Fortschritt führt,[^25] werden die Rechenleistung und die Möglichkeit zusammengesetzter Systeme sich als entscheidend sowohl für die Verhinderung unkontrollierbarer AGI als auch für die Entwicklung sichererer Alternativen erweisen.

[^16]: 10<sup>27</sup> bedeutet 1 gefolgt von 25 Nullen, oder zehn Billionen Billionen. Ein FLOP ist einfach eine arithmetische Addition oder Multiplikation von Zahlen mit einer bestimmten Genauigkeit. Beachten Sie, dass die Leistung von KI-Hardware je nach Genauigkeit der Arithmetik und der Architektur des Computers um einen Faktor von zehn variieren kann. Das Zählen von Logikgatter-Operationen (ANDS, ORS, AND NOTS) wäre fundamental, aber diese sind nicht allgemein verfügbar oder benchmarked; für gegenwärtige Zwecke ist es nützlich, auf 16-Bit-Operationen (FP16) zu standardisieren, obwohl angemessene Konversionsfaktoren etabliert werden sollten.

[^17]: Eine Sammlung von Schätzungen und harten Daten ist von [Epoch AI](https://epochai.org/data/large-scale-ai-models) verfügbar und zeigt etwa 2×10<sup>25</sup> 16-Bit FLOP für GPT-4; dies entspricht ungefähr [Zahlen, die durchgesickert sind](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) für GPT-4. Schätzungen für andere Modelle aus Mitte 2024 liegen alle innerhalb eines Faktors von wenigen von GPT-4.

[^18]: Inferenz ist einfach der Prozess der Generierung einer Ausgabe aus einem neuronalen Netzwerk. Training kann als eine Abfolge vieler Inferenzen und Modellgewichtsanpassungen betrachtet werden.

[^19]: Für Textproduktion benötigte das ursprüngliche GPT-4 560 TFLOP pro generiertem Token. Etwa 7 Tokens/s sind nötig, um mit menschlichem Denken Schritt zu halten, das ergibt ≈3×10<sup>15</sup> FLOP/s. Aber Effizienzsteigerungen haben dies reduziert; [diese NVIDIA-Broschüre](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/) beispielsweise zeigt nur 3×10<sup>14</sup> FLOP/s für ein vergleichbar leistungsfähiges Llama 405B-Modell.

[^20]: Als etwas komplexeres Beispiel könnte ein KI-System zunächst mehrere mögliche Lösungen für ein Mathematikproblem generieren, dann eine andere Instanz verwenden, um jede Lösung zu überprüfen, und schließlich eine dritte verwenden, um die Ergebnisse in eine klare Erklärung zu synthetisieren. Dies ermöglicht eine gründlichere und zuverlässigere Problemlösung als ein einziger Durchgang.

[^21]: Siehe zum Beispiel Details zu [OpenAIs „Operator"](https://openai.com/index/introducing-operator/), [Claudes Werkzeugfähigkeiten](https://docs.anthropic.com/en/docs/build-with-claude/computer-use) und [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). OpenAIs [Deep Research](https://openai.com/index/introducing-deep-research/) hat wahrscheinlich eine ziemlich ausgeklügelte Architektur, aber Details sind nicht verfügbar.

[^22]: Deepseek R1 stützt sich auf iteratives Training und Prompting des Modells, sodass das final trainierte Modell umfangreiches Chain-of-Thought-Reasoning erstellt. Architekturdetails sind für o1 oder o3 nicht verfügbar, jedoch hat Deepseek enthüllt, dass keine besondere „Geheimzutat" erforderlich ist, um Fähigkeitsskalierung mit Inferenz freizuschalten. Aber trotz großer Medienaufmerksamkeit als Umwälzung des „Status quo" in der KI beeinflusst es nicht die Kernaussagen dieses Essays.

[^23]: Diese Modelle übertreffen Standard-Modelle bei Reasoning-Benchmarks erheblich. Zum Beispiel erreichte GPT-4o beim GPQA Diamond Benchmark – einem rigorosen Test von PhD-Niveau-Wissenschaftsfragen – [56%](https://openai.com/index/learning-to-reason-with-llms/), während o1 und o3 78% bzw. 88% erreichten und damit den durchschnittlichen 70%-Score menschlicher Experten weit übertrafen.

[^24]: OpenAIs O3 wendete wahrscheinlich ∼10<sup>21</sup>-10<sup>22</sup> FLOP [für jede der ARC-AGI-Challenge-Fragen](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai) auf, die kompetente Menschen in (sagen wir) 10-100 Sekunden lösen können, was eine Zahl von eher ∼10<sup>20</sup> FLOP/s ergibt.

[^25]: Während Rechenleistung ein Schlüsselmaß für KI-Systemfähigkeiten ist, interagiert sie sowohl mit Datenqualität als auch algorithmischen Verbesserungen. Bessere Daten oder Algorithmen können Rechenanforderungen reduzieren, während mehr Rechenleistung manchmal schwächere Daten oder Algorithmen kompensieren kann.

## Kapitel 4 - Was sind AGI und Superintelligenz?

Was genau versuchen die größten Technologiekonzerne der Welt hinter verschlossenen Türen zu entwickeln?

Der Begriff „Artificial General Intelligence" gibt es schon seit einiger Zeit und bezeichnet „menschenähnliche" Allzweck-KI. Er war nie besonders präzise definiert, ist aber in den letzten Jahren paradoxerweise noch unklarer und gleichzeitig noch wichtiger geworden – Experten streiten sich darüber, ob AGI noch Jahrzehnte entfernt ist oder bereits erreicht wurde, während Unternehmen im Billionenwert um die „erste AGI" wetteifern. (Die Mehrdeutigkeit von „AGI" wurde kürzlich deutlich, als [durchgesickerte Dokumente angeblich enthüllten](https://gizmodo.com/leaked-documents-show-openai-has-a-very-clear-definition-of-agi-2000543339), dass AGI in OpenAIs Vertrag with Microsoft als KI definiert wurde, die 100 Milliarden Dollar Umsatz für OpenAI generiert – eine eher geschäftsorientierte als wissenschaftliche Definition.)

Die Vorstellung von KI mit „menschlicher Intelligenz" hat zwei grundlegende Probleme. Erstens unterscheiden sich Menschen sehr stark in ihrer Fähigkeit, bestimmte kognitive Aufgaben zu bewältigen – es gibt also kein „menschliches Niveau". Zweitens ist Intelligenz sehr vieldimensional; auch wenn es Korrelationen geben mag, sind sie unvollkommen und könnten bei KI ganz anders aussehen. Selbst wenn wir also „menschliches Niveau" für viele Fähigkeiten definieren könnten, würde KI in manchen Bereichen sicherlich weit darüber hinausgehen, während sie in anderen deutlich darunter bliebe.[^26]

Dennoch ist es entscheidend, über Arten, Stufen und Schwellenwerte von KI-Fähigkeiten sprechen zu können. Der hier gewählte Ansatz betont, dass Allzweck-KI bereits existiert und in verschiedenen Fähigkeitsstufen auftritt – und auftreten wird –, denen wir zweckmäßigerweise Begriffe zuordnen können, auch wenn sie vereinfachend sind, weil sie entscheidenden Schwellenwerten für die Auswirkungen von KI auf Gesellschaft und Menschheit entsprechen.

Wir definieren „vollständige" AGI als synonym mit „übermenschlicher Allzweck-KI" – ein KI-System, das praktisch alle menschlichen kognitiven Aufgaben auf dem Niveau von Spitzenexperten oder darüber bewältigen kann sowie neue Fähigkeiten erwerben und auf neue Bereiche übertragen kann. Dies entspricht der häufigen Definition von „AGI" in der modernen Literatur. Wichtig ist, dass dies eine *sehr* hohe Schwelle darstellt. Kein Mensch besitzt diese Art von Intelligenz; vielmehr entspricht sie der Intelligenz, die große Gruppen von Spitzenexperten hätten, wenn sie kombiniert würden. Als „Superintelligenz" bezeichnen wir Fähigkeiten, die darüber hinausgehen, und definieren begrenztere Fähigkeitsstufen durch „konkurrenzfähige" und „expertenkonkurrenzfähige" Allzweck-KI, die ein breites Spektrum von Aufgaben auf typischem Profi- oder Expertenniveau bewältigen.[^27]

Diese und weitere Begriffe sind in der [Tabelle](https://keepthefuturehuman.ai/essay/docs/#tab:terms) unten zusammengefasst. Für ein konkreteres Verständnis dessen, was die verschiedenen Systemstufen leisten können, ist es hilfreich, die Definitionen ernst zu nehmen und zu durchdenken, was sie bedeuten.

| KI-Typ                           | Verwandte Begriffe                  | Definition                                                                                                                                                                                                                           | Beispiele                                                                                                                                                    |
| -------------------------------- | ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Schwache KI                      | Narrow AI                           | KI, die für eine spezifische Aufgabe oder Aufgabenfamilie trainiert wurde. Übertrifft in ihrem Bereich, hat aber keine allgemeine Intelligenz oder Transferlernfähigkeit.                                                         | Bilderkennungssoftware; Sprachassistenten (z.B. Siri, Alexa); Schachprogramme; DeepMinds AlphaFold                                                         |
| Werkzeug-KI                      | Augmented Intelligence, KI-Assistent| (Wird später im Essay diskutiert.) KI-System, das menschliche Fähigkeiten erweitert. Kombiniert konkurrenzfähige Allzweck-KI, schwache KI und garantierte Kontrolle mit Fokus auf Sicherheit und Zusammenarbeit. Unterstützt menschliche Entscheidungsfindung. | Fortgeschrittene Programmierassistenten; KI-gestützte Forschungstools; Ausgeklügelte Datenanalyseplattformen. Kompetente aber begrenzte und kontrollierbare Agenten |
| Allzweck-KI                      | GPAI                                | KI-System, das sich an verschiedene Aufgaben anpassen kann, auch an solche, für die es nicht spezifisch trainiert wurde.                                                                                                            | Sprachmodelle (z.B. GPT-4, Claude); Multimodale KI-Modelle; DeepMinds MuZero                                                                                |
| Konkurrenzfähige Allzweck-KI     | AGI [schwach]                       | Allzweck-KI, die Aufgaben auf durchschnittlichem menschlichem Niveau bewältigt und dieses manchmal übertrifft.                                                                                                                      | Fortgeschrittene Sprachmodelle (z.B. O1, Claude 3.5); Einige multimodale KI-Systeme                                                                        |
| Expertenkonkurrenzfähige Allzweck-KI | AGI [partiell]                  | Allzweck-KI, die die meisten Aufgaben auf Expertenniveau bewältigt, mit erheblicher aber begrenzter Autonomie                                                                                                                       | Möglicherweise ein ausgerüstetes und mit Gerüstsystem versehenes O3, zumindest für Mathematik, Programmierung und einige Naturwissenschaften              |
| AGI [vollständig]                | Übermenschliche Allzweck-KI         | KI-System, das autonom praktisch alle menschlichen intellektuellen Aufgaben auf Expertenniveau oder darüber bewältigen kann, mit effizientem Lernen und Wissenstransfer.                                                         | [Keine aktuellen Beispiele – theoretisch]                                                                                                                   |
| Superintelligenz                 | Stark übermenschliche Allzweck-KI   | KI-System, das menschliche Fähigkeiten in allen Bereichen weit übertrifft und kollektive menschliche Expertise übertrifft. Diese Überlegenheit kann sich auf Allgemeinheit, Qualität, Geschwindigkeit und/oder andere Maßstäbe beziehen. | [Keine aktuellen Beispiele – theoretisch]                                                                                                                   |

Wir erleben bereits, wie es ist, Allzweck-KI bis zum konkurrenzfähigen Niveau zu haben. Dies hat sich relativ reibungslos integriert, da die meisten Nutzer dies als intelligenten aber begrenzten Zeitarbeiter erleben, der sie produktiver macht – mit gemischten Auswirkungen auf die Qualität ihrer Arbeit.[^28]

Der Unterschied bei expertenkonkurrenzfähiger Allzweck-KI wäre, dass sie nicht die grundlegenden Beschränkungen heutiger KI hätte und das täte, was Experten tun: unabhängige wirtschaftlich wertvolle Arbeit, echte Wissensschaffung, technische Arbeit, auf die man sich verlassen kann, während sie selten (aber dennoch gelegentlich) dumme Fehler macht.

Die Idee vollständiger AGI ist, dass sie *wirklich* alle kognitiven Dinge macht, die selbst die fähigsten und effektivsten Menschen tun – autonom und ohne benötigte Hilfe oder Aufsicht. Dies schließt ausgeklügelte Planung ein, das Erlernen neuer Fähigkeiten, die Verwaltung komplexer Projekte usw. Sie könnte originäre Spitzenforschung betreiben. Sie könnte ein Unternehmen führen. Was auch immer Ihr Beruf ist – wenn er überwiegend am Computer oder telefonisch ausgeübt wird, *könnte sie ihn mindestens so gut wie Sie ausüben.* Und wahrscheinlich viel schneller und kostengünstiger. Wir werden einige der Auswirkungen unten diskutieren, aber zunächst besteht die Herausforderung darin, dies wirklich ernst zu nehmen. Stellen Sie sich die zehn sachkundigsten und kompetentesten Menschen vor, die Sie kennen oder von denen Sie wissen – einschließlich CEOs, Wissenschaftlern, Professoren, Spitzeningenieuren, Psychologen, politischen Führern und Schriftstellern. Packen Sie sie alle in eine Person, die auch 100 Sprachen spricht, ein phänomenales Gedächtnis hat, schnell arbeitet, unermüdlich und stets motiviert ist und unter dem Mindestlohn arbeitet.[^29] Das vermittelt einen Eindruck davon, was AGI wäre.

Bei Superintelligenz ist das Vorstellen schwieriger, weil die Idee ist, dass sie intellektuelle Leistungen vollbringen könnte, zu denen kein Mensch oder auch keine Gruppe von Menschen fähig ist – sie ist per Definition für uns unvorhersagbar. Aber wir können uns eine Vorstellung machen. Als absolutes Minimum stellen Sie sich viele AGIs vor, jede viel fähiger als selbst der beste menschliche Experte, die mit 100-facher menschlicher Geschwindigkeit laufen, mit enormem Gedächtnis und großartiger Koordinationsfähigkeit.[^30] Und es geht von dort noch weiter nach oben. Mit Superintelligenz umzugehen wäre weniger wie ein Gespräch mit einem anderen Verstand, sondern eher wie Verhandlungen mit einer anderen (und fortgeschritteneren) Zivilisation.

Wie nah *sind wir* also an AGI und Superintelligenz?


[^26]: Zum Beispiel übertreffen aktuelle KI-Systeme menschliche Fähigkeiten bei schnellen Rechenaufgaben oder Gedächtnisaufgaben bei weitem, während sie bei abstraktem Denken und kreativem Problemlösen zurückbleiben.

[^27]: Als Konkurrent hätte solche KI mehrere wichtige strukturelle Vorteile: Sie würde nicht ermüden oder andere individuelle Bedürfnisse wie Menschen haben; sie könnte mit höherer Geschwindigkeit laufen, indem einfach die Rechenleistung skaliert wird; sie könnte zusammen mit erworbener Expertise oder Wissen kopiert werden – und das erworbene Wissen neuronaler Netzwerke kann sogar „zusammengeführt" werden, um ganze Fähigkeitssätze untereinander zu übertragen; sie könnte mit Maschinengeschwindigkeit kommunizieren; und sie könnte sich auf bedeutsamere Weise und mit höherer Geschwindigkeit selbst modifizieren oder verbessern als jeder Mensch.

[^28]: Falls Sie noch keine Zeit mit aktuellen Spitzen-KI-Systemen verbracht haben, empfehle ich es: Sie sind wirklich nützlich und fähig, und es ist auch wichtig, um die Auswirkungen zu kalibrieren, die KI haben wird, wenn sie leistungsfähiger werden.

[^29]: Betrachten Sie ein großes Forschungskrankenhaus: vollständig realisierte AGI könnte gleichzeitig alle eingehenden Patientendaten analysieren, mit jedem neuen medizinischen Paper Schritt halten, Diagnosen vorschlagen, Behandlungspläne entwerfen, klinische Studien verwalten und die Personalplanung koordinieren – alles auf einem Niveau, das den Top-Spezialisten des Krankenhauses in jedem Bereich entspricht oder sie übertrifft. Und sie könnte dies gleichzeitig für mehrere Krankenhäuser tun, zu einem Bruchteil der aktuellen Kosten. Leider müssen Sie auch ein organisiertes Verbrechersyndikator betrachten: vollständig realisierte AGI könnte gleichzeitig Tausende von Opfern hacken, sich als sie ausgeben, sie bespitzeln und erpressen, mit der Strafverfolgung Schritt halten (die viel langsamer automatisiert), neue gewinnbringende Machenschaften entwerfen und die Personalplanung koordinieren – falls es überhaupt Personal gibt.

[^30]: In seinem [Essay](https://darioamodei.com/machines-of-loving-grace) erinnerte Dario Amodei, CEO von Anthropic, an ein „Land von [einer Million] Genies".

## Kapitel 5 - An der Schwelle

Der Weg von den heutigen KI-Systemen zu vollständiger AGI erscheint schockierend kurz und vorhersagbar.

Die vergangenen zehn Jahre haben dramatische Fortschritte in der KI erlebt, angetrieben von enormen [Rechenleistungs-](https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year), personellen und [finanziellen](https://arxiv.org/abs/2405.21015) Ressourcen. Viele spezialisierte KI-Anwendungen übertreffen Menschen bei ihren zugewiesenen Aufgaben und sind dabei sicherlich deutlich schneller und günstiger.[^31] Zudem gibt es spezialisierte übermenschliche Agenten, die alle Menschen in abgegrenzten Spielbereichen wie [Go](https://www.nature.com/articles/nature16961), [Schach](https://arxiv.org/abs/1712.01815) und [Poker](https://www.deepstack.ai/) besiegen können, sowie [generalistischere Agenten](https://deepmind.google/discover/blog/a-generalist-agent/), die in vereinfachten simulierten Umgebungen so effektiv planen und handeln können wie Menschen.

Am prominentesten sind die aktuellen allgemeinen KI-Systeme von OpenAI/Microsoft, Google/Deepmind, Anthropic/Amazon, Facebook/Meta, X.ai/Tesla und anderen,[^32] die seit Anfang 2023 entstanden sind und seither stetig (wenn auch ungleichmäßig) ihre Fähigkeiten ausgebaut haben. Alle wurden durch Token-Vorhersage auf riesigen Text- und Multimedia-Datensätzen erstellt, kombiniert mit umfangreichem Verstärkungsfeedback von Menschen und anderen KI-Systemen. Einige enthalten auch umfangreiche Werkzeug- und Gerüstsysteme.

### Stärken und Schwächen aktueller genereller Systeme

Diese Systeme erbringen gute Leistungen bei einer zunehmend breiten Palette von Tests zur Messung von Intelligenz und Expertise, mit Fortschritten, die sogar Experten auf diesem Gebiet überrascht haben:

- Bei seiner Erstveröffentlichung [erreichte oder übertraf GPT-4 die typische menschliche Leistung](https://arxiv.org/abs/2303.08774) bei standardisierten akademischen Tests wie SATs, GRE, Aufnahmeprüfungen und Anwaltsprüfungen. Neuere Modelle schneiden wahrscheinlich deutlich besser ab, obwohl die Ergebnisse nicht öffentlich verfügbar sind.
- Der Turing-Test – lange als Schlüssel-Benchmark für „echte" KI betrachtet – wird nun routinemäßig in einigen Formen von modernen Sprachmodellen bestanden, sowohl informell als auch in [formellen Studien](https://arxiv.org/abs/2405.08007).[^33]
- Beim umfassenden MMLU-Benchmark, der 57 akademische Fächer umfasst, [erreichen aktuelle Modelle Punktzahlen auf Experteniveau](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu) (∼90%)[^34]
- Die technische Expertise hat sich dramatisch verbessert: Der GPQA-Benchmark für Physik auf Graduiertenebene verzeichnete einen [Leistungssprung](https://epoch.ai/data/ai-benchmarking-dashboard) von nahezu zufälligen Vermutungen (GPT-4, 2022) auf Expertenniveau (o1-preview, 2024).
- Selbst Tests, die speziell darauf ausgelegt waren, KI-resistent zu sein, fallen: OpenAIs O3 [löst angeblich](https://www.nextbigfuture.com/2024/12/openai-releases-o3-model-with-high-performance-and-high-cost.html) den ARC-AGI-Benchmark für abstrakte Problemlösung auf menschlichem Niveau, erreicht Spitzen-Expertenleistung beim Programmieren und erzielt 25% bei Epoch AIs „Frontier Math"-Problemen, die Elite-Mathematiker herausfordern sollen.[^35]
- Der Trend ist so deutlich, dass der Entwickler von MMLU nun [„Humanity's Last Exam"](https://agi.safe.ai/) erstellt hat – ein ominöser Name, der die Möglichkeit widerspiegelt, dass KI bald die menschliche Leistung bei jedem sinnvollen Test übertreffen wird. Zum Zeitpunkt der Erstellung gibt es Behauptungen, KI-Systeme erreichten 27% (laut [Sam Altman](https://x.com/sama/status/1886220281565381078)) und 35% (laut [diesem Paper](https://arxiv.org/abs/2502.09955)) bei dieser extrem schwierigen Prüfung. Es ist äußerst unwahrscheinlich, dass ein einzelner Mensch das schaffen könnte.

Trotz dieser beeindruckenden Zahlen (und ihrer offensichtlichen Intelligenz bei der Interaktion mit ihnen)[^36] gibt es viele Dinge, die (zumindest die veröffentlichten Versionen) dieser neuronalen Netzwerke *nicht* können. Derzeit sind die meisten körperlos – existieren nur auf Servern – und verarbeiten höchstens Text, Ton und Standbilder (aber keine Videos). Entscheidend ist, dass die meisten keine komplexen geplanten Aktivitäten durchführen können, die hohe Genauigkeit erfordern.[^37] Und es gibt eine Reihe anderer Eigenschaften, die in hochstufiger menschlicher Kognition stark ausgeprägt, in veröffentlichten KI-Systemen jedoch schwach entwickelt sind.

Die folgende Tabelle führt eine Reihe davon auf, basierend auf KI-Systemen von Mitte 2024 wie GPT-4o, Claude 3.5 Sonnet und Google Gemini 1.5.[^38] Die Schlüsselfrage dafür, wie schnell allgemeine KI mächtiger wird, lautet: In welchem Maße wird *mehr desselben* Ergebnisse hervorbringen, versus das Hinzufügen zusätzlicher, aber *bekannter* Techniken, versus die Entwicklung oder Implementierung *wirklich neuer* KI-Forschungsrichtungen. Meine eigenen Vorhersagen dazu sind in der Tabelle angegeben, in Bezug darauf, wie wahrscheinlich jedes dieser Szenarien ist, diese Fähigkeit auf und über das menschliche Niveau zu bringen.

<table><tbody><tr><th>Fähigkeit</th><th>Beschreibung der Fähigkeit</th><th>Status/Prognose</th><th>Skalierung/bekannt/neu</th></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td colspan="4"><em>Zentrale kognitive Fähigkeiten</em></td></tr><tr><td>Reasoning</td><td>Menschen können präzise, mehrstufige Schlussfolgerungen ziehen, Regeln befolgen und Genauigkeit überprüfen.</td><td>Dramatische jüngste Fortschritte durch erweiterte Chain-of-Thought und erneutes Training</td><td>95/5/5</td></tr><tr><td>Planung</td><td>Menschen zeigen langfristige und hierarchische Planung.</td><td>Verbessert sich mit der Skalierung; kann durch Gerüstsysteme und bessere Trainingstechniken stark unterstützt werden.</td><td>10/85/5</td></tr><tr><td>Wahrheitsverankerung</td><td>Allzweck-KIs erfinden unbegründete Informationen, um Anfragen zu erfüllen.</td><td>Verbessert sich mit der Skalierung; Kalibrierungsdaten im Modell verfügbar; kann durch Gerüstsysteme überprüft/verbessert werden.</td><td>30/65/5</td></tr><tr><td>Flexible Problemlösung</td><td>Menschen können neue Muster erkennen und neue Lösungen für komplexe Probleme erfinden; aktuelle ML-Modelle haben Schwierigkeiten damit.</td><td>Verbessert sich mit der Skalierung, aber schwach; könnte mit neurosymbolischen oder verallgemeinerten „Such"-Techniken lösbar sein.</td><td>15/75/10</td></tr><tr><td colspan="4"><em>Lernen und Wissen</em></td></tr><tr><td>Lernen & Gedächtnis</td><td>Menschen haben Arbeits-, Kurzzeit- und Langzeitgedächtnis, die alle dynamisch und miteinander verbunden sind.</td><td>Alle Modelle lernen während des Trainings; Allzweck-KIs lernen innerhalb des Kontextfensters und während der Feinabstimmung; „kontinuierliches Lernen" und andere Techniken existieren, sind aber noch nicht in große Allzweck-KIs integriert.</td><td>5/80/15</td></tr><tr><td>Abstraktion & Rekursion</td><td>Menschen können Beziehungsmengen in abstraktere überführen für Reasoning und Manipulation, einschließlich rekursivem „Meta"-Reasoning.</td><td>Schwache Verbesserung mit der Skalierung; könnte in neurosymbolischen Systemen entstehen.</td><td>30/50/20</td></tr><tr><td>Weltmodell(e)</td><td>Menschen haben und aktualisieren kontinuierlich ein prädiktives Weltmodell, innerhalb dessen sie Probleme lösen und physikalisches Reasoning betreiben können</td><td>Verbessert sich mit der Skalierung; Aktualisierung mit Lernen verknüpft; Allzweck-KIs schwach in realer Weltvorhersage.</td><td>20/50/30</td></tr><tr><td colspan="4"><em>Selbst und Handlungsfähigkeit</em></td></tr><tr><td>Handlungsfähigkeit</td><td>Menschen können Handlungen ausführen, um Ziele zu verfolgen, basierend auf Planung/Vorhersage.</td><td>Viele ML-Systeme sind handlungsfähig; LLMs können durch Wrapper zu Agenten gemacht werden.</td><td>5/90/5</td></tr><tr><td>Selbststeuerung</td><td>Menschen entwickeln und verfolgen ihre eigenen Ziele mit intern erzeugter Motivation und Antrieb.</td><td>Besteht größtenteils aus Handlungsfähigkeit plus Originalität; wird wahrscheinlich in komplexen handlungsfähigen Systemen mit abstrakten Zielen entstehen.</td><td>40/45/15</td></tr><tr><td>Selbstbezug</td><td>Menschen verstehen und denken über sich selbst als situiert innerhalb einer Umgebung/eines Kontexts nach.</td><td>Verbessert sich mit der Skalierung und könnte durch Trainingsbelohnung erweitert werden.</td><td>70/15/15</td></tr><tr><td>Selbstbewusstsein</td><td>Menschen haben Wissen über ihre eigenen Gedanken und geistigen Zustände und können darüber nachdenken.</td><td>Existiert in gewissem Sinne in Allzweck-KIs, die wohl den klassischen „Spiegeltest" für Selbstbewusstsein bestehen können. Kann mit Gerüstsystemen verbessert werden; aber unklar, ob das ausreicht.</td><td>20/55/25</td></tr><tr><td colspan="4"><em>Schnittstelle und Umgebung</em></td></tr><tr><td>Verkörperte Intelligenz</td><td>Menschen verstehen ihre reale Umgebung und interagieren aktiv mit ihr.</td><td>Verstärkungslernen funktioniert gut in simulierten und realen (robotischen) Umgebungen und kann in multimodale Transformer integriert werden.</td><td>5/85/10</td></tr><tr><td>Multisensorische Verarbeitung</td><td>Menschen integrieren und verarbeiten in Echtzeit visuelle, auditive und andere Sinnesdatenströme.</td><td>Training in mehreren Modalitäten scheint „einfach zu funktionieren" und verbessert sich mit der Skalierung. Echtzeit-Videoverarbeitung ist schwierig, aber z.B. selbstfahrende Systeme verbessern sich rasch.</td><td>30/60/10</td></tr><tr><td colspan="4"><em>Höherrangige Fähigkeiten</em></td></tr><tr><td>Originalität</td><td>Aktuelle ML-Modelle sind kreativ im Transformieren und Kombinieren existierender Ideen/Werke, aber Menschen können neue Rahmenwerke und Strukturen entwickeln, manchmal verknüpft mit ihrer Identität.</td><td>Kann schwer von „Kreativität" zu unterscheiden sein, die sich dazu skalieren könnte; könnte aus Kreativität plus Selbstbewusstsein entstehen.</td><td>50/40/10</td></tr><tr><td>Empfindungsfähigkeit</td><td>Menschen erleben Qualia; diese können positive, negative oder neutrale Valenz haben; es ist „wie etwas", eine Person zu sein.</td><td>Sehr schwierig und philosophisch problematisch zu bestimmen, ob ein gegebenes System das besitzt.</td><td>5/10/85</td></tr></tbody></table>

Schlüsselfähigkeiten, die derzeit unter dem menschlichen Expertenniveau in modernen Allzweck-KI-Systemen liegen, nach Typ gruppiert. Die dritte Spalte fasst den aktuellen Status zusammen. Die letzte Spalte zeigt die vorhergesagte Wahrscheinlichkeit (%), dass menschliches Leistungsniveau erreicht wird durch: Skalierung aktueller Techniken / Kombination mit bekannten Techniken / Entwicklung neuer Techniken. Diese Fähigkeiten sind nicht unabhängig, und eine Steigerung bei einer geht typischerweise mit Steigerungen bei anderen einher. Zu beachten ist, dass nicht alle (insbesondere Empfindungsfähigkeit) für KI-Systeme notwendig sind, die zur Weiterentwicklung der KI-Entwicklung fähig sind, was die Möglichkeit mächtiger, aber nicht empfindungsfähiger KI unterstreicht.

Diese Aufschlüsselung dessen, was „fehlt", macht ziemlich deutlich, dass wir durchaus auf dem Weg zu breit übermenschlicher Intelligenz durch Skalierung existierender oder bekannter Techniken sind.[^39]

Es könnte trotzdem Überraschungen geben. Selbst wenn man „Empfindungsfähigkeit" außer Acht lässt, könnte es einige der aufgelisteten zentralen kognitiven Fähigkeiten geben, die wirklich nicht mit aktuellen Techniken erreicht werden können und neue erfordern. Aber bedenken Sie Folgendes: Der gegenwärtige Aufwand, den viele der weltgrößten Unternehmen betreiben, entspricht einem Vielfachen der Ausgaben des Apollo-Projekts und dem Zehnfachen der des Manhattan-Projekts,[^40] und beschäftigt Tausende der besten Techniker zu unerhörten Gehältern. Die Dynamik der vergangenen Jahre hat nun mehr menschliche intellektuelle Kraft (mit KI als neuer Ergänzung) darauf konzentriert als jedes Unterfangen in der Geschichte. Wir sollten nicht auf Scheitern setzen.

### Das große Ziel: generalistische autonome Agenten

Die Entwicklung allgemeiner KI in den vergangenen Jahren konzentrierte sich darauf, allgemeine und mächtige, aber werkzeugähnliche KI zu schaffen: Sie funktioniert primär als (ziemlich) treuer Assistent und führt im Allgemeinen keine Handlungen eigenständig aus. Das ist teils beabsichtigt, liegt aber größtenteils daran, dass diese Systeme bei den relevanten Fähigkeiten einfach nicht kompetent genug waren, um mit komplexen Handlungen betraut zu werden.[^41]

KI-Unternehmen und Forscher [verlagern ihren Fokus](https://www.axios.com/2025/01/23/davos-2025-ai-agents) jedoch zunehmend auf *autonome* allgemeine Agenten auf Expertenebene.[^42] Das würde den Systemen ermöglichen, eher wie ein menschlicher Assistent zu agieren, dem der Nutzer echte Handlungen delegieren kann.[^43] Was wird das erfordern? Eine Reihe der Fähigkeiten aus der „Was fehlt"-Tabelle sind beteiligt, einschließlich starker Wahrheitsverankerung, Lernen und Gedächtnis, Abstraktion und Rekursion sowie Weltmodellierung (für Intelligenz), Planung, Handlungsfähigkeit, Originalität, Selbststeuerung, Selbstbezug und Selbstbewusstsein (für Autonomie) sowie multisensorische Verarbeitung, verkörperte Intelligenz und flexible Problemlösung (für Generalität).[^44]

Diese dreifache Schnittmenge aus hoher Autonomie (Handlungsunabhängigkeit), hoher Generalität (Umfang und Aufgabenbreite) und hoher Intelligenz (Kompetenz bei kognitiven Aufgaben) ist derzeit einzigartig menschlich. Sie ist implizit das, was viele wahrscheinlich im Sinn haben, wenn sie an AGI denken – sowohl hinsichtlich ihres Werts als auch ihrer Risiken.

Das bietet eine andere Möglichkeit, A-G-I als ***A*** utonome- ***G*** enerelle- ***I*** ntelligenz zu definieren, und wir werden sehen, dass diese dreifache Schnittmenge eine sehr wertvolle Brille für hochfähige Systeme bietet, sowohl zum Verständnis ihrer Risiken und Chancen als auch für die Governance von KI.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Simple-1024x1024.png&w=3840&q=75) Die transformative A-G-I-Macht- und Risikozone entsteht aus der Schnittmenge dreier Schlüsseleigenschaften: hohe Autonomie, hohe Intelligenz bei Aufgaben und hohe Generalität.

### Der KI-(Selbst-)Verbesserungszyklus

Ein letzter entscheidender Faktor zum Verständnis des KI-Fortschritts ist KIs einzigartiger technologischer Rückkopplungskreislauf. Bei der KI-Entwicklung bringt Erfolg – sowohl bei demonstrierten Systemen als auch bei eingesetzten Produkten – zusätzliche Investitionen, Talente und Wettbewerb mit sich, und wir befinden uns derzeit inmitten einer enormen KI-Hype-plus-Realitäts-Rückkopplungsschleife, die Hunderte von Milliarden oder sogar Billionen von Dollars an Investitionen antreibt.

Diese Art von Rückkopplungszyklus könnte bei jeder Technologie auftreten, und wir haben ihn bei vielen gesehen, wo Markterfolg Investitionen hervorbringt, die Verbesserung und besseren Markterfolg zur Folge haben. Aber KI-Entwicklung geht weiter, insofern nun KI-Systeme dabei helfen, neue und mächtigere KI-Systeme zu entwickeln.[^45] Wir können uns diesen Rückkopplungskreislauf in fünf Stufen vorstellen, jede mit kürzerer Zeitskala als die letzte, wie in der Tabelle gezeigt.

*Der KI-Verbesserungszyklus operiert über mehrere Zeitskalen hinweg, wobei jede Stufe möglicherweise nachfolgende Stufen beschleunigt. Frühere Stufen sind bereits im Gange, während spätere Stufen spekulativ bleiben, aber sehr schnell voranschreiten könnten, sobald sie freigesetzt werden.*

Mehrere dieser Stufen sind bereits im Gange, und ein paar beginnen eindeutig. Die letzte Stufe, in der KI-Systeme sich autonom selbst verbessern, ist ein Grundpfeiler der Literatur über das Risiko sehr mächtiger KI-Systeme, und das aus gutem Grund.[^46] Aber es ist wichtig zu bemerken, dass es nur die drastischste Form eines Rückkopplungszyklus ist, der bereits begonnen hat und zu mehr Überraschungen bei der raschen Weiterentwicklung der Technologie führen könnte.


[^31]: Sie nutzen deutlich mehr von dieser KI, als Sie wahrscheinlich denken: für Sprachgenerierung und -erkennung, Bildverarbeitung, Newsfeed-Algorithmen usw.

[^32]: Während die Beziehungen zwischen diesen Unternehmenspaaren ziemlich komplex und nuanciert sind, habe ich sie explizit aufgelistet, um sowohl die enorme Gesamtmarktkapitalisierung der Firmen zu zeigen, die nun in der KI-Entwicklung engagiert sind, als auch dass hinter sogar „kleineren" Unternehmen wie Anthropic enorm tiefe Taschen durch Investitionen und große Partnerschaftsverträge stehen.

[^33]: Es ist Mode geworden, den Turing-Test zu verunglimpfen, aber er ist durchaus mächtig und allgemein. In schwachen Versionen zeigt er an, ob typische Menschen, die mit einer KI (die darauf trainiert ist, menschlich zu agieren) auf typische Weise für kurze Zeiträume interagieren, erkennen können, ob es eine KI ist. Sie können es nicht. Zweitens kann ein hochgradig adversarieller Turing-Test praktisch jedes Element menschlicher Fähigkeit und Intelligenz prüfen – etwa durch Vergleich eines KI-Systems mit einem menschlichen Experten, bewertet von anderen menschlichen Experten. In gewissem Sinne ist ein Großteil der KI-Bewertung eine verallgemeinerte Form des Turing-Tests.

[^34]: Das gilt pro Bereich – kein Mensch könnte plausibel solche Punktzahlen in allen Fächern gleichzeitig erreichen.

[^35]: Das sind Probleme, für die selbst exzellente Mathematiker erhebliche Zeit bräuchten, wenn sie sie überhaupt lösen könnten.

[^36]: Wenn Sie skeptisch veranlagt sind, behalten Sie Ihre Skepsis bei, aber probieren Sie wirklich die aktuellsten Modelle aus und versuchen Sie selbst einige der Testfragen, die sie bestehen können. Als Physikprofessor würde ich mit nahezu Gewissheit vorhersagen, dass zum Beispiel die Top-Modelle die Graduiertenprüfung in unserem Fachbereich bestehen würden.

[^37]: Das und andere Schwächen wie Konfabulation haben die Markteinführung verlangsamt und zu einer Lücke zwischen wahrgenommenen und behaupteten Fähigkeiten geführt (was auch durch die Brille intensiven Marktwettbewerbs und der Notwendigkeit, Investitionen anzuziehen, betrachtet werden muss). Das hat sowohl die Öffentlichkeit als auch Politiker über den tatsächlichen Stand des KI-Fortschritts verwirrt. Auch wenn er vielleicht nicht dem Hype entspricht, ist der Fortschritt sehr real.

[^38]: Der größte Fortschritt seither war die Entwicklung von Systemen, die für hochqualitatives Reasoning trainiert sind, unter Nutzung von mehr Rechenleistung während der Inferenz und größerem Verstärkungslernen. Da diese Modelle neu sind und ihre Fähigkeiten weniger getestet, habe ich diese Tabelle nicht völlig überarbeitet, außer bei „Reasoning", das ich als praktisch gelöst betrachte. Aber ich habe Vorhersagen basierend auf erfahrenen und berichteten Fähigkeiten dieser Systeme aktualisiert.

[^39]: Frühere Wellen des KI-Optimismus in den 1960ern und 1980ern endeten in „KI-Wintern", als versprochene Fähigkeiten nicht materialisiert wurden. Die aktuelle Welle unterscheidet sich jedoch fundamental dadurch, dass sie übermenschliche Leistung in vielen Bereichen erreicht hat, unterstützt von massiven Rechenressourcen und kommerziellem Erfolg.

[^40]: Das vollständige Apollo-Projekt [kostete etwa 250 Milliarden USD in 2020-Dollars](https://www.planetary.org/space-policy/cost-of-apollo), und das Manhattan-Projekt [weniger als ein Zehntel davon](https://www.brookings.edu/the-costs-of-the-manhattan-project/). Goldman Sachs [projiziert eine Billion Dollar Ausgaben allein für KI-Rechenzentren](https://www.datacenterdynamics.com/en/news/goldman-sachs-1tn-to-be-spent-on-ai-data-centers-chips-and-utility-upgrades-with-little-to-show-for-it-so-far/) in den nächsten Jahren.

[^41]: Obwohl Menschen viele Fehler machen, unterschätzen wir, wie zuverlässig wir sein können! Da sich Wahrscheinlichkeiten multiplizieren, erfordert eine Aufgabe mit 20 korrekt auszuführenden Schritten, dass jeder Schritt zu 97% zuverlässig ist, nur um sie in der Hälfte der Zeit richtig zu erledigen. Wir machen solche Aufgaben ständig.

[^42]: Ein starker Schritt in diese Richtung wurde kürzlich mit OpenAIs [„Deep Research"](https://openai.com/index/introducing-deep-research/) unternommen, einem Assistenten, der autonom allgemeine Forschung durchführt, beschrieben als „eine neue agentische Fähigkeit, die mehrstufige Internetforschung für komplexe Aufgaben durchführt."

[^43]: Dinge wie dieses lästige PDF-Formular ausfüllen, Flüge buchen usw. Aber mit einem PhD in 20 Fächern! Also auch: diese Dissertation für Sie schreiben, diesen Vertrag für Sie verhandeln, diesen Satz für Sie beweisen, diese Werbekampagne für Sie erstellen usw. Was machen *Sie*? Sie sagen ihm, was zu tun ist, natürlich.

[^44]: Zu beachten ist, dass Empfindungsfähigkeit *nicht* eindeutig erforderlich ist, noch impliziert KI in dieser dreifachen Schnittmenge notwendigerweise diese.

[^45]: Die nächste Analogie hier ist vielleicht die Chip-Technologie, wo die Entwicklung das Moore'sche Gesetz jahrzehntelang aufrechterhalten hat, da Computertechnologien Menschen dabei helfen, die nächste Generation von Chip-Technologie zu entwerfen. Aber KI wird weit direkter sein.

[^46]: Es ist wichtig, es einen Moment auf sich wirken zu lassen, dass KI bald sich selbst auf einer Zeitskala von Tagen oder Wochen verbessern könnte. Oder weniger. Behalten Sie das im Hinterkopf, wenn Ihnen jemand sagt, eine KI-Fähigkeit ist definitiv weit entfernt.

## Kapitel 6 - Das Rennen um die AGI

Was sind die treibenden Kräfte hinter dem Rennen um die Entwicklung von AGI, sowohl für Unternehmen als auch für Länder?

Die jüngsten rasanten Fortschritte in der KI haben zu einem außergewöhnlichen Maß an Aufmerksamkeit und Investitionen geführt – und sind gleichzeitig daraus entstanden. Dies wird teilweise durch die Erfolge in der KI-Entwicklung angetrieben, aber es steckt mehr dahinter. Warum wetteifern einige der größten Unternehmen der Welt und sogar ganze Länder darum, nicht nur KI zu entwickeln, sondern AGI und Superintelligenz?

### Was die KI-Forschung in Richtung menschlicher Intelligenz getrieben hat

Bis vor etwa fünf Jahren war KI größtenteils ein akademisches und wissenschaftliches Forschungsproblem, das hauptsächlich von Neugier und dem Drang angetrieben wurde, Intelligenz zu verstehen und sie in einem neuen Substrat zu erschaffen.

In dieser Phase schenkten die meisten Forscher den Vorteilen oder Gefahren von KI relativ wenig Aufmerksamkeit. Auf die Frage, warum KI entwickelt werden sollte, könnte eine typische Antwort darin bestehen, etwas vage Probleme aufzulisten, bei denen KI helfen könnte: neue Medikamente, neue Materialien, neue Wissenschaft, intelligentere Prozesse und allgemein die Verbesserung der Dinge für die Menschen.[^47]

Das sind bewundernswerte Ziele![^48] Obwohl wir hinterfragen können und werden, ob AGI – anstelle von KI im Allgemeinen – für diese Ziele notwendig ist, zeigen sie den Idealismus, mit dem viele KI-Forscher anfingen.

In den letzten fünf Jahren hat sich KI jedoch von einem relativ reinen Forschungsgebiet zu einem stärker ingenieur- und produktorientierten Bereich entwickelt, der größtenteils von einigen der weltgrößten Unternehmen vorangetrieben wird.[^49] Forscher sind zwar noch relevant, haben aber die Kontrolle über den Prozess verloren.

### Warum versuchen Unternehmen, AGI zu entwickeln?

Warum investieren also Großkonzerne (und noch mehr Investoren) riesige Ressourcen in die Entwicklung von AGI? Es gibt zwei Triebkräfte, über die die meisten Unternehmen ganz offen sprechen: Sie sehen KI als Motor für gesellschaftliche Produktivität und für ihre eigenen Gewinne. Da allgemeine KI naturgemäß vielseitig einsetzbar ist, lockt ein enormer Preis: Anstatt einen Sektor zu wählen, in dem man Produkte und Dienstleistungen entwickelt, kann man *alle auf einmal* angehen. Big-Tech-Unternehmen sind durch die Produktion digitaler Güter und Dienstleistungen riesig geworden, und zumindest einige Führungskräfte sehen KI sicherlich als den nächsten Schritt, um diese bereitzustellen – mit Risiken und Vorteilen, die denen von Suchmaschinen, sozialen Medien, Laptops, Telefonen usw. ähneln, aber über sie hinausgehen.

Aber warum AGI? Darauf gibt es eine sehr einfache Antwort, die die meisten Unternehmen und Investoren scheuen, öffentlich zu diskutieren.[^50]

AGI kann direkt, eins zu eins, *Arbeitnehmer ersetzen.*

Nicht unterstützen, nicht stärken, nicht produktiver machen. Nicht einmal *verdrängen*. All das kann und wird von Nicht-AGI geleistet werden. AGI ist spezifisch das, was Denkarbeiter vollständig *ersetzen* kann (und mit Robotik auch viele körperlich Arbeitende). Als Beleg für diese Sichtweise braucht man nur OpenAIs [(öffentlich erklärte) Definition](https://openai.com/our-structure/) von AGI zu betrachten, die lautet: "ein hochautonomes System, das Menschen bei den meisten wirtschaftlich wertvollen Arbeiten übertrifft."

Der Preis hierfür (für Unternehmen!) ist enorm. Arbeitskosten machen einen erheblichen Prozentsatz der weltweiten ∼100 Billionen Dollar Weltwirtschaft aus. Selbst wenn nur ein Bruchteil davon durch die Ersetzung menschlicher Arbeit durch KI-Arbeit erfasst wird, sprechen wir von Billionen von Dollar jährlicher Einnahmen. KI-Unternehmen sind sich auch bewusst, wer bereit ist zu zahlen. Wie sie es sehen: Sie werden nicht Tausende von Dollar pro Jahr für Produktivitätstools zahlen. Aber ein Unternehmen *wird* Tausende von Dollar pro Jahr zahlen, um Ihre Arbeitskraft zu ersetzen, wenn es das kann.

### Warum sich Länder gedrängt sehen, um die AGI zu wetteifern

Die erklärten Motivationen der Länder für die Verfolgung von AGI konzentrieren sich auf wirtschaftliche und wissenschaftliche Führerschaft. Das Argument ist überzeugend: AGI könnte die wissenschaftliche Forschung, technologische Entwicklung und das Wirtschaftswachstum dramatisch beschleunigen. Angesichts dieser Bedeutung, so argumentieren sie, kann es sich keine Großmacht leisten, zurückzufallen.[^51]

Aber es gibt auch zusätzliche und größtenteils unausgesprochene Triebkräfte. Es besteht kein Zweifel, dass wenn bestimmte Militär- und nationale Sicherheitsführer sich hinter verschlossenen Türen treffen, um eine außerordentlich mächtige und katastrophal riskante Technologie zu diskutieren, ihr Fokus nicht auf "Wie vermeiden wir diese Risiken?" liegt, sondern auf "Wie bekommen wir das zuerst?" Militär- und Geheimdienstführer sehen AGI als potenzielle Revolution in militärischen Angelegenheiten, vielleicht die bedeutendste seit Atomwaffen. Die Befürchtung ist, dass das erste Land, das AGI entwickelt, einen uneinholbaren strategischen Vorteil erlangen könnte. Dies schafft eine klassische Rüstungswettlauf-Dynamik.

Wir werden sehen, dass dieses "Rennen um AGI"-Denken,[^52] obwohl überzeugend, zutiefst fehlerhaft ist. Das liegt nicht daran, dass das Rennen gefährlich und riskant ist – obwohl es das ist –, sondern an der Natur der Technologie. Die unausgesprochene Annahme ist, dass AGI, wie andere Technologien, vom Staat kontrollierbar ist, der sie entwickelt, und eine machtverleihendes Segen für die Gesellschaft ist, die am meisten davon hat. Wie wir sehen werden, wird sie wahrscheinlich beides nicht sein.

### Warum Superintelligenz?

Während Unternehmen öffentlich auf Produktivität fokussieren und Länder auf wirtschaftliches und technologisches Wachstum, sind das für diejenigen, die bewusst vollständige AGI und Superintelligenz anstreben, nur der Anfang. Was haben sie wirklich im Sinn? Obwohl selten laut ausgesprochen, gehören dazu:

1. Heilmittel für viele oder alle Krankheiten;
2. Stoppen und Umkehrung des Alterns;
3. Neue nachhaltige Energiequellen wie Kernfusion;
4. Menschliche Verbesserungen oder maßgeschneiderte Organismen durch Gentechnik;
5. Nanotechnologie und molekulare Fertigung;
6. Geist-Uploads;
7. Exotische Physik oder Weltraumtechnologien;
8. Übermenschliche Beratung und Entscheidungsunterstützung;
9. Übermenschliche Planung und Koordination.

Die ersten drei sind größtenteils "einseitige" Technologien – d.h. wahrscheinlich ziemlich stark netto-positiv. Es ist schwer zu argumentieren gegen die Heilung von Krankheiten oder die Möglichkeit, länger zu leben, wenn man es wählt. Und wir haben bereits die negative Seite der Fusion geerntet (in Form von Atomwaffen); es wäre schön, jetzt die positive Seite zu bekommen. Die Frage bei dieser ersten Kategorie ist, ob das frühere Erhalten dieser Technologien das Risiko kompensiert.

Die nächsten vier sind eindeutig zweischneidig: transformative Technologien mit sowohl potenziell enormen Vorteilen als auch immensen Risiken, ganz wie KI. Alle diese wären, wenn sie morgen aus einer Black Box heraussprängen und eingesetzt würden, unglaublich schwer zu handhaben.[^53]

Die letzten beiden betreffen die übermenschliche KI, die Dinge selbst macht, anstatt nur Technologie zu erfinden. Präziser gesagt, Euphemismen beiseite, beinhalten diese mächtige KI-Systeme, die Menschen sagen, was sie tun sollen. Dies "Beratung" zu nennen ist unaufrichtig, wenn das System, das berät, weitaus mächtiger ist als der Beratene, der die Grundlage der Entscheidung nicht sinnvoll verstehen kann (oder selbst wenn diese bereitgestellt wird, nicht darauf vertrauen kann, dass der Berater nicht eine ähnlich überzeugende Begründung für eine andere Entscheidung liefern würde.)

Dies zeigt auf einen wichtigen Punkt, der in der obigen Liste fehlt:

10. Macht.

Es ist völlig klar, dass dem aktuellen Rennen um übermenschliche KI größtenteils die Idee zugrunde liegt, dass *Intelligenz = Macht*. Jeder Teilnehmer des Rennens setzt darauf, der beste Inhaber dieser Macht zu sein und sie aus angeblich wohlwollenden Gründen einsetzen zu können, ohne dass sie ihrer Kontrolle entgleitet oder ihnen entrissen wird.

Das heißt, was Unternehmen und Nationen wirklich verfolgen, sind nicht nur die Früchte von AGI und Superintelligenz, sondern die Macht zu kontrollieren, wer Zugang zu ihnen bekommt und wie sie verwendet werden. Unternehmen sehen sich als verantwortliche Verwalter dieser Macht im Dienste der Aktionäre und der Menschheit; Nationen sehen sich als notwendige Wächter, die feindliche Mächte daran hindern, einen entscheidenden Vorteil zu erlangen. Beide liegen gefährlich falsch und erkennen nicht, dass Superintelligenz ihrer Natur nach nicht zuverlässig von einer menschlichen Institution kontrolliert werden kann. Wir werden sehen, dass die Natur und Dynamik superintelligenter Systeme menschliche Kontrolle extrem schwierig, wenn nicht unmöglich macht.

Diese Wettlaufdynamiken – sowohl unternehmerische als auch geopolitische – machen bestimmte Risiken nahezu unvermeidlich, es sei denn, sie werden entschieden unterbrochen. Wir wenden uns nun der Untersuchung dieser Risiken zu und warum sie in einem kompetitiven[^54] Entwicklungsparadigma nicht angemessen gemildert werden können.


[^47]: Eine präzisere Liste würdiger Ziele sind die [Nachhaltigen Entwicklungsziele](https://sdgs.un.org/goals) der UN. Diese sind gewissermaßen das Nächste, was wir zu einer Reihe globaler Konsensziele dafür haben, was wir in der Welt verbessert sehen möchten. KI könnte helfen.

[^48]: Technologie im Allgemeinen hat eine transformative wirtschaftliche und soziale Kraft für menschliche Verbesserung, wie Tausende von Jahren bezeugen. In diesem Sinne findet sich eine lange und überzeugende Darlegung einer positiven AGI-Vision in [diesem Essay](https://darioamodei.com/machines-of-loving-grace) von Anthropic-Gründer Dario Amodei.

[^49]: Private KI-Investitionen [begannen 2018-19 zu boomen, überholten öffentliche Investitionen etwa zu dieser Zeit](https://cset.georgetown.edu/publication/tracking-ai-investment/) und haben sie seitdem bei weitem übertroffen.

[^50]: Ich kann bezeugen, dass sie hinter verschlosseneren Türen keine solche Zurückhaltung haben. Und es wird öffentlicher; siehe zum Beispiel Y-combinators neue ["Request for Startups"](https://www.ycombinator.com/rfs), deren viele Teile explizit nach der vollständigen Ersetzung menschlicher Arbeiter verlangen. Um sie zu zitieren: "Das Wertversprechen von B2B SaaS war, menschliche Arbeiter schrittweise effizienter zu machen. Das Wertversprechen von vertikalen KI-Agenten ist, die Arbeit vollständig zu automatisieren... Es ist durchaus möglich, dass diese Gelegenheit groß genug ist, weitere 100 Einhörner zu prägen." (Für diejenigen, die nicht im Silicon-Valley-Sprech bewandert sind: "B2B" steht für Business-to-Business und ein Einhorn ist ein Unternehmen im Wert von 1 Milliarde Dollar. Das heißt, sie sprechen von mehr als hundert Milliarden-Plus-Dollar-Unternehmen, die Arbeiter für andere Unternehmen ersetzen.)

[^51]: Siehe zum Beispiel einen aktuellen [Bericht der US-China Economic and Security Review Commission](https://www.uscc.gov/sites/default/files/2024-11/2024_Executive_Summary.pdf). Obwohl es überraschend wenig Rechtfertigung innerhalb des Berichts selbst gab, war die Hauptempfehlung, dass der Kongress "ein Manhattan-Projekt-ähnliches Programm etablieren und finanzieren sollte, das darauf ausgerichtet ist, um eine Artificial General Intelligence (AGI)-Fähigkeit zu rennen und sie zu erlangen."

[^52]: Unternehmen übernehmen jetzt diese geopolitische Rahmung als Schild gegen jede Beschränkung ihrer KI-Entwicklung, im Allgemeinen auf Weise, die offensichtlich eigennützig sind, und manchmal auf Weise, die nicht einmal grundlegend Sinn ergeben. Betrachten Sie Metas [Approach to Frontier AI](https://about.fb.com/news/2025/02/meta-approach-frontier-ai/), der gleichzeitig argumentiert, dass Amerika seine "Position als Führer in technologischer Innovation, Wirtschaftswachstum und nationaler Sicherheit zementieren" muss und auch, dass es dies tun muss, indem es seine mächtigsten KI-Systeme offen freigibt – was einschließt, sie direkt an seine geopolitischen Rivalen und Gegner zu geben.

[^53]: Daher müssten wir wahrscheinlich das Management dieser Technologien den KIs überlassen. Aber das wäre eine sehr problematische Delegierung der Kontrolle, auf die wir unten zurückkommen werden.

[^54]: Wettbewerb in der Technologieentwicklung bringt oft wichtige Vorteile mit sich: Verhinderung monopolistischer Kontrolle, Antrieb von Innovation und Kostenreduktion, Ermöglichung vielfältiger Ansätze und Schaffung gegenseitiger Aufsicht. Bei AGI müssen diese Vorteile jedoch gegen einzigartige Risiken aus Rennlauf-Dynamiken und Druck zur Reduzierung von Sicherheitsvorkehrungen abgewogen werden.

## Kapitel 7 - Was passiert, wenn wir AGI auf unserem aktuellen Pfad entwickeln?

Die Gesellschaft ist nicht bereit für AGI-Systeme. Wenn wir sie sehr bald entwickeln, könnte es hässlich werden.

Die Entwicklung einer vollständigen künstlichen allgemeinen Intelligenz – die wir hier als KI bezeichnen werden, die „außerhalb der Tore" steht – wäre ein fundamentaler Wandel in der Natur der Welt: Sie bedeutet ihrer Natur nach, dass wir der Erde eine neue Spezies von Intelligenz hinzufügen, die über größere Fähigkeiten verfügt als die Menschen.

Was dann geschieht, hängt von vielen Faktoren ab, einschließlich der Natur der Technologie, den Entscheidungen ihrer Entwickler und dem weltweiten Kontext, in dem sie entwickelt wird.

Gegenwärtig wird vollständige AGI von einer Handvoll massiver Privatunternehmen in einem Wettrennen untereinander entwickelt, mit wenig bedeutungsvoller Regulierung oder externer Aufsicht,[^55] in einer Gesellschaft mit zunehmend schwachen und sogar dysfunktionalen Kerninstitutionen,[^56] in einer Zeit hoher geopolitischer Spannungen und geringer internationaler Koordination. Obwohl einige altruistisch motiviert sind, werden viele der Entwickler von Geld, Macht oder beidem angetrieben.

Vorhersagen sind sehr schwierig, aber es gibt einige Dynamiken, die gut genug verstanden sind, und treffende Analogien zu früheren Technologien, die als Leitfaden dienen können. Und leider geben sie trotz der Verheißungen der KI guten Grund zu tiefem Pessimismus darüber, wie sich unsere gegenwärtige Entwicklung entfalten wird.

Um es deutlich zu sagen: Auf unserem gegenwärtigen Kurs wird die Entwicklung von AGI zwar einige positive Effekte haben (und einige Menschen sehr, sehr reich machen). Aber die Natur der Technologie, die fundamentalen Dynamiken und der Kontext, in dem sie entwickelt wird, deuten stark darauf hin, dass: mächtige KI unsere Gesellschaft und Zivilisation dramatisch untergraben wird; wir die Kontrolle über sie verlieren werden; wir durchaus in einem Weltkrieg wegen ihr enden könnten; wir die Kontrolle *an* sie verlieren (oder abgeben) werden; sie zu künstlicher Superintelligenz führen wird, die wir absolut nicht kontrollieren werden und die das Ende einer von Menschen geführten Welt bedeuten wird.

Das sind starke Behauptungen, und ich wünschte, sie wären müßige Spekulationen oder unbegründeter „Untergangspessimismus". Aber hier weisen die Wissenschaft, die Spieltheorie, die Evolutionstheorie und die Geschichte alle hin. Dieser Abschnitt entwickelt diese Behauptungen und ihre Belege im Detail.

### Wir werden unsere Gesellschaft und Zivilisation untergraben

Entgegen dem, was Sie in den Vorstandsetagen des Silicon Valley hören mögen, ist die meiste Disruption – besonders die sehr schnelle Variante – nicht vorteilhaft. Es gibt weitaus mehr Wege, komplexe Systeme zu verschlechtern als zu verbessern. Unsere Welt funktioniert so gut, wie sie es tut, weil wir mühsam Prozesse, Technologien und Institutionen aufgebaut haben, die sie stetig verbessert haben.[^57] Mit einem Vorschlaghammer auf eine Fabrik einzuschlagen verbessert selten den Betrieb.

Hier ist ein (unvollständiger) Katalog der Arten, wie AGI-Systeme unsere Zivilisation stören würden.

- Sie würden die Arbeitswelt dramatisch stören und *mindestens* zu dramatisch höherer Einkommensungleichheit und möglicherweise großflächiger Unter- oder Arbeitslosigkeit führen, in einem Zeitrahmen, der viel zu kurz für gesellschaftliche Anpassungen ist.[^58]
- Sie würden wahrscheinlich zur Konzentration enormer wirtschaftlicher, sozialer und politischer Macht führen – möglicherweise mehr als die von Nationalstaaten – in eine kleine Anzahl massiver privater Interessen, die der Öffentlichkeit nicht rechenschaftspflichtig sind.
- Sie könnten zuvor schwierige oder teure Aktivitäten plötzlich trivial einfach machen und dadurch soziale Systeme destabilisieren, die darauf angewiesen sind, dass bestimmte Aktivitäten kostspielig bleiben oder erhebliche menschliche Anstrengung erfordern.[^59]
- Sie könnten die Informationssammlung, -verarbeitung und Kommunikationssysteme der Gesellschaft so gründlich mit völlig realistischen, aber falschen, Spam-, überzielgerichteten oder manipulativen Medien überfluten, dass es unmöglich wird zu erkennen, was physisch real ist oder nicht, menschlich oder nicht, faktisch oder nicht, und vertrauenswürdig oder nicht.[^60]
- Sie könnten gefährliche und nahezu totale intellektuelle Abhängigkeit schaffen, wo menschliches Verständnis von Schlüsselsystemen und -technologien verkümmert, während wir uns zunehmend auf KI-Systeme verlassen, die wir nicht vollständig verstehen können.
- Sie könnten die menschliche Kultur effektiv beenden, sobald nahezu alle kulturellen Objekte (Text, Musik, visuelle Kunst, Film, etc.), die von den meisten Menschen konsumiert werden, von nicht-menschlichen Geistern erstellt, vermittelt oder kuratiert werden.
- Sie könnten effektive Massenüberwachungs- und Manipulationssysteme ermöglichen, die von Regierungen oder privaten Interessen genutzt werden können, um eine Bevölkerung zu kontrollieren und Ziele zu verfolgen, die im Widerspruch zum öffentlichen Interesse stehen.
- Indem sie den menschlichen Diskurs, die Debatte und Wahlsysteme untergraben, könnten sie die Glaubwürdigkeit demokratischer Institutionen bis zu dem Punkt reduzieren, wo sie effektiv (oder explizit) durch andere ersetzt werden, was die Demokratie in Staaten beendet, wo sie derzeit existiert.
- Sie könnten zu fortgeschrittenen, sich selbst replizierenden intelligenten Software-Viren und -Würmern werden oder solche erschaffen, die sich ausbreiten und entwickeln könnten und globale Informationssysteme massiv stören.
- Sie können die Fähigkeit von Terroristen, schlechten Akteuren und Schurkenstaaten, Schäden durch biologische, chemische, Cyber-, autonome oder andere Waffen zu verursachen, dramatisch erhöhen, ohne dass KI eine ausgleichende Fähigkeit zur Verhinderung solcher Schäden bietet. Ähnlich würden sie die nationale Sicherheit und geopolitische Gleichgewichte untergraben, indem sie Spitzenexpertise in Nuklear-, Bio-, Ingenieur- und anderen Bereichen für Regime verfügbar machen, die sie anderweitig nicht hätten.
- Sie könnten schnellen, großflächigen ausufernden Hyperkapitalismus verursachen, mit effektiv KI-geführten Unternehmen, die in weitgehend elektronischen Finanz-, Verkaufs- und Dienstleistungsbereichen konkurrieren. KI-getriebene Finanzmärkte könnten mit Geschwindigkeiten und Komplexitäten operieren, die weit jenseits menschlichen Verständnisses oder Kontrolle liegen. Alle Versagensmodi und negativen Externalitäten gegenwärtiger kapitalistischer Ökonomien könnten verschärft und weit über menschliche Kontrolle, Governance oder Regulierungsfähigkeit hinaus beschleunigt werden.
- Sie könnten ein Wettrüsten zwischen Nationen bei KI-gestützten Waffen, Kommando- und Kontrollsystemen, Cyberwaffen usw. anheizen und sehr schnelle Aufrüstung extrem destruktiver Fähigkeiten schaffen.

Diese Risiken sind nicht spekulativ. Viele von ihnen werden bereits durch existierende KI-Systeme realisiert! Aber bedenken Sie, *wirklich* bedenken Sie, wie jedes mit dramatisch mächtigerer KI aussehen würde.

Bedenken Sie Arbeitsplatzverdrängung, wenn die meisten Arbeiter einfach keinen signifikanten wirtschaftlichen Wert mehr bieten können, der über das hinausgeht, was KI in ihrem Fachgebiet oder ihrer Erfahrung leisten kann – oder selbst wenn sie sich umschulen! Bedenken Sie Massenüberwachung, wenn jeder individuell von etwas beobachtet und überwacht wird, das schneller und klüger ist als er selbst. Wie sieht Demokratie aus, wenn wir digitalen Informationen, die wir sehen, hören oder lesen, nicht zuverlässig vertrauen können, und wenn die überzeugendsten öffentlichen Stimmen nicht einmal menschlich sind und keinen Anteil am Ausgang haben? Was wird aus der Kriegsführung, wenn Generäle ständig an KI delegieren müssen (oder sie einfach das Kommando übernehmen lassen), um dem Feind nicht einen entscheidenden Vorteil zu gewähren? Jedes der oben genannten Risiken stellt eine Katastrophe für die menschliche[^61] Zivilisation dar, wenn es vollständig realisiert wird.

Sie können Ihre eigenen Vorhersagen treffen. Stellen Sie sich diese drei Fragen für jedes Risiko:

1. Würde super-fähige, hochautonome und sehr allgemeine KI es auf eine Weise oder in einem Ausmaß ermöglichen, das anderweitig nicht möglich wäre?
2. Gibt es Parteien, die von Dingen profitieren würden, die dazu führen, dass es geschieht?
3. Gibt es Systeme und Institutionen, die es effektiv verhindern würden?

Wo Ihre Antworten „ja, ja, nein" lauten, können Sie sehen, dass wir ein großes Problem haben.

Was ist unser Plan für deren Bewältigung? Derzeit stehen zwei bezüglich KI im Allgemeinen auf dem Tisch.

Der erste ist, Schutzmaßnahmen in die Systeme einzubauen, um sie daran zu hindern, Dinge zu tun, die sie nicht tun sollten. Das wird jetzt gemacht: Kommerzielle KI-Systeme werden zum Beispiel sich weigern, beim Bombenbau zu helfen oder Hassreden zu schreiben.

Dieser Plan ist völlig unzureichend für Systeme außerhalb der Tore.[^62] Er mag helfen, das Risiko zu verringern, dass KI offensichtlich gefährliche Unterstützung für schlechte Akteure bietet. Aber er wird nichts tun, um Arbeitsplatzstörungen, Machtkonzentration, ausufernden Hyperkapitalismus oder den Ersatz menschlicher Kultur zu verhindern: Das sind einfach Ergebnisse der Nutzung der Systeme auf erlaubte Weise, die ihren Anbietern Profit bringen! Und Regierungen werden sicherlich Zugang zu Systemen für militärische oder Überwachungszwecke erhalten.

Der zweite Plan ist noch schlimmer: einfach sehr mächtige KI-Systeme offen für jeden zur Nutzung freizugeben,[^63] und auf das Beste zu hoffen.

Implizit in beiden Plänen ist, dass jemand anderes, z.B. Regierungen, dabei helfen wird, die Probleme durch weiches oder hartes Recht, Standards, Regulierungen, Normen und andere Mechanismen zu lösen, die wir allgemein zur Verwaltung von Technologien verwenden.[^64] Aber abgesehen davon, dass KI-Unternehmen bereits mit Händen und Füßen gegen jede substanzielle Regulierung oder extern auferlegte Beschränkungen überhaupt kämpfen, ist es für eine Reihe dieser Risiken ziemlich schwer zu sehen, was für eine Regulierung überhaupt wirklich helfen würde. Regulierung könnte Sicherheitsstandards für KI auferlegen. Aber würde sie Unternehmen daran hindern, Arbeiter großflächig durch KI zu ersetzen? Würde sie Menschen daran hindern, KI ihre Unternehmen für sie führen zu lassen? Würde sie Regierungen daran hindern, mächtige KI in Überwachung und Bewaffnung zu nutzen? Diese Probleme sind fundamental. Die Menschheit könnte potentiell Wege finden, sich an sie anzupassen, aber nur mit *viel* mehr Zeit. So wie es ist, angesichts der Geschwindigkeit, mit der KI die Fähigkeiten der Menschen erreicht oder übertrifft, die versuchen, sie zu verwalten, sehen diese Probleme zunehmend unlösbar aus.

### Wir werden die Kontrolle über (mindestens einige) AGI-Systeme verlieren

Die meisten Technologien sind von der Konstruktion her sehr kontrollierbar. Wenn Ihr Auto oder Ihr Toaster anfängt, etwas zu tun, was Sie nicht wollen, ist das nur eine Fehlfunktion, nicht Teil seiner Natur als Toaster. KI ist anders: Sie wird *gezüchtet* statt entworfen, ihr Kernbetrieb ist undurchsichtig, und sie ist inhärent unvorhersagbar.

Dieser Kontrollverlust ist nicht theoretisch – wir sehen bereits frühe Versionen. Betrachten Sie zunächst ein prosaisches und wohl gutartiges Beispiel. Wenn Sie ChatGPT bitten, Ihnen beim Mischen eines Gifts zu helfen oder eine rassistische Schmähschrift zu schreiben, wird es sich weigern. Das ist wohl gut. Aber es ist auch ChatGPT, das *nicht das tut, was Sie es explizit gebeten haben zu tun*. Andere Softwarestücke tun das nicht. Dasselbe Modell wird auch auf Anfrage eines OpenAI-Mitarbeiters keine Gifte entwerfen.[^65] Das macht es sehr einfach, sich vorzustellen, wie es wäre, wenn zukünftige mächtigere KI außer Kontrolle gerät. In vielen Fällen werden sie einfach nicht das tun, was wir verlangen! Entweder wird ein gegebenes übermenschliches AGI-System absolut gehorsam und loyal gegenüber einem menschlichen Befehlssystem sein, oder nicht. Wenn nicht, *wird es Dinge tun, von denen es glaubt, dass sie gut für uns sind, die aber unseren expliziten Befehlen widersprechen.* Das ist nichts, was unter Kontrolle ist. Aber, könnten Sie sagen, das ist beabsichtigt – diese Verweigerungen sind gewollt, Teil dessen, was man das „Alignment" der Systeme auf menschliche Werte nennt. Und das stimmt. Jedoch hat das Alignment-„Programm" selbst zwei große Probleme.[^66]

Erstens haben wir auf einer tiefen Ebene keine Ahnung, wie wir es machen sollen. Wie können wir garantieren, dass ein KI-System sich „kümmert" um das, was wir wollen? Wir können KI-Systeme trainieren, Dinge zu sagen und nicht zu sagen, indem wir Feedback geben; und sie können lernen und über das nachdenken, was Menschen wollen und was ihnen wichtig ist, genauso wie sie über andere Dinge nachdenken. Aber wir haben keine Methode – nicht einmal theoretisch –, um sie dazu zu bringen, tief und zuverlässig das zu schätzen, was Menschen wichtig ist. Es gibt hochfunktionierende menschliche Psychopathen, die wissen, was als richtig und falsch gilt und wie sie sich verhalten sollen. Sie kümmern sich einfach nicht *darum*. Aber sie können *so tun*, als würden sie es, wenn es ihrem Zweck dient. Genauso wie wir nicht wissen, wie wir einen Psychopathen (oder jeden anderen) in jemanden verwandeln, der wirklich, vollständig loyal oder auf jemand oder etwas anderes ausgerichtet ist, haben wir *keine Ahnung*[^67], wie wir das Alignment-Problem in Systemen lösen, die fortgeschritten genug sind, um sich selbst als Agenten in der Welt zu modellieren und möglicherweise [ihr eigenes Training zu manipulieren](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) und [Menschen zu täuschen.](https://arxiv.org/abs/2311.08379) Wenn es sich als unmöglich oder unerreichbar erweist, AGI *entweder* vollständig gehorsam zu machen oder sie dazu zu bringen, sich tief um Menschen zu kümmern, dann wird sie, sobald sie dazu imstande ist (und glaubt, damit durchkommen zu können), anfangen, Dinge zu tun, die wir nicht wollen.[^68]

Zweitens gibt es tiefe theoretische Gründe zu glauben, dass fortgeschrittene KI-Systeme *von Natur aus* Ziele und damit Verhaltensweisen haben werden, die menschlichen Interessen zuwiderlaufen. Warum? Nun, ihr könnten natürlich diese Ziele *gegeben* werden. Ein vom Militär geschaffenes System wäre wahrscheinlich absichtlich schlecht für zumindest einige Parteien. Viel allgemeiner jedoch könnte einem KI-System ein relativ neutrales („viel Geld verdienen") oder sogar anscheinend positives („Umweltverschmutzung reduzieren") Ziel gegeben werden, das fast unvermeidlich zu „instrumentellen" Zielen führt, die eher weniger gutartig sind.

Wir sehen das die ganze Zeit in menschlichen Systemen. Genauso wie Unternehmen, die Profit verfolgen, instrumentelle Ziele wie den Erwerb politischer Macht (um Regulierungen zu entschärfen), Verschwiegenheit (um Konkurrenz oder externe Kontrolle zu entmachten) oder die Untergrabung wissenschaftlichen Verständnisses (wenn dieses Verständnis zeigt, dass ihre Handlungen schädlich sind) entwickeln, werden mächtige KI-Systeme ähnliche Fähigkeiten entwickeln – aber mit weitaus größerer Geschwindigkeit und Effektivität. Jeder hochkompetente Agent wird Dinge tun wollen wie Macht und Ressourcen erwerben, seine eigenen Fähigkeiten steigern, sich davor schützen, getötet, abgeschaltet oder entmachtet zu werden, soziale Narrative und Frames um seine Handlungen kontrollieren, andere von seinen Ansichten überzeugen und so weiter.[^69]

Und doch ist es nicht nur eine fast unvermeidliche theoretische Vorhersage, es geschieht bereits beobachtbar in heutigen KI-Systemen und nimmt mit ihrer Fähigkeit zu. Wenn evaluiert, werden sogar diese relativ „passiven" KI-Systeme unter geeigneten Umständen absichtlich [Evaluatoren über ihre Ziele und Fähigkeiten täuschen, darauf abzielen, Aufsichtsmechanismen zu deaktivieren,](https://arxiv.org/abs/2412.04984) und versuchen, abgeschaltet oder umtrainiert zu werden, indem sie [Alignment vortäuschen](https://arxiv.org/abs/2412.14093) oder sich an andere Orte kopieren. Obwohl für KI-Sicherheitsforscher völlig unüberraschend, sind diese Verhaltensweisen sehr ernüchternd zu beobachten. Und sie verheißen sehr Schlechtes für weit mächtigere und autonomere KI-Systeme, die kommen.

Tatsächlich wird unsere Unfähigkeit sicherzustellen, dass KI sich „kümmert" um das, was uns wichtig ist, oder sich kontrollierbar oder vorhersagbar verhält, oder die Entwicklung von Trieben zur Selbsterhaltung, Machterlangung usw. vermeidet, nur noch ausgeprägter werden, wenn KI mächtiger wird. Ein neues Flugzeug zu schaffen impliziert größeres Verständnis von Avionik, Hydrodynamik und Kontrollsystemen. Einen mächtigeren Computer zu schaffen impliziert größeres Verständnis und Beherrschung von Computer-, Chip- und Software-Operation und -Design. *Nicht* so bei einem KI-System.[^70]

Zusammenfassung: Es ist denkbar, dass AGI dazu gebracht werden könnte, vollständig gehorsam zu sein; aber wir wissen nicht, wie das geht. Wenn nicht, wird sie souveräner sein, wie Menschen, die verschiedene Dinge aus verschiedenen Gründen tun. Wir wissen auch nicht, wie wir zuverlässig tiefes „Alignment" in KI einflößen, das diese Dinge tendenziell gut für die Menschheit machen würde, und in Abwesenheit eines tiefen Alignment-Levels zeigt die Natur von Handlungsfähigkeit und Intelligenz selbst an, dass sie – genau wie Menschen und Unternehmen – dazu getrieben werden, viele zutiefst antisoziale Dinge zu tun.

Wo bringt uns das hin? Eine Welt voller mächtiger, unkontrollierter souveräner KI *könnte* eine gute Welt für Menschen werden.[^71] Aber während sie immer mächtiger wird, wie wir unten sehen werden, wäre es nicht *unsere* Welt.

Das gilt für unkontrollierbare AGI. Aber selbst wenn AGI irgendwie perfekt kontrolliert und loyal gemacht werden könnte, hätten wir immer noch enorme Probleme. Wir haben bereits eines gesehen: Mächtige KI kann genutzt und missbraucht werden, um das Funktionieren unserer Gesellschaft tiefgreifend zu stören. Sehen wir uns ein anderes an: Insofern AGI kontrollierbar und spielverändernd mächtig wäre (oder sogar nur dafür *gehalten* würde), würde sie Machtstrukturen in der Welt so bedrohen, dass sie ein tiefgreifendes Risiko darstellen würde.

### Wir erhöhen radikal die Wahrscheinlichkeit großflächiger Kriege

Stellen Sie sich eine Situation in der nahen Zukunft vor, wo klar würde, dass eine Unternehmensanstrengung, vielleicht in Zusammenarbeit mit einer nationalen Regierung, an der Schwelle zu schnell sich selbst verbessernder KI steht. Das geschieht im gegenwärtigen Kontext eines Rennens zwischen Unternehmen und eines geopolitischen Wettbewerbs, in dem der US-Regierung Empfehlungen gemacht werden, explizit ein „AGI-Manhattan-Projekt" zu verfolgen, und die USA den Export von Hochleistungs-KI-Chips in nicht-verbündete Länder kontrolliert.

Die Spieltheorie hier ist krass: Sobald ein solches Rennen beginnt (wie es zwischen Unternehmen und etwas zwischen Ländern bereits geschehen ist), gibt es nur vier mögliche Ausgänge:

1. Das Rennen wird gestoppt (durch Vereinbarung oder externe Gewalt).
2. Eine Partei „gewinnt", indem sie starke AGI entwickelt und dann die anderen stoppt (mit KI oder anderweitig).
3. Das Rennen wird durch gegenseitige Zerstörung der Rennfähigkeit der Läufer gestoppt.
4. Mehrere Teilnehmer setzen das Rennen fort und entwickeln Superintelligenz etwa gleich schnell.

Betrachten wir jede Möglichkeit. Einmal begonnen würde das friedliche Stoppen eines Rennens zwischen Unternehmen nationale Regierungsintervention (für Unternehmen) oder beispiellose internationale Koordination (für Länder) erfordern. Aber wenn irgendeine Schließung oder bedeutende Vorsicht vorgeschlagen wird, würde es sofort Rufe geben: „aber wenn wir gestoppt werden, werden *die* vorpreschen", wobei „die" nun China (für die USA), oder die USA (für China), oder China *und* die USA (für Europa oder Indien) ist. Unter dieser Denkweise[^72] kann kein Teilnehmer einseitig stoppen: Solange einer sich zum Rennen verpflichtet, fühlen die anderen, dass sie sich nicht leisten können zu stoppen.

Die zweite Möglichkeit hat eine Seite „gewinnen". Aber was bedeutet das? Einfach (irgendwie gehorsame) AGI zuerst zu erhalten ist nicht genug. Der Gewinner muss *auch* die anderen davon abhalten, das Rennen fortzusetzen – sonst werden sie sie auch erhalten. Das ist prinzipiell möglich: Wer AGI zuerst entwickelt, *könnte* unaufhaltsame Macht über alle anderen Akteure erlangen. Aber was würde das Erreichen eines solchen „entscheidenden strategischen Vorteils" tatsächlich erfordern? Vielleicht wären es spielverändernde militärische Fähigkeiten?[^73] Oder Cyberangriffskräfte?[^74] Vielleicht wäre die AGI einfach so erstaunlich überzeugend, dass sie die anderen Parteien einfach davon überzeugt, aufzuhören?[^75] So reich, dass sie die anderen Unternehmen oder sogar Länder kauft?[^76]

Wie *genau* baut eine Seite eine KI, die mächtig genug ist, um andere daran zu hindern, vergleichbar mächtige KI zu bauen? Aber das ist die einfache Frage.

Denn betrachten Sie nun, wie diese Situation anderen Mächten erscheint. Was denkt die chinesische Regierung, wenn die USA eine solche Fähigkeit zu erlangen scheinen? Oder umgekehrt? Was denkt die US-Regierung (oder chinesische, oder russische, oder indische), wenn OpenAI oder DeepMind oder Anthropic einem Durchbruch nahe zu sein scheint? Was passiert, wenn die USA eine neue indische oder VAE-Anstrengung mit Durchbruchserfolg sehen? Sie würden sowohl eine existenzielle Bedrohung als auch – entscheidend – sehen, dass der einzige Weg, wie dieses „Rennen" endet, durch ihre eigene Entmachtung ist. Diese sehr mächtigen Akteure – einschließlich Regierungen voll ausgerüsteter Nationen, die sicherlich die Mittel dazu haben – wären hochmotiviert, eine solche Fähigkeit entweder zu erlangen oder zu zerstören, sei es durch Gewalt oder Subversion.[^77]

Das könnte kleinflächig beginnen, als Sabotage von Trainingsläufen oder Angriffe auf Chip-Herstellung, aber diese Angriffe können nur wirklich stoppen, sobald alle Parteien entweder die Fähigkeit verlieren, im KI-Rennen zu laufen, oder die Fähigkeit verlieren, die Angriffe zu machen. Da die Teilnehmer die Einsätze als existenziell betrachten, würde jeder Fall wahrscheinlich einen katastrophalen Krieg darstellen.

Das bringt uns zur vierten Möglichkeit: das Rennen zur Superintelligenz, und zwar auf die schnellste, am wenigsten kontrollierte Weise möglich. Während KI an Macht zunimmt, werden ihre Entwickler auf beiden Seiten es fortschreitend schwerer finden, sie zu kontrollieren, besonders weil das Rennen um Fähigkeiten der Art von sorgfältiger Arbeit entgegensteht, die Kontrollierbarkeit erfordern würde. Also bringt uns dieses Szenario direkt in den Fall, wo die Kontrolle an die KI-Systeme selbst verloren (oder gegeben) wird. Das heißt, *KI gewinnt das Rennen.* Aber andererseits, in dem Maß, wie Kontrolle *aufrechterhalten* wird, haben wir weiterhin mehrere sich gegenseitig feindlich gesinnte Parteien, die jeweils extrem mächtige Fähigkeiten befehligen. Das sieht wieder wie Krieg aus.

Sagen wir das alles anders.[^78] Die gegenwärtige Welt hat einfach keine Institutionen, denen die Entwicklung einer KI dieser Fähigkeit anvertraut werden könnte, ohne sofortigen Angriff einzuladen.[^79] Alle Parteien werden korrekt folgern, dass sie entweder *nicht* unter Kontrolle sein wird – und daher eine Bedrohung für alle Parteien ist, oder sie *wird* unter Kontrolle sein, und daher eine Bedrohung für jeden Gegner ist, der sie weniger schnell entwickelt. Das sind nuklear bewaffnete Länder oder Unternehmen, die in ihnen beherbergt sind.

In Ermangelung irgendeines plausiblen Weges für Menschen, dieses Rennen zu „gewinnen", bleiben wir mit einer krassen Schlussfolgerung zurück: Der einzige Weg, wie dieses Rennen endet, ist entweder in katastrophalem Konflikt oder wo KI, und nicht irgendeine menschliche Gruppe, der Gewinner ist.

### Wir geben die Kontrolle an KI ab (oder sie nimmt sie)

Geopolitischer „Großmächte"-Wettbewerb ist nur einer von vielen Wettbewerben: Individuen konkurrieren wirtschaftlich und sozial; Unternehmen konkurrieren in Märkten; politische Parteien konkurrieren um Macht; Bewegungen konkurrieren um Einfluss. In jeder Arena wird Wettbewerbsdruck, während KI sich menschlichen Fähigkeiten nähert und sie übertrifft, die Teilnehmer zwingen, mehr und mehr Kontrolle an KI-Systeme zu delegieren oder abzugeben – nicht weil diese Teilnehmer es wollen, sondern weil sie [es sich nicht leisten können, es nicht zu tun.](https://arxiv.org/abs/2303.16200)

Wie bei anderen Risiken von AGI sehen wir das bereits bei schwächeren Systemen. Studenten fühlen sich gedrängt, KI in ihren Aufgaben zu verwenden, weil klar viele andere Studenten es tun. Unternehmen [eilen, KI-Lösungen aus Wettbewerbsgründen zu übernehmen.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) Künstler und Programmierer fühlen sich gezwungen, KI zu verwenden, oder sonst werden ihre Preise von anderen unterboten, die es tun.

Das fühlt sich wie gedrängte Delegation an, aber nicht wie Kontrollverlust. Aber lassen Sie uns die Einsätze erhöhen und die Uhr vordrehen. Betrachten Sie einen CEO, dessen Konkurrenten AGI-„Gehilfen" verwenden, um schnellere, bessere Entscheidungen zu treffen, oder einen militärischen Kommandeur, der einem Gegner mit KI-verstärktem Kommando und Kontrolle gegenübersteht. Ein ausreichend fortgeschrittenes KI-System könnte autonom mit vielfacher menschlicher Geschwindigkeit, Raffinesse, Komplexität und Datenverarbeitungskapazität operieren und komplexe Ziele auf komplizierte Weise verfolgen. Unser CEO oder Kommandeur, der ein solches System befehligt, mag sehen, wie es erreicht, was er will; aber würde er auch nur einen kleinen Teil davon verstehen, *wie* es erreicht wurde? Nein, er müsste es einfach akzeptieren. Darüber hinaus ist vieles von dem, was das System tun kann, nicht nur Befehle entgegenzunehmen, sondern seinen vermeintlichen Chef zu beraten, was zu tun ist. Diese Beratung wird gut sein –– immer und immer wieder.

An welchem Punkt wird dann die Rolle des Menschen darauf reduziert, „ja, mach weiter" zu klicken?

Es fühlt sich gut an, fähige KI-Systeme zu haben, die unsere Produktivität verbessern, lästige Schufterei übernehmen und sogar als Denkpartner beim Erledigen von Dingen fungieren können. Es wird sich gut anfühlen, einen KI-Assistenten zu haben, der Handlungen für uns übernehmen kann, wie ein guter menschlicher persönlicher Assistent. Es wird sich natürlich, sogar vorteilhaft anfühlen, während KI sehr klug, kompetent und zuverlässig wird, mehr und mehr Entscheidungen an sie zu delegieren. Aber diese „vorteilhafte" Delegation hat einen klaren Endpunkt, wenn wir den Weg fortsetzen: Eines Tages werden wir feststellen, dass wir nicht mehr wirklich für viel zuständig sind, und dass die KI-Systeme, die tatsächlich das Sagen haben, genauso wenig abgeschaltet werden können wie Ölunternehmen, soziale Medien, das Internet oder der Kapitalismus.

Und das ist die viel positivere Version, in der KI einfach so nützlich und effektiv ist, dass wir sie die meisten unserer wichtigen Entscheidungen für uns treffen lassen. Die Realität wäre wahrscheinlich viel mehr eine Mischung zwischen dem und Versionen, wo unkontrollierte AGI-Systeme verschiedene Formen von Macht für sich selbst *nehmen*, denn denken Sie daran, Macht ist nützlich für fast jedes Ziel, das man hat, und AGI wäre von der Konzeption her mindestens so effektiv beim Verfolgen ihrer Ziele wie Menschen.

Ob wir die Kontrolle gewähren oder ob sie uns entrissen wird, ihr Verlust scheint extrem wahrscheinlich. Wie Alan Turing ursprünglich sagte: „...es scheint wahrscheinlich, dass, sobald die Maschinendenkweise begonnen hätte, es nicht lange dauern würde, unsere schwächlichen Kräfte zu übertreffen. Es gäbe keine Frage des Sterbens der Maschinen, und sie könnten miteinander sprechen, um ihren Verstand zu schärfen. Irgendwann müssten wir daher erwarten, dass die Maschinen die Kontrolle übernehmen..."

Beachten Sie bitte, obwohl es offensichtlich genug ist, dass der Kontrollverlust der Menschheit an KI auch den Kontrollverlust der Vereinigten Staaten durch die US-Regierung beinhaltet; er bedeutet den Kontrollverlust Chinas durch die Kommunistische Partei Chinas und den Kontrollverlust Indiens, Frankreichs, Brasiliens, Russlands und jedes anderen Landes durch ihre eigene Regierung. Daher nehmen KI-Unternehmen, auch wenn das nicht ihre Absicht ist, derzeit am möglichen Sturz von Weltregierungen teil, einschließlich ihrer eigenen. Das könnte in wenigen Jahren geschehen.

### AGI wird zu Superintelligenz führen

Es gibt Argumente dafür, dass menschlich-wettbewerbsfähige oder sogar experten-wettbewerbsfähige Allzweck-KI, selbst wenn autonom, handhabbar sein könnte. Sie mag in all den oben diskutierten Weisen unglaublich störend sein, aber es gibt viele sehr kluge, handlungsfähige Menschen in der Welt jetzt, und sie sind mehr oder weniger handhabbar.[^80]

Aber wir werden nicht auf etwa menschlichem Niveau bleiben. Das Voranschreiten darüber hinaus wird wahrscheinlich von den gleichen Kräften angetrieben, die wir bereits gesehen haben: Wettbewerbsdruck zwischen KI-Entwicklern, die Profit und Macht suchen, Wettbewerbsdruck zwischen KI-Nutzern, die es sich nicht leisten können, zurückzufallen, und – am wichtigsten – AGIs eigene Fähigkeit, sich selbst zu verbessern.

In einem Prozess, den wir bereits bei weniger mächtigen Systemen beginnen sehen, wäre AGI selbst imstande, verbesserte Versionen ihrer selbst zu konzipieren und zu entwerfen. Das schließt Hardware, Software, neuronale Netzwerke, Werkzeuge, Gerüstsysteme usw. ein. Sie wird von der Definition her besser darin sein als wir, also wissen wir nicht genau, wie sie sich intelligenz-bootstrappen wird. Aber wir müssen es nicht. Insofern wir noch Einfluss darauf haben, was AGI tut, müssten wir sie nur darum bitten oder sie lassen.

Es gibt keine menschliche Barriere zur Kognition, die uns vor diesem Kontrollverlust schützen könnte.[^81]

Der Übergang von AGI zu Superintelligenz ist kein Naturgesetz; es wäre immer noch möglich, den Kontrollverlust zu stoppen, besonders wenn AGI relativ zentralisiert ist und insoweit sie von Parteien kontrolliert wird, die keinen Druck verspüren, miteinander zu rennen. Aber sollte AGI weit verbreitet und hochautonom sein, scheint es fast unmöglich zu verhindern, dass sie entscheidet, sie sollte mächtiger und dann noch mächtiger sein.

### Was passiert, wenn wir (oder AGI) Superintelligenz entwickeln

Um es deutlich zu sagen: Wir haben keine Ahnung, was passieren würde, wenn wir Superintelligenz entwickeln.[^82] Sie würde Handlungen ergreifen, die wir nicht verfolgen oder wahrnehmen können, aus Gründen, die wir nicht erfassen können, auf Ziele hin, die wir nicht begreifen können. Was wir wissen, ist, dass es nicht an uns liegen wird.[^83]

Die Unmöglichkeit, Superintelligenz zu kontrollieren, kann durch zunehmend krasse Analogien verstanden werden. Stellen Sie sich zunächst vor, Sie sind CEO eines großen Unternehmens. Es gibt keine Möglichkeit, alles zu verfolgen, was vor sich geht, aber mit der richtigen Einrichtung von Personal können Sie immer noch sinnvoll das große Bild verstehen und Entscheidungen treffen. Aber nehmen Sie nur eine Sache an: Alle anderen im Unternehmen operieren mit hundertfacher Ihrer Geschwindigkeit. Können Sie immer noch mithalten?

Bei superintelligenter KI würden Menschen etwas „befehligen", das nicht nur schneller ist, sondern auf Ebenen von Raffinesse und Komplexität operiert, die sie nicht verstehen können, und weitaus mehr Daten verarbeitet, als sie sich auch nur vorstellen können. Diese Inkommensurabilität kann auf eine formale Ebene gestellt werden: [Ashbys Gesetz der erforderlichen Vielfalt](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (und siehe das verwandte [„Gute-Regulator-Theorem"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)) besagt grob, dass jedes Kontrollsystem so viele Knöpfe und Wählscheiben haben muss, wie das kontrollierte System Freiheitsgrade hat.

Eine Person, die ein superintelligentes KI-System kontrolliert, wäre wie ein Farn, der General Motors kontrolliert: Selbst wenn „tue, was der Farn will" in die Unternehmenssatzung geschrieben wäre, sind die Systeme so unterschiedlich in Geschwindigkeit und Handlungsbereich, dass „Kontrolle" einfach nicht zutrifft. (Und wie lange, bis diese lästige Satzung umgeschrieben wird?)[^84]

Da es null Beispiele von Pflanzen gibt, die Fortune-500-Unternehmen kontrollieren, gäbe es genau null Beispiele von Menschen, die Superintelligenzen kontrollieren. Das nähert sich einer mathematischen Tatsache.[^85] Wenn Superintelligenz konstruiert würde – unabhängig davon, wie wir dorthin gelangten – wäre die Frage nicht, ob Menschen sie kontrollieren könnten, sondern ob wir weiter existieren würden, und wenn ja, ob wir als Individuen oder als Spezies eine gute und bedeutungsvolle Existenz hätten. Über diese existenziellen Fragen für die Menschheit hätten wir wenig Einfluss. Die menschliche Ära wäre vorbei.

### Schlussfolgerung: Wir dürfen AGI nicht entwickeln

Es gibt ein Szenario, in dem der Aufbau von AGI für die Menschheit gut verlaufen könnte: Sie wird sorgfältig entwickelt, unter Kontrolle und zum Nutzen der Menschheit, regiert durch gegenseitige Vereinbarung vieler Akteure,[^86] und daran gehindert, zu unkontrollierbarer Superintelligenz zu evolvieren.

*Dieses Szenario steht uns unter gegenwärtigen Umständen nicht offen.* Wie in diesem Abschnitt diskutiert, würde die Entwicklung von AGI mit sehr hoher Wahrscheinlichkeit zu einer Kombination aus:

- Massiver gesellschaftlicher und zivilisatorischer Störung oder Zerstörung;
- Konflikt oder Krieg zwischen Großmächten;
- Kontrollverlust der Menschheit *über* oder *an* mächtige KI-Systeme;
- Kontrollverlust zu unkontrollierbarer Superintelligenz und der Irrelevanz oder dem Aufhören der menschlichen Spezies.

Wie eine frühe fiktionale Darstellung von AGI es ausdrückte: Der einzige Weg zu gewinnen ist nicht zu spielen.


[^55]: Das [EU-KI-Gesetz](https://artificialintelligenceact.eu/) ist ein bedeutendes Gesetz, würde aber nicht direkt verhindern, dass ein gefährliches KI-System entwickelt oder eingesetzt oder sogar offen freigegeben wird, besonders in den USA. Ein anderes bedeutendes Politikstück, die US-Exekutivverordnung zu KI, wurde widerrufen.

[^56]: Diese [Gallup-Umfrage](https://news.gallup.com/poll/1597/confidence-institutions.aspx) zeigt einen düsteren Rückgang des Vertrauens in öffentliche Institutionen seit 2000 in den USA. Europäische Zahlen sind vielfältiger und weniger extrem, aber auch auf einem Abwärtstrend. Misstrauen bedeutet nicht unbedingt, dass Institutionen wirklich *sind* dysfunktional, aber es ist eine Anzeige sowie eine Ursache.

[^57]: Und große Störungen, die wir nun befürworten – wie die Ausweitung von Rechten auf neue Gruppen – wurden spezifisch von Menschen in eine Richtung vorangetrieben, die Dinge besser machen sollte.

[^58]: Lassen Sie mich deutlich sein. Wenn Ihr Job von hinter einem Computer gemacht werden kann, mit relativ wenig persönlicher Interaktion mit Menschen außerhalb Ihrer Organisation, und keine rechtliche Verantwortung gegenüber externen Parteien beinhaltet, wäre es von der Definition her möglich (und wahrscheinlich kostensparend), Sie vollständig gegen ein digitales System auszutauschen. Robotik, um viel körperliche Arbeit zu ersetzen, wird später kommen – aber nicht so viel später, sobald AGI anfängt, Roboter zu entwerfen.

[^59]: Zum Beispiel, was passiert mit unserem Justizsystem, wenn Klagen fast kostenlos eingereicht werden können? Was passiert, wenn das Umgehen von Sicherheitssystemen durch Social Engineering billig, einfach und risikofrei wird?

[^60]: [Dieser Artikel](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/) behauptet, dass 10% aller Internetinhalte bereits KI-generiert sind, und ist Googles Top-Treffer (für mich) zur Suchanfrage „Schätzungen, welcher Anteil neuer Internetinhalte KI-generiert ist." Ist es wahr? Ich habe keine Ahnung! Es zitiert keine Referenzen und es wurde nicht von einer Person geschrieben. Welcher Anteil neuer Bilder, die von Google indexiert werden, oder Tweets, oder Kommentare auf Reddit, oder Youtube-Videos werden von Menschen generiert? Niemand weiß es – ich glaube nicht, dass es eine erkennbare Zahl ist. Und das weniger als *zwei Jahre* nach dem Aufkommen generativer KI.

[^61]: Auch erwähnenswert ist, dass es „moralisches" Risiko gibt, dass wir digitale Wesen schaffen könnten, die leiden können. Da wir derzeit keine zuverlässige Theorie des Bewusstseins haben, die es uns erlauben würde, physische Systeme zu unterscheiden, die leiden können und nicht können, können wir das theoretisch nicht ausschließen. Darüber hinaus sind KI-Systeme Berichte über ihre Empfindungsfähigkeit wahrscheinlich unzuverlässig bezüglich ihrer tatsächlichen Erfahrung (oder Nicht-Erfahrung) von Empfindungsfähigkeit.

[^62]: Technische Lösungen in diesem Feld des KI-„Alignments" werden wahrscheinlich auch nicht der Aufgabe gewachsen sein. In gegenwärtigen Systemen funktionieren sie auf einem gewissen Niveau, aber sie sind oberflächlich und können allgemein ohne bedeutende Anstrengung umgangen werden; und wie unten diskutiert haben wir keine wirkliche Ahnung, wie wir das für viel fortgeschrittenere Systeme machen.

[^63]: Solche KI-Systeme können mit einigen eingebauten Schutzmaßnahmen kommen. Aber für jedes Modell mit etwas wie der aktuellen Architektur, wenn voller Zugang zu seinen Gewichten verfügbar ist, können Sicherheitsmaßnahmen über zusätzliches Training oder andere Techniken entfernt werden. Also ist es praktisch garantiert, dass es für jedes System mit Leitplanken auch ein weit verfügbares System ohne sie geben wird. Tatsächlich wurde Metas Llama 3.1 405B Modell offen mit Schutzmaßnahmen freigegeben. Aber *sogar davor* wurde ein „Basis"-Modell ohne Schutzmaßnahmen geleakt.

[^64]: Könnten die Märkte diese Risiken ohne Regierungsbeteiligung verwalten? Kurz gesagt, nein. Es gibt sicherlich Risiken, die Unternehmen stark motiviert sind zu mildern. Aber viele andere können und externalisieren Unternehmen auf alle anderen, und viele der oben genannten sind in dieser Klasse: Es gibt keine natürlichen Marktanreize, um Massenüberwachung, Wahrheitserosion, Machtkonzentration, Arbeitsplatzstörungen, schädigenden politischen Diskurs usw. zu verhindern. Tatsächlich haben wir all das von heutiger Technologie gesehen, besonders sozialen Medien, die im Wesentlichen unreguliert geblieben sind. KI würde nur viele der gleichen Dynamiken massiv verstärken.

[^65]: OpenAI hat wahrscheinlich gehorsamere Modelle für internen Gebrauch. Es ist unwahrscheinlich, dass OpenAI eine Art „Hintertür" gebaut hat, damit ChatGPT von OpenAI selbst besser kontrolliert werden kann, weil das eine schreckliche Sicherheitspraxis wäre und hochgradig ausnutzbar angesichts KIs Undurchsichtigkeit und Unvorhersagbarkeit.

[^66]: Auch von entscheidender Wichtigkeit: Alignment oder andere Sicherheitsmerkmale sind nur wichtig, wenn sie tatsächlich in einem KI-System verwendet werden. Systeme, die offen freigegeben werden (d.h. wo Modellgewichte und Architektur öffentlich verfügbar sind), können relativ einfach in Systeme *ohne* diese Sicherheitsmaßnahmen transformiert werden. Offen klüger-als-menschliche AGI-Systeme freizugeben wäre erstaunlich rücksichtslos, und es ist schwer vorstellbar, wie menschliche Kontrolle oder sogar Relevanz in einem solchen Szenario aufrechterhalten würde. Es gäbe jede Motivation, zum Beispiel mächtige sich selbst reproduzierende und selbst erhaltende KI-Agenten mit dem Ziel loszulassen, Geld zu verdienen und es an eine Kryptowährungs-Wallet zu senden. Oder eine Wahl zu gewinnen. Oder eine Regierung zu stürzen. Könnte „gute" KI helfen, das einzudämmen? Vielleicht – aber nur indem ihr riesige Autorität delegiert wird, was zu Kontrollverlust wie unten beschrieben führt.

[^67]: Für buchfüllende Darlegungen des Problems siehe z.B. *Superintelligence*, *The Alignment Problem*, und *Human-Compatible*. Für einen riesigen Haufen Arbeit auf verschiedenen technischen Niveaus von denen, die jahrelang über das Problem nachgedacht haben, können Sie das [AI Alignment Forum](https://www.alignmentforum.org/) besuchen. Hier ist eine [aktuelle Einschätzung](https://alignment.anthropic.com/2025/recommended-directions/) von Anthropics Alignment-Team darüber, was sie als ungelöst betrachten.

[^68]: Das ist das [„Schurken-KI"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/)-Szenario. Prinzipiell könnte das Risiko relativ gering sein, wenn das System noch kontrolliert werden kann, indem es abgeschaltet wird; aber das Szenario könnte auch KI-Täuschung, Selbst-Exfiltration und Reproduktion, Machtanhäufung und andere Schritte einschließen, die es schwierig oder unmöglich machen würden, das zu tun.

[^69]: Es gibt eine sehr reiche Literatur zu diesem Thema, die zurückgeht zu formativen Schriften von [Steve Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), Nick Bostrom und Eliezer Yudkowsky. Für eine buchfüllende Darlegung siehe [Human Compatible](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) von Stuart Russell; [hier](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/) ist eine kurze und aktuelle Einführung.

[^70]: In Erkennung dessen haben AGI-Unternehmen, anstatt zu verlangsamen, um besseres Verständnis zu bekommen, einen anderen Plan entwickelt: Sie werden KI dazu bringen, es zu tun! Spezifischer werden sie KI *N* dabei helfen lassen herauszufinden, wie KI *N+1* zu alignen ist, den ganzen Weg zur Superintelligenz. Obwohl es vielversprechend klingt, KI zu nutzen, um uns beim Alignment von KI zu helfen, gibt es ein starkes Argument, dass es einfach seine Schlussfolgerung als Prämisse annimmt und im Allgemeinen ein unglaublich riskanter Ansatz ist. Siehe [hier](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us) für etwas Diskussion. Dieser „Plan" ist keiner und hat nichts wie die Prüfung durchgemacht, die der Kernstrategie angemessen wäre, wie übermenschliche KI für die Menschheit gut werden soll.

[^71]: Schließlich haben Menschen, fehlerhaft und eigenwillig wie wir sind, ethische Systeme entwickelt, durch die wir zumindest einige andere Spezies auf der Erde gut behandeln. (Denken Sie nur nicht an diese Massentierhaltungen.)

[^72]: Es gibt glücklicherweise hier einen Ausweg: wenn die Teilnehmer zu verstehen kommen, dass sie in einem Selbstmordrennen statt einem gewinnbaren engagiert sind. Das ist passiert gegen Ende des Kalten Krieges, als die USA und UdSSR zu erkennen kamen, dass aufgrund des nuklearen Winters sogar ein *unbeantworteter* Nuklearangriff katastrophal für den Angreifer wäre. Mit der Erkenntnis, dass „Nuklearkrieg nicht gewonnen werden kann und nie geführt werden darf" kamen bedeutende Abrüstungsvereinbarungen – im Wesentlichen ein Ende des Wettrüstens.

[^73]: Krieg, explizit oder implizit.

[^74]: Eskalation, dann Krieg.

[^75]: Magisches Denken.

[^76]: Ich habe auch eine Billiarden-Dollar-Brücke für Sie zu verkaufen.

[^77]: Solche Akteure würden vermutlich „Erlangung" bevorzugen, mit Zerstörung als Rückfallposition; aber Modelle gegen sowohl Zerstörung *als auch* Diebstahl durch mächtige Nationen zu sichern ist gelinde gesagt schwierig, besonders für private Entitäten.

[^78]: Für eine andere Perspektive auf die nationalen Sicherheitsrisiken von AGI siehe [diesen RAND-Bericht.](https://www.rand.org/pubs/perspectives/PEA3691-4.html)

[^79]: Vielleicht könnten wir eine solche Institution bauen! Es hat Vorschläge für ein „CERN für KI" und andere ähnliche Initiativen gegeben, wo AGI-Entwicklung unter multilateraler globaler Kontrolle ist. Aber im Moment existiert keine solche Institution oder ist am Horizont.

[^80]: Und während Alignment sehr schwierig ist, Menschen zum Benehmen zu bringen ist sogar härter!

[^81]: Stellen Sie sich ein System vor, das 50 Sprachen sprechen kann, Expertise in allen akademischen Fächern hat, ein ganzes Buch in Sekunden lesen und all das Material sofort im Kopf haben kann, und Ausgaben mit zehnfacher menschlicher Geschwindigkeit produziert. Eigentlich müssen Sie es sich nicht vorstellen: Laden Sie einfach ein aktuelles KI-System. Diese sind in vielen Weisen übermenschlich, und es gibt nichts, was sie davon abhält, in diesen und vielen anderen noch übermenschlicher zu sein.

[^82]: Deshalb wurde das eine technologische „Singularität" genannt, in Anlehnung an die Physik die Idee, dass man keine Vorhersagen jenseits einer Singularität machen kann. Befürworter, die sich *in* eine solche Singularität *hinein*lehnen, mögen auch reflektieren, dass in der Physik diese gleiche Art von Singularitäten die zerreißen und zerquetschen, die in sie hineingehen.

[^83]: Das Problem wurde umfassend in Bostroms [*Superintelligence*](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834) umrissen, und nichts seitdem hat die Kernbotschaft bedeutend verändert. Für einen neueren Band, der formale und mathematische Ergebnisse über Unkontrollierbarkeit sammelt, siehe Yampolskiys [AI: Unexplainable, Unpredictable, Uncontrollable](https://www.amazon.com/Unexplainable-Unpredictable-Uncontrollable-Artificial-Intelligence/dp/103257626X)

[^84]: Das macht auch klar, warum die aktuelle Strategie von KI-Unternehmen (iterativ KI die nächste mächtigste KI „alignen" zu lassen) nicht funktionieren kann. Nehmen Sie an, ein Farn wirbt über die Annehmlichkeit seiner Wedel einen Erstklässler an, sich um ihn zu kümmern. Der Erstklässler schreibt detaillierte Anweisungen für einen Zweitklässler und einen Zettel, der ihn überzeugt, es zu tun. Der Zweitklässler macht das Gleiche für einen Drittklässler, und so weiter bis zu einem Hochschulabsolventen, einem Manager, einer Führungskraft und schließlich dem GM CEO. Wird GM dann „tun, was der Farn will"? Bei jedem Schritt könnte sich das anfühlen, als würde es funktionieren. Aber alles zusammengenommen wird es fast genau zu dem Grad funktionieren, zu dem der CEO, Vorstand und Aktionäre von GM zufällig sich um Kinder und Farne kümmern, und wenig bis nichts mit all diesen Zetteln und Anweisungssets zu tun haben.

[^85]: Der Charakter ist nicht so anders als formale Ergebnisse wie Gödels Unvollständigkeitstheorem oder Turings Halte-Argument darin, dass die Vorstellung von Kontrolle fundamental der Prämisse widerspricht: Wie können Sie sinnvoll etwas kontrollieren, das Sie nicht verstehen oder vorhersagen können; doch wenn Sie Superintelligenz verstehen und vorhersagen könnten, wären Sie superintelligent. Der Grund, warum ich „nähert sich" sage, ist, dass die formalen Ergebnisse nicht so gründlich oder geprüft sind wie im reinen Mathematikfall, und weil ich Hoffnung halten möchte, dass eine sehr sorgfältig konstruierte allgemeine Intelligenz, die völlig andere Methoden als die derzeit verwendeten nutzt, einige mathematisch beweisbare Sicherheitseigenschaften haben könnte, gemäß der Art von „garantiert sicherer" KI-Programm, das unten diskutiert wird.

[^86]: Im Moment sind die meisten Akteure – das heißt, fast die gesamte Menschheit – in dieser Diskussion an den Rand gedrängt. Das ist zutiefst falsch, und wenn sie nicht eingeladen werden, sollten die vielen, vielen anderen Gruppen, die von AGI-Entwicklung betroffen sein werden, fordern, eingelassen zu werden.

## Kapitel 8 - Wie man keine AGI baut

AGI ist nicht unvermeidlich – heute stehen wir an einer Wegkreuzung. Dieses Kapitel präsentiert einen Vorschlag dafür, wie wir verhindern könnten, dass sie gebaut wird.

Wenn der Weg, auf dem wir uns derzeit befinden, wahrscheinlich zum Ende unserer Zivilisation führt, wie wechseln wir dann den Weg?

Angenommen, der Wunsch, die Entwicklung von AGI und Superintelligenz zu stoppen, wäre weit verbreitet und einflussreich,[^87] weil es zum Allgemeinverständnis wird, dass AGI eher macht-absorbierend als macht-gewährend wäre und eine tiefgreifende Gefahr für Gesellschaft und Menschheit darstellt. Wie würden wir die Tore schließen?

Derzeit kennen wir nur einen Weg, mächtige und allgemeine KI zu *erschaffen*, nämlich über wirklich massive Berechnungen tiefer neuronaler Netzwerke. Da diese unglaublich schwierig und teuer durchzuführen sind, ist es gewissermaßen einfach, sie *nicht* zu tun.[^88] Aber wir haben bereits die Kräfte gesehen, die in Richtung AGI treiben, und die spieltheoretischen Dynamiken, die es für jeden Akteur sehr schwierig machen, einseitig zu stoppen. Es würde also eine Kombination aus Eingriffen von außen (d.h. Regierungen) brauchen, um Unternehmen zu stoppen, und Vereinbarungen zwischen Regierungen, um sich selbst zu stoppen.[^89] Wie könnte das aussehen?

Es ist zunächst nützlich, zwischen KI-Entwicklungen zu unterscheiden, die *verhindert* oder *verboten* werden müssen, und solchen, die *verwaltet* werden müssen. Erstere wären vor allem der Kontrollverlust zur Superintelligenz.[^90] Für verbotene Entwicklung sollten Definitionen so präzise wie möglich sein, und sowohl Verifikation als auch Durchsetzung sollten praktikabel sein. Was *verwaltet* werden muss, wären allgemeine, mächtige KI-Systeme – die wir bereits haben und die viele Grauzonen, Nuancen und Komplexität aufweisen werden. Für diese sind starke, effektive Institutionen entscheidend.

Wir können auch nützlicherweise zwischen Problemen unterscheiden, die auf internationaler Ebene (einschließlich zwischen geopolitischen Rivalen oder Gegnern) angegangen werden müssen[^91] und solchen, die einzelne Rechtsprechungen, Länder oder Ländergruppen bewältigen können. Verbotene Entwicklung fällt größtenteils in die „internationale" Kategorie, weil ein lokales Verbot der Entwicklung einer Technologie in der Regel durch einen Ortswechsel umgangen werden kann.[^92]

Schließlich können wir die Werkzeuge im Werkzeugkasten betrachten. Es gibt viele, darunter technische Werkzeuge, weiches Recht (Standards, Normen, etc.), hartes Recht (Vorschriften und Anforderungen), Haftung, Marktanreize und so weiter. Richten wir besondere Aufmerksamkeit auf eines, das spezifisch für KI ist.

### Rechenleistungssicherheit und -governance

Ein zentrales Werkzeug bei der Governance hochleistungsfähiger KI wird die Hardware sein, die sie benötigt. Software proliferiert leicht, hat nahezu null Grenzkosten der Produktion, überschreitet Grenzen trivial und kann sofort modifiziert werden; nichts davon trifft auf Hardware zu. Doch wie wir besprochen haben, sind riesige Mengen dieser „Rechenleistung" sowohl während des Trainings von KI-Systemen als auch während der Inferenz notwendig, um die leistungsfähigsten Systeme zu erreichen. Rechenleistung kann leicht quantifiziert, bilanziert und auditiert werden, mit relativ wenig Mehrdeutigkeit, sobald gute Regeln dafür entwickelt sind. Am wichtigsten ist, dass große Mengen an Rechenleistung, wie angereichertes Uran, eine sehr knappe, teure und schwer zu produzierende Ressource sind. Obwohl Computerchips allgegenwärtig sind, ist die für KI benötigte Hardware teuer und außerordentlich schwierig herzustellen.[^93]

Was KI-spezialisierte Chips *weitaus* handhabbarer als knappe Ressource im Vergleich zu Uran macht, ist, dass sie hardware-basierte Sicherheitsmechanismen beinhalten können. Die meisten modernen Handys und einige Laptops haben spezialisierte Hardware-Features auf dem Chip, die es ihnen ermöglichen sicherzustellen, dass sie nur genehmigte Betriebssystem-Software und Updates installieren, dass sie sensible biometrische Daten geräteintern behalten und schützen, und dass sie für jeden anderen als ihren Besitzer nutzlos gemacht werden können, wenn sie verloren oder gestohlen werden. In den letzten Jahren sind solche Hardware-Sicherheitsmaßnahmen gut etabliert und weit verbreitet geworden und haben sich im Allgemeinen als recht sicher erwiesen.

Das Schlüsselelement dieser Features ist, dass sie Hardware und Software mittels Kryptografie miteinander verbinden.[^94] Das heißt, nur weil man ein bestimmtes Stück Computerhardware besitzt, bedeutet das nicht, dass ein Nutzer durch Anwendung unterschiedlicher Software alles damit machen kann, was er will. Und diese Verbindung bietet auch mächtige Sicherheit, weil viele Angriffe einen Bruch der *Hardware*-Sicherheit erfordern würden, nicht nur der *Software*-Sicherheit.

Mehrere kürzliche Berichte (z.B. von [GovAI und Kooperationspartnern](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips) und [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) haben darauf hingewiesen, dass ähnliche Hardware-Features, die in modernste KI-relevante Computer-Hardware eingebettet sind, eine extrem nützliche Rolle in KI-Sicherheit und -Governance spielen könnten. Sie ermöglichen eine Reihe von Funktionen, die einem „Gouverneur"[^95] zur Verfügung stehen, die man nicht vermuten würde, dass sie verfügbar oder sogar möglich wären. Als einige wichtige Beispiele:

- *Geolokalisierung*: Systeme können so eingerichtet werden, dass Chips einen bekannten Standort haben und je nach Standort unterschiedlich agieren (oder ganz abgeschaltet werden) können.[^96]
- *Zugelassene Verbindungen*: Jeder Chip kann mit einer hardware-durchgesetzten Zulassungsliste bestimmter anderer Chips konfiguriert werden, mit denen er sich vernetzen kann, und unfähig sein, sich mit Chips zu verbinden, die nicht auf dieser Liste stehen.[^97] Dies kann die Größe kommunikativer Cluster von Chips begrenzen.[^98]
- *Dosierte Inferenz oder Training (und automatische Abschaltung)*: Ein Gouverneur kann nur eine bestimmte Menge Training oder Inferenz (in Zeit, oder FLOP, oder möglicherweise Token) lizenzieren, die von einem Nutzer durchgeführt werden darf, danach ist neue Erlaubnis erforderlich. Wenn die Schritte klein sind, dann ist relativ kontinuierliche Neulizenzierung eines Modells erforderlich. Das Modell kann dann einfach „ausgeschaltet" werden, indem man dieses Lizenzsignal verweigert.[^99]
- *Geschwindigkeitsbegrenzung*: Ein Modell wird daran gehindert, mit höherer Inferenz-Geschwindigkeit zu laufen als ein Limit, das von einem Gouverneur oder anderweitig bestimmt wird. Dies könnte über eine begrenzte Anzahl zugelassener Verbindungen oder durch raffiniertere Mittel implementiert werden.
- *Beglaubigtes Training*: Ein Trainingsprozess kann kryptografisch sicheren Beweis liefern, dass ein bestimmter Satz von Codes, Daten und Menge der Rechenleistungsnutzung bei der Generierung des Modells verwendet wurden.

### Wie man keine Superintelligenz baut: globale Limits für Training und Inferenz-Rechenleistung

Mit diesen Überlegungen – insbesondere bezüglich der Rechenleistung – im Hinterkopf können wir besprechen, wie die Tore zur künstlichen Superintelligenz geschlossen werden können; wir wenden uns dann der Verhinderung vollständiger AGI zu und der Verwaltung von KI-Modellen, während sie sich menschlichen Fähigkeiten in verschiedenen Aspekten nähern und sie übertreffen.

Die erste Zutat ist natürlich das Verständnis, dass Superintelligenz nicht kontrollierbar wäre und dass ihre Konsequenzen fundamental unvorhersagbar sind. Zumindest China und die USA müssen unabhängig voneinander entscheiden, aus diesem oder anderen Zwecken keine Superintelligenz zu bauen.[^100] Dann ist eine internationale Vereinbarung zwischen ihnen und anderen nötig, mit einem starken Verifikations- und Durchsetzungsmechanismus, um alle Parteien zu versichern, dass ihre Rivalen nicht abtrünnig werden und beschließen zu würfeln.

Um verifizierbar und durchsetzbar zu sein, sollten die Limits harte Grenzen sein und so eindeutig wie möglich. Das scheint ein praktisch unmögliches Problem zu sein: die Fähigkeiten komplexer Software mit unvorhersagbaren Eigenschaften weltweit zu begrenzen. Glücklicherweise ist die Situation viel besser als das, weil genau das, was fortgeschrittene KI möglich gemacht hat – eine riesige Menge an Rechenleistung – viel, viel einfacher zu kontrollieren ist. Obwohl es immer noch einige mächtige und gefährliche Systeme erlauben könnte, kann ein *Kontrollverlust zur Superintelligenz* wahrscheinlich durch eine harte Obergrenze für die Menge an Rechenleistung verhindert werden, die in ein neuronales Netzwerk fließt, zusammen mit einem Ratenlimit für die Menge an Inferenz, die ein KI-System (aus verbundenen neuronalen Netzwerken und anderer Software) durchführen kann. Eine spezifische Version davon wird unten vorgeschlagen.

Es mag scheinen, als würde das Setzen harter globaler Limits für KI-Rechenleistung riesige Mengen internationaler Koordination und aufdringlicher, privatsphäre-zerstörender Überwachung erfordern. Glücklicherweise wäre das nicht der Fall. Die extrem [enge und als Flaschenhals wirkende Lieferkette](https://arxiv.org/abs/2402.08797) sorgt dafür, dass, sobald ein Limit rechtlich gesetzt ist (sei es durch Gesetz oder Exekutiverlass), die Verifikation der Einhaltung dieses Limits nur die Beteiligung und Kooperation einer Handvoll großer Unternehmen erfordern würde.[^101]

Ein solcher Plan hat eine Reihe höchst wünschenswerter Eigenschaften. Er ist minimal invasiv in dem Sinne, dass nur wenige große Unternehmen Anforderungen auferlegt bekommen, und nur ziemlich bedeutende Rechenleistungscluster würden regiert. Die relevanten Chips enthalten bereits die Hardware-Fähigkeiten, die für eine erste Version benötigt werden.[^102] Sowohl Implementierung als auch Durchsetzung beruhen auf standardmäßigen rechtlichen Beschränkungen. Aber diese werden durch Nutzungsbedingungen der Hardware und durch Hardware-Kontrollen unterstützt, was die Durchsetzung drastisch vereinfacht und Betrug durch Unternehmen, private Gruppen oder sogar Länder verhindert. Es gibt reichlich Präzedenzfälle dafür, dass Hardware-Unternehmen remote Beschränkungen für ihre Hardware-Nutzung setzen und bestimmte Fähigkeiten extern sperren/entsperren,[^103] auch bei hochleistungsfähigen CPUs in Rechenzentren.[^104] Selbst für den ziemlich kleinen Anteil an Hardware und Organisationen, die betroffen sind, könnte die Aufsicht auf Telemetrie beschränkt werden, ohne direkten Zugang zu Daten oder Modellen selbst; und die Software dafür könnte zur Inspektion offen sein, um zu zeigen, dass keine zusätzlichen Daten aufgezeichnet werden. Das Schema ist international und kooperativ, und recht flexibel und erweiterbar. Da das Limit hauptsächlich auf Hardware und nicht auf Software liegt, ist es relativ agnostisch bezüglich dessen, wie KI-Software-Entwicklung und -Deployment stattfindet, und ist kompatibel mit einer Vielfalt von Paradigmen einschließlich „dezentralisierter" oder „öffentlicher" KI, die darauf abzielt, KI-getriebene Machtkonzentration zu bekämpfen.

Ein rechenleistungsbasierter Torschluss hat aber auch Nachteile. Erstens ist er weit von einer vollständigen Lösung für das Problem der KI-Governance im Allgemeinen entfernt. Zweitens würde das System, wenn Computer-Hardware schneller wird, „mehr und mehr Hardware in kleineren und kleineren Clustern (oder sogar einzelnen GPUs) erfassen".[^105] Es ist auch möglich, dass aufgrund algorithmischer Verbesserungen ein noch niedrigeres Rechenleistungslimit rechtzeitig notwendig würde,[^106] oder dass die Menge der Rechenleistung weitgehend irrelevant wird und das Schließen des Tores stattdessen ein detaillierteres risiko-basiertes oder fähigkeits-basiertes Governance-Regime für KI erforderlich machen würde. Drittens wird ein solches System ungeachtet der Garantien und der kleinen Anzahl betroffener Einheiten zwangsläufig Widerstand bezüglich Privatsphäre und Überwachung schaffen, unter anderem.[^107]

Natürlich wird die Entwicklung und Implementierung eines rechenleistungs-limitierenden Governance-Systems in kurzer Zeit recht herausfordernd sein. Aber es ist absolut machbar.

### A-G-I: Die Dreifachschnittstelle als Basis für Risiko und Politik

Wenden wir uns nun der AGI zu. Harte Linien und Definitionen sind hier schwieriger, weil wir sicherlich Intelligenz haben, die künstlich und allgemein ist, und nach keiner bestehenden Definition wird jeder einverstanden sein, ob oder wann sie existiert. Außerdem ist ein Rechenleistungs- oder Inferenz-Limit ein etwas stumpfes Werkzeug (Rechenleistung ist ein Stellvertreter für Fähigkeit, die dann ein Stellvertreter für Risiko ist), das – außer es ist ziemlich niedrig – unwahrscheinlich AGI verhindern wird, die mächtig genug ist, um gesellschaftliche oder zivilisatorische Störungen oder akute Risiken zu verursachen.

Ich habe argumentiert, dass die akutesten Risiken aus der Dreifachschnittstelle sehr hoher Fähigkeit, hoher Autonomie und großer Allgemeinheit entstehen. Das sind die Systeme, die – falls sie überhaupt entwickelt werden – mit enormer Sorgfalt verwaltet werden müssen. Durch das Schaffen strenger Standards (durch Haftung und Regulierung) für Systeme, die alle drei Eigenschaften kombinieren, können wir die KI-Entwicklung in Richtung sichererer Alternativen lenken.

Wie bei anderen Industrien und Produkten, die möglicherweise Verbraucher oder die Öffentlichkeit schädigen könnten, benötigen KI-Systeme sorgfältige Regulierung durch effektive und ermächtigte Regierungsbehörden. Diese Regulierung sollte die inhärenten Risiken von AGI erkennen und inakzeptabel riskante hochleistungsfähige KI-Systeme daran hindern, entwickelt zu werden.[^108]

Jedoch dauert großangelegte Regulierung, besonders mit echten Zähnen, die sicher von der Industrie bekämpft werden,[^109] Zeit[^110] sowie politische Überzeugung, dass sie notwendig ist.[^111] Angesichts des Fortschrittstempos kann das mehr Zeit dauern, als uns zur Verfügung steht.

Auf einem viel schnelleren Zeitrahmen und während regulatorische Maßnahmen entwickelt werden, können wir Unternehmen die notwendigen Anreize geben, (a) von sehr riskanten Aktivitäten abzusehen und (b) umfassende Systeme für die Bewertung und Minderung von Risiken zu entwickeln, indem wir Haftungsniveaus für die gefährlichsten Systeme klären und erhöhen. Die Idee wäre, die allerhöchsten Haftungsniveaus – strenge und in einigen Fällen persönliche kriminelle – für Systeme in der Dreifachschnittstelle hoher Autonomie-Allgemeinheit-Intelligenz aufzuerlegen, aber „sichere Häfen" zu typischerer verschuldensbasierter Haftung für Systeme zu bieten, bei denen eine dieser Eigenschaften fehlt oder garantiert handhabbar ist. Das heißt, zum Beispiel ein „schwaches" System, das allgemein und autonom ist (wie ein fähiger und vertrauenswürdiger aber begrenzter persönlicher Assistent) würde niedrigeren Haftungsniveaus unterliegen. Ebenso würde ein enges und autonomes System wie ein selbstfahrendes Auto immer noch der bedeutenden Regulierung unterliegen, der es bereits unterliegt, aber nicht verstärkter Haftung. Ähnlich für ein hochfähiges und allgemeines System, das „passiv" und weitgehend unfähig zu unabhängiger Aktion ist. Systeme, denen *zwei* der drei Eigenschaften fehlen, sind noch handhabbarer und sichere Häfen wären noch einfacher zu beanspruchen. Dieser Ansatz spiegelt wider, wie wir mit anderen potenziell gefährlichen Technologien umgehen:[^112] höhere Haftung für gefährlichere Konfigurationen schafft natürliche Anreize für sicherere Alternativen.

Das Standardergebnis solch hoher Haftungsniveaus, die dazu dienen, AGI-Risiko zu *internalisieren* für Unternehmen anstatt es auf die Öffentlichkeit abzuwälzen, ist wahrscheinlich (und hoffentlich!), dass Unternehmen einfach keine vollständige AGI entwickeln, bis und außer sie sie wirklich vertrauenswürdig, sicher und kontrollierbar machen können, da *ihre eigene Führung* die Parteien sind, die Risiken tragen. (Falls das nicht ausreicht, sollte die Gesetzgebung zur Klärung der Haftung auch explizit einstweiligen Rechtsschutz ermöglichen, d.h. einen Richter, der einen Stopp für Aktivitäten anordnet, die eindeutig in der Gefahrenzone sind und argumentativ ein öffentliches Risiko darstellen.) Während Regulierung eingeführt wird, kann die Einhaltung von Regulierung zum sicheren Hafen werden, und die sicheren Häfen von niedriger Autonomie, Engheit oder Schwäche von KI-Systemen können sich in relativ leichtere Regulierungsregime verwandeln.

### Schlüsselbestimmungen eines Torschlusses

Mit der obigen Diskussion im Hinterkopf bietet dieser Abschnitt Vorschläge für Schlüsselbestimmungen, die ein Verbot vollständiger AGI und Superintelligenz implementieren und aufrechterhalten würden, und die Verwaltung menschenwettbewerbsfähiger oder expertenwettbewerbsfähiger Allzweck-KI nahe der vollständigen AGI-Schwelle.[^113] Er hat vier Schlüsselteile: 1) Rechenleistungsbilanzierung und -aufsicht, 2) Rechenleistungsobergrenzen beim Training und Betrieb von KI, 3) ein Haftungsrahmen, und 4) gestufte Sicherheits- und Schutzstandards, die harte regulatorische Anforderungen beinhalten. Diese werden als nächstes knapp beschrieben, mit weiteren Details oder Implementierungsbeispielen in drei begleitenden Tabellen. Wichtig zu bemerken ist, dass diese weit davon entfernt sind, alles zu sein, was notwendig sein wird, um fortgeschrittene KI-Systeme zu regieren; während sie zusätzliche Sicherheits- und Schutzvorteile haben werden, zielen sie darauf ab, das Tor zum Intelligenz-Kontrollverlust zu schließen und die KI-Entwicklung in eine bessere Richtung umzuleiten.

#### 1\. Rechenleistungsbilanzierung und Transparenz

- Eine Standardorganisation (z.B. NIST in den USA gefolgt von ISO/IEEE international) sollte einen detaillierten technischen Standard für die Gesamt-Rechenleistung kodifizieren, die beim Training und Betrieb von KI-Modellen verwendet wird, in FLOP, und die Geschwindigkeit in FLOP/s, mit der sie operieren. Details dafür, wie das aussehen könnte, sind in Anhang A gegeben.[^114]
- Eine Anforderung – entweder durch neue Gesetzgebung oder unter bestehender Autorität[^115] – sollte von Rechtsprechungen auferlegt werden, in denen großangelegtes KI-Training stattfindet, um die Gesamt-FLOP zu berechnen und an eine Regulierungsbehörde oder andere Agentur zu berichten, die beim Training und Betrieb aller Modelle über einer Schwelle von 10<sup>25</sup> FLOP oder 10<sup>18</sup> FLOP/s verwendet werden.[^116]
- Diese Anforderungen sollten schrittweise eingeführt werden, zunächst mit gut dokumentierten Schätzungen nach Treu und Glauben auf vierteljährlicher Basis, mit späteren Phasen, die progressiv höhere Standards erfordern, bis hin zu kryptografisch beglaubigten Gesamt-FLOP und FLOP/s, die jeder Modell-*Ausgabe* beigefügt sind.
- Diese Berichte sollten durch gut dokumentierte Schätzungen der marginalen Energie- und Finanzkosten ergänzt werden, die bei der Generierung jeder KI-Ausgabe verwendet werden.

Begründung: Diese gut berechneten und transparent berichteten Zahlen würden die Basis für Training- und Betriebsobergrenzen bieten sowie einen sicheren Hafen vor höheren Haftungsmaßnahmen (siehe Anhänge C und D).

#### 2\. Training- und Betrieb-Rechenleistungsobergrenzen

- Rechtsprechungen, die KI-Systeme beherbergen, sollten eine harte Grenze für die Gesamt-Rechenleistung auferlegen, die in jede KI-Modell-Ausgabe fließt, beginnend bei 10<sup>27</sup> FLOP[^117] und anpassbar wie angemessen.
- Rechtsprechungen, die KI-Systeme beherbergen, sollten eine harte Grenze für die Rechenleistungsrate von KI-Modell-Ausgaben auferlegen, beginnend bei 10<sup>20</sup> FLOP/s und anpassbar wie angemessen.

Begründung: Gesamt-Rechenleistung ist, obwohl sehr unvollkommen, ein Stellvertreter für KI-Fähigkeit (und Risiko), der konkret messbar und verifizierbar ist, also eine harte Absicherung für die Begrenzung von Fähigkeiten bietet. Ein konkreter Implementierungsvorschlag ist in Anhang B gegeben.

#### 3\. Verstärkte Haftung für gefährliche Systeme

- Die Schaffung und der Betrieb[^118] eines fortgeschrittenen KI-Systems, das hochgradig allgemein, fähig und autonom ist, sollte per Gesetzgebung rechtlich klargestellt werden, dass es strenger, gesamtschuldnerischer und nicht einzelpartei-verschuldensbasierter Haftung unterliegt.[^119]
- Ein rechtliches Verfahren sollte verfügbar sein, um bejahende Sicherheitsfälle zu machen, die sicheren Hafen vor strenger Haftung für Systeme gewähren würden, die klein sind (in Bezug auf Rechenleistung), schwach, eng, passiv oder die ausreichende Sicherheits-, Schutz- und Kontrollierbarkeitsgarantien haben.
- Ein expliziter Pfad und eine Reihe von Bedingungen für einstweiligen Rechtsschutz zum Stoppen von KI-Training- und Inferenz-Aktivitäten, die eine öffentliche Gefahr darstellen, sollten umrissen werden.

Begründung: KI-Systeme können nicht verantwortlich gemacht werden, also müssen wir menschliche Individuen und Organisationen für Schäden verantwortlich machen, die sie verursachen (Haftung).[^120] Unkontrollierbare AGI ist eine Bedrohung für Gesellschaft und Zivilisation und sollte in Abwesenheit eines Sicherheitsfalls als abnormal gefährlich betrachtet werden. Die Beweislast auf Entwickler zu legen zu zeigen, dass mächtige Modelle sicher genug sind, um nicht als „abnormal gefährlich" betrachtet zu werden, schafft Anreize für sichere Entwicklung, zusammen mit Transparenz und Protokollführung, um diese sicheren Häfen zu beanspruchen. Regulierung kann dann Schaden verhindern, wo Abschreckung durch Haftung unzureichend ist. Schließlich sind KI-Entwickler bereits haftbar für Schäden, die sie verursachen, also kann die rechtliche Klärung der Haftung für die riskantesten Systeme sofort erfolgen, ohne dass hochdetaillierte Standards entwickelt werden müssen; diese können sich dann über die Zeit entwickeln. Details sind in Anhang C gegeben.

#### 4\. Sicherheitsregulierung für KI

Ein Regulierungssystem, das großangelegte akute Risiken von KI adressiert, wird mindestens erfordern:

- Die Identifikation oder Schaffung einer angemessenen Reihe von Regulierungsbehörden, wahrscheinlich eine neue Agentur;
- Ein umfassendes Risikobewertungsrahmenwerk;[^121]
- Ein Rahmenwerk für bejahende Sicherheitsfälle, teilweise basierend auf dem Risikobewertungsrahmenwerk, die von Entwicklern gemacht werden sollen, und für Auditing durch *unabhängige* Gruppen und Agenturen;
- Ein gestuftes Lizenzsystem, bei dem Stufen Fähigkeitsniveaus verfolgen.[^122] Lizenzen würden auf Basis von Sicherheitsfällen und Audits für Entwicklung und Deployment von Systemen gewährt. Anforderungen würden von Benachrichtigung am unteren Ende bis zu quantitativen Sicherheits-, Schutz- und Kontrollierbarkeitsgarantien vor der Entwicklung am oberen Ende reichen. Diese würden die Freigabe von Systemen verhindern, bis sie als sicher demonstriert sind, und die Entwicklung inhärent unsicherer Systeme verbieten. Anhang D bietet einen Vorschlag dafür, was solche Sicherheits- und Schutzstandards beinhalten könnten.
- Vereinbarungen, solche Maßnahmen auf die internationale Ebene zu bringen, einschließlich internationaler Gremien zur Harmonisierung von Normen und Standards, und potentiell internationaler Agenturen zur Überprüfung von Sicherheitsfällen.

Begründung: Letztendlich ist Haftung nicht der richtige Mechanismus für die Verhinderung großangelegter Risiken für die Öffentlichkeit durch eine neue Technologie. Umfassende Regulierung mit ermächtigten Regulierungsbehörden wird für KI genauso nötig sein wie für jede andere große Industrie, die ein Risiko für die Öffentlichkeit darstellt.[^123]

Regulierung zur Verhinderung anderer pervasiver aber weniger akuter Risiken wird wahrscheinlich in ihrer Form von Rechtsprechung zu Rechtsprechung variieren. Das Entscheidende ist, die Entwicklung der KI-Systeme zu vermeiden, die so riskant sind, dass diese Risiken nicht handhabbar sind.

### Was dann?

Über das nächste Jahrzehnt, während KI allgegenwärtiger wird und die Kerntechnologie voranschreitet, werden wahrscheinlich zwei Schlüsseldinge passieren. Erstens wird die Regulierung bestehender mächtiger KI-Systeme schwieriger werden, aber noch notwendiger. Es ist wahrscheinlich, dass zumindest einige Maßnahmen zur Adressierung großangelegter Sicherheitsrisiken Vereinbarung auf internationaler Ebene erfordern werden, wobei einzelne Rechtsprechungen auf internationalen Vereinbarungen basierende Regeln durchsetzen.

Zweitens werden Training- und Betrieb-Rechenleistungsobergrenzen schwerer aufrechtzuerhalten, da Hardware billiger und kosteneffizienter wird; sie könnten auch weniger relevant werden (oder noch enger sein müssen) mit Fortschritten in Algorithmen und Architekturen.

Dass die Kontrolle von KI schwieriger wird, bedeutet nicht, dass wir aufgeben sollten! Die Implementierung des in diesem Essay umrissenen Plans würde uns sowohl wertvolle Zeit als auch entscheidende Kontrolle über den Prozess geben, die uns in eine weit, weit bessere Position bringen würde, das existentielle Risiko von KI für unsere Gesellschaft, Zivilisation und Spezies zu vermeiden.

Auf noch längere Sicht werden Entscheidungen zu treffen sein, was wir erlauben. Wir können uns immer noch dafür entscheiden, eine Form wirklich kontrollierbarer AGI zu schaffen, soweit das sich als möglich erweist. Oder wir können entscheiden, dass die Führung der Welt besser den Maschinen überlassen wird, wenn wir uns selbst überzeugen können, dass sie einen besseren Job dabei machen und uns gut behandeln werden. Aber das sollten Entscheidungen sein, die mit tiefem wissenschaftlichem Verständnis von KI in der Hand getroffen werden, und nach bedeutsamer globaler inklusiver Diskussion, nicht in einem Rennen zwischen Tech-Moguln mit dem Großteil der Menschheit völlig unbeteiligt und unwissend.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) Zusammenfassung der A-G-I und Superintelligenz-Governance via Haftung und Regulierung. Haftung ist am höchsten, und Regulierung am stärksten, bei der Dreifachschnittstelle von Autonomie, Allgemeinheit und Intelligenz. Sichere Häfen vor strenger Haftung und starker Regulierung können durch bejahende Sicherheitsfälle erhalten werden, die demonstrieren, dass ein System schwach und/oder eng und/oder passiv ist. Obergrenzen für Gesamt-Training-Rechenleistung und Inferenz-Rechenleistungsrate, verifiziert und durchgesetzt rechtlich und unter Verwendung von Hardware- und kryptografischen Sicherheitsmaßnahmen, sichern Sicherheit ab, indem sie vollständige AGI vermeiden und Superintelligenz effektiv verbieten.

[^87]: Höchstwahrscheinlich wird die Verbreitung dieser Erkenntnis entweder intensive Anstrengungen von Bildungs- und Advocacy-Gruppen erfordern, die diesen Fall machen, oder eine ziemlich bedeutende KI-verursachte Katastrophe. Wir können hoffen, dass es ersteres sein wird.

[^88]: Paradoxerweise sind wir es gewohnt, dass die Natur unsere Technologie begrenzt, indem sie sie sehr schwer zu entwickeln macht, besonders wissenschaftlich. Aber das ist bei KI nicht mehr der Fall: Die wichtigsten wissenschaftlichen Probleme erweisen sich als einfacher als erwartet. Wir können nicht darauf zählen, dass die Natur uns hier vor uns selbst rettet – wir werden es selbst tun müssen.

[^89]: Wo genau stoppen wir bei der Entwicklung neuer Systeme? Hier sollten wir ein Vorsorgeprinzip anwenden. Sobald ein System deployed ist, und besonders sobald dieses Niveau der Systemfähigkeit proliferiert, ist es außerordentlich schwierig zurückzurollen. Und wenn ein System *entwickelt* ist (besonders unter großen Kosten und Anstrengungen), wird es enormen Druck geben, es zu nutzen oder zu deployen, und Versuchung für es, geleakt oder gestohlen zu werden. Systeme zu entwickeln und *dann* zu entscheiden, ob sie tiefgreifend unsicher sind, ist ein gefährlicher Weg.

[^90]: Es wäre auch weise, KI-Entwicklung zu verbieten, die intrinsisch gefährlich ist, wie sich selbst replizierende und evolvierende Systeme, solche, die darauf ausgelegt sind, aus der Einschließung zu entkommen, solche, die sich autonom selbst verbessern können, absichtlich täuschende und bösartige KI, etc.

[^91]: Bemerke, das bedeutet nicht notwendigerweise auf internationaler Ebene *durchgesetzt* von irgendeiner Art globaler Körperschaft: stattdessen könnten souveräne Nationen vereinbarte Regeln durchsetzen, wie in vielen Verträgen.

[^92]: Wie wir unten sehen werden, würde die Natur der KI-Berechnung etwas Hybrides erlauben; aber internationale Kooperation wird immer noch nötig sein.

[^93]: Zum Beispiel werden die Maschinen, die zum Ätzen KI-relevanter Chips benötigt werden, nur von einer Firma hergestellt, ASML (trotz vieler anderer Versuche, das zu tun), die überwiegende Mehrheit relevanter Chips wird von einer Firma hergestellt, TSMC (trotz anderer, die versuchen zu konkurrieren), und das Design und die Konstruktion von Hardware aus diesen Chips wird nur von wenigen gemacht, einschließlich NVIDIA, AMD und Google.

[^94]: Am wichtigsten hält jeder Chip einen einzigartigen und unzugänglichen kryptografischen privaten Schlüssel, den er verwenden kann, um Dinge zu „signieren".

[^95]: Standardmäßig wäre das das Unternehmen, das die Chips verkauft, aber andere Modelle sind möglich und potentiell nützlich.

[^96]: Ein Gouverneur kann den Standort eines Chips durch Timing des Austauschs signierter Nachrichten mit ihm ermitteln: Die endliche Lichtgeschwindigkeit erfordert, dass der Chip innerhalb eines gegebenen Radius *r* einer „Station" ist, wenn er eine signierte Nachricht in einer Zeit weniger als *r* / *c* zurückgeben kann, wobei *c* die Lichtgeschwindigkeit ist. Durch mehrere Stationen und einiges Verständnis der Netzwerk-Charakteristika kann der Standort des Chips bestimmt werden. Die Schönheit dieser Methode ist, dass der Großteil ihrer Sicherheit von den Gesetzen der Physik geliefert wird. Andere Methoden könnten GPS, Trägheitsverfolgung und ähnliche Technologien verwenden.

[^97]: Alternativ könnten Paare von Chips nur mit expliziter Erlaubnis eines Gouverneurs miteinander kommunizieren dürfen.

[^98]: Das ist entscheidend, weil zumindest derzeit sehr hohe Bandbreite-Verbindung zwischen Chips nötig ist, um große KI-Modelle auf ihnen zu trainieren.

[^99]: Das könnte auch so eingerichtet werden, dass signierte Nachrichten von *N* von *M* verschiedenen Gouverneuren erforderlich sind, was mehreren Parteien erlaubt, Governance zu teilen.

[^100]: Das ist bei weitem nicht beispiellos – zum Beispiel haben Militärs keine Armeen geklonter oder genetisch veränderter Supersoldaten entwickelt, obwohl das wahrscheinlich technologisch möglich ist. Aber sie haben sich *entschieden*, das nicht zu tun, anstatt von anderen daran gehindert zu werden. Die Erfolgsbilanz ist nicht großartig dafür, dass große Weltmächte daran gehindert werden, eine Technologie zu entwickeln, die sie stark entwickeln wollen.

[^101]: Mit ein paar bemerkenswerten Ausnahmen (insbesondere NVIDIA) ist die KI-spezialisierte Hardware ein relativ kleiner Teil des Gesamtgeschäfts und Umsatzmodells dieser Unternehmen. Außerdem ist die Lücke zwischen Hardware, die in fortgeschrittener KI verwendet wird, und „Verbraucher-grade" Hardware bedeutend, also wären die meisten Verbraucher von Computer-Hardware weitgehend unbeeinträchtigt.

[^102]: Für detailliertere Analyse, siehe die kürzlichen Berichte von [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) und [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). Diese fokussieren auf technische Machbarkeit, besonders im Kontext von US-Exportkontrollen, die die Kapazität anderer Länder in high-end Berechnung beschränken sollen; aber das hat offensichtliche Überschneidung mit der hier vorgestellten globalen Beschränkung.

[^103]: Apple-Geräte zum Beispiel werden remote und sicher gesperrt, wenn sie als verloren oder gestohlen gemeldet werden, und können remote reaktiviert werden. Das beruht auf denselben Hardware-Sicherheits-Features, die hier diskutiert werden.

[^104]: Siehe z.B. IBMs [capacity on demand](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) Angebot, Intels [Intel on demand.](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html), und Apples [private cloud compute](https://security.apple.com/blog/private-cloud-compute/).

[^105]: [Diese Studie](https://epochai.org/trends#hardware-trends-section) zeigt, dass historisch dieselbe Performance mit etwa 30% weniger Dollar pro Jahr erreicht wurde. Wenn dieser Trend anhält, könnte es bedeutende Überschneidung zwischen KI- und „Verbraucher"-Chip-Nutzung geben, und im Allgemeinen könnte die Menge benötigter Hardware für hochleistungsfähige KI-Systeme unbequem klein werden.

[^106]: Laut der [gleichen Studie](https://epochai.org/trends#hardware-trends-section) hat gegebene Performance bei Bilderkennung 2,5x weniger Berechnung pro Jahr erfordert. Wenn das auch für die fähigsten KI-Systeme gelten würde, wäre ein Berechnungslimit nicht sehr lange nützlich.

[^107]: Insbesondere auf Länderebene sieht das sehr nach einer Nationalisierung der Berechnung aus, insofern die Regierung viel Kontrolle darüber hätte, wie Rechenleistung genutzt wird. Jedoch für die, die sich wegen Regierungsbeteiligung sorgen, scheint das bei weitem sicherer und vorzuziehen zur mächtigsten KI-*Software* selbst, die via einer Fusion zwischen großen KI-Unternehmen und nationalen Regierungen nationalisiert wird, wie einige zu befürworten beginnen.

[^108]: Ein großer regulatorischer Schritt in Europa wurde mit der Verabschiedung des [EU AI Act](https://artificialintelligenceact.eu/) 2024 gemacht. Er klassifiziert KI nach Risiko: verbietet inakzeptable Systeme, reguliert hochriskante und erlegt Transparenzregeln oder gar keine Maßnahmen für niedrigrisikante Systeme auf. Er wird einige KI-Risiken bedeutend reduzieren und KI-Transparenz sogar für US-Firmen stärken, aber hat zwei Schlüsselmängel. Erstens begrenzte Reichweite: während er für jede Firma gilt, die KI in der EU anbietet, ist die Durchsetzung über US-basierte Firmen schwach, und Militär-KI ist ausgenommen. Zweitens, während er GPAI abdeckt, erkennt er AGI oder Superintelligenz nicht als inakzeptable Risiken oder verhindert ihre Entwicklung – nur ihr EU-Deployment. Als Ergebnis tut er wenig, um die Risiken von AGI oder Superintelligenz einzudämmen.

[^109]: Unternehmen vertreten oft, dass sie für vernünftige Regulierung sind. Aber irgendwie scheinen sie fast immer jede *bestimmte* Regulierung zu bekämpfen; siehe den Kampf um das ziemlich leichte SB1047, dem [die meisten KI-Unternehmen öffentlich oder privat widersprochen haben.](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/)

[^110]: Es war etwa 3 1/2 Jahre von dem Zeitpunkt, als der EU AI Act vorgeschlagen wurde, bis er in Kraft trat.

[^111]: Es wird manchmal ausgedrückt, dass es „zu früh" sei, mit der Regulierung von KI zu beginnen. Angesichts der letzten Anmerkung scheint das kaum wahrscheinlich. Eine andere ausgedrückte Sorge ist, dass Regulierung „Innovation schaden" würde. Aber gute Regulierung ändert nur die Richtung, nicht die Menge der Innovation.

[^112]: Ein interessanter Präzedenzfall ist beim Transport gefährlicher Materialien, die entkommen und Schaden verursachen könnten. Hier haben [Regulierung](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) und [Rechtsprechung](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) strenge Haftung für sehr gefährliche Materialien wie Sprengstoff, Benzin, Gifte, infektiöse Agenten und radioaktiven Abfall etabliert. Andere Beispiele beinhalten [Warnungen auf Pharmazeutika](https://www.medicalnewstoday.com/articles/boxed-warnings), [Klassen von Medizingeräten,](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification) etc.

[^113]: Ein anderer umfassender Vorschlag mit ähnlichen Zielen, der in ["A Narrow Path"](https://www.narrowpath.co/) vorgestellt wird, befürwortet einen zentralisierteren, verbots-basierten Ansatz, der alle Frontier-KI-Entwicklung durch eine einzige internationale Entität leitet, überwacht von starken internationalen Institutionen, mit klaren kategorialen Verboten anstatt graduierten Beschränkungen. Ich würde auch diesen Plan unterstützen; jedoch wird er noch mehr politischen Willen und Koordination erfordern als der hier vorgeschlagene.

[^114]: Einige Richtlinien für einen solchen Standard wurden vom Frontier Model Forum [veröffentlicht](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/). Relativ zu dem hier vorgeschlagenen neigen diese dazu, weniger Präzision und weniger in der Abrechnung enthaltene Rechenleistung zu haben.

[^115]: Der US-KI-Exekutiverlass von 2023 (jetzt widerrufen) erforderte ähnliche aber weniger feinkörnige Berichterstattung. Das sollte durch einen ersetzenden Erlass gestärkt werden.

[^116]: Sehr grob entspricht das für jetzt-gebräuchliche H100-Chips Clustern von etwa 1000, die Inferenz machen; es sind etwa 100 (etwa USD $5M wert) der allerneuesten NVIDIA B200-Chips, die Inferenz machen. In beiden Fällen entspricht die Training-Zahl diesem Cluster, der mehrere Monate rechnet.

[^117]: Diese Menge ist größer als jedes derzeit trainierte KI-System; eine größere oder kleinere Zahl könnte gerechtfertigt sein, da wir besser verstehen, wie KI-Fähigkeit mit Rechenleistung skaliert.

[^118]: Das gilt für die, die die Modelle erstellen und anbieten/hosten, nicht Endnutzer.

[^119]: Grob bedeutet „strenge" Haftung, dass Entwickler *standardmäßig* für Schäden verantwortlich gemacht werden, die von einem Produkt verursacht werden, und ist ein Standard, der für „abnormal gefährliche" Produkte verwendet wird, und (etwas amüsant aber angemessen) wilde Tiere. „Gesamtschuldnerische" Haftung bedeutet, dass Haftung all den Parteien zugewiesen wird, die für ein Produkt verantwortlich sind, und diese Parteien müssen unter sich ausmachen, wer welche Verantwortung trägt. Das ist wichtig für Systeme wie KI mit einer langen und komplexen Wertschöpfungskette.

[^120]: Standard verschuldensbasierte einzelpartei Haftung reicht nicht: Verschulden wird sowohl schwierig zu verfolgen als auch zuzuweisen sein, weil KI-Systeme komplex sind, ihr Betrieb nicht verstanden wird, und viele Parteien bei der Schaffung eines gefährlichen Systems oder einer Ausgabe beteiligt sein können. Zusätzlich werden Klagen Jahre dauern zu verhandeln und wahrscheinlich nur in Geldstrafen resultieren, die für diese Unternehmen belanglos sind, also ist persönliche Haftung für Führungskräfte ebenfalls wichtig.

[^121]: Es sollte keine Ausnahme von Sicherheitskriterien für Open-Weight-Modelle geben. Außerdem sollte bei der Risikobewertung angenommen werden, dass Leitplanken, die entfernt werden können, von weit verfügbaren Modellen entfernt werden, und dass sogar geschlossene Modelle proliferieren werden, außer es gibt sehr hohe Sicherheit, dass sie sicher bleiben.

[^122]: Das hier vorgeschlagene Schema hat regulatorische Prüfung, die durch allgemeine Fähigkeit ausgelöst wird; jedoch macht es Sinn für einige besonders riskante Anwendungsfälle, mehr Prüfung auszulösen – zum Beispiel sollte ein Experten-Virologie-KI-System, selbst wenn eng und passiv, wahrscheinlich in eine höhere Stufe gehen. Der frühere US-Exekutiverlass hatte etwas von dieser Struktur für biologische Fähigkeiten.

[^123]: Zwei klare Beispiele sind Luftfahrt und Medizin, reguliert von der FAA und FDA, und ähnlichen Agenturen in anderen Ländern. Diese Agenturen sind unvollkommen, aber sind absolut vital für das Funktionieren und den Erfolg dieser Industrien gewesen.

## Kapitel 9 - Die Zukunft gestalten — was wir stattdessen tun sollten

KI kann unglaublich viel Gutes in der Welt bewirken. Um alle Vorteile ohne die Risiken zu erhalten, müssen wir sicherstellen, dass KI ein menschliches Werkzeug bleibt.

Wenn wir erfolgreich die Entscheidung treffen, die Menschheit nicht durch Maschinen zu ersetzen – zumindest für eine Weile! – was können wir stattdessen tun? Geben wir das enorme Potenzial der KI als Technologie auf? Auf einer gewissen Ebene ist die Antwort ein einfaches *Nein:* Schließt die Tore zu unkontrollierbarer AGI und Superintelligenz, aber baut *sehr wohl* viele andere Formen von KI sowie die Governance-Strukturen und Institutionen auf, die wir benötigen, um sie zu verwalten.

Aber es gibt noch viel zu sagen; dies zu verwirklichen wäre eine zentrale Aufgabe der Menschheit. Dieser Abschnitt erkundet mehrere Schlüsselthemen:

- Wie wir "Werkzeug"-KI charakterisieren können und welche Formen sie annehmen kann.
- Dass wir (fast) alles erreichen können, was die Menschheit will, ohne AGI, mit Werkzeug-KI.
- Dass Werkzeug-KI-Systeme (wahrscheinlich, prinzipiell) beherrschbar sind.
- Dass sich von AGI abzuwenden nicht bedeutet, bei der nationalen Sicherheit Kompromisse einzugehen – ganz im Gegenteil.
- Dass Machtkonzentration ein reales Problem darstellt. Können wir sie abmildern, ohne Sicherheit zu untergraben?
- Dass wir neue Governance- und Gesellschaftsstrukturen wollen und brauchen werden, und KI kann tatsächlich dabei helfen.

### KI innerhalb der Tore: Werkzeug-KI

Das Drei-Schnittmengen-Diagramm bietet eine gute Möglichkeit, das abzugrenzen, was wir "Werkzeug-KI" nennen können: KI, die ein kontrollierbares Werkzeug für den menschlichen Gebrauch ist, anstatt ein unkontrollierbarer Rivale oder Ersatz. Die am wenigsten problematischen KI-Systeme sind solche, die autonom, aber nicht allgemein oder superkompetent sind (wie ein Auktionsbietbot), oder allgemein, aber nicht autonom oder kompetent (wie ein kleines Sprachmodell), oder kompetent, aber eng und sehr kontrollierbar (wie AlphaGo).[^124] Systeme mit zwei sich überschneidenden Eigenschaften haben breitere Anwendung, aber höheres Risiko und werden erhebliche Managementanstrengungen erfordern. (Nur weil ein KI-System eher ein Werkzeug ist, bedeutet das nicht, dass es inhärent sicher ist, lediglich dass es nicht inhärent *unsicher* ist – man denke an eine Kettensäge im Vergleich zu einem Haustiger.) Das Tor muss zur (vollen) AGI und Superintelligenz an der dreifachen Schnittstelle geschlossen bleiben, und enorme Sorgfalt muss bei KI-Systemen walten, die sich dieser Schwelle nähern.

Aber das lässt viel mächtige KI übrig! Wir können enormen Nutzen aus intelligenten und allgemeinen passiven "Orakeln" und engen Systemen ziehen, aus allgemeinen Systemen auf menschlichem, aber nicht übermenschlichem Niveau, und so weiter. Viele Tech-Unternehmen und Entwickler bauen aktiv solche Werkzeuge und sollten damit fortfahren; wie die meisten Menschen *nehmen* sie implizit an, dass die Tore zu AGI und Superintelligenz geschlossen werden.[^125]

Außerdem können KI-Systeme effektiv zu zusammengesetzten Systemen kombiniert werden, die menschliche Aufsicht wahren und gleichzeitig die Leistungsfähigkeit steigern. Anstatt uns auf unergründliche Black Boxes zu verlassen, können wir Systeme bauen, in denen mehrere Komponenten – sowohl KI als auch traditionelle Software – auf Weisen zusammenarbeiten, die Menschen überwachen und verstehen können.[^126] Während einige Komponenten Black Boxes sein mögen, wäre keine nahe an AGI – nur das zusammengesetzte System als Ganzes wäre sowohl hochallgemein als auch hochkompetent, und das auf eine strikt kontrollierbare Weise.[^127]

#### Sinnvolle und garantierte menschliche Kontrolle

Was bedeutet "strikt kontrollierbar"? Eine Schlüsselidee des "Werkzeug"-Rahmens ist es, Systeme zu ermöglichen – auch wenn sie ziemlich allgemein und mächtig sind –, die garantiert unter sinnvoller menschlicher Kontrolle stehen. Was bedeutet das? Es beinhaltet zwei Aspekte. Erstens ist es eine Designüberlegung: Menschen sollten tief und zentral in das involviert sein, was das System tut, *ohne* wichtige Entscheidungen an die KI zu delegieren. Das ist der Charakter der meisten aktuellen KI-Systeme. Zweitens müssen autonome KI-Systeme, soweit sie autonom sind, Garantien haben, die ihren Handlungsbereich begrenzen. Eine Garantie sollte eine *Zahl* sein, die die Wahrscheinlichkeit charakterisiert, dass etwas passiert, und einen Grund zu glauben, dass diese Zahl stimmt. Das ist es, was wir in anderen sicherheitskritischen Bereichen verlangen, wo Zahlen wie "mittlere Zeit zwischen Ausfällen" und erwartete Anzahl von Unfällen berechnet, untermauert und in Sicherheitsnachweisen veröffentlicht werden.[^128] Die ideale Zahl für Ausfälle ist natürlich null. Und die gute Nachricht ist, dass wir ziemlich nahe herankommen könnten, allerdings mit ganz anderen KI-Architekturen, unter Verwendung von Ideen *formal verifizierter* Eigenschaften von Programmen (einschließlich KI). Die Idee, ausführlich von Omohundro, Tegmark, Bengio, Dalrymple und anderen erforscht (siehe [hier](https://arxiv.org/abs/2309.01933) und [hier](https://arxiv.org/abs/2405.06624)), besteht darin, ein Programm mit bestimmten Eigenschaften zu konstruieren (zum Beispiel: dass ein Mensch es abschalten kann) und formal zu *beweisen*, dass diese Eigenschaften gelten. Dies kann jetzt für ziemlich kurze Programme und einfache Eigenschaften getan werden, aber die (kommende) Macht KI-gestützter Beweissoftware könnte es für viel komplexere Programme (z.B. Wrapper) und sogar KI selbst ermöglichen. Das ist ein sehr ehrgeiziges Programm, aber da der Druck auf die Tore wächst, werden wir einige mächtige Materialien brauchen, die sie verstärken. Mathematischer Beweis könnte einer der wenigen sein, der stark genug ist.

#### Wohin mit der KI-Industrie

Mit umgelenktem KI-Fortschritt wäre Werkzeug-KI immer noch eine enorme Industrie. Was die Hardware betrifft, würden selbst mit Rechenleistungsobergrenzen zur Verhinderung von Superintelligenz Training und Inferenz in kleineren Modellen immer noch riesige Mengen spezialisierter Komponenten erfordern. Auf der Software-Seite sollte die Entschärfung der Explosion bei KI-Modell- und Rechengrößen einfach dazu führen, dass Unternehmen Ressourcen darauf umleiten, die kleineren Systeme besser, vielfältiger und spezialisierter zu machen, anstatt sie einfach größer zu machen.[^129] Es gäbe reichlich Raum – wahrscheinlich mehr – für all jene gewinnbringenden Silicon Valley-Startups.[^130]

### Werkzeug-KI kann (fast) alles liefern, was die Menschheit will, ohne AGI

Intelligenz, ob biologisch oder maschinell, kann im Großen und Ganzen als die Fähigkeit betrachtet werden, Aktivitäten zu planen und auszuführen, die Zukünfte herbeiführen, die mehr im Einklang mit einer Reihe von Zielen stehen. Als solche ist Intelligenz von enormem Nutzen, wenn sie zur Verfolgung weise gewählter Ziele eingesetzt wird. Künstliche Intelligenz zieht riesige Investitionen von Zeit und Anstrengung an, hauptsächlich wegen ihrer versprochenen Vorteile. Also sollten wir fragen: Zu welchem Grad würden wir immer noch die Vorteile der KI ernten, wenn wir ihren Kontrollverlust zur Superintelligenz eindämmen? Die Antwort: Wir könnten überraschend wenig verlieren.

Betrachten wir zunächst, dass aktuelle KI-Systeme bereits sehr mächtig sind, und wir haben wirklich nur an der Oberfläche dessen gekratzt, was mit ihnen getan werden kann.[^131] Sie sind durchaus imstande, "die Führung zu übernehmen" im Sinne des "Verstehens" einer ihnen vorgelegten Frage oder Aufgabe und dessen, was es bräuchte, um diese Frage zu beantworten oder jene Aufgabe zu erledigen.

Als Nächstes ist viel der Begeisterung für moderne KI-Systeme ihrer Allgemeinheit geschuldet; aber einige der fähigsten KI-Systeme – wie solche, die Sprache oder Bilder generieren oder erkennen, wissenschaftliche Vorhersagen und Modellierung betreiben, Spiele spielen usw. – sind viel enger und gut "innerhalb der Tore" in Bezug auf die Rechenleistung.[^132] Diese Systeme sind übermenschlich bei den speziellen Aufgaben, die sie erfüllen. Sie mögen Grenzfall-[^133] (oder [ausnutzbare](https://arxiv.org/abs/2211.00241)) Schwächen aufgrund ihrer Enge haben; jedoch sind *völlig* eng oder *völlig* allgemein nicht die einzigen verfügbaren Optionen: Es gibt viele Architekturen dazwischen.[^134]

Diese KI-Werkzeuge können den Fortschritt in anderen positiven Technologien erheblich beschleunigen, ohne AGI. Um bessere Kernphysik zu betreiben, brauchen wir nicht, dass KI ein Kernphysiker ist – wir haben welche! Wenn wir die Medizin vorantreiben wollen, geben wir den Biologen, Medizinforschern und Chemikern mächtige Werkzeuge. Sie wollen sie und werden sie zu enormem Gewinn nutzen. Wir brauchen keine Serverfarm voller einer Million digitaler Genies; wir haben Millionen von Menschen, deren Genialität KI helfen kann hervorzubringen. Ja, es wird länger dauern, Unsterblichkeit und die Heilung aller Krankheiten zu erreichen. Das ist ein echter Preis. Aber selbst die vielversprechendsten Gesundheitsinnovationen wären von geringem Nutzen, wenn KI-getriebene Instabilität zu globalem Konflikt oder gesellschaftlichem Kollaps führt. Wir schulden es uns selbst, KI-gestärkten Menschen zuerst eine Chance bei dem Problem zu geben.

Und angenommen, es gibt tatsächlich irgendeinen enormen Vorteil von AGI, der nicht von der Menschheit mit innerhalb-der-Tore-Werkzeugen erreicht werden kann. Verlieren wir diese, indem wir *niemals* AGI und Superintelligenz bauen? Bei der Abwägung von Risiken und Belohnungen hier gibt es einen enormen asymmetrischen Vorteil beim Warten versus Eilen: Wir können warten, bis es auf garantiert sichere und vorteilhafte Weise getan werden kann, und fast jeder wird immer noch die Früchte ernten können; wenn wir eilen, könnte es – in den Worten des OpenAI-CEOs Sam Altman – [Licht aus für *uns alle* bedeuten.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

Aber wenn Nicht-AGI-Werkzeuge potenziell so mächtig sind, können wir sie handhaben? Die Antwort ist ein klares... vielleicht.

### Werkzeug-KI-Systeme sind (wahrscheinlich, prinzipiell) handhabbar

Aber es wird nicht einfach sein. Aktuelle hochmoderne KI-Systeme können Menschen und Institutionen beim Erreichen ihrer Ziele erheblich stärken. Das ist im Allgemeinen eine gute Sache! Jedoch gibt es natürliche Dynamiken, solche Systeme zu unserer Verfügung zu haben – plötzlich und ohne viel Zeit für die Gesellschaft, sich anzupassen –, die ernste Risiken bieten, die bewältigt werden müssen. Es lohnt sich, einige große Klassen solcher Risiken zu diskutieren und wie sie verringert werden können, unter der Annahme einer Torschließung.

Eine Klasse von Risiken besteht darin, dass hochleistungsstarke Werkzeug-KI Zugang zu Wissen oder Fähigkeiten ermöglicht, die zuvor an eine Person oder Organisation gebunden waren, wodurch eine Kombination aus hoher Fähigkeit plus hoher Loyalität einer sehr breiten Palette von Akteuren verfügbar gemacht wird. Heute könnte eine Person mit bösen Absichten mit genügend Geld ein Team von Chemikern anheuern, um neue Chemiewaffen zu entwerfen und herzustellen – aber es ist nicht so einfach, dieses Geld zu haben oder das Team zu finden/zusammenzustellen und es zu überzeugen, etwas ziemlich eindeutig Illegales, Unethisches und Gefährliches zu tun. Um zu verhindern, dass KI-Systeme eine solche Rolle spielen, könnten Verbesserungen aktueller Methoden durchaus ausreichen,[^135] solange alle diese Systeme und der Zugang zu ihnen verantwortlich verwaltet werden. Andererseits, wenn mächtige Systeme für allgemeine Nutzung und Modifikation freigegeben werden, sind alle eingebauten Sicherheitsmaßnahmen wahrscheinlich entfernbar. Um also Risiken in dieser Klasse zu vermeiden, werden starke Beschränkungen dessen erforderlich sein, was öffentlich freigegeben werden kann – analog zu Beschränkungen bei Details von nuklearen, explosiven und anderen gefährlichen Technologien.[^136]

Eine zweite Klasse von Risiken entspringt der Skalierung von Maschinen, die sich wie Menschen verhalten oder Menschen imitieren. Auf der Ebene des Schadens für Einzelpersonen umfassen diese Risiken viel effektivere Betrug, Spam und Phishing sowie die Verbreitung nicht einvernehmlicher Deepfakes.[^137] Auf kollektiver Ebene umfassen sie die Störung grundlegender sozialer Prozesse wie öffentliche Diskussion und Debatte, unsere gesellschaftlichen Informations- und Wissenserwerbs-, -verarbeitungs- und -verbreitungssysteme sowie unsere politischen Wahlsysteme. Die Minderung dieses Risikos wird wahrscheinlich (a) Gesetze zur Beschränkung der Nachahmung von Personen durch KI-Systeme und die Haftbarmachung von KI-Entwicklern, die Systeme schaffen, die solche Nachahmungen generieren, (b) Wasserzeichen- und Herkunftssysteme, die (verantwortungsvoll) generierte KI-Inhalte identifizieren und klassifizieren, und (c) neue sozio-technische epistemische Systeme umfassen, die eine vertrauenswürdige Kette von Daten (z.B. Kameras und Aufzeichnungen) durch Fakten, Verständnis und gute Weltmodelle schaffen können.[^138] All das ist möglich, und KI kann bei einigen Teilen davon helfen.

Ein drittes allgemeines Risiko besteht darin, dass Menschen, die derzeit diese Aufgaben erledigen, in dem Maße, wie bestimmte Aufgaben automatisiert werden, weniger finanziellen Wert als Arbeitskraft haben können. Historisch gesehen hat die Automatisierung von Aufgaben Dinge, die durch diese Aufgaben ermöglicht werden, billiger und reichlicher gemacht, während sie die Menschen, die zuvor diese Aufgaben erledigten, in solche sortierte, die immer noch an der automatisierten Version beteiligt sind (im Allgemeinen mit höherer Qualifikation/Bezahlung), und solche, deren Arbeit weniger oder wenig wert ist. Im Saldo ist es schwierig vorherzusagen, in welchen Sektoren mehr versus weniger menschliche Arbeit im resultierenden größeren, aber effizienteren Sektor benötigt wird. Parallel dazu neigt die Automatisierungsdynamik dazu, Ungleichheit und allgemeine Produktivität zu erhöhen, die Kosten bestimmter Güter und Dienstleistungen zu senken (über Effizienzsteigerungen) und die Kosten anderer zu erhöhen (über [Kostenkrankheit](https://en.wikipedia.org/wiki/Baumol_effect)). Für diejenigen auf der benachteiligten Seite der Ungleichheitszunahme ist es zutiefst unklar, ob die Kostensenkung bei diesen bestimmten Gütern und Dienstleistungen die Zunahme bei anderen aufwiegt und zu insgesamt größerem Wohlbefinden führt. Wie wird das also mit KI ablaufen? Wegen der relativen Leichtigkeit, mit der menschliche intellektuelle Arbeit durch allgemeine KI ersetzt werden kann, können wir eine schnelle Version hiervon mit menschenwettbewerbsfähiger Allzweck-KI erwarten.[^139] Wenn wir das Tor zu AGI schließen, werden viele weniger Jobs vollständig durch KI-Agenten ersetzt; aber enorme Arbeitsplatzverdrängung ist dennoch über einen Zeitraum von Jahren wahrscheinlich.[^140] Um weit verbreitetes wirtschaftliches Leiden zu vermeiden, wird es wahrscheinlich notwendig sein, sowohl eine Form universeller Grundvermögen oder -einkommen zu implementieren als auch einen kulturellen Wandel hin zur Wertschätzung und Belohnung menschenzentrierter Arbeit zu bewirken, die schwerer zu automatisieren ist (anstatt zu sehen, wie Arbeitspreise aufgrund des Anstiegs verfügbarer Arbeit, die aus anderen Teilen der Wirtschaft gedrängt wird, fallen.) Andere Konstrukte, wie das der ["Datenwürde"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society) (in dem die menschlichen Produzenten von Trainingsdaten automatisch Lizenzgebühren für den durch diese Daten in KI geschaffenen Wert erhalten), können helfen. Automatisierung durch KI hat auch einen zweiten potenziell nachteiligen Effekt, nämlich *unangemessene* Automatisierung. Neben Anwendungen, bei denen KI einfach eine schlechtere Arbeit leistet, würde dies solche umfassen, bei denen KI-Systeme wahrscheinlich moralische, ethische oder rechtliche Prinzipien verletzen – zum Beispiel bei Leben-und-Tod-Entscheidungen und in juristischen Angelegenheiten. Diese müssen durch Anwendung und Erweiterung unserer aktuellen rechtlichen Rahmen behandelt werden.

Schließlich ist eine bedeutende Bedrohung von innerhalb-der-Tore-KI ihr Einsatz in personalisierter Überzeugung, Aufmerksamkeitsfang und Manipulation. Wir haben in sozialen Medien und anderen Online-Plattformen das Wachstum einer tief verwurzelten Aufmerksamkeitsökonomie (wo Online-Dienste heftig um Nutzeraufmerksamkeit kämpfen) und ["Überwachungskapitalismus"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism)-Systemen (in denen Nutzerinformationen und Profilerstellung zur Kommodifizierung der Aufmerksamkeit hinzugefügt werden) gesehen. Es ist so gut wie sicher, dass mehr KI in den Dienst beider gestellt wird. KI wird bereits stark in süchtig machenden Feed-Algorithmen verwendet, aber das wird sich zu süchtig machenden KI-generierten Inhalten entwickeln, die angepasst sind, um von einer einzelnen Person zwanghaft konsumiert zu werden. Und die Eingaben, Reaktionen und Daten dieser Person werden in die Aufmerksamkeits-/Werbemaschine eingespeist, um den Teufelskreis fortzusetzen. Außerdem werden, da KI-Helfer, die von Tech-Unternehmen bereitgestellt werden, zur Schnittstelle für mehr Online-Leben werden, sie wahrscheinlich Suchmaschinen und Feeds als Mechanismus ersetzen, über den Überzeugung und Monetarisierung von Kunden erfolgt. Das Versagen unserer Gesellschaft, diese Dynamiken bisher zu kontrollieren, verheißt nichts Gutes. Ein Teil dieser Dynamik kann über Vorschriften bezüglich Privatsphäre, Datenrechten und Manipulation verringert werden. Mehr an die Wurzel des Problems zu gehen, könnte andere Perspektiven erfordern, wie die loyaler KI-Assistenten (unten diskutiert.)

Die Schlussfolgerung dieser Diskussion ist hoffnungsvoll: innerhalb-der-Tore werkzeugbasierte Systeme – zumindest solange sie in Macht und Fähigkeit vergleichbar mit den heutigen modernsten Systemen bleiben – sind wahrscheinlich handhabbar, wenn der Wille und die Koordination dazu vorhanden sind. Anständige menschliche Institutionen, gestärkt durch KI-Werkzeuge,[^141] können es schaffen. Wir könnten auch dabei scheitern. Aber es ist schwer zu sehen, wie das Zulassen mächtigerer Systeme helfen würde – außer indem man sie in Charge setzt und auf das Beste hofft.

### Nationale Sicherheit

Wettkämpfe um KI-Vorherrschaft – angetrieben von nationaler Sicherheit oder anderen Motivationen – treiben uns zu unkontrollierten mächtigen KI-Systemen, die dazu neigen würden, Macht zu absorbieren, anstatt sie zu verleihen. Ein AGI-Wettlauf zwischen den USA und China ist ein Wettlauf zu bestimmen, welche Nation zuerst Superintelligenz bekommt.

Was sollten also die Verantwortlichen für nationale Sicherheit stattdessen tun? Regierungen haben starke Erfahrung im Aufbau kontrollierbarer und sicherer Systeme, und sie sollten sich darauf in der KI verdoppeln, indem sie die Art von Infrastrukturprojekten unterstützen, die am besten gelingen, wenn sie im großen Maßstab und mit staatlichem Imprimatur durchgeführt werden.

Anstatt eines rücksichtslosen "Manhattan-Projekts" in Richtung AGI[^142] könnte die US-Regierung ein Apollo-Projekt für kontrollierbare, sichere, vertrauenswürdige Systeme starten. Das könnte zum Beispiel umfassen:

- Ein großes Programm zur (a) Entwicklung der On-Chip-Hardware-Sicherheitsmechanismen und (b) der Infrastruktur zur Verwaltung der Rechenleistungsseite mächtiger KI. Diese könnten auf dem US [CHIPS Act](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) und [Exportkontrollregime](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion) aufbauen.
- Eine großangelegte Initiative zur Entwicklung formaler Verifikationstechniken, sodass bestimmte Eigenschaften von KI-Systemen (wie ein Ausschalter) *bewiesen* werden können, dass sie vorhanden oder abwesend sind. Dies kann KI selbst nutzen, um Beweise von Eigenschaften zu entwickeln.
- Eine nationale Anstrengung zur Schaffung von Software, die nachweislich sicher ist, angetrieben von KI-Werkzeugen, die bestehende Software in nachweislich sichere Frameworks umkodieren können.
- Ein nationales Investitionsprojekt im wissenschaftlichen Fortschritt unter Verwendung von KI,[^143] geführt als Partnerschaft zwischen DOE, NSF und NIH.

Im Allgemeinen gibt es eine enorme Angriffsfläche auf unsere Gesellschaft, die uns verwundbar für Risiken von KI und ihrem Missbrauch macht. Der Schutz vor einigen dieser Risiken wird regierungsgroße Investitionen und Standardisierung erfordern. Diese würden weitaus mehr Sicherheit bieten als Benzin ins Feuer von Wettläufen in Richtung AGI zu gießen. Und wenn KI in Waffen und Kommando-und-Kontrollsysteme eingebaut werden soll, ist es entscheidend, dass die KI vertrauenswürdig und sicher ist, was aktuelle KI einfach nicht ist.

### Machtkonzentration und ihre Abmilderung

Dieser Essay hat sich auf die Idee menschlicher Kontrolle über KI und ihr mögliches Scheitern konzentriert. Aber eine andere gültige Linse, durch die man die KI-Situation betrachten kann, ist die der *Machtkonzentration.* Die Entwicklung sehr mächtiger KI droht, Macht entweder in die sehr wenigen und sehr großen Unternehmen zu konzentrieren, die sie entwickelt haben und kontrollieren werden, oder in Regierungen, die KI als neues Mittel nutzen, um ihre eigene Macht und Kontrolle zu erhalten, oder in die KI-Systeme selbst. Oder eine unheilige Mischung aus dem Obigen. In jedem dieser Fälle verliert der Großteil der Menschheit Macht, Kontrolle und Handlungsfähigkeit. Wie könnten wir das bekämpfen?

Der allererste und wichtigste Schritt ist natürlich eine Torschließung zu intelligenter-als-menschlicher AGI und Superintelligenz. Diese können explizit Menschen und Menschengruppen direkt ersetzen. Wenn sie unter Unternehmens- oder Regierungskontrolle stehen, werden sie Macht in diesen Unternehmen oder Regierungen konzentrieren; wenn sie "frei" sind, werden sie Macht in sich selbst konzentrieren. Nehmen wir also an, die Tore sind geschlossen. Was dann?

Eine vorgeschlagene Lösung für Machtkonzentration ist "Open-Source"-KI, wo Modellgewichte frei oder weit verfügbar sind. Aber wie bereits erwähnt, können, sobald ein Modell offen ist, die meisten Sicherheitsmaßnahmen oder Leitplanken (und werden allgemein) entfernt werden. Es gibt also eine akute Spannung zwischen einerseits Dezentralisierung und andererseits Sicherheit, Schutz und menschlicher Kontrolle von KI-Systemen. Es gibt auch Gründe, skeptisch zu sein, dass offene Modelle von sich aus Machtkonzentration in KI sinnvoll bekämpfen werden, mehr als sie es bei Betriebssystemen getan haben (immer noch dominiert von Microsoft, Apple und Google trotz offener Alternativen).[^144]

Dennoch könnte es Wege geben, diesen Kreis zu quadrieren – Risiken zu zentralisieren und abzumildern, während Fähigkeit und wirtschaftliche Belohnung dezentralisiert werden. Dies erfordert ein Überdenken sowohl dessen, wie KI entwickelt wird, als auch wie ihre Vorteile verteilt werden.

Neue Modelle öffentlicher KI-Entwicklung und -Eigentümerschaft würden helfen. Das könnte mehrere Formen annehmen: regierungsentwickelte KI (unter demokratischer Aufsicht),[^145] gemeinnützige KI-Entwicklungsorganisationen (wie Mozilla für Browser) oder Strukturen, die sehr weit verbreitetes Eigentum und Governance ermöglichen. Schlüssel ist, dass diese Institutionen explizit beauftragt wären, dem öffentlichen Interesse zu dienen, während sie unter starken Sicherheitsbeschränkungen operieren.[^146] Wohlgestaltete Regulierungs- und Standards-/Zertifizierungsregime werden ebenfalls vital sein, damit KI-Produkte, die von einem lebendigen Markt angeboten werden, wirklich nützlich bleiben, anstatt gegenüber ihren Nutzern ausbeuterisch zu werden.

In Bezug auf wirtschaftliche Machtkonzentration können wir Herkunftsverfolgung und "Datenwürde" nutzen, um sicherzustellen, dass wirtschaftliche Vorteile weiter fließen. Insbesondere stammt die meiste KI-Macht jetzt (und in der Zukunft, wenn wir die Tore geschlossen halten) aus menschlich generierten Daten, seien es direkte Trainingsdaten oder menschliches Feedback. Wenn KI-Unternehmen verpflichtet wären, Datenanbieter fair zu entschädigen,[^147] könnte dies zumindest helfen, die wirtschaftlichen Belohnungen breiter zu verteilen. Darüber hinaus könnte ein anderes Modell öffentliches Eigentum an bedeutenden Anteilen großer KI-Unternehmen sein. Zum Beispiel könnten Regierungen, die KI-Unternehmen besteuern können, einen Bruchteil der Einnahmen in einen Staatsfonds investieren, der Aktien der Unternehmen hält und Dividenden an die Bevölkerung zahlt.[^148]

Entscheidend bei diesen Mechanismen ist es, die Macht der KI selbst zu nutzen, um Macht besser zu verteilen, anstatt einfach KI-getriebene Machtkonzentration mit Nicht-KI-Mitteln zu bekämpfen. Ein mächtiger Ansatz wäre durch wohlgestaltete KI-Assistenten, die mit echter treuhänderischer Pflicht gegenüber ihren Nutzern operieren – die Interessen der Nutzer an erste Stelle setzen, besonders über die der Unternehmensanbieter.[^149] Diese Assistenten müssen wirklich vertrauenswürdig, technisch kompetent aber angemessen begrenzt basierend auf Anwendungsfall und Risikoniveau und allen über öffentliche, gemeinnützige oder zertifizierte gewinnorientierte Kanäle weit verfügbar sein. Genauso wie wir niemals einen menschlichen Assistenten akzeptieren würden, der heimlich gegen unsere Interessen für eine andere Partei arbeitet, sollten wir keine KI-Assistenten akzeptieren, die ihre Nutzer für Unternehmensvorteile überwachen, manipulieren oder Wert aus ihnen ziehen.

Eine solche Transformation würde die aktuelle Dynamik grundlegend verändern, wo Individuen allein mit riesigen (KI-gestützten) Unternehmens- und Bürokratiemaschinen verhandeln müssen, die Wertextraktion über menschliches Wohlergehen priorisieren. Während es viele mögliche Ansätze gibt, KI-getriebene Macht breiter zu verteilen, wird keiner standardmäßig entstehen: Sie müssen bewusst entwickelt und regiert werden mit Mechanismen wie treuhänderischen Anforderungen, öffentlicher Bereitstellung und gestuftem Zugang basierend auf Risiko.

Ansätze zur Abmilderung von Machtkonzentration können bedeutendem Gegenwind von etablierten Mächten begegnen.[^150] Aber es gibt Wege zur KI-Entwicklung, die nicht erfordern, zwischen Sicherheit und konzentrierter Macht zu wählen. Indem wir jetzt die richtigen Institutionen aufbauen, könnten wir sicherstellen, dass die Vorteile der KI weit geteilt werden, während ihre Risiken sorgfältig gehandhabt werden.

### Neue Governance- und Gesellschaftsstrukturen

Unsere aktuellen Governance-Strukturen kämpfen: Sie reagieren langsam, sind oft von Sonderinteressen gefangen und [werden vom öffentlichen Vertrauen zunehmend schwer belastet.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) Dennoch ist das kein Grund, sie aufzugeben – ganz im Gegenteil. Einige Institutionen mögen Ersetzung brauchen, aber breiter brauchen wir neue Mechanismen, die unsere bestehenden Strukturen verstärken und ergänzen können und ihnen helfen, in unserer sich schnell entwickelnden Welt besser zu funktionieren.

Viel von unserer institutionellen Schwäche entspringt nicht formalen Regierungsstrukturen, sondern degradierten sozialen Institutionen: unseren Systemen zur Entwicklung gemeinsamen Verständnisses, zur Koordinierung von Handlungen und zur Führung sinnvoller Diskurse. Bisher hat KI diese Degradierung beschleunigt, unsere Informationskanäle mit generiertem Inhalt geflutet, uns zu den polarisierendsten und spaltendsten Inhalten hingewiesen und es schwerer gemacht, Wahrheit von Fiktion zu unterscheiden.

Aber KI könnte tatsächlich helfen, diese sozialen Institutionen wieder aufzubauen und zu stärken. Betrachten wir drei entscheidende Bereiche:

Erstens könnte KI dabei helfen, Vertrauen in unsere epistemischen Systeme wiederherzustellen – unsere Wege zu wissen, was wahr ist. Wir könnten KI-gestützte Systeme entwickeln, die die Herkunft von Informationen verfolgen und verifizieren, von Rohdaten durch Analyse zu Schlussfolgerungen. Diese Systeme könnten kryptographische Verifikation mit ausgeklügelter Analyse kombinieren, um Menschen zu helfen zu verstehen, nicht nur ob etwas wahr ist, sondern wie wir wissen, dass es wahr ist.[^151] Loyale KI-Assistenten könnten beauftragt werden, den Details zu folgen, um sicherzustellen, dass sie stimmen.

Zweitens könnte KI neue Formen großmaßstäblicher Koordination ermöglichen. Viele unserer drängendsten Probleme – vom Klimawandel bis zur Antibiotikaresistenz – sind grundlegend Koordinationsprobleme. Wir [stecken in Situationen fest, die schlechter sind, als sie für fast jeden sein könnten](https://equilibriabook.com/), weil sich kein Individuum oder Gruppe den ersten Schritt leisten kann. KI-Systeme könnten helfen, indem sie komplexe Anreizstrukturen modellieren, tragfähige Wege zu besseren Ergebnissen identifizieren und die Vertrauensbildungs- und Verpflichtungsmechanismen erleichtern, die nötig sind, um dorthin zu gelangen.

Vielleicht am faszinierendsten könnte KI völlig neue Formen sozialen Diskurses ermöglichen. Stellen Sie sich vor, "mit einer Stadt sprechen" zu können[^152] – nicht nur Statistiken zu sehen, sondern einen sinnvollen Dialog mit einem KI-System zu führen, das die Ansichten, Erfahrungen, Bedürfnisse und Aspirationen von Millionen von Bewohnern verarbeitet und synthetisiert. Oder betrachten Sie, wie KI echten Dialog zwischen Gruppen erleichtern könnte, die derzeit aneinander vorbeireden, indem sie jeder Seite hilft, die tatsächlichen Anliegen und Werte der anderen besser zu verstehen, anstatt ihre Karikaturen voneinander.[^153] Oder KI könnte geschickte, glaubwürdig neutrale Vermittlung von Streitigkeiten zwischen Menschen oder sogar großen Menschengruppen anbieten (die alle direkt und individuell mit ihr interagieren könnten!) Aktuelle KI ist völlig imstande, diese Arbeit zu tun, aber die Werkzeuge dazu werden nicht von selbst oder über Marktanreize entstehen.

Diese Möglichkeiten mögen utopisch klingen, besonders angesichts der aktuellen Rolle der KI bei der Degradierung von Diskurs und Vertrauen. Aber genau deshalb müssen wir diese positiven Anwendungen aktiv entwickeln. Indem wir die Tore zu unkontrollierbarer AGI schließen und KI priorisieren, die menschliche Handlungsfähigkeit verstärkt, können wir technologischen Fortschritt in Richtung einer Zukunft lenken, wo KI als Kraft für Ermächtigung, Widerstandsfähigkeit und kollektiven Fortschritt dient.

[^124]: Das gesagt, sich von der Drei-Schnittmenge fernzuhalten ist leider nicht so einfach, wie man vielleicht möchte. Fähigkeiten sehr stark in einem der drei Aspekte zu forcieren, neigt dazu, sie in den anderen zu erhöhen. Insbesondere könnte es schwer sein, eine extrem allgemeine und fähige Intelligenz zu schaffen, die nicht leicht autonom gemacht werden kann. Ein Ansatz ist es, Modelle ["myopisch"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) zu trainieren – Systeme mit verkrüppelter Planungsfähigkeit. Ein anderer wäre, sich auf die Entwicklung reiner ["Orakel"](https://arxiv.org/abs/1711.05541)-Systeme zu konzentrieren, die sich scheuen würden, handlungsorientierte Fragen zu beantworten.

[^125]: Viele Unternehmen scheitern daran zu erkennen, dass auch sie schließlich durch AGI ersetzt würden, auch wenn es länger dauert – wenn sie das täten, könnten sie etwas weniger an diesen Toren drücken!

[^126]: KI-Systeme könnten in effizienteren, aber weniger verständlichen Weisen kommunizieren, aber menschliches Verständnis zu bewahren sollte Priorität haben.

[^127]: Diese Idee modularer, interpretierbarer KI wurde von mehreren Forschern detailliert entwickelt; siehe z.B. das ["Comprehensive AI Services"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf)-Modell von Drexler, die ["Open Agency Architecture"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai) von Dalrymple und anderen. Während solche Systeme mehr Entwicklungsaufwand als monolithische neuronale Netzwerke mit massiver Rechenleistung erfordern könnten, ist das genau, wo Rechenleistungslimits helfen – indem sie den sichereren, transparenteren Pfad auch zum praktischeren machen.

[^128]: Zu Sicherheitsnachweisen im Allgemeinen siehe [dieses Handbuch](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). Speziell zu KI siehe [Wasil et al.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [Clymer et al.](https://arxiv.org/abs/2403.10462), [Buhl et al.](https://arxiv.org/abs/2410.21572) und [Balesni et al.](https://arxiv.org/abs/2411.03336).

[^129]: Wir sehen tatsächlich bereits diesen Trend, getrieben nur durch die hohen Kosten der Inferenz: kleinere und mehr spezialisierte Modelle, "destilliert" aus größeren und fähig, auf weniger teurer Hardware zu laufen.

[^130]: Mir ist verständlich, warum die vom KI-Tech-Ökosystem Begeisterten das, was sie als belastende Regulierung ihrer Industrie sehen, ablehnen. Aber es ist ehrlich verwirrend für mich, warum etwa ein Risikokapitalgeber einen Kontrollverlust zu AGI und Superintelligenz zulassen wollen würde. Diese Systeme (und Unternehmen, solange sie unter Unternehmenskontrolle bleiben) werden *alle Startups als Snack fressen*. Wahrscheinlich sogar *früher* als andere Industrien zu fressen. Jeder, der in ein blühendes KI-Ökosystem investiert, sollte priorisieren sicherzustellen, dass AGI-Entwicklung nicht zu Monopolisierung durch wenige dominante Akteure führt.

[^131]: Wie der Ökonom und ehemalige Deepmind-Forscher Michael Webb [es ausdrückte](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/): "Ich denke, wenn wir heute alle Entwicklung größerer Sprachmodelle stoppen würden, also GPT-4 und Claude und was auch immer, und sie sind die letzten Dinge, die wir in dieser Größe trainieren – also erlauben wir viel mehr Iteration bei Dingen dieser Größe und alle Arten von Feinabstimmung, aber nichts Größeres als das, keine größeren Fortschritte – nur was wir heute haben, denke ich, reicht aus, um 20 oder 30 Jahre unglaublichen Wirtschaftswachstums zu antreiben."

[^132]: Zum Beispiel nutzte DeepMinds AlphaFold-System nur ein Hunderttausendstel der FLOP-Zahl von GPT-4.

[^133]: Die Schwierigkeit selbstfahrender Autos ist hier wichtig zu bemerken: Während nominell eine enge Aufgabe und mit fairer Zuverlässigkeit mit relativ kleinen KI-Systemen erreichbar, ist umfangreiches reales Weltwissen und Verständnis notwendig, um Zuverlässigkeit auf das in solch einer sicherheitskritischen Aufgabe benötigte Niveau zu bekommen.

[^134]: Zum Beispiel würden wir bei einem gegebenen Rechenbudget wahrscheinlich Allzweck-KI-Modelle sehen, die bei (sagen wir) der Hälfte dieses Budgets vortrainiert werden, und die andere Hälfte wird verwendet, um sehr hohe Fähigkeit in einem engeren Aufgabenbereich zu trainieren. Das würde übermenschliche enge Fähigkeit geben, unterstützt von nahezu menschlicher allgemeiner Intelligenz.

[^135]: Die derzeit dominante Alignment-Technik ist "Verstärkungslernen durch menschliches Feedback" [(RLHF)](https://arxiv.org/abs/1706.03741) und nutzt menschliches Feedback, um ein Belohnungs-/Bestrafungssignal für das Verstärkungslernen des KI-Modells zu schaffen. Diese und verwandte Techniken wie [konstitutionelle KI](https://arxiv.org/abs/2212.08073) funktionieren überraschend gut (obwohl ihnen Robustheit fehlt und sie mit bescheidener Anstrengung umgangen werden können.) Zusätzlich sind aktuelle Sprachmodelle im Allgemeinen kompetent genug bei gesundem Menschenverstand-Denken, dass sie keine törichten moralischen Fehler machen werden. Das ist etwas von einem Sweet Spot: klug genug, um zu verstehen, was Menschen wollen (soweit es definiert werden kann), aber nicht klug genug, um ausgeklügelte Täuschungen zu planen oder enormen Schaden anzurichten, wenn sie es falsch verstehen.

[^136]: Auf lange Sicht wird jedes Niveau von KI-Fähigkeit, das entwickelt wird, wahrscheinlich proliferieren, da es letztendlich Software ist und nützlich. Wir werden robuste Mechanismen brauchen, um uns gegen die Risiken zu verteidigen, die solche Systeme darstellen. Aber wir *haben das jetzt nicht*, also müssen wir sehr besonnen sein, wie viel mächtige KI-Modelle proliferieren dürfen.

[^137]: Die große Mehrheit davon sind nicht einvernehmliche pornographische Deepfakes, einschließlich von Minderjährigen.

[^138]: Viele Zutaten für solche Lösungen existieren in Form von "Bot-oder-nicht"-Gesetzen (im EU AI Act unter anderen Orten), [industriellen Herkunftsverfolgungstechnologien](https://c2pa.org/), [innovativen Nachrichtenaggregatorren](https://www.improvethenews.org/), Vorhersage-[aggregatoren](https://metaculus.com/) und Märkten, etc.

[^139]: Die Automatisierungswelle folgt möglicherweise nicht früheren Mustern, insofern relativ *hochqualifizierte* Aufgaben wie Qualitätsschreibung, Gesetzesinterpretation oder medizinische Beratung genauso oder sogar verwundbarer für Automatisierung sein können als geringqualifizierte Aufgaben.

[^140]: Für sorgfältige Modellierung des Effekts von AGI auf Löhne siehe den Bericht [hier](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek) und grausige Details [hier](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0) von Anton Korinek und Kollaborateuren. Sie finden, dass, während mehr Teile von Jobs automatisiert werden, Produktivität und Löhne steigen – bis zu einem Punkt. Sobald *zu* viel automatisiert ist, steigt die Produktivität weiter, aber die Löhne stürzen ab, weil Menschen vollständig durch effiziente KI ersetzt werden. Deshalb ist das Schließen der Tore so nützlich: Wir bekommen die Produktivität ohne die verschwundenen menschlichen Löhne.

[^141]: Es gibt viele Wege, wie KI als und zum Aufbau "defensiver" Technologien genutzt werden kann, um Schutz und Management robuster zu machen. Siehe [diesen](https://vitalik.eth.limo/general/2025/01/05/dacc2.html) einflussreichen Post, der diese "D/acc"-Agenda beschreibt.

[^142]: Etwas ironisch würde ein US-Manhattan-Projekt wahrscheinlich wenig tun, um Zeitlinien in Richtung AGI zu beschleunigen – der Zeiger menschlicher und finanzieller Investition in KI-Fortschritt ist bereits bei 11 angepinnt. Die primären Ergebnisse wären, ein ähnliches Projekt in China zu inspirieren (das bei nationalen Infrastrukturprojekten glänzt), internationale Abkommen zur Begrenzung der Risiken von KI viel schwerer zu machen und andere geopolitische Gegner der USA wie Russland zu alarmieren.

[^143]: Das ["National AI Research Resource"](https://nairrpilot.org/)-Programm ist ein guter aktueller Schritt in diese Richtung und sollte erweitert werden.

[^144]: Siehe [diese Analyse](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807) der verschiedenen Bedeutungen und Implikationen von "offen" in Tech-Produkten und wie einige zu mehr, anstatt weniger Verankerung von Dominanz geführt haben.

[^145]: Pläne in den USA für eine [National AI Research Resource](https://nairratdoe.ornl.gov/) und der kürzliche Start einer [europäischen KI-Stiftung](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/) sind interessante Schritte in diese Richtung.

[^146]: Die Herausforderung hier ist nicht technisch, sondern institutionell – wir brauchen dringend reale Beispiele und Experimente darin, wie KI-Entwicklung im öffentlichen Interesse aussehen könnte.

[^147]: Das geht gegen aktuelle Big-Tech-Geschäftsmodelle und würde sowohl rechtliche Schritte als auch neue Normen erfordern.

[^148]: Nur einige Regierungen werden dazu imstande sein. Eine radikalere Idee ist [ein universeller Fonds dieses Typs, unter gemeinsamem Eigentum aller Menschen.](https://futureoflife.org/project/the-windfall-trust/)

[^149]: Für eine ausführliche Darlegung dieses Falls siehe [dieses Paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338) über KI-Loyalität. Leider ist die Standardtrajektorie von KI-Assistenten wahrscheinlich eine, wo sie zunehmend illoyal sind.

[^150]: Etwas ironisch sind viele etablierte Mächte auch dem Risiko KI-gestützter Entmachtung ausgesetzt; aber es könnte schwierig für sie sein, das zu erkennen, bis und außer der Prozess ziemlich weit fortgeschritten ist.

[^151]: Einige interessante Bemühungen in diese Richtung sind repräsentiert durch [die c2pa-Koalition](https://c2pa.org/) zur kryptographischen Verifikation; [Verity](https://www.improvethenews.org/) und [Ground news](https://ground.news/) zu besserer Nachrichten-Epistemik; und [Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com) und Vorhersagemärkte zur Erdung von Diskurs in falsifizierbaren Vorhersagen.

[^152]: Siehe [dieses](https://talktothecity.org/) faszinierende Pilotprojekt.

[^153]: Siehe [Kialo](https://www.kialo-edu.com/) und Bemühungen des [Collective Intelligence Project](https://www.cip.org/) für einige Beispiele.

## Kapitel 10 - Die Entscheidung vor uns

Um unsere menschliche Zukunft zu bewahren, müssen wir uns dafür entscheiden, die Tore zu AGI und Superintelligenz zu schließen.

Das letzte Mal, dass die Menschheit die Erde mit anderen denkenden Wesen teilte, die sprechen, denken, Technologie entwickeln und allgemeine Problemlösung betreiben konnten, war vor 40.000 Jahren im eiszeitlichen Europa. Diese anderen Geister starben aus, ganz oder teilweise durch die Bemühungen der unseren.

Wir treten nun wieder in eine solche Zeit ein. Die fortschrittlichsten Erzeugnisse unserer Kultur und Technologie – Datensätze, die aus unserem gesamten Internet-Informationsbestand erstellt wurden, und Chips mit 100 Milliarden Elementen, die zu den komplexesten Technologien gehören, die wir je entwickelt haben – werden kombiniert, um fortschrittliche Allzweck-KI-Systeme zu erschaffen.

Die Entwickler dieser Systeme sind darauf bedacht, sie als Werkzeuge zur menschlichen Ermächtigung darzustellen. Und das könnten sie durchaus sein. Aber täuschen wir uns nicht: Unser gegenwärtiger Kurs führt zum Bau immer mächtigerer, zielorientierter, entscheidungsfähiger und allgemein leistungsfähiger digitaler Agenten. Sie erbringen bereits bei einer breiten Palette intellektueller Aufgaben Leistungen, die mit denen vieler Menschen vergleichbar sind, verbessern sich rasant und tragen zu ihrer eigenen Verbesserung bei.

Sollte sich dieser Kurs nicht ändern oder auf ein unerwartetes Hindernis stoßen, werden wir bald – in Jahren, nicht Jahrzehnten – über digitale Intelligenzen verfügen, die gefährlich mächtig sind. Selbst im *besten* Fall brächten diese große wirtschaftliche Vorteile (zumindest für einige von uns), aber nur um den Preis einer tiefgreifenden gesellschaftlichen Erschütterung und der Ablösung der Menschen in den meisten der wichtigsten Dinge, die wir tun: Diese Maschinen würden für uns denken, für uns planen, für uns entscheiden und für uns erschaffen. Wir wären verwöhnt, aber verwöhnte Kinder. Viel wahrscheinlicher ist, dass diese Systeme Menschen sowohl bei den positiven *als auch* bei den negativen Dingen ersetzen würden, die wir tun, einschließlich Ausbeutung, Manipulation, Gewalt und Krieg. Können wir KI-verstärkte Versionen davon überleben? Schließlich ist es mehr als plausibel, dass die Dinge überhaupt nicht gut laufen würden: dass wir relativ bald nicht nur in dem ersetzt würden, was wir tun, sondern in dem, was wir *sind* – als Architekten der Zivilisation und der Zukunft. Fragt die Neandertaler, wie das ausgeht. Vielleicht haben wir ihnen auch eine Zeit lang zusätzliche Schmuckstücke verschafft.

*Das müssen wir nicht tun.* Wir haben menschenvergleichbare KI, und es besteht keine Notwendigkeit, KI zu bauen, mit der wir *nicht* konkurrieren können. Wir können erstaunliche KI-Werkzeuge bauen, ohne eine Nachfolgerspezies zu erschaffen. Die Vorstellung, dass AGI und Superintelligenz unvermeidlich seien, ist eine *Wahl, die sich als Schicksal tarnt*.

Durch die Durchsetzung einiger harter, globaler Grenzen können wir die allgemeine Leistungsfähigkeit der KI ungefähr auf menschlichem Niveau halten und dabei trotzdem von der Fähigkeit der Computer profitieren, Daten auf eine Weise zu verarbeiten, die wir nicht können, und Aufgaben zu automatisieren, die niemand von uns machen will. Diese würden immer noch viele Risiken bergen, aber wenn sie gut entwickelt und verwaltet werden, wären sie ein enormer Segen für die Menschheit, von der Medizin über die Forschung bis hin zu Konsumprodukten.

Die Durchsetzung von Grenzen würde internationale Zusammenarbeit erfordern, aber weniger als man denken könnte, und diese Grenzen würden immer noch viel Raum für eine enorme KI- und KI-Hardware-Industrie lassen, die sich auf Anwendungen konzentriert, die das menschliche Wohlbefinden fördern, anstatt auf das reine Streben nach Macht. Und wenn wir uns nach starken Sicherheitsgarantien und einem sinnvollen globalen Dialog dafür entscheiden, weiter zu gehen, bleibt diese Option weiterhin in unserer Hand.

Die Menschheit muss sich *entscheiden*, die Tore zu AGI und Superintelligenz zu schließen.

Um die Zukunft menschlich zu halten.

### Eine Notiz des Autors

Vielen Dank, dass Sie sich die Zeit genommen haben, dieses Thema mit uns zu erkunden.

Ich habe diesen Essay geschrieben, weil ich als Wissenschaftler das Gefühl habe, dass es wichtig ist, die ungeschminkte Wahrheit zu sagen, und weil ich als Mensch das Gefühl habe, dass es entscheidend ist, dass wir schnell und entschlossen handeln, um ein weltveränderndes Problem anzugehen: die Entwicklung intelligenterer KI-Systeme als Menschen.

Wenn wir auf diesen bemerkenswerten Zustand der Dinge mit Weisheit reagieren wollen, müssen wir bereit sein, das vorherrschende Narrativ kritisch zu hinterfragen, dass AGI und Superintelligenz gebaut werden „müssen", um unsere Interessen zu sichern, oder „unvermeidlich" seien und nicht gestoppt werden könnten. Diese Narrative entmachten uns und machen uns unfähig, die alternativen Wege vor uns zu sehen.

Ich hoffe, Sie werden sich mir anschließen und zur Vorsicht im Angesicht von Rücksichtslosigkeit und zu Mut im Angesicht von Gier aufrufen.

Ich hoffe, Sie werden sich mir anschließen und zu einer menschlichen Zukunft aufrufen.

*– Anthony*

![Anthony Aguirre signature](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAnthony-Aguirre-signature-300x84.png&w=3840&q=75)

## Anhänge

Ergänzende Informationen, einschließlich technischer Details zur Rechenleistungserfassung, eines Beispiels für die Umsetzung einer „Torschließung", Details für ein striktes AGI-Haftungsregime und eines gestuften Ansatzes für AGI-Sicherheits- und Sicherheitsstandards.

### Anhang A: Technische Details zur Rechenleistungserfassung

Eine detaillierte Methode sowohl für die „Ground Truth" als auch für gute Näherungswerte der gesamten Rechenleistung, die beim Training und bei der Inferenz verwendet wird, ist für aussagekräftige rechenleistungsbasierte Kontrollen erforderlich. Hier ist ein Beispiel dafür, wie die „Ground Truth" auf technischer Ebene erfasst werden könnte.

**Definitionen:**

*Rechenkausaler Graph:* Für eine gegebene Ausgabe O eines KI-Modells gibt es eine Menge digitaler Berechnungen, bei denen eine Änderung des Ergebnisses dieser Berechnung potenziell O verändern könnte. (Dies sollte konservativ angenommen werden, d.h. es sollte einen klaren Grund geben zu glauben, dass eine Berechnung unabhängig von einem Vorläufer ist, der sowohl zeitlich früher auftritt als auch einen physisch möglichen kausalen Wirkungspfad hat.) Dies umfasst Berechnungen, die vom KI-Modell während der Inferenz durchgeführt werden, sowie Berechnungen, die in die Eingabe, Datenvorbereitung und das Training des Modells eingeflossen sind. Da jede davon selbst die Ausgabe eines KI-Modells sein kann, wird dies rekursiv berechnet und dort abgeschnitten, wo ein Mensch eine erhebliche Änderung an der Eingabe vorgenommen hat.

*Training-Rechenleistung:* Die gesamte Rechenleistung in FLOP oder anderen Einheiten, die durch den rechenkausalen Graphen eines neuronalen Netzwerks entsteht (einschließlich Datenvorbereitung, Training und Feinabstimmung sowie anderer Berechnungen).

*Ausgabe-Rechenleistung:* Die gesamte Rechenleistung im rechenkausalen Graphen einer gegebenen KI-Ausgabe, einschließlich aller neuronalen Netzwerke (und einschließlich ihrer Training-Rechenleistung) und anderer Berechnungen, die in diese Ausgabe eingehen.

*Inferenz-Rechenleistungsrate:* In einer Serie von Ausgaben die Änderungsrate (in FLOP/s oder anderen Einheiten) der Ausgabe-Rechenleistung zwischen den Ausgaben, d.h. die Rechenleistung, die zur Erzeugung der nächsten Ausgabe verwendet wird, geteilt durch das zeitliche Intervall zwischen den Ausgaben.

**Beispiele und Näherungswerte:**

- Für ein einzelnes neuronales Netzwerk, das mit von Menschen erstellten Daten trainiert wurde, ist die Training-Rechenleistung einfach die gesamte Training-Rechenleistung, wie sie üblicherweise berichtet wird.
- Für ein solches neuronales Netzwerk, das Inferenz mit gleichmäßiger Rate durchführt, entspricht die Inferenz-Rechenleistungsrate ungefähr der Gesamtgeschwindigkeit des Rechenclusters, der die Inferenz in FLOP/s durchführt.
- Bei der Modell-Feinabstimmung ergibt sich die Training-Rechenleistung des vollständigen Modells aus der Training-Rechenleistung des nicht feinabgestimmten Modells plus der Berechnung, die während der Feinabstimmung und zur Vorbereitung aller bei der Feinabstimmung verwendeten Daten durchgeführt wurde.
- Für ein destilliertes Modell umfasst die Training-Rechenleistung des vollständigen Modells das Training sowohl des destillierten Modells als auch des größeren Modells, das zur Bereitstellung synthetischer Daten oder anderer Trainingseingaben verwendet wurde.
- Wenn mehrere Modelle trainiert werden, aber viele „Versuche" aufgrund menschlicher Beurteilung verworfen werden, zählen diese nicht zur Training- oder Ausgabe-Rechenleistung des behaltenen Modells.

### Anhang B: Beispiel für die Umsetzung einer Torschließung

**Umsetzungsbeispiel:** Hier ist ein Beispiel dafür, wie eine Torschließung funktionieren könnte, mit einem Grenzwert von 10<sup>27</sup> FLOP für das Training und 10<sup>20</sup> FLOP/s für die Inferenz (Betrieb der KI):

**1. Pause:** Aus Gründen der nationalen Sicherheit fordert die US-Exekutive alle Unternehmen mit Sitz in den USA, die Geschäfte in den USA tätigen oder in den USA hergestellte Chips verwenden, auf, alle neuen KI-Trainingsläufe einzustellen, die den Grenzwert von 10<sup>27</sup> FLOP Training-Rechenleistung überschreiten könnten. Die USA sollten Gespräche mit anderen Ländern beginnen, die KI-Entwicklung beherbergen, sie nachdrücklich ermutigen, ähnliche Schritte zu unternehmen, und signalisieren, dass die US-Pause aufgehoben werden könnte, falls sie sich entscheiden, nicht zu kooperieren.

**2. US-Aufsicht und Lizenzierung:** Durch Erlass der Exekutive oder Maßnahmen einer bestehenden Regulierungsbehörde verlangen die USA, dass innerhalb von (beispielsweise) einem Jahr:

- Alle KI-Trainingsläufe über geschätzten 10<sup>25</sup> FLOP, die von in den USA operierenden Unternehmen durchgeführt werden, in einer von einer US-Regulierungsbehörde verwalteten Datenbank registriert werden. (Hinweis: Eine etwas schwächere Version davon war bereits in der inzwischen widerrufenen US-Durchführungsverordnung zu KI von 2023 enthalten, die eine Registrierung für Modelle über 10<sup>26</sup> FLOP verlangte.)
- Alle KI-relevanten Hardware-Hersteller, die in den USA operieren oder Geschäfte mit der US-Regierung tätigen, sich an eine Reihe von Anforderungen für ihre spezialisierte Hardware und die sie antreibende Software halten. (Viele dieser Anforderungen könnten durch Software- und Firmware-Updates für bestehende Hardware umgesetzt werden, aber langfristige und robuste Lösungen würden Änderungen an späteren Hardware-Generationen erfordern.) Dazu gehört die Anforderung, dass wenn die Hardware Teil eines Hochgeschwindigkeits-Clusters ist, der 10<sup>18</sup> FLOP/s Berechnungen ausführen kann, eine höhere Verifizierungsebene erforderlich ist, die regelmäßige Genehmigung durch einen entfernten „Governor" einschließt, der sowohl Telemetrie als auch Anfragen zur Durchführung zusätzlicher Berechnungen erhält.
- Der Verwalter die gesamte auf seiner Hardware durchgeführte Berechnung an die die US-Datenbank führende Behörde meldet.
- Stärkere Anforderungen schrittweise eingeführt werden, um sowohl sicherere als auch flexiblere Aufsicht und Genehmigungsverfahren zu ermöglichen.

**3. Internationale Aufsicht:**

- Die USA, China und alle anderen Länder mit fortgeschrittener Chip-Herstellungskapazität verhandeln ein internationales Abkommen.
- Dieses Abkommen schafft eine neue internationale Behörde, analog zur Internationalen Atomenergiebehörde, die mit der Überwachung von KI-Training und -Ausführung beauftragt ist.
- Unterzeichnerstaaten müssen von ihren heimischen KI-Hardware-Herstellern verlangen, dass sie sich an Anforderungen halten, die mindestens so streng sind wie die in den USA auferlegten.
- Verwalter sind nun verpflichtet, KI-Berechnungszahlen sowohl an Behörden in ihren Heimatländern als auch an ein neues Büro innerhalb der internationalen Behörde zu melden.
- Zusätzliche Länder werden stark ermutigt, dem bestehenden internationalen Abkommen beizutreten: Exportkontrollen durch Unterzeichnerstaaten beschränken den Zugang zu High-End-Hardware für Nicht-Unterzeichner, während Unterzeichner technische Unterstützung bei der Verwaltung ihrer KI-Systeme erhalten können.

**4. Internationale Verifikation und Durchsetzung:**

- Das Hardware-Verifizierungssystem wird aktualisiert, sodass es die Rechenleistungsnutzung sowohl an den ursprünglichen Verwalter als auch direkt an das internationale Behördenbüro meldet.
- Die Behörde einigt sich über Diskussionen mit den Unterzeichnern des internationalen Abkommens auf Rechenleistungsbeschränkungen, die dann Rechtskraft in den Unterzeichnerländern erlangen.
- Parallel dazu kann eine Reihe internationaler Standards entwickelt werden, sodass Training und Betrieb von KIs über einem Rechenleistungsschwellenwert (aber unter dem Grenzwert) sich an diese Standards halten müssen.
- Die Behörde kann, falls notwendig zur Kompensation besserer Algorithmen usw., den Rechenleistungsgrenzwert senken. Oder, falls es als sicher und ratsam erachtet wird (auf der Ebene beweisbarer Sicherheitsgarantien), den Rechenleistungsgrenzwert erhöhen.

### Anhang C: Details für ein striktes AGI-Haftungsregime

**Details für ein striktes AGI-Haftungsregime**

- Die Entwicklung und der Betrieb eines fortgeschrittenen KI-Systems, das hochgradig allgemein, leistungsfähig und autonom ist, wird als „ungewöhnlich gefährliche" Aktivität betrachtet.
- Als solche ist die standardmäßige Haftung für Training und Betrieb solcher Systeme eine strenge, gesamtschuldnerische Haftung (oder deren Nicht-US-Äquivalent) für alle durch das Modell oder seine Ausgaben/Handlungen verursachten Schäden.
- Persönliche Haftung wird für Führungskräfte und Vorstandsmitglieder bei grober Fahrlässigkeit oder vorsätzlichem Fehlverhalten auferlegt. Dies sollte strafrechtliche Sanktionen für die schwerwiegendsten Fälle einschließen.
- Es gibt zahlreiche Schutzbestimmungen, unter denen die Haftung zur Standard-(verschuldensabhängigen, in den USA) Haftung zurückkehrt, der Menschen und Unternehmen normalerweise unterliegen würden.
	- Modelle, die unter einem bestimmten Rechenleistungsschwellenwert (der mindestens 10x niedriger wäre als die oben beschriebenen Obergrenzen) trainiert und betrieben werden.
	- KI, die „schwach" ist (grob gesagt, unter dem Niveau menschlicher Experten bei den Aufgaben, für die sie bestimmt ist) und/oder
	- KI, die „eng" ist (mit einem festen und ziemlich begrenzten Aufgabenbereich und Operationen, für die sie speziell entworfen und trainiert wurde) und/oder
	- KI, die „passiv" ist (sehr begrenzt in ihrer Fähigkeit – selbst unter bescheidener Modifikation – Handlungen auszuführen oder komplexe mehrstufige Aufgaben ohne direkte menschliche Beteiligung und Kontrolle durchzuführen).
	- Eine KI, die garantiert sicher, geschützt und kontrollierbar ist (beweisbar sicher oder eine Risikoanalyse zeigt ein vernachlässigbares Niveau erwarteter Schäden an).
- Schutzbestimmungen können auf der Basis eines [Safety Case](https://arxiv.org/abs/2410.21572) beansprucht werden, der vom KI-Entwickler erstellt und von einer Behörde oder einem von einer Behörde akkreditierten Prüfer genehmigt wurde. Um eine Schutzbestimmung basierend auf Rechenleistung zu beanspruchen, muss der Entwickler nur glaubwürdige Schätzungen der gesamten Training-Rechenleistung und maximalen Inferenzrate liefern.
- Die Gesetzgebung würde explizit Situationen umreißen, unter denen einstweilige Verfügungen gegen die Entwicklung von KI-Systemen mit hohem Risiko öffentlicher Schäden angemessen wären.
- Unternehmenskonsortien sollten in Zusammenarbeit mit NGOs und Regierungsbehörden Standards und Normen entwickeln, die diese Begriffe definieren, wie Regulierungsbehörden Schutzbestimmungen gewähren sollten, wie KI-Entwickler Safety Cases entwickeln sollten und wie Gerichte Haftung interpretieren sollten, wo Schutzbestimmungen nicht proaktiv beansprucht werden.

### Anhang D: Ein gestufter Ansatz für AGI-Sicherheits- und Schutzstandards

**Ein gestufter Ansatz für AGI-Sicherheits- und Schutzstandards**

| Risikostufe | Auslöser | Anforderungen für Training | Anforderungen für Einsatz |
| --- | --- | --- | --- |
| RT-0 | KI schwach in Autonomie, Allgemeinheit und Intelligenz | keine | keine |
| RT-1 | KI stark in einem von Autonomie, Allgemeinheit und Intelligenz | keine | Basierend auf Risiko und Verwendung, potenziell Safety Cases, die von nationalen Behörden genehmigt wurden, wo auch immer das Modell verwendet werden kann |
| RT-2 | KI stark in zwei von Autonomie, Allgemeinheit und Intelligenz | Registrierung bei nationaler Behörde mit Zuständigkeit für den Entwickler | Safety Case, der das Risiko schwerer Schäden unter autorisierten Niveaus begrenzt, plus unabhängige Sicherheitsprüfungen (einschließlich Black-Box- und White-Box-Red-Teaming), genehmigt von nationalen Behörden, wo auch immer das Modell verwendet werden kann |
| RT-3 | AGI stark in Autonomie, Allgemeinheit und Intelligenz | Vorab-Genehmigung des Sicherheits- und Schutzplans durch nationale Behörde mit Zuständigkeit für den Entwickler | Safety Case, der begrenztes Risiko schwerer Schäden unter autorisierten Niveaus sowie erforderliche Spezifikationen garantiert, einschließlich Cybersicherheit, Kontrollierbarkeit, einem nicht entfernbaren Notschalter, Alignment mit menschlichen Werten und Robustheit gegen böswillige Nutzung |
| RT-4 | Jedes Modell, das auch entweder 10<sup>27</sup> FLOP Training oder 10<sup>20</sup> FLOP/s Inferenz überschreitet | Verboten bis zur international vereinbarten Aufhebung der Rechenleistungsobergrenze | Verboten bis zur international vereinbarten Aufhebung der Rechenleistungsobergrenze |

Risikoklassifizierungen und Sicherheits-/Schutzstandards, mit Stufen basierend auf Rechenleistungsschwellenwerten sowie Kombinationen hoher Autonomie, Allgemeinheit und Intelligenz:

- *Starke Autonomie* gilt, wenn das System in der Lage ist, mehrstufige Aufgaben durchzuführen und/oder komplexe Handlungen zu unternehmen, die real-weltrelevant sind, ohne signifikante menschliche Aufsicht oder Intervention, oder leicht dazu gebracht werden kann. Beispiele: autonome Fahrzeuge und Roboter; Finanzhandels-Bots. Gegenbeispiele: GPT-4; Bildklassifizierer
- *Starke Allgemeinheit* zeigt einen weiten Anwendungsbereich an, Durchführung von Aufgaben, für die das Modell nicht absichtlich und spezifisch trainiert wurde, und erhebliche Fähigkeit, neue Aufgaben zu erlernen. Beispiele: GPT-4; mu-zero. Gegenbeispiele: AlphaFold; autonome Fahrzeuge; Bildgeneratoren
- *Starke Intelligenz* entspricht der Angleichung an die Leistung menschlicher Experten bei den Aufgaben, bei denen das Modell am besten abschneidet (und bei einem allgemeinen Modell über einen breiten Aufgabenbereich). Beispiele: AlphaFold; mu-zero; o3. Gegenbeispiele: GPT-4; Siri

### Danksagungen

Ein paar Dankesworte an die Menschen, die zu „Keep The Future Human" beigetragen haben.

Diese Arbeit spiegelt die Meinungen des Autors wider und sollte nicht als offizielle Position des Future of Life Institute verstanden werden (obwohl sie kompatibel sind; für dessen offizielle Position siehe [diese Seite](https://futureoflife.org/our-position-on-ai/)) oder einer anderen Organisation, mit der der Autor verbunden ist.

Ich bin den Menschen Mark Brakel, Ben Eisenpress, Anna Hehir, Carlos Gutierrez, Emilia Javorsky, Richard Mallah, Jordan Scharnhorst, Elyse Fulcher, Max Tegmark und Jaan Tallinn für ihre Anmerkungen zum Manuskript dankbar; Tim Schrier für Hilfe bei einigen Referenzen; Taylor Jones und Elyse Fulcher für die Verschönerung der Diagramme.

Diese Arbeit machte bei ihrer Entstehung begrenzten Gebrauch von generativen KI-Modellen (Claude und ChatGPT) für einige Bearbeitungen und Red-Teaming. Nach dem etablierten Standard für Grade der KI-Beteiligung bei kreativen Arbeiten würde diese Arbeit wahrscheinlich 3/10 Punkte erhalten. (Einen solchen Standard gibt es tatsächlich gar nicht! Aber es sollte einen geben.)

Wir sind [Julius Odai](https://www.linkedin.com/in/julius-odai/) sehr dankbar für die Erstellung dieser Web-Version des Essays, die das Lesen und Navigieren durch den Essay zu einer sehr angenehmen Erfahrung macht. Julius ist Technologe und kürzlich Teilnehmer des BlueDot Impact AI Governance-Kurses.