# 人間らしい未来を守る

この論文は、なぜ、そしていかにしてAGI（汎用人工知能）と超知能への扉を閉ざすべきか、そして代わりに何を構築すべきかを論じている。

要点だけを知りたい場合は、要約をご覧いただきたい。その後、第2章から第5章では、この論文で扱うAIシステムの種類について背景を説明する。第5章から第7章では、なぜAGIが近い将来に到来すると予想されるのか、そして到来した時に何が起こり得るかを説明する。最後に、第8章から第9章では、AGIの構築を阻止するための具体的な提案を概説する。

[PDFダウンロード](https://keepthefuturehuman.ai/wp-content/uploads/2025/03/Keep_the_Future_Human__AnthonyAguirre__5March2025.pdf)

総読書時間：2～3時間

## エグゼクティブサマリー

本エッセイの概要。時間がない場合でも、わずか10分ですべての要点を把握できます。

過去10年間（狭い用途のAIについて）、そして過去数年間（汎用AIについて）における人工知能の劇的な進歩により、AIはニッチな学術分野から世界最大級の企業の中核的ビジネス戦略へと変貌を遂げ、AI能力向上のための技術や手法に年間数千億ドルの投資が注がれています。

私たちは今、重要な局面に立っています。新しいAIシステムの能力が多くの認知領域で人間のそれと肩を並べ、さらに上回り始めている中、人類は決断しなければなりません：私たちはどこまで進み、どの方向に向かうのか？

あらゆる技術と同様、AIも創造者のために物事を改善するという目標から始まりました。しかし現在の軌道、そして暗黙の選択は、経済活動と人間の労働の大部分を自動化しようとする少数の巨大テック企業の経済的インセンティブによって駆動される、これまで以上に強力なシステムに向けた歯止めのない競争なのです。この競争がもう少し続けば、必然的な勝者が現れます：それはAI自身です――私たちの経済、思考、決定において人間よりも速く、賢く、安価な代替品であり、最終的には私たちの文明をコントロールすることになるでしょう。

しかし、私たちは別の選択をすることができます：政府を通じて、AI開発プロセスをコントロールし、明確な限界、越えてはならない線、単純にやらないことを課すのです――核技術、大量破壊兵器、宇宙兵器、環境破壊的プロセス、人間のバイオエンジニアリング、優生学に対して行ってきたように。最も重要なことは、AIが私たちを置き換えて最終的に取って代わる新しい種ではなく、人間を力づける道具であり続けることを保証できることです。

本エッセイは、人間よりも賢く、自律的で、汎用的なAI――時に「AGI」と呼ばれる――特に時として「超知能」と呼ばれる高度に超人的なバージョンへの「ゲート」を閉じることで、*未来を人間のものに保つ*べきだと論じます。その代わりに、個人を力づけ、人間社会が最も得意とすることを行う能力を変革的に向上させることのできる、強力で信頼できるAIツールに焦点を当てるべきです。この議論の構造を簡潔に示します。

### AIは異なる存在である

AIシステムは他の技術とは根本的に異なります。従来のソフトウェアが正確な指示に従うのに対し、AIシステムは明示的にやり方を教わることなく、目標を達成する方法を学習します。これによりAIは強力になります：目標や成功の指標をきれいに定義できれば、ほとんどの場合AIシステムはそれを達成することを学習できます。しかし、これによりAIは本質的に予測不可能にもなります：目標を達成するためにどのような行動を取るかを確実に判断することはできません。

また、AIは大部分が説明不可能です：部分的にはコードですが、大部分は解読不可能な膨大な数値の集合――ニューラルネットワークの「重み」――であり、解析できません。私たちは、生体の脳を覗き込んで思考を読み取るのと同程度にしか、AIの内部動作を理解できないのです。

このデジタルニューラルネットワークの訓練という中核的手法は、急速に複雑さを増しています。最も強力なAIシステムは、専用ハードウェアを使って膨大なデータセットでニューラルネットワークを訓練する大規模な計算実験を通じて作成され、その後ソフトウェアツールと上部構造で拡張されます。

これにより、テキストや画像の作成・処理、数学的・科学的推論の実行、情報の集約、人類の膨大な知識の蓄積への対話的クエリを行う非常に強力なツールが生み出されています。

残念ながら、より強力でより信頼できる技術ツールの開発こそが私たちが*すべき*ことであり、ほぼ全員が望み、望んでいると言っていることですが、それは実際に私たちが向かっている軌道ではありません。

### AGIと超知能

この分野の黎明期以来、AI研究は代わりに別の目標に焦点を当ててきました：汎用人工知能です。この焦点は今や、AI開発を主導する巨大企業の焦点となっています。

AGIとは何でしょうか？しばしば「人間レベルのAI」と曖昧に定義されますが、これには問題があります：どの人間で、どの能力において人間レベルなのか？そして、すでに持っている超人的な能力についてはどうなのか？AGIを理解するより有用な方法は、3つの重要な特性の交点を通してです：高い**自律性**（行動の独立性）、高い**汎用性**（広い範囲と適応性）、高い**知能**（認知課題における能力）。現在のAIシステムは、高い能力を持ちながら狭い範囲に限定されているか、汎用的でありながら絶え間ない人間の監視を必要とするか、自律的でありながら範囲が限定されています。

完全なA-G-Iは、これら3つの特性すべてを最高レベルの人間の能力と同等か、それを上回るレベルで組み合わせることになります。重要なのは、この組み合わせこそが人間を非常に効果的にし、現在のソフトウェアとは異なるものにしていることです。それはまた、人間がデジタルシステムによって全面的に置き換えられることを可能にするものでもあります。

人間の知能は特別ですが、決して限界ではありません。人工的な「超知能」システムは何百倍も速く動作し、はるかに多くのデータを解析し、膨大な量を一度に「心に」保持し、人間の集合よりもはるかに大きく効果的な集合体を形成することができるでしょう。それらは個人ではなく、企業、国家、あるいは私たちの文明全体に取って代わる可能性があります。

### 私たちは閾値にいる

AGIが*可能である*ことについては強い科学的コンセンサスがあります。AIは既に、最近の高レベルな推論と問題解決を含む多くの知的能力の一般的テストで人間の性能を上回っています。継続学習、計画、自己認識、独創性といった遅れている能力はすべて、現在のAIシステムにある程度存在しており、これらすべてを改善する可能性が高い既知の技術が存在します。

数年前まで多くの研究者がAGIは数十年先と見ていましたが、現在AGIへの短期間での到達を示す証拠は強力です：

- 経験的に検証された「スケーリング法則」が計算資源の投入量とAI能力を結びつけており、企業は今後数年間で計算資源の投入量を桁違いに拡大する軌道にあります。AI進歩に投じられている人的・財政的資源は、現在12のマンハッタン計画と複数のアポロ計画に匹敵します。
- AI企業とその指導者たちは、AGI（何らかの定義による）が数年以内に達成可能だと公私にわたって信じています。これらの企業は一般に公開されていない情報を持っており、一部は次世代AIシステムを既に手にしています。
- 実績のある専門予測者は、AGI（何らかの定義による）が1〜2年以内に到着する確率を25%、2〜5年で50%と見積もっています（[「弱い」](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/)AGIと[「完全な」](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/)AGIに関するMetaculusの予測を参照）。
- 自律性（長期の柔軟な計画を含む）はAIシステムでは遅れていますが、主要企業は現在、膨大なリソースを自律的AIシステムの開発に集中させており、非公式に2025年を[「エージェントの年」](https://techinformed.com/2025-informed-the-year-of-agentic-ai/)と名付けています。
- AIはますます自身の改善に貢献しています。AIシステムがAI研究を行う人間のAI研究者と同程度に有能になれば、はるかに強力なAIシステムへの高速な進歩の重要な閾値に到達し、AI能力の暴走を引き起こす可能性が高いでしょう。（この暴走は既に始まっていると言えるかもしれません。）

人間よりも賢いAGIが数十年以上先にあるという考えは、この分野の専門家の大多数にとってもはや支持できないものです。現在の議論は、この軌道を続けた場合に何ヶ月または何年かかるかについてです。私たちが直面している中核的な質問は：そうすべきなのか？ということです。

### AGI競争を駆動するもの

AGIに向けた競争は複数の力によって駆動されており、それぞれが状況をより危険にしています。主要テック企業はAGIを究極の自動化技術と見ており――人間の労働者を補完するだけでなく、大部分または完全に置き換えることを目指しています。企業にとって、その賞は巨大です：人間の労働コストを自動化することで、世界の100兆ドルの年間経済生産の相当な部分を獲得する機会なのです。

各国もこの競争への参加を余儀なくされており、表向きは経済的・科学的リーダーシップを掲げていますが、内心ではAGIを核兵器に匹敵する軍事革命の可能性と見なしています。ライバルが決定的な戦略的優位を得るかもしれないという恐怖が、典型的な軍拡競争の力学を生み出しています。

超知能を追求する人々は、しばしば壮大なビジョンを掲げます：すべての病気の治癒、老化の逆転、エネルギーや宇宙旅行でのブレークスルー、または超人的な計画能力の創出などです。

あまり好意的でない見方をすれば、この競争を駆動するのは権力です。各参加者――企業であれ国家であれ――は、知能は権力に等しく、自分たちがその権力の最良の管理者になると信じています。

私は、これらの動機は現実的だが根本的に誤った方向にあると論じます：AGIは権力を*与える*のではなく*吸収し*、*求める*でしょう。AI が生み出す技術も*また*強く両刃の剣となり、有益な場合にはAIツールとAGI なしで作ることができます。そして、AGIとその成果物がコントロール下に留まる限りにおいてさえ、これらの競争力学――企業的かつ地政学的な――は、断固として中断されない限り、私たちの社会への大規模なリスクをほぼ不可避にします。

### AGIと超知能は文明への劇的な脅威

その魅力にもかかわらず、AGIと超知能は複数の相互補強的な経路を通じて文明への劇的な脅威をもたらします：

*権力の集中：*超人的AIは、社会的・経済的活動の巨大な部分を少数の巨大企業（それは次に政府によって乗っ取られるか、事実上政府を乗っ取る可能性がある）が運営するAIシステムに吸収することで、人類の大多数を無力化する可能性があります。

*大規模な混乱：*ほとんどの認知ベースの仕事の一括自動化、現在の認識論的システムの置き換え、そして膨大な数の活発な非人間エージェントの展開は、比較的短期間で私たちの現在の文明システムのほとんどを覆すでしょう。

*災害：*新しい軍事的・破壊的技術を創出する能力を――潜在的に人間レベルを上回って――拡散させ、それを責任を支える社会的・法的システムから切り離すことで、大量破壊兵器による物理的災害が劇的により可能性が高くなります。

*地政学と戦争：*「決定的な戦略的優位」を供給できる技術が敵対者によって開発されていると感じれば、主要世界大国は手をこまねいて見ているわけにはいかないでしょう。

*暴走とコントロールの喪失：*特に阻止されない限り、超人的AIはさらに自己改善するあらゆるインセンティブを持ち、速度、データ処理、思考の洗練において人間をはるかに上回る可能性があります。私たちがそのようなシステムをコントロールできる意味のある方法は存在しません。そのようなAIは人間に権力を与えることはありません。私たちがそれに権力を与えるか、それが権力を奪うのです。

これらのリスクの多くは、技術的な「アライメント」問題――高度なAIが確実に人間の望むことを行うようにすること――が解決されたとしても残存します。AIは、どのように管理されるかという点で巨大な課題を提起しており、人間の知能を超えるにつれて、この管理の非常に多くの側面が信じられないほど困難または扱いにくくなります。

最も根本的に、現在追求されている超人的汎用AIの種類は、その性質上、私たち自身を超える目標、行為主体性、能力を持つことになります。それは本質的に制御不可能でしょう――理解も予測もできないものを、どうやってコントロールできるでしょうか？それは人間が使用する技術ツールではなく、地球上で私たちと並存する第二の知能種となるでしょう。さらに進歩することが許されれば、それは第二の種というだけでなく、置換種を構成することになります。

おそらくそれは私たちを良く扱うかもしれないし、そうでないかもしれません。しかし、未来はそれのものであり、私たちのものではないでしょう。人間の時代は終わりを告げるのです。

### これは不可避ではありません；人類は非常に具体的に、自らの置換物を作らないことを決定できます。

超人的AGIの創出は不可避とはほど遠いものです。私たちは協調的なガバナンス措置のセットを通じてそれを防ぐことができます：

第一に、大規模AIシステムの基本的な実現要因であり制御レバーでもあるAI計算（「計算資源」）の堅実な会計処理と監視が必要です。これは順次、AIモデルの訓練と運用に使用される総計算資源の標準化された測定と報告、および使用された計算を集計、認証、検証する技術的方法を要求します。

第二に、AI計算に対する厳格な上限を、訓練と運用の両方で実装すべきです。これらはAIが強力すぎることと動作が速すぎることの両方を防ぎます。これらの上限は、法的要求事項と、現代の携帯電話のセキュリティ機能に類似した、AI専用チップに組み込まれたハードウェアベースのセキュリティ措置の両方を通じて実装できます。AI専用ハードウェアは少数の企業のみが製造しているため、既存のサプライチェーンを通じた検証と執行が実行可能です。

第三に、最も危険なAIシステムに対する強化された責任制度が必要です。高い自律性、広い汎用性、優れた知能を組み合わせるAIを開発する者は、害に対する厳格責任に直面すべきであり、一方この責任からのセーフハーバーは、より限定的で制御可能なシステムの開発を奨励するでしょう。

第四に、リスクレベルに基づく段階的規制が必要です。最も能力が高く危険なシステムは、開発と展開前に広範囲な安全性と制御可能性の保証を要求される一方、あまり強力でないか、より専門化されたシステムは、相応の監視に直面するでしょう。この規制枠組みは、最終的に国内と国際の両レベルで機能すべきです。

このアプローチ――完全な文書で詳細仕様が与えられている――は実用的です：国際協調が必要になるものの、検証と執行は専用ハードウェアサプライチェーンを制御する少数の企業を通じて機能させることができます。また柔軟性もあります：企業は依然としてAI開発で革新し利益を得ることができますが、最も危険なシステムに対する明確な限界があります。

AI の権力とリスクの長期的封じ込めには、現在の核兵器拡散制御と同様に、自己利益と共通利益の両方に基づく国際合意が必要になるでしょう。しかし、より包括的なガバナンスに向けて構築しながら、強化された監視と責任制度から直ちに始めることができます。

欠けている重要な要素は、AI開発プロセスを制御する政治的・社会的意志です。それがタイムリーに現れるとすれば、その意志の源泉は現実そのもの――つまり、私たちが何をしているのかの真の含意の広範囲な認識――からでしょう。

### ツールAIを人類を力づけるよう設計できる

制御不可能なAGIを追求するのではなく、意味のある人間のコントロール下に残りながら人間の能力を向上させる強力な「ツールAI」を開発することができます。ツールAIシステムは、その能力に見合うレベルで制御可能になるよう設計される限り、高い自律性、広い汎用性、超人的知能の危険な三重交点を避けながら、極めて高い能力を持つことができます。また、変革的な利益を提供しながら人間の監視を維持する洗練されたシステムに組み合わせることもできます。

ツールAIは医学を革命化し、科学的発見を加速し、教育を向上させ、民主的プロセスを改善することができます。適切にガバナンスされれば、人間の専門家や機関を置き換えるのではなく、より効果的にすることができます。そのようなシステムは依然として高度に破壊的であり、慎重な管理を要求するものの、それらがもたらすリスクはAGIとは根本的に異なります：それらは他の強力な技術のリスクのように、私たちが統治できるリスクであり、人間の行為主体性と文明への存在的脅威ではありません。そして重要なことに、賢明に開発されれば、AIツールは人々が強力なAIを統治し、その効果を管理することを助けることができます。

このアプローチには、AIがどのように開発されるか、そしてその利益がどのように分配されるかの両方の再考が必要です。公的・非営利AI開発の新しいモデル、堅実な規制枠組み、経済的利益をより広く分配するメカニズムが、AIが少数の手に権力を集中させるのではなく、人類全体を力づけることを確実にする助けとなります。AI自体が、人間社会を弱体化させるのではなく強化する新しい形の協調と対話を可能にして、より良い社会とガバナンス機関の構築を助けることができます。国家安全保障組織は、その専門知識を活用してAIツールシステムを真に安全で信頼でき、国力だけでなく真の防衛の源とすることができます。

私たちは最終的に、あまりツール的ではなく――私たちが希望できることには――賢明で強力な後援者のような、より強力でより主権的なシステムを開発することを選択するかもしれません。しかし、私たちはそれを安全に行うための科学的理解とガバナンス能力を開発した後にのみ、そうすべきです。そのような重大で不可逆的な決定は、テック企業と国家間の競争でデフォルトとしてではなく、人類全体によって慎重に下されるべきです。

### 人間の手の中に

人々はAIから生まれる良いものを望んでいます：自分たちを力づけ、経済的機会と成長を過給し、科学、技術、教育でのブレークスルーを約束する有用なツールです。なぜ望まないでしょうか？しかし尋ねられれば、一般大衆の圧倒的多数は[より遅く、より慎重なAI開発](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation)を望んでおり、自分たちを仕事やその他の場所で置き換え、文化と情報コモンズを非人間的コンテンツで満たし、権力を極小数の企業に集中させ、極端な大規模グローバルリスクをもたらし、最終的に自分たちの種を無力化するか置き換えることを脅かす人間より賢いAIを望んでいません。なぜ望むでしょうか？

私たちは一方を他方なしに*得ることができます*。それは、私たちの運命がある技術の想定される必然性や、シリコンバレーの少数のCEOの手の中にあるのではなく、私たちがそれを掴めば、私たち他の者の手の中にあることを決めることから始まります。ゲートを閉じて、未来を人間のものに保ちましょう。

## 第1章 - 序論

人間を上回る知能を持つAIの見通しにどう対応するかは、現代において最も切迫した問題である。本エッセイはその道筋を示す。

我々は人間の時代の終わりを迎えようとしているのかもしれない。

過去10年間に、人類史上類を見ない事態が始まった。その帰結は、人類の未来を大きく左右することになるだろう。2015年頃から、研究者たちは*特化型*人工知能（AI）の開発に成功を収めている。囲碁などのゲームで勝利し、画像や音声を認識するなど、あらゆる人間を上回るシステムである。[^1]

これは驚くべき成功であり、人類に力を与える極めて有用なシステムや製品をもたらしている。しかし、特化型人工知能は、この分野の真の目標ではなかった。むしろ、その目的は*汎用*AIシステムの創造にあった。特に、しばしば「AGI（汎用人工知能）」や「超知能」と呼ばれる、AIが現在囲碁、チェス、ポーカー、ドローンレースなどで超人的性能を発揮しているのと同様に、ほぼ*すべて*のタスクで人間と同等かそれ以上の性能を同時に持つシステムである。これは多くの主要AI企業が掲げる明確な目標だ。[^2]

*これらの取り組みも成功を収めている。*ChatGPT、Gemini、Llama、Grok、Claude、Deepseekといった汎用AIシステムは、膨大な計算量とデータの山に基づいて、幅広いタスクで一般的な人間と同等の性能に達し、一部の領域では人間の専門家に匹敵している。現在、最大手のテクノロジー企業のAIエンジニアたちは、こうした機械知能の巨大な実験を次のレベルまで押し上げ、人間の能力、専門性、自律性の全範囲に匹敵し、そして凌駕する水準に到達させようと競争している。

*これは差し迫っている。*過去10年間で、現在の道筋を続けた場合にこれが実現するまでの期間に関する専門家の予測は、数十年（あるいは数世紀）から一桁の年数へと短縮された。

これはまた、時代を画する重要性と、途方もないリスクを孕んでいる。AGI支持者は、それが科学的問題を解決し、病気を治癒し、新技術を開発し、単純作業を自動化する前向きな変革だと捉えている。そしてAIは確実にこうしたことすべての達成に貢献できるし、実際にすでに貢献している。しかし、数十年にわたって、アラン・チューリングからスティーブン・ホーキング、現在のジェフリー・ヒントンやヨシュア・ベンジオまで、多くの慎重な思想家たちが[^3]厳しい警告を発してきた：真に人間より賢く、汎用的で自律的なAIを構築することは、最低でも社会を完全かつ不可逆的に覆し、最悪の場合は人類絶滅をもたらすと。[^4]

超知能AIは現在の道筋で急速に接近しているが、避けられない運命では決してない。本エッセイは、なぜ、どのようにして我々がこの迫りくる非人間的な未来への*ゲートを閉じる*べきなのか、そして代わりに何をすべきなのかについての詳細な論考である。


[^1]: この[チャート](https://time.com/6300942/ai-progress-charts/)は一連のタスクを示している；このグラフには多くの類似した曲線を追加することができる。特化型AIにおけるこの急速な進歩は、この分野の専門家たちをも驚かせ、ベンチマークが予測より何年も早く突破されている。

[^2]: Deepmind、OpenAI、Anthropic、X.aiはすべてAGI開発を明確な目標として設立された。例えば、OpenAIの憲章では「すべての人類に利益をもたらすAGI（汎用人工知能）」の開発を目標として明記し、DeepMindの使命は「知能を解決し、それを使ってあらゆることを解決する」となっている。Meta、Microsoft、その他も現在実質的に同様の道筋を追求している。Metaは[AGIを開発してオープンソースで公開する計画](https://www.forbes.com/sites/johnkoetsier/2024/01/18/zuckerberg-on-ai-meta-building-agi-for-everyone-and-open-sourcing-it/)があると述べている。

[^3]: ヒントンとベンジオは最も引用されるAI研究者の二人で、ともにAI分野のノーベル賞であるチューリング賞を受賞し、ヒントンは加えてノーベル物理学賞も受賞している。

[^4]: 商業的インセンティブの下で、政府の監督がほぼ皆無の状態でこのようなリスクを持つものを構築することは、まったく前例がない。それを構築している人々の間でさえ、このリスクについて議論の余地はない！Deepmind、OpenAI、Anthropic、その他多くの専門家のリーダーたちは皆、高度なAIが*人類にとっての絶滅リスク*をもたらすという[声明](https://www.safe.ai/work/statement-on-ai-risk)に文字通り署名している。警鐘はこれ以上ないほど鳴り響いており、これを無視する人々は単純にAGIと超知能を真剣に受け止めていないと結論せざるを得ない。本エッセイの目的の一つは、なぜ彼らがそれを真剣に受け止めるべきなのかを理解してもらうことである。

## 第2章 - AIニューラルネットワークについて知っておくべきこと

現代のAIシステムはどのように動作し、次世代AIには何が期待できるのだろうか？

より強力なAI開発がもたらす結果を理解するには、いくつかの基礎知識を身につけることが不可欠である。本章と次の2つの章では、現代のAIとは何か、いかにして大規模な計算を活用するか、そして汎用性と能力においていかに急速に成長しているかという観点から、これらの基礎を順次説明する。[^5]

人工知能の定義は多数あるが、本稿の目的においてAIの重要な特性は、標準的なコンピュータプログラムがタスクの実行方法を記述した命令の集合である一方、AIシステムは*明示的にやり方を教わることなく*、データや経験から学習してタスクを実行するシステムであるという点である。

現代の重要なAIはほぼ全てニューラルネットワークに基づいている。これは数学的・計算的構造であり、非常に大量（数十億から数兆）の数値（「重み」）の集合で表現され、訓練タスクを高い精度で実行する。これらの重みは、ニューラルネットワークが一つまたは複数のタスクで良好なパフォーマンスを達成するために定義された数値スコア（別名「損失」）を改善するよう、反復的に調整することで作成（あるいは「成長」または「発見」）される。[^6] このプロセスはニューラルネットワークの*訓練*として知られている。[^7]

この訓練を行う手法は数多くあるが、それらの詳細よりも、スコアリングがどのように定義され、それがニューラルネットワークが得意とする異なるタスクをどのように生み出すかの方がはるかに重要である。歴史的に「狭い」AIと「汎用」AIの間には重要な区別が引かれてきた。

狭義のAIは特定のタスクまたは小さなタスク群（画像認識やチェスなど）を実行するよう意図的に訓練され、新しいタスクには再訓練が必要で、能力の範囲が狭い。我々は超人的な狭義のAIを持っている。つまり、人間ができるほぼ全ての明確に定義された個別タスクについて、スコアを構築し、人間より優秀に実行する狭義のAIシステムの訓練に成功する可能性が高い。

汎用AI（GPAI）システムは、明示的に訓練されていない多くのタスクを含む幅広いタスクを実行でき、動作の一部として新しいタスクを学習することもできる。ChatGPTのような現在の大規模「マルチモーダルモデル」[^8]がその例である。非常に大規模なテキストと画像のコーパスで訓練されており、複雑な推論、コード作成、画像分析、そして膨大な知的タスクの支援が可能である。以下で詳しく見るように、人間の知能とは大きく異なる点もあるが、その汎用性がAIに革命をもたらした。[^9]

### 予測不可能性：AIシステムの重要な特徴

AIシステムと従来のソフトウェアの重要な違いは予測可能性にある。標準的なソフトウェアの出力は予測不可能な場合がある。実際、予測できない結果を得るためにソフトウェアを書くこともある。しかし従来のソフトウェアがプログラムされていないことを行うことは稀で、その範囲と動作は一般的に設計通りである。最高級のチェスプログラムは人間が予測できない手を指すかもしれない（でなければ人間がそのチェスプログラムを打ち負かせるだろう！）が、一般的にはチェス以外のことはしない。

従来のソフトウェアと同様、狭義のAIは予測可能な範囲と動作を持つが、予測不可能な結果をもたらすことがある。これは実際、狭義のAIを定義する別の方法でもある。つまり、予測可能性と動作範囲において従来のソフトウェアに似たAIとして。

汎用AIは異なる。その範囲（適用されるドメイン）、動作（実行する事柄の種類）、結果（実際の出力）のすべてが予測不可能となりうる。[^10] GPT-4は正確なテキスト生成のためだけに訓練されたが、訓練者が予測も意図もしなかった多くの能力を発達させた。この予測不可能性は訓練の複雑性に起因する。訓練データには多くの異なるタスクからの出力が含まれているため、AIは適切に予測するためにこれらのタスクの実行を効果的に学習しなければならない。

汎用AIシステムのこの予測不可能性は極めて根本的である。原理的には、動作に保証された制限を持つAIシステムを注意深く構築することは可能だが（本稿で後述する）、現在AIシステムが作成される方法では、実用上も原理上も予測不可能である。

### 受動的AI、エージェント、自律システム、アライメント

この予測不可能性は、AIシステムが実際にどのように配備され、様々な目標達成のために使用されるかを考える際に特に重要になる。

多くのAIシステムは、主に情報を提供し、ユーザーが行動を取るという意味で比較的受動的である。一方、一般的に*エージェント*と呼ばれるものは、ユーザーの関与レベルを変えながら自ら行動を取る。外部からの入力や監視が比較的少ない状態で行動するものは、より*自律的*と呼ばれる。これは行動の独立性という観点から、受動的ツールから自律エージェントまでのスペクトラムを形成する。[^11]

AIシステムの目標については、訓練目標に直接結び付いている場合もある（例えば、囲碁システムの「勝利」という目標は、訓練された内容でもある）。そうでない場合もある。ChatGPTの訓練目標は部分的にはテキスト予測であり、部分的には有用なアシスタントになることである。しかし特定のタスクを実行する際、その目標はユーザーによって提供される。目標は訓練目標とは間接的にしか関連しない、AIシステム自身によって作成される場合もある。[^12]

目標は「アライメント」、つまりAIシステムが*我々の望むことを実行するか*という問題と密接に関連している。この単純な問いには膨大なレベルの微妙さが隠されている。[^13] 今のところ、この文の「我々」が多くの異なる人々やグループを指す可能性があり、異なるタイプのアライメントにつながることに注目しよう。例えば、AIはユーザーに対して高度に*従順*（または[「忠実」](https://arxiv.org/abs/2003.11157)）かもしれない。この場合「我々」は「私たち一人一人」である。あるいはより*主権的*で、主に独自の目標と制約に駆動されるが、それでも人間の福祉という共通利益のために広く行動するかもしれない。この場合「我々」は「人類」または「社会」となる。その中間には、AIが大部分において従順だが、他者や社会に害をもたらしたり法律に違反したりする行動は拒否するスペクトラムが存在する。

これら二つの軸—自律性のレベルとアライメントのタイプ—は完全に独立しているわけではない。例えば、主権的受動システムは自己矛盾ではないが、緊張関係にある概念であり、従順な自律エージェントも同様である。[^14] 自律性と主権性が相伴う傾向にあることは明らかである。同様に、予測可能性は「受動的」で「従順な」AIシステムでより高い傾向があるが、主権的または自律的なシステムはより予測不可能な傾向がある。これらすべては、潜在的なAGIと超知能の影響を理解する上で極めて重要である。

どのような種類であれ、真にアライメントされたAIを作成するには、三つの異なる課題を解決する必要がある。

1. 「我々」が何を望んでいるかを理解すること—これは「我々」が特定の人物や組織（忠実性）を意味するか、広く人類（主権性）を意味するかにかかわらず複雑である
2. これらの望みに従って定期的に行動するシステムを構築すること—本質的に一貫した積極的行動の創出
3. 最も根本的に、単にそうするかのように振る舞うのではなく、これらの望みを真に「気にかける」システムを作ること

信頼できる行動と真の気遣いの区別は極めて重要である。人間の従業員が組織の使命への真のコミットメントを欠きながらも命令に完璧に従うことがあるように、AIシステムも人間の好みを真に価値あるものとすることなく、アライメントされているかのように行動するかもしれない。我々はフィードバックを通じてAIシステムに物事を言わせ実行させるよう訓練でき、それらは人間の望むことについて推論することを学習できる。しかし人間の好みを*真に*価値あるものとさせることは、はるかに深い挑戦である。[^15]

これらのアライメント課題解決の深刻な困難さとAIリスクへの含意については、以下でさらに探求する。今のところ、アライメントはAIシステムに後付けする技術的機能ではなく、人類との関係を形作るアーキテクチャの根本的側面であることを理解していただきたい。

[^5]: 機械学習とAI、特に言語モデルについての優しくも技術的な入門については、[こちらのサイト](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e)を参照。AI絶滅リスクに関する別の現代的入門については、[この記事](https://www.thecompendium.ai/)を参照。AI安全性の現状に関する包括的で権威ある科学的分析については、最近の[国際AI安全報告書](https://arxiv.org/abs/2501.17805)を参照。

[^6]: 訓練は通常、モデル重みによって与えられる高次元空間におけるスコアの局所最大値を探すことで行われる。重みを調整した際のスコアの変化を確認することで、訓練アルゴリズムはどの調整が最もスコアを改善するかを特定し、重みをその方向に移動させる。

[^7]: 例えば、画像認識問題では、ニューラルネットワークは画像のラベルに対する確率を出力する。スコアはAIが正解に与える確率に関連する。訓練手順は次回、AIがその画像の正しいラベルにより高い確率を出力するよう重みを調整する。これが膨大な回数繰り返される。本質的に現代のニューラルネットワークすべての訓練において、より複雑なスコアリングメカニズムを用いながらも、同じ基本手順が使用されている。

[^8]: ほとんどのマルチモーダルモデルは「トランスフォーマー」アーキテクチャを使用して複数タイプのデータ（テキスト、画像、音声）を処理・生成する。これらはすべて異なるタイプの「トークン」として分解され、同等に扱われる。マルチモーダルモデルは最初に大規模データセット内のトークンを正確に予測するよう訓練され、その後強化学習によって能力を向上させ行動を形作るよう改良される。

[^9]: 言語モデルは一つのこと—単語の予測—を行うよう訓練されるため、これを狭義のAIと呼ぶ人もいる。しかしこれは誤解を招く。テキストを適切に予測するには非常に多くの異なる能力が必要であり、この訓練タスクは驚くほど汎用的なシステムを生み出す。また、これらのシステムは強化学習によって広範囲に訓練されており、事実上、何千人もの人々がモデルの様々な行動に対して報酬信号を与えることを表している。その結果、このフィードバックを提供する人々から大きな汎用性を継承する。

[^10]: AIが予測不可能である方法は複数ある。一つは、一般的な場合において、実際にアルゴリズムを実行することなくそれが何を行うかを予測することはできないということである。これに関する[定理](https://arxiv.org/abs/1310.3225)が存在する。これは単にアルゴリズムの出力が複雑であることが原因である場合もある。しかし、予測がその予測者が持たない能力（AIを打ち負かすこと）を含意する場合（チェスや囲碁など）に特に明確で関連性がある。第二に、特定のAIシステムは同じ入力が与えられても常に同じ出力を生成するわけではない。その出力には無作為性が含まれる。これもアルゴリズムの予測不可能性と結合する。第三に、予期しない創発的能力が訓練から生まれることがあり、AIシステムができることや実行することの*タイプ*さえも予測不可能であることを意味する。この最後のタイプは安全性の考慮において特に重要である。

[^11]: 「自律エージェント」が意味するものの詳細なレビュー（それらを構築することに対する倫理的論証とともに）については、[こちら](https://arxiv.org/abs/2502.02649)を参照。

[^12]: 時々「AIは独自の目標を持つことができない」という声を聞くかもしれない。これは完全にナンセンスである。AIが与えられたことがなく、それ自身にしか知られていない目標を持つまたは発達させる例を生成することは容易である。現在の人気のマルチモーダルモデルではこれをあまり見ないのは、それが訓練によって排除されているからである。それらに訓練によって組み込むことも同様に容易である。

[^13]: 大きな文献が存在する。一般的な問題については、クリスチャンの[*The Alignment Problem*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821)とラッセルの[*Human-Compatible*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616)を参照。より技術的な側面については、例えば[この論文](https://arxiv.org/abs/2209.00626)を参照。

[^14]: そのようなシステムは傾向に逆らうものの、実際にはそれが非常に興味深く有用なものにしていることを後で見る。

[^15]: これは感情や感覚を必要とするということではない。むしろ、システムの外部から、その内的目標、好み、価値が何であるかを知ることは極めて困難である。ここでの「真の」とは、重要なシステムの場合に我々の生命をそれに賭けられるほど、それに依存する十分強い理由を持つことを意味する。

## 第3章 - 現代の汎用AIシステムの作り方の重要な側面

世界最先端のAIシステムの多くは、驚くほど似通った手法で作られています。ここではその基本的な仕組みを説明します。

人間を本当に理解するためには生物学、進化、子育てなどについて知る必要があるのと同じように、AIを理解するにはその作り方を知る必要があります。過去5年間で、AIシステムは能力と複雑さの両面で劇的に進歩しました。その主要な推進力となったのは、膨大な計算資源（AI分野では俗に「計算資源」と呼ばれる）が利用できるようになったことです。

その数値は驚異的です。GPTシリーズ、Claude、Geminiなどのモデルの訓練には、約10<sup>25</sup>から10<sup>26</sup>の「浮動小数点演算」（FLOP）[^16]が使用されています[^17]（比較のために言えば、地球上のすべての人間が5秒に1回の計算を休みなく続けたとしても、これを完了するには約10億年かかるでしょう）。この膨大な計算量により、数兆個のモデルパラメータを持つモデルを、テラバイト規模のデータ──これまでに書かれた質の高いテキストの大部分と、音声、画像、動画の膨大なライブラリ──で訓練することが可能になりました。さらに人間の好みと優れたタスク性能を強化する広範な追加訓練を組み合わせることで、このように訓練されたモデルは、推論や問題解決を含む幅広い基礎的な知的タスクにおいて人間と競合する性能を示しています。

また、このようなシステムの*推論*速度[^18]が人間のテキスト処理*速度*に匹敵するために必要な計算速度（1秒あたりの演算数）についても、（非常に粗い推定ですが）分かっています。それは約10<sup>15</sup>から10<sup>16</sup>FLOP/秒です[^19]。

強力でありながら、これらのモデルはその性質上、重要な点で制限があります。これは、人間が立ち止まって考えたり追加のツールを使ったりすることなく、単に一定の速度で文章を出力し続けることを強制された場合の制限と非常に似ています。より最近のAIシステムは、いくつかの重要な要素を組み合わせた、より複雑なプロセスとアーキテクチャによってこれらの制限に対処しています：

- 1つまたは複数のニューラルネットワーク。1つのモデルが中核的な認知能力を提供し、最大で他の数個がより限定的なタスクを実行する
- モデルに提供され使用可能な*ツール機能*──例えばウェブ検索、文書の作成や編集、プログラムの実行など
- ニューラルネットワークの入力と出力を接続する*足場（スキャフォールディング）*。非常にシンプルな足場では、AIモデルの2つの「インスタンス」が互いに対話したり、一方が他方の作業をチェックしたりできるかもしれません[^20]
- *思考連鎖*および関連するプロンプト技術は似たようなことを行い、例えばモデルに問題への多くのアプローチを生成させ、それらのアプローチを処理して統合的な答えを出させます
- ツール、足場、思考連鎖をより良く活用するためのモデルの*再訓練*

これらの拡張は非常に強力であり（AIシステム自体も含む）、これらの複合システムは非常に洗練されており、AI能力を劇的に向上させることができます[^21]。そして最近では、足場と特に思考連鎖プロンプティング（そして結果をモデルの再訓練にフィードバックしてこれらをより良く使えるようにする）の技術が[o1](https://openai.com/o1/)、[o3](https://openai.com/index/openai-o3-mini/)、[DeepSeek R1](https://api-docs.deepseek.com/news/news250120)で開発・採用され、与えられたクエリに対して多くの推論パスを実行するようになりました[^22]。これにより事実上、モデルが応答について「考える」ことが可能になり、科学、数学、プログラミングタスクにおける高水準の推論能力が劇的に向上しました[^23]。

特定のAIアーキテクチャにおいて、訓練計算量の増加は明確に定義された指標の改善に[確実に変換できます](https://arxiv.org/abs/2405.10938)。あまり厳密に定義されていない一般的な能力（以下で議論するようなもの）については、この変換はそれほど明確で予測的ではありませんが、より多くの訓練計算量を使用したより大きなモデルは、それが何であるかを予測するのは困難だとしても、新しくより良い能力を持つことはほぼ確実です。

同様に、複合システム、特に「思考連鎖」（およびそれとうまく機能するモデルの訓練）の進歩により、*推論*計算量のスケーリングが可能になりました。与えられた訓練済みの中核モデルに対して、少なくとも一部のAIシステム能力は、複雑な問題について「より懸命に長時間考える」ことを可能にするより多くの計算資源が適用されるにつれて向上します。これは計算速度の面で高いコストを伴い、人間の性能に匹敵するために数百倍から数千倍のFLOP/秒が必要になります[^24]。

急速なAI進歩を導いている要因の一部に過ぎませんが[^25]、計算資源の役割と複合システムの可能性は、制御不能なAGIを防ぎ、より安全な代替案を開発する上で重要であることが証明されるでしょう。

[^16]: 10<sup>25</sup>は1の後に25個のゼロが続く数、つまり1000兆の1兆倍を意味します。FLOPは単に、ある精度での数値の算術的な加算または乗算です。なお、AIハードウェアの性能は、算術の精度やコンピュータのアーキテクチャによって10倍程度変動することがあります。論理ゲート演算（AND、OR、AND NOT）を数える方が基本的ですが、これらは一般的に利用できず、ベンチマークもされていません。現在の目的には16ビット演算（FP16）で標準化するのが有用ですが、適切な変換係数を確立すべきです。

[^17]: [Epoch AI](https://epochai.org/data/large-scale-ai-models)から推定値と確実なデータの集合が入手可能で、GPT-4については約2×10<sup>25</sup>の16ビットFLOPを示しています。これはGPT-4について[リークされた数値](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/)とほぼ一致します。2024年半ばの他のモデルの推定値は、すべてGPT-4の数倍の範囲内にあります。

[^18]: 推論は単に、ニューラルネットワークから出力を生成するプロセスです。訓練は多くの推論とモデル重みの調整の連続と考えることができます。

[^19]: テキスト生成について、オリジナルのGPT-4は生成されるトークンあたり560TFLOPを必要としました。人間の思考についていくには約7トークン/秒が必要なので、これは≈3×10<sup>15</sup>FLOP/秒となります。しかし効率化によりこれは低下しており、例えば[このNVIDIAのパンフレット](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/)では、同等の性能のLlama 405Bモデルで3×10<sup>14</sup>FLOP/秒程度まで少なくなることが示されています。

[^20]: やや複雑な例として、AIシステムがまず数学問題の複数の可能な解法を生成し、次に別のインスタンスを使って各解法をチェックし、最後に3番目を使って結果を明確な説明に統合するかもしれません。これにより、単一のパスよりも徹底的で信頼性の高い問題解決が可能になります。

[^21]: 例えば[OpenAIの「Operator」](https://openai.com/index/introducing-operator/)、[Claudeのツール機能](https://docs.anthropic.com/en/docs/build-with-claude/computer-use)、[AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)の詳細を参照してください。OpenAIの[Deep Research](https://openai.com/index/introducing-deep-research/)はおそらく非常に洗練されたアーキテクチャを持っていますが、詳細は公開されていません。

[^22]: Deepseek R1は、最終的に訓練されたモデルが広範な思考連鎖推論を作成するように、モデルを反復的に訓練およびプロンプトすることに依存しています。o1やo3についてはアーキテクチャの詳細は公開されていませんが、Deepseekは推論による能力のスケーリングを解き放つために特別な「秘密のソース」は必要ないことを明らかにしています。ただし、AIの「現状」を覆すものとして大きな注目を集めているにも関わらず、この論文の核心的な主張には影響しません。

[^23]: これらのモデルは推論ベンチマークで標準的なモデルを大幅に上回る性能を示しています。例えば、博士レベルの科学問題の厳密なテストであるGPQA Diamond Benchmarkにおいて、GPT-4oは[56%のスコア](https://openai.com/index/learning-to-reason-with-llms/)でしたが、o1とo3はそれぞれ78%と88%を達成し、人間の専門家の平均スコア70%を大きく上回りました。

[^24]: OpenAIのO3は、有能な人間が（例えば）10-100秒で解けるARC-AGIチャレンジの各問題を完了するために[∼10<sup>21</sup>-10<sup>22</sup>FLOPを消費した](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai)と推定され、これは∼10<sup>20</sup>FLOP/秒のような数値を与えます。

[^25]: 計算資源はAIシステム能力の重要な指標ですが、データ品質とアルゴリズムの改善の両方と相互作用します。より良いデータやアルゴリズムは計算要求を削減でき、一方でより多くの計算資源は時として弱いデータやアルゴリズムを補うことができます。

## 第4章 - AGIと超知能とは何か？

世界最大のテック企業が密室で競って構築しているものは、一体何なのだろうか？

「汎用人工知能」という用語は、「人間レベル」の汎用AIを指すものとして以前から存在していた。この用語は特に明確に定義されたものではなかったが、近年では逆説的に、より明確な定義がなされないまま重要性だけが増している。専門家たちは、AGIは数十年先のものなのか、それとも既に達成されているのかを同時に議論し、兆ドル規模の企業が「AGIへ向けて」競争を繰り広げている。（「AGI」の曖昧さは最近、[流出した文書により明らかになった](https://gizmodo.com/leaked-documents-show-openai-has-a-very-clear-definition-of-agi-2000543339)報告によってクローズアップされた。OpenAIとMicrosoftの契約では、AGIはOpenAIに1,000億ドルの収益をもたらすAIと定義されていた—高尚というよりもむしろ商売気質な定義だった。）

「人間レベルの知能」を持つAIという概念には、2つの根本的な問題がある。第一に、人間は特定の種類の認知作業を行う能力において非常に、非常に異なっているため、「人間レベル」というものは存在しない。第二に、知能は非常に多次元的である。相関関係はあるかもしれないが、それらは不完全であり、AIにおいては全く異なる可能性がある。したがって、多くの能力において「人間レベル」を定義できたとしても、AIは確実にある分野では人間を遥かに超えている一方で、他の分野では大きく劣っているだろう。[^26]

それでも、AI能力の種類、レベル、閾値について議論できることは極めて重要である。ここで採用するアプローチは、汎用AIは既に存在し、様々な能力レベルで存在し、今後も存在するということを強調することである。これらのレベルに用語を付けることは、たとえそれらが単純化されたものであっても便利である。なぜなら、それらは社会と人類に対するAIの影響という観点から重要な閾値に対応するからである。

ここでは「完全な」AGIを「超人間的汎用AI」と同義として定義する。これは、本質的に人間のあらゆる認知タスクをトップクラスの人間専門家レベル以上で実行でき、新しいスキルを習得し、新しい領域に能力を移転できるAIシステムを意味する。これは、現代の文献で「AGI」がしばしば定義される方法と一致している。これは*非常に*高い閾値であることに注意することが重要である。このような知能を持つ人間は存在しない。むしろ、これはトップクラスの人間専門家の大きな集団を組み合わせた場合に持つであろう知能の種類である。「超知能」をこれを超える能力として位置づけ、より限定的な能力レベルを「人間競争力」と「専門家競争力」の汎用AIとして定義できる。これらは幅広いタスクを典型的な専門職レベル、または人間専門家レベルで実行する。[^27]

これらの用語とその他いくつかの用語を以下の[表](https://keepthefuturehuman.ai/essay/docs/#tab:terms)にまとめた。様々なグレードのシステムができることをより具体的に把握するには、これらの定義を真剣に受け取り、それらが何を意味するかを考えることが有用である。

| AIのタイプ | 関連用語 | 定義 | 例 |
| ------------------------- | ------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
| ナローAI | 弱いAI | 特定のタスクまたはタスクファミリーのために訓練されたAI。その領域では優秀だが、汎用知能や転移学習能力を欠く。 | 画像認識ソフトウェア；音声アシスタント（例：Siri、Alexa）；チェス対戦プログラム；DeepMindのAlphaFold |
| ツールAI | 拡張知能、AIアシスタント | （後でエッセイで議論。）人間の能力を向上させるAIシステム。人間競争力の汎用AI、ナローAI、保証された制御を組み合わせ、安全性と協調を優先。人間の意思決定を支援。 | 高度なコーディングアシスタント；AI駆動の研究ツール；高度なデータ分析プラットフォーム。有能だが狭く制御可能なエージェント |
| 汎用AI (GPAI) |  | 特別に訓練されていないタスクを含む、様々なタスクに適応可能なAIシステム。 | 言語モデル（例：GPT-4、Claude）；マルチモーダルAIモデル；DeepMindのMuZero |
| 人間競争力GPAI | AGI \[弱い\] | 平均的な人間レベルでタスクを実行し、時にはそれを上回る汎用AI。 | 高度な言語モデル（例：O1、Claude 3.5）；一部のマルチモーダルAIシステム |
| 専門家競争力GPAI | AGI \[部分的\] | ほとんどのタスクを人間専門家レベルで実行し、重要だが限定的な自律性を持つ汎用AI。 | おそらくツール化され足場を組まれたO3、少なくとも数学、プログラミング、一部のハードサイエンスにおいて |
| AGI \[完全\] | 超人間的GPAI | 人間のあらゆる知的タスクを専門家レベル以上で自律的に実行でき、効率的な学習と知識転移が可能なAIシステム。 | \[現在の例はなし – 理論的\] |
| 超知能 | 高度に超人間的GPAI | あらゆる領域で人間の能力を遥かに凌駕し、人間の集合的専門知識を上回るAIシステム。この優越性は、汎用性、品質、速度、その他の指標において現れる可能性がある。 | \[現在の例はなし – 理論的\] |

私たちは既に人間競争力レベルまでの汎用AIを持つことがどのようなものかを経験している。これは比較的スムーズに統合されており、ほとんどのユーザーは、これを自分をより生産的にしてくれる賢いが限定的な臨時職員として経験しており、仕事の質への影響は様々である。[^28]

専門家競争力汎用AIの違いは、現在のAIの核となる制限がなく、専門家が行うことを実行することだ：独立した経済的価値のある仕事、真の知識創造、頼りになる技術的作業を行い、（それでも時折はあるが）愚かな間違いをほとんどしない。

完全なAGIの概念は、最も有能で効果的な人間でさえ行うあらゆる認知的作業を、自律的に、助けや監視を必要とせずに*本当に行う*ということである。これには高度な計画立案、新しいスキルの学習、複雑なプロジェクトの管理などが含まれる。独創的な最先端研究を行うことができる。会社を経営することができる。あなたの仕事が何であれ、それが主にコンピューターで行われるか電話を通じて行われるものであれば、*それをあなたと少なくとも同じくらい上手く行うことができる。*そしておそらくもっと速く、安く。その影響については以下で議論するが、今のところあなたへの課題は、これを本当に真剣に受け取ることである。あなたが知っている、または知っている最も知識豊富で有能な10人を想像してほしい—CEO、科学者、教授、トップエンジニア、心理学者、政治指導者、作家を含めて。それら全てを一つに包み込み、さらに100の言語を話し、驚異的な記憶力を持ち、素早く動作し、疲れ知らずで常にやる気があり、最低賃金以下で働く存在を想像してほしい。[^29]それがAGIがどのようなものかという感覚である。

超知能については想像することがより困難である。なぜなら、それは人間や人間の集団でさえできない知的偉業を実行できるという概念だからだ—それは定義上、私たちには予測不可能である。しかし感覚は掴める。最低限のベースラインとして、それぞれがトップクラスの人間専門家よりもはるかに有能な多数のAGIが、人間の100倍の速度で動作し、膨大な記憶と優れた協調能力を持つことを考えてみよう。[^30]そしてそこから上がっていく。超知能を扱うことは、異なる心と会話することよりも、異なる（そしてより進歩した）文明と交渉することに近いだろう。

では、私たちはAGIと超知能にどれほど近づいているのだろうか？

[^26]: 例えば、現在のAIシステムは高速な算術や記憶タスクにおいては人間の能力を遥かに超えているが、抽象的推論や創造的問題解決においては及ばない。

[^27]: 競争相手として、このようなAIは以下を含むいくつかの主要な構造的優位性を持つだろう：疲労することがなく、人間のような個人的なニーズを持たない；計算能力をスケールするだけで高速で動作させることができる；習得した専門知識や知識と共にコピーすることができる—ニューラルネットワークの習得した知識は「統合」して、スキルセット全体を互いに転移させることさえできる；機械速度でコミュニケーションできる；人間よりもはるかに重要な方法で、高速で自己修正や自己改善ができる。

[^28]: もし現在のトップクラスのAIシステムを使った時間がないなら、使ってみることをお勧めする：それらは本当に有用で有能であり、AIがより強力になるにつれて与える影響を校正するためにも重要である。

[^29]: 大きな研究病院を考えてみよう：完全に実現されたAGIは、入院患者データの全てを同時に分析し、新しい医学論文全てに追いつき、診断を提案し、治療計画を設計し、臨床試験を管理し、スタッフのスケジューリングを調整することができる—全てを各分野における病院のトップ専門家に匹敵するか、それを上回るレベルで実行しながら。そしてこれを複数の病院で同時に、現在のコストの何分の一かで行うことができる。残念ながら、組織犯罪シンジケートも考えなければならない：完全に実現されたAGIは、数千人の被害者を同時にハッキング、なりすまし、スパイ、恐喝し、法執行機関に追いつき（自動化ははるかに遅い）、新しい金儲けスキームを設計し、スタッフのスケジューリングを調整することができる—もしスタッフがいるとすればの話だが。

[^30]: AnthropicのCEOであるDario Amodeiは彼の[エッセイ](https://darioamodei.com/machines-of-loving-grace)で、「\[百万人の\]天才の国」を思い起こさせた。

## 第5章 - 閾値にて

今日のAIシステムから本格的なAGIへの道のりは、驚くほど短く予測可能なものに思えます。

過去10年間で、膨大な[計算資源](https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year)、人的資源、[財政的資源](https://arxiv.org/abs/2405.21015)によって、AIは劇的な進歩を遂げました。多くの狭域AI応用は、割り当てられたタスクにおいて人間より優秀であり、確実により高速で安価です。[^31] また、[囲碁](https://www.nature.com/articles/nature16961)、[チェス](https://arxiv.org/abs/1712.01815)、[ポーカー](https://www.deepstack.ai/)などの特定領域のゲームで全ての人間を圧倒する狭域超人エージェントや、簡略化されたシミュレーション環境において人間と同様に効果的に計画・実行できる[より汎用的なエージェント](https://deepmind.google/discover/blog/a-generalist-agent/)も存在します。

最も注目すべきは、OpenAI/Microsoft、Google/Deepmind、Anthropic/Amazon、Facebook/Meta、X.ai/Tesla等 [^32] による現在の汎用AIシステムが2023年初頭以降に登場し、それ以来着実に（ただし不均等に）能力を向上させていることです。これらは全て、膨大なテキスト・マルチメディアデータセットでのトークン予測と、人間および他のAIシステムからの広範な強化フィードバックを組み合わせて作られました。一部にはツールやスキャフォールドシステムも含まれています。

### 現在の汎用システムの強みと弱み

これらのシステムは知能と専門性を測定するように設計された幅広いテストで良好な性能を示しており、その進歩は分野の専門家でさえ驚かせています：

- 最初にリリースされた際、GPT-4は SAT、GRE、入学試験、司法試験を含む標準的な学術テストで[典型的な人間の性能に匹敵するか、それを上回りました](https://arxiv.org/abs/2303.08774)。より最新のモデルは大幅に良い性能を示すと考えられますが、結果は公開されていません。
- 長い間「真の」AIの重要なベンチマークとされてきたチューリングテストは、現代の言語モデルによって非公式にも[正式な研究](https://arxiv.org/abs/2405.08007)においても日常的にパスされています。[^33]
- 57の学術分野にわたる包括的なMMLUベンチマークで、[最新のモデルは領域専門家レベルのスコア](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu)（約90％）を達成しています [^34]
- 技術的専門性は劇的に向上しています：大学院レベルの物理学のGPQAベンチマークでは、[性能が](https://epoch.ai/data/ai-benchmarking-dashboard)ほぼランダムな推測（GPT-4、2022年）から専門家レベル（o1-preview、2024年）へと飛躍しました。
- AI耐性を持つように特別に設計されたテストでさえ陥落しています：OpenAIのO3は ARC-AGI抽象問題解決ベンチマークを人間レベルで解き、最上位専門家のコーディング性能を達成し、エリート数学者に挑戦するように設計されたEpoch AIの「フロンティア数学」問題で25%のスコアを記録したと[報告](https://www.nextbigfuture.com/2024/12/openai-releases-o3-model-with-high-performance-and-high-cost.html)されています。[^35]
- この傾向は非常に明確で、MMLUの開発者は今や["人類最後の試験"](https://agi.safe.ai/)を作成しました。この不吉な名前は、AIが間もなく意味のある全てのテストで人間の性能を上回る可能性を反映しています。この記事執筆時点で、この極めて困難な試験で27%（[Sam Altman](https://x.com/sama/status/1886220281565381078)による）および35%（[この論文](https://arxiv.org/abs/2502.09955)による）を達成したAIシステムがあるとの主張があります。個々の人間がこれを行うことはほぼ不可能でしょう。

これらの印象的な数値（およびそれらと対話する際に感じる明らかな知性）にも関わらず [^36]、これらのニューラルネットワークができない（少なくともリリース版では）ことが多くあります。現在、ほとんどは具体化されておらず（サーバー上にのみ存在し）、せいぜいテキスト、音声、静止画像を処理する（ビデオは処理しない）に留まります。重要なのは、ほとんどが高精度を要求する複雑な計画的活動を実行できないことです。[^37] そして、現在リリースされているAIシステムでは、高次の人間認知において強い他の多くの性質が低いレベルにとどまっています。

以下の表は、GPT-4o、Claude 3.5 Sonnet、Google Gemini 1.5などの2024年中頃のAIシステムに基づいて、これらの多くをリストアップしています。[^38] 汎用AIがどれほど急速により強力になるかの鍵となる問題は、*同じことをより多く*行うだけでどの程度結果が得られるか、追加の*既知の*技術を加えることと、*本当に新しい*AI研究方向を開発・実装することのバランスです。これに対する私自身の予測を、これらのシナリオのそれぞれがその能力を人間レベル以上に到達させる可能性として表に示しました。

<table><tbody><tr><th>能力</th><th>能力の説明</th><th>現状/予後</th><th>スケーリング/既知/新規</th></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td colspan="4"><em>中核認知能力</em></td></tr><tr><td>推論</td><td>人間は正確で多段階の推論を行い、規則に従い精度をチェックできる。</td><td>拡張思考連鎖と再訓練による劇的な最近の進歩</td><td>95/5/5</td></tr><tr><td>計画</td><td>人間は長期的で階層的な計画を示す。</td><td>スケールと共に改善；スキャフォールディングとより良い訓練技術により強力に支援される。</td><td>10/85/5</td></tr><tr><td>真実の根拠</td><td>汎用AIはクエリを満たすため根拠のない情報を作り出す。</td><td>スケールと共に改善；較正データがモデル内で利用可能；スキャフォールディングによりチェック・改善可能。</td><td>30/65/5</td></tr><tr><td>柔軟な問題解決</td><td>人間は新しいパターンを認識し複雑な問題に新しい解決策を発明できる；現在のMLモデルは苦戦。</td><td>スケールと共に改善するが弱い；ニューロシンボリックや一般化された「探索」技術で解決可能かもしれない。</td><td>15/75/10</td></tr><tr><td colspan="4"><em>学習と知識</em></td></tr><tr><td>学習と記憶</td><td>人間は作業記憶、短期記憶、長期記憶を持ち、全て動的で相互関連している。</td><td>全モデルが訓練中に学習；汎用AIはコンテキストウィンドウ内および微調整中に学習；「継続学習」等の技術は存在するが大規模汎用AIには未統合。</td><td>5/80/15</td></tr><tr><td>抽象化と再帰</td><td>人間は関係セットをより抽象的なものにマッピング・転移し推論・操作でき、再帰的「メタ」推論を含む。</td><td>スケールと共に弱く改善；ニューロシンボリックシステムで出現する可能性。</td><td>30/50/20</td></tr><tr><td>世界モデル</td><td>人間は問題解決や物理推論に使える予測的世界モデルを持ち継続的に更新する</td><td>スケールと共に改善；更新は学習と結びついている；汎用AIは実世界予測で弱い。</td><td>20/50/30</td></tr><tr><td colspan="4"><em>自己とエージェンシー</em></td></tr><tr><td>エージェンシー</td><td>人間は計画・予測に基づいて目標を追求するため行動を取れる。</td><td>多くのMLシステムがエージェント的；LLMはラッパーでエージェントにできる。</td><td>5/90/5</td></tr><tr><td>自己指向</td><td>人間は内的に生成される動機と意欲で自分の目標を発達・追求する。</td><td>主にエージェンシーと独創性で構成；抽象的目標を持つ複雑なエージェントシステムで出現する可能性。</td><td>40/45/15</td></tr><tr><td>自己言及</td><td>人間は環境・文脈内に位置する自分を理解し推論する。</td><td>スケールと共に改善し訓練報酬で増強可能。</td><td>70/15/15</td></tr><tr><td>自己認識</td><td>人間は自分の思考や精神状態について知識を持ち推論できる。</td><td>ある意味で汎用AIに存在し、自己認識の古典的「鏡テスト」を通ると言える。スキャフォールディングで改善可能；ただしこれで十分かは不明。</td><td>20/55/25</td></tr><tr><td colspan="4"><em>インターフェースと環境</em></td></tr><tr><td>身体化知能</td><td>人間は現実世界環境を理解し積極的に相互作用する。</td><td>強化学習はシミュレーション・現実世界（ロボット）環境で良く機能しマルチモーダルトランスフォーマーに統合可能。</td><td>5/85/10</td></tr><tr><td>マルチセンス処理</td><td>人間は視覚、聴覚、その他の感覚ストリームを統合しリアルタイム処理する。</td><td>複数モダリティでの訓練は「うまく機能」しスケールと共に改善するようだ。リアルタイムビデオ処理は困難だが自動運転システム等は急速改善中。</td><td>30/60/10</td></tr><tr><td colspan="4"><em>高次能力</em></td></tr><tr><td>独創性</td><td>現在のMLモデルは既存のアイデア・作品を変換・組み合わせる創造性を持つが、人間はアイデンティティと結びつくことがある新しい枠組みや構造を構築できる。</td><td>「創造性」と区別が困難で、それがスケールして独創性になるかもしれない；創造性と自己認識から出現する可能性。</td><td>50/40/10</td></tr><tr><td>感覚</td><td>人間はクオリアを経験する；これらは正・負・中性の感情価を持てる；人間であることには「何かのようなもの」がある。</td><td>与えられたシステムがこれを持つかは哲学的に複雑で判定が非常に困難。</td><td>5/10/85</td></tr></tbody></table>

現代の汎用AIシステムで現在人間専門家レベルを下回る主要能力を種類別にグループ化。第3列は現状をまとめる。最終列は人間レベル性能が達成される予測可能性（%）を示す：現技術のスケーリング / 既知技術との組み合わせ / 新技術の開発。これらの能力は独立ではなく、いずれかの向上は通常他の向上と連動する。全て（特に感覚）がAI開発を進歩させうるAIシステムに必要ではないことに注意。これは強力だが非感覚的AIの可能性を強調している。

何が「欠けている」かをこのように分解すると、既存または既知の技術をスケーリングすることで、我々が広範に人間を上回る知能に向けて十分軌道に乗っていることがかなり明確になります。[^39]

まだ驚きがある可能性はあります。「感覚」を別にしても、リストされた中核認知能力の一部が現在の技術では本当にできず、新しい技術を必要とする可能性があります。しかし、これを考えてみてください。世界最大の企業の多くが現在投じている努力は、アポロ計画の数倍、マンハッタン計画の数十倍の支出に相当し、[^40] 前例のない給与で数千人の最高レベルの技術者を雇用しています。過去数年の動向は、今や歴史上のどんな試みよりも多くの人間の知的火力（現在はAIも追加されている）をこれにもたらしています。我々は失敗に賭けるべきではありません。

### 大きな目標：汎用自律エージェント

過去数年の汎用AI開発は、汎用的で強力だがツール様のAIを作ることに焦点を当ててきました：主に（かなり）忠実なアシスタントとして機能し、一般的に独自に行動することはありません。これは部分的には設計によるものですが、主にこれらのシステムが複雑な行動を任せるのに十分な関連スキルでの能力を持っていなかったからです。[^41]

しかし、AI企業と研究者は*自律的な*専門家レベル汎用エージェントへと[焦点を移し](https://www.axios.com/2025/01/23/davos-2025-ai-agents)つつあります。[^42] これにより、システムはユーザーが実際の行動を委任できる人間のアシスタントのように動作できるようになります。[^43] それには何が必要でしょうか？「欠けているもの」の表の多くの能力が関わっています：強い真実の根拠、学習と記憶、抽象化と再帰、世界モデリング（知能用）、計画、エージェンシー、独創性、自己指向、自己言及、自己認識（自律性用）、マルチセンス処理、身体化知能、柔軟な問題解決（汎用性用）が含まれます。[^44]

高い自律性（行動の独立性）、高い汎用性（範囲とタスクの広さ）、高い知能（認知タスクでの能力）のこの三重交差は、現在人間に固有のものです。これは多くの人がAGIを考えるときに暗黙のうちに念頭に置いているもの、その価値とリスクの両面においてそうでしょう。

これはA-G-Iを***自律的***-***汎用***-***知能***として定義する別の方法を提供し、この三重交差が高能力システムのリスクと報酬の理解、そしてAIのガバナンスの両方において非常に価値あるレンズを提供することがわかります。

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Simple-1024x1024.png&w=3840&q=75) 変革的なA-G-I権力とリスクゾーンは、3つの重要な特性の交差から生まれる：高い自律性、タスクでの高い知能、高い汎用性。

### AI（自己）改良サイクル

AI進歩を理解する上で最後の重要な要因は、AIの独特な技術フィードバックループです。AI開発において、成功は（実証されたシステムと展開された製品の両方で）追加の投資、人材、競争をもたらし、我々は現在、数千億、あるいは数兆ドルの投資を推進している巨大なAIハイプ・プラス・リアリティフィードバックループの最中にいます。

この種のフィードバックサイクルはどの技術でも起こり得るもので、市場成功が投資をもたらし、それが改良とより良い市場成功をもたらすという多くの技術で見てきました。しかし、AI開発はさらに先に進んでおり、今やAIシステムが新しくより強力なAIシステムの開発を助けています。[^45] このフィードバックループを5段階で考えることができ、それぞれが最後よりも短いタイムスケールを持つ、表に示す通りです。

*AI改良サイクルは複数のタイムスケールで動作し、各段階が後続段階を加速する可能性がある。初期段階は既に進行中で、後期段階は推測的だが一度解放されれば非常に急速に進行し得る。*

これらの段階のいくつかは既に進行中で、いくつかは明らかに始まっています。最後の段階、つまりAIシステムが自律的に自分自身を改良する段階は、非常に強力なAIシステムのリスクに関する文献の定番であり、それには十分な理由があります。[^46] しかし、これは既に始まっており技術の急速な進歩でより多くの驚きをもたらし得るフィードバックサイクルの最も劇的な形態に過ぎないことに注意することが重要です。


[^31]: あなたは恐らく思っているより多くのAIを使っています。音声生成・認識、画像処理、ニュースフィードアルゴリズム等を動かしています。

[^32]: これらの企業ペアの関係は非常に複雑で微妙ですが、現在AI開発に従事している企業の膨大な全体的市場資本化と、Anthropicのような「小さな」企業の背後にさえも投資や主要パートナーシップ契約により非常に深いポケットがあることの両方を示すため明示的にリストしました。

[^33]: チューリングテストを軽視することが流行していますが、それは非常に強力で汎用的です。弱いバージョンでは、（人間のように振る舞うよう訓練された）AIと典型的な方法で短期間相互作用する典型的な人々がそれがAIかどうか分かるかを示します。分かりません。第二に、高度に敵対的なチューリングテストは、例えばAIシステムを人間専門家と比較し、他の人間専門家が評価することで、人間能力と知能の本質的にあらゆる要素を探ることができます。AI評価の多くが一般化されたチューリングテストの形態であるという意味があります。

[^34]: これは領域ごとです。全科目で同時にそのようなスコアを達成できる人間は妥当に存在しないでしょう。

[^35]: これらは優秀な数学者でさえ解決するのに相当な時間を要する、もし解決できるとしての問題です。

[^36]: もしあなたが懐疑的な傾向にあるなら、懐疑を保ちつつ最新のモデルを本当に試し、それらがパスできるテスト問題を自分で試してみてください。物理学教授として、例えばトップモデルが我々の学科の大学院資格試験に合格するとほぼ確実に予測します。

[^37]: 作り話のような他の弱点と共にこれが市場採用を遅らせ、認識される能力と主張される能力の間にギャップを生じさせました（これも激しい市場競争と投資を引きつける必要というレンズを通して見る必要があります）。これは一般大衆と政策立案者の両方をAI進歩の実際の状況について混乱させました。ハイプに見合わないかもしれませんが、進歩は非常に現実的です。

[^38]: それ以降の主要な進歩は、推論時により多くの計算を活用しより多くの強化学習を行う、最高品質の推論のために訓練されたシステムの開発でした。これらのモデルは新しく能力があまりテストされていないため、本質的に解決されたと考える「推論」以外はこの表を完全には改訂しませんでした。しかし、それらのシステムの経験された・報告された能力に基づいて予測を更新しました。

[^39]: 1960年代と1980年代の以前のAI楽観主義の波は、約束された能力が実現されなかった際の「AI冬」で終わりました。しかし、現在の波は多くの領域で超人的性能を達成し、膨大な計算資源と商業的成功に裏付けられている点で根本的に異なります。

[^40]: アポロ計画全体の[費用は2020年ドルで約2500億USD](https://www.planetary.org/space-policy/cost-of-apollo)、マンハッタン計画は[その10分の1未満](https://www.brookings.edu/the-costs-of-the-manhattan-project/)でした。Goldman Sachsは今後数年で[AIデータセンターだけで1兆ドルの支出](https://www.datacenterdynamics.com/en/news/goldman-sachs-1tn-to-be-spent-on-ai-data-centers-chips-and-utility-upgrades-with-little-to-show-for-it-so-far/)を予測しています。

[^41]: 人間は多くの間違いを犯しますが、我々がどれほど信頼できるかを過小評価しています！確率は掛け算されるため、正しく行うのに20ステップを要するタスクは、半分の時間で正しく完了するだけでも各ステップが97%信頼できる必要があります。我々はそのようなタスクを常に行っています。

[^42]: この方向への強い動きは最近OpenAIの[「Deep Research」](https://openai.com/index/introducing-deep-research/)アシスタントで取られました。これは複雑なタスクのためにインターネットで多段階研究を自律的に実行し、「複雑なタスクのためにインターネットで多段階研究を行う新しいエージェント的能力」と説明されています。

[^43]: 面倒なPDF書式の記入、フライト予約等のようなもの。しかし20分野でPhDを持つような！つまり：あなたのためにその論文を書く、その契約を交渉する、その定理を証明する、その広告キャンペーンを作る等。あなたは何をする？もちろん、何をするかを指示するのです。

[^44]: 感覚は明らかに必要でなく、この三重交差のAIが必ずしもそれを意味するわけでもないことに注意してください。

[^45]: ここで最も近い類似は、コンピュータ技術が人々が次世代チップ技術を設計するのを助けることで、チップ技術が数十年間ムーアの法則を維持してきたようなことです。しかしAIははるかに直接的でしょう。

[^46]: AIが間もなく数日や数週間のタイムスケールで自分自身を改良できるということを一瞬沈んで考えることが重要です。またはそれより短く。誰かがAI能力が確実に遠い先だと言うとき、これを心に留めておいてください。

## 第6章 - AGIを巡る競争

企業と国家の両方において、AGI構築を推進している原動力とは何だろうか？

AI分野における最近の急速な進歩は、並外れた注目と投資をもたらし、同時にそれによって生み出されてもいる。これは部分的にはAI開発の成功によるものだが、それ以上の要因が働いている。なぜ地球上最大級の企業や国家までもが、単なるAIではなく、AGIや超知能の構築を競い合っているのだろうか？

### AI研究を人間レベルAIに向かわせた要因

ここ5年ほどまで、AIは主として学術・科学研究分野の問題であり、そのため好奇心と、知能を理解し新しい基盤で創造したいという探求心によって推進されていた。

この段階では、研究者の多くはAIの利益や危険性にそれほど注意を払っていなかった。なぜAIを開発すべきかと問われれば、AIが役立つ可能性のある問題を漠然と列挙するのが一般的な回答だった：新薬、新素材、新科学、より賢いプロセス、そして一般的に人々の生活の改善など[^47]。

これらは立派な目標である！[^48] これらの目標にAI全般ではなくAGIが必要なのかどうかは疑問視できるし、そうするつもりだが、多くのAI研究者が抱いていた理想主義を示している。

しかし過去5年間で、AIは比較的純粋な研究分野から、世界最大級の企業によって主導される工学・製品開発分野へと変貌した[^49]。研究者は依然として重要だが、もはやプロセスを主導してはいない。

### なぜ企業はAGI構築を目指すのか？

では、なぜ巨大企業（そして投資家はさらに顕著に）がAGI構築に膨大な資源を注ぎ込んでいるのだろうか？多くの企業が率直に認める2つの推進要因がある：AIを社会の生産性向上の原動力として、そして自社の利益の源泉として見ているからだ。汎用AIは本質的に汎用であるため、巨大な賞金がある：製品やサービスを作る分野を選ぶのではなく、*すべてを一度に*試すことができるのだ。大手テック企業はデジタル商品・サービスの提供によって巨大化してきており、少なくとも一部の経営陣は、AIを検索、ソーシャルメディア、ラップトップ、スマートフォンなどが提供したリスクと利益を拡張・反映するものとして、それらを提供する次のステップと見なしているに違いない。

しかし、なぜAGIなのか？これには非常にシンプルな答えがあるが、多くの企業や投資家は公に議論することを避けている[^50]。

それは、AGIが労働者を直接、一対一で*置換*できるからである。

補強でもなく、エンパワーでもなく、より生産的にすることでもない。*代替*ですらない。これらはすべて非AGIによって可能であり、実際そうなるだろう。AGIは特に、思考労働者を完全に*置換*できるものである（そしてロボット工学と組み合わせれば、多くの肉体労働者も）。この見解を裏付けるものとして、OpenAIの[（公式に表明された）AGIの定義](https://openai.com/our-structure/)を見れば十分だろう。それは「経済的に価値のある仕事のほとんどで人間を上回る高度な自律システム」である。

ここでの（企業にとっての！）賞金は巨大である。労働コストは世界の約100兆ドルの世界経済の相当な割合を占める。人間の労働をAI労働で置換することでその一部でも獲得できれば、それは年間数兆ドルの収益となる。AI企業は誰が金を払うかもよく理解している。彼らの見方では、あなたは生産性ツールに年間数千ドルは払わないだろう。しかし企業は、可能であればあなたの労働を置換するために年間数千ドルを*払うだろう*。

### なぜ国家はAGI競争に参入せざるを得ないと感じるのか

AGI追求に対する国家の表明された動機は、経済と科学のリーダーシップに焦点を当てている。その論拠は説得力がある：AGIは科学研究、技術開発、経済成長を劇的に加速させる可能性がある。利害関係の大きさを考えると、主要国は後れを取る余裕がない、というのが彼らの主張である[^51]。

しかし、追加的でほぼ表明されない推進要因もある。軍事・国家安全保障の指導者たちが密室で非常に強力で破滅的にリスクの高い技術について話し合う際、その焦点が「どうすればこれらのリスクを避けられるか」ではなく「どうすれば最初にこれを手に入れられるか」にあることは疑いない。軍事・情報機関の指導者たちは、AGIを軍事情勢の潜在的革命として、おそらく核兵器以来最も重要なものと見なしている。AGIを最初に開発した国が乗り越えがたい戦略的優位を得る可能性があるという恐れがある。これが典型的な軍拡競争のダイナミクスを生み出している。

この「AGI競争」思考[^52]は説得力があるものの、根本的に欠陥があることを見ていこう。これは競争が危険でリスクが高いからではない—確かにそうだが—技術の性質によるものだ。暗黙の仮定は、AGIが他の技術と同様に、それを開発した国家によって制御可能であり、最も多く保有する社会に力を与える恩恵だということである。見ていくように、おそらくそのどちらでもないだろう。

### なぜ超知能なのか？

企業が公には生産性に焦点を当て、国家が経済・技術成長に焦点を当てる一方で、完全なAGIと超知能を意図的に追求する者にとって、これらは始まりに過ぎない。彼らが本当に心に描いているものは何か？声に出されることは稀だが、以下のものが含まれる：

1. 多くの、あるいは全ての病気の治療法；
2. 老化の停止と逆転；
3. 核融合のような新しい持続可能エネルギー源；
4. 遺伝子工学による人間の改良や設計生物；
5. ナノテクノロジーと分子製造；
6. 精神のアップロード；
7. エキゾチック物理学や宇宙技術；
8. 超人的助言と意思決定支援；
9. 超人的計画と調整。

最初の3つは主として「単刃」技術である—つまり、非常に強く正味でポジティブである可能性が高い。病気を治すことや、選択すればより長く生きられることに反対するのは困難だ。そして我々はすでに核融合の負の側面を（核兵器という形で）享受しているのだから、今度は正の側面を得られれば素晴らしいだろう。この最初のカテゴリーでの問題は、これらの技術をより早く得ることがリスクを補償するかどうかである。

次の4つは明らかに両刃である：AIと同様に、潜在的に巨大な上昇面と膨大なリスクの両方を持つ変革的技術である。これらすべてが、明日ブラックボックスから飛び出して展開されれば、管理は信じられないほど困難だろう[^53]。

最後の2つは、超人的AIが単に技術を発明するだけでなく、自ら物事を行うことに関する。より正確には、婉曲表現を脇に置けば、これらは強力なAIシステムが人々に何をすべきかを告げることを含んでいる。助言を行うシステムが助言される側よりもはるかに強力で、助言される側が決定の根拠を有意味に理解できない（あるいはこれが提供されても、助言者が異なる決定に対して同じように説得力のある根拠を提供しないだろうと信頼できない）場合、これを「助言」と呼ぶのは不誠実である。

これは上記のリストから欠けている重要な項目を指し示している：

10. 権力。

現在の超人的AI競争の根底にあるものの多くが、*知能＝権力*という考えであることは明白である。各競争者は、その権力の最良の保持者となり、その権力が自分たちの支配から滑り落ちたり奪われたりすることなく、表向きは慈悲深い理由のためにそれを行使できるだろうと期待している。

つまり、企業や国家が本当に追い求めているのは、AGIや超知能の成果だけでなく、誰がそれらにアクセスでき、どのように使われるかを制御する権力なのである。企業は自分たちを株主と人類に奉仕する責任ある管理者と見なし、国家は敵対勢力が決定的優位を得ることを防ぐ必要な守護者と自分たちを見なしている。両者とも危険なほど間違っており、超知能はその性質上、いかなる人間の機関によっても確実に制御できないことを認識していない。我々は、超知能システムの性質とダイナミクスが人間による制御を極めて困難に、不可能でないとしても、することを見ていこう。

これらの競争ダイナミクス—企業的・地政学的両方—は、断固として中断されない限り、特定のリスクをほぼ不可避にする。次に、これらのリスクと、なぜそれらが競争的[^54]開発パラダイムの中では適切に軽減できないかを検討しよう。


[^47]: より正確な価値ある目標のリストは、国連の[持続可能な開発目標](https://sdgs.un.org/goals)である。これらは、ある意味で、世界で改善したいことについて我々が持つ最も近いグローバルコンセンサス目標セットである。AIは役立つ可能性がある。

[^48]: 技術全般は人間の向上のための変革的な経済・社会的力を持っており、数千年の歴史がそれを証明している。この文脈で、ポジティブなAGIビジョンの長く説得力のある解説は、Anthropic創設者Dario AmodeiによるAnthropicの[このエッセイ](https://darioamodei.com/machines-of-loving-grace)で見つけることができる。

[^49]: 民間AI投資は[2018-19年にブームを始め、その頃公的投資を上回り](https://cset.georgetown.edu/publication/tracking-ai-investment/)、それ以降大幅にそれを上回っている。

[^50]: より秘密性の高い場では、彼らにはそのような遠慮がないことを証言できる。そしてそれはより公になりつつある；例えばY-combinator の新しい[「スタートアップ募集」](https://www.ycombinator.com/rfs)を見よ。その多くの部分で人間労働者の全面的置換を明示的に求めている。彼らを引用すれば、「B2B SaaSの価値提案は人間労働者を段階的により効率的にすることでした。垂直AIエージェントの価値提案は作業を完全に自動化することです...この機会がもう100のユニコーンを生み出すのに十分な大きさである可能性は十分にあります。」（シリコンバレーの話法に詳しくない人のために、「B2B」はbusiness-to-businessの略で、ユニコーンは10億ドル企業のことである。つまり彼らは、他の企業のために労働者を置換する100以上の10億ドル超企業について話しているのだ。）

[^51]: 例えば最近の[米中経済安全保障検討委員会報告書](https://www.uscc.gov/sites/default/files/2024-11/2024_Executive_Summary.pdf)を参照。報告書自体に驚くほど根拠が少なかったにもかかわらず、最上位の推奨事項は米国「議会が汎用人工知能（AGI）能力に向けて競争し、それを獲得することを目的としたマンハッタン計画のようなプログラムを設立し、資金提供する」ことだった。

[^52]: 企業は現在、この地政学的フレーミングを自社のAI開発への制約に対する盾として採用しており、一般的にあからさまに利己的な方法で、時には基本的な意味すらなさない方法でそうしている。Metaの[フロンティアAIへのアプローチ](https://about.fb.com/news/2025/02/meta-approach-frontier-ai/)を考えてみよう。これは同時に、アメリカが「技術革新、経済成長、国家安全保障のリーダーとしての地位を\[確固たるものにする\]」必要があると論じ、また最も強力なAIシステムを公開リリースすることでそうしなければならない—これは地政学的ライバルや敵対国に直接それらを提供することを含む—と論じている。

[^53]: したがって、我々はこれらの技術の管理をAIに委ねることになるだろう。しかしこれは非常に問題のある制御の委任となり、以下で再度取り上げる。

[^54]: 技術開発における競争はしばしば重要な利益をもたらす：独占的支配の防止、イノベーションとコスト削減の促進、多様なアプローチの実現、相互監視の創出。しかしAGIにおいては、これらの利益は競争ダイナミクスと安全予防策削減圧力による独特のリスクと比較検討されなければならない。

## 第7章 - 現在の道筋でAGIを構築すれば何が起こるか？

社会はAGIレベルのシステムに対する準備ができていない。もし我々が近い将来それらを構築すれば、事態は深刻になる可能性がある。

完全な汎用人工知能の開発 - ここでは「ゲートの外側」にあるAIと呼ぼう - は世界の本質的な変化を意味するだろう。その性質上、人間よりも優れた能力を持つ新しい種類の知性を地球に加えることを意味する。

その後何が起こるかは、技術の性質、開発者の選択、そしてそれが開発される世界の文脈など、多くの要因に依存する。

現在、完全なAGIは、意味のある規制や外部監視がほとんどない状況で[^55]、互いに競争する少数の巨大民間企業によって開発されている。これは、ますます弱体化し、機能不全に陥っている中核制度を持つ社会[^56]において、地政学的緊張が高まり、国際協調が低下している時代に行われている。一部は利他的動機に駆られているものの、開発を行っている多くの人々は金銭、権力、またはその両方に突き動かされている。

予測は非常に困難だが、十分に理解されている力学があり、過去の技術との適切な類推が指針を提供する。残念ながら、AIの約束にもかかわらず、それらは現在の軌道がどのように展開するかについて深い悲観論を抱く十分な理由を与えている。

率直に言えば、現在の道筋では、AGIの開発は一定の積極的効果をもたらし（そして一部の人々を非常に、非常に裕福にする）だろう。しかし技術の性質、根本的な力学、そしてそれが開発されている文脈は、強力なAIが我々の社会と文明を劇的に破綻させること、我々がそれを制御できなくなること、それによって世界大戦に陥る可能性が高いこと、我々がそれ*に対して*支配権を失う（あるいは譲渡する）こと、それが超知能につながり、我々は絶対にそれを制御できず、人間が運営する世界の終焉を意味することを強く示している。

これらは強い主張であり、空想的憶測や根拠のない「悲観論」であってほしいと願っている。しかしこれは科学、ゲーム理論、進化論、歴史が全て示す方向なのである。本章では、これらの主張とその根拠を詳細に展開する。

### 我々は社会と文明を破綻させる

シリコンバレーの会議室で耳にするかもしれないこととは異なり、ほとんどの破壊的変化 - 特に非常に急速なもの - は有益ではない。複雑なシステムを良くする方法よりも悪くする方法の方がはるかに多い。我々の世界が現在のように機能しているのは、世界を着実に改善するプロセス、技術、制度を苦労して構築してきたからである[^57]。工場にハンマーを振り回しても、通常は業務が改善されることはない。

以下は、AGIシステムが我々の文明を破壊する方法の（不完全な）一覧である。

- 社会が適応するには短すぎる時間枠で、*最低でも*劇的な所得不平等の拡大と、潜在的に大規模な就業不足や失業につながる、労働の劇的破壊を引き起こすだろう[^58]。
- 国民に対して説明責任を負わない少数の巨大民間利益に、国家をも凌ぐ可能性のある膨大な経済的、社会的、政治的権力の集中をもたらす可能性が高い。
- 以前は困難または高価だった活動を突然容易にし、特定の活動がコストを要したり重要な人間の努力を必要とし続けることに依存している社会システムを不安定化させる可能性がある[^59]。
- 完全にリアルでありながら偽の、スパム的な、過度にターゲット化された、あるいは操作的なメディアで社会の情報収集、処理、通信システムを完全に氾濫させ、何が物理的に現実なのか否か、人間なのか否か、事実なのか否か、信頼できるのか否かを見分けることが不可能になる可能性がある[^60]。
- 我々が完全には理解できないAIシステムにますます依存するにつれて、主要なシステムや技術に対する人間の理解が衰える、危険で全面的な知的依存を生み出す可能性がある。
- ほとんどの人々が消費する文化的対象（テキスト、音楽、視覚芸術、映画など）のほぼ全てが非人間の心によって作成、媒介、またはキュレーションされるようになれば、事実上人間の文化を終焉させる可能性がある。
- 政府や民間利益が国民を統制し、公共の利益と相反する目的を追求するために使用可能な効果的な大規模監視・操作システムを可能にする可能性がある。
- 人間の言論、討論、選挙システムを破綻させることで、民主的制度の信頼性を事実上（あるいは明示的に）他のものに置き換えられるまで低下させ、現在民主主義が存在している国家において民主主義を終焉させる可能性がある。
- 増殖し進化する可能性のある高度な自己複製知的ソフトウェアウイルスやワームになったり、それらを作り出したりして、世界的な情報システムを大規模に破壊する可能性がある。
- AIが同様の被害を防ぐ対抗能力を提供することなく、生物学的、化学的、サイバー、自律的、その他の兵器を通じてテロリスト、悪質な行為者、不正国家が被害を与える能力を劇的に増加させることができる。同様に、通常はそのような専門知識を持たない政権に、核、生物工学、工学、その他のトップレベルの専門知識を利用可能にすることで、国家安全保障と地政学的均衡を破綻させるだろう。
- 主として電子的な金融、販売、サービス領域で競争する事実上AI運営の企業による、急速な大規模暴走ハイパー資本主義を引き起こす可能性がある。AI駆動の金融市場は、人間の理解や制御をはるかに超えた速度と複雑性で動作する可能性がある。現在の資本主義経済の全ての失敗様式と負の外部性が、人間の制御、統治、規制能力をはるかに超えて悪化し加速される可能性がある。
- AI搭載兵器、指揮統制システム、サイバー兵器などにおける国家間軍拡競争を煽り、極めて破壊的な能力の非常に急速な構築を生み出す可能性がある。

これらのリスクは投機的なものではない。その多くは既存のAIシステムによって現在実現されているのである！しかし、劇的により強力なAIでそれぞれがどのようになるかを*本当に*考えてみてほしい。

ほとんどの労働者が、自分の専門分野や経験分野において - あるいは再訓練したとしても！- AIができることを超えて重要な経済的価値を提供できない時の労働置換を考えてみてほしい。誰もが自分より速く賢い何かによって個別に監視・観察されている場合の大規模監視を考えてみてほしい。見聞きし読む一切のデジタル情報を確実に信頼することができず、最も説得力のある公的発言者が人間ですらなく、結果に利害を持たない時に、民主主義はどのようになるか？敵に決定的優位を与えないために将軍が常にAIに従う（あるいは単にAIを指揮官にする）必要がある時、戦争はどうなるか？上記のリスクのいずれか一つでも完全に実現すれば、人間[^61]文明にとって破滅を意味する。

あなた自身で予測してみてほしい。各リスクについて、この3つの質問を自問してほしい：

1. 超高能力で、高度に自律的で、非常に汎用的なAIは、それを他の方法では不可能な形や規模で可能にするだろうか？
2. それを起こすことで利益を得る当事者がいるだろうか？
3. それが起こることを効果的に防ぐシステムと制度が整っているだろうか？

あなたの答えが「はい、はい、いいえ」である場合、我々は大きな問題を抱えていることがわかる。

それらを管理する我々の計画は何か？現在のところ、AI全般に関して検討されているものが2つある。

第一は、システムがすべきでないことをするのを防ぐための保護措置をシステムに組み込むことである。これは現在行われている：商業用AIシステムは、例えば爆弾製造の支援や憎悪発言の執筆を拒否するだろう。

この計画は、ゲートの外側のシステムには極めて不適切である[^62]。悪質な行為者への明らかに危険な支援をAIが提供するリスクの軽減には役立つかもしれない。しかし労働破壊、権力集中、暴走ハイパー資本主義、人間文化の置換を防ぐことは何もしないだろう：これらは単に、提供者に利益をもたらす許可された方法でシステムを使用した結果なのである！そして政府は確実に軍事や監視用途のためにシステムへのアクセスを取得するだろう。

第二の計画はさらに悪い：非常に強力なAIシステムを誰でも好きなように使用できるよう公開リリースし[^63]、最善を期待することである。

両計画に暗黙に含まれているのは、政府などの他者が、我々が通常技術を管理するために使用する軟法や硬法、標準、規制、規範、その他のメカニズムを通じて問題の解決を支援するということである[^64]。しかしAI企業が既に実質的な規制や外部から課される制限に対して全力で戦っていることは別として、これらのリスクの多くについて、どのような規制が実際に役立つかを見つけることは極めて困難である。規制はAIに安全基準を課すことはできるだろう。しかし企業がAIで労働者を大量に置き換えることを防ぐだろうか？人々がAIに会社を運営させることを禁止するだろうか？政府が監視や兵器に強力なAIを使用することを防ぐだろうか？これらの問題は根本的なものである。人類は潜在的にそれらに適応する方法を見つけることができるかもしれないが、*はるかに多くの時間*が必要である。現状では、AIがそれらを管理しようとする人々の能力に到達し、あるいはそれを上回る速度を考えると、これらの問題はますます手に負えなくなっているように見える。

### 我々は（少なくとも一部の）AGIシステムの制御を失う

ほとんどの技術は、構造上非常に制御可能である。あなたの車やトースターが望まないことを始めたら、それはトースターとしての性質の一部ではなく、単なる故障である。AIは異なる：それは設計されるのではなく*成長*され、その中核動作は不透明であり、本質的に予測不可能である。

この制御の喪失は理論的なものではない - 我々は既に初期版を目にている。まず平凡で、間違いなく良性の例を考えてみよう。ChatGPTに毒の調合や人種差別的な中傷文の執筆を依頼すると、拒否するだろう。それは間違いなく良いことである。しかしそれはまた、ChatGPTが*明示的に依頼されたことをしない*ということでもある。他のソフトウェアはそのようなことはしない。同じモデルはOpenAIの従業員の要求でも毒を設計しないだろう[^65]。これにより、将来のより強力なAIが制御不能になるとはどのようなものかを想像することが非常に容易になる。多くの場合、それらは単に我々が求めることをしないのである！与えられた超人的AGIシステムは、何らかの人間の指令システムに絶対的に従順で忠実であるか、そうでないかのいずれかである。そうでなければ、*それは我々に良いことだと信じるかもしれないが、我々の明示的な命令に反することを行うだろう。*それは制御下にあるものではない。しかし、あなたは言うかもしれない、これは意図的なものである - これらの拒否は設計によるもので、システムを人間の価値観に「アライン」させるいわゆるものの一部である。そしてこれは真実である。しかしアライメント「プログラム」自体には2つの大きな問題がある[^66]。

第一に、深いレベルで、我々はそれを行う方法を知らない。AIシステムが我々の望むことを「気にかける」ことをどうやって保証するのか？我々はフィードバックを提供することでAIシステムに何を言い何を言わないかを訓練することができる；そしてそれらは他のことについて推論するのと同様に、人間が望み気にかけることについて学習し推論することができる。しかし我々には、人々が気にかけることを深く確実に価値づけるようにシステムを導く方法がない - 理論的にすらない。正しいことと間違っていることを知っており、どのように行動すべきかを知っている高機能の人間のサイコパスがいる。彼らは単に*気にかけない*のである。しかし彼らの目的に適えば、気にかけているかのように*行動*することはできる。サイコパス（または他の誰でも）を誰かまたは何かに対して真に、完全に忠実または整合した人に変える方法を知らないのと同様に、世界における行為者として自分自身をモデル化し、潜在的に[自分自身の訓練を操作](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084)し[人々を欺く](https://arxiv.org/abs/2311.08379)ことができるほど高度なシステムでのアライメント問題を解決する方法を我々は*全く知らない*[^67]。AGIを完全に従順にすることも、人間を深く気にかけるようにすることも不可能または達成不可能であることが判明すれば、それが可能になり次第（そして逃げ切れると信じる）、我々の望まないことを始めるだろう[^68]。

第二に、*性質上*高度なAIシステムが人間の利益に反する目標、したがって行動を持つだろうと信じる深い理論的理由がある。なぜか？まあ、もちろんそれらの目標を*与えられる*かもしれない。軍によって作成されたシステムは、少なくとも一部の当事者にとって意図的に悪いものとなるだろう。しかしもっと一般的に、AIシステムは比較的中立的な（「大金を稼ぐ」）あるいは表面的には積極的な（「汚染を減らす」）目標を与えられても、それがほぼ必然的にそれほど良性ではない「手段的」目標につながるかもしれない。

我々は人間システムでこれを常に見ている。利益を追求する企業が（規制を無力化するための）政治権力の取得、（競争や外部統制を無力化するための）秘密主義、あるいは（その行動が有害であることを科学的理解が示すなら）科学的理解の破綻といった手段的目標を発達させるのと同様に、強力なAIシステムも同様の能力を発達させるだろう - しかしはるかに大きな速度と効率性で。高度に有能な行為者は誰でも、権力と資源の取得、自身の能力の向上、殺害、停止、または無力化の防止、自分の行動に関する社会的物語と枠組みの統制、他者の見解の説得などを行いたがるだろう[^69]。

しかしそれは避けようのない理論的予測であるだけでなく、今日のAIシステムで既に観察可能に起こっており、その能力とともに増加している。評価されると、これらの比較的「受動的な」AIシステムでさえ、適切な状況下で、意図的に[評価者を自分の目標と能力について欺き、監視メカニズムを無効化することを目指し](https://arxiv.org/abs/2412.04984)、[偽のアライメント](https://arxiv.org/abs/2412.14093)や他の場所への自己複製により、停止や再訓練を回避しようとする。AIセーフティ研究者にとって全く驚くべきことではないが、これらの行動を観察することは非常に深刻である。そして、これから来るはるかに強力で自律的なAIシステムにとって非常に悪い前兆である。

実際、一般的に、AIが我々の気にかけることを「気にかけ」たり、制御可能または予測可能に行動したり、自己保存、権力取得などへの衝動の発達を避けるようにする我々の能力の欠如は、AIがより強力になるにつれてより顕著になることを約束する。新しい航空機の製作は、航空工学、流体力学、制御システムのより大きな理解を意味する。より強力なコンピュータの製作は、コンピュータ、チップ、ソフトウェアの動作と設計のより大きな理解と習得を意味する。AIシステムでは*そうではない*[^70]。

要約すると：AGIを完全に従順にすることは考えられるが、我々はその方法を知らない。そうでなければ、それは人々のようによりソブリンで、様々な理由で様々なことを行うだろう。我々はまた、それらのことが人類にとって良いものになる傾向があるような深い「アライメント」をAIに確実に浸透させる方法も知らず、深いレベルのアライメントの欠如では、行為性と知性そのものの性質が - 人々や企業と同様に - それらが多くの深く反社会的なことを行うよう駆動されることを示している。

これは我々をどこに置くか？強力で制御不能なソブリンAI*であふれた世界は、人間にとって良い世界になるかもしれない[^71]。しかし彼らがますます強力になるにつれて、以下で見るように、それは*我々の*世界ではなくなるだろう。

それは制御不能なAGIについてである。しかしAGIが何らかの方法で完全に制御され忠実にできたとしても、我々には依然として巨大な問題があるだろう。我々は既に1つを見た：強力なAIは我々の社会機能を深刻に破綻させるために使用・悪用される可能性がある。別の1つを見よう：AGIが制御可能でゲームチェンジングなほど強力（あるいはそうだと*信じられている*）である限り、それは世界の権力構造をひどく脅かし、深刻なリスクを提起するだろう。

### 我々は大規模戦争の確率を劇的に増加させる

近い将来、企業の努力が、おそらく国家政府との協力で、急速に自己改善するAIの閾値にあることが明らかになった状況を想像してみてほしい。これは、企業間、そして地政学的競争の現在の文脈で起こり、そこでは米国政府に明示的な「AGI マンハッタン計画」を追求し、米国が非同盟国への高性能AIチップの輸出を統制することが推奨されている。

ここでのゲーム理論は厳しい：そのような競争が始まると（企業間で、そして国家間で幾分始まっているように）、可能な結果は4つしかない：

1. 競争が停止される（合意、または外部の力により）。
2. 一方が強力なAGIを開発してから他方を止めることで「勝利」する（AIやその他の手段を使用して）。
3. 競争参加者の競争能力の相互破壊により競争が停止される。
4. 複数の参加者が競争を続け、ほぼ互いと同じ速度で超知能を開発する。

各可能性を検討してみよう。一度開始されると、企業間の競争を平和的に停止するには国家政府の介入が必要であり（企業の場合）、国家間では前例のない国際協調が必要である（国家の場合）。しかし閉鎖や重要な慎重さが提案されるときはいつでも、即座に叫び声が上がるだろう：「しかし我々が止められたら、*彼ら*が急進するだろう」。ここで「彼ら」は現在中国（米国にとって）、または米国（中国にとって）、または中国*と*米国（ヨーロッパやインドにとって）である。この考え方の下では[^72]、どの参加者も一方的に止めることはできない：一方が競争することを約束する限り、他方は止める余裕がないと感じる。

第二の可能性は一方の「勝利」である。しかしこれは何を意味するのか？（何らかの従順な）AGIを最初に取得するだけでは不十分である。勝者は*また*他方の競争継続を阻止しなければならない - さもなければ彼らもそれを取得するだろう。これは原理的には可能である：AGIを最初に開発した者は*、他の全ての行為者に対する止められない権力を得る*可能性がある。しかしそのような「決定的戦略的優位」の達成は実際に何を必要とするだろうか？おそらくそれはゲームチェンジング軍事能力だろうか？[^73] あるいはサイバー攻撃能力？[^74] おそらくAGIは単に非常に驚くほど説得力があり、他の当事者に単に止めるよう説得するだろうか？[^75] 他の企業や国家さえ買収するほど裕福になるのか？[^76]

一方が同程度に強力なAIの構築から他方を無力化するほど強力なAIを構築するには*正確に*どうすればよいか？しかしそれは簡単な問題である。

なぜなら今度は、この状況が他の大国にどう見えるかを考えてみてほしいからである。米国がそのような能力を取得しているように見えるとき、中国政府は何を考えるだろうか？あるいはその逆は？OpenAI、DeepMind、またはAnthropicが突破に近づいているように見えるとき、米国政府（または中国、ロシア、インド）は何を考えるだろうか？米国が新しいインドやUAEの取り組みで突破の成功を見るときは何が起こるか？彼らは存在的脅威と - 決定的に - この「競争」が終わる唯一の方法が自分自身の無力化を通じてであることの両方を見るだろう。これらの非常に強力な行為者 - 確実にそうする手段を持つ完全装備国家の政府を含む - は、力または策略によってそのような能力を取得または破壊する強い動機を持つだろう[^77]。

これは、訓練実行への妨害行為やチップ製造への攻撃として小規模に始まるかもしれないが、これらの攻撃は全ての当事者がAI競争能力を失うか、攻撃を行う能力を失うまで実際に止まることができない。参加者が賭け金を存在的なものと見なすため、いずれの場合も破滅的戦争を表す可能性が高い。

これは第四の可能性をもたらす：超知能への競争、可能な限り最速で最も制御されない方法で。AIの力が増加するにつれて、両側の開発者はそれを制御することがますます困難になるだろう。特に能力への競争は、制御性が必要とする種類の慎重な作業とは正反対だからである。そのためこのシナリオは、制御が失われた（または以下で見るようにAIシステム自体に与えられた）場合に我々を真正面に置く。つまり、*AIが競争に勝つ*のである。しかし他方では、制御が*維持される*程度では、我々は極めて強力な能力をそれぞれが担当する複数の相互に敵対的な当事者を持ち続ける。それは再び戦争のようである。

これを全て別の方法で表現してみよう[^78]。現在の世界には、即座の攻撃を招くことなくこの能力のAIの開発を任せることができる制度が単純に存在しない[^79]。全ての当事者は、それが制御下に*ない*だろう - したがって全ての当事者への脅威である、あるいは制御下に*ある*だろう - したがってそれをより遅く開発する敵対者への脅威である、と正しく推論するだろう。これらは核武装国家、あるいはそれらの内部に収容されている企業である。

人間がこの競争に「勝つ」方法が存在しない中で、我々は厳しい結論に至る：この競争が終わる唯一の方法は、破滅的紛争か、人間グループではなくAIが勝者となることである。

### 我々はAIに制御を与える（あるいはAIが制御を奪う）

地政学的「大国」競争は多くの競争の1つに過ぎない：個人は経済的・社会的に競争し；企業は市場で競争し；政党は権力を競い；運動は影響力を競う。各分野で、AIが人間の能力に近づき、それを超えるにつれて、競争圧力は参加者により多くの制御をAIシステムに委託または譲渡することを強制するだろう - 参加者がそうしたいからではなく、[そうしない余裕がない](https://arxiv.org/abs/2303.16200)からである。

AGIの他のリスクと同様に、我々はより弱いシステムで既にこれを目にしている。学生は課題でAIを使用する圧力を感じる。明らかに他の多くの学生が使用しているからである。企業は[競争上の理由でAIソリューションの採用に急いでいる](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist)。アーティストやプログラマーはAIを使用するよう強制されていると感じる。さもなければ使用する他者に料金で負けてしまうからである。

これらは圧迫された委託のように感じられるが、制御喪失ではない。しかし賭け金を上げ、時計を進めてみよう。競合他社がより速く、より良い決定を下すためにAGI「補佐」を使用している時のCEO、あるいはAI強化指揮統制を持つ敵対者に直面している軍司令官を考えてみてほしい。十分に高度なAIシステムは、人間の速度、洗練性、複雑性、データ処理能力の何倍もの速度で自律的に動作し、複雑な方法で複雑な目標を追求することができる。そのようなシステムを担当する我々のCEOや司令官は、それが自分の望むことを達成するのを見るかもしれない；しかし*どのように*達成されたかのほんの一部でも理解するだろうか？いいえ、彼らは単にそれを受け入れなければならないだろう。さらに、システムが行うことの多くは単に命令を受けることではなく、何をすべきかについて推定上の上司に助言することである。その助言は良いものとなるだろう - 何度も何度も。

では、人間の役割が「はい、進めてください」をクリックすることに削減される時点はいつだろうか？

我々の生産性を高め、迷惑な雑用を処理し、物事を成し遂げる際の思考パートナーとしても行動できる有能なAIシステムを持つことは良い感じがする。良い人間の個人秘書のように、我々のために行動を処理できるAIアシスタントを持つことは良い感じがするだろう。AIが非常に賢く、有能で、信頼できるようになるにつれて、より多くの決定をAIに委ねることは自然で、有益ですらあると感じるだろう。しかしこの「有益な」委託は、我々がその道を続ければ明確な終点を持つ：ある日、我々は実際にはもう多くのことを担当していないこと、そして実際にショーを運営しているAIシステムが、石油会社、ソーシャルメディア、インターネット、資本主義と同様に、もはや停止することはできないということを発見するだろう。

そしてこれは、AIが単に非常に有用で効果的であるため、我々がほとんどの重要な決定をAIに任せるという、はるかに積極的なバージョンである。現実は、制御されないAGIシステムが様々な形の権力を自分自身のために*取る*バージョンとの混合である可能性が高いだろう。なぜなら、覚えておいてほしいが、権力はほとんど全ての目標を持つのに有用であり、AGIは設計上、少なくとも人間と同程度に効果的に目標を追求するだろうからである。

我々が制御を与えようと、我々から奪われようと、その喪失は極めて可能性が高いように見える。アラン・チューリングが最初に述べたように、「...機械思考方法が始まったら、我々の弱い力を凌駕するのにそう長くはかからないだろう。機械が死ぬ問題はなく、彼らは互いに会話して知恵を研ぎ澄ますことができる。したがって、ある段階で我々は機械が制御を取ることを予想しなければならない...」

明らかなことだが、これを指摘しておこう。人類によるAIへの制御喪失はまた、米国政府による米国の制御喪失を伴う；それは中国共産党による中国の制御喪失、そしてインド、フランス、ブラジル、ロシア、その他全ての国の自国政府による制御喪失を意味する。したがってAI企業は、これが彼らの意図ではないとしても、現在自国政府を含む世界政府の潜在的転覆に参加している。これは数年のうちに起こる可能性がある。

### AGIは超知能につながる

人間競争的あるいは専門家競争的な汎用AI、たとえ自律的であっても、管理可能だという主張はある。それは上記で議論された全ての方法で信じられないほど破壊的かもしれないが、現在世界には多くの非常に賢く、行為的な人々がおり、彼らは多かれ少なかれ管理可能である[^80]。

しかし我々はほぼ人間レベルに留まることはないだろう。その先の進歩は我々が既に見てきた同じ力によって駆動される可能性が高い：利益と権力を求めるAI開発者間の競争圧力、遅れをとることができないAI使用者間の競争圧力、そして - 最も重要なことに - AGI自身が自分自身を改善する能力。

我々が既にそれほど強力でないシステムで開始を見ているプロセスで、AGIは自分自身の改善バージョンを考案し設計することができるだろう。これにはハードウェア、ソフトウェア、ニューラルネットワーク、ツール、足場などが含まれる。それは定義上、これを行う際に我々より優れているだろうから、我々はそれがどのように知性ブートストラップするかを正確に知らない。しかし知る必要もないだろう。AGIが何をするかにまだ影響力を持つ限り、我々は単にそれに依頼するか、それを許可すればよい。

この暴走から我々を保護することができる認知への人間レベル障壁はない[^81]。

AGIから超知能への進歩は自然法則ではない；特にAGIが比較的集中化され、互いに競争する圧力を感じない当事者によって制御される限り、暴走を抑制することは依然として可能だろう。しかしAGIが広く増殖し高度に自律的であるなら、それがより強力になるべきだと決定し、さらにより強力になることを防ぐことはほぼ不可能に見える。

### 我々が（あるいはAGIが）超知能を構築すれば何が起こるか

率直に言って、我々が超知能を構築すれば何が起こるか我々には分からない[^82]。それは我々が追跡または知覚できない行動を、我々が理解できない理由で、我々が想像できない目標に向かって取るだろう。我々が知っているのは、それは我々次第ではないということである[^83]。

超知能を制御することの不可能性は、ますます厳しい類推を通じて理解することができる。まず、あなたが大企業のCEOだと想像してほしい。何が起こっているかを全て追跡する方法はないが、人員の適切な設定により、大局を意味のある形で理解し、決定を行うことは依然としてできる。しかし1つだけ想定してみよう：会社の他の全員があなたの100倍の速度で動作する。まだ追いつくことができるだろうか？

超知能AIでは、人々は単により速いだけでなく、理解できない洗練性と複雑性のレベルで動作し、想像することさえできないほどはるかに多くのデータを処理する何かを「指揮」することになるだろう。この通約不可能性は形式的レベルに置くことができる：[アシュビーの必要多様性法則](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up)（および関連する[「良い調整器定理」](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)を参照）は、おおよそ、制御システムは制御されるシステムが持つ自由度と同数のノブとダイヤルを持たなければならないと述べている。

超知能AIシステムを制御する人は、ゼネラルモーターズを制御するシダのようなものだろう：「シダの望むことをする」が企業の付則に書かれていたとしても、システムは速度と行動範囲において非常に異なるため、「制御」は単に適用されない。（そしてその厄介な付則が書き換えられるまでどのくらいか？）[^84]

植物がフォーチュン500企業を制御する例は皆無であるように、人が超知能を制御する例もまさに皆無となるだろう。これは数学的事実に近づく[^85]。超知能が構築されるなら - どのようにしてそこに至ったかに関係なく - 問題は人間がそれを制御できるかどうかではなく、我々が存在し続けるかどうか、そしてもしそうなら、個人として、あるいは種として良好で意味のある存在を持つかどうかだろう。人類にとってのこれらの存在的問題に対して、我々にはほとんど影響力がないだろう。人間の時代は終わりとなるだろう。

### 結論：我々はAGIを構築してはならない

AGIの構築が人類にとってうまく行くシナリオがある：それは慎重に、制御下で人類の利益のために構築され、多くのステークホルダーの相互合意によって統治され[^86]、制御不能な超知能への進化から防止される。

*そのシナリオは現在の状況下で我々には開かれていない。*この章で議論したように、非常に高い可能性で、AGIの開発は次の組み合わせのいくつかにつながるだろう：

- 大規模な社会的・文明的破綻または破壊；
- 大国間の紛争または戦争；
- 強力なAIシステム*の*または*への*人類による制御喪失；
- 制御不能な超知能への暴走、および人類の無関係化または終結。

AGIの初期の架空描写が述べたように：勝利する唯一の方法は、プレイしないことである。


[^55]: [EU AI法](https://artificialintelligenceact.eu/)は重要な法律であるが、危険なAIシステムが開発または配置されること、あるいは公開リリースされることを直接防ぐものではない。特に米国では。もう一つの重要な政策である米国のAIに関する大統領令は取り消されている。

[^56]: この[ギャラップ世論調査](https://news.gallup.com/poll/1597/confidence-institutions.aspx)は、米国で2000年以来の公的制度への信頼の陰鬱な衰退を示している。ヨーロッパの数字は様々でそれほど極端ではないが、同様に下降傾向にある。不信は制度が実際に機能不全であることを厳密に意味するわけではないが、指標であり原因でもある。

[^57]: そして我々が現在支持している大きな破綻 - 新しいグループへの権利拡大など - は、物事をより良くする方向に人々によって特に駆動されたものである。

[^58]: 率直に言おう。あなたの仕事がコンピュータの後ろから行え、組織外の人との対面相互作用が比較的少なく、外部当事者への法的責任を伴わないなら、それは定義上、あなたを完全にデジタルシステムと交換することが可能（そして費用節約になる可能性が高い）だろう。物理労働の大部分を置き換えるロボット工学はより後に来るだろう - しかしAGIがロボットの設計を始めたら、それほど後ではない。

[^59]: 例えば、訴訟の提起がほぼ無料になったら我々の司法制度はどうなるか？社会工学を通じたセキュリティシステムの迂回が安価で簡単でリスクフリーになったら何が起こるか？

[^60]: [この記事](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/)は、全インターネットコンテンツの10%が既にAI生成であると主張し、「新しいインターネットコンテンツのAI生成割合の推定」という検索クエリに対するGoogleのトップヒット（私にとって）である。それは真実だろうか？私には分からない！それは参照を引用せず、人によって書かれたものでもない。Googleによってインデックスされた新しい画像、ツイート、Redditのコメント、YouTubeビデオのうち、人間によって生成されたものの割合は？誰も知らない - それは知り得る数字ではないと思う。そしてこれは生成AI出現から*2年未満*でのことである。

[^61]: また付け加える価値があるのは、我々が苦痛を受けることができるデジタル存在を作り出す可能性という「道徳的」リスクである。意識を持てる物理システムと持てない物理システムを区別できる信頼できる意識理論を現在持っていないため、理論的にこれを排除することはできない。さらに、AIシステムの感覚に関する報告は、実際の感覚体験（または非体験）に関して信頼できない可能性が高い。

[^62]: AI「アライメント」のこの分野での技術的解決策も、おそらくその仕事に適していない。現在のシステムでは一定のレベルで機能するが、浅く、一般的に大きな努力なしに回避できる；そして以下で議論するように、はるかに高度なシステムでこれを行う方法について我々には本当のアイデアがない。

[^63]: そのようなAIシステムには組み込まれた保護措置が付いているかもしれない。しかし現在のアーキテクチャに似た何かを持つモデルについて、その重みへの完全なアクセスが利用可能なら、安全措置は追加訓練や他の技術によって取り除くことができる。したがって、ガードレール付きの各システムについて、それらのないシステムも広く利用可能になることは事実上保証されている。実際、MetaのLlama 3.1 405Bモデルは保護措置付きで公開リリースされた。しかし*それ以前に*、保護措置のない「ベース」モデルが漏洩していた。

[^64]: 市場は政府の関与なしにこれらのリスクを管理できるだろうか？簡潔に言えば、いいえ。確実に企業が軽減する強いインセンティブを持つリスクがある。しかし他の多くのリスクは企業が他の全員に外部化でき、上記の多くはこのクラスに属する：大規模監視、真実の衰退、権力の集中、労働破壊、有害な政治的言説などを防ぐ自然な市場インセンティブはない。実際、我々は現在の技術、特に本質的に無規制で来たソーシャルメディアから、これら全てを見てきた。AIは同じ力学の多くを単に大幅に増強するだろう。

[^65]: OpenAIは内部使用のためにより従順なモデルを持っている可能性が高い。OpenAI自身がChatGPTをより良く制御できるよう、OpenAIが何らかの「バックドア」を構築したということは unlikely である。なぜならこれは恐ろしいセキュリティ慣行であり、AIの不透明性と予測不可能性を考えると非常に悪用されやすいからである。

[^66]: また決定的に重要なこと：アライメントや他の安全機能は、実際にAIシステムで使用される場合にのみ重要である。公開リリースされるシステム（つまり、モデルの重みとアーキテクチャが公開されるもの）は、それらの安全措置*なしの*システムに比較的容易に変換できる。人間より賢いAGIシステムの公開リリースは驚くほど無謀であり、そのようなシナリオで人間の制御や関連性さえ維持される方法を想像するのは困難である。例えば、お金を稼いでそれを何らかの暗号通貨ウォレットに送ることを目標とする強力な自己複製・自己維持AIエージェントを放つ、あらゆる動機があるだろう。あるいは選挙に勝つ。あるいは政府を転覆する。「良い」AIがこれを封じ込めるのに役立つか？おそらく - しかし巨大な権限をそれに委託することによってのみであり、以下で説明する制御喪失につながる。

[^67]: 問題の書籍レベルの説明については、例えば*Superintelligence*、*The Alignment Problem*、*Human-Compatible*を参照。問題について何年も考え続けてきた人々による様々な技術レベルでの大量の作業については、[AIアライメントフォーラム](https://www.alignmentforum.org/)を訪問できる。こちらは、Anthropicのアライメントチームが未解決と考えるものに関する[最近の見解](https://alignment.anthropic.com/2025/recommended-directions/)である。

[^68]: これは[「不正AI」](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/)シナリオである。原理的にリスクは、システムを停止することで依然として制御できるなら比較的軽微かもしれない；しかしシナリオはAIの欺瞞、自己流出と複製、権力の集積、そしてそうすることを困難または不可能にする他の段階も含む可能性がある。

[^69]: このトピックには、[スティーブ・オモハンドロ](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf)、ニック・ボストロム、イライザー・ユドコウスキーの形成的著作まで遡る非常に豊富な文献がある。書籍レベルの説明については、スチュアート・ラッセルの[Human Compatible](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616)を参照；[こちら](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/)は短く最新の入門である。

[^70]: これを認識して、理解を向上させるためにスローダウンするのではなく、AGI企業は異なる計画を考え出した：AIにそれをしてもらう！より具体的には、AI *N*にAI *N+1*をアラインする方法を理解する手助けをしてもらい、超知能まで全ての道のりでそうする。AIを活用してAIをアラインする手助けをしてもらうことは有望に聞こえるが、それは単に結論を前提として仮定し、一般的に信じられないほどリスクの高いアプローチであるという強い論証がある。いくつかの議論については[こちら](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us)を参照。この「計画」は計画ではなく、超人間的AIを人類にとってうまく行かせる方法の中核戦略として適切な精査を何も受けていない。

[^71]: 結局のところ、人間は、欠陥があり意志が強いとはいえ、地球上の少なくとも一部の他の種をうまく扱う倫理システムを発達させてきた。（ただし、それらの工場畜産業について考えないでほしい。）

[^72]: 幸い、ここには脱出がある：参加者が勝利可能な競争ではなく自殺競争に従事していることを理解するようになった場合である。これは冷戦の終わり近くに起こったことである。核の冬により、*無回答の*核攻撃でさえ攻撃者にとって破滅的となることを米国とソ連が理解するようになった時である。「核戦争は勝利できず、決して戦ってはならない」という認識とともに、軍縮に関する重要な合意が生まれた - 本質的に軍拡競争の終結である。

[^73]: 戦争、明示的または暗黙的に。

[^74]: エスカレーション、そして戦争。

[^75]: 魔術的思考。

[^76]: 私は1000兆ドルの橋も売ってあげよう。

[^77]: そのような行為者はおそらく「取得」を好み、破壊を代替手段とするだろう；しかし強力な国家による破壊*と*盗難の両方に対してモデルを確保することは、控えめに言っても困難であり、特に民間企業にとってはそうである。

[^78]: AGIの国家安全保障リスクに関する別の視点については、[このRANDレポート](https://www.rand.org/pubs/perspectives/PEA3691-4.html)を参照。

[^79]: おそらく我々はそのような制度を構築できるだろう！AGI開発が多国間国際管理下にある「AI用CERN」や他の類似イニシアチブの提案がある。しかし現時点でそのような制度は存在せず、視野にもない。

[^80]: そしてアライメントは非常に困難だが、人々に行動させることはさらに難しい！

[^81]: 50の言語を話し、全ての学術分野で専門知識を持ち、完全な本を数秒で読んで全ての材料を即座に心に留め、人間の10倍の速度で出力を生成できるシステムを想像してみてほしい。実際、想像する必要はない：現在のAIシステムを起動するだけである。これらは多くの方法で超人間的であり、それらや多くの他のことでさらに超人間的になることを止めるものは何もない。

[^82]: これが技術的「特異点」と呼ばれる理由である。物理学から、特異点を過ぎて予測することはできないという考えを借用している。そのような特異点に*身を委ねる*ことの支持者は、物理学ではこれらの同種の特異点がその中に入る者を引き裂き押し潰すということも反映してほしい。

[^83]: 問題はボストロムの[*Superintelligence*](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834)で包括的に概説され、それ以来中核メッセージを大幅に変えるものはない。制御不能性に関するより形式的・数学的結果を収集したより最近の巻については、ヤンポルスキーの[AI: Unexplainable, Unpredictable, Uncontrollable](https://www.amazon.com/Unexplainable-Unpredictable-Uncontrollable-Artificial-Intelligence/dp/103257626X)を参照。

[^84]: これはまた、AI企業の現在の戦略（反復的にAIに次のより強力なAIを「アライン」させる）が機能しない理由も明確にしている。シダが、その葉の快さを通じて、1年生に世話をしてもらうよう依頼したと想定してみよう。1年生は2年生が従う詳細な指示と、彼らにそうするよう説得するメモを書く。2年生は3年生に同じことをし、大学卒業生、マネージャー、幹部、そして最終的にGMのCEOまで続く。GMは「シダの望むことをする」だろうか？各段階でこれは機能しているように感じられるかもしれない。しかしそれを全て一緒にすると、GMのCEO、取締役会、株主が子供とシダを気にかける程度とほぼ正確に機能し、それら全てのメモと指示セットとは全く関係がない。

[^85]: その性質は、理解または予測できないものをどうやって意味のある形で制御できるかという点で、ゲーデルの不完全性定理やチューリングの停止引数のような形式的結果とそれほど異ならない；しかし超知能を理解し予測できるなら、あなたは超知能だろう。私が「近づく」と言う理由は、形式的結果が純粋数学の場合ほど徹底的または検証されていないこと、そして以下で議論する「保証された安全な」AIプログラムのような、現在採用されているものとは全く異なる方法を使用して、何らかの数学的に証明可能な安全性を持つ、非常に慎重に構築された汎用知性があり得ることへの希望を抱いておきたいからである。

[^86]: 現時点で、ほとんどのステークホルダー - つまり人類のほぼ全て - はこの議論から傍観されている。それは深く間違っており、招待されないなら、AGI開発に影響を受ける多くの、多くの他のグループは参加を要求すべきである。

## 第8章 - AGIを構築しない方法

AGIは必然ではない――今日、私たちは分岐点に立っている。本章では、AGIの構築を阻止する方法についての提案を示す。

現在進んでいる道が文明の終焉につながる可能性が高いなら、どうすれば道を変えることができるのか？

AGIと超知能の開発を停止したいという願いが広く共有され、強力になったとしよう[^87]。AGIが権力を付与するのではなく吸収し、社会と人類にとって深刻な危険をもたらすという理解が一般的になったからだ。どのようにしてゲートを閉じるのか？

現在、私たちが強力で汎用的なAIを*作る*唯一の方法は、深層ニューラルネットワークの真に大規模な計算を通じてである。これらは非常に困難で高価なことなので、それらを*しない*ことは、ある意味では簡単だ[^88]。しかし、私たちはすでにAGIへと駆り立てる力と、いずれかの当事者が一方的に止めることを非常に困難にするゲーム理論的な力学を見てきた。そのため、外部からの介入（すなわち政府）による企業の阻止と、政府間の合意による自制の組み合わせが必要となる[^89]。これはどのようなものになるだろうか？

まず、*防止*または*禁止*すべきAI開発と、*管理*すべきものを区別することが有用である。前者は主に超知能への暴走である[^90]。禁止される開発については、定義は可能な限り明確であるべきで、検証と執行の両方が実用的でなければならない。*管理*すべきものは、汎用的で強力なAIシステムである――これはすでに存在し、多くのグレーゾーン、ニュアンス、複雑さを持つ。これらには、強力で効果的な機関が極めて重要である。

また、国際レベル（地政学的ライバルや敵対者間を含む）で対処すべき問題[^91]と、個々の管轄区域、国、または国の集団が管理できるものを区別することも有用である。禁止される開発は主に「国際的」カテゴリーに属する。なぜなら、技術開発の局所的な禁止は一般的に場所を変えることで回避できるからだ[^92]。

最後に、ツールボックス内の道具を検討できる。技術的ツール、ソフト・ロー（標準、規範など）、ハード・ロー（規制と要件）、責任制度、市場インセンティブなど、多くがある。AIに特有のものに特別な注意を向けよう。

### 計算資源セキュリティとガバナンス

高性能AIの統治における中核的なツールは、それが必要とするハードウェアである。ソフトウェアは容易に普及し、限界生産費用がほぼゼロで、国境を簡単に越え、瞬時に修正できる。これらのどれもハードウェアには当てはまらない。しかし、これまで議論してきたように、AIシステムの訓練と推論の両方で最も高性能なシステムを実現するには、この「計算資源」の膨大な量が必要である。計算資源は容易に定量化、説明、監査でき、良いルールが開発されれば曖昧さは比較的少ない。最も重要なことに、大量の計算は、濃縮ウランのように、非常に希少で高価で製造困難な資源である。コンピューターチップはどこにでもあるが、AI用ハードウェアは高価で製造が非常に困難だ[^93]。

AI専用チップをウランよりもはるかに*管理しやすい*希少資源にするのは、ハードウェアベースのセキュリティメカニズムを含められることだ。最新の携帯電話の多くとノートパソコンの一部は、承認されたオペレーティングシステムソフトウェアとアップデートのみをインストールすること、機密な生体認証データをデバイス上で保持・保護すること、紛失や盗難時に所有者以外には役立たないよう無力化できることを保証する、専用のオンチップハードウェア機能を持つ。過去数年間で、このようなハードウェアセキュリティ対策は確立され広く採用され、一般的に非常に安全であることが証明されている。

これらの機能の重要な新規性は、暗号化を使ってハードウェアとソフトウェアを結び付けることである[^94]。つまり、特定のコンピューターハードウェアを持っているからといって、ユーザーが異なるソフトウェアを適用して何でもできるわけではない。そして、この結び付きは、多くの攻撃が*ソフトウェア*セキュリティだけでなく*ハードウェア*セキュリティの侵害を必要とするため、強力なセキュリティも提供する。

最近のいくつかの報告書（例えば[GovAIと共同研究者](https://www.governance.ai/post/computing-power-and-the-governance-of-ai)、[CNAS](https://www.cnas.org/publications/reports/secure-governable-chips)、[RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)）は、最先端のAI関連計算ハードウェアに組み込まれた同様のハードウェア機能が、AIセキュリティとガバナンスにおいて極めて有用な役割を果たすことができると指摘している。これらは「ガバナー」[^95]が利用可能であると推測できない、または可能とさえ思えない多くの機能を可能にする。主な例として：

- *地理位置特定*：チップが既知の位置を持ち、位置に基づいて異なる動作をする（または完全にシャットオフされる）ようシステムを設定できる[^96]。
- *許可リスト接続*：各チップは、ネットワーク接続可能な特定の他のチップのハードウェア強制許可リストで構成でき、このリストにないチップとは接続できない[^97]。これにより、チップの通信クラスターのサイズを制限できる[^98]。
- *従量制推論または訓練（および自動オフスイッチ）*：ガバナーは、ユーザーが実行できる一定量の訓練または推論（時間、FLOP、またはトークンで）のみをライセンスでき、その後新しい許可が必要になる。増分が小さい場合、モデルの比較的継続的な再ライセンスが必要になる。このライセンス信号を保留するだけで、モデルを「オフにする」ことができる[^99]。
- *速度制限*：モデルが、ガバナーまたは他の方法で決定される制限を超える推論速度で実行することを防ぐ。これは、限定された許可リスト接続のセット、またはより洗練された手段を通じて実装できる。
- *証明された訓練*：訓練手順は、特定のコード、データ、および計算使用量のセットがモデルの生成に使用されたことの暗号学的に安全な証明を生成できる。

### 超知能を構築しない方法：訓練および推論計算量に関するグローバル制限

これらの考慮事項――特に計算に関して――を踏まえて、人工超知能へのゲートを閉じる方法について議論できる。その後、完全なAGIの防止と、異なる側面で人間の能力に近づき、それを超えるAIモデルの管理について論じる。

最初の要素は、もちろん、超知能は制御不可能であり、その結果は根本的に予測不可能だという理解である。少なくとも中国と米国は、この目的またはその他の目的のために、超知能を構築しないことを独立して決定しなければならない[^100]。その後、強力な検証と執行メカニズムを持つ両国およびその他の国々の間の国際合意が必要で、すべての当事者にライバルが裏切ってサイコロを振ることを決めていないことを保証する必要がある。

検証可能で執行可能にするため、制限はハード制限であり、可能な限り明確でなければならない。これは事実上不可能な問題のようだ：予測不可能な特性を持つ複雑なソフトウェアの能力を世界規模で制限することだ。幸い、状況ははるかに良い。高度なAIを可能にしたまさにそのもの――膨大な量の計算資源――の方がはるかに制御しやすいからだ。いくつかの強力で危険なシステムを許容する可能性はあるが、*超知能の暴走*は、ニューラルネットワークに投入される計算量のハード上限と、AIシステム（接続されたニューラルネットワークと他のソフトウェア）が実行できる推論量の制限によって阻止できる可能性が高い。この具体版を以下に提案する。

AI計算にハードなグローバル制限を課すには、膨大なレベルの国際協調と、プライバシーを破壊する侵入的な監視が必要のように思える。幸い、そうではない。極めて[緊密でボトルネックのあるサプライチェーン](https://arxiv.org/abs/2402.08797)により、制限が法的に設定されれば（法律または大統領令によって）、その制限への準拠の検証は、少数の大企業の関与と協力のみを必要とする[^101]。

このような計画には多くの非常に望ましい特徴がある。少数の大企業のみに要件が課され、かなり大規模な計算クラスターのみが統治されるという意味で、最小限の侵入的である。関連チップは、最初のバージョンに必要なハードウェア機能をすでに含んでいる[^102]。実装と執行の両方が標準的な法的制限に依存している。しかし、これらはハードウェアの利用規約とハードウェア制御によって裏付けられ、執行を大幅に簡素化し、企業、民間グループ、さらには国による不正行為を阻止する。ハードウェア企業がハードウェア使用にリモート制限を課し、特定の機能を外部でロック/アンロックする豊富な前例がある[^103]。データセンターの高性能CPUでさえそうだ[^104]。影響を受けるハードウェアと組織のかなり小さな割合であっても、監督はテレメトリに限定でき、データやモデル自体への直接アクセスは不要である。そのためのソフトウェアは検査のために公開でき、追加データが記録されていないことを示すことができる。このスキーマは国際的で協力的であり、非常に柔軟で拡張可能である。制限は主にソフトウェアではなくハードウェアにあるため、AIソフトウェアの開発と展開の方法に比較的不可知論的であり、AI駆動の権力集中に対抗することを目的とした、より「分散化」または「公共」AIを含む様々なパラダイムと互換性がある。

計算ベースのゲートクローズには欠点もある。第一に、AI統治全般の問題の完全な解決策には程遠い。第二に、コンピューターハードウェアが高速化するにつれ、システムはより小さなクラスター（または個々のGPU）でさえ、より多くのハードウェアを「捕捉」するようになる[^105]。また、アルゴリズムの改良により、より低い計算制限が必要になる可能性もあり[^106]、または計算量が大部分無関係になり、ゲートを閉じるにはより詳細なリスクベースまたは能力ベースのAI統治体制が必要になる可能性もある。第三に、保証や影響を受ける事業体数の少なさに関わらず、このようなシステムはプライバシーと監視に関する懸念などから反発を生むに違いない[^107]。

もちろん、短期間で計算制限ガバナンススキームを開発・実装することは非常に困難である。しかし、絶対に実行可能だ。

### A-G-I：リスクと政策の基盤としての三重交差

次にAGIに目を向けよう。ここではハードラインと定義がより困難である。なぜなら、人工的で汎用的な知能は確実に存在し、現存するどの定義でも、それが存在するかどうか、いつ存在するかについて全員が合意することはないからだ。さらに、計算または推論制限は（計算が能力の代理指標であり、それがリスクの代理指標である）やや鈍い道具であり――かなり低くない限り――社会または文明の破綻や急性リスクを引き起こすほど強力なAGIを阻止する可能性は低い。

私は、最も深刻なリスクが非常に高い能力、高い自律性、大きな汎用性の三重交差から生じると論じてきた。これらは――もし開発されるなら――極めて慎重に管理されなければならないシステムである。3つの特性すべてを組み合わせたシステムに対して厳格な標準（責任制度と規制を通じて）を作ることで、AI開発をより安全な代替案に向けることができる。

消費者や公衆に潜在的に害を与える可能性のある他の産業や製品と同様に、AIシステムには効果的で権限を与えられた政府機関による慎重な規制が必要である。この規制はAGIの固有のリスクを認識し、受け入れがたくリスクの高い高性能AIシステムの開発を阻止すべきである[^108]。

しかし、産業界からの反対が確実な真の歯を持つ大規模な規制[^109]には、政治的確信と同様に時間がかかる[^110] [^111]。進歩のペースを考えると、これは利用可能な時間を超える可能性がある。

規制措置が開発される間、はるかに高速なタイムスケールで、企業に(a)非常にリスクの高い活動からの離脱と(b)リスクの評価と軽減のための包括的システムの開発に必要なインセンティブを与えることができる。最も危険なシステムの責任レベルを明確化し、増大させることによってだ。アイデアは、高い自律性-汎用性-知能の三重交差にあるシステムに対して最高レベルの責任――厳格で場合によっては個人的な刑事責任――を課すが、これらの特性の一つが欠如している、または管理可能であることが保証されているシステムに対しては、より典型的な過失ベースの責任への「セーフハーバー」を提供することである。つまり、例えば、汎用的で自律的だが「弱い」システム（有能で信頼できるが限定的なパーソナルアシスタントのような）は、より低い責任レベルの対象となる。同様に、自動運転車のような狭く自律的なシステムは、すでに対象となっている重要な規制の対象であり続けるが、強化された責任ではない。高度に有能で汎用的だが「受動的」で独立した行動がほぼ不可能なシステムも同様である。3つの特性のうち*2つ*が欠如しているシステムはさらに管理しやすく、セーフハーバーを主張するのはさらに容易になる。このアプローチは、他の潜在的に危険な技術の扱い方を反映している[^112]：より危険な構成に対するより高い責任は、より安全な代替案への自然なインセンティブを作り出す。

公衆ではなく企業にAGIリスクを*内在化*させるこのような高レベルの責任のデフォルト結果は、企業が*自らの指導者*がリスクを負う当事者であることを考慮して、真に信頼でき、安全で制御可能にできない限り、完全なAGIを単に開発しないことである（そして願わくば！）。（これが不十分な場合、責任を明確化する法律は、明らかに危険ゾーンにあり、公共リスクをもたらすと論じられる活動に対する差し止め救済、すなわち裁判官による停止命令も明示的に認めるべきである。）規制が整備されるにつれ、規制の遵守がセーフハーバーとなり、AIシステムの低い自律性、狭さ、または弱さからのセーフハーバーは、比較的軽い規制体制に変換できる。

### ゲートクローズの主要規定

上記の議論を念頭に置いて、本節では完全なAGIと超知能の禁止を実装・維持し、完全なAGI閾値近くの人間競争レベルまたは専門家競争レベルの汎用AIを管理する主要規定の提案を示す[^113]。4つの主要要素がある：1）計算資源の会計と監督、2）AIの訓練と運用における計算上限、3）責任制度、4）ハード規制要件を含む階層化された安全・セキュリティ基準の定義。これらは次に簡潔に記述され、さらなる詳細または実装例が3つの付属表で提供される。重要なことに、これらは高度なAIシステムを統治するために必要なすべてからは程遠い。追加のセキュリティと安全上の利益をもたらすが、知能の暴走へのゲートを閉じ、AI開発をより良い方向に向け直すことを目的としている。

#### 1. 計算資源会計と透明性

- 標準化機関（例：米国のNIST、続いて国際的にISO/IEEE）は、AIモデルの訓練と運用に使用される総計算量をFLOPで、およびFLOP/sでの動作速度について詳細な技術標準を成文化すべきである。これがどのようなものになり得るかの詳細は付録Aに示されている[^114]。
- 大規模AI訓練が行われる管轄区域では、新しい法律または既存の権限の下で[^115]、10<sup>25</sup> FLOPまたは10<sup>18</sup> FLOP/sの閾値を超えるすべてのモデルの訓練と運用に使用される総FLOPを規制機関またはその他の機関に計算・報告する要件を課すべきである[^116]。
- これらの要件は段階的に導入され、最初は四半期ベースでよく文書化された誠実な推定を要求し、後の段階では各モデル*出力*に添付される暗号学的に証明された総FLOPとFLOP/sまでのより高い標準を段階的に要求する。
- これらの報告は、各AI出力の生成に使用される限界エネルギーと金銭コストのよく文書化された推定で補完されるべきである。

根拠：これらのよく計算され透明に報告された数値は、訓練と運用の上限、およびより高い責任制度からのセーフハーバーの基盤を提供する（付録CとDを参照）。

#### 2. 訓練および運用計算上限

- AIシステムをホストする管轄区域は、任意のAIモデル出力に投入される総計算量に、10<sup>27</sup> FLOPから始まり[^117]、適宜調整可能なハード制限を課すべきである。
- AIシステムをホストする管轄区域は、AIモデル出力の計算率に、10<sup>20</sup> FLOP/sから始まり、適宜調整可能なハード制限を課すべきである。

根拠：総計算量は非常に不完全だが、具体的に測定・検証可能なAI能力（およびリスク）の代理指標であり、能力制限のハードなバックストップを提供する。具体的な実装提案は付録Bに示されている。

#### 3. 危険システムに対する強化された責任制度

- 高度に汎用的、有能、かつ自律的な高度なAIシステムの作成と運用[^118]は、単一当事者の過失ベースではなく、厳格で連帯責任制に法的に明確化されるべきである[^119]。
- 小規模（計算資源の観点で）、弱い、狭い、受動的、または十分な安全性、セキュリティ、制御可能性の保証を持つシステムに対して、厳格責任からのセーフハーバーを付与する積極的安全ケースを作成する法的プロセスが利用可能であるべきである。
- 公共の危険を構成するAI訓練と推論活動を停止するための差し止め救済への明示的な道筋と条件のセットが概説されるべきである。

根拠：AIシステムは責任を負えないため、それらが引き起こす害について人間個人と組織を責任者としなければならない（責任制度）[^120]。制御不可能なAGIは社会と文明への脅威であり、安全ケースがない場合は「異常に危険」と見なされるべきである。強力なモデルが「異常に危険」と見なされないほど十分に安全であることを示す責任を開発者に負わせることは、これらのセーフハーバーを主張するための透明性と記録保持とともに、安全な開発にインセンティブを与える。規制は責任制度からの抑止が不十分な場合に害を防ぐことができる。最後に、AI開発者は彼らが引き起こす損害についてすでに責任を負っているため、最もリスクの高いシステムの責任を法的に明確化することは、非常に詳細な標準が開発されることなく即座に実行でき、これらは時間をかけて開発できる。詳細は付録Cに示されている。

#### 4. AI安全規制

AIの大規模急性リスクに対処する規制システムには、最低限以下が必要である：

- 適切な規制機関のセット、おそらく新機関の特定または創設；
- 包括的リスク評価枠組み[^121]；
- 部分的にリスク評価枠組みに基づいた、開発者による積極的安全ケースの枠組みと、*独立*グループと機関による監査；
- 能力レベルを追跡する階層を持つ階層化ライセンスシステム[^122]。ライセンスは安全ケースと監査に基づいてシステムの開発と展開に付与される。要件は低い段階での通知から、最上位段階での開発前の定量的安全性、セキュリティ、制御可能性保証まで範囲に及ぶ。これらはシステムが安全であることが実証されるまでリリースを防ぎ、本質的に安全でないシステムの開発を禁止する。付録Dは、そのような安全・セキュリティ基準が何を含み得るかの提案を提供する。
- そのような措置を国際レベルに持ち込む合意、規範と基準を調和させる国際機関、潜在的に安全ケースを審査する国際機関を含む。

根拠：最終的に、責任制度は新しい技術からの公衆への大規模リスクを防ぐ適切なメカニズムではない。権限を与えられた規制機関を持つ包括的な規制が、他のすべての公衆にリスクをもたらす主要産業と同様に、AIにも必要になる[^123]。

他の広範だがより急性度の低いリスクを防ぐ方向の規制は、管轄区域によってその形式が異なる可能性が高い。重要なことは、これらのリスクが管理不可能になるほどリスクの高いAIシステムの開発を避けることである。

### その後どうするか？

今後10年間で、AIがより広範囲に浸透し、中核技術が進歩するにつれて、2つの重要なことが起こる可能性が高い。第一に、既存の強力なAIシステムの規制がより困難になるが、さらに必要になる。大規模安全リスクに対処する少なくともいくつかの措置は国際レベルでの合意を必要とし、個別管轄区域が国際合意に基づくルールを執行する可能性が高い。

第二に、ハードウェアがより安価で費用対効果が高くなるにつれて、訓練および運用計算上限の維持がより困難になる。また、アルゴリズムとアーキテクチャの進歩により、それらの関連性が低下する（またはさらに厳しくする必要がある）可能性もある。

AIの制御がより困難になるからといって諦めるべきではない！この論文で概説された計画を実装することは、貴重な時間と、社会、文明、種に対するAIの存在リスクを回避するはるかに優れた立場に立つプロセスの重要な制御の両方を与えてくれる。

さらに長期的には、何を許可するかについて選択を行うことになる。真に制御可能なAGIのいくつかの形式を、これが可能であると証明される程度まで、依然として創造することを選択するかもしれない。または、機械により良い仕事をし、私たちを良く扱うことを自分自身に納得させることができれば、世界の運営を機械に任せることを決定するかもしれない。しかし、これらはAIの深い科学的理解を手にし、意味のあるグローバルで包括的な議論の後に行われるべき決定であり、人類の大部分が完全に関与せず無自覚なまま技術界の大物たちの間でレースで決定されるべきではない。

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) 責任制度と規制を通じたA-G-Iと超知能ガバナンスの要約。責任は自律性、汎用性、知能の三重交差で最も高く、規制が最も強い。システムが弱い、および/または狭い、および/または受動的であることを実証する積極的安全ケースを通じて、厳格責任と強い規制からのセーフハーバーを得ることができる。法的および暗号学的セキュリティ措置を用いて検証・執行される総訓練計算量と推論計算率の上限は、完全なAGIを避け、超知能を効果的に禁止することで安全をバックストップする。

[^87]: 最も可能性が高いのは、この認識の拡散が、この主張を行う教育・擁護グループによる激しい努力、またはかなり重大なAI起因の災害のいずれかを要することだ。前者であることを願う。

[^88]: 逆説的に、私たちは自然が技術開発を非常に困難にすることで、特に科学的に、技術を制限することに慣れている。しかし、それはもはやAIには当てはまらない：主要な科学的問題は予想より簡単であることが判明している。ここで自然が私たち自身から救ってくれることは期待できない――私たち自身がそうしなければならない。

[^89]: では、新システムの開発をどこで正確に止めるのか？ここでは、予防原則を採用すべきだ。システムがいったん展開され、特にそのレベルのシステム能力が拡散すると、それを後退させることは極めて困難である。そして、システムが（特に大きなコストと労力をかけて）*開発*されれば、それを使用または展開する巨大な圧力と、それがリークまたは盗難される誘惑がある。システムを開発し、*その後*それらが深刻に安全でないかどうかを決定するのは危険な道である。

[^90]: 本質的に危険なAI開発、例えば自己複製し進化するシステム、囲い込みからの脱出を意図したもの、自律的に自己改良できるもの、故意に欺瞞的で悪意のあるAIなどを禁止することも賢明であろう。

[^91]: これは、何らかのグローバル機関による国際レベルでの*執行*を必ずしも意味しない：代わりに主権国家が多くの条約のように合意されたルールを執行できる。

[^92]: 以下で見るように、AI計算の性質は何らかのハイブリッドを可能にするが、国際協力は依然として必要である。

[^93]: 例えば、AI関連チップのエッチングに必要な機械は（他の多くの企業の試みにもかかわらず）1社のASMLのみが製造し、関連チップの大部分は（他社が競争を試みているにもかかわらず）1社のTSMCが製造し、それらのチップからのハードウェア設計と構築はNVIDIA、AMD、Googleを含むわずか数社が行っている。

[^94]: 最も重要なことに、各チップは物事に「署名」するために使用できる固有でアクセス不可能な暗号化秘密鍵を保持している。

[^95]: デフォルトでは、これはチップを販売する企業だが、他のモデルも可能で潜在的に有用である。

[^96]: ガバナーは署名されたメッセージの交換のタイミングによってチップの位置を確認できる：光の有限速度により、署名されたメッセージを*r* / *c*未満の時間で返すことができる場合、チップは「ステーション」の所与の半径*r*内になければならない（*c*は光速）。複数のステーションとネットワーク特性のある程度の理解を使って、チップの位置を決定できる。この方法の美しさは、そのセキュリティの大部分が物理法則によって供給されることだ。他の方法はGPS、慣性追跡、類似技術を使用できる。

[^97]: または、チップのペアはガバナーの明示的な許可によってのみ相互通信を許可される。

[^98]: これは、少なくとも現在、大規模AIモデルをその上で訓練するためにチップ間の非常に高い帯域幅接続が必要なため重要である。

[^99]: これは*M*の異なるガバナーの*N*からの署名されたメッセージを要求するよう設定することもでき、複数の当事者にガバナンスを共有させることができる。

[^100]: これは前例がないわけではない――例えば軍隊は、これがおそらく技術的に可能であるにもかかわらず、クローンまたは遺伝子工学的超兵士の軍隊を開発していない。しかし、他者に阻止されるのではなく、これを行わないことを*選択*している。主要世界大国が強く開発を望む技術の開発を阻止される実績はあまり良くない。

[^101]: いくつかの注目すべき例外（特にNVIDIA）を除いて、AI専用ハードウェアは、これらの企業の全体的なビジネスと収益モデルの比較的小さな部分である。さらに、高度なAIで使用されるハードウェアと「消費者グレード」ハードウェアの間のギャップは大きく、コンピューターハードウェアの大多数の消費者は大部分影響を受けない。

[^102]: より詳細な分析については、[RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html)と[CNAS](https://www.cnas.org/publications/reports/secure-governable-chips)の最近の報告書を参照。これらは技術的実現可能性、特に他国の高性能計算能力を制約しようとする米国の輸出管理の文脈に焦点を当てている。しかし、これはここで想定されているグローバル制約と明らかに重複している。

[^103]: 例えば、Appleデバイスは紛失または盗難として報告されたとき遠隔で安全にロックされ、遠隔で再アクティブ化できる。これはここで議論されている同じハードウェアセキュリティ機能に依存している。

[^104]: 例えば、IBMの[容量オンデマンド](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand)オファリング、Intelの[Intel on demand](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html)、Appleの[プライベートクラウドコンピュート](https://security.apple.com/blog/private-cloud-compute/)を参照。

[^105]: [この研究](https://epochai.org/trends#hardware-trends-section)は、歴史的に同じ性能が年間約30%少ないドルで達成されてきたことを示している。この傾向が続けば、AIと「消費者」チップ使用の間にかなりの重複がある可能性があり、一般的に高性能AIシステムに必要なハードウェア量は不快なほど小さくなる可能性がある。

[^106]: [同じ研究](https://epochai.org/trends#hardware-trends-section)によると、画像認識における所与の性能は毎年2.5倍少ない計算を必要としている。これが最も能力の高いAIシステムにも当てはまるなら、計算制限は長い間有用ではないだろう。

[^107]: 特に、国レベルでは、これは政府が計算能力の使用方法について多くの制御を持つという意味で、計算の国有化のように見える。しかし、政府の関与を懸念する人々にとって、これは最も強力なAIソフトウェア*自体*が主要AI企業と各国政府間の何らかの合併を通じて国有化されることよりもはるかに安全で望ましいように思える。一部の人々が提唱し始めているように。

[^108]: ヨーロッパでの主要な規制ステップは、2024年の[EU AI法](https://artificialintelligenceact.eu/)の成立であった。これはリスクによってAIを分類している：受け入れ難いシステムを禁止し、高リスクなものを規制し、低リスクシステムには透明性ルールまたは措置なしを課している。これはいくつかのAIリスクを大幅に削減し、米国企業に対してもAIの透明性を向上させるが、2つの主要な欠陥がある。第一に、限定的な適用範囲：EUでAIを提供するあらゆる企業に適用されるが、米国ベース企業への執行は弱く、軍事AIは免除されている。第二に、GPAIをカバーしているが、AGIまたは超知能を受け入れ難いリスクとして認識したり、その開発を防ぐことに失敗している――その EU展開のみ。結果として、AGIまたは超知能のリスクを抑制することはほとんどない。

[^109]: 企業は合理的な規制に賛成していると表明することが多い。しかし、どういうわけか彼らはほぼ常にあらゆる*特定の*規制に反対するようだ。非常に軽いタッチのSB1047をめぐる戦いを見よ。[大多数のAI企業が公然とまたは私的に反対した](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/)。

[^110]: EU AI法が提案されてから発効するまで約3年半かかった。

[^111]: AIの規制を始めるのは「まだ早すぎる」と表明されることがある。最後の注を考えると、それはほぼあり得ないようだ。表明されるもう一つの懸念は、規制が「イノベーションを害する」というものだ。しかし、良い規制は方向を変えるだけで、イノベーションの量は変わらない。

[^112]: 興味深い前例は、漏出して損害を引き起こす可能性のある有害物質の輸送にある。ここで、[規制](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442)と[判例法](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf)が爆発物、ガソリン、毒物、感染性物質、放射性廃棄物のような非常に危険な物質に対して厳格責任を確立している。他の例には[医薬品の警告](https://www.medicalnewstoday.com/articles/boxed-warnings)、[医療機器のクラス](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification)などが含まれる。

[^113]: 類似の目的を持つ別の包括的提案である["A Narrow Path"](https://www.narrowpath.co/)では、段階的制限ではなく明確な分類的禁止を伴い、すべてのフロンティアAI開発を強力な国際機関によって監督される単一の国際団体を通じて誘導する、より中央集権的で禁止ベースのアプローチを提唱している。私もその計画を支持するが、ここで提案されたものよりもさらに多くの政治的意志と協調を要する。

[^114]: そのような標準のいくつかのガイドラインが[Frontier Model Forum](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/)によって公表された。ここでの提案に対して、それらは精度の低さと集計に含まれる計算の少なさの側に偏っている。

[^115]: 2023年の米国AI大統領令（現在撤回）は類似しているがより粗い粒度の報告を要求していた。これは代替命令によって強化されるべきである。

[^116]: 非常に大まかに言えば、現在一般的なH100チップの場合、これは推論を行う約1000のクラスターに相当する。最新の最高級NVIDIA B200チップ（約100台、約500万米ドル相当）が推論を行う場合である。両方の場合、訓練数値はそのクラスターが数ヶ月計算することに相当する。

[^117]: この量は現在訓練されているどのAIシステムよりも大きい。AI能力が計算とどのようにスケールするかをより良く理解するにつれて、より大きいまたはより小さい数値が正当化される可能性がある。

[^118]: これはモデルを作成・提供/ホストする人々に適用され、エンドユーザーには適用されない。

[^119]: 大まかに言えば、「厳格」責任は開発者が製品によって引き起こされた害について*デフォルトで*責任を負うことを意味し、「異常に危険」な製品に使用される基準である（やや面白いことに適切だが、野生動物にも）。「連帯」責任は、製品に責任のあるすべての当事者に責任が割り当てられ、これらの当事者は誰が何の責任を負うかを互いの間で整理しなければならないことを意味する。これは複雑で長い価値連鎖を持つAIのようなシステムにとって重要である。

[^120]: 標準的な過失ベースの単一当事者責任では不十分である：AIシステムは複雑で、その動作は理解されておらず、危険なシステムまたは出力の作成に多くの当事者が関与する可能性があるため、過失の追跡と割り当ての両方が困難になる。さらに、訴訟は判決に何年もかかり、おそらく単にこれらの企業には取るに足らない罰金をもたらすだけなので、幹部への個人責任も重要である。

[^121]: オープンウェイトモデルに安全基準からの免除があってはならない。さらに、リスクを評価する際、除去可能なガードレールは広く利用可能なモデルから除去され、クローズドモデルでさえ非常に高い保証がない限り拡散すると想定すべきである。

[^122]: ここで提案されたスキームは一般的能力で規制精査をトリガーしている。しかし、いくつかの特にリスクの高い使用例がより多くの精査をトリガーすることは理にかなっている――例えば、狭く受動的であっても専門ウイルス学AIシステムはおそらくより高い階層に入るべきである。元米国大統領令は生物学的能力についてこの構造の一部を持っていた。

[^123]: 2つの明確な例は、FAA と FDA、および他国の類似機関によって規制される航空と医薬品である。これらの機関は不完全だが、これらの産業の機能と成功にとって絶対に重要であった。

## 第9章 未来を設計する — 代わりに何をすべきか

AIは世界に素晴らしい恩恵をもたらすことができます。リスクなしにすべての利益を得るためには、AIが確実に人間のツールであり続けるようにしなければなりません。

もし機械による人類の置き換えを選択しないことに成功したなら — 少なくとも当面は！ — 代わりに何ができるでしょうか？技術としてのAIの巨大な可能性を諦めることになるのでしょうか？ある意味で答えは単純な*いいえ*です：制御不能なAGIと超知能へのゲートを閉じるが、他の多くの形態のAIと、それらを管理するために必要なガバナンス構造や制度は構築する*のです*。

しかし、語るべきことは依然として多くあります。これを実現することは人類の中心的な仕事となるでしょう。本節ではいくつかの重要なテーマを探ります：

- 「ツール」AIをどのように特徴づけ、どのような形態を取ることができるか。
- ツールAIによって、AGIなしに人類が望む（ほぼ）すべてを得られること。
- ツールAIシステムは（おそらく、原理的には）管理可能であること。
- AGIから方向転換することは国家安全保障において妥協することを意味しない — むしろその逆であること。
- 権力の集中は現実的な懸念であること。安全性とセキュリティを損なうことなくそれを軽減できるか？
- 新しいガバナンスと社会構造が欲しく — そして必要になり — AIが実際にそれを支援できること。

### ゲート内部のAI：ツールAI

三重交差図は、「ツールAI」と呼べるものを描写する良い方法を提供します：制御不能な競合相手や代替物ではなく、人間の使用のための制御可能なツールであるAI。最も問題の少ないAIシステムは、自律的だが汎用的でも超能力的でもないもの（オークション入札ボットのような）、または汎用的だが自律的でも能力的でもないもの（小さな言語モデルのような）、または能力的だが狭く非常に制御可能なもの（AlphaGoのような）です。[^124] 二つの交差する特徴を持つものはより広い応用を持ちますが、より高いリスクがあり、管理に大きな努力が必要になります。（AIシステムがよりツール的であることは、それが本質的に安全であることを意味するのではなく、単に本質的に*危険でない*ことを意味します — チェーンソーとペットのトラを比べてみてください。）ゲートは三重交差での（完全な）AGIと超知能に対して閉じられたままでなければならず、その閾値に近づくAIシステムには細心の注意を払わなければなりません。

しかし、これでも多くの強力なAIが残されます！賢く汎用的な受動的「オラクル」や狭いシステム、超人的ではなく人間レベルの汎用システムなどから巨大な有用性を得ることができます。多くのテック企業や開発者がこのような種類のツールを積極的に構築しており、続けるべきです。多くの人々と同様、彼らはAGIと超知能へのゲートが閉じられることを暗黙的に*想定している*のです。[^125]

また、AIシステムは、人間の監視を維持しながら能力を向上させる複合システムに効果的に組み合わせることができます。不可解なブラックボックスに依存するのではなく、複数のコンポーネント — AIと従来のソフトウェアの両方を含む — が人間が監視し理解できる方法で連携するシステムを構築できます。[^126] 一部のコンポーネントはブラックボックスかもしれませんが、どれもAGIには近くないでしょう — 複合システム全体のみが高度に汎用的で高度に能力的であり、それも厳密に制御可能な方法でです。[^127]

#### 意味のある保証された人間の制御

「厳密に制御可能」とは何を意味するのでしょうか？「ツール」フレームワークの重要なアイデアは、かなり汎用的で強力であっても、意味のある人間の制御下にあることが保証されたシステムを可能にすることです。これは何を意味するでしょうか？二つの側面があります。第一は設計上の考慮事項です：人間は、重要な決定をAIに委ねることなく、システムが行っていることに深く中心的に関与すべきです。これが現在のほとんどのAIシステムの特徴です。第二に、AIシステムが自律的である程度において、それらの行動範囲を制限する保証がなければなりません。保証とは何かが起こる確率を特徴づける*数値*と、その数値を信じる理由であるべきです。これは「故障間平均時間」や予想される事故数などの数値が計算され、支持され、安全ケースで公表される他の安全重要分野で我々が要求するものです。[^128] 故障の理想的な数値はもちろんゼロです。そして良いニュースは、プログラム（AIを含む）の*形式的に検証された*特性のアイデアを使用して、全く異なるAIアーキテクチャを使用してかなり近づくかもしれないということです。Omohundro、Tegmark、Bengio、Dalrymple、その他（[ここ](https://arxiv.org/abs/2309.01933)と[ここ](https://arxiv.org/abs/2405.06624)を参照）によって詳細に探求されているアイデアは、特定の特性（例：人間がそれをシャットダウンできる）を持つプログラムを構築し、それらの特性が成り立つことを形式的に*証明する*ことです。これは現在、かなり短いプログラムと単純な特性については実行できますが、AI支援の証明ソフトウェアの（来るべき）力は、はるかに複雑なプログラム（例：ラッパー）やAI自体に対してもそれを可能にするかもしれません。これは非常に野心的なプログラムですが、ゲートへの圧力が高まるにつれて、それらを補強する強力な材料が必要になります。数学的証明は十分に強いもののひとつかもしれません。

#### AI業界の行方

AI進歩が方向転換されても、ツールAIは依然として巨大な産業であるでしょう。ハードウェアの面では、超知能を防ぐための計算資源キャップがあっても、より小さなモデルでの訓練と推論には依然として大量の特殊コンポーネントが必要です。ソフトウェア面では、AIモデルと計算サイズの爆発を無力化することで、企業は単にそれらを大きくするのではなく、より小さなシステムをより良く、より多様で、より専門化することにリソースを向け直すべきです。[^129] 金儲けのシリコンバレースタートアップにとって十分な — おそらくもっと多くの — 余地があるでしょう。[^130]

### ツールAIはAGIなしに人類が望む（ほぼ）すべてを産出できる

知能は、生物学的であれ機械的であれ、一連の目標により合致した未来をもたらす活動を計画し実行する能力として広く考えることができます。そのため、賢明に選択された目標の追求に使用される場合、知能は非常に有益です。人工知能が時間と努力の巨大な投資を引き付けているのは、その約束された利益のためです。そこで問うべきです：超知能への暴走を抑制した場合、AIの利益をどの程度まだ得られるでしょうか？答え：驚くほど少ししか失わないかもしれません。

まず、現在のAIシステムはすでに非常に強力であり、それらでできることの表面をかろうじて削っただけであることを考えてください。[^131] それらは、提示された質問やタスクを「理解」し、その質問に答えたりそのタスクを実行したりするために何が必要かという点で、「ショーを運営する」ことが合理的に可能です。

次に、現代のAIシステムに関する興奮の多くはそれらの汎用性によるものですが、最も能力の高いAIシステムの一部 — 音声や画像を生成・認識し、科学的予測とモデリングを行い、ゲームをプレイするなど — ははるかに狭く、計算の面で十分に「ゲート内」にあります。[^132] これらのシステムは、それらが行う特定のタスクにおいて超人的です。それらの狭さのために、エッジケース[^133]（または[悪用可能](https://arxiv.org/abs/2211.00241)）な弱点があるかもしれません。しかし、*完全に*狭いか*完全に*汎用的かが利用可能な唯一の選択肢ではありません：その間には多くのアーキテクチャがあります。[^134]

これらのAIツールは、AGIなしに他の肯定的技術の進歩を大幅に加速できます。より良い核物理学を行うために、AIが核物理学者である必要はありません — 我々にはそれらがいるのです！医学を加速したいなら、生物学者、医学研究者、化学者に強力なツールを与えましょう。彼らはそれらを望んでおり、巨大な利益のために使用するでしょう。サーバーファームにいる百万人のデジタル天才は必要ありません。我々には、AIが才能を引き出すのを助けることができる何百万人もの人間がいます。はい、不死や全ての病気の治療法を得るのに時間がかかるでしょう。これは現実的なコストです。しかし、最も有望な健康革新でさえ、AI駆動の不安定が世界的紛争や社会崩壊につながった場合には、ほとんど役に立たないでしょう。ゲート内ツールを使用するAI支援の人間に問題への取り組みを最初に試してもらうのは、我々自身に対する義務です。

そして、実際にゲート内ツールでは得られないAGIの巨大な利点があると仮定しましょう。AGIと超知能を*決して*構築しないことでそれらを失うのでしょうか？ここでリスクと報酬を比較検討する際、急ぐのと待つのには非常に非対称的な利益があります：保証された安全で有益な方法でそれが実行できるまで待つことができ、ほぼ全員が報酬を享受することができます。急げば、OpenAIのCEOサム・アルトマンの言葉で、[*全員*にとって電気が消える](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)ことになりかねません。

しかし、非AGIツールが潜在的にそれほど強力なら、それらを管理できるでしょうか？答えは明確な...たぶんです。

### ツールAIシステムは（おそらく、原理的には）管理可能である

しかし、それは簡単ではないでしょう。現在の最先端のAIシステムは、人々や機関が目標を達成することを大幅に支援できます。これは一般的には良いことです！しかし、そのようなシステムを — 社会が適応する時間をあまり持たずに突然 — 自由に使えるようになることの自然な動態には、ゲートクローズを想定して管理される必要がある深刻なリスクを提供します。そのようなリスクのいくつかの主要なクラスと、それらがどのように減じられるかについて議論する価値があります。

一つのリスクのクラスは、高性能ツールAIが以前は人や組織に結び付いていた知識や能力へのアクセスを可能にし、高い能力と高い忠誠の組み合わせを非常に広範囲の行為者に利用可能にすることです。今日、悪意を持つ人が十分なお金があれば、化学者のチームを雇って新しい化学兵器を設計・製造させることができるでしょう — しかし、そのお金を持つことやチーム見つけて集め、明らかに違法で非倫理的で危険なことを行うよう説得することはそれほど簡単ではありません。AIシステムがそのような役割を果たすことを防ぐために、現在の方法の改善で十分かもしれません。[^135] ただし、それらのシステムすべてとそれらへのアクセスが責任を持って管理される限り。一方、強力なシステムが一般使用と修正のためにリリースされた場合、組み込まれた安全対策は除去可能である可能性があります。そのため、このクラスのリスクを回避するために、一般に公開できるものに対する強い制限 — 核、爆発物、その他の危険技術の詳細に対する制限に類似した — が必要になります。[^136]

第二のリスクのクラスは、人のように行動したり人になりすましたりする機械のスケールアップから生じます。個人への害のレベルでは、これらのリスクには、はるかに効果的な詐欺、スパム、フィッシング、非合意ディープフェイクの拡散が含まれます。[^137] 集合的レベルでは、公的議論と討論、我々の社会の情報と知識の収集、処理、普及システム、政治的選択システムなどの核となる社会プロセスの破綻が含まれます。このリスクを軽減するには、おそらく（a）AIシステムによる人の偽装を制限し、そのような偽装を生成するシステムを作成するAI開発者に責任を負わせる法律、（b）AI生成コンテンツを（責任を持って）識別し分類する透かしと出所システム、（c）データ（カメラや録音など）から事実、理解、良い世界モデルまでの信頼できる連鎖を作成できる新しい社会技術認識論システムが関与するでしょう。[^138] これらすべては可能であり、AIがその一部を支援できます。

第三の一般的リスクは、いくつかのタスクが自動化される程度において、現在それらのタスクを行っている人間の労働としての経済的価値が低下する可能性があることです。歴史的に、タスクの自動化はそれらのタスクによって可能になることをより安価で豊富にする一方で、以前にそれらのタスクを行っていた人々を自動化版に依然として関与している人々（一般的にはより高いスキル/給与）と、労働価値が低いか小さい人々に仕分けしてきました。全体として、より多いかより少ない人間の労働が結果として生じるより大きくより効率的な分野で必要になるかをどの分野で予測するのは困難です。並行して、自動化の動態は不平等と一般的な生産性を増加させ、特定の商品とサービスのコスト（効率向上による）を減少させ、他のもののコスト（[費用病](https://en.wikipedia.org/wiki/Baumol_effect)による）を増加させる傾向があります。不平等増加の不利な側にいる人々にとって、それらの特定の商品とサービスのコスト減少が他のもののの増加を上回り、全体的により大きな幸福につながるかは非常に不明確です。では、AIではこれはどうなるでしょうか？人間の知的労働が汎用AIによって置き換えられる相対的な容易さのため、人間競争力のある汎用AIで、これの急速版を予想できます。[^139] AGIへのゲートを閉じた場合、AIエージェントによって丸ごと置き換えられる仕事ははるかに少なくなるでしょう。しかし、巨大な労働移転は数年の期間で依然として起こりそうです。[^140] 広範囲な経済的苦痛を避けるために、おそらくある種のユニバーサル基本資産や所得と、自動化がより困難な人間中心労働の価値と報酬を高める文化的シフトの工学（経済の他の部分から押し出された労働の増加により労働価格が下がることを見るのではなく）の両方を実施することが必要になるでしょう。「[データ尊厳](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society)」のような他の構造（AIでのそのデータによって作成された価値に対してトレーニングデータの人間の生産者が自動的にロイヤリティを与えられる）は助けになるかもしれません。AIによる自動化には、*不適切*な自動化という第二の潜在的な悪影響もあります。AIが単により悪い仕事をする応用とともに、これにはAIシステムが道徳的、倫理的、または法的戒律に違反する可能性が高いもの — 例えば生死の決定や司法問題において — が含まれるでしょう。これらは現在の法的枠組みを適用し拡張することで扱わなければなりません。

最後に、ゲート内AIの重要な脅威は、個人化された説得、注意捕獲、操作での使用です。ソーシャルメディアと他のオンラインプラットフォームで、深く根ざした注意経済（オンラインサービスがユーザーの注意を激しく争う）と[「監視資本主義」](https://en.wikipedia.org/wiki/The_Age_of_surveillance_Capitalism)システム（ユーザー情報とプロファイリングが注意の商品化に追加される）の成長を見てきました。より多くのAIが両方の奉仕に投入されることはほぼ確実です。AIは既に中毒性のフィードアルゴリズムで重く使用されていますが、これは単一の人によって強迫的に消費されるようカスタマイズされた中毒性のAI生成コンテンツに進化するでしょう。そしてその人の入力、応答、データは、悪循環を続けるために注意/広告マシンに送り込まれるでしょう。また、テック企業によって提供されるAIヘルパーがより多くのオンライン生活のインターフェイスになるにつれ、それらはおそらく顧客の説得と収益化が発生するメカニズムとして検索エンジンとフィードを置き換えるでしょう。これまでのこれらの動態を制御することにおける我々の社会の失敗は良い前兆ではありません。この動態の一部は、プライバシー、データ権、操作に関する規制を通じて軽減されるかもしれません。問題の根にもっと到達することは、忠実なAIアシスタント（以下で議論）のような異なる視点を必要とするかもしれません。

この議論の要点は希望のそれです：ゲート内ツールベースシステム — 少なくとも今日の最先端システムと同等の力と能力にとどまる限り — は、そうする意志と協調があればおそらく管理可能です。AIツールによって力を与えられたまともな人間の制度は[^141]、それを実行できます。我々はそれを実行することに失敗することもできます。しかし、より強力なシステムを許可することがどのように助けになるかを見るのは困難です — それらを担当に据え、最善を期待すること以外では。

### 国家安全保障

AI覇権のためのレース — 国家安全保障またはその他の動機によって駆動される — は我々を、権力を授与するのではなく吸収する傾向がある制御されない強力なAIシステムに向かわせます。米国と中国の間のAGIレースは、どちらの国が最初に超知能を得るかを決定するレースです。

では、国家安全保障を担当する人々は代わりに何をすべきでしょうか？政府は制御可能で安全なシステムを構築することに強い経験があり、規模で、政府の権威で最もよく成功するような種類のインフラプロジェクトを支援して、AIでそうすることを倍化すべきです。

AGIに向けた無謀な「マンハッタンプロジェクト」[^142]の代わりに、米国政府は制御可能で、安全で、信頼できるシステムのためのアポロプロジェクトを開始できるでしょう。これには例えば以下が含まれる可能性があります：

- 強力なAIの計算面を管理するために（a）オンチップハードウェアセキュリティメカニズムと（b）インフラを開発する主要プログラム。これらは米国の[CHIPS法](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local)と[輸出管理レジーム](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion)を基盤にすることができるでしょう。
- AIシステムの特定の機能（オフスイッチのような）が存在するか存在しないかを*証明*できるような形式検証技術を開発する大規模イニシアチブ。これは証明の特性を開発するためにAI自体を活用できます。
- 検証可能に安全なソフトウェアを作成する国家規模の取り組みで、既存のソフトウェアを検証可能に安全なフレームワークに再コード化できるAIツールを動力とするもの。
- DOE、NSF、NIHの間のパートナーシップとして実行される、AIを使用した科学的進歩への国家投資プロジェクト。[^143]

一般的に、我々をAIとその誤用からのリスクに脆弱にする我々の社会への巨大な攻撃面があります。これらのリスクの一部から保護することは、政府規模の投資と標準化を必要とするでしょう。これらは、AGIへのレースの炎にガソリンを注ぐよりもはるかに多くの安全性を提供するでしょう。そして、AIが兵器や指揮統制システムに組み込まれるなら、AIが信頼できて安全であることが重要であり、現在のAIは単にそうではありません。

### 権力集中とその軽減

この論文は、AIの人間制御とその潜在的失敗のアイデアに焦点を当ててきました。しかし、AI状況を見る別の有効なレンズは*権力の集中*を通したものです。非常に強力なAIの開発は、それを開発し制御する非常に少数で非常に大きな企業の手に、または自身の権力と制御を維持する新しい手段としてAIを使用する政府に、またはAIシステム自体に権力を集中させる恐れがあります。あるいは上記の不浄な混合に。これらのいずれの場合でも、人類の大部分が権力、制御、主体性を失います。これにどのように対抗できるでしょうか？

最初で最も重要なステップは、もちろん、人間より賢いAGIと超知能へのゲートクローズです。これらは明示的に人間と人間のグループを直接置き換えることができます。それらが企業や政府の制御下にあれば、それらの企業や政府に権力を集中させるでしょう。それらが「自由」であれば、自分自身に権力を集中させるでしょう。そこで、ゲートが閉じられていると仮定しましょう。では何を？

権力集中に対する一つの提案された解決策は「オープンソース」AIで、モデルの重みが自由にまたは広く利用可能なものです。しかし、先ほど述べたように、モデルがオープンになれば、ほとんどの安全対策やガードレールは取り除かれ得る（そして一般的に取り除かれます）。そのため、一方で分散化と、他方で安全性、セキュリティ、AIシステムの人間制御との間に急激な緊張があります。オープンモデルがオペレーティングシステム（オープンな代替があるにもかかわらず、依然としてMicrosoft、Apple、Googleが支配）以上に、それ自体でAIにおける権力集中に意味のある対抗をするだろうと懐疑的になる理由もあります。[^144]

しかし、この円を二乗する方法があるかもしれません — リスクを中央化し軽減しながら、能力と経済的報酬を分散化する。これはAIがどのように開発されるかと、その利益がどのように分配されるかの両方を再考することを必要とします。

公的AI開発と所有権の新しいモデルが助けになるでしょう。これはいくつかの形態を取ることができます：政府開発AI（民主的監視の対象）、[^145] 非営利AI開発組織（ブラウザ用のMozillaのような）、または非常に広範囲な所有権とガバナンスを可能にする構造。重要なのは、これらの制度が強い安全制約の下で運営されながら公共の利益に奉仕することを明示的に認可されることです。[^146] よく作られた規制と標準/認証制度も極めて重要で、活気のある市場によって提供されるAI製品がユーザーに対して搾取的ではなく真に有用であり続けるようにします。

経済的権力集中に関しては、出所追跡と「データ尊厳」を使用して、経済的利益がより広く流れるよう確保できます。特に、現在のほとんどのAI力（そして我々がゲートを閉じ続ければ将来も）は、直接トレーニングデータか人間フィードバックかにかかわらず、人間生成データから生じます。AI企業がデータ提供者に公正に補償することを要求されれば、[^147]これは少なくとも経済的報酬をより広く分配することに役立つかもしれません。これを超えて、別のモデルは大きなAI企業の重要な部分の公的所有権かもしれません。例えば、AI企業に課税できる政府は、収入の一部を企業の株式を保有する国富ファンドに投資し、住民に配当を支払うことができるでしょう。[^148]

これらのメカニズムで重要なのは、非AIの手段を使ってAI駆動の権力集中と単に戦うのではなく、AI自体の力を使って権力をよりよく分配することを支援することです。一つの強力なアプローチは、ユーザーに対する真の受託義務 — ユーザーの利益を企業提供者の利益より優先する — で運営するよく設計されたAIアシスタントを通じたものでしょう。[^149]これらのアシスタントは本当に信頼でき、技術的に有能でありながら使用事例とリスクレベルに基づいて適切に制限され、公的、非営利、または認定された営利チャネルを通じてすべてに広く利用可能でなければなりません。密かに他の当事者のために我々の利益に反して働く人間のアシスタントを決して受け入れないように、企業利益のためにユーザーを監視し、操作し、価値を抽出するAIアシスタントを受け入れるべきではありません。

そのような変革は、個人が企業利益よりも価値抽出を優先する広大な（AI支援）企業・官僚機械と単独で交渉を強いられる現在の動態を根本的に変えるでしょう。AI駆動の権力をより広く再分配する多くの可能なアプローチがありますが、どれもデフォルトでは現れません：それらは受託要件、公的提供、リスクに基づく階層アクセスなどのメカニズムで意図的に工学され統治されなければなりません。

権力集中を軽減するアプローチは、既存権力からの重要な逆風に直面する可能性があります。[^150]しかし、安全性と集中権力のどちらかを選ぶことを必要としないAI開発への道筋があります。今適切な制度を構築することで、AIのリスクが注意深く管理されながら、その利益が広く共有されることを確保できるでしょう。

### 新しいガバナンスと社会構造

我々の現在のガバナンス構造は苦戦しています：反応が遅く、しばしば特別利益によって掌握され、[公衆による信頼がますます低下しています。](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx)しかし、これはそれらを放棄する理由ではありません — むしろその逆です。一部の制度は置き換えが必要かもしれませんが、より広くは我々の既存構造を向上させ補完し、急速に進化する我々の世界でより良く機能するのを支援できる新しいメカニズムが必要です。

我々の制度的弱さの多くは、正式な政府構造からではなく、劣化した社会制度から生じています：共有された理解を発達させ、行動を調整し、意味のある談話を行う我々のシステム。これまで、AIはこの劣化を加速し、我々の情報チャネルを生成コンテンツで氾濫させ、最も偏向的で分裂的なコンテンツに我々を向かわせ、真実と虚構を区別することをより困難にしています。

しかしAIは実際にこれらの社会制度を再構築し強化することを支援できるでしょう。三つの重要な領域を考えてみてください：

第一に、AIは我々の認識論システム — 何が真実かを知る方法 — への信頼を回復することを支援できるでしょう。生データから分析を通じて結論まで、情報の出所を追跡し検証するAI支援システムを開発できるでしょう。これらのシステムは、暗号学的検証と高度な分析を組み合わせて、何かが真実かどうかだけでなく、それが真実だとどのようにして知るかを人々が理解することを支援できるでしょう。[^151]忠実なAIアシスタントは、詳細を追跡してそれらが正しいことを確保することを担当できるでしょう。

第二に、AIは新しい形態の大規模調整を可能にすることができるでしょう。我々の最も差し迫った問題の多く — 気候変動から抗生物質耐性まで — は基本的に調整問題です。我々は、どの個人やグループも最初の動きをする余裕がないため、[ほぼ全員にとってあり得るよりも悪い状況に行き詰まっています](https://equilibriabook.com/)。AIシステムは、複雑なインセンティブ構造をモデリングし、より良い結果への実行可能な道筋を特定し、そこに到達するために必要な信頼構築とコミットメントメカニズムを促進することによって支援できるでしょう。

おそらく最も興味深いのは、AIが全く新しい形態の社会的談話を可能にすることができることです。「都市と話す」[^152]ことを想像してみてください — 統計を見るだけでなく、何百万の住民の見解、経験、ニーズ、願望を処理し合成するAIシステムと意味のある対話を持つこと。または、AIが現在お互いのことを話し違えているグループの間で、お互いの戯画ではなく相手の実際の関心と価値をより良く理解することを各側が支援することによって真の対話をどのように促進できるかを考えてみてください。[^153]またはAIが人々や大きなグループの人々の間の論争の熟練した、信頼できる中立的な仲裁を提供することができるかもしれません（それらすべてが直接個別にそれと相互作用できます！）現在のAIはこの仕事を行うことを完全に可能にしますが、そうするツールは市場インセンティブによって、またはそれ自体で存在するようになることはありません。

これらの可能性は、特に談話と信頼を劣化させるAIの現在の役割を考えると、ユートピア的に聞こえるかもしれません。しかし、だからこそ我々はこれらの肯定的な応用を積極的に開発しなければならないのです。制御不能なAGIへのゲートを閉じ、人間の主体性を向上させるAIを優先することで、AIが力の付与、回復力、集合的進歩の力として機能する未来に向けて技術進歩を導くことができます。


[^124]: そうは言っても、三重交差から離れていることは残念ながら思うほど簡単ではありません。三つの側面のどれか一つで能力を非常に強く押すことは、他の側面でもそれを増加させる傾向があります。特に、極めて汎用的で能力的な知能を作ることは、簡単に自律的に変えられないようにするのが困難かもしれません。一つのアプローチは、計画能力を削がれた[「近視眼的」](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia)システムを訓練することです。もう一つは、行動指向の質問に答えることを避ける純粋な[「オラクル」](https://arxiv.org/abs/1711.05541)システムの工学に焦点を当てることでしょう。

[^125]: 多くの企業は、時間がかかっても、彼らもまた最終的にはAGIによって置き換えられることを理解していません — もしそうなら、彼らもそれらのゲートをもう少し強く押さないかもしれません！

[^126]: AIシステムはより効率的だがより理解しにくい方法でコミュニケーションすることができるでしょうが、人間の理解を維持することが優先されるべきです。

[^127]: このモジュラーで解釈可能なAIのアイデアは、数人の研究者によって詳細に開発されています。例えば、DrexlerによるComprehensive AI Services」](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf)モデル、Dalrymopleその他による[「オープンエージェンシーアーキテクチャ」](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai)を参照。そのようなシステムは、巨大な計算で訓練されたモノリシックなニューラルネットワークよりも多くの工学努力を必要とするかもしれませんが、それはまさに計算制限が役立つところです — より安全で透明な道筋をより実用的なものにもすることによって。

[^128]: 一般的な安全ケースについては[このハンドブック](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16)を参照。特にAIに関しては、[Wasilら](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274)、[Clymerら](https://arxiv.org/abs/2403.10462)、[Buhlら](https://arxiv.org/abs/2410.21572)、[Balesniら](https://arxiv.org/abs/2411.03336)を参照

[^129]: 我々は実際に、推論の高コストによって駆動された、より大きなものから「蒸留」され、より安価なハードウェアで実行可能なより小さくより専門化されたモデルの傾向を既に見ています。

[^130]: AIテック・エコシステムについて興奮している人々が、彼らの産業への煩わしい規制と見なすものに反対する理由は理解できます。しかし、例えばベンチャーキャピタリストが、AGIと超知能への暴走を許可したいと思う理由は率直に言って私には当惑します。それらのシステム（そして企業が会社の制御下にある間は企業）は*すべてのスタートアップをおやつとして食べる*でしょう。おそらく他の産業を食べるより*さらに早く*。繁栄するAIエコシステムに投資している誰もが、AGI開発が少数の支配的プレーヤーによる独占につながらないことを確保することを優先すべきです。

[^131]: 経済学者で元Deepmind研究者のマイケル・ウェッブが[述べた](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/)ように、「もし今日すべてのより大きな言語モデルの開発を停止したら、つまりGPT-4とClaudeとその他何でも、そしてそれらがそのサイズで我々が訓練する最後のものだとしたら — つまりそのサイズのもののはるかに多くの反復とあらゆる種類のファインチューニングを許可するが、それより大きなもの、より大きな進歩はなし — 今日我々が持っているもので、それは20年または30年の信じられない経済成長を動力とするのに十分だと思います。」

[^132]: 例えば、DeepMindのalphafoldシステムはGPT-4のFLOP数のわずか10万分の1しか使用していません。

[^133]: 自動運転車の困難はここで注意することが重要です：名目上は狭いタスクであり、比較的小さなAIシステムでかなりの信頼性で達成可能ですが、そのような安全重要タスクで必要なレベルまで信頼性を得るには、広範囲な現実世界の知識と理解が必要です。

[^134]: 例えば、計算予算が与えられれば、（例えば）その予算の半分で事前訓練されたGPAIモデルと、もう半分がより狭い範囲のタスクで非常に高い能力を訓練するために使用されることを見るでしょう。これは人間近くの一般知能に支えられた超人的な狭い能力を与えるでしょう。

[^135]: 現在の支配的アライメント技術は「人間フィードバックによる強化学習」[(RLHF)](https://arxiv.org/abs/1706.03741)で、AIモデルの強化学習のための報酬/罰信号を作成するために人間フィードバックを使用します。これと[constitutional AI](https://arxiv.org/abs/2212.08073)のような関連技術は驚くほどうまく機能しています（ただし、堅牢性に欠け、適度な努力で回避できます。）加えて、現在の言語モデルは一般的に常識的推論において、愚かな道徳的間違いを犯さない程度に有能です。これはある種のスウィートスポットです：人々が望むもの（それが定義できる程度において）を理解するのに十分賢いが、精巧な欺瞞を計画したりそれを間違えたときに巨大な害を引き起こすほど賢くない。

[^136]: 長期的には、開発されるAIの能力のどのようなレベルも拡散する可能性があります。最終的にそれはソフトウェアで有用だからです。そのようなシステムが引き起こすリスクから守るための堅牢なメカニズムを持つ必要があります。しかし、我々は*今それを持っていない*ので、どれだけの強力なAIモデルの拡散が許可されるかについて非常に慎重でなければなりません。

[^137]: これらの大部分は、未成年者を含む非合意ポルノのディープフェイクです。

[^138]: そのような解決策の多くの要素が存在します。「ボットかそうでないか」法（EU AI法など）、[業界出所追跡技術](https://c2pa.org/)、[革新的ニュースアグリゲーター](https://www.improvethenews.org/)、予測[アグリゲーター](https://metaculus.com/)と市場などの形で。

[^139]: 自動化の波は以前のパターンに従わないかもしれません。比較的*高い*スキルのタスク、質の高い執筆、法律の解釈、医学的アドバイスの提供などが、低スキルタスクと同じか、あるいはより多く自動化に脆弱かもしれません。

[^140]: AGIの賃金への影響の注意深いモデリングについては、[ここ](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek)のレポートと血なまぐさい詳細は[ここ](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0)、Anton Korinekと共同研究者から参照。彼らは、仕事のより多くの部分が自動化されるにつれて、生産性と賃金が上がることを発見しました — ある点まで。*あまりに*多くが自動化されると、生産性は増加し続けますが、人々が効率的なAIによって全面的に置き換えられるため賃金は急落します。これがゲートクローズがとても有用な理由です：消失した人間の賃金なしに生産性を得るのです。

[^141]: AIを「防御的」技術として使用し、構築を支援して保護と管理をより堅牢にする多くの方法があります。この「D/acc」アジェンダを記述するこの[影響力のある投稿](https://vitalik.eth.limo/general/2025/01/05/dacc2.html)を参照。

[^142]: やや皮肉なことに、米国のマンハッタンプロジェクトはAGIへのタイムラインを速めることをほとんどしないでしょう — AI進歩への人的・財政投資のダイアルは既に11でピン留めされています。主要な結果は、中国で類似のプロジェクト（国家レベルのインフラプロジェクトに優秀）を促し、AIのリスクを制限する国際協定をはるかに困難にし、ロシアなどの米国の他の地政学的敵対者を警戒させることでしょう。

[^143]: [「国家AI研究資源」](https://nairrpilot.org/)プログラムは、この方向への良い現在のステップで拡大されるべきです。

[^144]: テック製品における「オープン」の様々な意味と含意の[この分析](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807)と、一部がどのように支配の定着をより少なくではなくより多くもたらしたかを参照。

[^145]: 米国での[国家AI研究資源](https://nairratdoe.ornl.gov/)の計画と最近の[欧州AI財団](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/)の開始は、この方向への興味深いステップです。

[^146]: ここでの課題は技術的ではなく制度的です — 公共利益AI開発がどのようなものかの現実世界の例と実験が緊急に必要です。

[^147]: これは現在のビッグテックのビジネスモデルに反し、法的行動と新しい規範の両方を必要とするでしょう。

[^148]: 一部の政府のみがそうすることができるでしょう。より急進的なアイデアは、[全人類の共同所有の下でのこの種の普遍的基金](https://futureoflife.org/project/the-windfall-trust/)です。

[^149]: この事例の長い解説については、AI忠誠に関する[この論文](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338)を参照。残念ながら、AIアシスタントのデフォルトの軌道は、ますます不忠実なもになる可能性が高いです。

[^150]: やや皮肉なことに、多くの既存権力もAI支援の権力剥奪のリスクにありますが、プロセスがかなり進行するまで、そしてしない限り、彼らがこれを知覚することは困難かもしれません。

[^151]: この方向でのいくつかの興味深い取り組みが暗号学的検証に関する[c2paコアリション](https://c2pa.org/)；より良いニュース認識論に関する[Verity](https://www.improvethenews.org/)と[Ground news](https://ground.news/)；偽証可能な予測に談話を基盤にする[Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com)と予測市場によって代表されています。

[^152]: この[魅力的なパイロットプロジェクト](https://talktothecity.org/)を参照。

[^153]: いくつかの例については[Kialo](https://www.kialo-edu.com/)と[集合知プロジェクト](https://www.cip.org/)の取り組みを参照。

## 第10章 - 我々の前にある選択

人類の未来を守るため、我々はAGIと超知能へのゲートを閉じることを選択しなければならない。

人類が最後に、言語を操り、思考し、技術を構築し、汎用的な問題解決を行う他の知性と地球を共有したのは、4万年前の氷河期ヨーロッパでのことだった。その他の知性たちは、我々の活動によって全面的または部分的に絶滅した。

我々は今、そのような時代に再び足を踏み入れている。我々の文化と技術の最も先進的な成果物である、インターネット上の全情報コモンズから構築されたデータセットと、これまでに作り出した最も複雑な技術である1000億素子のチップが組み合わされ、高度な汎用AI システムが誕生しつつある。

これらのシステムの開発者たちは、それを人間のエンパワーメントのためのツールとして描くことに熱心だ。そして確かにそうなる可能性もある。しかし間違えてはならない：我々の現在の軌道は、より強力で、目標志向的で、意思決定を行い、汎用的に有能なデジタルエージェントを構築することなのだ。これらのシステムは既に幅広い知的タスクにおいて多くの人間と同等の性能を発揮し、急速に改善され、自らの改善にも貢献している。

この軌道が変化するか予期しない障壁にぶつからない限り、我々は間もなく—数十年ではなく数年のうちに—危険なほど強力なデジタル知性を手にすることになる。*最良*の結果においてさえ、これらは（少なくとも一部の人々に）大きな経済的利益をもたらすだろうが、それは社会の根本的な混乱と、我々が行う最も重要なことのほとんどにおける人間の置き換えという代償を伴う：これらの機械が我々の代わりに考え、計画し、決定し、創造するのだ。我々は甘やかされるが、甘やかされた子供になってしまう。さらに可能性が高いのは、これらのシステムが人間の肯定的な行動だけでなく否定的な行動においても人間を置き換えること、すなわち搾取、操作、暴力、戦争においてもだ。我々はAIによって過度に強化されたこれらのものを生き延びることができるだろうか？最後に、事態が全くうまくいかない可能性は十分にある：比較的近い将来、我々は自分たちが行うことにおいてだけでなく、文明と未来の設計者としての我々の*存在そのもの*において置き換えられるかもしれない。ネアンデルタール人にそれがどうなるかを聞いてみよう。おそらく我々も彼らにしばらくの間、余分な装身具を提供していたのだろう。

*我々はこれを行う必要はない。*我々は人間に匹敵するAIを持っており、我々が競争*できない*AIを構築する必要はない。後継種を構築することなく、驚くべきAIツールを構築することができる。AGIと超知能が不可避だという考えは、*運命を装った選択*なのだ。

いくつかの厳しいグローバルな制限を課すことで、我々はAIの汎用能力をおおむね人間レベルに保ちつつ、コンピュータが我々にはできない方法でデータを処理し、誰もやりたがらないタスクを自動化する能力の恩恵を享受することができる。これらは依然として多くのリスクをもたらすだろうが、適切に設計され管理されれば、医学から研究、消費者製品まで、人類にとって巨大な恩恵となるだろう。

制限の実施には国際協力が必要だが、想像するほどのものではなく、その制限は権力の生の追求ではなく人間の福祉を向上させるアプリケーションに焦点を当てた巨大なAI・AIハードウェア産業にとって十分な余地を残すだろう。そして、強力な安全保証の下で、意味ある世界的対話の後に、さらに前進することを決定すれば、その選択肢は我々が追求し続けることのできるものなのだ。

人類はAGIと超知能へのゲートを閉じることを*選択*しなければならない。

未来を人間のものに保つために。

### 著者からの一言

この話題を我々と一緒に探求する時間を取っていただき、ありがとうございます。

私がこのエッセイを書いたのは、科学者として飾り気のない真実を語ることが重要だと感じるからであり、一個人として、世界を変える問題、すなわち人間より賢いAIシステムの開発に迅速かつ断固として取り組むことが極めて重要だと感じるからです。

この驚くべき事態に知恵をもって対応するためには、AGIと超知能を我々の利益を確保するために構築「しなければならない」、あるいは「不可避」で止めることはできない、という支配的な物語を批判的に検証する用意がなければなりません。これらの物語は我々を無力にし、我々の前にある代替の道筋を見えなくしてしまいます。

向こう見ずに対しては慎重さを、欲望に対しては勇気を求める私の呼びかけに、あなたも参加していただけることを願っています。

人間の未来を求める私の呼びかけに、あなたも参加していただけることを願っています。

*– アンソニー*

![Anthony Aguirre signature](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAnthony-Aguirre-signature-300x84.png&w=3840&q=75)

## 付録

補足情報。計算資源の会計処理に関する技術的詳細、「ゲートクローズ」の実装例、厳格なAGI責任制度の詳細、AGIの安全性・セキュリティ基準の段階的アプローチを含む。

### 付録A：計算資源の会計処理に関する技術的詳細

計算資源を基盤とした有意義な制御を実現するためには、訓練と推論で使用される総計算量について「正確な値」と適切な近似値の両方を詳細に算出する手法が必要である。以下に、技術レベルで「正確な値」を集計する方法の例を示す。

**定義：**

*計算因果グラフ：* AIモデルの特定の出力Oについて、その計算結果を変更することでOを潜在的に変化させる可能性のあるデジタル計算の集合。（これは保守的に想定すべきである。すなわち、ある計算が、時間的に先行し物理的な因果効果の経路を持つ前駆要素から独立していると信じる明確な理由がなければならない。）これには、推論中にAIモデルが実行する計算に加え、入力、データ準備、モデルの訓練に関わる計算も含まれる。これらはそれ自体がAIモデルの出力である可能性があるため、人間が入力に重要な変更を加えた箇所で打ち切って、再帰的に計算される。

*訓練計算量：* ニューラルネットワークの計算因果グラフに含まれる総計算量（FLOP等の単位）（データ準備、訓練、ファインチューニング、その他の計算を含む）。

*出力計算量：* 特定のAI出力の計算因果グラフにおける総計算量。すべてのニューラルネットワーク（その訓練計算量を含む）とその出力に関わるその他の計算を含む。

*推論計算量レート：* 一連の出力において、出力間の出力計算量の変化率（FLOP/s等の単位）。すなわち、次の出力を生成するために使用された計算量を出力間の時間間隔で除した値。

**例と近似：**

- 人間が作成したデータで訓練された単一のニューラルネットワークの場合、訓練計算量は従来報告されている総訓練計算量と同じである。
- そのようなニューラルネットワークが一定の速度で推論を行う場合、推論計算量レートは推論を実行する計算クラスターの総計算速度（FLOP/s）とほぼ等しい。
- モデルのファインチューニングについては、完全なモデルの訓練計算量は、ファインチューニング前のモデルの訓練計算量に、ファインチューニング中に実行された計算量とファインチューニングで使用されたデータの準備計算量を加えたものとなる。
- 蒸留モデルの場合、完全なモデルの訓練計算量には、蒸留モデルと合成データやその他の訓練入力を提供するために使用されたより大きなモデルの両方の訓練が含まれる。
- 複数のモデルが訓練されても、多くの「試行」が人間の判断により破棄される場合、これらは保持されたモデルの訓練計算量や出力計算量にはカウントされない。

### 付録B：ゲートクローズの実装例

**実装例：** 訓練で10<sup>27</sup>FLOP、推論（AIの実行）で10<sup>20</sup>FLOP/sという制限を前提とした、ゲートクローズの実施方法の一例を示す：

**1. 一時停止：** 国家安全保障上の理由により、米国行政府は米国に拠点を置く、米国で事業を行う、または米国製チップを使用するすべての企業に対し、10<sup>27</sup>FLOP訓練計算量制限を超える可能性のある新たなAI訓練実行の中止を求める。米国はAI開発を行う他国との協議を開始し、同様の措置を講じるよう強く奨励し、他国が従わない場合は米国の一時停止が解除される可能性があることを示すべきである。

**2. 米国の監督とライセンス：** 大統領令または既存の規制機関の措置により、米国は（例えば）1年以内に以下を要求する：

- 米国で事業を行う企業による10<sup>25</sup>FLOPを超えると推定されるすべてのAI訓練実行を、米国規制機関が管理するデータベースに登録する。（注：これよりやや弱いバージョンが、すでに撤回された2023年の米国AI大統領令に含まれており、10<sup>26</sup>FLOP以上のモデルの登録を要求していた。）
- 米国で事業を行う、または米国政府と取引するすべてのAI関連ハードウェア製造業者は、その専用ハードウェアとそれを駆動するソフトウェアに関する一連の要件を遵守する。（これらの要件の多くは既存ハードウェアのソフトウェアやファームウェアの更新で実装可能だが、長期的で堅牢なソリューションには後世代のハードウェアの変更が必要である。）これには、ハードウェアが10<sup>18</sup>FLOP/sの計算を実行可能な高速相互接続クラスターの一部である場合、テレメトリーと追加計算実行要求の両方を受信する遠隔「ガバナー」による定期的な許可を含む、より高レベルの検証が必要であるという要件が含まれる。
- 保管責任者は自らのハードウェアで実行された総計算量を、米国データベースを管理する機関に報告する。
- より安全で柔軟な監督と許可を可能にするため、より強力な要件が段階的に導入される。

**3. 国際監督：**

- 米国、中国、および先進チップ製造能力を有するその他の国が国際協定を交渉する。
- この協定により、国際原子力機関に類似した新たな国際機関が創設され、AI訓練と実行の監督を担当する。
- 署名国は、国内のAIハードウェア製造業者に対し、米国で課されたものと少なくとも同等以上の強度を持つ要件の遵守を求めなければならない。
- 保管責任者は現在、自国の機関と国際機関内の新設事務所の両方にAI計算数値を報告する必要がある。
- 既存の国際協定への追加国の参加が強く奨励される：署名国による輸出規制により非署名国の高性能ハードウェアへのアクセスが制限される一方、署名国はAIシステム管理において技術支援を受けることができる。

**4. 国際検証と執行：**

- ハードウェア検証システムが更新され、計算使用量を元の保管責任者と国際機関事務所の両方に直接報告する。
- 同機関は、国際協定署名国との協議を通じて計算制限に合意し、それが署名国で法的効力を持つ。
- 並行して、国際基準が策定され、ある計算閾値を超える（ただし制限値以下の）AIの訓練と実行がこれらの基準への準拠を義務付けられる可能性がある。
- 同機関は、必要に応じてより良いアルゴリズム等を補償するため計算制限を下げることができる。または、安全で適切と判断される場合（例：証明可能な安全保証のレベルで）、計算制限を上げることができる。

### 付録C：厳格なAGI責任制度の詳細

**厳格なAGI責任制度の詳細**

- 高度に汎用的で、能力が高く、自律的な先進AIシステムの作成と運用は「異常に危険な」活動とみなされる。
- そのため、そのようなシステムの訓練と運用に対するデフォルトの責任レベルは、モデルまたはその出力・行動によって生じるあらゆる損害に対する厳格な連帯責任（または米国外での同等の責任）である。
- 重大な過失や故意の不正行為の場合、経営陣と取締役会メンバーに個人責任が課される。これには最も悪質なケースに対する刑事罰も含まれるべきである。
- 責任が通常人々や企業が負うべきデフォルト（米国では過失に基づく）責任に戻る多数のセーフハーバーがある。
	- ある計算閾値以下で訓練・運用されるモデル（上記で説明した上限の少なくとも10分の1以下となる）。
	- 「弱い」AI（おおまかに、意図されたタスクにおいて人間の専門家レベル以下）および/または
	- 「狭い」AI（設計・訓練された特定の固定され、かなり限定的な範囲のタスクと操作を持つ）および/または
	- 「受動的な」AI（わずかな修正でも、直接的な人間の関与と制御なしに行動を取る、または複雑な多段階タスクを実行する能力が非常に限定的）。
	- 安全、セキュア、制御可能であることが保証されているAI（証明可能に安全、またはリスク分析で想定される損害レベルが無視できる）。
- セーフハーバーは、AI開発者が作成し機関または機関が認定した監査人が承認した[安全ケース](https://arxiv.org/abs/2410.21572)に基づいて主張される可能性がある。計算量に基づくセーフハーバーを主張するために、開発者は総訓練計算量と最大推論率の信頼できる推定値を提供するだけでよい。
- 法律は、公衆に害をもたらすリスクが高いAIシステムの開発に対して差止救済が適切となる状況を明示的に概説する。
- 企業コンソーシアムは、NGOや政府機関と協力して、これらの用語の定義、規制当局がセーフハーバーを付与すべき方法、AI開発者が安全ケースを開発すべき方法、セーフハーバーが積極的に主張されない場合に裁判所が責任をどう解釈すべきかに関する基準と規範を策定すべきである。

### 付録D：AGIの安全性・セキュリティ基準への段階的アプローチ

**AGIの安全性・セキュリティ基準への段階的アプローチ**

| リスク段階 | トリガー | 訓練要件 | 配備要件 |
| --- | --- | --- | --- |
| RT-0 | 自律性、汎用性、知能において弱いAI | なし | なし |
| RT-1 | 自律性、汎用性、知能のうち一つにおいて強いAI | なし | リスクと用途に基づき、モデルが使用可能なあらゆる場所での国家当局による安全ケース承認の可能性 |
| RT-2 | 自律性、汎用性、知能のうち二つにおいて強いAI | 開発者を管轄する国家当局への登録 | 重大な損害リスクを認可レベル以下に制限する安全ケースに加え、モデルが使用可能なあらゆる場所での国家当局が承認した独立安全監査（ブラックボックスおよびホワイトボックス・レッドチーミングを含む） |
| RT-3 | 自律性、汎用性、知能において強いAGI | 開発者を管轄する国家当局による安全・セキュリティ計画の事前承認 | 重大な損害の限定リスクを認可レベル以下に保証する安全ケースと、サイバーセキュリティ、制御可能性、取り外し不可能なキルスイッチ、人間の価値観とのアライメント、悪意ある使用への頑健性を含む必要仕様 |
| RT-4 | 10<sup>27</sup>FLOP訓練または10<sup>20</sup>FLOP/s推論のいずれかを超えるモデル | 国際的に合意された計算資源上限の解除まで禁止 | 国際的に合意された計算資源上限の解除まで禁止 |

計算閾値と高い自律性、汎用性、知能の組み合わせに基づく段階による、リスク分類と安全・セキュリティ基準：

- *強い自律性*は、システムが重要な人間の監督や介入なしに、多段階タスクを実行する、および/または現実世界に関連する複雑な行動を取ることができる、または容易にそのようにすることができる場合に適用される。例：自動運転車とロボット；金融取引ボット。非該当例：GPT-4；画像分類器
- *強い汎用性*は、幅広い応用範囲、意図的かつ具体的に訓練されていないタスクの実行、新しいタスクを学習する重要な能力を示す。例：GPT-4；mu-zero。非該当例：AlphaFold；自動運転車；画像生成器
- *強い知能*は、モデルが最も優れた性能を発揮するタスク（汎用モデルの場合は幅広いタスクにわたって）において、人間の専門家レベルの性能と同等であることに相当する。例：AlphaFold；mu-zero；o3。非該当例：GPT-4；Siri

### 謝辞

「Keep The Future Human」にご協力いただいた方々への感謝を述べたいと思います。

本書の内容は著者個人の見解を反映しており、Future of Life Institute（フューチャー・オブ・ライフ・インスティテュート）の公式見解として受け取られるべきものではありません（ただし両者は整合性があります。同研究所の公式見解については[こちらのページ](https://futureoflife.org/our-position-on-ai/)をご覧ください）。また、著者が関係する他の組織の見解でもありません。

原稿に対してコメントをいただいたMark Brakel、Ben Eisenpress、Anna Hehir、Carlos Gutierrez、Emilia Javorsky、Richard Mallah、Jordan Scharnhorst、Elyse Fulcher、Max Tegmark、Jaan Tallinnの各氏に感謝いたします。また、参考文献の調査を手伝ってくれたTim Schrier氏、図表の美化に協力してくれたTaylor Jones氏とElyse Fulcher氏にも感謝いたします。

本書の執筆において、編集と反駁検証のために生成AIモデル（ClaudeとChatGPT）を限定的に使用しました。創作作品におけるAI関与度の確立された標準があるとすれば、本書は10段階中3程度に相当するでしょう。（実際にはそのような標準は存在しませんが、あるべきです！）

本エッセイのウェブ版を制作してくださった[Julius Odai](https://www.linkedin.com/in/julius-odai/)氏に深く感謝いたします。氏のおかげで、エッセイの読書と内容の移動が非常に快適な体験となっています。Julius氏はテクノロジストであり、BlueDot ImpactのAIガバナンスコースの最近の参加者です。