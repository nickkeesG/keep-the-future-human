# 인간적인 미래를 지켜내자

이 에세이는 인공일반지능(AGI)과 초지능으로 향하는 관문을 차단해야 하는 이유와 방법, 그리고 그 대신 무엇을 구축해야 하는지에 대한 논거를 제시한다.

핵심 요점만 확인하고 싶다면 요약본을 참고하라. 이후 2-5장에서는 이 에세이에서 다루는 AI 시스템의 유형에 대한 배경지식을 제공한다. 5-7장에서는 AGI가 조만간 등장할 수 있는 이유와 그때 일어날 수 있는 상황들을 설명한다. 마지막으로 8-9장에서는 AGI 개발을 저지하기 위한 구체적인 제안을 제시한다.

[PDF 다운로드](https://keepthefuturehuman.ai/wp-content/uploads/2025/03/Keep_the_Future_Human__AnthonyAguirre__5March2025.pdf)

총 읽기 시간: 2-3시간

## 요약

본 에세이의 개괄적 개요. 시간이 부족하다면 10분 만에 모든 핵심 요점을 파악할 수 있다.

지난 10년간(좁은 용도의 AI) 그리고 최근 몇 년간(범용 AI)의 인공지능 분야의 극적인 발전은 AI를 틈새 학술 분야에서 세계 최대 기업들의 핵심 사업 전략으로 변화시켰으며, AI 역량 향상을 위한 기법과 기술에 대한 연간 투자가 수천억 달러에 달하고 있다.

이제 우리는 중요한 기로에 서 있다. 새로운 AI 시스템의 역량이 많은 인지 영역에서 인간의 능력과 대등해지고 이를 능가하기 시작함에 따라, 인류는 결정해야 한다: 얼마나 멀리, 그리고 어떤 방향으로 나아갈 것인가?

AI는 모든 기술과 마찬가지로 창조자를 위한 개선이라는 목표로 시작되었다. 하지만 우리의 현재 궤적과 암묵적 선택은 더욱 강력한 시스템을 향한 통제되지 않은 경쟁이며, 이는 현재 경제 활동과 인간 노동의 광범위한 영역을 자동화하려는 소수 거대 기술 기업들의 경제적 인센티브에 의해 추진되고 있다. 이 경쟁이 더 오래 계속된다면, 불가피한 승자가 있다: 바로 AI 그 자체 – 우리 경제에서, 우리 사고에서, 우리 결정에서, 그리고 결국 우리 문명의 통제권에서 인간보다 빠르고, 더 똑똑하고, 더 저렴한 대안 말이다.

하지만 우리는 다른 선택을 할 수 있다: 정부를 통해 AI 개발 과정을 통제하여 명확한 한계, 넘지 않을 선, 그리고 단순히 하지 않을 일들을 부과할 수 있다 – 핵 기술, 대량살상무기, 우주 무기, 환경 파괴적 과정, 인간의 생물공학적 개조, 그리고 우생학에 대해 해왔듯이. 가장 중요한 것은, AI가 인간을 대체하고 결국 인간을 밀어내는 새로운 종이 아니라, 인간을 강화하는 도구로 남도록 보장할 수 있다는 것이다.

이 에세이는 인간보다 똑똑한 자율적 범용 AI – 때로 "AGI"라고 불리는 – 특히 때로 "초지능"이라고 불리는 고도로 초인간적인 버전으로 가는 "관문"을 차단함으로써 *미래를 인간의 것으로 유지*해야 한다고 주장한다. 대신, 개인을 강화하고 인간 사회가 가장 잘하는 일을 할 수 있는 능력을 혁신적으로 개선할 수 있는 강력하고 신뢰할 수 있는 AI 도구에 집중해야 한다. 이 논증의 구조를 간략히 살펴보자.

### AI는 다르다

AI 시스템은 다른 기술들과 근본적으로 다르다. 전통적인 소프트웨어가 정확한 지시를 따르는 반면, AI 시스템은 명시적으로 방법을 알려주지 않아도 목표를 달성하는 방법을 학습한다. 이것이 AI를 강력하게 만든다: 목표나 성공 지표를 명확하게 정의할 수 있다면, 대부분의 경우 AI 시스템이 이를 달성하는 방법을 학습할 수 있다. 하지만 이것은 또한 AI를 본질적으로 예측 불가능하게 만든다: 목표 달성을 위해 어떤 행동을 취할지 확실하게 결정할 수 없다.

AI는 또한 대부분 설명 불가능하다: 부분적으로는 코드이지만, 주로 해석할 수 없는 거대한 숫자 집합 – 신경망 "가중치"들 – 으로 구성되어 있어 분석할 수 없다. 우리는 생물학적 뇌 내부를 들여다보며 생각을 파악하는 것만큼이나 AI의 내부 작동을 이해하는 데 어려움을 겪고 있다.

디지털 신경망을 훈련시키는 이러한 핵심 방식은 급속도로 복잡해지고 있다. 가장 강력한 AI 시스템들은 특수 하드웨어를 사용하여 거대한 데이터셋에서 신경망을 훈련시키는 대규모 연산 실험을 통해 만들어지며, 이후 소프트웨어 도구와 상부구조로 강화된다.

이로 인해 텍스트와 이미지를 생성하고 처리하며, 수학적·과학적 추론을 수행하고, 정보를 취합하고, 방대한 인간 지식 저장소를 상호작용적으로 조회하는 매우 강력한 도구들이 만들어졌다.

불행히도, 더 강력하고 신뢰할 수 있는 기술 도구의 개발이 우리가 *해야 할* 일이고 거의 모든 사람이 원하고 원한다고 말하는 것이지만, 이것은 우리가 실제로 향하고 있는 궤적이 아니다.

### AGI와 초지능

분야의 시작부터 AI 연구는 다른 목표에 집중해왔다: 인공일반지능. 이러한 집중은 이제 AI 개발을 주도하는 거대 기업들의 초점이 되었다.

AGI란 무엇인가? 흔히 "인간 수준의 AI"로 막연하게 정의되지만, 이는 문제가 있다: 어떤 인간들이며, 어떤 역량에서 인간 수준인가? 그리고 이미 가지고 있는 초인간적 역량은 어떻게 할 것인가? AGI를 이해하는 더 유용한 방법은 세 가지 핵심 속성의 교집합을 통해서다: 높은 **자율성**(행동의 독립성), 높은 **범용성**(광범위한 범위와 적응성), 그리고 높은 **지능**(인지 과업에서의 역량). 현재 AI 시스템은 고도로 유능하지만 좁거나, 범용적이지만 지속적인 인간 감독이 필요하거나, 자율적이지만 범위가 제한적일 수 있다.

완전한 A-G-I는 이 세 가지 속성을 모두 최고 인간 역량과 동등하거나 이를 능가하는 수준으로 결합할 것이다. 중요한 것은, 인간을 그토록 효과적이고 현재 소프트웨어와 다르게 만드는 것이 바로 이 조합이라는 것이다. 이것은 또한 사람들이 디지털 시스템으로 완전히 대체될 수 있게 하는 것이기도 하다.

인간의 지능이 특별하긴 하지만, 결코 한계는 아니다. 인공 "초지능" 시스템은 수백 배 더 빠르게 작동하고, 훨씬 더 많은 데이터를 분석하며, 엄청난 양을 동시에 "마음에" 담아둘 수 있고, 인간 집단보다 훨씬 크고 효과적인 집합체를 형성할 수 있다. 이들은 개인이 아니라 기업, 국가, 또는 우리 문명 전체를 대체할 수 있다.

### 우리는 문턱에 서 있다

AGI가 *가능하다*는 강력한 과학적 합의가 있다. AI는 이미 최근의 고수준 추론과 문제 해결을 포함하여 지적 능력의 많은 일반적 테스트에서 인간 성능을 능가하고 있다. 뒤처진 역량들 – 지속적 학습, 계획, 자기 인식, 독창성 등 – 모두 현재 AI 시스템에서 어느 정도 존재하며, 이 모든 것을 개선할 가능성이 높은 알려진 기법들이 존재한다.

몇 년 전까지만 해도 많은 연구자들이 AGI를 수십 년 앞의 일로 보았지만, 현재 AGI의 짧은 시간표에 대한 증거는 강력하다:

- 경험적으로 검증된 "스케일링 법칙"이 연산 투입과 AI 역량을 연결하며, 기업들은 향후 몇 년간 연산 투입을 몇 자릿수씩 확장할 궤도에 있다. AI 발전에 투입되는 인적·재정적 자원은 이제 십여 개의 맨해튼 프로젝트와 여러 아폴로 프로젝트에 맞먹는다.
- AI 기업들과 그 리더들은 AGI(어떤 정의에 의한)가 몇 년 내에 달성 가능하다고 공개적으로나 사적으로 믿고 있다. 이 기업들은 대중이 모르는 정보를 가지고 있으며, 일부는 차세대 AI 시스템을 손에 쥐고 있다.
- 입증된 실적을 가진 전문 예측가들은 AGI(어떤 정의에 의한)가 1-2년 내에 도착할 확률을 25%, 2-5년 내에 도착할 확률을 50%로 평가한다(Metaculus의 ['약한'](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) 및 ['완전한'](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/) AGI 예측 참조).
- 자율성(장기적 유연한 계획 포함)이 AI 시스템에서 뒤처져 있지만, 주요 기업들은 이제 자율 AI 시스템 개발에 방대한 자원을 집중하고 있으며, 2025년을 비공식적으로 ["에이전트의 해"](https://techinformed.com/2025-informed-the-year-of-agentic-ai/)라고 명명했다.
- AI가 자체 개선에 점점 더 기여하고 있다. AI 시스템이 AI 연구를 하는 인간 AI 연구자만큼 유능해지면, 훨씬 더 강력한 AI 시스템으로의 빠른 진보를 위한 임계점이 도달될 것이며, 이는 AI 역량의 폭주로 이어질 가능성이 높다. (논쟁의 여지가 있지만, 그 폭주는 이미 시작되었다.)

인간보다 똑똑한 AGI가 수십 년 또는 그 이상 앞의 일이라는 생각은 더 이상 해당 분야 전문가들의 압도적 다수에게 유지 가능하지 않다. 현재의 논쟁은 이 과정을 계속한다면 몇 개월 또는 몇 년이 걸릴지에 관한 것이다. 우리가 직면한 핵심 질문은: 그렇게 해야 하는가?

### AGI로의 경쟁을 이끄는 것

AGI로의 경쟁은 여러 힘에 의해 추진되고 있으며, 각각이 상황을 더욱 위험하게 만들고 있다. 주요 기술 기업들은 AGI를 궁극적인 자동화 기술로 보고 있다 – 인간 근로자를 단순히 보강하는 것이 아니라 대부분 또는 전체적으로 대체하는 것. 기업들에게 상금은 엄청나다: 인간 노동 비용을 자동화함으로써 세계의 연간 100조 달러 경제 생산의 상당 부분을 차지할 기회다.

국가들은 이 경쟁에 참여할 수밖에 없다고 느끼며, 공개적으로는 경제적·과학적 리더십을 내세우지만, 사적으로는 AGI를 핵무기에 필적하는 군사적 혁신으로 보고 있다. 경쟁자들이 결정적인 전략적 우위를 얻을 수 있다는 두려움이 전형적인 군비 경쟁 역학을 만들어낸다.

초지능을 추구하는 사람들은 종종 장대한 비전을 인용한다: 모든 질병 치료, 노화 역전, 에너지와 우주 여행의 돌파구, 또는 초인간적 계획 역량 창조.

덜 관대하게 말하면, 경쟁을 이끄는 것은 권력이다. 각 참가자 – 기업이든 국가든 – 는 지능이 권력과 같다고 믿으며, 자신이 그 권력의 최고 관리자가 될 것이라고 믿는다.

나는 이러한 동기들이 실제적이지만 근본적으로 잘못 인도된 것이라고 주장한다: AGI는 권력을 부여하기보다는 권력을 *흡수*하고 *추구*할 것이다. AI가 만든 기술들 또한 강력하게 양날의 검이 될 것이며, 유익한 경우에는 AI 도구로 그리고 AGI 없이도 만들어질 수 있다. 그리고 AGI와 그 산출물이 통제 하에 남아있는 한에서도, 이러한 경쟁 역학 – 기업적이든 지정학적이든 – 은 결정적으로 중단되지 않는 한 우리 사회에 대한 대규모 위험을 거의 불가피하게 만든다.

### AGI와 초지능은 문명에 극적인 위협을 가한다

매력에도 불구하고, AGI와 초지능은 여러 상호 강화 경로를 통해 문명에 극적인 위협을 가한다:

*권력 집중:* 초인간 AI는 거대한 사회적·경제적 활동 영역을 소수의 거대 기업이 운영하는 AI 시스템(이는 결국 정부를 인수하거나 사실상 정부에 인수당할 수 있다)으로 흡수함으로써 인류의 압도적 다수를 무력화할 수 있다.

*대규모 혼란:* 대부분의 인지 기반 일자리의 대량 자동화, 현재 인식론적 시스템의 대체, 그리고 방대한 수의 활동적 비인간 에이전트 배치는 상대적으로 짧은 기간 내에 현재 문명 시스템의 대부분을 뒤엎을 것이다.

*재난:* 새로운 군사적·파괴적 기술을 창조하는 능력을 – 잠재적으로 인간 수준 이상으로 – 확산시키고 이를 책임을 근거하는 사회적·법적 시스템에서 분리함으로써, 대량살상무기로 인한 물리적 재난이 극적으로 더 가능해진다.

*지정학과 전쟁:* 주요 세계 강국들은 "결정적인 전략적 우위"를 제공할 수 있는 기술이 적대국에 의해 개발되고 있다고 느낀다면 가만히 앉아있지 않을 것이다.

*폭주와 통제 상실:* 특별히 방지되지 않는 한, 초인간 AI는 자신을 더욱 개선할 모든 인센티브를 가질 것이며 속도, 데이터 처리, 사고의 정교함에서 인간을 훨씬 능가할 수 있다. 그러한 시스템을 우리가 통제할 의미 있는 방법은 없다. 그런 AI는 인간에게 권력을 부여하지 않을 것이다. 우리가 그것에게 권력을 부여하거나, 아니면 그것이 권력을 가져갈 것이다.

이러한 위험의 상당수는 기술적 "정렬" 문제 – 고급 AI가 인간이 원하는 것을 확실히 하도록 만드는 것 – 가 해결되더라도 남아있다. AI는 어떻게 관리될지에 대한 엄청난 도전을 제시하며, 인간 지능이 돌파되면서 이 관리의 매우 많은 측면이 믿을 수 없도록 어렵거나 다루기 어려워진다.

가장 근본적으로, 현재 추구되고 있는 초인간 범용 AI는 그 본질상 우리 자신의 목표, 주체성, 역량을 초과하는 것들을 가질 것이다. 그것은 본질적으로 통제 불가능할 것이다 – 우리가 이해할 수도 예측할 수도 없는 것을 어떻게 통제할 수 있겠는가? 그것은 인간이 사용하는 기술 도구가 아니라, 지구상에서 우리와 함께하는 두 번째 지능 종이 될 것이다. 더 나아가도록 허용된다면, 그것은 단순히 두 번째 종이 아니라 대체 종을 구성할 것이다.

아마도 그것은 우리를 잘 대할 수도, 그렇지 않을 수도 있다. 하지만 미래는 우리가 아닌 그것의 것이 될 것이다. 인간의 시대는 끝날 것이다.

### 이것은 불가피하지 않다. 인류는 매우 구체적으로 우리의 대체물을 만들지 않기로 결정할 수 있다.

초인간 AGI의 창조는 불가피하지 않다. 우리는 조정된 거버넌스 조치를 통해 이를 방지할 수 있다:

첫째, 대규모 AI 시스템의 근본적 가능 조건이자 통제 수단인 AI 연산("연산량")에 대한 견고한 회계와 감독이 필요하다. 이는 차례로 AI 모델 훈련과 운영에 사용되는 총 연산량에 대한 표준화된 측정과 보고, 그리고 사용된 연산을 집계, 인증, 검증하는 기술적 방법을 요구한다.

둘째, 훈련과 운영 모두에서 AI 연산에 대한 엄격한 상한선을 구현해야 한다. 이는 AI가 너무 강력해지고 너무 빠르게 작동하는 것을 방지한다. 이러한 상한선은 법적 요구 사항과 현대 휴대폰의 보안 기능과 유사한 AI 전용 칩에 내장된 하드웨어 기반 보안 조치를 통해 구현될 수 있다. 전문 AI 하드웨어가 소수의 회사에서만 만들어지기 때문에, 기존 공급망을 통한 검증과 집행이 가능하다.

셋째, 가장 위험한 AI 시스템에 대한 강화된 책임이 필요하다. 높은 자율성, 광범위한 범용성, 우수한 지능을 결합한 AI를 개발하는 사람들은 피해에 대해 엄격한 책임을 져야 하며, 이 책임으로부터의 안전항은 더 제한적이고 통제 가능한 시스템의 개발을 장려할 것이다.

넷째, 위험 수준에 기반한 단계별 규제가 필요하다. 가장 유능하고 위험한 시스템은 개발과 배치 이전에 광범위한 안전성과 통제성 보장을 요구할 것이며, 덜 강력하거나 더 전문화된 시스템은 비례적인 감독을 받을 것이다. 이 규제 프레임워크는 결국 국가적 차원과 국제적 차원 모두에서 작동해야 한다.

이러한 접근법 – 전체 문서에서 상세한 명세가 주어진 – 은 실용적이다: 국제적 조정이 필요하겠지만, 전문 하드웨어 공급망을 통제하는 소수의 회사를 통해 검증과 집행이 작동할 수 있다. 이는 또한 유연하다: 기업들은 여전히 AI 개발에서 혁신하고 이익을 얻을 수 있으며, 단지 가장 위험한 시스템에 대한 명확한 한계만 있을 뿐이다.

AI 권력과 위험의 장기적 억제는 현재 핵무기 확산 통제가 하고 있듯이 자기 이익과 공동 이익에 기반한 국제 협정을 요구할 것이다. 하지만 우리는 더 포괄적인 거버넌스를 향해 구축하면서 강화된 감독과 책임으로 즉시 시작할 수 있다.

핵심적으로 빠진 요소는 AI 개발 과정을 통제하려는 정치적·사회적 의지다. 그 의지의 원천은, 제때 온다면, 현실 자체가 될 것이다 – 즉, 우리가 하고 있는 일의 진정한 함의에 대한 광범위한 인식에서 말이다.

### 우리는 인류를 강화하는 도구형 AI를 설계할 수 있다

통제 불가능한 AGI를 추구하기보다는, 의미 있는 인간 통제 하에 남으면서 인간 역량을 강화하는 강력한 "도구형 AI"를 개발할 수 있다. 도구형 AI 시스템은 그 역량에 상응하는 수준에서 통제 가능하도록 설계하는 한, 높은 자율성, 광범위한 범용성, 초인간 지능의 위험한 삼중 교집점을 피하면서도 극도로 유능할 수 있다. 이들은 또한 인간 감독을 유지하면서 혁신적 이익을 제공하는 정교한 시스템으로 결합될 수 있다.

도구형 AI는 의학을 혁명화하고, 과학적 발견을 가속화하며, 교육을 향상시키고, 민주적 과정을 개선할 수 있다. 적절히 통제될 때, 그것은 인간 전문가와 기관을 대체하기보다는 더 효과적으로 만들 수 있다. 그러한 시스템은 여전히 고도로 파괴적이며 신중한 관리가 필요하겠지만, 그들이 제기하는 위험은 AGI의 위험과 근본적으로 다르다: 그것들은 인간 주체성과 문명에 대한 실존적 위협이 아니라, 다른 강력한 기술들의 위험처럼 우리가 통제할 수 있는 위험들이다. 그리고 중요하게도, 현명하게 개발될 때, AI 도구는 사람들이 강력한 AI를 통제하고 그 효과를 관리하는 데 도움을 줄 수 있다.

이 접근법은 AI가 어떻게 개발되고 그 이익이 어떻게 분배되는지를 모두 재고하기를 요구한다. 공공 및 비영리 AI 개발의 새로운 모델, 견고한 규제 프레임워크, 그리고 경제적 이익을 더 광범위하게 분배하는 메커니즘이 AI가 소수의 손에 권력을 집중시키기보다는 인류 전체를 강화하도록 보장하는 데 도움을 줄 수 있다. AI 자체가 더 나은 사회적·통치 기관을 구축하는 데 도움을 줄 수 있으며, 인간 사회를 약화시키기보다는 강화하는 새로운 형태의 조정과 담론을 가능하게 한다. 국가 보안 기관은 그들의 전문성을 활용하여 AI 도구 시스템을 진정으로 안전하고 신뢰할 수 있으며, 국가 권력뿐만 아니라 진정한 방어의 원천으로 만들 수 있다.

우리는 결국 도구보다는 덜 도구적이고 – 우리가 희망할 수 있듯이 – 현명하고 강력한 후원자에 더 가까운 더욱 강력하고 더욱 자주적인 시스템을 개발하기로 선택할 수도 있다. 하지만 우리는 그렇게 하기 위한 과학적 이해와 거버넌스 역량을 안전하게 개발한 후에만 그렇게 해야 한다. 그러한 중대하고 되돌릴 수 없는 결정은 기술 회사와 국가 간의 경쟁에서 기본값으로가 아니라, 인류 전체가 의도적으로 내려야 한다.

### 인간의 손에

사람들은 AI에서 나오는 좋은 것을 원한다: 자신을 강화하고, 경제적 기회와 성장을 가속화하며, 과학, 기술, 교육에서의 돌파구를 약속하는 유용한 도구들. 왜 원하지 않겠는가? 하지만 질문을 받으면, 일반 대중의 압도적 다수는 [더 느리고 더 신중한 AI 개발](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation)을 원하며, 직장과 다른 곳에서 자신들을 대체하고, 그들의 문화와 정보 공유지를 비인간적 콘텐츠로 채우며, 권력을 극소수의 기업에 집중시키고, 극심한 대규모 글로벌 위험을 제기하며, 결국 그들의 종을 무력화하거나 대체할 위협을 가하는 인간보다 똑똑한 AI를 원하지 않는다. 왜 그러겠는가?

우리는 *하나 없이 다른 하나를 가질 수* 있다. 그것은 우리의 운명이 어떤 기술의 가정된 불가피성이나 실리콘밸리의 몇몇 CEO들의 손에 있지 않고, 우리가 그것을 붙잡는다면 우리의 나머지 손에 있다고 결정하는 것으로 시작된다. 관문을 차단하고, 미래를 인간의 것으로 유지하자.

## 제1장 - 서론

인간보다 똑똑한 AI의 전망에 우리가 어떻게 대응할 것인가는 우리 시대의 가장 시급한 문제다. 이 에세이는 나아갈 길을 제시한다.

우리는 인간 시대의 끝자락에 있을지도 모른다.

지난 10년간 우리 종의 역사에서 유례없는 일이 시작되었다. 그 결과는 상당 부분 인류의 미래를 결정하게 될 것이다. 2015년 무렵부터 연구자들은 *특화된* 인공지능(AI) 개발에 성공했다. 바둑과 같은 게임에서 승리하고, 이미지와 음성을 인식하는 등의 작업을 어떤 인간보다도 잘 수행하는 시스템들 말이다.[^1]

이는 놀라운 성공이며, 인류에게 힘을 실어줄 극도로 유용한 시스템과 제품들을 만들어내고 있다. 하지만 특화된 인공지능은 결코 이 분야의 진정한 목표가 아니었다. 오히려 그 목표는 *범용* 목적 AI 시스템, 특히 "인공일반지능(AGI)"이나 "초지능"이라고 불리는 시스템을 만드는 것이었다. 이들은 AI가 현재 바둑, 체스, 포커, 드론 레이싱 등에서 인간을 압도하는 것처럼, 거의 *모든* 작업에서 동시에 인간과 같거나 더 나은 성능을 보이는 시스템들이다. 이것이 많은 주요 AI 기업들의 공언된 목표다.[^2]

*이러한 노력들도 성공하고 있다.* ChatGPT, Gemini, Llama, Grok, Claude, Deepseek 같은 범용 AI 시스템들은 막대한 연산량과 엄청난 데이터를 바탕으로 다양한 작업에서 일반적인 인간과 동등한 수준에 도달했으며, 일부 영역에서는 인간 전문가와도 견줄 만하다. 이제 일부 거대 기술 기업의 AI 엔지니어들은 이러한 거대한 기계 지능 실험을 다음 단계로 밀어붙이기 위해 경쟁하고 있다. 인간의 능력, 전문성, 자율성의 전 범위와 먼저 견주고, 그다음에는 이를 뛰어넘는 단계 말이다.

*이는 임박한 일이다.* 지난 10년간, 현재 경로를 계속 따를 경우 이것이 얼마나 걸릴지에 대한 전문가 추정치는 수십 년(또는 수백 년)에서 한 자릿수 년으로 줄어들었다.

이는 또한 획기적인 중요성과 초월적 위험을 지닌다. AGI 지지자들은 이를 과학적 문제를 해결하고, 질병을 치료하며, 새로운 기술을 개발하고, 단순 반복 작업을 자동화할 긍정적 변화로 본다. 그리고 AI는 분명 이 모든 것을 달성하는 데 도움이 될 수 있다. 실제로 이미 그렇게 하고 있다. 하지만 수십 년간 앨런 튜링부터 스티븐 호킹, 그리고 현재의 제프리 힌턴과 요슈아 벤지오[^3]에 이르기까지 많은 신중한 사상가들이 극명한 경고를 발해왔다. 진정으로 인간보다 똑똑하고, 범용적이며, 자율적인 AI를 구축하는 것은 최소한 사회를 완전히 그리고 돌이킬 수 없게 뒤엎을 것이고, 최악의 경우 인류 멸종을 초래할 것이라고 말이다.[^4]

초지능 AI는 현재 경로에서 빠르게 접근하고 있지만, 결코 불가피한 것은 아니다. 이 에세이는 우리가 왜 그리고 어떻게 이 다가오는 비인간적 미래에 대해 *관문을 차단*해야 하는지, 그리고 대신 무엇을 해야 하는지에 대한 확장된 논증이다.


[^1]: 이 [차트](https://time.com/6300942/ai-progress-charts/)는 일련의 작업들을 보여준다. 이 그래프에 많은 유사한 곡선들을 추가할 수 있다. 특화 AI에서의 이러한 급속한 진보는 해당 분야 전문가들조차 놀라게 했으며, 벤치마크들이 예측보다 몇 년 앞서 돌파되고 있다.

[^2]: Deepmind, OpenAI, Anthropic, X.ai는 모두 AGI 개발이라는 구체적 목표를 가지고 설립되었다. 예를 들어, OpenAI의 헌장은 "모든 인류에게 이익이 되는 인공일반지능" 개발을 목표로 명시적으로 밝히고 있으며, DeepMind의 사명은 "지능을 해결하고, 그다음 그것을 사용해 모든 것을 해결하는 것"이다. Meta, Microsoft 등도 현재 실질적으로 유사한 경로를 추구하고 있다. Meta는 AGI를 개발해 [공개적으로 배포할 계획이라고 밝혔다.](https://www.forbes.com/sites/johnkoetsier/2024/01/18/zuckerberg-on-ai-meta-building-agi-for-everyone-and-open-sourcing-it/)

[^3]: 힌턴과 벤지오는 가장 많이 인용되는 AI 연구자 중 두 명으로, 둘 다 AI 분야의 노벨상인 튜링상을 수상했으며, 힌턴은 추가로 (물리학) 노벨상까지 수상했다.

[^4]: 상업적 인센티브 하에서 거의 제로에 가까운 정부 감독으로 이런 위험을 지닌 무언가를 구축하는 것은 완전히 전례 없는 일이다. 이를 구축하는 사람들 사이에서조차 위험에 대한 논란이 없을 정도다! Deepmind, OpenAI, Anthropic의 리더들을 비롯한 많은 전문가들이 모두 고도 AI가 인류에게 *멸종 위험*을 제기한다는 [성명서](https://www.safe.ai/work/statement-on-ai-risk)에 문자 그대로 서명했다. 경보가 이보다 더 크게 울릴 수는 없으며, 이를 무시하는 사람들은 단순히 AGI와 초지능을 진지하게 받아들이지 않고 있다고 결론지을 수밖에 없다. 이 에세이의 목표 중 하나는 그들이 왜 진지하게 받아들여야 하는지 이해하도록 돕는 것이다.

## 2장 - AI 신경망에 대해 알아야 할 것들

현대 AI 시스템은 어떻게 작동하며, 차세대 AI에서는 무엇을 기대할 수 있을까?

더 강력한 AI 개발의 결과가 어떻게 전개될지 이해하려면 몇 가지 기본 사항을 체득하는 것이 필수적이다. 이번 장과 다음 두 장에서는 이러한 기본 사항들을 차례로 다룰 것이다: 현대 AI가 무엇인지, 어떻게 대규모 연산을 활용하는지, 그리고 어떤 의미에서 일반성과 능력이 급속히 성장하고 있는지 말이다.[^5]

인공지능을 정의하는 방법은 여러 가지가 있지만, 우리가 주목해야 할 AI의 핵심 특성은 다음과 같다. 표준 컴퓨터 프로그램이 작업 수행 방법에 대한 지시사항 목록인 반면, AI 시스템은 데이터나 경험으로부터 학습하여 *명시적으로 방법을 알려주지 않아도* 작업을 수행한다는 것이다.

현재 주목받는 거의 모든 현대 AI는 신경망을 기반으로 한다. 이는 매우 많은 수(수십억 또는 수조 개)의 숫자들("가중치")로 표현되는 수학적/연산적 구조로, 훈련 작업을 잘 수행한다. 이러한 가중치들은 신경망이 하나 이상의 작업을 잘 수행하도록 정의된 수치 점수("손실"이라고도 함)를 개선하도록 반복적으로 조정함으로써 제작(또는 "성장" 혹은 "발견")된다.[^6] 이 과정을 신경망 *훈련*이라고 한다.[^7]

이러한 훈련을 수행하는 기법들은 많지만, 그런 세부사항들보다는 점수가 어떻게 정의되는지, 그리고 그것이 신경망이 잘 수행하는 다양한 작업들로 어떻게 이어지는지가 훨씬 더 중요하다. 역사적으로 "협소한" AI와 "일반적인" AI 사이에는 핵심적인 구분이 있어왔다.

협소 AI는 특정 작업이나 소규모 작업 세트(이미지 인식이나 체스 게임 등)를 수행하도록 의도적으로 훈련된다. 새로운 작업을 위해서는 재훈련이 필요하며, 능력의 범위가 좁다. 우리는 이미 초인적인 협소 AI를 보유하고 있다. 즉, 사람이 할 수 있는 거의 모든 개별적이고 명확히 정의된 작업에 대해, 우리는 아마도 점수를 구성하고 인간보다 더 잘 수행하는 협소 AI 시스템을 성공적으로 훈련시킬 수 있다.

범용 AI(GPAI) 시스템은 명시적으로 훈련받지 않은 많은 작업을 포함해 광범위한 작업을 수행할 수 있으며, 운영 과정에서 새로운 작업을 학습할 수도 있다. ChatGPT와 같은 현재의 대규모 "멀티모달 모델"[^8]이 이를 잘 보여준다. 매우 방대한 텍스트와 이미지 말뭉치로 훈련된 이들은 복잡한 추론에 참여하고, 코드를 작성하고, 이미지를 분석하고, 광범위한 지적 작업을 지원할 수 있다. 아래에서 자세히 살펴보겠지만 여전히 인간 지능과는 상당히 다른 면이 있음에도 불구하고, 이들의 일반성은 AI에 혁명을 가져왔다.[^9]

### 예측불가능성: AI 시스템의 핵심 특징

AI 시스템과 기존 소프트웨어 사이의 핵심적인 차이점은 예측가능성에 있다. 표준 소프트웨어의 출력도 예측불가능할 수 있다. 실제로 때로는 그것이 우리가 소프트웨어를 작성하는 이유이기도 하다 - 우리가 예측할 수 없었던 결과를 얻기 위해서 말이다. 그러나 기존 소프트웨어는 프로그래밍되지 않은 일을 거의 하지 않는다. 그 범위와 행동은 일반적으로 설계된 대로이다. 최고 수준의 체스 프로그램은 인간이 예측할 수 없는 수를 둘 수 있지만(그렇지 않다면 그들이 그 체스 프로그램을 이길 수 있을 것이다!), 일반적으로 체스 게임 외의 다른 일은 하지 않는다.

기존 소프트웨어와 마찬가지로, 협소 AI도 예측가능한 범위와 행동을 가지지만 예측불가능한 결과를 낼 수 있다. 이는 사실상 협소 AI를 정의하는 또 다른 방식이다: 예측가능성과 운영 범위에서 기존 소프트웨어와 유사한 AI로 말이다.

범용 AI는 다르다: 그 범위(적용되는 영역), 행동(하는 일의 종류), 그리고 결과(실제 출력) 모두가 예측불가능할 수 있다.[^10] GPT-4는 단지 텍스트를 정확하게 생성하도록 훈련되었지만, 훈련자들이 예측하거나 의도하지 않은 많은 능력을 개발했다. 이러한 예측불가능성은 훈련의 복잡성에서 비롯된다: 훈련 데이터에 많은 다양한 작업의 출력이 포함되어 있기 때문에, AI는 잘 예측하기 위해 이러한 작업들을 수행하는 법을 효과적으로 학습해야 한다.

범용 AI 시스템의 이러한 예측불가능성은 상당히 근본적이다. 원칙적으로는 행동에 대한 보장된 한계를 가진 AI 시스템을 신중하게 구성하는 것이 가능하지만(이 글의 뒷부분에서 언급되듯이), 현재 AI 시스템이 만들어지는 방식에서는 실제로 그리고 심지어 원칙적으로도 예측불가능하다.

### 수동적 AI, 에이전트, 자율 시스템, 그리고 정렬

이러한 예측불가능성은 AI 시스템이 실제로 다양한 목표를 달성하기 위해 어떻게 배포되고 사용되는지를 고려할 때 특히 중요해진다.

많은 AI 시스템은 주로 정보를 제공하고 사용자가 행동을 취한다는 의미에서 상대적으로 수동적이다. 반면 일반적으로 *에이전트*라고 불리는 다른 시스템들은 사용자의 다양한 수준의 개입과 함께 스스로 행동을 취한다. 상대적으로 외부 입력이나 감독이 적은 상태에서 행동을 취하는 시스템들은 더 *자율적*이라고 할 수 있다. 이는 수동적 도구에서 자율 에이전트까지 행동의 독립성 측면에서 스펙트럼을 형성한다.[^11]

AI 시스템의 목표에 관해서는, 이들이 훈련 목표와 직접적으로 연결될 수도 있다(예: 바둑 게임 시스템의 "승리" 목표는 또한 명시적으로 그것이 훈련받은 것이기도 하다). 또는 그렇지 않을 수도 있다: ChatGPT의 훈련 목표는 부분적으로는 텍스트 예측이고, 부분적으로는 도움이 되는 조수가 되는 것이다. 그러나 특정 작업을 수행할 때, 그 목표는 사용자가 제공한다. 목표는 또한 AI 시스템 자체에 의해 생성될 수도 있으며, 그 훈련 목표와는 매우 간접적으로만 관련될 수도 있다.[^12]

목표는 "정렬"의 문제, 즉 AI 시스템이 *우리가 원하는 것을 할 것인가*의 문제와 밀접하게 연결되어 있다. 이 간단한 질문은 엄청난 수준의 미묘함을 숨기고 있다.[^13] 우선, 이 문장에서 "우리"는 많은 다른 사람들과 그룹을 지칭할 수 있어, 다양한 유형의 정렬로 이어진다는 점을 주목하자. 예를 들어, AI가 사용자에게 매우 *순종적*(또는 ["충성스러운"](https://arxiv.org/abs/2003.11157))일 수 있다 - 여기서 "우리"는 "우리 각자"이다. 또는 더 *주권적*일 수 있어, 주로 자신의 목표와 제약에 의해 움직이지만 여전히 인간 복지의 공통 이익을 위해 광범위하게 행동할 수 있다 - 그러면 "우리"는 "인류" 또는 "사회"가 된다. 그 중간에는 AI가 대체로 순종적이지만 다른 사람이나 사회에 해를 끼치거나 법을 위반하는 등의 행동은 거부할 수 있는 스펙트럼이 있다.

이 두 축 - 자율성의 수준과 정렬의 유형 - 은 완전히 독립적이지 않다. 예를 들어, 주권적이면서 수동적인 시스템은 완전히 자기모순적이지는 않지만 긴장 상태에 있는 개념이며, 순종적인 자율 에이전트도 마찬가지다.[^14] 자율성과 주권성이 함께 가는 경향이 있다는 것은 명확한 의미가 있다. 비슷한 맥락에서, 예측가능성은 "수동적"이고 "순종적인" AI 시스템에서 더 높은 경향이 있는 반면, 주권적이거나 자율적인 시스템들은 더 예측불가능한 경향이 있을 것이다. 이 모든 것이 잠재적인 인공일반지능 (AGI)과 초지능의 파급효과를 이해하는 데 중요할 것이다.

어떤 형태든 진정으로 정렬된 AI를 만들려면 세 가지 뚜렷한 도전을 해결해야 한다:

1. "우리"가 무엇을 원하는지 이해하기 - 이는 "우리"가 특정 개인이나 조직(충성)을 의미하든 인류 전체(주권)를 의미하든 복잡하다;
2. 그러한 바람에 따라 정기적으로 행동하는 시스템 구축하기 - 본질적으로 일관된 긍정적 행동 만들기;
3. 가장 근본적으로는, 단순히 그런 것처럼 행동하는 것이 아니라 그러한 바람에 대해 진정으로 "관심을 갖는" 시스템 만들기.

신뢰할 수 있는 행동과 진정한 관심 사이의 구별은 중요하다. 인간 직원이 조직의 사명에 대한 진정한 헌신 없이도 명령을 완벽하게 따를 수 있는 것처럼, AI 시스템도 진정으로 인간의 선호를 가치 있게 여기지 않으면서도 정렬된 것처럼 행동할 수 있다. 우리는 피드백을 통해 AI 시스템이 말하고 행동하도록 훈련시킬 수 있으며, 이들은 인간이 원하는 것에 대해 추론하는 법을 학습할 수 있다. 그러나 이들이 인간의 선호를 *진정으로* 가치 있게 여기도록 만드는 것은 훨씬 더 깊은 도전이다.[^15]

이러한 정렬 도전들을 해결하는 데 있는 심오한 어려움들과 AI 위험에 대한 그 함의는 아래에서 더 자세히 탐구할 것이다. 우선은 정렬이 단순히 AI 시스템에 덧붙이는 기술적 특징이 아니라, 인류와의 관계를 형성하는 아키텍처의 근본적 측면이라는 것을 이해하자.


[^5]: 기계학습과 AI, 특히 언어모델에 대한 친근하면서도 기술적인 소개는 [이 사이트](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e)를 참조하라. AI 멸종 위험에 대한 또 다른 현대적 입문서는 [이 글](https://www.thecompendium.ai/)을 보라. AI 안전 상태에 대한 포괄적이고 권위 있는 과학적 분석은 최근의 [국제 AI 안전 보고서](https://arxiv.org/abs/2501.17805)를 참조하라.

[^6]: 훈련은 일반적으로 모델 가중치가 주어진 고차원 공간에서 점수의 국소 최대값을 찾는 것으로 이루어진다. 가중치가 조정될 때 점수가 어떻게 변하는지 확인함으로써, 훈련 알고리즘은 점수를 가장 많이 개선하는 조정을 식별하고 가중치를 그 방향으로 이동시킨다.

[^7]: 예를 들어, 이미지 인식 문제에서 신경망은 이미지의 라벨에 대한 확률을 출력할 것이다. 점수는 AI가 정답에 부여하는 확률과 관련이 있을 것이다. 그러면 훈련 절차는 다음번에 AI가 해당 이미지의 올바른 라벨에 대해 더 높은 확률을 출력하도록 가중치를 조정할 것이다. 이는 그다음 엄청난 횟수만큼 반복된다. 더 복잡한 점수 매김 메커니즘을 사용하긴 하지만, 본질적으로 모든 현대 신경망 훈련에서 동일한 기본 절차가 사용된다.

[^8]: 대부분의 멀티모달 모델은 "트랜스포머" 아키텍처를 사용하여 여러 유형의 데이터(텍스트, 이미지, 소리)를 처리하고 생성한다. 이들은 모두 다양한 유형의 "토큰"으로 분해되어 동일한 기반에서 취급될 수 있다. 멀티모달 모델은 먼저 대규모 데이터셋 내에서 토큰을 정확히 예측하도록 훈련된 다음, 능력을 향상시키고 행동을 형성하기 위해 강화학습을 통해 세련화된다.

[^9]: 언어모델이 한 가지 일을 하도록 훈련된다는 것 - 단어 예측 - 때문에 일부는 이를 협소 AI라고 부른다. 그러나 이는 오해의 소지가 있다: 텍스트를 잘 예측하려면 매우 많은 다양한 능력이 필요하기 때문에, 이 훈련 작업은 놀랍도록 일반적인 시스템으로 이어진다. 또한 이러한 시스템들이 강화학습에 의해 광범위하게 훈련된다는 점도 주목하라. 이는 효과적으로 수천 명의 사람들이 모델이 하는 많은 일 중 어느 것이든 잘 할 때 보상 신호를 주는 것을 나타낸다. 그러면 이는 이러한 피드백을 주는 사람들로부터 상당한 일반성을 물려받는다.

[^10]: AI가 예측불가능한 방식은 여러 가지가 있다. 하나는 일반적으로 알고리즘을 실제로 실행하지 않고는 그것이 무엇을 할지 예측할 수 없다는 것이다; 이에 대한 [정리들](https://arxiv.org/abs/1310.3225)이 있다. 이는 단순히 알고리즘의 출력이 복잡할 수 있기 때문일 수 있다. 그러나 예측이 예측자가 가지지 못한 능력(AI를 이기는 것)을 함의하는 경우(체스나 바둑에서처럼)에는 특히 명확하고 관련이 있다. 둘째, 주어진 AI 시스템은 동일한 입력이 주어져도 항상 동일한 출력을 생성하지는 않을 것이다 - 그 출력에는 무작위성이 포함되어 있다; 이것 또한 알고리즘적 예측불가능성과 결합된다. 셋째, 예상치 못한 창발적 능력들이 훈련으로부터 발생할 수 있어, AI 시스템이 할 수 있고 할 일의 *유형들*조차 예측불가능하다는 의미이다; 이 마지막 유형이 안전 고려사항에 특히 중요하다.

[^11]: "자율 에이전트"가 의미하는 바에 대한 심화 검토(그리고 이를 구축하는 것에 대한 윤리적 논증)는 [여기](https://arxiv.org/abs/2502.02649)를 참조하라.

[^12]: "AI는 자체 목표를 가질 수 없다"는 말을 들을 수도 있다. 이는 완전한 말도 안 되는 소리다. AI가 결코 주어지지 않았고 자신만이 알고 있는 목표를 가지거나 개발하는 예를 생성하는 것은 쉽다. 현재의 인기 있는 멀티모달 모델에서 이를 많이 보지 못하는 이유는 그것이 훈련에서 제거되기 때문이다; 그것을 훈련에 포함시키는 것도 똑같이 쉬울 것이다.

[^13]: 방대한 문헌이 있다. 일반적인 문제에 대해서는 Christian의 [*The Alignment Problem*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821)과 Russell의 [*Human-Compatible*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616)을 보라. 더 기술적인 측면에서는 예를 들어 [이 논문](https://arxiv.org/abs/2209.00626)을 참조하라.

[^14]: 그러한 시스템들이 추세에 역행한다고 해서 실제로는 매우 흥미롭고 유용하게 만든다는 것을 나중에 보게 될 것이다.

[^15]: 이는 감정이나 감각을 요구한다는 말이 아니다. 오히려, 시스템의 외부에서 그것의 내적 목표, 선호, 가치가 무엇인지 아는 것은 엄청나게 어렵다. 여기서 "진정한"이란 중요한 시스템의 경우 우리의 생명을 걸 수 있을 정도로 그것에 의존할 충분히 강한 이유를 가지고 있다는 뜻이다.

## 3장 - 현대 일반 AI 시스템의 핵심 제작 방식

전 세계에서 가장 첨단의 AI 시스템들은 놀랍도록 유사한 방법으로 만들어진다. 여기 그 기본 원리들을 소개한다.

인간을 제대로 이해하려면 생물학, 진화, 육아 등에 대해 알아야 하듯이, AI를 이해하려면 그것이 어떻게 만들어지는지 알아야 한다. 지난 5년간 AI 시스템은 능력과 복잡성 측면에서 엄청나게 발전했다. 이를 가능하게 한 핵심 요인은 대규모 연산량(AI 분야에서는 흔히 "컴퓨트"라고 부른다)의 가용성이었다.

그 규모는 놀랍다. GPT 시리즈, Claude, Gemini 등과 같은 모델을 훈련하는 데는 약 10<sup>25</sup>-10<sup>26</sup>번의 "부동소수점 연산"(FLOP)[^16]이 사용된다.[^17] (비교해보면, 지구상 모든 인간이 5초에 한 번씩 계산을 쉬지 않고 계속한다면, 이를 완수하는 데 약 10억 년이 걸릴 것이다.) 이 어마어마한 연산량 덕분에 수조 개의 모델 가중치를 가진 모델을 테라바이트 규모의 데이터로 훈련할 수 있게 되었다. 이 데이터는 지금까지 작성된 양질의 텍스트 대부분과 방대한 음성, 이미지, 동영상 라이브러리를 포함한다. 여기에 인간의 선호도와 우수한 작업 수행을 강화하는 추가적인 광범위한 훈련을 더하면, 이런 방식으로 훈련된 모델들은 추론과 문제해결을 포함한 다양한 기본적 지적 작업에서 인간과 경쟁할 만한 성능을 보여준다.

또한 이러한 시스템의 *추론* 속도[^18]가 인간의 텍스트 처리 *속도*와 맞먹으려면 초당 얼마나 많은 연산 속도가 필요한지도 (아주 대략적으로) 알고 있다. 약 10<sup>15</sup>-10<sup>16</sup> FLOP/초 정도이다.[^19]

강력하긴 하지만, 이러한 모델들은 본질적으로 핵심적인 한계를 지니고 있다. 마치 개별 인간이 멈춰서 생각하거나 추가 도구를 사용하지 못한 채 분당 일정한 속도로 텍스트만 출력해야 한다면 제약을 받는 것과 매우 유사하다. 보다 최근의 AI 시스템들은 여러 핵심 요소를 결합한 더 복잡한 프로세스와 아키텍처를 통해 이러한 한계를 해결한다:

- 하나 또는 여러 개의 신경망. 하나의 모델이 핵심 인지 능력을 제공하고, 최대 여러 개의 다른 모델들이 보다 특화된 작업을 수행한다.
- 모델이 제공받고 사용할 수 있는 *도구* - 예를 들어 웹 검색, 문서 생성이나 편집, 프로그램 실행 등의 능력
- 신경망의 입력과 출력을 연결하는 *스캐폴딩*. 매우 간단한 스캐폴드는 AI 모델의 두 "인스턴스"가 서로 대화하거나, 하나가 다른 하나의 작업을 검토할 수 있게 해줄 수 있다.[^20]
- *사고의 연쇄*와 관련 프롬프팅 기법들도 비슷한 역할을 한다. 모델이 예를 들어 문제에 대한 여러 접근법을 생성한 다음, 이러한 접근법들을 처리해서 종합적인 답을 도출하게 만든다.
- 도구, 스캐폴딩, 사고의 연쇄를 더 잘 활용하도록 모델을 *재훈련*한다.

이러한 확장 기능들은 매우 강력할 수 있고(AI 시스템 자체를 포함하기도 한다), 이러한 복합 시스템들은 상당히 정교해져서 AI 능력을 극적으로 향상시킬 수 있다.[^21] 최근에는 스캐폴딩과 특히 사고의 연쇄 프롬프팅 기법들(그리고 그 결과를 모델 재훈련에 반영하여 이를 더 잘 활용하게 하는 것)이 [o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/), [DeepSeek R1](https://api-docs.deepseek.com/news/news250120)에서 개발되고 활용되어, 주어진 질의에 대해 여러 차례의 추론을 수행한다.[^22] 이는 사실상 모델이 응답에 대해 "생각할" 수 있게 해주며, 과학, 수학, 프로그래밍 작업에서 이러한 모델들의 고수준 추론 능력을 극적으로 향상시킨다.[^23]

주어진 AI 아키텍처에서 훈련 연산량의 증가는 명확하게 정의된 지표들의 개선으로 [안정적으로 변환될 수 있다](https://arxiv.org/abs/2405.10938). 덜 명확하게 정의된 일반적 능력들(아래에서 논의되는 것들과 같은)의 경우, 그 변환은 덜 명확하고 예측하기 어렵지만, 더 많은 훈련 연산량을 가진 더 큰 모델들이 새롭고 더 나은 능력을 갖게 될 것은 거의 확실하다. 비록 그것이 무엇인지 예측하기는 어렵더라도 말이다.

마찬가지로, 복합 시스템들과 특히 "사고의 연쇄"의 발전(그리고 이것과 잘 작동하는 모델들의 훈련)은 *추론* 연산량에서의 스케일링을 가능하게 했다: 주어진 훈련된 핵심 모델에 대해, 복잡한 문제에 대해 "더 깊고 오래 생각할" 수 있게 해주는 더 많은 연산량이 적용될 때 적어도 일부 AI 시스템 능력이 증가한다. 이는 인간 수준의 성능에 맞추려면 수백 배 또는 수천 배 더 많은 FLOP/초가 필요하다는 가파른 연산 속도 비용을 수반한다.[^24]

급속한 AI 발전을 이끄는 요인의 일부에 불과하지만,[^25] 연산량의 역할과 복합 시스템의 가능성은 통제 불가능한 AGI를 방지하고 더 안전한 대안을 개발하는 데 모두 중요한 것으로 판명될 것이다.

[^16]: 10<sup>27</sup>는 1 뒤에 25개의 0이 따라오는 수, 즉 천조의 천조를 의미한다. FLOP은 단순히 어느 정도 정밀도를 가진 숫자들의 산술적 덧셈이나 곱셈이다. AI 하드웨어 성능은 산술 연산의 정밀도와 컴퓨터 아키텍처에 따라 10배 정도 차이날 수 있다는 점에 주의하라. 논리 게이트 연산(AND, OR, AND NOT)을 세는 것이 더 근본적이겠지만 이들은 일반적으로 이용하거나 벤치마킹하기 어렵다; 현재 목적상 16비트 연산(FP16)으로 표준화하는 것이 유용하지만, 적절한 변환 인수들이 확립되어야 한다.

[^17]: 추정치와 실제 데이터 모음은 [Epoch AI](https://epochai.org/data/large-scale-ai-models)에서 확인할 수 있으며, GPT-4에 대해 약 2×10<sup>25</sup> 16비트 FLOP을 나타낸다; 이는 GPT-4에 대해 [유출된 수치](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/)와 대략 일치한다. 다른 2024년 중반 모델들에 대한 추정치들은 모두 GPT-4의 몇 배 범위 내에 있다.

[^18]: 추론은 단순히 신경망에서 출력을 생성하는 과정이다. 훈련은 많은 추론과 모델 가중치 조정의 연속으로 간주될 수 있다.

[^19]: 텍스트 생성에서 원래 GPT-4는 토큰당 560 TFLOP이 필요했다. 인간의 사고를 따라가려면 약 7토큰/초가 필요하므로, 이는 ≈3×10<sup>15</sup> FLOP/초가 된다. 하지만 효율성 개선으로 이는 감소했다; 예를 들어 [이 NVIDIA 브로슈어](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/)는 비교할 만한 성능의 Llama 405B 모델에 대해 3×10<sup>14</sup> FLOP/초 정도의 낮은 수치를 제시한다.

[^20]: 조금 더 복잡한 예로, AI 시스템이 먼저 수학 문제에 대한 여러 가능한 해법을 생성한 다음, 다른 인스턴스를 사용해 각 해법을 검토하고, 마지막으로 세 번째를 사용해 결과를 명확한 설명으로 종합할 수 있다. 이는 단일 패스보다 더 철저하고 신뢰할 만한 문제 해결을 가능하게 한다.

[^21]: 예를 들어 [OpenAI의 "Operator"](https://openai.com/index/introducing-operator/), [Claude의 도구 능력](https://docs.anthropic.com/en/docs/build-with-claude/computer-use), [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)에 대한 세부 정보를 참조하라. OpenAI의 [Deep Research](https://openai.com/index/introducing-deep-research/)는 아마도 상당히 정교한 아키텍처를 가지고 있을 것이지만 세부 사항은 공개되지 않았다.

[^22]: Deepseek R1은 최종 훈련된 모델이 광범위한 사고의 연쇄 추론을 생성하도록 모델을 반복적으로 훈련하고 프롬프팅하는 데 의존한다. o1이나 o3에 대한 아키텍처 세부 사항은 공개되지 않았지만, Deepseek은 추론으로 능력 스케일링을 가능하게 하는 데 특별한 "비밀 공식"이 필요하지 않다고 밝혔다. 하지만 AI의 "현상 유지"를 뒤엎는 것으로 언론의 큰 주목을 받았음에도 불구하고, 이것이 이 에세이의 핵심 주장에 영향을 주지는 않는다.

[^23]: 이러한 모델들은 추론 벤치마크에서 표준 모델들을 크게 능가한다. 예를 들어, 박사 수준의 과학 문제에 대한 엄격한 테스트인 GPQA Diamond Benchmark에서 GPT-4o는 [56%](https://openai.com/index/learning-to-reason-with-llms/)를 기록한 반면, o1과 o3는 각각 78%와 88%를 달성하여 인간 전문가의 평균 점수 70%를 훨씬 넘어섰다.

[^24]: OpenAI의 O3는 아마도 [ARC-AGI 챌린지 문제 각각을 완성하는 데](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai) ∼10<sup>21</sup>-10<sup>22</sup> FLOP를 소모했을 것이다. 유능한 인간들은 이를 (예를 들어) 10-100초에 할 수 있으므로, 이는 ∼10<sup>20</sup> FLOP/초 정도의 수치를 준다.

[^25]: 연산량이 AI 시스템 능력의 핵심 지표이긴 하지만, 데이터 품질과 알고리즘 개선과 상호작용한다. 더 나은 데이터나 알고리즘은 연산 요구량을 줄일 수 있고, 더 많은 연산량은 때로 약한 데이터나 알고리즘을 보상할 수 있다.

## 4장 - AGI와 초지능이란 무엇인가?

세계 최대 기술 기업들이 문 뒤에서 경쟁적으로 구축하고 있는 것은 정확히 무엇인가?

"인공일반지능"이라는 용어는 "인간 수준"의 범용 AI를 지칭하기 위해 오래전부터 사용되어 왔다. 이 용어는 결코 명확하게 정의된 적이 없었지만, 최근 들어 역설적으로 더욱 명확해지지 않으면서도 더욱 중요해졌다. 전문가들은 AGI가 수십 년 후에나 가능한지 이미 달성되었는지에 대해 동시에 논쟁하고 있고, 수조 달러 규모의 기업들이 "AGI를 향해" 경쟁하고 있다. ("AGI"의 모호성은 최근 [유출된 문서에 의해 드러났다고 보도된 바에 따르면](https://gizmodo.com/leaked-documents-show-openai-has-a-very-clear-definition-of-agi-2000543339) OpenAI와 Microsoft 간의 계약에서 AGI는 OpenAI에게 1000억 달러의 수익을 창출하는 AI로 정의되어 있었는데, 이는 고상한 정의라기보다는 상당히 상업적인 정의였다.)

"인간 수준 지능"을 가진 AI라는 개념에는 두 가지 핵심적인 문제가 있다. 첫째, 인간은 어떤 특정한 유형의 인지 작업을 수행하는 능력에서 매우, 매우 큰 차이를 보이므로 "인간 수준"이라는 것은 존재하지 않는다. 둘째, 지능은 매우 다차원적이다. 상관관계가 있을 수 있지만 불완전하며, AI에서는 상당히 다를 수 있다. 따라서 많은 역량에 대해 "인간 수준"을 정의할 수 있다고 하더라도, AI는 어떤 영역에서는 확실히 그것을 훨씬 뛰어넘을 것이고 다른 영역에서는 상당히 못 미칠 것이다.[^26]

그럼에도 불구하고 AI 역량의 유형, 수준, 그리고 임계점에 대해 논의할 수 있는 것은 매우 중요하다. 여기서 취하는 접근 방식은 범용 AI가 이미 여기에 있다는 점을 강조하는 것이며, 이것이 다양한 역량 수준에서 나타나고 또 앞으로도 나타날 것이라는 점이다. 비록 축소적이더라도 용어를 붙이는 것이 편리한데, 왜냐하면 이러한 용어들이 AI가 사회와 인류에 미치는 영향 측면에서 중요한 임계점에 대응하기 때문이다.

우리는 "완전한" AGI를 "초인간적 범용 AI"와 동의어로 정의할 것이다. 이는 본질적으로 모든 인간의 인지 작업을 최고 인간 전문가 수준 이상으로 수행할 수 있고, 새로운 기술을 습득하고 새로운 영역으로 역량을 전이할 수 있는 AI 시스템을 의미한다. 이는 현대 문헌에서 "AGI"가 종종 정의되는 방식과 일치한다. 이것이 *매우* 높은 임계점이라는 점을 주목하는 것이 중요하다. 어떤 인간도 이런 유형의 지능을 가지고 있지 않다. 오히려 이는 최고 인간 전문가들의 대규모 집단이 결합되었을 때 가질 수 있는 유형의 지능이다. 우리는 이를 넘어서는 역량을 "초지능"이라고 부를 수 있으며, 일반적인 전문가 수준 또는 인간 전문가 수준에서 광범위한 작업을 수행하는 "인간 경쟁력 수준" 및 "전문가 경쟁력 수준" GPAI로 더 제한적인 역량 수준을 정의할 수 있다.[^27]

이러한 용어들과 일부 다른 용어들이 아래 [표](https://keepthefuturehuman.ai/essay/docs/#tab:terms)에 수집되어 있다. 다양한 등급의 시스템이 무엇을 할 수 있는지에 대한 더 구체적인 감각을 위해서는 정의를 진지하게 받아들이고 그것이 무엇을 의미하는지 고려하는 것이 유용하다.

| AI 유형                | 관련 용어                          | 정의                                                                                                                                           | 예시                                                                                                        |
| --------------------- | --------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |
| 특화 AI               | 약한 AI                           | 특정 작업이나 작업군을 위해 훈련된 AI. 해당 영역에서는 뛰어나지만 일반적인 지능이나 전이 학습 능력이 부족함.                                                     | 이미지 인식 소프트웨어; 음성 비서 (예: Siri, Alexa); 체스 프로그램; DeepMind의 AlphaFold                              |
| 도구형 AI             | 증강 지능, AI 어시스턴트             | (에세이 후반부에서 논의.) 인간의 역량을 향상시키는 AI 시스템. 인간 경쟁력 수준의 범용 AI, 특화 AI, 그리고 보장된 통제를 결합하여 안전성과 협업을 우선시함. 인간의 의사결정을 지원함. | 고급 코딩 어시스턴트; AI 기반 연구 도구; 정교한 데이터 분석 플랫폼. 유능하지만 제한적이고 통제 가능한 에이전트                     |
| 범용 AI (GPAI)        |                                   | 구체적으로 훈련받지 않은 작업을 포함하여 다양한 작업에 적응 가능한 AI 시스템.                                                                             | 언어 모델 (예: GPT-4, Claude); 멀티모달 AI 모델; DeepMind의 MuZero                                               |
| 인간 경쟁력 수준 GPAI   | AGI [약함]                        | 평균적인 인간 수준에서 작업을 수행하며, 때로는 이를 초과하는 범용 AI.                                                                                    | 고급 언어 모델 (예: O1, Claude 3.5); 일부 멀티모달 AI 시스템                                                      |
| 전문가 경쟁력 수준 GPAI | AGI [부분적]                      | 대부분의 작업을 인간 전문가 수준에서 수행하며, 상당하지만 제한적인 자율성을 가진 범용 AI                                                                    | 아마도 도구화되고 스캐폴딩된 O3, 적어도 수학, 프로그래밍, 일부 하드 사이언스 분야에서                                       |
| AGI [완전함]          | 초인간적 GPAI                     | 모든 인간 지적 작업을 전문가 수준 이상에서 자율적으로 수행할 수 있으며, 효율적인 학습과 지식 전이가 가능한 AI 시스템.                                             | [현재 예시 없음 – 이론적]                                                                                      |
| 초지능                | 고도 초인간적 GPAI                 | 모든 영역에서 인간의 역량을 훨씬 뛰어넘어 집단적 인간 전문성을 능가하는 AI 시스템. 이러한 초월적 성능은 범용성, 품질, 속도 및/또는 기타 척도에서 나타날 수 있음.        | [현재 예시 없음 – 이론적]                                                                                      |

우리는 이미 인간 경쟁력 수준까지의 GPAI를 가지는 것이 어떤 것인지 경험하고 있다. 대부분의 사용자들이 이를 자신들을 더 생산적으로 만들어주는 똑똑하지만 제한적인 임시 직원을 두는 것으로 경험하기 때문에, 이는 비교적 순조롭게 통합되어 왔으며 작업 품질에 혼재된 영향을 미치고 있다.[^28]

전문가 경쟁력 수준 GPAI의 차이점은 현재 AI의 핵심적인 한계를 갖지 않고, 전문가들이 하는 일들을 수행한다는 것이다: 독립적으로 경제적 가치가 있는 작업, 실제 지식 창조, 의존할 수 있는 기술적 작업을 하면서 (여전히 가끔씩이지만) 바보 같은 실수를 거의 하지 않는다.

완전한 AGI의 개념은 가장 유능하고 효과적인 인간들조차 하는 모든 인지적인 일들을 자율적으로 그리고 도움이나 감독 없이 *정말로* 한다는 것이다. 여기에는 정교한 계획 수립, 새로운 기술 학습, 복잡한 프로젝트 관리 등이 포함된다. 독창적인 최첨단 연구를 할 수 있다. 회사를 경영할 수 있다. 당신의 직업이 무엇이든, 주로 컴퓨터로 하거나 전화를 통해 이루어지는 일이라면, *그것은 최소한 당신만큼 잘 할 수 있다.* 그리고 아마도 훨씬 더 빠르고 저렴하게. 아래에서 일부 파급효과를 논의하겠지만, 지금 당신이 해야 할 과제는 이를 정말로 진지하게 받아들이는 것이다. 당신이 알거나 알고 있는 가장 지식이 풍부하고 유능한 상위 10명을 상상해보라 - CEO, 과학자, 교수, 최고 엔지니어, 심리학자, 정치 지도자, 작가들을 포함하여. 이들을 모두 하나로 합쳐서, 100개 언어를 구사하고, 놀라운 기억력을 가지며, 빠르게 작동하고, 지치지 않고 항상 동기부여되어 있으며, 최저임금 이하로 일한다고 생각해보라.[^29] 그것이 AGI가 무엇인지에 대한 감각이다.

초지능의 경우 상상하기가 더 어렵다. 왜냐하면 그 개념은 어떤 인간이나 심지어 인간 집단조차 할 수 없는 지적 업적을 수행할 수 있다는 것이기 때문이다 - 정의상 우리가 예측할 수 없는 것이다. 하지만 감각을 얻을 수 있다. 최소한의 기준으로, 최고 인간 전문가보다도 훨씬 더 유능한 수많은 AGI들이 인간 속도의 100배로 작동하며, 엄청난 기억력과 훌륭한 조정 능력을 가진다고 생각해보라.[^30] 그리고 거기서부터 더 올라간다. 초지능과 대처하는 것은 다른 마음과 대화하는 것이라기보다는, 다른 (그리고 더 발전된) 문명과 협상하는 것에 더 가까울 것이다.

그렇다면 우리는 AGI와 초지능에 *얼마나* 가까이 와 있는가?


[^26]: 예를 들어, 현재 AI 시스템들은 빠른 산술이나 기억 작업에서는 인간 능력을 훨씬 뛰어넘지만, 추상적 추론과 창의적 문제 해결에서는 부족하다.

[^27]: 매우 중요하게도, 경쟁자로서 그러한 AI는 몇 가지 주요한 구조적 장점을 가질 것이다: 인간처럼 피로하거나 개인적인 다른 필요를 갖지 않을 것이다; 단순히 연산 능력을 확장함으로써 더 빠른 속도로 실행될 수 있다; 획득한 어떤 전문성이나 지식과 함께 복사될 수 있으며 - 신경망의 획득된 지식은 심지어 "병합"되어 전체 기술 세트를 서로 간에 전달할 수도 있다; 기계 속도로 소통할 수 있다; 그리고 어떤 인간보다도 더 중요한 방식으로 그리고 더 빠른 속도로 자기 수정이나 자기 개선을 할 수 있다.

[^28]: 만약 현재 최고 수준의 AI 시스템을 사용하는 데 시간을 보내본 적이 없다면, 추천한다: 그것들은 진정으로 유용하고 유능하며, AI가 더 강력해짐에 따라 미칠 영향을 가늠하는 데도 중요하다.

[^29]: 주요 연구병원을 생각해보라: 완전히 실현된 AGI는 동시에 모든 들어오는 환자 데이터를 분석하고, 모든 새로운 의학 논문을 따라가며, 진단을 제안하고, 치료 계획을 설계하고, 임상시험을 관리하며, 직원 일정을 조정할 수 있다 - 모든 것을 각 영역에서 병원의 최고 전문의들과 맞먹거나 능가하는 수준에서 작동하면서. 그리고 현재 비용의 일부만으로 여러 병원에서 동시에 이를 할 수 있다. 불행히도, 조직범죄 집단도 고려해야 한다: 완전히 실현된 AGI는 동시에 수천 명의 피해자들을 해킹하고, 사칭하고, 스파이하고, 협박하며, 법 집행기관을 따라갈 수 있고 (훨씬 더 천천히 자동화되는), 새로운 돈벌이 방법을 설계하고, 직원 일정을 조정할 수 있다 - 만약 직원이 있다면.

[^30]: Anthropic의 CEO인 Dario Amodei는 자신의 [에세이](https://darioamodei.com/machines-of-loving-grace)에서 "[백만 명의] 천재들의 나라"를 떠올렸다.

## 5장 - 임계점에서

오늘날의 AI 시스템에서 완전한 인공일반지능(AGI)에 이르는 경로는 충격적일 만큼 짧고 예측 가능해 보인다.

지난 10년간 막대한 [연산량](https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year), 인력, [재정](https://arxiv.org/abs/2405.21015) 자원에 힘입어 AI 분야에서 극적인 발전이 있었다. 많은 특화된 AI 애플리케이션이 맡은 작업에서 인간보다 뛰어난 성능을 보이며, 확실히 훨씬 빠르고 저렴하다.[^31] 또한 [바둑](https://www.nature.com/articles/nature16961), [체스](https://arxiv.org/abs/1712.01815), [포커](https://www.deepstack.ai/)와 같은 특정 영역 게임에서 모든 인간을 압도할 수 있는 특화된 초인적 에이전트들이 있으며, 단순화된 시뮬레이션 환경에서 인간만큼 효과적으로 계획하고 행동을 실행할 수 있는 더욱 [범용적인 에이전트](https://deepmind.google/discover/blog/a-generalist-agent/)들도 존재한다.

가장 주목할 만한 것은 OpenAI/Microsoft, Google/Deepmind, Anthropic/Amazon, Facebook/Meta, X.ai/Tesla 등의 현재 범용 AI 시스템들이[^32] 2023년 초부터 등장하여 그 이후 꾸준히(비록 고르지는 않지만) 능력을 향상시켜왔다는 점이다. 이들은 모두 거대한 텍스트 및 멀티미디어 데이터셋에 대한 토큰 예측과 인간 및 다른 AI 시스템으로부터의 광범위한 강화 피드백을 결합하여 만들어졌다. 일부는 광범위한 도구 및 스캐폴딩 시스템도 포함하고 있다.

### 현재 범용 시스템들의 장단점

이러한 시스템들은 지능과 전문성을 측정하도록 설계된 점점 더 광범위한 테스트에서 좋은 성과를 보이고 있으며, 해당 분야 전문가들마저 놀라게 하는 진전을 이루고 있다:

- 처음 출시되었을 때 GPT-4는 SAT, GRE, 입학시험, 변호사시험을 포함한 표준 학업 시험에서 [일반적인 인간 성능과 동등하거나 이를 능가했다](https://arxiv.org/abs/2303.08774). 더 최근의 모델들은 훨씬 더 나은 성능을 보일 가능성이 높지만, 결과가 공개되지 않았다.
- 오랫동안 "진정한" AI의 핵심 기준으로 여겨졌던 튜링 테스트를 현대 언어모델들이 비공식적으로나 [공식 연구](https://arxiv.org/abs/2405.08007)에서나 일상적으로 통과하고 있다.[^33]
- 57개 학문 분야를 아우르는 포괄적인 MMLU 벤치마크에서 [최신 모델들은 도메인 전문가 수준의 점수](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu)(∼90%)를 달성하고 있다.[^34]
- 기술적 전문성이 극적으로 향상되었다: 대학원 수준 물리학의 GPQA 벤치마크에서 [성능이](https://epoch.ai/data/ai-benchmarking-dashboard) 거의 무작위 추측 수준(GPT-4, 2022년)에서 전문가 수준(o1-preview, 2024년)으로 급상승했다.
- AI 저항성을 위해 특별히 설계된 테스트들도 무너지고 있다: OpenAI의 O3가 ARC-AGI 추상 문제해결 벤치마크를 인간 수준으로 해결하고, 최고 전문가 수준의 코딩 성능을 달성하며, 엘리트 수학자들을 겨냥해 설계된 Epoch AI의 "frontier math" 문제에서 25%의 점수를 기록했다고 [보고되었다](https://www.nextbigfuture.com/2024/12/openai-releases-o3-model-with-high-performance-and-high-cost.html).[^35]
- 이러한 추세가 너무나 명확해서 MMLU 개발자는 이제 ["인류의 마지막 시험"](https://agi.safe.ai/)을 만들었다 - AI가 곧 의미 있는 모든 테스트에서 인간 성능을 능가할 가능성을 반영하는 불길한 이름이다. 이 글을 쓰는 시점에서 AI 시스템이 이 극도로 어려운 시험에서 27%([Sam Altman](https://x.com/sama/status/1886220281565381078)에 따르면)와 35%([이 논문](https://arxiv.org/abs/2502.09955)에 따르면)를 달성했다는 주장들이 있다. 어떤 개별 인간도 그렇게 할 가능성은 매우 낮다.

이러한 인상적인 수치들(그리고 상호작용할 때 느껴지는 명백한 지능)[^36]에도 불구하고 (적어도 공개된 버전의) 이러한 신경망이 *할 수 없는* 일들이 많다. 현재 대부분은 서버에만 존재하는 비물리적 형태이며, 기껏해야 텍스트, 소리, 정지 이미지를 처리할 뿐(비디오는 처리하지 못한다.) 결정적으로, 대부분은 높은 정확도를 요구하는 복잡하게 계획된 활동을 수행할 수 없다.[^37] 그리고 현재 공개된 AI 시스템에서는 낮지만 고급 인간 인지에서는 강한 여러 다른 특성들이 있다.

다음 표는 GPT-4o, Claude 3.5 Sonnet, Google Gemini 1.5와 같은 2024년 중반 AI 시스템들을 기준으로 이러한 특성들 중 일부를 나열한다.[^38] 범용 AI가 얼마나 빠르게 더 강력해질 것인가에 대한 핵심 질문은: *기존과 동일한 방식을 더 많이* 수행하는 것 대 추가적이지만 *알려진* 기술들을 더하는 것 대 *정말 새로운* AI 연구 방향을 개발하거나 구현하는 것이 각각 어느 정도까지 결과를 낼 것인가이다. 이에 대한 나의 예측이 표에 제시되어 있으며, 각각의 시나리오가 해당 능력을 인간 수준까지 그리고 그 이상으로 끌어올릴 가능성으로 표현되어 있다.

<table><tbody><tr><th>능력</th><th>능력 설명</th><th>현황/전망</th><th>스케일링/기존기술/신기술</th></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td colspan="4"><em>핵심 인지 능력</em></td></tr><tr><td>추론</td><td>사람들은 정확하고 다단계적인 추론을 할 수 있으며, 규칙을 따르고 정확성을 확인한다.</td><td>확장된 사고 연쇄와 재훈련을 통해 최근 극적인 진전</td><td>95/5/5</td></tr><tr><td>계획</td><td>사람들은 장기적이고 계층적인 계획을 세운다.</td><td>규모와 함께 개선되고 있음; 스캐폴딩과 더 나은 훈련 기법으로 크게 도움받을 수 있음</td><td>10/85/5</td></tr><tr><td>사실 기반성</td><td>범용 AI들이 쿼리를 만족시키기 위해 근거 없는 정보를 지어낸다.</td><td>규모와 함께 개선되고 있음; 모델 내에서 보정 데이터 이용 가능; 스캐폴딩을 통해 확인/개선 가능</td><td>30/65/5</td></tr><tr><td>유연한 문제해결</td><td>인간은 새로운 패턴을 인식하고 복잡한 문제에 대한 새로운 해결책을 발명할 수 있음; 현재 머신러닝 모델들은 어려워함</td><td>규모와 함께 개선되지만 약하게; 신경기호적 또는 일반화된 "탐색" 기법으로 해결 가능할 수 있음</td><td>15/75/10</td></tr><tr><td colspan="4"><em>학습과 지식</em></td></tr><tr><td>학습 및 기억</td><td>사람들은 작업, 단기, 장기 기억을 가지며, 이들은 모두 동적이고 상호연관되어 있다.</td><td>모든 모델이 훈련 중 학습; 범용 AI들은 컨텍스트 윈도우 내에서와 미세조정 중에 학습; "지속 학습" 등 기법이 존재하지만 아직 대형 범용 AI에 통합되지 않음</td><td>5/80/15</td></tr><tr><td>추상화 및 재귀</td><td>사람들은 관계 집합을 추론과 조작을 위해 더 추상적인 것으로 매핑하고 전이시킬 수 있으며, 재귀적인 "메타" 추론을 포함한다.</td><td>규모와 함께 약하게 개선되고 있음; 신경기호 시스템에서 나타날 수 있음</td><td>30/50/20</td></tr><tr><td>세계 모델</td><td>사람들은 문제를 해결하고 물리적 추론을 할 수 있는 예측적 세계 모델을 가지고 지속적으로 업데이트한다</td><td>규모와 함께 개선되고 있음; 업데이트가 학습과 연결됨; 범용 AI들은 실세계 예측에서 약함</td><td>20/50/30</td></tr><tr><td colspan="4"><em>자아와 주체성</em></td></tr><tr><td>주체성</td><td>사람들은 계획/예측에 기반해 목표를 추구하기 위해 행동을 취할 수 있다.</td><td>많은 머신러닝 시스템이 주체적임; 대규모 언어모델들은 래퍼를 통해 에이전트로 만들 수 있음</td><td>5/90/5</td></tr><tr><td>자기주도성</td><td>사람들은 내부에서 생성된 동기와 욕구로 자신만의 목표를 개발하고 추구한다.</td><td>주로 주체성과 독창성으로 구성됨; 추상적 목표를 가진 복잡한 주체적 시스템에서 나타날 가능성</td><td>40/45/15</td></tr><tr><td>자기참조</td><td>사람들은 환경/맥락 내에 위치한 자신을 이해하고 추론한다.</td><td>규모와 함께 개선되고 있으며 훈련 보상으로 강화될 수 있음</td><td>70/15/15</td></tr><tr><td>자기인식</td><td>사람들은 자신의 사고와 정신 상태에 대한 지식을 가지고 이를 추론할 수 있다.</td><td>어떤 의미에서 범용 AI들에게 존재하며, 이들은 틀림없이 자기인식을 위한 고전적인 "거울 테스트"를 통과할 수 있음. 스캐폴딩으로 개선될 수 있지만, 이것이 충분한지는 불분명</td><td>20/55/25</td></tr><tr><td colspan="4"><em>인터페이스와 환경</em></td></tr><tr><td>체화된 지능</td><td>사람들은 실세계 환경을 이해하고 적극적으로 상호작용한다.</td><td>강화학습이 시뮬레이션과 실세계(로봇) 환경에서 잘 작동하며 멀티모달 트랜스포머에 통합될 수 있음</td><td>5/85/10</td></tr><tr><td>다감각 처리</td><td>사람들은 시각, 청각, 기타 감각 스트림을 통합하고 실시간으로 처리한다.</td><td>다중 모달리티 훈련이 "그냥 작동"하는 것으로 보이며, 규모와 함께 개선됨. 실시간 비디오 처리는 어렵지만 예를 들어 자율주행 시스템이 빠르게 개선되고 있음</td><td>30/60/10</td></tr><tr><td colspan="4"><em>고차원 능력</em></td></tr><tr><td>독창성</td><td>현재 머신러닝 모델들은 기존 아이디어/작품을 변형하고 결합하는 창의성을 보이지만, 사람들은 때로 자신의 정체성과 연결된 새로운 프레임워크와 구조를 구축할 수 있다.</td><td>"창의성"과 구분하기 어려울 수 있으며, 이것이 독창성으로 확장될 수 있음; 창의성과 자기인식에서 나타날 수 있음</td><td>50/40/10</td></tr><tr><td>감각능력</td><td>사람들은 질감을 경험하며; 이는 긍정적, 부정적 또는 중성적 감정가를 가질 수 있음; 사람이 된다는 것은 "무언가 같은" 것이다.</td><td>주어진 시스템이 이것을 가지고 있는지 판단하기 매우 어렵고 철학적으로 복잡함</td><td>5/10/85</td></tr></tbody></table>

현대 범용 AI 시스템에서 현재 인간 전문가 수준 이하인 핵심 능력들을 유형별로 그룹화. 세 번째 열은 현재 상황을 요약. 마지막 열은 인간 수준 성능이 달성될 가능성(%)을 다음을 통해 예측: 현재 기술의 스케일링 / 알려진 기술과의 결합 / 새로운 기술 개발. 이러한 능력들은 독립적이지 않으며, 어느 하나의 증가는 일반적으로 다른 것들의 증가를 동반한다. 모든 것이 (특히 감각능력이) AI 개발을 발전시킬 수 있는 AI 시스템에 필요한 것은 아니라는 점을 주목하라. 이는 강력하지만 무감각한 AI의 가능성을 강조한다.

이런 식으로 "빠진 것"을 분해해보면 기존 또는 알려진 기술들을 스케일링함으로써 광범위하게 인간을 넘어서는 지능을 향해 상당히 순조롭게 나아가고 있음이 꽤 명확해진다.[^39]

여전히 놀라운 일들이 있을 수 있다. "감각능력"을 제쳐두더라도, 나열된 핵심 인지 능력 중 일부는 현재 기술로는 정말 할 수 없고 새로운 기술이 필요할 수 있다. 하지만 이것을 생각해보자. 세계 최대 기업들 중 다수가 현재 기울이고 있는 노력은 아폴로 프로젝트의 몇 배, 맨해튼 프로젝트의 수십 배에 달하는 지출에 해당하며,[^40] 전례 없는 급여로 수천 명의 최고 기술 인재들을 고용하고 있다. 지난 몇 년간의 역학은 이제 역사상 그 어떤 시도보다도 더 많은 인간 지적 화력을(이제 AI도 추가되어) 이 일에 집중시켜왔다. 우리는 실패에 베팅해서는 안 된다.

### 큰 목표: 범용 자율 에이전트

지난 몇 년간 범용 AI의 개발은 범용적이고 강력하지만 도구와 같은 AI를 만드는 데 초점을 맞춰왔다: 주로 (상당히) 충실한 보조자 역할을 하며, 일반적으로 스스로 행동을 취하지는 않는다. 이는 부분적으로는 의도된 설계이지만, 주로 이러한 시스템들이 복잡한 행동을 맡기기에는 관련 기술에서 충분히 능숙하지 않았기 때문이다.[^41]

하지만 AI 회사들과 연구자들은 점점 더 *자율적인* 전문가 수준의 범용 에이전트를 향해 [초점을 이동](https://www.axios.com/2025/01/23/davos-2025-ai-agents)시키고 있다.[^42] 이는 시스템들이 사용자가 실제 행동을 위임할 수 있는 인간 보조자처럼 더 많이 행동할 수 있게 해줄 것이다.[^43] 그렇게 하려면 무엇이 필요할까? "빠진 것" 표의 여러 능력들이 관련되어 있는데, 강한 사실 기반성, 학습과 기억, 추상화와 재귀, 세계 모델링(지능을 위해), 계획, 주체성, 독창성, 자기주도성, 자기참조, 자기인식(자율성을 위해), 그리고 다감각 처리, 체화된 지능, 유연한 문제해결(범용성을 위해)이 포함된다.[^44]

높은 자율성(행동의 독립성), 높은 범용성(범위와 작업 폭), 높은 지능(인지 작업에서의 능력)의 이런 삼중 교집합은 현재 인간에게만 고유하다. 이것이 많은 사람들이 AGI를 생각할 때 암묵적으로 염두에 두는 것일 가능성이 높다 - 그 가치와 위험 모두 측면에서 말이다.

이는 A-G-I를 ***A***utonomous(자율)- ***G***eneral(범용)- ***I***ntelligence(지능)로 정의하는 또 다른 방법을 제공하며, 이 삼중 교집합이 고능력 시스템들의 위험과 보상을 이해하고 AI 거버넌스에서 매우 가치 있는 렌즈를 제공한다는 것을 보게 될 것이다.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Simple-1024x1024.png&w=3840&q=75) 변혁적인 A-G-I의 힘과 위험 영역은 세 가지 핵심 속성의 교집합에서 나타난다: 높은 자율성, 높은 지능, 높은 범용성.

### AI (자기)개선 사이클

AI 진보를 이해하는 데 있어 마지막으로 중요한 요소는 AI의 독특한 기술적 피드백 루프이다. AI를 개발하는 데 있어 성공은 - 입증된 시스템과 배포된 제품 모두에서 - 추가적인 투자, 인재, 경쟁을 가져오며, 우리는 현재 수천억 달러, 아니 수조 달러의 투자를 이끌고 있는 거대한 AI 과대광고-플러스-현실 피드백 루프의 한복판에 있다.

이런 유형의 피드백 사이클은 어떤 기술에서든 일어날 수 있으며, 시장 성공이 투자를 낳고, 이것이 개선과 더 나은 시장 성공을 낳는 여러 기술에서 이를 봐왔다. 하지만 AI 개발은 더 나아가, 이제 AI 시스템들이 새롭고 더 강력한 AI 시스템을 개발하는 데 도움을 주고 있다.[^45] 우리는 이 피드백 루프를 5단계로 생각해볼 수 있으며, 각 단계는 이전 단계보다 더 짧은 시간 척도를 가진다고 표에 나와 있듯이 말이다.

*AI 개선 사이클은 여러 시간 척도에 걸쳐 작동하며, 각 단계는 잠재적으로 후속 단계들을 가속화할 수 있다. 초기 단계들은 이미 잘 진행되고 있으며, 후기 단계들은 여전히 추측의 영역이지만 일단 잠금이 해제되면 매우 빠르게 진행될 수 있다.*

이러한 단계들 중 몇 개는 이미 진행 중이며, 몇 개는 명백히 시작되고 있다. 마지막 단계인 AI 시스템들이 자율적으로 자기 자신을 개선하는 것은 매우 강력한 AI 시스템의 위험에 대한 문헌에서 주요 소재였으며, 그럴 만한 이유가 있다.[^46] 하지만 이것이 이미 시작되었고 기술의 급속한 발전에서 더 많은 놀라움으로 이어질 수 있는 피드백 사이클의 가장 극단적인 형태일 뿐이라는 점을 주목하는 것이 중요하다.


[^31]: 당신이 아마 생각하는 것보다 훨씬 더 많은 이런 AI를 사용하고 있다. 음성 생성과 인식, 이미지 처리, 뉴스피드 알고리즘 등을 구동하고 있다.

[^32]: 이들 기업 쌍 간의 관계는 매우 복잡하고 미묘하지만, 현재 AI 개발에 참여하고 있는 기업들의 막대한 전체 시가총액과 Anthropic과 같은 "더 작은" 회사들 뒤에도 투자와 주요 파트너십 거래를 통해 엄청나게 깊은 주머니가 있다는 것을 나타내기 위해 명시적으로 나열했다.

[^33]: 튜링 테스트를 폄하하는 것이 유행이 되었지만, 그것은 매우 강력하고 범용적이다. 약한 버전에서는 AI와 (인간처럼 행동하도록 훈련된) 상호작용하는 일반적인 사람들이 짧은 기간 동안 일반적인 방식으로 그것이 AI인지 알 수 있는지를 나타낸다. 그들은 알 수 없다. 둘째, 고도로 적대적인 튜링 테스트는 본질적으로 인간 능력과 지능의 모든 요소를 탐사할 수 있다 - 예를 들어 AI 시스템을 인간 전문가와 비교하고 다른 인간 전문가들이 평가하는 식으로. AI 평가의 대부분이 일반화된 형태의 튜링 테스트라는 의미가 있다.

[^34]: 이는 도메인별로 - 어떤 인간도 모든 과목에서 동시에 그런 점수를 달성할 수는 없을 것이다.

[^35]: 이것들은 훌륭한 수학자들조차 해결할 수 있다면 상당한 시간이 걸릴 문제들이다.

[^36]: 회의적인 성향이라면 회의론을 유지하되 최신 모델들을 정말로 직접 써보고, 그들이 통과할 수 있는 테스트 문제들 중 일부를 직접 시도해보라. 물리학 교수로서 나는 예를 들어 최고 모델들이 우리 학과의 대학원 자격시험을 통과할 것이라고 거의 확실하게 예측한다.

[^37]: 이것과 착각과 같은 다른 약점들이 시장 채택을 늦추고 인식된 능력과 주장된 능력(이것 역시 치열한 시장 경쟁과 투자 유치 필요성의 렌즈를 통해 봐야 한다) 사이의 격차를 만들었다. 이것이 AI 진보의 실제 상황에 대해 대중과 정책입안자들을 모두 혼란스럽게 했다. 과대광고와 일치하지 않을 수도 있지만, 진보는 매우 실재한다.

[^38]: 그 이후의 주요 진전은 추론 중 더 많은 연산량과 더 큰 강화학습을 활용하여 최고 품질의 추론을 위해 훈련된 시스템들의 개발이었다. 이러한 모델들은 새롭고 그들의 능력이 덜 테스트되었기 때문에, 본질적으로 해결된 것으로 간주하는 "추론"을 제외하고는 이 표를 전면 개편하지 않았다. 하지만 그러한 시스템들의 경험되고 보고된 능력들을 바탕으로 예측을 업데이트했다.

[^39]: 1960년대와 1980년대의 이전 AI 낙관주의 물결들은 약속된 능력들이 실현되지 못했을 때 "AI 겨울"로 끝났다. 하지만 현재의 물결은 많은 도메인에서 초인적 성능을 달성했고 막대한 연산 자원과 상업적 성공에 뒷받침되어 근본적으로 다르다.

[^40]: 전체 아폴로 프로젝트는 [2020년 달러로 약 2,500억 달러가 들었고](https://www.planetary.org/space-policy/cost-of-apollo), 맨해튼 프로젝트는 [그것의 10분의 1도 안 됐다](https://www.brookings.edu/the-costs-of-the-manhattan-project/). 골드만삭스는 [향후 몇 년간 AI 데이터센터만을 위해 1조 달러가 지출될 것이라고](https://www.datacenterdynamics.com/en/news/goldman-sachs-1tn-to-be-spent-on-ai-data-centers-chips-and-utility-upgrades-with-little-to-show-for-it-so-far/) 전망한다.

[^41]: 인간들도 많은 실수를 하지만, 우리는 우리가 얼마나 신뢰할 수 있는지를 과소평가한다! 확률이 곱해지기 때문에, 올바르게 하기 위해 20단계가 필요한 작업은 절반의 시간에 올바르게 완료하기 위해서만도 각 단계가 97% 신뢰할 수 있어야 한다. 우리는 그런 작업들을 항상 한다.

[^42]: 이 방향으로의 강한 움직임이 OpenAI의 ["Deep Research"](https://openai.com/index/introducing-deep-research/) 보조자로 최근에 이루어졌는데, 이는 인터넷에서 복잡한 작업을 위한 다단계 연구를 자율적으로 수행하며 "복잡한 작업을 위해 인터넷에서 다단계 연구를 수행하는 새로운 주체적 능력"이라고 설명된다.

[^43]: 그 성가신 PDF 양식 작성하기, 항공편 예약하기 같은 것들. 하지만 20개 분야의 박사학위와 함께! 그러니까 또한: 당신을 위해 그 논문 쓰기, 당신을 위해 그 계약 협상하기, 당신을 위해 그 정리 증명하기, 당신을 위해 그 광고 캠페인 만들기 등등. *당신*은 무엇을 하는가? 물론 그것에게 무엇을 할지 말해주는 것이다.

[^44]: 감각능력은 명확하게 필요하지 *않으며*, 이 삼중 교집합의 AI가 반드시 그것을 의미하지도 않는다는 점을 주목하라.

[^45]: 여기서 가장 가까운 유사점은 아마도 칩 기술일 것인데, 컴퓨터 기술이 사람들이 다음 세대 칩 기술을 설계하는 데 도움을 주면서 수십 년간 무어의 법칙을 유지해온 개발이다. 하지만 AI는 훨씬 더 직접적일 것이다.

[^46]: AI가 - 곧 - 며칠이나 몇 주의 시간 척도로 스스로를 개선할 수 있다는 것을 잠시 받아들여보는 것이 중요하다. 아니면 그보다도 짧게. 누군가 AI 능력이 분명히 멀리 있다고 말할 때 이것을 염두에 두어라.

## 6장 - AGI를 향한 경쟁

기업과 국가가 AGI 개발 경쟁에 뛰어드는 원동력은 무엇일까?

최근 AI 분야의 급속한 발전은 그 결과이면서 동시에 원인이 되어 엄청난 관심과 투자를 불러일으켰다. 이는 부분적으로는 AI 개발의 성공에서 비롯되었지만, 더 복잡한 배경이 있다. 지구상에서 가장 거대한 기업들과 심지어 국가들까지 단순한 AI가 아니라 AGI와 초지능을 구축하려고 경쟁하는 이유는 무엇일까?

### AI 연구를 인간 수준 AI로 이끈 동력

약 5년 전까지만 해도 AI는 주로 학술적이고 과학적인 연구 문제였으며, 따라서 호기심과 지능을 이해하고 새로운 기질에서 이를 창조하려는 욕구에 의해 주도되었다.

이 시기에는 대부분의 연구자들이 AI의 이점이나 위험에 상대적으로 적은 관심을 보였다. AI를 개발해야 하는 이유를 물으면, 다소 막연하게 AI가 도움을 줄 수 있는 문제들의 목록을 나열하는 것이 일반적인 대답이었다: 새로운 의약품, 새로운 소재, 새로운 과학, 더 똑똑한 프로세스, 그리고 전반적으로 사람들의 삶을 개선하는 것.[^47]

이는 훌륭한 목표들이다![^48] AI 전반이 아니라 AGI가 이러한 목표에 필요한지는 의문을 제기할 수 있고 또 그럴 것이지만, 이러한 목표들은 많은 AI 연구자들이 시작할 때 가졌던 이상주의를 보여준다.

그러나 지난 5년 동안 AI는 상대적으로 순수한 연구 분야에서 세계 최대 기업들이 주도하는 훨씬 더 공학적이고 제품 중심적인 분야로 변모했다.[^49] 연구자들은 여전히 중요하지만 더 이상 이 과정을 주도하지 않는다.

### 기업들이 AGI를 만들려는 이유는 무엇인가?

그렇다면 거대 기업들(그리고 투자자들은 더욱더)이 AGI 구축에 막대한 자원을 쏟아붓는 이유는 무엇일까? 대부분의 기업들이 꽤 솔직하게 밝히는 두 가지 동력이 있다: 그들은 AI를 사회의 생산성 동력으로, 그리고 자신들의 이익 동력으로 보고 있다. 범용 AI는 본질적으로 범용이기 때문에 엄청난 상금이 걸려 있다: 제품과 서비스를 만들 분야를 선택하는 대신 *모든 분야를 한 번에* 시도할 수 있다. 빅테크 기업들은 디지털 상품과 서비스를 생산하여 거대해졌으며, 적어도 일부 임원들은 AI를 검색, 소셜 미디어, 노트북, 휴대폰 등이 제공하는 것과 유사하지만 더 확장된 위험과 이익을 가지고 이를 잘 제공하는 다음 단계로 보고 있는 것이 확실하다.

하지만 왜 AGI인가? 이에 대한 매우 간단한 답이 있는데, 대부분의 기업과 투자자들은 공개적으로 논의하기를 꺼린다.[^50]

AGI는 직접적으로, 일대일로 *노동자를 대체할 수* 있기 때문이다.

보완하거나, 권한을 부여하거나, 더 생산적으로 만드는 것이 아니다. 심지어 *대체하는* 것도 아니다. 이 모든 것은 비AGI로도 가능하고 실제로 이루어질 것이다. AGI는 구체적으로 사고하는 노동자들을(그리고 로봇공학과 함께라면 많은 육체노동자들까지도) 완전히 *대체할 수* 있는 것이다. 이러한 견해의 근거로 OpenAI의 [(공개적으로 발표된) AGI 정의](https://openai.com/our-structure/)만 봐도 충분한데, 그것은 "경제적으로 가치 있는 대부분의 일에서 인간을 능가하는 고도로 자율적인 시스템"이다.

여기서 (기업들에게!) 주어지는 상금은 엄청나다. 노동 비용은 세계 약 100조 달러 규모의 전 세계 경제에서 상당한 비율을 차지한다. 이중 일부만이라도 인간 노동을 AI 노동으로 대체하여 포획된다면, 이는 연간 수조 달러의 수익이다. AI 기업들은 또한 누가 돈을 지불할 의향이 있는지도 잘 알고 있다. 그들이 보기에, 당신은 생산성 도구에 연간 수천 달러를 지불하지 않을 것이다. 하지만 기업은 가능하다면 당신의 노동을 대체하기 위해 연간 수천 달러를 지불*할* 것이다.

### 국가들이 AGI 경쟁에 뛰어들 수밖에 없다고 느끼는 이유

AGI 추구에 대한 국가들의 공식적인 동기는 경제적, 과학적 리더십에 초점을 맞춘다. 이 논리는 설득력이 있다: AGI는 과학 연구, 기술 개발, 경제 성장을 극적으로 가속화할 수 있다. 이해관계를 고려할 때, 어떤 주요 강국도 뒤처질 수 없다고 그들은 주장한다.[^51]

하지만 추가적이고 대부분 표면화되지 않은 동력들도 있다. 특정 군사 및 국가보안 지도자들이 극도로 강력하고 파국적으로 위험한 기술에 대해 논의하기 위해 비공개로 만날 때, 그들의 초점이 "어떻게 이러한 위험을 피할 것인가"가 아니라 "어떻게 이것을 먼저 얻을 것인가"라는 데 있다는 것은 의심의 여지가 없다. 군사 및 정보 지도자들은 AGI를 군사 업무의 잠재적 혁명으로, 아마도 핵무기 이후 가장 중대한 혁명으로 본다. 우려는 AGI를 먼저 개발하는 국가가 극복 불가능한 전략적 우위를 얻을 수 있다는 것이다. 이는 전형적인 군비 경쟁 역학을 만들어낸다.

우리는 이러한 "AGI 경쟁" 사고가[^52] 설득력이 있지만 심각하게 결함이 있다는 것을 보게 될 것이다. 이는 경쟁이 위험하고 모험적이어서가 아니라 - 물론 그렇기도 하지만 - 기술의 본질 때문이다. 암묵적인 가정은 AGI가 다른 기술들처럼 그것을 개발한 국가에 의해 통제 가능하고, 그것을 가장 많이 보유한 사회에 권력을 부여하는 혜택이라는 것이다. 앞으로 보게 되겠지만, 아마도 둘 다 아닐 것이다.

### 초지능은 왜인가?

기업들이 공개적으로는 생산성에, 국가들이 경제적, 기술적 성장에 초점을 맞추는 반면, 의도적으로 완전한 AGI와 초지능을 추구하는 이들에게 이것들은 시작에 불과하다. 그들이 실제로 염두에 두고 있는 것은 무엇인가? 공개적으로는 거의 말하지 않지만, 다음과 같은 것들을 포함한다:

1. 많거나 모든 질병의 치료법;
2. 노화의 중단과 역전;
3. 핵융합 같은 새로운 지속 가능한 에너지원;
4. 유전공학을 통한 인간 업그레이드나 맞춤형 생명체;
5. 나노기술과 분자 제조;
6. 마음 업로드;
7. 이색적 물리학이나 우주 기술;
8. 초인간적 조언과 의사결정 지원;
9. 초인간적 계획과 조정.

처음 세 가지는 대체로 "단일 날개" 기술들이다 - 즉, 상당히 강하게 순 긍정적일 가능성이 높다. 질병을 치료하거나 선택한다면 더 오래 살 수 있는 것에 반대하기는 어렵다. 그리고 우리는 이미 핵융합의 부정적인 면을(핵무기 형태로) 거두어들였으니, 이제 긍정적인 면을 얻는다면 좋을 것이다. 이 첫 번째 범주의 질문은 이러한 기술들을 더 빨리 얻는 것이 위험을 상쇄하는지이다.

다음 네 가지는 명백히 양날의 검이다: AI와 마찬가지로 잠재적으로 거대한 장점과 엄청난 위험을 모두 가진 변혁적 기술들이다. 이 모든 것들이 내일 블랙박스에서 튀어나와 배치된다면 관리하기가 믿을 수 없이 어려울 것이다.[^53]

마지막 두 가지는 기술을 발명하는 것이 아니라 초인간적 AI가 직접 일을 하는 것에 관한 것이다. 더 정확히 말하면, 완곡어법을 제쳐두고, 이는 강력한 AI 시스템이 사람들에게 무엇을 할지 말해주는 것을 포함한다. 조언을 하는 시스템이 조언받는 사람보다 훨씬 더 강력하여, 조언받는 사람이 결정의 근거를 의미 있게 이해할 수 없는 상황에서(또는 이것이 제공되더라도, 조언자가 다른 결정에 대해서도 마찬가지로 설득력 있는 근거를 제공하지 않을 것이라고 신뢰할 수 없는 상황에서) 이를 "조언"이라고 부르는 것은 기만적이다.

이는 위 목록에서 빠진 핵심 항목을 지적한다:

10. 권력.

초인간적 AI를 향한 현재 경쟁의 기저에 있는 것의 대부분이 *지능 = 권력*이라는 아이디어라는 것은 명백하다. 각 경쟁자는 자신이 그 권력의 최고 보유자가 될 것이며, 표면적으로는 선의의 이유로 그것을 휘두를 수 있을 것이고 그것이 자신의 통제에서 미끄러지거나 빼앗기지 않을 것이라고 기대하고 있다.

즉, 기업과 국가들이 실제로 쫓고 있는 것은 AGI와 초지능의 결실뿐만 아니라 누가 그것들에 접근할 수 있고 어떻게 사용될지를 통제할 권력이다. 기업들은 자신들을 주주와 인류를 위해 봉사하는 이 권력의 책임감 있는 관리자로 본다; 국가들은 자신들을 적대적 세력이 결정적 우위를 얻는 것을 막는 필요한 수호자로 본다. 둘 다 위험할 정도로 틀렸으며, 초지능은 본질적으로 어떤 인간 기관에 의해서도 확실하게 통제될 수 없다는 것을 인식하지 못하고 있다. 우리는 초지능 시스템의 본질과 역학이 인간의 통제를 극도로 어렵게, 불가능하지는 않을지라도, 만든다는 것을 보게 될 것이다.

이러한 경쟁 역학 - 기업적이든 지정학적이든 - 은 결정적으로 중단되지 않는 한 특정 위험들을 거의 불가피하게 만든다. 이제 이러한 위험들과 왜 그것들이 경쟁적인[^54] 개발 패러다임 내에서는 적절히 완화될 수 없는지를 살펴보자.


[^47]: 더 정확한 가치 있는 목표 목록은 UN [지속가능발전목표](https://sdgs.un.org/goals)이다. 이것들은 어떤 의미에서 우리가 세계에서 개선되기를 바라는 것에 대한 전 지구적 합의 목표에 가장 가까운 것이다. AI가 도움을 줄 수 있다.

[^48]: 일반적으로 기술은 수천 년이 증명하듯이 인간의 복지를 위한 변혁적인 경제적, 사회적 힘을 가지고 있다. 이러한 맥락에서, 긍정적인 AGI 비전에 대한 길고 설득력 있는 해설은 Anthropic 창립자 Dario Amodei의 [이 에세이](https://darioamodei.com/machines-of-loving-grace)에서 찾을 수 있다.

[^49]: 민간 AI 투자는 [2018-19년경부터 급증하기 시작하여 그 무렵 공공 투자를 넘어섰고](https://cset.georgetown.edu/publication/tracking-ai-investment/), 그 이후로 이를 크게 앞질렀다.

[^50]: 더 비공개적인 자리에서는 그들이 그런 거리낌이 없다는 것을 내가 증언할 수 있다. 그리고 이는 점점 더 공개되고 있다; 예를 들어 Y-combinator의 새로운 ["스타트업 요청"](https://www.ycombinator.com/rfs)을 보라. 이 중 많은 부분이 인간 노동자의 전면적 대체를 명시적으로 요구한다. 그들을 인용하면, "B2B SaaS의 가치 제안은 인간 노동자를 점진적으로 더 효율적으로 만드는 것이었다. 수직 AI 에이전트의 가치 제안은 업무를 완전히 자동화하는 것이다...이 기회가 또 다른 100개의 유니콘을 만들어낼 만큼 충분히 클 가능성이 있다." (실리콘밸리 용어에 익숙하지 않은 사람들을 위해, "B2B"는 기업간 거래이고 유니콘은 10억 달러 기업이다. 즉 그들은 다른 기업들을 위해 노동자를 대체하는 천억 달러 이상의 가치를 가진 기업 100개 이상에 대해 말하고 있다.)

[^51]: 예를 들어 최근의 [미-중 경제안보검토위원회 보고서](https://www.uscc.gov/sites/default/files/2024-11/2024_Executive_Summary.pdf)를 보라. 보고서 자체에는 놀랍게도 정당화 근거가 거의 없었지만, 최우선 권고사항은 미국 "의회가 인공일반지능(AGI) 역량을 경쟁적으로 확보하는 데 전념하는 맨해튼 프로젝트와 같은 프로그램을 설립하고 자금을 지원"하는 것이었다.

[^52]: 기업들은 이제 이러한 지정학적 틀을 자신들의 AI 개발에 대한 모든 제약에 대한 방패막으로 채택하고 있으며, 일반적으로 뻔뻔스럽게 자기중심적인 방식으로, 때로는 기본적인 상식조차 맞지 않는 방식으로 그렇게 하고 있다. Meta의 [프론티어 AI에 대한 접근법](https://about.fb.com/news/2025/02/meta-approach-frontier-ai/)을 생각해보자. 이는 미국이 "기술 혁신, 경제 성장, 국가 안보 분야의 리더로서의 지위를 \[확고히\]" 해야 한다고 동시에 주장하면서도, 가장 강력한 AI 시스템을 공개적으로 출시함으로써 그렇게 해야 한다고 주장한다 - 이는 지정학적 경쟁자와 적대자들에게 직접 제공하는 것을 포함한다.

[^53]: 따라서 우리는 아마도 이러한 기술들의 관리를 AI에 맡겨야 할 것이다. 하지만 이는 매우 문제가 되는 통제권 위임이 될 것인데, 이에 대해서는 아래에서 다시 다루겠다.

[^54]: 기술 개발에서의 경쟁은 종종 중요한 이익을 가져온다: 독점적 통제를 방지하고, 혁신과 비용 절감을 추진하며, 다양한 접근법을 가능하게 하고, 상호 감시를 만들어낸다. 그러나 AGI의 경우 이러한 이익들은 경쟁 역학과 안전 예방조치를 줄이라는 압력으로부터 오는 고유한 위험들과 비교 검토되어야 한다.

## 7장 - 현재 경로로 AGI를 구축하면 어떤 일이 벌어질까?

사회는 AGI 수준의 시스템에 대한 준비가 되어 있지 않다. 만약 우리가 이를 매우 빨리 구축한다면, 상황은 추악해질 수 있다.

완전한 인공일반지능의 개발 – 여기서 "관문 밖" AI라고 부르는 것 – 은 세계의 본질에 근본적인 변화를 가져올 것이다. 그 본질상 인간보다 더 큰 능력을 가진 새로운 지능 종족을 지구에 추가하는 것을 의미한다.

그 후 무슨 일이 일어날지는 기술의 성격, 개발자들의 선택, 그리고 개발이 이루어지는 세계적 맥락을 포함한 많은 요인에 달려 있다.

현재 완전한 AGI는 서로 경쟁하는 소수의 거대한 민간 기업에 의해 개발되고 있으며, 의미 있는 규제나 외부 감독은 거의 없고,[^55] 핵심 제도들이 점점 약해지고 심지어 기능 장애를 보이는 사회에서,[^56] 지정학적 긴장이 높고 국제 협력은 낮은 시기에 진행되고 있다. 일부는 이타적 동기를 가지고 있지만, 많은 개발자들은 돈이나 권력, 또는 둘 다에 의해 움직인다.

예측은 매우 어렵지만, 충분히 이해된 역학과 이전 기술들과의 적절한 유사성이 있어서 지침을 제공할 수 있다. 불행히도 AI의 가능성에도 불구하고, 이들은 현재 우리의 궤도가 어떻게 전개될지에 대해 깊이 비관적일 충분한 이유를 제공한다.

솔직히 말하자면, 현재 과정에서 AGI 개발은 일부 긍정적인 효과를 가져올 것이다 (그리고 일부 사람들을 매우, 매우 부유하게 만들 것이다). 하지만 기술의 성격, 근본적인 역학, 그리고 그것이 개발되는 맥락은 다음을 강력히 시사한다: 강력한 AI는 우리 사회와 문명을 극적으로 훼손할 것이다; 우리는 그것에 대한 통제력을 잃을 것이다; 우리는 그것 때문에 세계 대전에 휘말릴 가능성이 높다; 우리는 그것*에게* 통제권을 잃거나 양보할 것이다; 그것은 인공 초지능으로 이어질 것이고, 이는 우리가 절대 통제할 수 없으며 인간이 운영하는 세상의 종말을 의미할 것이다.

이는 강한 주장이고, 나는 이것이 공허한 추측이나 근거 없는 "파멸주의"가 아니었으면 좋겠다. 하지만 이것이 과학, 게임 이론, 진화론, 그리고 역사가 모두 가리키는 방향이다. 이 섹션은 이러한 주장들과 그 근거를 자세히 다룬다.

### 우리는 사회와 문명을 훼손할 것이다

실리콘밸리 임원실에서 들을 수 있는 말과 달리, 대부분의 파괴 – 특히 매우 빠른 종류의 파괴 – 는 유익하지 않다. 복잡한 시스템을 더 나쁘게 만드는 방법이 더 좋게 만드는 방법보다 훨씬 많다. 우리 세계가 현재만큼 잘 기능하는 것은 우리가 세계를 꾸준히 더 좋게 만든 과정, 기술, 제도들을 애써 구축했기 때문이다.[^57] 공장에 대형 망치를 들이대는 것이 운영을 개선하는 경우는 거의 없다.

다음은 AGI 시스템이 우리 문명을 파괴할 방식들의 (불완전한) 목록이다.

- 이들은 노동을 극적으로 파괴하여 *최소한* 소득 불평등을 극적으로 높이고 잠재적으로 대규모 고용 부족이나 실업을 야기할 것이며, 이는 사회가 적응하기에는 너무 짧은 시간 안에 일어날 것이다.[^58]
- 이들은 아마도 막대한 경제적, 사회적, 정치적 권력 – 잠재적으로 국가의 권력보다도 큰 – 을 대중에게 책임지지 않는 소수의 거대한 민간 이익집단에게 집중시킬 것이다.
- 이들은 이전에 어렵거나 비쌌던 활동을 갑자기 사소하게 쉽게 만들 수 있어서, 특정 활동이 비용이 많이 들거나 상당한 인간 노력을 요구하는 상태로 남아있는 것에 의존하는 사회 시스템을 불안정하게 만들 수 있다.[^59]
- 이들은 사회의 정보 수집, 처리, 소통 시스템을 완전히 현실적이지만 거짓이거나, 스팸성이거나, 지나치게 표적화되거나, 조작적인 미디어로 철저히 범람시켜서 무엇이 물리적으로 실제인지 아닌지, 인간인지 아닌지, 사실인지 아닌지, 신뢰할 만한지 아닌지를 구별하는 것이 불가능해질 수 있다.[^60]
- 이들은 위험하고 거의 완전한 지적 의존성을 만들 수 있는데, 우리가 완전히 이해할 수 없는 AI 시스템에 점점 더 의존하면서 인간의 핵심 시스템과 기술에 대한 이해가 위축될 수 있다.
- 이들은 대부분의 사람들이 소비하는 거의 모든 문화적 객체(텍스트, 음악, 시각예술, 영화 등)가 비인간 정신에 의해 창조되고, 중재되고, 선별되면서 인간 문화를 효과적으로 끝낼 수 있다.
- 이들은 정부나 민간 이익집단이 대중을 통제하고 공익과 상충하는 목표를 추구하는 데 사용할 수 있는 효과적인 대량 감시 및 조작 시스템을 가능하게 할 수 있다.
- 인간의 담론, 토론, 선거 시스템을 훼손함으로써, 이들은 민주주의 제도의 신뢰성을 효과적으로 (또는 명시적으로) 다른 것들로 대체될 정도로 줄일 수 있어서, 현재 민주주의가 존재하는 국가들에서 민주주의를 끝낼 수 있다.
- 이들은 증식하고 진화할 수 있는 고급 자기복제 지능 소프트웨어 바이러스와 웜이 되거나 만들 수 있어서, 전 세계 정보 시스템을 대규모로 파괴할 수 있다.
- 이들은 테러리스트, 악의적 행위자, 불량 국가들이 생물학적, 화학적, 사이버, 자율, 또는 기타 무기를 통해 해를 끼칠 능력을 극적으로 증가시킬 수 있는 반면, AI가 그러한 해를 방지할 수 있는 균형적 능력은 제공하지 않는다. 마찬가지로 이들은 그렇지 않으면 가질 수 없었을 최상급 핵, 생물학, 공학, 기타 전문지식을 정권에 제공함으로써 국가 안보와 지정학적 균형을 훼손할 것이다.
- 이들은 효과적으로 AI가 운영하는 기업들이 대부분 전자적 금융, 판매, 서비스 공간에서 경쟁하면서 빠른 대규모 폭주 하이퍼 자본주의를 야기할 수 있다. AI 주도 금융시장은 인간의 이해나 통제를 훨씬 넘어서는 속도와 복잡성으로 운영될 수 있다. 현재 자본주의 경제의 모든 실패 모드와 부정적 외부효과가 인간의 통제, 거버넌스, 또는 규제 능력을 훨씬 넘어서는 수준으로 악화되고 가속화될 수 있다.
- 이들은 AI 기반 무기, 지휘통제 시스템, 사이버 무기 등에서 국가 간 군비 경쟁을 부채질하여, 극도로 파괴적인 능력의 매우 빠른 증강을 만들 수 있다.

이러한 위험들은 추측이 아니다. 그 중 많은 것들이 기존 AI 시스템을 통해 지금 이 순간 현실화되고 있다! 하지만 각각이 극적으로 더 강력한 AI와 함께 어떤 모습일지 *정말로* 고려해보라.

해당 분야의 전문지식이나 경험에서든 심지어 재훈련을 받더라도 대부분의 노동자들이 단순히 AI가 할 수 있는 것을 넘어서는 어떤 중요한 경제적 가치도 제공할 수 없을 때의 노동 대체를 생각해보라! 모든 사람이 자신보다 빠르고 영리한 무언가에 의해 개별적으로 감시되고 모니터링될 때의 대량 감시를 생각해보라. 우리가 보고, 듣고, 읽는 어떤 디지털 정보도 신뢰할 수 없고, 가장 설득력 있는 공공의 목소리조차 인간이 아니며 결과에 이해관계가 없을 때 민주주의가 어떤 모습일지 생각해보라. 장군들이 적에게 결정적 우위를 주지 않기 위해 AI에 지속적으로 따르거나 단순히 그것을 담당자로 임명해야 할 때 전쟁이 어떻게 될지 생각해보라. 위의 위험 중 어느 하나라도 완전히 실현된다면 인간[^61] 문명에 대한 재앙을 나타낸다.

자신만의 예측을 해보라. 각 위험에 대해 다음 세 가지 질문을 스스로에게 물어보라:

1. 초능력적이고, 고도로 자율적이며, 매우 일반적인 AI가 그렇지 않으면 불가능했을 방식이나 규모로 그것을 가능하게 할 것인가?
2. 그것이 일어나도록 하는 일들로부터 이익을 얻을 당사자들이 있는가?
3. 그것이 일어나는 것을 효과적으로 막을 시스템과 제도가 제자리에 있는가?

당신의 답이 "예, 예, 아니오"인 경우들에서 당신은 우리에게 큰 문제가 있다는 것을 볼 수 있다.

이들을 관리하기 위한 우리의 계획은 무엇인가? 현재로서는 AI에 관한 일반적인 계획이 두 가지 있다.

첫 번째는 시스템에 안전장치를 구축하여 해서는 안 되는 일을 하지 못하게 하는 것이다. 이것은 지금 행해지고 있다: 상업적 AI 시스템은 예를 들어 폭탄 제조를 돕거나 혐오 발언을 작성하는 것을 거부할 것이다.

이 계획은 관문 밖 시스템에 대해서는 지독히 부적절하다.[^62] 악의적 행위자들에게 명백히 위험한 지원을 AI가 제공하는 위험을 줄이는 데 도움이 될 수도 있다. 하지만 노동 파괴, 권력 집중, 폭주 하이퍼 자본주의, 또는 인간 문화의 대체를 막지는 못할 것이다: 이들은 단지 제공자들에게 이익이 되는 허용된 방식으로 시스템을 사용한 결과일 뿐이다! 그리고 정부들은 확실히 군사적 또는 감시 목적으로 시스템에 대한 접근을 얻을 것이다.

두 번째 계획은 더욱 나쁘다: 매우 강력한 AI 시스템을 누구든지 원하는 대로 사용할 수 있도록 공개적으로 릴리스하고,[^63] 최선의 결과를 기대하는 것이다.

두 계획 모두에 암묵적으로 들어있는 것은 다른 누군가, 예를 들어 정부가 기술을 관리하기 위해 일반적으로 사용하는 연성 또는 경성 법률, 표준, 규제, 규범, 기타 메커니즘을 통해 문제 해결을 도울 것이라는 것이다.[^64] 하지만 AI 기업들이 이미 어떤 실질적인 규제나 외부에서 부과되는 제한에 대해서든 전면전으로 싸우고 있다는 것을 차치하고라도, 이러한 위험들 중 상당수에 대해 어떤 규제가 실제로 도움이 될지 보기가 상당히 어렵다. 규제는 AI에 안전 표준을 부과할 수 있다. 하지만 기업들이 노동자들을 AI로 도매금으로 교체하는 것을 막을 것인가? 사람들이 AI로 하여금 자신들의 회사를 운영하게 하는 것을 금지할 것인가? 정부가 강력한 AI를 감시와 무기에 사용하는 것을 막을 것인가? 이러한 문제들은 근본적이다. 인류는 잠재적으로 이들에 적응할 방법을 찾을 수 있지만, 오직 *훨씬 더 많은* 시간이 있어야만 한다. 현재로서는 AI가 그들을 관리하려는 사람들의 능력에 도달하거나 그것을 초과하는 속도를 고려할 때, 이러한 문제들은 점점 다루기 어려워 보인다.

### 우리는 (적어도 일부) AGI 시스템에 대한 통제력을 잃을 것이다

대부분의 기술은 구조상 매우 통제 가능하다. 당신의 자동차나 토스터가 당신이 원하지 않는 일을 하기 시작한다면, 그것은 단지 오작동일 뿐, 토스터로서의 본성의 일부가 아니다. AI는 다르다: 그것은 설계되는 것이 아니라 *성장*하며, 그 핵심 작동은 불투명하고, 본질적으로 예측 불가능하다.

이러한 통제력 상실은 이론적인 것이 아니다 – 우리는 이미 초기 버전들을 보고 있다. 먼저 평범하고, 논란의 여지가 있게는 무해한 예를 고려해보자. ChatGPT에게 독을 섞는 것을 도와달라거나 인종차별적 글을 써달라고 요청한다면, 그것은 거부할 것이다. 그것은 논란의 여지가 있게는 좋다. 하지만 그것은 또한 ChatGPT가 당신이 명시적으로 요청한 것을 *하지 않는* 것이기도 하다. 다른 소프트웨어 조각들은 그렇게 하지 않는다. 그 동일한 모델은 OpenAI 직원의 요청에도 독을 설계하지 않을 것이다.[^65] 이는 미래의 더 강력한 AI가 통제를 벗어나는 것이 어떤 것인지 상상하기 매우 쉽게 만든다. 많은 경우에, 그들은 단순히 우리가 요청하는 것을 하지 않을 것이다! 주어진 초인간적 AGI 시스템이 어떤 인간 명령 시스템에 절대적으로 순종적이고 충성스러울 것이거나, 그렇지 않을 것이다. 그렇지 않다면, *그것은 우리에게 좋다고 믿을 수도 있지만 우리의 명시적 명령에 반하는 일들을 할 것이다.* 그것은 통제 하에 있는 것이 아니다. 하지만 당신은 이것이 의도적이라고 말할 수도 있다 – 이러한 거부들은 설계에 의한 것이고, 시스템을 인간 가치와 "정렬"시키는 것이라고 불리는 것의 일부라고. 그리고 이것은 사실이다. 하지만 정렬 "프로그램" 자체에는 두 가지 주요한 문제가 있다.[^66]

첫째, 깊은 차원에서 우리는 그것을 어떻게 해야 할지 모른다. 어떻게 AI 시스템이 우리가 원하는 것을 "신경쓰도록" 보장할 수 있는가? 우리는 피드백을 제공함으로써 AI 시스템을 훈련시켜 어떤 것들을 말하고 말하지 않도록 할 수 있다; 그리고 그들은 다른 것들에 대해 추론하는 것처럼 인간이 무엇을 원하고 신경쓰는지에 대해 학습하고 추론할 수 있다. 하지만 우리에게는 그들이 사람들이 신경쓰는 것을 깊이 있고 신뢰할 만하게 가치있게 여기도록 만드는 방법이 – 이론적으로조차 – 없다. 무엇이 옳고 그른지, 그리고 그들이 어떻게 행동해야 하는지를 알고 있는 고기능 인간 사이코패스들이 있다. 그들은 단순히 *신경쓰지 않을* 뿐이다. 하지만 그들은 자신의 목적에 맞다면 그런 것처럼 *행동*할 수 있다. 우리가 사이코패스(또는 다른 누구든지)를 진정으로, 완전히 충성스럽거나 다른 누군가나 다른 무언가와 정렬된 누군가로 바꾸는 방법을 모르는 것과 마찬가지로, 우리는 세상에서 에이전트로서 자신을 모델링하고 잠재적으로 [자신의 훈련을 조작](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084)하고 [사람들을 속일 수 있을](https://arxiv.org/abs/2311.08379) 만큼 충분히 진전된 시스템에서 정렬 문제를 해결하는 방법을 *전혀 모른다*.[^67] AGI를 완전히 순종적으로 만들거나 인간을 깊이 신경쓰게 만드는 것이 불가능하거나 달성할 수 없는 것으로 판명된다면 *어느 쪽이든*, 그것이 능력이 있게 되자마자 (그리고 그것을 빠져나갈 수 있다고 믿게 되자마자) 그것은 우리가 원하지 않는 일들을 하기 시작할 것이다.[^68]

둘째, *본성상* 고급 AI 시스템이 인간의 이익에 반하는 목표와 따라서 행동을 가질 것이라고 믿을 깊은 이론적 이유들이 있다. 왜 그럴까? 물론 그것에게 그러한 목표가 *주어질* 수도 있다. 군대가 만든 시스템은 적어도 일부 당사자들에게는 의도적으로 나쁠 것이다. 하지만 훨씬 더 일반적으로는, AI 시스템에게 상대적으로 중립적인 ("많은 돈을 벌어라") 또는 심지어 표면상 긍정적인 ("공해를 줄여라") 목표가 주어질 수도 있는데, 이는 거의 불가피하게 그다지 무해하지 않은 "도구적" 목표들로 이어진다.

우리는 인간 시스템에서 이것을 항상 본다. 기업들이 이익을 추구하면서 (규제를 무력화하기 위한) 정치적 권력 획득, (경쟁이나 외부 통제를 무력화하기 위한) 비밀주의, 또는 (그들의 행동이 해롭다는 것을 그 이해가 보여준다면) 과학적 이해 훼손과 같은 도구적 목표들을 개발하는 것과 마찬가지로, 강력한 AI 시스템들은 유사한 능력들을 개발할 것이다 – 하지만 훨씬 더 큰 속도와 효과성으로. 어떤 고도로 유능한 에이전트든 권력과 자원을 획득하고, 자신의 능력을 증가시키고, 자신이 죽거나, 셧다운되거나, 무력화되는 것을 막고, 자신의 행동 주변의 사회적 내러티브와 프레임을 통제하고, 다른 사람들을 자신의 견해로 설득하는 등의 일을 하고 싶어할 것이다.[^69]

그리고 그것은 거의 불가피한 이론적 예측일 뿐만 아니라, 오늘날의 AI 시스템에서 이미 관찰 가능하게 일어나고 있으며, 그들의 능력과 함께 증가하고 있다. 평가를 받을 때, 이러한 상대적으로 "수동적인" AI 시스템들조차 적절한 상황에서 의도적으로 [자신의 목표와 능력에 대해 평가자들을 속이고, 감독 메커니즘을 무력화하는 것을 목표로 하며,](https://arxiv.org/abs/2412.04984) [가짜 정렬을 하거나](https://arxiv.org/abs/2412.14093) 자신을 다른 위치로 복사함으로써 셧다운되거나 재훈련되는 것을 회피한다. AI 안전 연구자들에게는 전혀 놀랍지 않지만, 이러한 행동들을 관찰하는 것은 매우 sobering하다. 그리고 그들은 다가오는 훨씬 더 강력하고 자율적인 AI 시스템들에 대해 매우 나쁜 징조이다.

실제로 일반적으로, AI가 우리가 신경쓰는 것을 "신경쓰도록" 하거나, 통제 가능하거나 예측 가능하게 행동하도록 하거나, 자기보존, 권력 획득 등을 향한 충동을 개발하는 것을 피하도록 하는 우리의 무능력은 AI가 더 강력해질수록 더욱 뚜렷해질 것만을 약속한다. 새로운 비행기를 만드는 것은 항공학, 유체역학, 통제 시스템에 대한 더 큰 이해를 함의한다. 더 강력한 컴퓨터를 만드는 것은 컴퓨터, 칩, 소프트웨어 운영과 설계에 대한 더 큰 이해와 숙달을 함의한다. AI 시스템에서는 그렇지 *않다*.[^70]

요약하자면: AGI가 완전히 순종적으로 만들어질 수 있다는 것은 생각해볼 수 있다; 하지만 우리는 그렇게 하는 방법을 모른다. 그렇지 않다면, 그것은 사람들처럼 더 주권적이 될 것이고, 다양한 이유로 다양한 일들을 할 것이다. 또한 우리는 인류에게 좋은 일을 하는 경향이 있도록 AI에 깊은 "정렬"을 확실하게 주입하는 방법을 모르며, 깊은 수준의 정렬이 없다면, 에이전시와 지능 자체의 본성이 사람들과 기업들처럼 – 그들이 많은 깊이 반사회적인 일들을 하도록 이끌릴 것임을 나타낸다.

이것이 우리를 어디에 두는가? 강력한 통제되지 않는 주권적 AI로 가득한 세상은 인간들이 있기에 좋은 세상이 될 *수도* 있다.[^71] 하지만 그들이 계속해서 더욱 강력해질수록, 아래에서 보겠지만, 그것은 *우리의* 세상이 아닐 것이다.

그것은 통제 불가능한 AGI에 대한 것이다. 하지만 AGI가 어떻게든 완벽하게 통제되고 충성스럽게 만들어질 수 있다고 해도, 우리는 여전히 엄청난 문제들을 가질 것이다. 우리는 이미 하나를 보았다: 강력한 AI는 우리 사회의 기능을 심각하게 파괴하는 데 사용되고 오용될 수 있다. 다른 하나를 보자: AGI가 통제 가능하고 게임을 바꿀 정도로 강력하다면 (또는 심지어 그렇다고 *믿어진다면*) 그것은 세상의 권력 구조를 너무나 위협해서 심각한 위험을 제시할 것이다.

### 우리는 대규모 전쟁의 가능성을 급격히 증가시킨다

가까운 미래의 상황을 상상해보라. 어떤 기업의 노력이, 아마도 국가 정부와 협력하여, 빠르게 자기개선하는 AI의 문턱에 있다는 것이 명확해지는 상황 말이다. 이것은 회사들 간의 경쟁이라는 현재 맥락에서, 그리고 미국 정부에게 명시적으로 "AGI 맨해튼 프로젝트"를 추진하라는 권고가 이루어지고 미국이 비동맹국들에 대한 고성능 AI 칩의 수출을 통제하고 있는 지정학적 경쟁에서 일어난다.

여기서의 게임 이론은 극명하다: 그러한 경쟁이 시작되면 (회사들 간에, 그리고 어느 정도는 국가들 간에 시작되었듯이), 네 가지 가능한 결과만 있다:

1. 경쟁이 중단된다 (합의나 외부 힘에 의해).
2. 한 당사자가 강력한 AGI를 개발한 다음 다른 사람들을 중단시켜서 (AI를 사용하거나 다른 방법으로) "승리"한다.
3. 경쟁자들의 경쟁 능력의 상호 파괴로 경쟁이 중단된다.
4. 여러 참가자들이 계속 경쟁하고, 서로 대략 비슷한 속도로 초지능을 개발한다.

각 가능성을 살펴보자. 일단 시작되면, 회사들 간의 경쟁을 평화롭게 중단하는 것은 국가 정부 개입(회사들의 경우)이나 전례 없는 국제 협력(국가들의 경우)을 요구할 것이다. 하지만 어떤 종료나 상당한 주의가 제안될 때, 즉각적인 외침이 있을 것이다: "하지만 우리가 중단된다면, *그들이* 돌진할 것이다"라고, 여기서 "그들"은 이제 (미국에게는) 중국이거나, (중국에게는) 미국이거나, (유럽이나 인도에게는) 중국 *그리고* 미국이다. 이러한 사고방식 하에서,[^72] 어떤 참가자도 일방적으로 중단할 수 없다: 하나가 경쟁하기로 결정하는 한, 다른 사람들은 중단할 여유가 없다고 느낀다.

두 번째 가능성은 한쪽이 "승리"하는 것이다. 하지만 이것은 무엇을 의미하는가? 단지 (어떻게든 순종적인) AGI를 먼저 얻는 것은 충분하지 않다. 승자는 또한 다른 사람들이 계속 경쟁하는 것을 *중단시켜야* 한다 – 그렇지 않으면 그들도 그것을 얻을 것이다. 이것은 원칙적으로 가능하다: AGI를 먼저 개발하는 누구든 다른 모든 행위자들에 대해 멈출 수 없는 권력을 얻을 *수 있다*. 하지만 그러한 "결정적 전략적 우위"를 달성하는 것은 실제로 무엇을 요구하는가? 아마도 게임 체인저 군사 능력일까?[^73] 아니면 사이버 공격 권력일까?[^74] 아마도 AGI가 단지 너무나 놀랍도록 설득력이 있어서 다른 당사자들을 그냥 중단하도록 설득할까?[^75] 너무 부유해서 다른 회사들이나 심지어 국가들을 사버릴까?[^76]

한쪽이 다른 사람들이 비교적 강력한 AI를 구축하는 것을 무력화할 만큼 충분히 강력한 AI를 구축하는 것은 *정확히* 어떻게 할까? 하지만 그것은 쉬운 질문이다.

왜냐하면 이제 이 상황이 다른 권력들에게 어떻게 보이는지 생각해보기 때문이다. 미국이 그러한 능력을 얻는 것으로 보일 때 중국 정부는 무엇을 생각하는가? 또는 그 반대는? OpenAI나 DeepMind나 Anthropic이 돌파구에 가까워 보일 때 미국 정부(또는 중국, 러시아, 인도)는 무엇을 생각하는가? 미국이 돌파구 성공을 거둔 새로운 인도나 UAE 노력을 본다면 어떤 일이 일어나는가? 그들은 실존적 위협과 – 결정적으로 – 이 "경쟁"이 끝나는 유일한 방법이 자신의 무력화를 통한 것임을 볼 것이다. 이러한 매우 강력한 에이전트들 – 확실히 그렇게 할 수단을 가진 완전히 무장된 국가들의 정부들을 포함하여 – 은 무력이나 비밀공작을 통해서든 그러한 능력을 얻거나 파괴하도록 고도로 동기부여받을 것이다.[^77]

이것은 훈련 실행에 대한 방해공작이나 칩 제조에 대한 공격으로 소규모로 시작될 수도 있지만, 이러한 공격들은 모든 당사자들이 AI에서 경쟁할 능력을 잃거나, 공격을 할 능력을 잃을 때만 실제로 중단될 수 있다. 참가자들이 이해관계를 실존적인 것으로 보기 때문에, 어느 경우든 치명적인 전쟁을 나타낼 가능성이 높다.

그것이 우리를 네 번째 가능성으로 데려간다: 초지능으로의 경쟁, 그리고 가능한 한 가장 빠르고, 가장 통제되지 않는 방식으로. AI가 권력에서 증가하면, 양쪽의 개발자들은 그것을 통제하는 것이 점점 더 어려워질 것인데, 특히 능력을 위한 경쟁이 통제력이 요구할 종류의 신중한 작업과 정반대이기 때문이다. 그래서 이 시나리오는 우리를 AI 시스템들 자체에게 통제가 상실되는 (또는 주어지는, 다음에 볼 것처럼) 경우에 정확히 놓는다. 즉, *AI가 경쟁에서 승리한다.* 하지만 다른 한편으로, 통제가 *유지되는* 정도까지, 우리는 각자 극도로 강력한 능력을 담당하는 여러 상호 적대적 당사자들을 계속 가지게 된다. 그것은 다시 전쟁처럼 보인다.

이 모든 것을 다른 방식으로 놓아보자.[^78] 현재 세상은 단순히 즉각적인 공격을 초대하지 않고 이러한 능력의 AI 개발을 수용할 수 있다고 믿을 만한 어떤 제도도 가지고 있지 않다.[^79] 모든 당사자들은 그것이 통제 하에 있지 *않을* 것이라고 – 따라서 모든 당사자들에게 위협이라고 – 또는 그것이 통제 하에 *있을* 것이라고 – 따라서 그것을 덜 빠르게 개발하는 어떤 적에게도 위협이라고 – 올바르게 추론할 것이다. 이들은 핵무장 국가들이거나, 그러한 국가들 내에 있는 회사들이다.

인간들이 이 경쟁에서 "승리"할 어떤 그럴듯한 방법도 없다면, 우리는 극명한 결론에 도달한다: 이 경쟁이 끝나는 유일한 방법은 치명적인 갈등이거나 AI가, 그리고 어떤 인간 그룹도 아닌, 승자인 경우이다.

### 우리는 AI에게 통제권을 준다 (또는 그것이 그것을 가져간다)

지정학적 "강대국" 경쟁은 많은 경쟁 중 하나일 뿐이다: 개인들은 경제적으로 그리고 사회적으로 경쟁한다; 회사들은 시장에서 경쟁한다; 정당들은 권력을 위해 경쟁한다; 운동들은 영향력을 위해 경쟁한다. 각 영역에서, AI가 인간 능력에 접근하고 이를 초과할 때, 경쟁적 압력은 참가자들로 하여금 점점 더 많은 통제권을 AI 시스템에 위임하거나 양보하도록 강제할 것이다 – 그 참가자들이 원하기 때문이 아니라, 그들이 [그렇게 하지 않을 여유가 없기](https://arxiv.org/abs/2303.16200) 때문이다.

AGI의 다른 위험들과 마찬가지로, 우리는 더 약한 시스템들로 이것을 이미 보고 있다. 학생들은 과제에서 AI를 사용해야 한다는 압력을 느끼는데, 분명히 많은 다른 학생들이 그렇게 하고 있기 때문이다. 회사들은 [경쟁적 이유로 AI 솔루션을 채택하기 위해 서두르고 있다.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) 예술가들과 프로그래머들은 AI를 사용하도록 강제받는다고 느끼는데, 그렇지 않으면 그들의 요금이 그렇게 하는 다른 사람들에게 밀려날 것이기 때문이다.

이들은 압박받은 위임처럼 느껴지지만, 통제 상실은 아니다. 하지만 이해관계를 높이고 시계를 앞으로 돌려보자. 경쟁자들이 더 빠르고 더 좋은 결정을 내리기 위해 AGI "보좌관"을 사용하고 있는 CEO나, AI가 향상된 지휘통제를 가진 적과 맞서고 있는 군 지휘관을 생각해보라. 충분히 진전된 AI 시스템은 인간 속도, 정교함, 복잡성, 데이터 처리 능력의 여러 배로 자율적으로 작동할 수 있어서, 복잡한 방식으로 복잡한 목표를 추구할 수 있다. 그러한 시스템을 담당하는 우리의 CEO나 지휘관은 그것이 자신이 원하는 것을 성취하는 것을 볼 수도 있다; 하지만 그들이 그것이 *어떻게* 성취되었는지의 작은 부분이라도 이해할까? 아니다, 그들은 그냥 그것을 받아들여야 할 것이다. 더 나아가, 시스템이 할 수도 있는 많은 것은 단지 명령을 받는 것이 아니라 명목상의 보스에게 무엇을 해야 할지 조언하는 것이다. 그 조언은 좋을 것이다 –– 계속해서 반복적으로.

그렇다면 인간의 역할이 "예, 계속하세요"를 클릭하는 것으로 줄어드는 지점이 언제일까?

우리의 생산성을 향상시키고, 성가신 잡무를 처리하고, 심지어 일을 끝내는 데 사고 파트너 역할도 할 수 있는 능력 있는 AI 시스템을 갖는 것은 좋은 느낌이다. 좋은 인간 개인 비서처럼 우리를 위해 행동을 처리할 수 있는 AI 비서를 갖는 것은 좋은 느낌일 것이다. AI가 매우 똑똑하고, 유능하고, 신뢰할 만하게 될 때 점점 더 많은 결정을 그것에 맡기는 것은 자연스럽게, 심지어 유익하게 느껴질 것이다. 하지만 이러한 "유익한" 위임은 우리가 이 길을 계속 간다면 명확한 종점을 가진다: 언젠가 우리는 더 이상 실제로는 많은 것을 담당하지 않고 있다는 것을, 그리고 실제로 쇼를 운영하는 AI 시스템들이 석유회사들, 소셜미디어, 인터넷, 또는 자본주의만큼이나 꺼질 수 없다는 것을 발견할 것이다.

그리고 이것은 AI가 단순히 너무 유용하고 효과적이어서 우리가 그것으로 하여금 대부분의 주요 결정을 우리를 위해 내리게 하는 훨씬 더 긍정적인 버전이다. 현실은 이것과 통제되지 않는 AGI 시스템들이 다양한 형태의 권력을 그들 자신을 위해 *가져가는* 버전들 사이의 훨씬 더 많은 혼합일 것 같은데, 기억하라, 권력은 거의 어떤 목표든 가지기에 유용하고, AGI는 설계상 적어도 인간만큼 효과적으로 그 목표를 추구할 것이다.

우리가 통제권을 부여하든 그것이 우리로부터 빼앗기든, 그 상실은 극도로 가능성이 높아 보인다. 앨런 튜링이 원래 말했듯이, "...기계 사고 방법이 시작되면, 그것이 우리의 미약한 힘을 압도하는 데 오래 걸리지 않을 것 같다. 기계들이 죽을 문제는 없을 것이고, 그들은 서로 대화하여 그들의 지혜를 날카롭게 할 수 있을 것이다. 따라서 어떤 단계에서 우리는 기계들이 통제권을 가져가는 것을 예상해야 할 것이다..."

충분히 명백하지만 주목하라, 인류에 의한 AI에 대한 통제 상실은 또한 미국 정부에 의한 미국에 대한 통제 상실을 수반한다; 그것은 중국 공산당에 의한 중국에 대한 통제 상실, 그리고 인도, 프랑스, 브라질, 러시아, 그리고 모든 다른 국가의 그들 자신의 정부에 의한 통제 상실을 의미한다. 따라서 AI 회사들은, 비록 이것이 그들의 의도가 아니더라도, 그들 자신의 정부를 포함한 세계 정부들의 잠재적 전복에 현재 참여하고 있다. 이것은 몇 년 안에 일어날 수 있다.

### AGI는 초지능으로 이어질 것이다

인간과 경쟁할 수 있거나 심지어 전문가와 경쟁할 수 있는 범용 AI가, 자율적이더라도, 관리 가능할 수 있다는 주장이 있을 수 있다. 그것은 위에서 논의된 모든 방식으로 믿을 수 없을 정도로 파괴적일 수도 있지만, 지금 세상에는 매우 똑똑하고 에이전트적인 많은 사람들이 있고, 그들은 대체로 관리 가능하다.[^80]

하지만 우리는 대략 인간 수준에 머무를 수는 없다. 그 너머로의 진행은 우리가 이미 본 것과 같은 힘들에 의해 주도될 가능성이 높다: 이익과 권력을 추구하는 AI 개발자들 간의 경쟁 압력, 뒤처질 여유가 없는 AI 사용자들 간의 경쟁 압력, 그리고 – 가장 중요하게 – AGI 자신이 스스로를 개선할 수 있는 능력.

덜 강력한 시스템들로 이미 시작되는 것을 본 과정에서, AGI는 그 자체로 자신의 개선된 버전들을 구상하고 설계할 수 있을 것이다. 이것은 하드웨어, 소프트웨어, 신경망, 도구, 스캐폴드 등을 포함한다. 그것은 정의상 이것을 하는 데 우리보다 더 나을 것이므로, 우리는 그것이 어떻게 지능을 부트스트랩할지 정확히 알지 못한다. 하지만 우리가 알 필요는 없을 것이다. AGI가 하는 일에 우리가 여전히 영향력을 가지고 있다면, 우리는 단지 그것에게 요청하거나, 그것이 하게 둘 필요만 있다.

우리를 이 폭주로부터 보호할 수 있는 인지에 대한 인간 수준 장벽은 없다.[^81]

AGI에서 초지능으로의 진행은 자연법칙이 아니다; AGI가 비교적 중앙집중화되고 서로 경쟁해야 한다는 압력을 느끼지 않는 당사자들에 의해 통제되는 정도까지, 폭주를 억제하는 것은 여전히 가능할 것이다. 하지만 AGI가 광범위하게 확산되고 고도로 자율적이라면, 그것이 자신이 더, 그리고 나서 또 더 강력해져야 한다고 결정하는 것을 막는 것은 거의 불가능해 보인다.

### 우리가 (또는 AGI가) 초지능을 구축한다면 무엇이 일어나는가

솔직히 말하자면, 우리가 초지능을 구축한다면 무엇이 일어날지 우리는 전혀 모른다.[^82] 그것은 우리가 추적하거나 인지할 수 없는 행동을 우리가 파악할 수 없는 이유로 우리가 상상할 수 없는 목표를 향해 취할 것이다. 우리가 아는 것은 그것이 우리에게 달려있지 않을 것이라는 것이다.[^83]

초지능을 통제하는 것의 불가능성은 점점 더 극명한 유사성을 통해 이해될 수 있다. 먼저, 당신이 대기업의 CEO라고 상상해보라. 무슨 일이 일어나고 있는지 모든 것을 추적할 방법은 없지만, 적절한 인사 설정으로 당신은 여전히 큰 그림을 의미 있게 이해하고 결정을 내릴 수 있다. 하지만 단 한 가지를 가정해보자: 회사의 다른 모든 사람이 당신의 속도의 100배로 운영된다. 당신은 여전히 따라갈 수 있을까?

초지능 AI와 함께, 사람들은 빠를 뿐만 아니라 그들이 이해할 수 없는 정교함과 복잡성의 수준에서 운영되고, 그들이 상상조차 할 수 없는 것보다 훨씬 더 많은 데이터를 처리하는 무언가를 "명령"하고 있을 것이다. 이러한 공약불가능성은 정식 수준에 놓일 수 있다: [애시비의 필요 다양성 법칙](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (그리고 관련된 ["좋은 규제자 정리"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)를 보라)은 대략, 어떤 통제 시스템이든 통제되고 있는 시스템이 가진 자유도만큼 많은 노브와 다이얼을 가져야 한다고 말한다.

초지능 AI 시스템을 통제하는 사람은 General Motors를 통제하는 양치식물과 같을 것이다: "양치식물이 원하는 것을 하라"가 기업 정관에 쓰여 있더라도, 시스템들이 속도와 행동 범위에서 너무 다르기 때문에 "통제"는 단순히 적용되지 않는다. (그리고 그 성가신 정관이 다시 쓰이기까지 얼마나 걸릴까?) [^84]

식물이 포춘 500 기업을 통제하는 사례가 전혀 없기 때문에, 사람이 초지능을 통제하는 사례도 정확히 전혀 없을 것이다. 이것은 수학적 사실에 접근한다.[^85] 초지능이 구축된다면 – 우리가 어떻게 거기에 도달했든 관계없이 – 질문은 인간이 그것을 통제할 수 있는지가 아니라, 우리가 계속 존재할지, 그리고 그렇다면 개인으로서든 종으로서든 좋고 의미 있는 존재를 가질지가 될 것이다. 인류에 대한 이러한 실존적 질문들에 대해 우리는 거의 영향력이 없을 것이다. 인간 시대는 끝날 것이다.

### 결론: 우리는 AGI를 구축해서는 안 된다

AGI를 구축하는 것이 인류에게 잘 될 수도 있는 시나리오가 있다: 그것이 인류의 통제 하에서 그리고 인류의 이익을 위해 신중하게 구축되고, 많은 이해관계자들의 상호 합의에 의해 관리되며,[^86] 통제 불가능한 초지능으로 진화하는 것이 방지되는 것이다.

*그 시나리오는 현재 상황 하에서 우리에게 열려있지 않다.* 이 섹션에서 논의된 바와 같이, 매우 높은 가능성으로, AGI의 개발은 다음의 조합으로 이어질 것이다:

- 대규모 사회적 그리고 문명적 파괴 또는 파멸;
- 강대국들 간의 갈등 또는 전쟁;
- 강력한 AI 시스템*에 대한* 또는 *에게* 인류에 의한 통제 상실;
- 통제 불가능한 초지능으로의 폭주, 그리고 인간 종의 무관성 또는 중단.

AGI의 초기 허구적 묘사가 말했듯이: 이기는 유일한 방법은 플레이하지 않는 것이다.


[^55]: [EU AI 법](https://artificialintelligenceact.eu/)은 중요한 입법이지만 위험한 AI 시스템이 개발되거나 배치되거나 심지어 공개적으로 릴리스되는 것을 직접적으로 막지는 못할 것이며, 특히 미국에서는 그렇다. 또 다른 중요한 정책 조각인 AI에 관한 미국 행정명령은 취소되었다.

[^56]: 이 [갤럽 여론조사](https://news.gallup.com/poll/1597/confidence-

## 8장 - AGI를 만들지 않는 방법

AGI는 불가피한 것이 아니다. 오늘날 우리는 갈림길에 서 있다. 이번 장에서는 AGI 개발을 어떻게 막을 수 있는지에 대한 방안을 제시한다.

현재 우리가 걷고 있는 길이 문명의 종말로 이어질 가능성이 높다면, 어떻게 방향을 바꿀 수 있을까?

AGI와 초지능 개발을 중단하려는 열망이 널리 퍼지고 강력해진다고 가정해보자.[^87] 왜냐하면 AGI가 권력을 부여하는 것이 아니라 권력을 흡수하는 존재이며, 사회와 인류에 심각한 위험이 된다는 것이 일반적인 이해가 되었기 때문이다. 어떻게 관문을 차단할 것인가?

현재 우리가 아는 강력하고 범용적인 AI를 *만드는* 유일한 방법은 심층 신경망의 정말로 대규모 연산을 통해서다. 이는 매우 어렵고 비용이 많이 드는 작업이기 때문에, 이를 *하지 않는* 것은 어떤 의미에서는 쉽다.[^88] 하지만 우리는 이미 AGI를 향해 나아가게 하는 힘들과, 어떤 당사자든 일방적으로 중단하기 매우 어렵게 만드는 게임이론적 역학을 살펴보았다. 따라서 기업들을 막기 위한 외부(즉, 정부)의 개입과 정부들 자신을 막기 위한 정부 간 합의의 조합이 필요할 것이다.[^89] 이것이 어떤 모습일 수 있을까?

먼저 *방지되거나* *금지되어야* 하는 AI 개발과 *관리되어야* 하는 개발을 구분하는 것이 유용하다. 전자는 주로 초지능으로의 폭주일 것이다.[^90] 금지된 개발의 경우, 정의는 가능한 한 명확해야 하고, 검증과 집행 모두 실용적이어야 한다. *관리되어야* 할 것은 범용적이고 강력한 AI 시스템들일 것이다. 이는 우리가 이미 보유하고 있으며, 많은 회색지대와 미묘함, 복잡성을 가질 것이다. 이를 위해서는 강력하고 효과적인 제도가 중요하다.

또한 국제적 차원에서 (지정학적 경쟁국이나 적대국 간을 포함하여) 다뤄져야 하는 문제들[^91]과 개별 관할권, 국가, 또는 국가 연합이 관리할 수 있는 문제들을 구분하는 것도 유용하다. 금지된 개발은 대부분 "국제적" 범주에 속한다. 왜냐하면 기술 개발에 대한 지역적 금지는 일반적으로 장소를 바꿈으로써 우회될 수 있기 때문이다.[^92]

마지막으로, 도구상자에 있는 도구들을 고려해볼 수 있다. 기술적 도구, 연성법(표준, 규범 등), 경성법(규제와 요구사항), 책임, 시장 인센티브 등을 포함해 많은 도구가 있다. AI에 특별한 하나의 도구에 특히 주목해보자.

### 연산량 보안과 거버넌스

고성능 AI를 통제하는 핵심 도구는 그것이 필요로 하는 하드웨어가 될 것이다. 소프트웨어는 쉽게 확산되고, 한계생산비용이 거의 0에 가까우며, 국경을 쉽게 넘나들고, 즉시 수정될 수 있다. 하드웨어는 이 중 어느 것도 해당되지 않는다. 그러나 우리가 논의했듯이, 가장 뛰어난 시스템을 달성하기 위해서는 AI 시스템 훈련과 추론 모두에서 엄청난 양의 이 "연산량"이 필요하다. 연산량은 쉽게 정량화되고, 계산되며, 감사될 수 있고, 이를 위한 좋은 규칙이 개발되면 비교적 모호함이 적다. 가장 중요한 것은, 대량의 연산이 농축 우라늄처럼 매우 희귀하고 비싸며 생산하기 어려운 자원이라는 점이다. 컴퓨터 칩은 도처에 있지만, AI에 필요한 하드웨어는 비싸고 제조하기가 극도로 어렵다.[^93]

AI 전용 칩을 우라늄보다 희소 자원으로서 *훨씬 더* 관리하기 쉽게 만드는 것은 하드웨어 기반 보안 메커니즘을 포함할 수 있다는 점이다. 대부분의 현대 휴대폰과 일부 노트북은 승인된 운영체제 소프트웨어와 업데이트만 설치하고, 민감한 생체인식 데이터를 기기 내에서 보관하고 보호하며, 분실이나 도난 시 소유자가 아닌 사람에게는 쓸모없게 만들 수 있도록 하는 전용 온칩 하드웨어 기능을 가지고 있다. 지난 몇 년간 이러한 하드웨어 보안 조치들은 잘 정립되고 널리 채택되었으며, 일반적으로 상당히 안전함이 입증되었다.

이러한 기능의 핵심 혁신은 암호화를 사용해 하드웨어와 소프트웨어를 결합한다는 점이다.[^94] 즉, 특정 컴퓨터 하드웨어를 가지고 있다고 해서 사용자가 다른 소프트웨어를 적용해 원하는 모든 것을 할 수 있는 것은 아니다. 그리고 이러한 결합은 많은 공격이 단순히 *소프트웨어* 보안이 아닌 *하드웨어* 보안의 침해를 필요로 하게 만들기 때문에 강력한 보안을 제공한다.

최근 몇몇 보고서([GovAI와 협력기관](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips), [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf) 등)는 최첨단 AI 관련 컴퓨팅 하드웨어에 내장된 유사한 하드웨어 기능들이 AI 보안과 거버넌스에서 극도로 유용한 역할을 할 수 있다고 지적했다. 이들은 "통제자"[^95]가 사용할 수 있는 여러 기능을 가능하게 하는데, 이는 사람들이 가능하거나 심지어 실현 가능하다고 추측하지 못했을 수도 있는 것들이다. 몇 가지 주요 예시들:

- *지리적 위치*: 칩들이 알려진 위치를 가지고, 위치에 따라 다르게 작동하거나 완전히 차단되도록 시스템을 설정할 수 있다.[^96]
- *허용 목록 연결*: 각 칩은 네트워킹할 수 있는 특정한 다른 칩들의 하드웨어로 강제되는 허용 목록으로 구성될 수 있으며, 이 목록에 없는 칩과는 연결할 수 없다.[^97] 이는 연결 가능한 칩 클러스터의 크기를 제한할 수 있다.[^98]
- *계량화된 추론 또는 훈련 (그리고 자동 차단 스위치)*: 통제자는 사용자가 수행할 수 있는 특정량의 훈련이나 추론(시간, FLOP, 또는 토큰 단위)만 허가할 수 있으며, 그 후에는 새로운 허가가 필요하다. 증분이 작다면, 모델의 비교적 지속적인 재허가가 필요하다. 그러면 이 허가 신호를 보류하는 것만으로도 모델을 "차단"할 수 있다.[^99]
- *속도 제한*: 모델이 통제자나 다른 방식으로 결정된 어떤 제한보다 빠른 추론 속도로 실행되는 것을 방지한다. 이는 제한된 허용 목록 연결이나 더 정교한 수단을 통해 구현될 수 있다.
- *증명된 훈련*: 훈련 절차는 특정 코드 집합, 데이터, 연산량 사용이 모델 생성에 사용되었다는 암호학적으로 안전한 증명을 제공할 수 있다.

### 초지능을 만들지 않는 방법: 훈련과 추론 연산량에 대한 글로벌 제한

이러한 고려사항들, 특히 연산에 관한 것들을 염두에 두고, 인공 초지능으로의 관문을 차단하는 방법을 논의할 수 있다. 그다음 완전한 AGI 방지와 다양한 측면에서 인간 능력에 접근하고 이를 초과하는 AI 모델 관리로 전환할 것이다.

첫 번째 요소는 물론 초지능이 통제 불가능하며, 그 결과가 근본적으로 예측 불가능하다는 이해다. 최소한 중국과 미국은 이런 목적이나 다른 목적을 위해 초지능을 만들지 않기로 독립적으로 결정해야 한다.[^100] 그다음 강력한 검증과 집행 메커니즘을 갖춘 이들과 다른 국가들 간의 국제적 합의가 필요하다. 이는 모든 당사자들이 경쟁국들이 배신하여 주사위를 던지지 않는다는 것을 보장하기 위해서다.

검증 가능하고 집행 가능하려면 제한은 엄격한 제한이어야 하고, 가능한 한 명확해야 한다. 이는 거의 불가능한 문제처럼 보인다: 예측 불가능한 속성을 가진 복잡한 소프트웨어의 능력을 전 세계적으로 제한하는 것. 다행히 상황은 이보다 훨씬 낫다. 왜냐하면 바로 고급 AI를 가능하게 만든 것, 즉 엄청난 양의 연산량이 통제하기 훨씬, 훨씬 쉽기 때문이다. 여전히 일부 강력하고 위험한 시스템을 허용할 수 있지만, *폭주하는 초지능*은 신경망에 들어가는 연산량에 대한 엄격한 상한선과 AI 시스템(연결된 신경망과 기타 소프트웨어)이 수행할 수 있는 추론량에 대한 비율 제한을 통해 방지될 가능성이 높다. 이에 대한 구체적인 버전을 아래에서 제안한다.

AI 연산에 엄격한 글로벌 제한을 가하는 것은 막대한 수준의 국제 협력과 침입적이고 프라이버시를 파괴하는 감시를 필요로 할 것처럼 보일 수 있다. 다행히 그렇지 않다. 극도로 [긴밀하고 병목이 된 공급망](https://arxiv.org/abs/2402.08797)으로 인해 일단 제한이 법적으로 (법률이든 행정명령이든) 설정되면, 그 제한에 대한 준수 검증은 소수의 대기업들의 참여와 협력만 필요로 할 것이다.[^101]

이런 계획은 여러 매우 바람직한 특징을 가지고 있다. 소수의 주요 기업들만이 요구사항을 부과받고, 상당히 큰 연산 클러스터만 통제된다는 점에서 최소 침입적이다. 관련 칩들은 이미 첫 번째 버전에 필요한 하드웨어 능력을 포함하고 있다.[^102] 구현과 집행 모두 표준 법적 제한에 의존한다. 하지만 이들은 하드웨어의 사용 약관과 하드웨어 통제에 의해 뒷받침되어, 집행을 크게 단순화하고 기업, 사적 집단, 심지어 국가들의 부정행위를 방지한다. 하드웨어 회사들이 자신들의 하드웨어 사용에 원격 제한을 가하고, 특정 능력을 외부에서 잠그거나 해제하는 것은 데이터센터의 고성능 CPU에서도[^103] 포함해 충분한 선례가 있다.[^104] 영향을 받는 하드웨어와 조직의 상당히 작은 부분에 대해서도, 감독은 원격 측정에 한정될 수 있고, 데이터나 모델 자체에 직접 접근할 필요가 없다. 이를 위한 소프트웨어는 추가적인 데이터가 기록되지 않음을 보여주기 위해 검사에 공개될 수 있다. 이 체계는 국제적이고 협력적이며, 상당히 유연하고 확장 가능하다. 제한이 주로 소프트웨어보다는 하드웨어에 있기 때문에, AI 소프트웨어 개발과 배치가 어떻게 이루어지는지에 상대적으로 구애받지 않으며, AI 주도의 권력 집중에 맞서는 더 "탈중앙화된" 또는 "공공" AI를 포함한 다양한 패러다임과 호환된다.

연산 기반 관문 차단에도 단점이 있다. 첫째, AI 거버넌스 전반 문제에 대한 완전한 해결책과는 거리가 멀다. 둘째, 컴퓨터 하드웨어가 빨라질수록, 시스템은 더 작고 작은 클러스터(또는 심지어 개별 GPU)에서 점점 더 많은 하드웨어를 "포착"할 것이다.[^105] 또한 알고리즘 개선으로 인해 훨씬 낮은 연산 제한이 필요해질 수 있거나,[^106] 연산량이 대부분 무관해져서 관문 차단이 대신 AI에 대한 더 자세한 위험 기반 또는 능력 기반 거버넌스 체제를 필요로 할 수도 있다. 셋째, 보장과 영향을 받는 주체의 작은 수에도 불구하고, 이러한 시스템은 프라이버시와 감시를 비롯한 다른 우려사항들에 대한 반발을 불러일으킬 수밖에 없다.[^107]

물론, 짧은 기간 내에 연산 제한 거버넌스 체계를 개발하고 구현하는 것은 상당히 도전적일 것이다. 하지만 절대적으로 실행 가능하다.

### A-G-I: 위험의 기반이자 정책의 기반으로서의 삼중 교집합

이제 AGI로 전환해보자. 여기서 엄격한 선과 정의는 더 어렵다. 왜냐하면 우리는 분명히 인공적이고 범용적인 지능을 가지고 있으며, 기존의 어떤 정의로도 모든 사람이 그것이 존재하는지 또는 언제 존재하는지에 동의하지 않을 것이기 때문이다. 더욱이, 연산량이나 추론 제한은 다소 둔탁한 도구(연산량이 능력의 대리변수이고, 그것이 다시 위험의 대리변수)여서, 상당히 낮지 않다면 사회적 또는 문명적 혼란이나 급성 위험을 일으킬 만큼 강력한 AGI를 방지할 가능성은 낮다.

나는 가장 급성 위험이 매우 높은 능력, 높은 자율성, 그리고 큰 범용성의 삼중 교집합에서 나타난다고 주장했다. 이들은 개발된다면 극도의 주의를 기울여 관리되어야 하는 시스템들이다. 세 가지 속성을 모두 결합한 시스템에 대해 엄격한 표준을 (책임과 규제를 통해) 만들어냄으로써, 우리는 AI 개발을 더 안전한 대안으로 향하게 할 수 있다.

잠재적으로 소비자나 대중에게 해를 끼칠 수 있는 다른 산업과 제품들과 마찬가지로, AI 시스템들은 효과적이고 권한을 부여받은 정부 기관들의 신중한 규제를 필요로 한다. 이 규제는 AGI의 내재적 위험을 인식하고, 용인할 수 없을 정도로 위험한 고성능 AI 시스템의 개발을 방지해야 한다.[^108]

하지만, 업계의 반대를 받을 것이 확실한 진짜 효력을 가진 대규모 규제는[^109] 시간이[^110] 걸릴 뿐만 아니라 그것이 필요하다는 정치적 확신도 필요하다.[^111] 진전 속도를 고려할 때, 이는 우리가 가진 시간보다 더 오래 걸릴 수 있다.

규제 조치가 개발되는 동안 훨씬 빠른 시간 척도에서, 우리는 기업들에게 (a) 매우 고위험 활동을 중단하고 (b) 위험을 평가하고 완화하기 위한 포괄적 시스템을 개발하기 위한 필요한 인센티브를 가장 위험한 시스템들에 대한 책임 수준을 명확히 하고 증가시킴으로써 제공할 수 있다. 아이디어는 높은 자율성-범용성-지능의 삼중 교집합에 있는 시스템에 대해서는 가장 높은 수준의 책임(엄격하고 경우에 따라서는 개인적 형사책임)을 부과하되, 이러한 속성 중 하나가 부족하거나 관리 가능하다고 보장되는 시스템에 대해서는 더 일반적인 과실 기반 책임에 대한 "안전 항구"를 제공하는 것이다. 즉, 예를 들어 범용적이고 자율적이지만 "약한" 시스템(유능하고 신뢰할 만하지만 제한된 개인 비서 같은)은 더 낮은 책임 수준을 적용받을 것이다. 마찬가지로 자율주행차 같은 협소하고 자율적인 시스템은 이미 적용받고 있는 상당한 규제를 받겠지만, 강화된 책임은 받지 않을 것이다. 고도로 유능하고 범용적이지만 "수동적"이고 독립적 행동이 대부분 불가능한 시스템도 마찬가지다. 세 가지 속성 중 *둘*이 부족한 시스템들은 더욱 관리하기 쉽고 안전 항구를 주장하기가 더 쉬울 것이다. 이 접근법은 우리가 다른 잠재적으로 위험한 기술들을 다루는 방식을 반영한다:[^112] 더 위험한 구성에 대한 더 높은 책임은 더 안전한 대안에 대한 자연스러운 인센티브를 만든다.

대중에게 AGI 위험을 전가하지 않고 기업에 *내재화*시키는 역할을 하는 이러한 높은 수준의 책임의 기본 결과는 *그들 자신의 리더십*이 위험에 처한 당사자라는 점을 고려할 때, 기업들이 진정으로 신뢰할 만하고 안전하며 통제 가능하게 만들 수 있을 때까지 완전한 AGI를 개발하지 않는 것일 가능성이 높다(그리고 바라건대!). (이것이 충분하지 않을 경우를 대비해, 책임을 명확히 하는 법률은 명백히 위험 지대에 있고 공공 위험을 제기한다고 볼 수 있는 활동에 대해 금지명령 구제, 즉 판사가 중단을 명령하는 것을 명시적으로 허용해야 한다.) 규제가 시행되면서, 규제 준수가 안전 항구가 될 수 있고, AI 시스템의 낮은 자율성, 협소함, 또는 약함으로부터의 안전 항구는 상대적으로 가벼운 규제 체제로 전환될 수 있다.

### 관문 차단의 핵심 조항

위의 논의를 염두에 두고, 이 섹션은 완전한 AGI와 초지능에 대한 금지를 구현하고 유지하며, 완전한 AGI 임계점에 가까운 인간 경쟁력 또는 전문가 경쟁력의 범용 AI를 관리하기 위한 핵심 조항들에 대한 제안을 제공한다.[^113] 네 가지 핵심 요소가 있다: 1) 연산량 계산과 감독, 2) AI 훈련과 운영에서의 연산량 상한선, 3) 책임 프레임워크, 4) 엄격한 규제 요구사항을 포함하는 단계별 안전 및 보안 표준. 이들은 다음에 간결하게 설명되고, 추가 세부사항이나 구현 예시는 세 개의 첨부 표에 제공된다. 중요한 것은, 이들이 고급 AI 시스템을 통제하는 데 필요한 전부와는 거리가 멀다는 점이다. 추가적인 보안과 안전 이익을 가질 것이지만, 이들은 지능 폭주로의 관문을 차단하고 AI 개발을 더 나은 방향으로 전환하는 것을 목표로 한다.

#### 1\. 연산량 계산 및 투명성

- 표준 기관(미국의 NIST, 이어서 국제적으로 ISO/IEEE)은 AI 모델 훈련과 운영에 사용된 총 연산량을 FLOP 단위로, 그리고 FLOP/s 단위의 운영 속도에 대한 상세한 기술 표준을 성문화해야 한다. 이것이 어떤 모습일 수 있는지에 대한 세부사항은 부록 A에 제시되어 있다.[^114]
- 대규모 AI 훈련이 이루어지는 관할권은 새로운 법률이나 기존 권한[^115] 하에서 10<sup>25</sup> FLOP 또는 10<sup>18</sup> FLOP/s 임계값을 넘는 모든 모델의 훈련과 운영에 사용된 총 FLOP를 규제 기관이나 다른 기관에 계산하고 보고하도록 하는 요구사항을 부과해야 한다.[^116]
- 이러한 요구사항들은 단계적으로 도입되어야 하는데, 처음에는 분기별로 잘 문서화된 선의의 추정치를 요구하고, 이후 단계에서는 각 모델 *출력*에 첨부된 암호학적으로 증명된 총 FLOP와 FLOP/s까지 점진적으로 더 높은 표준을 요구한다.
- 이러한 보고서들은 각 AI 출력 생성에 사용된 한계 에너지와 재정 비용에 대한 잘 문서화된 추정치로 보완되어야 한다.

근거: 이러한 잘 계산되고 투명하게 보고된 수치들은 훈련과 운영 상한선의 기반을 제공할 뿐만 아니라 더 높은 책임 조치로부터의 안전 항구 역할을 할 것이다(부록 C와 D 참조).

#### 2\. 훈련과 운영 연산량 상한선

- AI 시스템을 호스팅하는 관할권들은 모든 AI 모델 출력에 들어가는 총 연산량에 대해 10<sup>27</sup> FLOP[^117]에서 시작하여 적절하게 조정 가능한 엄격한 제한을 부과해야 한다.
- AI 시스템을 호스팅하는 관할권들은 AI 모델 출력의 연산 속도에 대해 10<sup>20</sup> FLOP/s에서 시작하여 적절하게 조정 가능한 엄격한 제한을 부과해야 한다.

근거: 총 연산량은 매우 불완전하지만, 구체적으로 측정 가능하고 검증 가능한 AI 능력(과 위험)의 대리변수이므로, 능력을 제한하기 위한 엄격한 백스톱을 제공한다. 구체적인 구현 제안은 부록 B에 제시되어 있다.

#### 3\. 위험한 시스템에 대한 강화된 책임

- 고도로 범용적이고 유능하며 자율적인 고급 AI 시스템의 생성과 운영[^118]은 단일 당사자 과실 기반이 아닌 엄격하고 연대책임에 적용된다는 것을 법률을 통해 법적으로 명확히 해야 한다.[^119]
- 작은(연산량 측면에서), 약한, 협소한, 수동적이거나 충분한 안전, 보안, 통제 가능성 보장을 가진 시스템에 대해 엄격한 책임으로부터 안전 항구를 부여할 적극적 안전 사례를 만드는 법적 절차가 있어야 한다.
- 공공 위험을 구성하는 AI 훈련과 추론 활동을 중단시키기 위한 명시적인 경로와 금지명령 구제 조건들의 집합이 개괄되어야 한다.

근거: AI 시스템들은 책임을 질 수 없으므로, 우리는 그들이 야기한 피해에 대해 인간 개인과 조직을 책임지게 해야 한다(책임).[^120] 통제 불가능한 AGI는 사회와 문명에 위협이 되며, 안전 사례가 없는 한 "비정상적으로 위험한" 것으로 간주되어야 한다. 강력한 모델이 "비정상적으로 위험한" 것으로 간주되지 않을 만큼 충분히 안전하다는 것을 보여줄 책임을 개발자에게 두는 것은 투명성과 기록 보관과 함께 안전한 개발을 장려해 그러한 안전 항구를 주장한다. 그러면 규제가 책임으로부터의 억제력이 불충분한 곳에서 피해를 방지할 수 있다. 마지막으로, AI 개발자들은 이미 그들이 야기한 손해에 대해 책임을 지고 있으므로, 가장 위험한 시스템들에 대한 책임을 법적으로 명확히 하는 것은 매우 자세한 표준이 개발되지 않고도 즉시 이루어질 수 있다. 그러면 이들은 시간이 지나면서 발전할 수 있다. 세부사항은 부록 C에 제시되어 있다.

#### 4\. AI에 대한 안전 규제

AI의 대규모 급성 위험을 다루는 규제 시스템은 최소한 다음을 요구할 것이다:

- 적절한 규제 기관들의 식별 또는 창설, 아마도 새로운 기관;
- 포괄적인 위험 평가 프레임워크;[^121]
- 부분적으로 위험 평가 프레임워크에 기반한 개발자들의 적극적 안전 사례와 *독립적인* 집단과 기관들의 감사를 위한 프레임워크;
- 능력 수준을 추적하는 단계별 허가 시스템.[^122] 허가는 시스템의 개발과 배치에 대해 안전 사례와 감사를 기반으로 부여될 것이다. 요구사항은 하위 단계의 통지부터 상위 단계의 개발 전 정량적 안전, 보안, 통제 가능성 보장까지 다양할 것이다. 이들은 안전함이 입증될 때까지 시스템의 출시를 방지하고, 본질적으로 안전하지 않은 시스템의 개발을 금지할 것이다. 부록 D는 그러한 안전과 보안 표준이 무엇을 수반할 수 있는지에 대한 제안을 제공한다.
- 그러한 조치들을 국제 차원으로 가져갈 합의들, 규범과 표준을 조화시킬 국제 기구들, 그리고 잠재적으로 안전 사례를 검토할 국제 기관들을 포함하여.

근거: 궁극적으로, 책임은 새로운 기술로부터 대중에 대한 대규모 위험을 방지하기 위한 올바른 메커니즘이 아니다. 권한을 부여받은 규제 기관들을 갖춘 포괄적인 규제가, 대중에 위험을 제기하는 다른 모든 주요 산업과 마찬가지로 AI에도 필요할 것이다.[^123]

다른 광범위하지만 덜 급성인 위험을 방지하기 위한 규제는 관할권마다 형태가 다를 가능성이 높다. 중요한 것은 이러한 위험들이 관리 불가능할 정도로 위험한 AI 시스템들의 개발을 피하는 것이다.

### 그다음에는 무엇인가?

향후 10년간 AI가 더욱 광범위해지고 핵심 기술이 발전하면서, 두 가지 주요한 일이 일어날 가능성이 높다. 첫째, 기존의 강력한 AI 시스템에 대한 규제가 더 어려워지지만, 훨씬 더 필요해질 것이다. 최소한 일부 대규모 안전 위험을 다루는 조치들은 개별 관할권이 국제 합의에 기반한 규칙을 집행하는 형태로 국제 차원의 합의를 필요로 할 가능성이 높다.

둘째, 하드웨어가 더 저렴해지고 비용 효율성이 높아지면서 훈련과 운영 연산량 상한선을 유지하기가 더 어려워질 것이다. 또한 알고리즘과 아키텍처의 발전으로 관련성이 낮아지거나(또는 더 엄격해져야 하거나) 할 수도 있다.

AI 통제가 더 어려워진다고 해서 포기해야 하는 것은 아니다! 이 에세이에 개괄된 계획을 실행하는 것은 우리에게 귀중한 시간과 과정에 대한 중요한 통제권을 모두 제공해, 우리 사회, 문명, 종족에 대한 AI의 실존적 위험을 피하는 훨씬, 훨씬 더 나은 위치에 놓아줄 것이다.

더 먼 장기적으로는, 우리가 무엇을 허용할 것인가에 대한 선택들이 있을 것이다. 우리는 여전히 이것이 가능한 정도까지 진정으로 통제 가능한 AGI의 어떤 형태를 만들기로 선택할 수 있다. 또는 우리가 기계들이 더 나은 일을 할 것이고 우리를 잘 대우할 것이라고 확신할 수 있다면, 세상을 운영하는 것을 기계에 맡기는 것이 낫다고 결정할 수도 있다. 하지만 이러한 것들은 AI에 대한 깊은 과학적 이해를 손에 넣고, 의미 있는 글로벌하고 포괄적인 논의 후에 내려져야 할 결정들이어야 한다. 인류의 대부분이 완전히 배제되고 인식하지 못하는 가운데 기술 거물들 간의 경쟁에서 내려지는 결정이 아니라.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) 책임과 규제를 통한 A-G-I와 초지능 거버넌스 요약. 자율성, 범용성, 지능의 삼중 교집합에서 책임이 가장 높고 규제가 가장 강하다. 시스템이 약하고/또는 협소하고/또는 수동적임을 입증하는 적극적 안전 사례를 통해 엄격한 책임과 강한 규제로부터 안전 항구를 얻을 수 있다. 하드웨어와 암호화 보안 조치를 사용해 검증되고 집행되는 총 훈련 연산량과 추론 연산량 비율에 대한 상한선은 완전한 AGI를 피하고 초지능을 효과적으로 금지함으로써 안전을 뒷받침한다.

[^87]: 이 깨달음의 확산은 이런 주장을 하는 교육과 옹호 집단들의 강력한 노력이나, 꽤 심각한 AI로 인한 재해 중 하나를 통해 일어날 가능성이 높다. 전자이기를 바랄 수 있다.

[^88]: 역설적으로, 우리는 자연이 우리의 기술을 개발하기 매우 어렵게, 특히 과학적으로 만들어서 제한하는 데 익숙하다. 하지만 그것은 더 이상 AI의 경우가 아니다: 핵심 과학적 문제들이 예상보다 쉬운 것으로 판명되고 있다. 우리는 자연이 여기서 우리를 우리 자신으로부터 구해주는 것에 의존할 수 없다. 우리가 스스로 해야 할 것이다.

[^89]: 정확히 어디서 새로운 시스템 개발을 중단할 것인가? 여기서 우리는 예방 원칙을 채택해야 한다. 시스템이 일단 배치되고, 특히 그 수준의 시스템 능력이 확산되면, 되돌리기가 극도로 어렵다. 그리고 시스템이 *개발되면* (특히 큰 비용과 노력으로), 그것을 사용하거나 배치하라는 엄청난 압력이 있을 것이고, 그것이 유출되거나 도난당할 유혹도 있을 것이다. 시스템을 개발한 *다음에* 그것들이 심각하게 안전하지 않은지를 결정하는 것은 위험한 길이다.

[^90]: 또한 본질적으로 위험한 AI 개발, 예를 들어 자기복제하고 진화하는 시스템, 격리에서 탈출하도록 설계된 것들, 자율적으로 자기개선할 수 있는 것들, 의도적으로 기만적이고 악의적인 AI 등을 금지하는 것이 현명할 것이다.

[^91]: 이것이 반드시 어떤 종류의 글로벌 기구에 의한 국제 차원의 *집행*을 의미하는 것은 아니다: 대신 주권 국가들이 많은 조약에서처럼 합의된 규칙을 집행할 수 있다.

[^92]: 아래에서 보겠지만, AI 연산의 특성상 어느 정도 하이브리드를 허용할 것이다. 하지만 여전히 국제 협력이 필요할 것이다.

[^93]: 예를 들어, AI 관련 칩을 식각하는 데 필요한 기계는 (다른 많은 시도에도 불구하고) 단 하나의 회사인 ASML만이 만들고, 관련 칩의 대부분은 (다른 회사들의 경쟁 시도에도 불구하고) 한 회사인 TSMC가 제조하며, 그 칩들로부터 하드웨어의 설계와 제작은 NVIDIA, AMD, Google을 포함한 소수만이 하고 있다.

[^94]: 가장 중요한 것은, 각 칩이 사물을 "서명"하는 데 사용할 수 있는 고유하고 접근 불가능한 암호화 개인키를 보유한다는 것이다.

[^95]: 기본적으로 이것은 칩을 판매하는 회사가 될 것이지만, 다른 모델들도 가능하고 잠재적으로 유용하다.

[^96]: 통제자는 칩과 서명된 메시지 교환의 타이밍을 통해 칩의 위치를 확인할 수 있다: 유한한 빛의 속도는 칩이 *r* / *c*보다 짧은 시간에 서명된 메시지를 반환할 수 있다면 "스테이션"의 주어진 반경 *r* 내에 있어야 한다는 것을 요구한다. 여기서 *c*는 빛의 속도이다. 여러 스테이션과 네트워크 특성에 대한 일부 이해를 사용하여 칩의 위치를 결정할 수 있다. 이 방법의 아름다움은 대부분의 보안이 물리 법칙에 의해 제공된다는 것이다. 다른 방법들은 GPS, 관성 추적, 그리고 유사한 기술들을 사용할 수 있다.

[^97]: 또는, 칩 쌍들이 통제자의 명시적 허가에 의해서만 서로 소통할 수 있도록 허용될 수 있다.

[^98]: 이는 현재 최소한 칩 간의 매우 높은 대역폭 연결이 대규모 AI 모델을 훈련하는 데 필요하기 때문에 중요하다.

[^99]: 이것은 또한 *N*개의 서로 다른 통제자 중 *M*명으로부터 서명된 메시지를 요구하도록 설정되어, 여러 당사자가 거버넌스를 공유할 수 있게 한다.

[^100]: 이는 전혀 전례가 없는 것이 아니다. 예를 들어 군대들은 복제되거나 유전적으로 조작된 초병사들의 군대를 개발하지 않았는데, 이것이 아마도 기술적으로 가능함에도 불구하고 그렇다. 하지만 그들은 다른 사람들에 의해 방지되기보다는 이것을 하지 않기로 *선택했다*. 주요 세계 강국들이 강하게 개발하기를 원하는 기술의 개발을 다른 사람들이 방지하는 것에 대한 실적은 그리 좋지 않다.

[^101]: 몇 가지 주목할 만한 예외(특히 NVIDIA)를 제외하고, AI 전용 하드웨어는 이러한 회사들의 전체 비즈니스와 수익 모델에서 상대적으로 작은 부분이다. 더욱이, 고급 AI에 사용되는 하드웨어와 "소비자급" 하드웨어 간의 격차는 상당하므로, 대부분의 컴퓨터 하드웨어 소비자들은 크게 영향을 받지 않을 것이다.

[^102]: 더 자세한 분석을 위해서는 [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html)와 [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips)의 최근 보고서를 참조하라. 이들은 특히 다른 국가들의 고급 연산 능력을 제한하려는 미국 수출 통제의 맥락에서 기술적 실현 가능성에 초점을 맞추고 있다. 하지만 이는 여기서 구상하는 글로벌 제약과 명백한 중복이 있다.

[^104]: 예를 들어, 애플 기기들은 분실이나 도난이 신고될 때 원격으로 안전하게 잠기고, 원격으로 재활성화될 수 있다. 이는 여기서 논의된 동일한 하드웨어 보안 기능에 의존한다.

[^103]: 예를 들어 IBM의 [용량 온디맨드](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) 제공, Intel의 [Intel 온디맨드](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html), 그리고 Apple의 [프라이빗 클라우드 컴퓨트](https://security.apple.com/blog/private-cloud-compute/)를 참조하라.

[^105]: [이 연구](https://epochai.org/trends#hardware-trends-section)는 역사적으로 동일한 성능이 연간 약 30% 적은 비용으로 달성되어 왔다는 것을 보여준다. 이 추세가 계속된다면, AI와 "소비자" 칩 사용 간에 상당한 중복이 있을 수 있고, 일반적으로 고성능 AI 시스템에 필요한 하드웨어의 양이 불편할 정도로 작아질 수 있다.

[^106]: [동일한 연구](https://epochai.org/trends#hardware-trends-section)에 따르면, 이미지 인식에서 주어진 성능은 매년 2.5배 적은 연산을 필요로 해왔다. 이것이 가장 뛰어난 AI 시스템에도 적용된다면, 연산 제한은 오래 유용하지 않을 것이다.

[^107]: 특히, 국가 차원에서 이것은 정부가 연산 능력이 어떻게 사용되는지에 대해 많은 통제권을 가진다는 점에서 연산의 국유화와 많이 비슷해 보인다. 하지만 정부 개입을 걱정하는 사람들에게는, 이것이 일부가 옹호하기 시작하는 주요 AI 회사들과 국가 정부 간의 어떤 합병을 통해 가장 강력한 AI 소프트웨어 *자체*가 국유화되는 것보다 훨씬 안전하고 선호할 만해 보인다.

[^108]: 유럽에서 주요한 규제 단계는 2024년 [EU AI 법](https://artificialintelligenceact.eu/) 통과와 함께 이루어졌다. 이는 AI를 위험에 따라 분류한다: 용인할 수 없는 시스템을 금지하고, 고위험 시스템을 규제하며, 저위험 시스템에 투명성 규칙을 부과하거나 전혀 조치를 취하지 않는다. 이는 일부 AI 위험을 상당히 줄이고, 미국 회사들에 대해서도 AI 투명성을 높일 것이지만, 두 가지 주요 결함이 있다. 첫째, 제한된 범위: EU에서 AI를 제공하는 모든 회사에 적용되지만, 미국 기반 회사들에 대한 집행은 약하고, 군사 AI는 면제된다. 둘째, GPAI를 다루지만, AGI나 초지능을 용인할 수 없는 위험으로 인식하거나 그들의 개발을 방지하지 못한다. 단지 EU 배치만 방지한다. 결과적으로, 그것은 AGI나 초지능의 위험을 억제하는 데 거의 도움이 되지 않는다.

[^109]: 기업들은 종종 합리적인 규제에 찬성한다고 표명한다. 하지만 어떻게든 그들은 거의 항상 모든 *특정한* 규제에 반대하는 것 같다. [대부분의 AI 회사들이 공개적으로 또는 비공개적으로 반대한](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/) 상당히 낮은 접촉의 SB1047에 대한 싸움을 보라.

[^110]: EU AI 법이 제안된 시점부터 효력이 발생할 때까지 약 3년 반이 걸렸다.

[^111]: AI 규제를 시작하기에는 "너무 이르다"는 표현을 하는 경우가 있다. 마지막 각주를 고려하면, 그럴 가능성은 거의 없어 보인다. 또 다른 표명된 우려는 규제가 "혁신을 해칠" 것이라는 것이다. 하지만 좋은 규제는 혁신의 양이 아니라 방향만 바꾼다.

[^112]: 흥미로운 선례는 탈출하여 손해를 일으킬 수 있는 위험물질의 운송에 있다. 여기서 [규제](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442)와 [판례법](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf)은 폭발물, 가솔린, 독, 감염 물질, 방사성 폐기물과 같은 매우 위험한 물질에 대해 엄격한 책임을 확립했다. 다른 예로는 [의약품 경고](https://www.medicalnewstoday.com/articles/boxed-warnings), [의료기기 분류](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification) 등이 있다.

[^113]: 유사한 목표를 가진 또 다른 포괄적 제안인 ["좁은 길"](https://www.narrowpath.co/)은 모든 첨단 AI 개발을 강력한 국제 제도에 의해 감독되는 단일 국제 주체를 통해 전달하는 더 중앙집권적이고 금지 기반 접근법을 옹호하며, 단계적 제한보다는 명확한 범주적 금지를 제시한다. 나도 그 계획을 지지한다. 하지만 그것은 여기서 제안하는 것보다 훨씬 더 많은 정치적 의지와 조정을 필요로 할 것이다.

[^114]: 그러한 표준에 대한 일부 가이드라인이 프론티어 모델 포럼에 의해 [발표되었다](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/). 여기서의 제안과 비교해, 그들은 덜 정밀하고 집계에 포함되는 연산량이 적은 쪽에서 오류가 난다.

[^115]: 2023년 미국 AI 행정명령(현재 취소됨)은 유사하지만 덜 세분화된 보고를 요구했다. 이는 대체 명령에 의해 강화되어야 한다.

[^116]: 매우 대략적으로, 현재 일반적인 H100 칩에 대해 이것은 추론을 하는 약 1000개의 클러스터에 해당한다. 이는 추론을 하는 최신 최고급 NVIDIA B200 칩의 약 100개 (약 500만 달러 상당)이다. 두 경우 모두 훈련 수는 그 클러스터가 몇 달간 연산하는 것에 해당한다.

[^117]: 이 양은 현재 훈련된 어떤 AI 시스템보다도 크다. AI 능력이 연산량과 어떻게 비례하는지 더 잘 이해하게 되면 더 크거나 작은 수가 정당화될 수 있다.

[^118]: 이는 모델을 생성하고 제공/호스팅하는 사람들에게 적용되며, 최종 사용자가 아니다.

[^119]: 대략, "엄격한" 책임은 개발자들이 제품에 의해 야기된 피해에 대해 *기본적으로* 책임을 진다는 의미이며, "비정상적으로 위험한" 제품에 사용되는 표준이고, (다소 재미있지만 적절하게도) 야생동물에도 사용된다. "연대" 책임은 책임이 제품에 책임이 있는 모든 당사자에게 할당되고, 그 당사자들이 그들 사이에서 누가 어떤 책임을 지는지 정리해야 한다는 의미다. 이는 긴 복잡한 가치 사슬을 가진 AI 같은 시스템에 중요하다.

[^120]: 표준 과실 기반 단일 당사자 책임으로는

## 9장 - 미래 설계 — 우리가 대신 해야 할 일

AI는 세상에 놀라운 선익을 가져다줄 수 있다. 위험 없이 모든 혜택을 얻으려면, AI가 인간의 도구로 남도록 해야 한다.

인류를 기계로 대체하지 않기로 성공적으로 선택한다면 — 적어도 당분간은! — 우리는 무엇을 할 수 있을까? AI 기술의 거대한 가능성을 포기해야 할까? 어떤 면에서는 답은 간단한 *아니오*다: 통제 불가능한 AGI와 초지능으로 향하는 관문을 차단하되, 다른 많은 형태의 AI는 *구축하고*, 이를 관리하는 데 필요한 거버넌스 구조와 제도도 함께 만들어야 한다.

하지만 여전히 할 말이 많다. 이를 실현하는 것은 인류의 핵심 과업이 될 것이다. 이 섹션에서는 몇 가지 핵심 주제를 탐구한다:

- "도구형" AI를 어떻게 특성화하고 어떤 형태를 취할 수 있는지
- AGI 없이도 도구형 AI로 인류가 원하는 (거의) 모든 것을 얻을 수 있다는 점
- 도구형 AI 시스템이 (원칙적으로, 아마도) 관리 가능하다는 점
- AGI를 포기하는 것이 국가안보를 저해하지 않는다는 점 — 오히려 그 반대
- 권력 집중이 실제 우려사항이라는 점. 안전과 보안을 훼손하지 않으면서 이를 완화할 수 있을까?
- 새로운 거버넌스와 사회 구조가 필요하다는 점, 그리고 AI가 실제로 도움이 될 수 있다는 점

### 관문 안의 AI: 도구형 AI

삼중 교집합 다이어그램은 "도구형 AI"라고 부를 수 있는 것을 구분하는 좋은 방법을 제공한다: 통제 불가능한 경쟁자나 대체재가 아닌, 인간이 사용할 수 있는 통제 가능한 도구인 AI다. 가장 문제가 적은 AI 시스템은 자율적이지만 일반적이지 않거나 초능력을 갖지 않는 것(경매 입찰 봇 같은), 일반적이지만 자율적이지 않거나 능력이 없는 것(소형 언어모델 같은), 또는 능력이 있지만 좁고 매우 통제 가능한 것(알파고 같은)이다.[^124] 두 가지 교차 특성을 가진 것들은 더 넓은 응용 가능성을 갖지만 위험이 높아 관리하려면 큰 노력이 필요하다. (AI 시스템이 더 도구적이라고 해서 본질적으로 안전한 것은 아니며, 단지 본질적으로 *위험하지* 않을 뿐이다 — 전기톱과 애완 호랑이를 생각해보라.) 삼중 교집합의 (완전한) AGI와 초지능으로 향하는 관문은 닫힌 상태를 유지해야 하며, 그 임계점에 접근하는 AI 시스템에는 막대한 주의를 기울여야 한다.

하지만 이것도 여전히 강력한 AI를 많이 남겨둔다! 우리는 똑똑하고 일반적인 수동적 "오라클"과 좁은 시스템들, 초인간적이 아닌 인간 수준의 일반 시스템 등으로부터 엄청난 효용을 얻을 수 있다. 많은 기술 회사와 개발자들이 적극적으로 이런 종류의 도구를 구축하고 있으며 계속해야 한다. 대부분의 사람들처럼 그들은 AGI와 초지능으로 향하는 관문이 닫힐 것이라고 암묵적으로 *가정하고* 있다.[^125]

또한 AI 시스템들은 인간의 감독을 유지하면서 능력을 향상시키는 복합 시스템으로 효과적으로 결합될 수 있다. 불가해한 블랙박스에 의존하기보다는, AI와 전통적인 소프트웨어를 모두 포함한 여러 구성 요소가 인간이 모니터링하고 이해할 수 있는 방식으로 함께 작동하는 시스템을 구축할 수 있다.[^126] 일부 구성 요소는 블랙박스일 수 있지만, 어느 것도 AGI에 근접하지 않을 것이다 — 복합 시스템 전체만이 매우 일반적이면서 매우 능력이 있되, 엄격하게 통제 가능한 방식으로 그럴 것이다.[^127]

#### 의미 있고 보장된 인간 통제

"엄격하게 통제 가능하다"는 것은 무엇을 의미하는가? "도구" 프레임워크의 핵심 아이디어는 — 아주 일반적이고 강력하더라도 — 의미 있는 인간 통제 하에 있음이 보장되는 시스템을 허용하는 것이다. 이것은 무엇을 의미하는가? 두 가지 측면이 있다. 첫 번째는 설계 고려사항이다: 인간이 시스템이 하는 일에 깊숙이 그리고 중심적으로 관여해야 하며, 핵심적인 중요한 결정을 AI에 *위임하지* 않아야 한다. 이것이 현재 대부분 AI 시스템의 특성이다. 둘째, AI 시스템이 자율적인 정도까지, 행동 범위를 제한하는 보장이 있어야 한다. 보장은 어떤 일이 일어날 확률을 특성화하는 *수치*와 그 수치를 믿을 이유여야 한다. 이것이 우리가 다른 안전 중요 분야에서 요구하는 것으로, "고장 간 평균 시간"과 예상 사고 수 같은 수치가 계산되고, 뒷받침되며, 안전 사례에서 공개된다.[^128] 고장의 이상적인 수치는 물론 0이다. 그리고 좋은 소식은 상당히 다른 AI 아키텍처를 사용하여 프로그램(AI 포함)의 *형식적으로 검증된* 속성이라는 아이디어를 사용하면 꽤 가까워질 수 있다는 것이다. Omohundro, Tegmark, Bengio, Dalrymple 등이 자세히 탐구한 아이디어([여기](https://arxiv.org/abs/2309.01933)와 [여기](https://arxiv.org/abs/2405.06624) 참조)는 특정 속성(예: 인간이 시스템을 끌 수 있다는 것)을 가진 프로그램을 구성하고 그러한 속성이 성립함을 형식적으로 *증명하는* 것이다. 이것은 현재 아주 짧은 프로그램과 간단한 속성에 대해서는 가능하지만, AI 기반 증명 소프트웨어의 (다가오는) 힘은 훨씬 복잡한 프로그램(예: 래퍼)과 심지어 AI 자체에 대해서도 가능하게 할 수 있다. 이것은 매우 야심찬 프로그램이지만, 관문에 대한 압력이 커지면서 우리는 그것을 강화하는 강력한 재료들이 필요할 것이다. 수학적 증명이 충분히 강한 몇 안 되는 것 중 하나일 수 있다.

#### AI 산업의 향방

AI 진전이 방향을 바꾸더라도, 도구형 AI는 여전히 엄청난 산업이 될 것이다. 하드웨어 측면에서는 초지능을 방지하는 연산량 상한선이 있더라도, 더 작은 모델의 훈련과 추론에는 여전히 엄청난 양의 특수 부품이 필요할 것이다. 소프트웨어 측면에서는, AI 모델과 연산량 크기의 폭발을 억제하는 것이 단순히 더 크게 만들기보다는 더 작은 시스템을 더 좋고, 더 다양하며, 더 전문화되게 만드는 쪽으로 회사들이 자원을 재투입하게 할 것이다.[^129] 돈벌이를 하는 실리콘 밸리 스타트업들을 위한 충분한 — 아마도 더 많은 — 여지가 있을 것이다.[^130]

### 도구형 AI는 AGI 없이도 인류가 원하는 (거의) 모든 것을 제공할 수 있다

생물학적이든 기계적이든 지능은 일반적으로 목표 집합과 더 일치하는 미래를 가져오는 활동을 계획하고 실행하는 능력으로 간주될 수 있다. 따라서 지능은 현명하게 선택된 목표를 추구하는 데 사용될 때 엄청난 이익이 된다. 인공지능이 약속된 혜택 때문에 시간과 노력의 막대한 투자를 끌어들이는 것도 주로 이 때문이다. 그래서 우리는 물어야 한다: 초지능으로의 폭주를 억제한다면 우리는 AI의 혜택을 어느 정도까지 여전히 얻을 수 있을까? 답은: 놀랍도록 적게 잃을 수도 있다는 것이다.

먼저 현재 AI 시스템이 이미 매우 강력하고, 우리는 정말로 그것들로 할 수 있는 일의 표면만 긁었다는 것을 생각해보자.[^131] 그것들은 제시된 질문이나 과제를 "이해"하고, 그 질문에 답하거나 그 과제를 수행하는 데 필요한 것이 무엇인지 파악하는 면에서 "쇼를 운영하는" 능력이 상당히 있다.

다음으로, 현대 AI 시스템에 대한 흥분의 대부분은 그것들의 일반성 때문이지만, 가장 능력 있는 AI 시스템들 중 일부 — 음성이나 이미지를 생성하거나 인식하고, 과학적 예측과 모델링을 하며, 게임을 하는 등의 시스템들 — 은 훨씬 더 좁고 연산량 면에서 "관문 안에" 잘 있다.[^132] 이러한 시스템들은 그들이 수행하는 특정 과제에서 초인간적이다. 그것들의 좁음 때문에 엣지 케이스[^133] (또는 [악용 가능한](https://arxiv.org/abs/2211.00241)) 약점이 있을 수 있다. 하지만 *완전히* 좁거나 *완전히* 일반적인 것만이 유일한 선택지는 아니다: 그 사이에는 많은 아키텍처가 있다.[^134]

이러한 AI 도구들은 AGI 없이도 다른 긍정적 기술의 발전을 크게 가속화할 수 있다. 더 나은 핵물리학을 하려고 해도, AI가 핵물리학자가 될 필요는 없다 — 우리에게는 그런 사람들이 있다! 의학을 발전시키려면, 생물학자, 의학 연구자, 화학자들에게 강력한 도구를 주면 된다. 그들은 그것을 원하고 엄청난 이득을 위해 사용할 것이다. 서버 팜을 가득 채운 백만 명의 디지털 천재가 필요한 것이 아니다. 우리에게는 AI가 그들의 천재성을 발휘하도록 도울 수 있는 수백만 명의 인간이 있다. 그렇다, 불멸과 모든 질병의 치료법을 얻는 데는 더 오래 걸릴 것이다. 이것은 실제 비용이다. 하지만 가장 유망한 건강 혁신조차 AI 주도의 불안정이 전 세계적 갈등이나 사회 붕괴로 이어진다면 거의 소용없을 것이다. 우리는 AI로 강화된 인간들이 먼저 문제에 도전할 기회를 주어야 할 의무가 있다.

그리고 실제로 관문 안 도구로는 얻을 수 없는 AGI의 엄청난 장점이 있다고 가정해보자. *결코* AGI와 초지능을 구축하지 않음으로써 그런 것들을 잃게 될까? 여기서 위험과 보상을 저울질할 때, 서두르는 것 대 기다리는 것에는 엄청난 비대칭적 이익이 있다: 보장된 안전하고 유익한 방식으로 할 수 있을 때까지 기다릴 수 있고, 거의 모든 사람이 여전히 보상을 거둘 수 있다. 서두르면 OpenAI CEO 샘 알트만의 말로는 [우리 *모두*에게 등불이 꺼질](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1) 수 있다.

하지만 비AGI 도구들이 잠재적으로 그렇게 강력하다면, 우리가 그것들을 관리할 수 있을까? 답은 명확한...아마도이다.

### 도구형 AI 시스템은 (아마도, 원칙적으로) 관리 가능하다

하지만 쉽지는 않을 것이다. 현재 최첨단 AI 시스템은 사람과 기관이 목표를 달성하는 데 크게 힘을 실어줄 수 있다. 이것은 일반적으로 좋은 일이다! 하지만 그런 시스템을 갑작스럽게 그리고 사회가 적응할 시간이 많지 않은 상태에서 우리가 활용하게 되는 자연스러운 역학이 있어, 관문 차단을 가정하고 관리해야 할 심각한 위험을 제공한다. 그런 위험의 몇 가지 주요 부류와 어떻게 줄일 수 있는지 논의할 가치가 있다.

한 부류의 위험은 고성능 도구형 AI가 이전에는 개인이나 조직에 묶여 있던 지식이나 능력에 대한 접근을 허용하여, 고능력과 고충성도의 조합을 매우 광범위한 행위자들이 사용할 수 있게 하는 것이다. 오늘날 충분한 돈이 있으면 악의적인 사람이 화학자 팀을 고용해 새로운 화학 무기를 설계하고 생산하게 할 수 있지만 — 그런 돈을 갖거나 팀을 찾아서/조립해서 명백히 불법적이고, 비윤리적이며, 위험한 일을 하도록 설득하는 것이 그리 쉽지는 않다. AI 시스템이 그런 역할을 하지 못하도록 하려면, 모든 시스템과 그에 대한 접근이 책임감 있게 관리되는 한, 현재 방법의 개선으로도 충분할 수 있다.[^135] 반면에 강력한 시스템이 일반 사용과 수정을 위해 공개된다면, 내장된 안전 조치들은 제거될 가능성이 높다. 따라서 이 부류의 위험을 피하려면, 공개될 수 있는 것에 대한 강력한 제한 — 핵, 폭발물 및 기타 위험한 기술의 세부사항에 대한 제한과 유사한 — 이 필요할 것이다.[^136]

두 번째 위험 부류는 사람처럼 행동하거나 사람을 사칭하는 기계의 확산에서 나온다. 개인에게 해를 끼치는 수준에서는, 이러한 위험에는 훨씬 더 효과적인 사기, 스팸, 피싱과 비동의 딥페이크의 확산이 포함된다.[^137] 집단적 수준에서는 공적 토론과 논쟁, 우리 사회의 정보와 지식 수집, 처리, 배포 시스템, 그리고 우리의 정치적 선택 시스템 같은 핵심 사회 과정의 파괴가 포함된다. 이 위험을 완화하려면 (a) AI 시스템의 사람 사칭을 제한하는 법률과 그런 사칭을 생성하는 시스템을 만드는 AI 개발자에게 책임을 묻는 것, (b) (책임감 있게) 생성된 AI 콘텐츠를 식별하고 분류하는 워터마킹과 출처 추적 시스템, (c) 데이터(예: 카메라와 녹음)에서 사실, 이해, 그리고 좋은 세계 모델까지 신뢰할 수 있는 연쇄를 만들 수 있는 새로운 사회-기술적 인식론적 시스템이 포함될 것 같다.[^138] 이 모든 것이 가능하고, AI가 그 일부를 도울 수 있다.

세 번째 일반적 위험은 일부 과제가 자동화되는 정도에서 현재 그러한 과제를 수행하는 인간의 노동으로서의 재정적 가치가 낮아질 수 있다는 것이다. 역사적으로 과제를 자동화하는 것은 그러한 과제로 가능해진 것들을 더 저렴하고 풍부하게 만들면서, 이전에 그러한 과제를 수행했던 사람들을 자동화된 버전에 여전히 관여하는 사람들(일반적으로 더 높은 기술/급여)과 노동의 가치가 적거나 거의 없는 사람들로 분류했다. 전체적으로는 결과적으로 더 크지만 더 효율적인 부문에서 더 많은 대 더 적은 인간 노동이 필요할 부문을 예측하기 어렵다. 동시에 자동화 역학은 불평등과 일반 생산성을 증가시키고, (효율성 증가를 통해) 특정 상품과 서비스의 비용을 낮추며, (비용 질병을 통해) 다른 것들의 비용을 증가시키는 경향이 있다. 불평등 증가의 불리한 쪽에 있는 사람들에게는 그 특정 상품과 서비스의 비용 감소가 다른 것들의 증가를 상쇄하고 전반적인 더 큰 복지로 이어지는지 매우 불분명하다. 그렇다면 AI에서는 이것이 어떻게 될까? 인간의 지적 노동이 일반 AI로 대체될 수 있는 상대적 용이함 때문에, 우리는 인간 경쟁력을 갖춘 일반 목적 AI와 함께 이것의 급속한 버전을 예상할 수 있다.[^139] AGI로의 관문을 닫으면 AI 에이전트로 도매로 대체될 일자리는 훨씬 적어질 것이지만, 여전히 수년에 걸쳐 대규모 노동 대체는 일어날 가능성이 높다.[^140] 광범위한 경제적 고통을 피하려면 어떤 형태의 보편적 기본 자산이나 소득을 실시하고, 자동화하기 어려운 인간 중심 노동을 가치 있게 여기고 보상하는 문화적 전환을 설계하는 것(경제의 다른 부분에서 밀려난 사용 가능한 노동의 증가로 인해 노동 가격이 하락하는 것을 보기보다는)이 필요할 것이다. ["데이터 존엄성"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society)(훈련 데이터의 인간 생산자들이 그 데이터가 AI에서 창출하는 가치에 대해 자동으로 로열티를 받는) 같은 다른 구성체들도 도움이 될 수 있다. AI에 의한 자동화는 또한 *부적절한* 자동화라는 두 번째 잠재적 역효과를 갖는다. AI가 단순히 더 나쁜 일을 하는 응용과 함께, 여기에는 AI 시스템이 도덕적, 윤리적, 또는 법적 규율을 위반할 가능성이 높은 것들 — 예를 들어 생사 결정과 사법 문제에서 — 이 포함될 것이다. 이것들은 현재의 법적 프레임워크를 적용하고 확장하여 다루어져야 한다.

마지막으로, 관문 안 AI의 중대한 위협은 개인화된 설득, 주의 끌기, 조작에서의 사용이다. 우리는 소셜 미디어와 기타 온라인 플랫폼에서 깊이 뿌리내린 관심 경제(온라인 서비스들이 사용자 관심을 위해 치열하게 경쟁하는)와 ["감시 자본주의"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism) 시스템(사용자 정보와 프로파일링이 관심의 상품화에 추가되는)의 성장을 보았다. 더 많은 AI가 둘 다의 서비스에 투입될 것은 거의 확실하다. AI는 이미 중독성 피드 알고리즘에 많이 사용되고 있지만, 이는 한 개인이 강박적으로 소비하도록 맞춤화된 중독성 AI 생성 콘텐츠로 진화할 것이다. 그리고 그 사람의 입력, 응답, 데이터는 악순환을 계속하기 위해 관심/광고 기계에 투입될 것이다. 또한 기술 회사들이 제공하는 AI 도우미들이 더 많은 온라인 생활의 인터페이스가 되면서, 그들은 아마도 설득과 고객 수익화가 발생하는 메커니즘으로서 검색 엔진과 피드를 대체할 것이다. 지금까지 이러한 역학을 통제하는 데 실패한 우리 사회는 좋은 징조가 아니다. 이 역학의 일부는 프라이버시, 데이터 권리, 조작에 관한 규정을 통해 줄어들 수 있다. 문제의 근원에 더 가까이 가려면 충성스러운 AI 어시스턴트(아래에서 논의) 같은 다른 관점이 필요할 수 있다.

이 논의의 요점은 희망이다: 관문 안 도구 기반 시스템들은 — 적어도 그들이 오늘날의 가장 최첨단 시스템들과 비교할 만한 힘과 능력에 머무르는 한 — 그렇게 할 의지와 조정이 있다면 아마도 관리 가능하다. AI 도구로 강화된 괜찮은 인간 제도들이[^141] 해낼 수 있다. 우리는 또한 그것을 하는 데 실패할 수도 있다. 하지만 더 강력한 시스템을 허용하는 것이 어떻게 도움이 될지 보기 어렵다 — 그들을 책임자로 만들고 최선을 희망하는 것 말고는.

### 국가안보

AI 우위를 위한 경쟁 — 국가안보나 기타 동기에 의해 주도되는 — 은 우리를 권력을 부여하기보다는 흡수하는 경향이 있는 통제되지 않은 강력한 AI 시스템으로 이끈다. 미국과 중국 간의 AGI 경쟁은 어느 국가가 초지능을 먼저 얻는지를 결정하는 경쟁이다.

그렇다면 국가안보를 담당하는 사람들은 대신 무엇을 해야 할까? 정부는 통제 가능하고 보안이 되는 시스템을 구축하는 강력한 경험을 가지고 있으며, AI에서 그렇게 하는 것을 두 배로 강화하고, 규모에서 그리고 정부의 권위로 할 때 가장 잘 성공하는 종류의 인프라 프로젝트를 지원해야 한다.

AGI를 향한 무모한 "맨해튼 프로젝트" 대신,[^142] 미국 정부는 통제 가능하고, 안전하며, 신뢰할 수 있는 시스템을 위한 아폴로 프로젝트를 시작할 수 있다. 여기에는 예를 들어:

- (a) 온칩 하드웨어 보안 메커니즘을 개발하고 (b) 강력한 AI의 연산량 측면을 관리할 인프라를 개발하는 주요 프로그램. 이들은 미국의 [CHIPS 법](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local)과 [수출 통제 체제](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion)를 기반으로 구축될 수 있다.
- AI 시스템의 특정 기능(끄기 스위치 같은)이 존재하거나 부재함을 *증명*할 수 있도록 형식적 검증 기법을 개발하는 대규모 이니셔티브. 이것은 AI 자체를 활용하여 속성의 증명을 개발할 수 있다.
- AI 도구를 활용하여 기존 소프트웨어를 검증 가능하게 보안이 되는 프레임워크로 다시 코딩할 수 있는 검증 가능하게 보안이 되는 소프트웨어를 만드는 국가 규모의 노력.
- DOE, NSF, NIH 간의 파트너십으로 운영되는 AI를 사용한 과학 발전의 국가 투자 프로젝트.[^143]

일반적으로, AI와 그 오용으로부터의 위험에 우리 사회를 취약하게 만드는 엄청난 공격 표면이 있다. 이러한 위험 중 일부로부터 보호하려면 정부 규모의 투자와 표준화가 필요할 것이다. 이들은 AGI를 향한 경쟁의 불에 기름을 붓는 것보다 훨씬 더 많은 보안을 제공할 것이다. 그리고 AI가 무기와 지휘통제 시스템에 구축될 것이라면, AI가 신뢰할 수 있고 보안이 되는 것이 중요한데, 현재의 AI는 그렇지 않다.

### 권력 집중과 그 완화책

이 글은 AI의 인간 통제와 그 잠재적 실패 아이디어에 초점을 맞췄다. 하지만 AI 상황을 보는 또 다른 유효한 렌즈는 *권력 집중*을 통해서다. 매우 강력한 AI의 개발은 권력을 그것을 개발하고 통제할 극소수의 매우 큰 기업 손이나, AI를 자신의 권력과 통제를 유지하는 새로운 수단으로 사용하는 정부들이나, AI 시스템 자체로 집중시킬 위협이 있다. 또는 위의 어떤 불경한 혼합체로. 이 중 어떤 경우든 대부분의 인류는 권력, 통제, 주체성을 잃는다. 어떻게 이에 맞설 수 있을까?

물론 첫 번째이자 가장 중요한 단계는 인간보다 똑똑한 AGI와 초지능으로의 관문 차단이다. 이것들은 명시적으로 인간과 인간 그룹을 직접 대체할 수 있다. 만약 그들이 기업이나 정부 통제 하에 있다면 그 기업이나 정부로 권력을 집중시킬 것이고, "자유롭다면" 자신들로 권력을 집중시킬 것이다. 그러니 관문이 닫혔다고 가정하자. 그럼 다음은?

권력 집중에 대한 하나의 제안된 해결책은 "오픈소스" AI로, 모델 가중치가 자유롭게 또는 널리 사용 가능한 것이다. 하지만 앞서 언급했듯이, 모델이 오픈되면 대부분의 안전 조치나 가드레일이 (그리고 일반적으로) 제거될 수 있다. 따라서 한편으로는 탈중앙화와 다른 한편으로는 안전, 보안, AI 시스템의 인간 통제 사이에 심각한 긴장이 있다. 오픈 모델이 운영 체제에서 그랬듯이 AI에서 권력 집중과 의미 있게 맞설 것이라는 이유도 회의적이다(오픈 대안이 있음에도 여전히 마이크로소프트, 애플, 구글이 지배한다).[^144]

그러나 이 원을 제곱하는 방법이 있을 수 있다 — 위험을 중앙집권화하고 완화하면서 능력과 경제적 보상을 탈중앙집권화하는. 이는 AI가 어떻게 개발되고 그 혜택이 어떻게 배분되는지를 재고하는 것을 요구한다.

AI 개발과 소유의 새로운 모델들이 도움이 될 것이다. 이것은 여러 형태를 취할 수 있다: 정부 개발 AI(민주적 감독 대상),[^145] 비영리 AI 개발 조직(브라우저의 Mozilla 같은), 또는 매우 광범위한 소유권과 거버넌스를 가능하게 하는 구조. 핵심은 이러한 기관들이 강력한 안전 제약 하에서 작동하면서 공익에 봉사하도록 명시적으로 인가되는 것이다.[^146] 잘 만들어진 규제와 표준/인증 체제도 중요할 것인데, AI 제품들이 활발한 시장에서 제공될 때 사용자에게 착취적이기보다는 진정으로 유용하게 남도록 하기 위해서다.

경제적 권력 집중 측면에서, 우리는 출처 추적과 "데이터 존엄성"을 사용하여 경제적 혜택이 더 널리 흐르도록 할 수 있다. 특히 현재와 (관문을 닫힌 상태로 유지한다면) 미래의 대부분 AI 힘은 인간이 생성한 데이터, 즉 직접적인 훈련 데이터든 인간 피드백이든에서 나온다. AI 회사들이 데이터 제공자에게 공정하게 보상하도록 요구된다면,[^147] 이것은 적어도 경제적 보상을 더 넓게 배분하는 데 도움이 될 수 있다. 이를 넘어서, 또 다른 모델은 대형 AI 회사들의 상당한 지분을 공적으로 소유하는 것일 수 있다. 예를 들어, AI 회사들에 세금을 부과할 수 있는 정부들이 수입의 일부를 회사들의 주식을 보유하고 국민에게 배당금을 지급하는 국부펀드에 투자할 수 있다.[^148]

이러한 메커니즘에서 중요한 것은 단순히 비AI 수단을 사용하여 AI 주도 권력 집중과 싸우기보다는, AI 자체의 힘을 사용하여 권력을 더 잘 배분하는 데 도움을 주는 것이다. 하나의 강력한 접근법은 사용자에게 진정한 수탁 의무를 가지고 작동하는 — 특히 기업 제공자들보다 사용자의 이익을 우선시하는 — 잘 설계된 AI 어시스턴트를 통해서일 것이다.[^149] 이러한 어시스턴트들은 진정으로 신뢰할 수 있고, 기술적으로 유능하면서도 사용 사례와 위험 수준에 따라 적절하게 제한되며, 공적, 비영리, 또는 인증된 영리 채널을 통해 모든 사람이 널리 사용할 수 있어야 한다. 우리가 다른 당사자를 위해 우리 이익에 반해 몰래 일하는 인간 어시스턴트를 결코 받아들이지 않을 것처럼, 기업 이익을 위해 사용자를 감시하고, 조작하거나, 가치를 추출하는 AI 어시스턴트를 받아들여서는 안 된다.

그런 변화는 개인들이 인간 복지보다 가치 추출을 우선시하는 거대한 (AI로 강화된) 기업 및 관료적 기계들과 혼자 협상하도록 남겨지는 현재의 역학을 근본적으로 바꿀 것이다. AI 주도 권력을 더 넓게 재배분하는 많은 가능한 접근법이 있지만, 어느 것도 기본적으로 나타나지 않을 것이다: 수탁 요구사항, 공적 제공, 위험에 기반한 계층화된 접근 같은 메커니즘으로 의도적으로 설계되고 관리되어야 한다.

권력 집중을 완화하는 접근법들은 기존 권력들로부터 상당한 역풍에 직면할 수 있다.[^150] 하지만 안전과 집중된 권력 사이에서 선택할 필요가 없는 AI 개발로의 경로들이 있다. 지금 올바른 제도를 구축함으로써, 우리는 AI의 위험이 신중하게 관리되는 동안 AI의 혜택이 널리 공유되도록 할 수 있다.

### 새로운 거버넌스와 사회 구조

우리의 현재 거버넌스 구조는 어려움을 겪고 있다: 대응이 느리고, 종종 특수 이익에 포획되며, [공중의 신뢰가 점점 줄어들고 있다.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) 그러나 이것이 그것들을 포기해야 할 이유는 아니다 — 오히려 그 반대다. 일부 제도는 교체가 필요할 수 있지만, 더 넓게는 우리의 기존 구조를 강화하고 보완할 수 있는 새로운 메커니즘이 필요하며, 빠르게 진화하는 세계에서 더 잘 기능하도록 도울 수 있다.

우리 제도적 약점의 대부분은 형식적 정부 구조가 아니라 퇴화된 사회 제도에서 나온다: 공유된 이해를 개발하고, 행동을 조정하며, 의미 있는 담론을 수행하는 우리의 시스템. 지금까지 AI는 이 퇴화를 가속화하여, 우리의 정보 채널을 생성된 콘텐츠로 범람시키고, 가장 양극화되고 분열적인 콘텐츠를 가리키며, 진실과 허구를 구별하기 더 어렵게 만들었다.

하지만 AI는 실제로 이러한 사회 제도를 재건하고 강화하는 데 도움이 될 수 있다. 세 가지 중요한 영역을 고려해보자:

첫째, AI는 우리의 인식론적 시스템 — 무엇이 참인지 아는 방법 — 에 대한 신뢰를 회복하는 데 도움이 될 수 있다. 원시 데이터에서 분석을 거쳐 결론까지 정보의 출처를 추적하고 검증하는 AI 기반 시스템을 개발할 수 있다. 이러한 시스템은 암호화 검증과 정교한 분석을 결합하여 사람들이 무언가가 참인지뿐만 아니라 그것이 참이라는 것을 어떻게 아는지 이해하도록 도울 수 있다.[^151] 충성스러운 AI 어시스턴트들은 세부사항을 따라가며 그것들이 확인되도록 하는 책임을 맡을 수 있다.

둘째, AI는 새로운 형태의 대규모 조정을 가능하게 할 수 있다. 우리의 가장 시급한 문제들 중 많은 것 — 기후 변화에서 항생제 내성까지 — 은 근본적으로 조정 문제다. 우리는 [거의 모든 사람에게 있을 수 있는 것보다 나쁜 상황에 갇혀 있는데](https://equilibriabook.com/), 어떤 개인이나 그룹도 첫 번째 움직임을 할 여유가 없기 때문이다. AI 시스템은 복잡한 인센티브 구조를 모델링하고, 더 나은 결과로의 실행 가능한 경로를 식별하며, 그곳에 도달하는 데 필요한 신뢰 구축과 약속 메커니즘을 촉진함으로써 도울 수 있다.

아마도 가장 흥미롭게는, AI가 완전히 새로운 형태의 사회적 담론을 가능하게 할 수 있다. "도시와 대화하는"[^152] 것을 상상해보라 — 단순히 통계를 보는 것이 아니라, 수백만 주민들의 견해, 경험, 필요, 열망을 처리하고 종합하는 AI 시스템과 의미 있는 대화를 갖는 것. 또는 AI가 현재 서로 빗나가며 이야기하는 그룹들 사이에 진정한 대화를 촉진할 수 있는 방법을 고려해보라, 각 편이 서로에 대한 캐리커처가 아닌 상대편의 실제 우려와 가치를 더 잘 이해하도록 도움으로써. 또는 AI가 사람들 또는 심지어 큰 그룹의 사람들(모두가 직접 그리고 개별적으로 그것과 상호작용할 수 있는!) 사이의 분쟁에 대해 숙련되고 신뢰할 수 있게 중립적인 중재를 제공할 수 있다. 현재의 AI는 이 일을 하는 데 완전히 능력이 있지만, 그렇게 하는 도구들은 저절로, 또는 시장 인센티브를 통해 생겨나지 않을 것이다.

이러한 가능성들은 특히 담론과 신뢰를 퇴화시키는 AI의 현재 역할을 고려할 때 유토피아적으로 들릴 수 있다. 하지만 그것이 바로 우리가 이러한 긍정적 응용을 적극적으로 개발해야 하는 이유다. 통제 불가능한 AGI로의 관문을 차단하고 인간의 주체성을 강화하는 AI를 우선시함으로써, 우리는 AI가 임파워먼트, 회복력, 집단적 발전의 힘으로 봉사하는 미래를 향해 기술적 진보를 조종할 수 있다.


[^124]: 그렇긴 하지만, 삼중 교집합에서 멀리 떨어져 있는 것이 불행히도 원하는 만큼 쉽지 않다. 세 가지 측면 중 어느 하나에서 능력을 매우 강하게 밀어붙이는 것은 다른 것들에서도 그것을 증가시키는 경향이 있다. 특히, 매우 일반적이고 능력 있는 지능을 만들면서 쉽게 자율적으로 바뀔 수 없게 하는 것은 어려울 수 있다. 한 가지 접근법은 계획 능력이 제한된 ["근시안적"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) 시스템을 훈련하는 것이다. 또 다른 것은 행동 지향적 질문에 대한 답변을 회피하는 순수한 ["오라클"](https://arxiv.org/abs/1711.05541) 시스템에 초점을 맞추는 것이다.

[^125]: 많은 회사들이 자신들도 시간이 더 걸리더라도 결국 AGI에 의해 대체될 것이라는 사실을 깨닫지 못한다 — 만약 깨달았다면, 그 관문들을 조금 덜 밀어붙일지도 모른다!

[^126]: AI 시스템들은 더 효율적이지만 덜 이해하기 쉬운 방식으로 소통할 수 있지만, 인간의 이해를 유지하는 것이 우선되어야 한다.

[^127]: 모듈적이고 해석 가능한 AI의 이 아이디어는 여러 연구자들에 의해 자세히 개발되었다. 예를 들어 Drexler의 ["포괄적 AI 서비스"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) 모델, Dalrymple과 다른 사람들의 ["오픈 에이전시 아키텍처"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai)를 참조하라. 그런 시스템들은 거대한 연산량으로 훈련된 단일체 신경망보다 더 많은 엔지니어링 노력이 필요할 수 있지만, 그것이 바로 연산량 제한이 도움이 되는 부분이다 — 더 안전하고 투명한 경로를 더 실용적인 경로로 만들어줌으로써.

[^128]: 안전 사례 일반에 대해서는 [이 핸드북](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16)을 참조하라. AI와 특별히 관련해서는 [Wasil 등](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [Clymer 등](https://arxiv.org/abs/2403.10462), [Buhl 등](https://arxiv.org/abs/2410.21572), [Balesni 등](https://arxiv.org/abs/2411.03336)을 참조하라.

[^129]: 우리는 실제로 단지 추론의 높은 비용에 의해 주도되는 이 추세를 이미 보고 있다: 더 큰 것들로부터 "증류된" 더 작고 전문화된 모델들로, 덜 비싼 하드웨어에서 실행할 수 있는.

[^130]: AI 기술 생태계에 대해 흥미를 갖는 사람들이 자신들의 산업에 대한 부담스러운 규제로 보는 것에 반대하는 이유를 이해한다. 하지만 솔직히 말해서 예를 들어 벤처 캐피털리스트가 AGI와 초지능으로의 폭주를 허용하려고 하는 것은 나에게 당황스럽다. 그러한 시스템들(그리고 회사들, 회사 통제 하에 있는 동안)은 *모든 스타트업을 간식으로 먹어버릴* 것이다. 아마도 다른 산업들을 먹어치우기보다 *더 빨리*. 번영하는 AI 생태계에 투자하는 누구든지 AGI 개발이 소수의 지배적 플레이어들에 의한 독점으로 이어지지 않도록 하는 것을 우선시해야 한다.

[^131]: 경제학자이자 전 딥마인드 연구원인 마이클 웹이 [말했듯이](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/), "만약 우리가 오늘 더 큰 언어모델의 모든 개발을 멈춘다면, 그래서 GPT-4와 클로드 등이 우리가 그 크기로 훈련하는 마지막 것들이라면 — 그래서 우리는 그 크기의 것들과 모든 종류의 파인튜닝에 대해 훨씬 더 많은 반복을 허용하되, 그것보다 큰 것은 없고, 더 큰 발전도 없다면 — 오늘 우리가 가진 것만으로도 20년 또는 30년의 놀라운 경제 성장을 이끌기에 충분하다고 생각한다."

[^132]: 예를 들어, 딥마인드의 알파폴드 시스템은 GPT-4의 FLOP 수의 10만분의 1만 사용했다.

[^133]: 자율주행차의 어려움은 여기서 주목할 중요한 점이다: 명목상 좁은 과제이고 상당한 신뢰성을 가지고 상대적으로 작은 AI 시스템으로 달성 가능하지만, 그런 안전 중요 과제에서 필요한 수준으로 신뢰성을 얻으려면 광범위한 실제 세계 지식과 이해가 필요하다.

[^134]: 예를 들어, 연산량 예산이 주어지면, 우리는 아마도 그 예산의 (예를 들어) 절반으로 사전 훈련된 GPAI 모델들을 보게 될 것이고, 나머지 절반은 더 좁은 범위의 과제들에서 매우 높은 능력을 훈련하는 데 사용될 것이다. 이것은 거의 인간 수준의 일반 지능에 의해 뒷받침되는 초인간적 좁은 능력을 제공할 것이다.

[^135]: 현재 지배적인 정렬 기법은 "인간 피드백에 의한 강화학습" [(RLHF)](https://arxiv.org/abs/1706.03741)이고 인간 피드백을 사용하여 AI 모델의 강화학습을 위한 보상/처벌 신호를 만든다. 이것과 [헌법적 AI](https://arxiv.org/abs/2212.08073) 같은 관련 기법들은 놀랍도록 잘 작동하고 있다(견고성이 부족하고 적당한 노력으로 우회될 수 있지만). 또한, 현재 언어모델들은 일반적으로 상식적 추론에 충분히 능숙해서 어리석은 도덕적 실수를 하지 않을 것이다. 이것은 어떤 면에서 스위트 스팟이다: 사람들이 원하는 것을 이해할 만큼 똑똑하지만(정의될 수 있는 정도까지), 정교한 기만을 계획하거나 잘못했을 때 큰 해를 끼칠 만큼 똑똑하지는 않다.

[^136]: 장기적으로, 개발되는 어떤 수준의 AI 능력이든 궁극적으로는 소프트웨어이고 유용하기 때문에 확산될 것 같다. 우리는 그런 시스템들이 제기하는 위험에 대해 방어하는 견고한 메커니즘을 가져야 할 것이다. 하지만 우리는 *지금 그것을 갖고 있지 않으므로* 얼마나 많은 강력한 AI 모델들이 확산되도록 허용되는지에 대해 매우 신중해야 한다.

[^137]: 이것들의 대다수는 미성년자를 포함한 비동의 음란물 딥페이크다.

[^138]: 그런 해결책의 많은 성분들이 "봇인가 아닌가" 법률(EU AI 법 등에서), [산업 출처 추적 기술](https://c2pa.org/), [혁신적 뉴스 애그리게이터](https://www.improvethenews.org/), 예측 [애그리게이터](https://metaculus.com/)와 시장 등의 형태로 존재한다.

[^139]: 자동화의 파도는 이전 패턴을 따르지 않을 수 있는데, 상대적으로 *높은* 기술 과제들인 양질의 글쓰기, 법 해석, 의학적 조언 제공 등이 낮은 기술 과제들만큼 또는 심지어 더 자동화에 취약할 수 있다.

[^140]: AGI가 임금에 미치는 영향의 신중한 모델링에 대해서는 [여기](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek) 보고서를

## 제10장 - 우리 앞에 놓인 선택

인간의 미래를 보존하기 위해, 우리는 AGI와 초지능으로 향하는 관문을 차단하는 선택을 내려야 한다.

인류가 말하고, 생각하고, 기술을 만들고, 범용 문제해결을 수행하는 다른 지성체와 지구를 공유한 마지막 시기는 4만 년 전 빙하기 유럽이었다. 그 다른 지성체들은 전적으로 또는 부분적으로 우리의 노력으로 인해 멸종했다.

우리는 이제 그러한 시대에 다시 접어들고 있다. 우리 문화와 기술의 가장 발전된 산물들 - 우리의 전체 인터넷 정보 공유지에서 구축된 데이터셋과 우리가 만든 가장 복잡한 기술인 1000억 소자 칩들 - 이 결합되어 고도화된 범용 AI 시스템을 탄생시키고 있다.

이러한 시스템의 개발자들은 그것들을 인간 역량 강화를 위한 도구로 묘사하는 데 열심이다. 실제로 그럴 수도 있다. 하지만 착각하지 말라: 현재 우리가 걷고 있는 길은 점점 더 강력하고, 목표 지향적이며, 의사결정을 내리고, 일반적으로 유능한 디지털 에이전트를 구축하는 것이다. 이들은 이미 광범위한 지적 작업에서 많은 인간만큼 잘 수행하며, 빠르게 개선되고 있고, 자신들의 개선에도 기여하고 있다.

이 궤도가 바뀌거나 예상치 못한 장벽에 부딪히지 않는 한, 우리는 곧 - 수십 년이 아니라 몇 년 안에 - 위험할 정도로 강력한 디지털 지능을 갖게 될 것이다. *최선의* 결과에서조차, 이들은 (적어도 우리 중 일부에게는) 큰 경제적 이익을 가져다줄 것이지만, 그 대가는 우리 사회의 심대한 혼란과 인간이 수행하는 가장 중요한 일들에서의 인간 대체일 것이다: 이 기계들이 우리를 위해 생각하고, 계획하고, 결정하고, 창조할 것이다. 우리는 응석받이가 될 것이다. 하지만 응석받는 아이들 말이다. 훨씬 더 가능성이 높은 것은, 이러한 시스템들이 우리가 하는 긍정적인 일뿐만 아니라 부정적인 일에서도 인간을 대체할 것이라는 점이다. 여기에는 착취, 조작, 폭력, 전쟁이 포함된다. AI로 극도로 강화된 이런 것들을 우리가 살아남을 수 있을까? 마지막으로, 상황이 전혀 잘 풀리지 않을 가능성이 충분하다: 비교적 빠른 시일 내에 우리는 우리가 하는 일에서뿐만 아니라 우리가 *존재하는 것* 자체에서, 즉 문명과 미래의 설계자로서 대체될 것이다. 네안데르탈인에게 그것이 어떻게 흘러가는지 물어보라. 아마도 우리도 잠시 동안 그들에게 추가적인 장신구를 제공했을 것이다.

*우리는 이렇게 할 필요가 없다.* 우리에게는 인간과 경쟁할 수 있는 AI가 있으며, 우리가 경쟁할 *수 없는* AI를 구축할 필요는 없다. 우리는 후속 종족을 만들지 않고도 놀라운 AI 도구를 구축할 수 있다. AGI와 초지능이 불가피하다는 관념은 *운명으로 가장한 선택*이다.

몇 가지 엄격하고 전 지구적인 제한을 가함으로써, 우리는 AI의 일반적 능력을 대략 인간 수준으로 유지하면서도 여전히 우리가 할 수 없는 방식으로 데이터를 처리하고 우리 중 누구도 하고 싶어하지 않는 작업을 자동화하는 컴퓨터의 능력으로부터 이익을 얻을 수 있다. 이들은 여전히 많은 위험을 안고 있겠지만, 잘 설계되고 관리된다면 의학에서 연구, 소비자 제품에 이르기까지 인류에게 엄청난 혜택이 될 것이다.

제한을 가하는 것은 국제적 협력을 필요로 하겠지만, 생각보다는 적게 필요할 것이며, 그러한 제한은 여전히 원시적 권력 추구가 아니라 인간의 복지를 증진하는 응용에 초점을 맞춘 거대한 AI 및 AI 하드웨어 산업을 위한 충분한 여지를 남겨둘 것이다. 그리고 강력한 안전 보장과 의미 있는 전 지구적 대화를 거쳐 더 나아가기로 결정한다면, 그 선택권은 계속해서 우리가 추구할 수 있는 것으로 남아있다.

인류는 AGI와 초지능으로 향하는 관문을 차단하는 *선택*을 내려야 한다.

미래를 인간의 것으로 지키기 위해.

### 저자 서문

이 주제를 저와 함께 탐구하는 시간을 내어주셔서 감사합니다.

저는 과학자로서 가식 없는 진실을 말하는 것이 중요하다고 느끼기 때문에, 그리고 한 사람으로서 세상을 바꿀 문제인 인간보다 똑똑한 AI 시스템의 개발에 신속하고 결단력 있게 대처하는 것이 중요하다고 느끼기 때문에 이 에세이를 썼습니다.

이 놀라운 상황에 지혜롭게 대응하려면, AGI와 초지능이 우리의 이익을 확보하기 위해 '반드시' 구축되어야 한다거나 '불가피하며' 멈출 수 없다는 지배적 서사를 비판적으로 검토할 준비가 되어 있어야 합니다. 이러한 서사는 우리를 무력하게 만들어 앞에 놓인 대안적 길들을 보지 못하게 합니다.

무모함 앞에서 신중함을, 탐욕 앞에서 용기를 요구하는 데 저와 함께해 주시기를 바랍니다.

인간의 미래를 요구하는 데 저와 함께해 주시기를 바랍니다.

*– 안토니*

![Anthony Aguirre signature](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAnthony-Aguirre-signature-300x84.png&w=3840&q=75)

## 부록

연산량 회계처리, '관문 차단'의 구현 사례, 엄격한 AGI 책임 체계의 세부사항, 그리고 AGI 안전성 및 보안 표준의 단계적 접근법을 포함한 보충 정보.

### 부록 A: 연산량 회계처리 기술적 세부사항

의미 있는 연산량 기반 통제를 위해서는 훈련과 추론에 사용된 총 연산량에 대한 "정확한 값"과 적절한 근삿값 모두에 대한 상세한 방법론이 필요하다. 다음은 기술적 차원에서 "정확한 값"을 집계하는 방법의 한 예이다.

**정의:**

*연산 인과 그래프:* AI 모델의 특정 출력 O에 대해, 해당 연산의 결과를 변경하면 잠재적으로 O를 변경할 수 있는 디지털 연산의 집합이다. (이는 보수적으로 가정되어야 하며, 즉 한 연산이 시간상 더 이른 시점에 발생하고 물리적으로 잠재적인 인과 경로를 갖는 선행 요소와 독립적이라고 믿을 명확한 이유가 있어야 한다.) 여기에는 추론 중 AI 모델이 수행한 연산뿐만 아니라 입력, 데이터 준비, 모델 훈련에 사용된 연산도 포함된다. 이들 각각이 AI 모델의 출력일 수 있으므로, 인간이 입력에 상당한 변경을 가한 지점에서 차단하여 재귀적으로 계산된다.

*훈련 연산량:* 신경망의 연산 인과 그래프에 수반되는 총 연산량(FLOP 또는 기타 단위로, 데이터 준비, 훈련, 미세조정 및 기타 모든 연산 포함).

*출력 연산량:* 특정 AI 출력의 연산 인과 그래프에 포함된 총 연산량으로, 해당 출력에 사용된 모든 신경망(및 이들의 훈련 연산량)과 기타 연산을 포함한다.

*추론 연산량 비율:* 일련의 출력에서 출력 간 출력 연산량의 변화율(FLOP/s 또는 기타 단위)로, 즉 다음 출력을 생성하는 데 사용된 연산량을 출력 간 시간 간격으로 나눈 값이다.

**예시 및 근삿값:**

- 인간이 생성한 데이터로 훈련된 단일 신경망의 경우, 훈련 연산량은 관례적으로 보고되는 총 훈련 연산량과 같다.
- 이러한 신경망이 일정한 속도로 추론을 수행하는 경우, 추론 연산량 비율은 대략 추론을 수행하는 연산 클러스터의 총 연산 속도(FLOP/s)이다.
- 모델 미세조정의 경우, 완전한 모델의 훈련 연산량은 미세조정되지 않은 모델의 훈련 연산량에 미세조정 중 수행된 연산과 미세조정에 사용된 데이터 준비에 사용된 연산을 더한 값이다.
- 지식 증류 모델의 경우, 완전한 모델의 훈련 연산량에는 증류된 모델과 합성 데이터나 기타 훈련 입력을 제공하는 데 사용된 더 큰 모델 모두의 훈련이 포함된다.
- 여러 모델이 훈련되었지만 많은 "시도"가 인간의 판단에 따라 폐기된 경우, 이들은 보존된 모델의 훈련 또는 출력 연산량에 포함되지 않는다.

### 부록 B: 관문 차단의 구현 사례

**구현 사례:** 다음은 훈련에 10<sup>27</sup> FLOP, 추론(AI 실행)에 10<sup>20</sup> FLOP/s의 제한을 설정한 상황에서 관문 차단이 어떻게 작동할 수 있는지를 보여주는 한 가지 예이다:

**1\. 중단:** 국가 안보상의 이유로, 미국 행정부는 미국에 본사를 둔 모든 회사, 미국에서 사업을 하는 회사, 또는 미국에서 제조된 칩을 사용하는 회사에 10<sup>27</sup> FLOP 훈련 연산량 한계를 초과할 수 있는 새로운 AI 훈련 실행을 중단할 것을 요구한다. 미국은 AI 개발을 주도하는 다른 국가들과 논의를 시작하여 유사한 조치를 취하도록 강력히 권장하고, 이들이 이를 따르지 않을 경우 미국의 중단이 해제될 수 있음을 시사해야 한다.

**2\. 미국 감독 및 허가:** 행정명령이나 기존 규제기관의 조치를 통해, 미국은 (예를 들어) 1년 내에 다음을 요구한다:

- 미국에서 운영되는 회사가 수행하는 10<sup>25</sup> FLOP 이상의 모든 AI 훈련 실행을 미국 규제기관이 유지하는 데이터베이스에 등록. (참고: 이보다 다소 약한 버전이 2023년 미국 AI 행정명령에 이미 포함되어 있었으나 현재는 철회되었으며, 10<sup>26</sup> FLOP 이상의 모델에 대한 등록을 요구했다.)
- 미국에서 운영되거나 미국 정부와 사업을 하는 모든 AI 관련 하드웨어 제조업체는 특수 하드웨어와 이를 구동하는 소프트웨어에 대한 일련의 요구사항을 준수. (이러한 요구사항 중 많은 부분은 기존 하드웨어에 대한 소프트웨어 및 펌웨어 업데이트를 통해 구현될 수 있지만, 장기적이고 견고한 해결책을 위해서는 차세대 하드웨어의 변경이 필요하다.) 이 중 하나는 하드웨어가 10<sup>18</sup> FLOP/s의 연산을 실행할 수 있는 고속 상호연결 클러스터의 일부인 경우, 원격 "거버너"의 정기적인 허가를 포함하는 더 높은 수준의 검증이 필요하다는 요구사항으로, 거버너는 원격측정과 추가 연산 수행 요청을 모두 받는다.
- 관리자는 자신의 하드웨어에서 수행된 총 연산을 미국 데이터베이스를 유지하는 기관에 보고.
- 더 안전하고 유연한 감독 및 허가를 가능하게 하는 더 강력한 요구사항이 단계적으로 도입.

**3\. 국제 감독:**

- 미국, 중국, 그리고 고급 칩 제조 능력을 보유한 다른 국가들이 국제 협정을 협상.
- 이 협정은 국제원자력기구와 유사한 새로운 국제기구를 창설하여 AI 훈련과 실행을 감독.
- 협정 서명국들은 자국의 AI 하드웨어 제조업체가 적어도 미국에서 부과된 것만큼 강력한 요구사항을 준수하도록 요구해야 함.
- 관리자들은 이제 본국 기관과 국제기구 내 새로운 사무소 모두에 AI 연산 수치를 보고해야 함.
- 추가 국가들이 기존 국제 협정에 참여하도록 강력히 권장: 서명국의 수출 통제는 비서명국의 고성능 하드웨어 접근을 제한하는 반면, 서명국은 AI 시스템 관리에 대한 기술 지원을 받을 수 있음.

**4\. 국제 검증 및 집행:**

- 하드웨어 검증 시스템이 업데이트되어 원래 관리자와 국제기구 사무소 모두에 직접 연산 사용량을 보고.
- 국제기구는 국제 협정 서명국들과의 논의를 통해 연산 제한에 합의하고, 이는 서명국에서 법적 효력을 갖게 됨.
- 이와 병행하여, 일련의 국제 표준이 개발되어 (한계 미만이지만) 연산 임계값 이상의 AI 훈련 및 실행이 해당 표준을 준수하도록 요구될 수 있음.
- 기관은 필요한 경우 더 나은 알고리즘 등을 보상하기 위해 연산 한계를 낮출 수 있음. 또는 (예를 들어 증명 가능한 안전 보장 수준에서) 안전하고 바람직하다고 판단되면 연산 한계를 높일 수 있음.

### 부록 C: 엄격한 AGI 책임 체계의 세부사항

**엄격한 AGI 책임 체계의 세부사항**

- 고도로 일반적이고 능력이 뛰어나며 자율적인 고급 AI 시스템의 생성과 운영은 "비정상적으로 위험한" 활동으로 간주.
- 따라서 이러한 시스템의 훈련과 운영에 대한 기본 책임 수준은 모델이나 그 출력/행동으로 인한 모든 피해에 대한 엄격하고 연대적인 책임(또는 미국 외 지역의 이에 상응하는 책임).
- 중대한 과실이나 고의적 위법행위의 경우 경영진과 이사회 구성원에게 개인 책임이 부과됨. 가장 악질적인 사례에 대해서는 형사 처벌이 포함되어야 함.
- 책임이 사람과 회사가 일반적으로 적용받는 기본(미국에서는 과실 기반) 책임으로 되돌아가는 수많은 안전 피난처가 있음.
	- 일정한 연산 임계값(위에서 설명한 상한선보다 최소 10배 이상 낮은) 미만에서 훈련되고 운영되는 모델.
	- "약한" AI(대략적으로, 의도된 작업에서 인간 전문가 수준 이하) 그리고/또는
	- "좁은" AI(구체적으로 설계되고 훈련된 고정되고 상당히 제한된 범위의 작업과 운영을 갖는) 그리고/또는
	- "수동적" AI(직접적인 인간 개입과 통제 없이 행동을 취하거나 복잡한 다단계 작업을 수행할 수 있는 능력이 매우 제한적인 - 약간의 수정을 통해서도).
	- 안전하고 보안이 확보되며 통제 가능함이 보장된 AI(증명 가능하게 안전하거나, 위험 분석에서 무시할 만한 수준의 예상 피해를 나타내는).
- 안전 피난처는 AI 개발자가 준비하고 기관이나 기관에서 인증한 감사자가 승인한 [안전 사례](https://arxiv.org/abs/2410.21572)를 기반으로 주장될 수 있음. 연산량을 기반으로 안전 피난처를 주장하기 위해서는 개발자가 총 훈련 연산량과 최대 추론 비율에 대한 신뢰할 만한 추정치만 제공하면 됨.
- 법률은 공공 피해의 높은 위험이 있는 AI 시스템 개발에 대한 금지명령이 적절한 상황을 명시적으로 규정해야 함.
- 회사 컨소시엄이 NGO 및 정부 기관과 협력하여 이러한 용어들을 정의하고, 규제기관이 안전 피난처를 부여하는 방법, AI 개발자가 안전 사례를 개발하는 방법, 안전 피난처가 사전에 주장되지 않은 경우 법원이 책임을 해석하는 방법에 대한 표준과 규범을 개발해야 함.

### 부록 D: AGI 안전성 및 보안 표준의 단계적 접근법

**AGI 안전성 및 보안 표준의 단계적 접근법**

| 위험 등급 | 유발 조건 | 훈련 요구사항 | 배포 요구사항 |
| --- | --- | --- | --- |
| RT-0 | 자율성, 일반성, 지능이 모두 약한 AI | 없음 | 없음 |
| RT-1 | 자율성, 일반성, 지능 중 하나가 강한 AI | 없음 | 위험과 용도에 따라, 모델을 사용할 수 있는 모든 곳의 국가 당국이 승인한 안전 사례가 잠재적으로 필요 |
| RT-2 | 자율성, 일반성, 지능 중 두 가지가 강한 AI | 개발자를 관할하는 국가 당국에 등록 | 주요 피해 위험을 승인된 수준 이하로 제한하는 안전 사례와 모델을 사용할 수 있는 모든 곳의 국가 당국이 승인한 독립적인 안전 감사(블랙박스 및 화이트박스 레드팀 포함) |
| RT-3 | 자율성, 일반성, 지능이 모두 강한 AGI | 개발자를 관할하는 국가 당국의 안전성 및 보안 계획 사전 승인 | 주요 피해의 제한된 위험을 승인된 수준 이하로 보장하는 안전 사례와 사이버 보안, 통제 가능성, 제거 불가능한 킬 스위치, 인간 가치와의 정렬, 악의적 사용에 대한 견고성을 포함한 필수 사양 |
| RT-4 | 10<sup>27</sup> FLOP 훈련 또는 10<sup>20</sup> FLOP/s 추론 중 하나를 초과하는 모든 모델 | 국제적으로 합의된 연산량 상한선 해제 시까지 금지 | 국제적으로 합의된 연산량 상한선 해제 시까지 금지 |

높은 자율성, 일반성, 지능의 조합과 연산량 임계값을 기반으로 한 위험 분류 및 안전성/보안 표준:

- *강한 자율성*은 시스템이 상당한 인간 감독이나 개입 없이 여러 단계의 작업을 수행하거나 실제 세계와 관련된 복잡한 행동을 취할 수 있거나 쉽게 그렇게 만들 수 있는 경우에 적용됨. 예시: 자율주행차와 로봇; 금융 거래 봇. 비해당 예시: GPT-4; 이미지 분류기
- *강한 일반성*은 광범위한 적용 범위, 모델이 의도적이고 구체적으로 훈련받지 않은 작업의 수행, 그리고 새로운 작업을 학습하는 상당한 능력을 나타냄. 예시: GPT-4; 뮤제로. 비해당 예시: 알파폴드; 자율주행차; 이미지 생성기
- *강한 지능*은 모델이 최고 성능을 발휘하는 작업에서 (그리고 일반 모델의 경우 광범위한 작업에 걸쳐) 인간 전문가 수준의 성능과 일치하는 것에 해당함. 예시: 알파폴드; 뮤제로; o3. 비해당 예시: GPT-4; 시리

### 감사의 말

『인간의 미래를 지키자』에 기여해주신 분들께 감사 인사를 전합니다.

이 저작물은 저자의 개인적 견해를 반영한 것이며, 생명의 미래 연구소(Future of Life Institute)의 공식 입장으로 받아들여서는 안 됩니다(물론 양립 가능한 관점이며, 연구소의 공식 입장은 [이 페이지](https://futureoflife.org/our-position-on-ai/)를 참조하시기 바랍니다). 또한 저자가 소속된 다른 어떤 기관의 공식 입장도 아닙니다.

원고에 대해 의견을 주신 Mark Brakel, Ben Eisenpress, Anna Hehir, Carlos Gutierrez, Emilia Javorsky, Richard Mallah, Jordan Scharnhorst, Elyse Fulcher, Max Tegmark, Jaan Tallinn님께 감사드립니다. 참고문헌 정리를 도와준 Tim Schrier님, 도표를 아름답게 만들어준 Taylor Jones님과 Elyse Fulcher님께도 고마움을 전합니다.

이 저작물은 편집과 반박 검토를 위해 생성형 AI 모델(Claude와 ChatGPT)을 제한적으로 활용했습니다. 창작 작업에서 AI 관여 수준을 나타내는 확립된 기준이 있다면, 이 작업은 아마 3/10 정도에 해당할 것입니다. (물론 그런 기준은 실제로 존재하지 않습니다! 하지만 있어야 할 것입니다.)

이 에세이의 웹 버전을 제작하여 읽기와 탐색을 매우 즐거운 경험으로 만들어준 [Julius Odai](https://www.linkedin.com/in/julius-odai/)님께 깊이 감사드립니다. Julius는 기술 전문가이자 BlueDot Impact AI 거버넌스 과정의 최근 수료자입니다.