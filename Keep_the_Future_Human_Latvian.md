# Saglabāsim nākotni cilvēcīgu

Šis esejs pamato, kāpēc un kā mums būtu jāaizver vārti mākslīgajam vispārējam intelektam (MVI) un superintelektam, un ko mums tā vietā vajadzētu radīt.

Ja vēlaties tikai galvenos secinājumus, dodieties uz kopsavilkumu. Pēc tam 2.–5. nodaļa sniegs nepieciešamo kontekstu par AI sistēmu veidiem, kas aplūkoti šajā esejā. 5.–7. nodaļa skaidro, kāpēc varam sagaidīt MVI drīzu ierašanos un kas varētu notikt, kad tas būs sasniegts. Visbeidzot, 8.–9. nodaļā izklāstīts konkrēts priekšlikums, kā novērst MVI izveidi.

[Lejupielādēt PDF](https://keepthefuturehuman.ai/wp-content/uploads/2025/03/Keep_the_Future_Human__AnthonyAguirre__5March2025.pdf)

Kopējais lasīšanas laiks: 2–3 stundas

## Kopsavilkums

Augsta līmeņa esejas pārskats. Ja jums ir maz laika, iegūstiet visas galvenās idejas tikai 10 minūtēs.

Dramatiskie sasniegumi mākslīgajā intelektā pēdējās desmitgades laikā (šaurās nozīmes AI) un pēdējos gados (vispārējās nozīmes AI) ir pārveidojuši AI no nišas akadēmiskās jomas par daudzu pasaules lielāko uzņēmumu galveno biznesa stratēģiju, ar simtiem miljardu dolāru ikgadējām investīcijām tehnikās un tehnoloģijās AI spēju attīstībai.

Tagad mēs nonākam pie kritiski svarīga pagrieziena punkta. Tā kā jauno AI sistēmu spējas sāk līdzināties un pārspēt cilvēku spējas daudzos kognitīvajos jomās, cilvēcei jāizšķiras: cik tālu mēs ejam un kādā virzienā?

AI, tāpat kā jebkura tehnoloģija, sākās ar mērķi uzlabot lietas savam radītājam. Bet mūsu pašreizējā trajektorija un netiešā izvēle ir nekontrolēta sacīkste uz arvien jaudīgākām sistēmām, ko virza ekonomiskie stimuli dažiem milzīgiem tehnoloģiju uzņēmumiem, kas cenšas automatizēt lielas daļas no pašreizējās ekonomiskās darbības un cilvēku darba. Ja šī sacīkste turpinās vēl ilgi, ir neizbēgams uzvarētājs: pats AI – ātrāka, gudrāka, lētāka alternatīva cilvēkiem mūsu ekonomikā, mūsu domāšanā, mūsu lēmumos un galu galā mūsu civilizācijas kontrolē.

Bet mēs varam izdarīt citu izvēli: caur savām valdībām mēs varam pārņemt kontroli pār AI attīstības procesu, lai noteiktu skaidrus ierobežojumus, līnijas, kuras nedrīkstam šķērsot, un lietas, ko vienkārši nedarīsim – kā mēs esam darījuši ar kodoltehnoloģijām, masu iznīcināšanas ieročiem, kosmosa ieročiem, videi kaitīgiem procesiem, cilvēku bioinženieriju un eigēniku. Vissvarīgāk, mēs varam nodrošināt, ka AI paliek kā rīks cilvēku iespēju paplašināšanai, nevis jauna suga, kas mūs aizstāj un galu galā nomaina.

Šis esejs argumentē, ka mums vajadzētu *saglabāt nākotni cilvēcisku*, aizverot "vārtus" gudrākam par cilvēku, autonomam, vispārējās nozīmes AI – dažkārt saucot to par "MVI" – un jo īpaši augsti pārcilvēciskajai versijai, ko dažkārt sauc par "superintelektu". Tā vietā mums vajadzētu fokusēties uz jaudīgiem, uzticamiem AI rīkiem, kas var dot spēku indivīdiem un transformējoši uzlabot cilvēku sabiedrību spējas darīt to, ko tās dara vislabāk. Šī argumenta struktūra īsumā seko tālāk.

### AI ir atšķirīgs

AI sistēmas fundamentāli atšķiras no citām tehnoloģijām. Kamēr tradicionālā programmatūra seko precīzām instrukcijām, AI sistēmas mācās, kā sasniegt mērķus, nepastāstot tām precīzi kā. Tas padara tās jaudīgas: ja mēs varam skaidri definēt mērķi vai panākumu metriku, lielākajā daļā gadījumu AI sistēma var iemācīties to sasniegt. Bet tas arī padara tās būtībā neprognozējamas: mēs nevaram droši noteikt, kādas darbības tās veiks, lai sasniegtu savus mērķus.

Tās ir arī lielākoties neizskaidrojamas: lai gan tās daļēji ir kods, tās lielākoties ir milzīgs neskaidru skaitļu kopums – neironu tīklu "svari" – kas nav analizējami; mēs neesam daudz labāki to iekšējo mehānismu izpratnē nekā domu noskaidrošanā, ieskatoties bioloģiskajās smadzenēs.

Šis digitālo neironu tīklu apmācības pamatveids strauji palielinās sarežģītībā. Jaudīgākās AI sistēmas tiek radītas ar masīviem skaitļošanas eksperimentiem, izmantojot specializētu aparatūru neironu tīklu apmācībai uz milzīgām datu kopām, ko pēc tam papildina ar programmatūras rīkiem un virsbūvi.

Tas ir novedis pie ļoti jaudīgu rīku radīšanas teksta un attēlu veidošanai un apstrādei, matemātisku un zinātnisku spriedumu veikšanai, informācijas apkopošanai un interaktīvai cilvēku zināšanu milzīgā krājuma vaicāšanai.

Diemžēl, kamēr jaudīgāku, uzticamāku tehnoloģisko rīku attīstība ir tas, ko mums *vajadzētu* darīt, un ko gandrīz visi vēlas un saka, ka vēlas, tā nav trajektorija, uz kuras mēs faktiski atrodamies.

### MVI un superintelekts

Kopš jomas pirmsākumiem AI pētniecība tā vietā ir fokusējusies uz citu mērķi: Mākslīgo vispārējo intelektu. Šis fokuss tagad ir kļuvis par titānisku uzņēmumu, kas vada AI attīstību, fokusu.

Kas ir MVI? To bieži vagt definē kā "cilvēka līmeņa AI", bet tas ir problemātiski: kuri cilvēki un kurās spējās tas ir cilvēka līmenī? Un kā ar pārcilvēciskajām spējām, kas tam jau ir? Noderīgāks veids, kā izprast MVI, ir caur trīs galveno īpašību krustošanos: augsta **A**utonomija (darbības neatkarība), augsta **V**ispārīgums (plašs darbības lauks un pielāgošanās spēja) un augsts **I**ntelekts (kompetence kognitīvos uzdevumos). Pašreizējās AI sistēmas var būt ļoti spējīgas, bet šauras, vai vispārīgas, bet prasīt pastāvīgu cilvēku uzraudzību, vai autonomas, bet ierobežotas darbības jomā.

Pilnīgs A-V-I apvienotu visas trīs īpašības līmeņos, kas atbilst vai pārspēj augstākās cilvēku spējas. Kritiski svarīgi, ka tieši šī kombinācija padara cilvēkus tik efektīvus un tik atšķirīgus no pašreizējās programmatūras; tas ir arī tas, kas ļautu cilvēkus pilnībā aizstāt ar digitālām sistēmām.

Lai gan cilvēka intelekts ir īpašs, tas nekādā ziņā nav ierobežojums. Mākslīgas "superintelektuālas" sistēmas varētu darboties simtiem reižu ātrāk, analizēt daudz vairāk datu un vienlaikus turēt "prātā" milzīgas daudzumus, un veidot apvienības, kas ir daudz lielākas un efektīvākas nekā cilvēku kolekcijas. Tās varētu aizstāt ne atsevišķus indivīdus, bet uzņēmumus, nācijas vai mūsu civilizāciju kopumā.

### Mēs esam pie sliekšņa

Pastāv stiprs zinātnisks konsenss, ka MVI ir *iespējams.* AI jau pārspēj cilvēka veiktspēju daudzos vispārējos intelektuālo spēju testos, ieskaitot nesen arī augsta līmeņa spriedumu veidošanu un problēmu risināšanu. Atpaliekošās spējas – piemēram, nepārtraukta mācīšanās, plānošana, pašapzināšanās un oriģinalitāte – visas pastāv kādā līmenī pašreizējās AI sistēmās, un ir zināmas tehnikas, kas visticamāk uzlabos tās visas.

Kamēr vēl pirms dažiem gadiem daudzi pētnieki uzskatīja MVI par gadu desmitiem attālu, pašlaik pierādījumi īsiem termiņiem līdz MVI ir spēcīgi:

- Empīriski pārbaudīti "mērogošanas likumi" savieno skaitļošanas ievadi ar AI spējām, un korporācijas ir uz ceļa palielināt skaitļošanas ievadi par vairākām pakāpēm nākamajos gados. Cilvēku un finanšu resursi, kas veltīti AI attīstībai, tagad līdzinās ducim Manhetenas projektu un vairākiem Apollo projektiem.
- AI korporācijas un to vadītāji publiski un privāti tic, ka MVI (pēc kādas definīcijas) ir sasniedzams dažu gadu laikā. Šiem uzņēmumiem ir informācija, kas sabiedrībai nav pieejama, ieskaitot dažiem ir nākamā AI sistēmu paaudze rokās.
- Ekspertu prognozētāji ar pierādītiem sasniegumiem piešķir 25% varbūtību MVI (pēc kādas definīcijas) ierašanās 1-2 gadu laikā, un 50% 2-5 gadiem (skatiet Metaculus prognozes ['vājam'](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) un ['pilnīgam'](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/) MVI).
- Autonomija (ieskaitot tālas darbības elastīgu plānošanu) atpaliek AI sistēmās, bet lielākie uzņēmumi tagad fokusē savus milzīgos resursus autonomu AI sistēmu izstrādē un neformāli nosaukuši 2025. gadu par ["aģentu gadu."](https://techinformed.com/2025-informed-the-year-of-agentic-ai/)
- AI arvien vairāk piedalās savā uzlabošanā. Tiklīdz AI sistēmas būs tikpat kompetents kā cilvēku AI pētnieki AI pētniecībā, tiks sasniegts kritisks slieksnis ātram progresam uz daudz jaudīgākām AI sistēmām un visticamāk novedīs pie AI spēju strauja pieauguma. (Varētu apgalvot, ka šis straujais pieaugums jau ir sācies.)

Ideja, ka gudrāks par cilvēku MVI ir gadu desmitiem attāls vai vairāk, vienkārši vairs nav aizturam lielākajai daļai ekspertu šajā jomā. Domstarpības tagad ir par to, cik mēnešu vai gadu tas prasīs, ja mēs paliekam uz šī kursa. Pamatjautājums, ar ko saskaramies, ir: vai mums vajadzētu?

### Kas virza sacīksti uz MVI

Sacīksti uz MVI virza vairāki spēki, katrs padarot situāciju bīstamāku. Lielākie tehnoloģiju uzņēmumi redz MVI kā galējo automatizācijas tehnoloģiju – ne tikai papildinot cilvēku darbiniekus, bet aizstājot tos lielākoties vai pilnībā. Uzņēmumiem balva ir milzīga: iespēja iegūt ievērojamu daļu no pasaules 100 triljoniem dolāru ikgadējās ekonomiskās produkcijas, automatizējot cilvēku darba izmaksas.

Nācijas jūtas spiestās pievienoties šai sacīkstei, publiski atsaucoties uz ekonomisko un zinātnisko līderību, bet privāti uzskatot MVI par potenciālu revolūciju militārajās lietās, salīdzināmu ar kodolieroču. Bailes, ka konkurenti varētu iegūt izšķirošu stratēģisku priekšrocību, rada klasisku bruņošanās sacīkstes dinamiku.

Tie, kas cenšas pēc superintelekta, bieži min grandiozu vīziju: visu slimību izārstēšanu, novecošanas apgriezšanu, energijas un kosmosa ceļojumu caursitspējīgus sasniegums, vai pārcilvēcisku plānošanas spēju radīšanu.

Mazāk labvēlīgi, to, kas virza sacīksti, ir vara. Katrs dalībnieks – vai tas būtu uzņēmums vai valsts – tic, ka intelekts līdzinās varai un ka viņi būs vislabākie šīs varas pārvaldītāji.

Es argumentēju, ka šīs motivācijas ir reālas, bet fundamentāli maldīgas: MVI *absorbēs* un *meklēs* varu, nevis to piešķirs; AI radītas tehnoloģijas *arī* būs spēcīgi divpusējas, un tur, kur tās ir labvēlīgas, tās var radīt ar AI rīkiem un bez MVI; un pat tiktāl, ciktāl MVI un tā rezultāti paliek kontrolē, šīs sacīkstes dinamikas – gan korporatīvās, gan ģeopolitiskās – padara liela mēroga riskus mūsu sabiedrībai gandrīz neizbēgamus, ja vien tie netiek izšķiroši pārtraukti.

### MVI un superintelekts rada dramatisku draudu civilizācijai

Neraugoties uz to pievilcību, MVI un superintelekts rada dramatiskus draudus civilizācijai caur vairākiem savstarpēji stiprinošiem ceļiem:

*Varas koncentrācija:* pārcilvēcisks AI varētu atņemt varu lielākajai daļai cilvēcības, absorbējot milzīgas sociālās un ekonomiskās darbības daļas AI sistēmās, ko vada sauja gigantisko uzņēmumu (kuri savukārt var būt vai nu pārņemti, vai efektīvi pārņemt valdības).

*Masīvi traucējumi:* vairuma kognitīvo darbu lielapjoma automatizācija, mūsu pašreizējo epistēmisko sistēmu aizstāšana un milzīga daudzuma aktīvu necilvēcisku aģentu ieviešana īsā laika periodā apgrieztu otrādi lielāko daļu mūsu pašreizējo civilizācijas sistēmu.

*Katastrofas:* paplašinot spēju – potenciāli virs cilvēka līmeņa – radīt jaunas militāras un destruktīvas tehnoloģijas un atdalot to no sociālajām un juridiskajām sistēmām, kas pamato atbildību, fiziski katastrofu no masu iznīcināšanas ieročiem kļūst dramatiski ticamāka.

*Ģeopolitika un karš:* lielās pasaules varas nesēdēs mierīgi, ja jūtas, ka viņu pretinieki attīsta tehnoloģiju, kas varētu nodrošināt "izšķirošu stratēģisku priekšrocību".

*Straujais pieaugums un kontroles zaudēšana:* Ja vien tas nav īpaši novērsts, pārcilvēciskam AI būs visi stimuli sevi tālāk uzlabot un tas varētu tālu apsteigt cilvēkus ātrumā, datu apstrādē un domāšanas sarežģītībā. Nav jēgpilna veida, kādā mēs varam būt šādas sistēmas kontrolē. Šāds AI nepiešķirs varu cilvēkiem; mēs piešķirsim varu tam, vai tas to paņems.

Daudzi no šiem riskiem paliek pat ja tehniskā "saskaņošanas" problēma – nodrošinot, ka progresīvs AI uzticami dara to, ko cilvēki vēlas, lai tas darītu – ir atrisināta. AI rada milzīgu izaicinājumu tajā, kā to pārvaldīt, un ļoti daudzi šīs pārvaldības aspekti kļūst ļoti grūti vai neatrisināmi, kad cilvēka intelekts tiek pārspēts.

Vissvarīgākais, pārcilvēcisku vispārējās nozīmes AI veids, kas pašlaik tiek attīstīts, pēc savas būtības būtu ar mērķiem, aģentūru un spējām, kas pārspēj mūsējās. Tas būtu būtībā nekontrolējams – kā mēs varam kontrolēt kaut ko, ko mēs nevaram ne izprast, ne prognozēt? Tas nebūtu tehnoloģisks rīks cilvēku lietošanai, bet otrā intelektuālā suga uz Zemes līdzās mūsējai. Ja ļautos progresēt tālāk, tas būtu ne tikai otrā suga, bet aizstājējsuga.

Varbūt tā pret mums izturētos labi, varbūt nē. Bet nākotne piederētu tai, ne mums. Cilvēku ēra būtu beigusies.

### Tas nav neizbēgami; cilvēce var ļoti konkrēti izlemt neveidot savu aizstājēju.

Pārcilvēcisku MVI radīšana ir tālu no neizbēgamas. Mēs varam to novērst ar koordinētu pārvaldības pasākumu kopumu:

Pirmkārt, mums nepieciešama stabila AI skaitļošanas ("skaitļošanas jaudas") uzskaite un uzraudzība, kas ir fundamentāls lielāka mēroga AI sistēmu veicinātājs un svira to pārvaldībai. Tas savukārt prasa standartizētu kopējās skaitļošanas jaudas, kas izmantota AI modeļu apmācībā un to darbināšanā, mērīšanu un ziņošanu, kā arī tehniskās metodes izmantotās skaitļošanas saskaitīšanai, sertificēšanai un verifikācijai.

Otrkārt, mums vajadzētu ieviest cietus AI skaitļošanas jaudas ierobežojumus gan apmācībai, gan darbībai; tie novērš AI no būšanas pārāk jaudīgam un darbošanās pārāk ātri. Šos ierobežojumus var īstenot gan ar likumīgām prasībām, gan aparatūrā balstītiem drošības pasākumiem, kas iebūvēti AI specializētajos mikroshēmās, analoģiski drošības līdzekļiem mūsdienu telefonos. Tā kā specializēto AI aparatūru ražo tikai sauja uzņēmumu, verifikācija un ieviešana ir īstenojama caur esošo piegādes ķēdi.

Treškārt, mums nepieciešama pastiprināta atbildība visbisstamākajām AI sistēmām. Tiem, kas attīsta AI, kurš apvieno augstu autonomiju, plašu vispārīgumu un pārāku intelektu, jāsaskaras ar stingru atbildību par kaitējumu, kamēr drošie ostas no šīs atbildības mudinātu attīstīt ierobežotākas un kontrolējamākas sistēmas.

Ceturtkārt, mums nepieciešama pakāpeniska regulācija, pamatojoties uz riska līmeņiem. Spējīgākajām un bīstamākajām sistēmām būtu nepieciešamas plašas drošības un kontrolējamības garantijas pirms attīstības un ieviešanas, kamēr mazāk jaudīgas vai specializētākas sistēmas saskartos ar proporcionālu uzraudzību. Šim regulatīvajam ietvaram galu galā vajadzētu darboties gan nacionālā, gan starptautiskā līmenī.

Šī pieeja – ar detalizētu specifikāciju, kas sniegta pilnā dokumentā – ir praktiska: lai gan būs nepieciešama starptautiska koordinācija, verifikācija un ieviešana var darboties caur nelielu skaitu uzņēmumu, kas kontrolē specializētās aparatūras piegādes ķēdi. Tā ir arī elastīga: uzņēmumi joprojām var ieviest jauninājumus un gūt peļņu no AI attīstības, tikai ar skaidriem ierobežojumiem visbisstamākajām sistēmām.

Ilgtermiņa AI varas un risku ierobežošana prasītu starptautiskas vienošanās, pamatojoties gan uz paš-, gan kopējām interesēm, tieši tāpat kā kodolierožu proliferācijas kontrole tagad. Bet mēs varam sākt tūlīt ar pastiprinātu uzraudzību un atbildību, vienlaikus veidojot ceļu uz visaptverošāku pārvaldību.

Galvenā trūkstošā sastāvdaļa ir politiskā un sociālā griba pārņemt kontroli pār AI attīstības procesu. Šīs gribas avots, ja tas nāks laikā, būs pati realitāte – tas ir, no plaša apzināšanās par īstajām sekām tam, ko mēs darām.

### Mēs varam konstruēt AI rīkus, lai dotu spēku cilvēcei

Nevis cenšoties pēc nekontrolējama MVI, mēs varam attīstīt jaudīgus "AI rīkus", kas uzlabo cilvēku spējas, vienlaikus paliekot jēgpilnas cilvēku kontroles ietvaros. AI rīku sistēmas var būt ārkārtīgi spējīgas, vienlaikus izvairoties no bīstamā trīskāršā krustojuma starp augstu autonomiju, plašu vispārīgumu un pārcilvēcisku intelektu, ja vien mēs tās konstruējam, lai būtu kontrolējamas līmenī, kas atbilst to spējām. Tās var arī apvienot sarežģītās sistēmās, kas saglabā cilvēku uzraudzību, vienlaikus piegādājot transformējošus ieguvumus.

AI rīki var revolucionizēt medicīnu, paātrināt zinātniskos atklājumus, uzlabot izglītību un pilnveidot demokrātiskos procesus. Kad pareizi pārvaldīti, tie var padarīt cilvēku ekspertus un institūcijas efektīvākus, nevis tos aizstāt. Lai gan šādas sistēmas joprojām būs ļoti traucējošas un prasīs rūpīgu pārvaldību, riski, ko tās rada, ir fundamentāli atšķirīgi no MVI: tie ir riski, kurus mēs varam pārvaldīt, kā citu jaudīgo tehnoloģiju riski, nevis eksistenciāli draudi cilvēku aģentūrai un civilizācijai. Un kritiski svarīgi, ka, kad gudri attīstīti, AI rīki var palīdzēt cilvēkiem pārvaldīt jaudīgu AI un pārvaldīt tā ietekmi.

Šī pieeja prasa pārdomāt gan to, kā AI tiek attīstīts, gan to, kā tā ieguvumi tiek sadalīti. Jauni publiskās un bezpeļņas AI attīstības modeļi, spēcīgi regulatīvie ietvari un mehānismi ekonomisko ieguvumu plašākai izplatīšanai var palīdzēt nodrošināt, ka AI dod spēku visai cilvēcei, nevis koncentrē varu dažās rokās. Pats AI var palīdzēt veidot labākas sociālās un pārvaldības institūcijas, ļaujot jaunas koordinācijas un diskursa formas, kas stiprina, nevis grauj cilvēku sabiedrību. Nacionālās drošības iestādes var izmantot savu ekspertīzi, lai padarītu AI rīku sistēmas īsti drošas un uzticamas, un īstu aizsardzības, kā arī nacionālās varas avotu.

Mēs galu galā varam izlemt attīstīt vēl jaudīgākas un suverēnākas sistēmas, kas ir mazāk līdzīgas rīkiem un – varam cerēt – vairāk līdzīgas gudriem un jaudīgiem labdariem. Bet mums vajadzētu to darīt tikai pēc tam, kad esam attīstījuši zinātnisko izpratni un pārvaldības spēju to darīt droši. Šāds monumentāls un neatgriezenisks lēmums vajadzētu pieņemt apzināti visai cilvēcei, nevis pēc noklusējuma sacīkstē starp tehnoloģiju uzņēmumiem un nācijām.

### Cilvēku rokās

Cilvēki vēlas to labumu, kas nāk no AI: noderīgus rīkus, kas dod tiem spēku, dod papildu spēku ekonomiskajām iespējām un izaugsmei, un sola caursitspēju zinātnē, tehnoloģijā un izglītībā. Kāpēc gan nē? Bet kad vaicā, milzīga vairākums sabiedrības [vēlas lēnāku un piesardzīgāku AI attīstību](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation), un nevēlas gudrāku par cilvēku AI, kas aizstās viņus darba vietās un citur, piepildīs viņu kultūru un informācijas telpas ar necilvēcisku saturu, koncentrēs varu nelielajā uzņēmumu kopā, radīs ārkārtējus liela mēroga globālus riskus un galu galā draudēs atņemt varu vai aizstāt viņu sugu. Kāpēc gan viņi to vēlētos?

Mēs *varam* iegūt vienu bez otra. Tas sākas ar izlēmi, ka mūsu liktenis nav kādas tehnoloģijas iedomātajā neizbēgamībā vai dažu dyrektoru rokās Silikona ielejā, bet mūsu pārējo rokās, ja mēs to satveram. Aizverīsim Vārtus un saglabāsim nākotni cilvēcisku.

## 1. nodaļa - Ievads

To, kā mēs reaģēsim uz cilvēku pārspējošā mākslīgā intelekta perspektīvu, ir mūsu laika vissvarīgākais jautājums. Šis esejs piedāvā ceļu uz priekšu.

Mēs, iespējams, atrodamies cilvēces ēras beigās.

Pēdējos desmit gados ir sācies kaut kas, kas ir unikāls mūsu sugas vēsturē. Tā sekas lielā mērā noteiks cilvēces nākotni. Sākot no aptuveni 2015. gada, pētniekiem ir izdevies izstrādāt *šauru* mākslīgo intelektu (AI) – sistēmas, kas spēj uzvarēt spēlēs kā Go, atpazīt attēlus un runu un tā tālāk labāk nekā jebkurš cilvēks.[^1]

Tas ir apbrīnojams sasniegums, un tas rada ārkārtīgi noderīgas sistēmas un produktus, kas stiprinās cilvēci. Taču šaurais mākslīgais intelekts nekad nav bijis šīs jomas īstais mērķis. Drīzāk, mērķis ir bijis radīt *vispārēja* mērķa AI sistēmas, īpaši tās, ko bieži sauc par "mākslīgo vispārējo intelektu" (MVI) vai "superintelektu", kas vienlaikus ir tikpat labas vai labākas nekā cilvēki gandrīz *visos* uzdevumos, tāpat kā AI tagad ir pārcilvēciski laba Go, šahā, pokerī, dronu sacīkstēs utt. Tas ir daudzu lielāko AI uzņēmumu deklarētais mērķis.[^2]

*Šie centieni arī gūst panākumus.* Vispārēja mērķa AI sistēmas kā ChatGPT, Gemini, Llama, Grok, Claude un Deepseek, kas balstītas uz milzīgiem aprēķiniem un datu kalniem, ir sasniegušas paritāti ar parastajiem cilvēkiem plašā uzdevumu klāstā un pat pielīdzinās cilvēku ekspertiem dažās jomās. Tagad AI inženieri dažos no lielākajiem tehnoloģiju uzņēmumiem sacenšas, lai pavirzītu šos gigantiskos mašīnintelekta eksperimentus uz nākamajiem līmeņiem, kuros tie pielīdzinās un pēc tam pārspēj pilno cilvēku spēju, ekspertīzes un autonomijas spektru.

*Tas ir neizbēgami tuvs.* Pēdējo desmit gadu laikā ekspertu prognozes par to, cik ilgi tas prasīs – ja turpināsim pašreizējo kursu –, ir kritušas no desmitgadēm (vai gadsimtiem) līdz viencipara gadu skaitam.

Tas ir arī epohas nozīmes un transcendents risks. MVI atbalstītāji to redz kā pozitīvu pārveidošanos, kas atrisinātu zinātniskās problēmas, izārstētu slimības, attīstītu jaunas tehnoloģijas un automatizētu vienveidīgo darbu. Un AI noteikti varētu palīdzēt sasniegt visas šīs lietas – patiesībā tas jau to dara. Bet gadu desmitos daudzi uzmanīgi domātāji, no Alana Tjūringa līdz Stivenам Hokingam līdz mūsdienu Džefrijam Hintonam un Jošua Bendžio,[^3] ir izteikuši stingru brīdinājumu: patiesi cilvēku pārspējošu, vispārēju, autonomu AI veidošana minimāli pilnībā un neatgriezeniski apgriezīs sabiedrību otrādi, un maksimāli rezultēsies ar cilvēces iznīcību.[^4]

Superintelekts ātri tuvojas mūsu pašreizējā ceļā, bet ir tālu no neizbēgamības. Šis esejs ir plašs arguments par to, kāpēc un kā mums vajadzētu *aizvērt Vārtus* šai tuvojošajai necilvēcīgajai nākotnei un ko mums vajadzētu darīt tā vietā.

[^1]: Šī [diagramma](https://time.com/6300942/ai-progress-charts/) parāda uzdevumu kopu; daudzi līdzīgi līkņi varētu tikt pievienoti šim grafikam. Šis straujais progress šaurajā AI ir pārsteiguši pat jomas ekspertus, ar etaloniem, kas tiek pārspēti gadiem pirms prognozēm.

[^2]: Deepmind, OpenAI, Anthropic un X.ai visi tika dibināti ar konkrētu mērķi attīstīt MVI. Piemēram, OpenAI statūtos skaidri norādīts tā mērķis kā "mākslīgā vispārējā intelekta attīstīšana, kas labvēl visai cilvēcei", savukārt DeepMind misija ir "atrisināt intelektu un pēc tam to izmantot, lai atrisinātu visu pārējo." Meta, Microsoft un citi tagad īsteno būtiski līdzīgus ceļus. Meta ir teikusi, ka tā [plāno attīstīt MVI un izlaist to atklāti.](https://www.forbes.com/sites/johnkoetsier/2024/01/18/zuckerberg-on-ai-meta-building-agi-for-everyone-and-open-sourcing-it/)

[^3]: Hintons un Bendžio ir divi no visvairāk citētajiem AI pētniekiem, abi ir ieguvuši AI jomas Nobela prēmiju – Tjūringa balvu, un Hintons ir iegūjis arī Nobela prēmiju (fizikā).

[^4]: Kaut kā tāda būvēšana ar šādu risku, komerciālu stimulu ietekmē un gandrīz bez valdības uzraudzības, ir pilnībā bez precedenta. Pat nav strīdu par risku starp tiem, kas to būvē! Deepmind, OpenAI un Anthropic vadītāji, citu ekspertu vidū, visi ir burtiski parakstījuši [paziņojumu](https://www.safe.ai/work/statement-on-ai-risk), ka progresīvais AI rada *izmiršanas risku cilvēcei.* Trauksmes zvani nevarētu skanēt skaļāk, un var tikai secināt, ka tie, kas tos ignorē, vienkārši neuztver MVI un superintelektu nopietni. Viens no šī eseja mērķiem ir palīdzēt viņiem saprast, kāpēc viņiem vajadzētu.

## 2. nodaļa - Būtiskākais par AI neironu tīkliem

Kā darbojas mūsdienu AI sistēmas un kas mūs var sagaidīt nākamajā AI paaudzē?

Lai saprastu, kā attīstīsies spēcīgākas AI izveides sekas, ir būtiski apgūt dažus pamatus. Šī un nākamās divas sadaļas tos aplūko, pēc kārtas apskatot, kas ir mūsdienu AI, kā tas izmanto milzīgas skaitļošanas jaudas, un kādā mērā tas strauji kļūst vispārīgāks un spējīgāks.[^5]

Mākslīgo intelektu var definēt dažādi, bet mūsu nolūkiem galvenā AI īpašība ir tā, ka kamēr standarta datora programma ir instrukciju saraksts uzdevuma veikšanai, AI sistēma ir tāda, kas *mācās no datiem vai pieredzes veikt uzdevumus, netiekot skaidri pastāstīta, kā to darīt.*

Gandrīz viss nozīmīgais mūsdienu AI balstās uz neironu tīkliem. Tie ir matemātiski/skaitļošanas struktūras, ko reprezentē ļoti liels (miljardiem vai triljoniem) skaitļu kopums ("svari"), kas labi veic apmācības uzdevumu. Šos svarus rada (vai varbūt "izaudzē" vai "atrod"), tos iteratīvi pielāgojot, lai neironu tīkls uzlabotu skaitlisko rādītāju (arī saukts par "zudumu"), kas definēts, lai labi veiktu vienu vai vairākus uzdevumus.[^6] Šo procesu sauc par neironu tīkla *apmācību*.[^7]

Šādai apmācībai ir daudz paņēmienu, bet šīs detaļas ir daudz mazāk svarīgas nekā veids, kā tiek definēts vērtējums, un kā tas rezultējas dažādos uzdevumos, ko neironu tīkls veic labi. Vēsturiski ir tikts vilkts būtisks šķīrums starp "šauru" un "vispārējo" AI.

Šaurais AI ir apzināti apmācīts darīt konkrētu uzdevumu vai nelielu uzdevumu kopu (piemēram, atpazīt attēlus vai spēlēt šahu); tam nepieciešama pārapmācība jauniem uzdevumiem, un tam ir šaurs spēju diapazons. Mums ir pārcilvēcisks šaurais AI, kas nozīmē, ka gandrīz jebkuram diskrētam, labi definētam uzdevumam, ko cilvēks var darīt, mēs, iespējams, varam izveidot vērtējumu un tad veiksmīgi apmācīt šauru AI sistēmu, kas to darītu labāk nekā cilvēks.

Vispārējas nozīmes AI (GPAI) sistēmas var veikt plašu uzdevumu klāstu, tostarp daudzus, kam tās netika skaidri apmācītas; tās var arī apgūt jaunus uzdevumus kā daļu no savas darbības. Pašreizējie lielie "daudzveidīgie modeļi"[^8] kā ChatGPT to ilustrē: apmācīti uz ļoti liela tekstu un attēlu korpusa, tie var iesaistīties sarežģītā spriedumu veidošanā, rakstīt kodu, analizēt attēlus un palīdzēt plašā intelektuālo uzdevumu klāstā. Lai gan joprojām diezgan atšķiras no cilvēka intelekta veidos, ko redzēsim dziļi zemāk, to vispārīgums ir izraisījis revolūciju AI.[^9]

### Neparedzamība: galvenā AI sistēmu īpašība

Galvenā atšķirība starp AI sistēmām un parastajām programmām ir paredzamībā. Standarta programmatūras izvade var būt neparedzama - tiešām, dažkārt tāpēc mēs rakstām programmatūru, lai iegūtu rezultātus, ko nevarējām paredzēt. Bet parasta programmatūra reti dara kaut ko, kam tā nebija programmēta - tās darbības joma un uzvedība parasti atbilst plānotajam. Augstākā līmeņa šaha programma var veikt gājienus, ko neviens cilvēks nevarēja paredzēt (citādi viņi varētu uzvarēt šo šaha programmu!), bet tā parasti nedarīs neko citu kā vien spēlēs šahu.

Tāpat kā parastā programmatūra, šaurajam AI ir paredzama darbības joma un uzvedība, bet var būt neparedzami rezultāti. Tas patiešām ir tikai cits veids, kā definēt šauro AI: kā AI, kas līdzinās parastai programmatūrai savā paredzamībā un darbības diapazonā.

Vispārējas nozīmes AI ir citāds: tā darbības joma (domēni, kuros tas darbojas), uzvedība (veidi, kā tas rīkojas) un rezultāti (tā faktiskā izvade) visi var būt neparedzami.[^10] GPT-4 tika apmācīts tikai precīzi ģenerēt tekstu, bet attīstīja daudzas spējas, ko tā apmācītāji neparedzēja vai neieredzēja. Šī neparedzamība izriet no apmācības sarežģītības: tā kā apmācības dati satur daudzu dažādu uzdevumu izvadi, AI faktiski jāiemācās veikt šos uzdevumus, lai labi prognozētu.

Šī vispārējo AI sistēmu neparedzamība ir diezgan fundamentāla. Lai gan principā ir iespējams rūpīgi konstruēt AI sistēmas, kurām ir garantēti to uzvedības ierobežojumi (kā minēts vēlāk esejā), veids, kā AI sistēmas tagad tiek veidotas, tās ir neparedzamas gan praksē, gan principā.

### Pasīvais AI, aģenti, autonomās sistēmas un saskaņošana

Šī neparedzamība kļūst īpaši svarīga, kad apskatām, kā AI sistēmas faktiski tiek izvietotas un izmantotas dažādu mērķu sasniegšanai.

Daudzas AI sistēmas ir relatīvi pasīvas tādā ziņā, ka tās galvenokārt sniedz informāciju, un lietotājs veic darbības. Citas, ko parasti sauc par *aģentiem*, pašas veic darbības ar dažādu lietotāja iesaistes līmeni. Tās, kas veic darbības ar relatīvi mazāku ārēju ievadi vai uzraudzību, var saukt par vairāk *autonomām*. Tas veido spektru darbības neatkarības ziņā - no pasīviem rīkiem līdz autonomiem aģentiem.[^11]

Attiecībā uz AI sistēmu mērķiem, tie var būt tieši saistīti ar to apmācības mērķi (piemēram, "uzvaras" mērķis Go spēlējošai sistēmai ir arī tieši tas, kam tā tika apmācīta). Vai arī tie var nebūt: ChatGPT apmācības mērķis daļēji ir prognozēt tekstu, daļēji būt noderīgam palīgam. Bet, veicot konkrētu uzdevumu, tā mērķi piegādā lietotājs. Mērķus var izveidot arī pati AI sistēma, tikai ļoti netiešā saistībā ar tās apmācības mērķi.[^12]

Mērķi ir cieši saistīti ar "saskaņošanas" jautājumu, tas ir ar jautājumu, vai AI sistēmas *darīs to, ko mēs vēlamies, lai tās darītu*. Šis vienkāršais jautājums slēpj milzīgu subtilitāšu līmeni.[^13] Pagaidām ievērojiet, ka "mēs" šajā teikumā var attiekties uz daudziem dažādiem cilvēkiem un grupām, radot dažādus saskaņošanas veidus. Piemēram, AI varētu būt ļoti *paklausīgs* (vai ["uzticīgs"](https://arxiv.org/abs/2003.11157)) savam lietotājam - šeit "mēs" ir "katrs no mums." Vai tas varētu būt vairāk *suverēns*, galvenokārt vadoties no saviem mērķiem un ierobežojumiem, bet joprojām rīkojoties plaši cilvēku labklājības kopējās interesēs - "mēs" tad ir "cilvēce" vai "sabiedrība." Starpā ir spektrs, kur AI būtu lielākoties paklausīgs, bet varētu atteikties veikt darbības, kas kaitē citiem vai sabiedrībai, pārkāpj likumu utt.

Šie divi asi - autonomijas līmenis un saskaņošanas veids - nav pilnīgi neatkarīgi. Piemēram, suverēna pasīva sistēma, lai gan nav pilnīgi pašpretrunīga, ir koncepta spriedze, tāpat kā paklausīgs autonoms aģents.[^14] Ir skaidra izpratne, ka autonomija un suverenitāte mēdz iet roku rokā. Līdzīgā garā paredzamība mēdz būt augstāka "pasīvās" un "paklausīgās" AI sistēmās, turpretī suverēnās vai autonomās būs tieksme būt neparedzamākām. Viss tas būs izšķiroši svarīgi, lai saprastu potenciālā MVI un superintelekta sekas.

Patiesi saskaņota AI izveide, lai kāda tā būtu, prasa atrisināt trīs dažādus izaicinājumus:

1. Saprast, ko "mēs" vēlamies - kas ir sarežģīti neatkarīgi no tā, vai "mēs" nozīmē konkrētu personu vai organizāciju (lojalitāte) vai cilvēci plaši (suverenitāte);
2. Veidot sistēmas, kas regulāri rīkojas saskaņā ar šīm vēlmēm - būtībā radot konsekventu pozitīvu uzvedību;
3. Vispamatīgāk, veidot sistēmas, kas patiešām "rūpējas" par šīm vēlmēm, nevis tikai rīkojas tā, it kā tās to darītu.

Atšķirība starp uzticamu uzvedību un patieso rūpešanos ir būtiska. Tāpat kā cilvēks darbinieks varētu paklausīt rīkojumiem nevainojami, vienlaikus neizjūtot īstu apņēmību organizācijas misijai, AI sistēma varētu rīkoties saskaņoti, nepatiesībā novērtējot cilvēku preferences. Mēs varam apmācīt AI sistēmas teikt un darīt lietas, izmantojot atgriezenisko saiti, un tās var iemācīties spriest par to, ko cilvēki vēlas. Bet panākt, lai tās *patiešām* vērtē cilvēku preferences, ir daudz dziļāks izaicinājums.[^15]

Šo saskaņošanas izaicinājumu atrisināšanas pamatīgās grūtības un to sekas AI riskam tiks pētītas sīkāk zemāk. Pagaidām sapratiet, ka saskaņošana nav tikai tehniska funkcija, ko mēs pievienojam AI sistēmām, bet pamata aspekts to arhitektūrā, kas veido to attiecības ar cilvēci.


[^5]: For a gentle but technical introduction to machine learning and AI, particularly language models, see [this site.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) For another modern primer on AI extinction risks, see [this piece.](https://www.thecompendium.ai/) For a comprehensive and authoritative scientific analysis of the state of AI safety, see the recent [International AI Safety Report.](https://arxiv.org/abs/2501.17805)

[^6]: Training typically occurs by looking for a local maximum of the score in a high-dimensional space given by the model weights. By checking how the score changes as weights are tweaked, the training algorithm identifies which tweaks improve score the most, and moves the weights in that direction.

[^7]: For example, in an image recognition problem, the neural network would output probabilities for labels for the image. A score would be related to the probability the AI accords to the correct answer. The training procedure would then adjust weights so that next time, the AI would output a higher probability for the correct label for that image. This is then repeated a huge number of times. The same basic procedure is used in training essentially all modern neural networks, albeit with more complex scoring mechanism.

[^8]: Most multimodal models use the "transformer" architecture to process and generate multiple types of data (text, images, sound). These can all decomposed into, and then treated on the same footing, as different types of "tokens." Multimodal models are trained first to accurately predict tokens within massive datasets, then refined through reinforcement learning to enhance capabilities and shape behaviors.

[^9]: That language models are trained to do one thing – predict words – has caused some to call them narrow AI. But this is misleading: because predicting text well requires so many different capabilities, this training task leads to a surprisingly general system. Also note that these systems are extensively trained by reinforcement learning, effectively representing thousands of people giving the model a reward signal when it does a good job at any of the many things it does. It then inherits significant generality from the people giving this feedback.

[^10]: There are multiple ways in which AI is unpredictable. One is that in the general case one cannot predict what an algorithm will do without actually running it; there are [theorems](https://arxiv.org/abs/1310.3225) to this effect. This can be true just because the output of algorithms can be complex. But it is particularly clear and relevant in the case (such as in chess or Go) where the prediction would imply a capability (beating the AI) the would-be predictor does not have. Second, a given AI system will not always produce the same output even given the same input – its outputs contain randomness; this also couples with algorithmic unpredictability. Third, unexpected and emergent capabilities can arise from training, meaning even the *types* of things an AI system can and will do are unpredictable; This last type is particularly important for safety considerations.

[^11]: See [here](https://arxiv.org/abs/2502.02649) for an in-depth review of what is meant by an "autonomous agent" (along with ethical arguments against building them).

[^12]: You may sometimes hear "AI can't have its own goals." This is absolute nonsense. It is easy to generate examples where AI has or develops goals that were never given to it and are known only to itself. You don't see this much in current popular multimodal models because it is trained out of them; it could just as easily be trained into them.

[^13]: There's a large literature. On the general problem see Christian's [*The Alignment Problem*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821), and Russell's [*Human-Compatible*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616). On a more technical side see e.g. [this paper](https://arxiv.org/abs/2209.00626).

[^14]: We'll later see that while such systems buck the trend, that actually makes them very interesting and useful.

[^15]: This is not to say we require emotions or sentience. Rather, it is enormously difficult from the outside of a system to know what its inner goals, preferences, and values are. "Genuine" here would mean that we have strong enough reason to rely on it that in the case of critical systems we can bet our lives on it.

## 3. nodaļa - Galvenie mūsdienu vispārējā AI sistēmu izstrādes aspekti

Lielākā daļa pasaules vismodernāko AI sistēmu tiek radītas, izmantojot pārsteidzoši līdzīgas metodes. Šeit ir pamati.

Lai patiesi izprastu cilvēku, nepieciešamas zināšanas par bioloģiju, evolūciju, bērnu audzināšanu un citām jomām; lai izprastu AI, jāzina arī par tā izstrādes procesu. Pēdējo piecu gadu laikā AI sistēmas ir ievērojami attīstījušās gan spēju, gan sarežģītības ziņā. Galvenais veicinošais faktors ir bijusi ļoti lielu skaitļošanas resursu (vai sarunvalodā "skaitļošanas jaudas", runājot par AI) pieejamība.

Skaitļi ir pārsteidzoši. Modeļu apmācībai, piemēram, GPT sērijai, Claude, Gemini utt., tiek izmantotas aptuveni 10 <sup>25</sup> -10 <sup>26</sup> "peldošā komata operācijas" (FLOP) [^16].[^17] (Salīdzinājumam: ja visi Zemes cilvēki nepārtraukti strādātu, veicot vienu aprēķinu ik pēc piecām sekundēm, būtu nepieciešams aptuveni miljards gadu, lai to paveiktu.) Šis milzīgais skaitļošanas apjoms ļauj apmācīt modeļus ar līdz pat triljoniem modeļa parametru, izmantojot terabaitus datu - lielu daļu no visa kvalitatīvā teksta, kas jebkad ir uzrakstīts, kopā ar plašām skaņu, attēlu un video bibliotēkām. Papildus šai apmācībai tiek veikta plaša papildu apmācība, kas pastiprina cilvēku preferences un labu uzdevumu izpildi. Šādā veidā apmācīti modeļi uzrāda ar cilvēkiem salīdzināmu sniegumu plašā pamata intelektuālo uzdevumu spektrā, ieskaitot spriedumu veidošanu un problēmu risināšanu.

Mēs arī zinām (ļoti, ļoti aptuveni), cik liela skaitļošanas ātruma (operāciju sekundē) ir pietiekami, lai šādas sistēmas *secinājumu izdarīšanas* ātrums [^18] atbilstu cilvēka teksta apstrādes *ātrumam*. Tas ir aptuveni 10 <sup>15</sup> -10 <sup>16</sup> FLOP sekundē.[^19]

Lai gan šie modeļi ir spēcīgi, to raksturam raksturīgi būtiski ierobežojumi, kas ir diezgan līdzīgi tam, kā būtu ierobežots atsevišķs cilvēks, ja viņš būtu spiests vienkārši izvadīt tekstu ar fiksētu vārdu ātrumu minūtē, neapstājoties domāt vai neizmantojot nekādus papildu rīkus. Jaunākās AI sistēmas novērš šos ierobežojumus, izmantojot sarežģītāku procesu un arhitektūru, kas apvieno vairākus galvenos elementus:

- Vienu vai vairākus neironu tīklus, kur viens modelis nodrošina galveno kognitīvo spēju, bet līdz pat vairāki citi veic citus, šaurākus uzdevumus;
- *Instrumentus*, kas tiek nodrošināti modelim un ko tas var izmantot - piemēram, spēja meklēt internetā, izveidot vai rediģēt dokumentus, izpildīt programmas utt.
- *Atbalsta struktūru*, kas savieno neironu tīklu ieejās un izejas. Ļoti vienkārša atbalsta struktūra varētu vienkārši ļaut diviem AI modeļa "gadījumiem" sarunāties savā starpā vai vienam pārbaudīt otra darbu.[^20]
- *Domāšanas ķēde* un saistītās uzvednes metodes dara kaut ko līdzīgu, liekot modelim, piemēram, izveidot daudzas problēmas risinājuma pieejas, pēc tam apstrādāt šīs pieejas kopēja atbilžu iegūšanai.
- Modeļu *pārapmācība*, lai tie labāk izmantotu instrumentus, atbalsta struktūru un domāšanas ķēdi.

Tā kā šie paplašinājumi var būt ļoti spēcīgi (un ietvert pašas AI sistēmas), šīs kompozītsistēmas var būt diezgan sarežģītas un dramatiski uzlabot AI spējas.[^21] Un nesen scaffolding un īpaši domāšanas ķēdes uzvedņu metodes (un rezultātu iekļaušana atpakaļ modeļu pārapmācībā, lai tie tos labāk izmantotu) ir izstrādātas un ieviests [o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/) un [DeepSeek R1](https://api-docs.deepseek.com/news/news250120), lai veiktu daudzus secinājumu ciklus atbildē uz konkrētu vaicājumu.[^22] Tas faktiski ļauj modelim "domāt par" savu atbildi un dramatiski pastiprina šo modeļu spēju veikt augsta līmeņa spriedumus zinātnes, matemātikas un programmēšanas uzdevumos.[^23]

Konkrētai AI arhitektūrai apmācības skaitļošanas palielināšana [var būt ticami pārvērsta](https://arxiv.org/abs/2405.10938) uzlabojumos skaidri definētu rādītāju kopā. Mazāk precīzi definētām vispārējām spējām (piemēram, tām, kas aplūkotas tālāk), šī pārvēršana ir mazāk skaidra un paredzama, bet ir gandrīz noteikti, ka lielākiem modeļiem ar vairāk apmācības skaitļošanas būs jaunas un labākas spējas, pat ja ir grūti paredzēt, kādas tās būs.

Līdzīgi kompozītsistēmas un īpaši "domāšanas ķēdes" sasniegumi (un modeļu apmācība, kas labi darbojas ar to) ir atklājuši mērogošanu *secinājumu izdarīšanas* skaitļošanā: konkrētam apmācītam pamatmodelim vismaz dažas AI sistēmas spējas palielinās, kad tiek pielietota vairāk skaitļošanas, kas ļauj tām "domāt smagāk un ilgāk" par sarežģītām problēmām. Tas nāk ar augstām skaitļošanas ātruma izmaksām, prasot simtiem vai tūkstošiem vairāk FLOP/s, lai atbilstu cilvēka sniegumam.[^24]

Lai gan tas ir tikai daļa no tā, kas izraisa strauju AI progresu,[^25] skaitļošanas loma un kompozītsistēmu iespējas izrādīsies būtiskas gan nekontrolējama MVI novēršanā, gan drošāku alternatīvu izstrādē.

[^16]: 10 <sup>27</sup> nozīmē 1, kam seko 25 nulles, jeb desmit triljoni triljonu. FLOP ir vienkārši aritmētiska skaitļu saskaitīšana vai reizināšana ar kādu precizitāti. Ņemiet vērā, ka AI aparatūras veiktspēja var atšķirties par desmit reižu faktoru atkarībā no aritmētikas precizitātes un datora arhitektūras. Loģisko vārtu operāciju (UN, VAI, UN NE) skaitīšana būtu fundamentāla, bet tās nav plaši pieejamas vai testētas; pašreizējiem mērķiem ir lietderīgi standartizēt 16 bitu operācijas (FP16), lai gan būtu jāizveido atbilstoši konversijas faktori.

[^17]: Novērtējumu un precīzu datu kolekcija ir pieejama no [Epoch AI](https://epochai.org/data/large-scale-ai-models) un norāda uz aptuveni 2×10 <sup>25</sup> 16 bitu FLOP GPT-4; tas aptuveni atbilst [numuriem, kas tika nopludināti](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) GPT-4. Novērtējumi citiem 2024. gada vidus modeļiem visi ir dažu faktoru robežās no GPT-4.

[^18]: Secinājumu izdarīšana ir vienkārši process, kā no neironu tīkla ģenerēt izvadi. Apmācību var uzskatīt par daudziem secinājumiem un modeļa parametru korekcijām.

[^19]: Teksta ražošanai sākotnējam GPT-4 bija nepieciešami 560 TFLOP uz katru ģenerēto žetonu. Aptuveni 7 žetoni/s ir nepieciešami, lai sekotu līdzi cilvēka domāšanai, tāpēc tas dod ≈3×10 <sup>15</sup> FLOP/s. Bet efektivitātes uzlabojumi to ir samazinājuši; [šajā NVIDIA brošūrā](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/), piemēram, norādīts uz tik maz kā 3×10 <sup>14</sup> FLOP/s salīdzināmi darboties spējīgam Llama 405B modelim.

[^20]: Kā nedaudz sarežģītāks piemērs, AI sistēma varētu vispirms ģenerēt vairākus iespējamos matemātikas problēmas risinājumus, pēc tam izmantot citu gadījumu, lai pārbaudītu katru risinājumu, un beigās izmantot trešo, lai sintezētu rezultātus skaidrā skaidrojumā. Tas ļauj rūpīgāk un uzticamāk risināt problēmas nekā viens cikls.

[^21]: Skatiet, piemēram, detaļas par [OpenAI "Operator"](https://openai.com/index/introducing-operator/), [Claude rīku iespējām](https://docs.anthropic.com/en/docs/build-with-claude/computer-use) un [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). OpenAI [Deep Research](https://openai.com/index/introducing-deep-research/) visticamāk ir diezgan sarežģīta arhitektūra, bet detaļas nav pieejamas.

[^22]: Deepseek R1 paļaujas uz iteratīvu modeļa apmācību un uzvednēm, lai gala apmācītais modelis radītu plašu domāšanas ķēdes spriedumu veidošanu. Arhitektūras detaļas nav pieejamas o1 vai o3, tomēr Deepseek ir atklājis, ka nav nepieciešama īpaša "maģiska sastāvdaļa", lai atbloķētu spēju mērogošanu ar secinājumu izdarīšanu. Bet, neskatoties uz to, ka saņēma lielu preses uzmanību kā "status quo" apgāzošana AI jomā, tas neietekmē šī raksta pamatapgalvojumus.

[^23]: Šie modeļi ievērojami pārspēj standarta modeļus spriedumu testēšanas etalonos. Piemēram, GPQA Diamond etalona testā - stingrā PhD līmeņa zinātnes jautājumu testā - GPT-4o [ieguva](https://openai.com/index/learning-to-reason-with-llms/) 56%, kamēr o1 un o3 sasniedza 78% un 88% attiecīgi, tālu pārsniedzot cilvēku ekspertu 70% vidējo rezultātu.

[^24]: OpenAI O3 visticamāk tērēja ∼10 <sup>21</sup> -10 <sup>22</sup> FLOP [lai pabeigtu katru no ARC-AGI izaicinājuma jautājumiem](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai), ko kompetenti cilvēki var izdarīt (sakot) 10-100 sekundēs, dodot skaitli vairāk kā ∼10 <sup>20</sup> FLOP/s.

[^25]: Lai gan skaitļošana ir galvenais AI sistēmu spēju mērs, tā mijiedarbojas gan ar datu kvalitāti, gan algoritmisko uzlabojumiem. Labāki dati vai algoritmi var samazināt skaitļošanas prasības, kamēr vairāk skaitļošanas dažkārt var kompensēt vājākus datus vai algoritmus.

## 4. nodaļa - Kas ir MVI un superintelekts?

Ko tieši pasaules lielākie tehnoloģiju uzņēmumi cenšas izveidot aiz slēgtām durvīm?

Termins "mākslīgais vispārējais intelekts" jau kādu laiku eksistē, apzīmējot "cilvēka līmeņa" vispārējas nozīmes mākslīgo intelektu. Tas nekad nav bijis īpaši precīzi definēts termins, taču pēdējos gados tas paradoksāli ir kļuvis ne labāk definēts, bet vēl nozīmīgāks - eksperti vienlaikus strīdas par to, vai MVI ir desmitiem gadu attālumā vai jau sasniegts, savukārt triljonu dolāru vērtībā uzņēmumi sacenšas "ceļā uz MVI". (MVI neskaidrību nesen uzskatāmi parādīja [noplūdušie dokumenti, kas it kā atklāja](https://gizmodo.com/leaked-documents-show-openai-has-a-very-clear-definition-of-agi-2000543339), ka OpenAI līgumā ar Microsoft MVI tika definēts kā MI, kas OpenAI ģenerē 100 miljardu dolāru ieņēmumus – gana merkantilistiska un ne pārāk intelektuāla definīcija.)

Ar ideju par MI ar "cilvēka līmeņa intelektu" ir divas pamatproblēmas. Pirmkārt, cilvēki ļoti, ļoti atšķiras savā spējā veikt jebkuru konkrētu kognitīvā darba veidu, tāpēc "cilvēka līmenis" neeksistē. Otrkārt, intelekts ir ļoti daudzpusīgs; lai gan var būt korelācijas, tās ir nepilnīgas un MI gadījumā var būt diezgan atšķirīgas. Tātad pat ja mēs varētu definēt "cilvēka līmeni" daudzām spējām, MI noteikti būtu tālu pārāk šo līmeni dažās jomās, vienlaikus būdams krietni zemāk citu spēju ziņā.[^26]

Tomēr ir ļoti svarīgi spēt diskutēt par MI spēju veidiem, līmeņiem un robežvērtībām. Šeit izmantotā pieeja uzsver, ka vispārējas nozīmes MI jau ir klāt un ka tas nāk - un nāks - dažādos spēju līmeņos, kuriem ir ērti piesaistīt terminus, pat ja tie ir reduktīvi, jo tie atbilst svarīgām robežvērtībām MI ietekmes uz sabiedrību un cilvēci ziņā.

Mēs definēsim "pilnīgu" MVI kā sinonīmu "pārcilvēciskajam vispārējas nozīmes MI", kas nozīmē MI sistēmu, kas spēj veikt būtībā visus cilvēka kognitīvos uzdevumus eksperta līmenī vai virs tā, kā arī apgūt jaunas prasmes un pārnest spējas uz jaunām jomām. Tas atbilst tam, kā "MVI" bieži tiek definēts mūsdienu literatūrā. Ir svarīgi atzīmēt, ka šī ir *ļoti* augsta robežvērtība. Nevienam cilvēkam nav šāda veida intelekts; drīzāk tas ir tāds intelekts, kāds būtu lielu augstākā līmeņa cilvēku ekspertu kopumu, ja tos apvienotu. Mēs varam nosaukt "superintelektu" par spēju, kas iet tālāk par to, un definēt ierobežotākus spēju līmeņus kā "cilvēku konkurētspējīgu" un "ekspertu konkurētspējīgu" VNMI, kas veic plašu uzdevumu klāstu tipiskas profesionāļu vai cilvēku ekspertu līmenī.[^27]

Šie termini un daži citi ir apkopoti [tabulā](https://keepthefuturehuman.ai/essay/docs/#tab:terms) zemāk. Lai konkrētāk izprastu, ko dažādi sistēmu līmeņi spēj darīt, ir lietderīgi ņemt definīcijas nopietni un apsvērt, ko tās nozīmē.

| MI veids                          | Saistītie termini                               | Definīcija                                                                                                                                                                                       | Piemēri                                                                                                                                              |
| --------------------------------- | ----------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------- |
| Šaurais MI                        | Vājais MI                                       | MI, kas apmācīts konkrētam uzdevumam vai uzdevumu grupai. Pārspēj savā jomā, bet trūkst vispārēja intelekta vai pārneses mācīšanās spējas.                                                      | Attēlu atpazīšanas programmatūra; Balss asistenti (piem., Siri, Alexa); Šaha spēlēšanas programmas; DeepMind AlphaFold                               |
| AI rīki                           | Paplašinātais intelekts, MI asistents           | (Tiek apspriests tālāk esejā.) MI sistēma, kas uzlabo cilvēka spējas. Apvieno cilvēku konkurētspējīgu vispārējas nozīmes MI, šauro MI un garantētu kontroli, prioritāri nosakot drošību.        | Progresīvi kodēšanas asistenti; MI darbināmi pētniecības rīki; Sarežģītas datu analīzes platformas. Kompetenti, bet šauri un kontrolējami aģenti    |
| Vispārējas nozīmes MI (VNMI)      |                                                 | MI sistēma, kas adaptējama dažādiem uzdevumiem, ieskaitot tos, kuriem tā nav īpaši apmācīta.                                                                                                     | Valodu modeļi (piem., GPT-4, Claude); Daudzveidīgie MI modeļi; DeepMind MuZero                                                                       |
| Cilvēku konkurētspējīgs VNMI      | MVI [vājais]                                    | Vispārējas nozīmes MI, kas veic uzdevumus vidējā cilvēka līmenī, dažkārt to pārsniedzot.                                                                                                         | Progresīvi valodu modeļi (piem., O1, Claude 3.5); Daži daudzveidīgie MI sistēmas                                                                    |
| Ekspertu konkurētspējīgs VNMI     | MVI [daļējs]                                    | Vispārējas nozīmes MI, kas veic lielāko daļu uzdevumu cilvēku ekspertu līmenī, ar nozīmīgu, bet ierobežotu autonomiju                                                                            | Iespējams, ar rīkiem un atbalsta struktūru aprīkots O3, vismaz matemātikā, programmēšanā un dažās precīzajās zinātnēs                               |
| MVI [pilnīgs]                     | Pārcilvēcisks VNMI                              | MI sistēma, kas spējīga autonomi veikt apmēram visus cilvēku intelektuālos uzdevumus eksperta līmenī vai virs tā, ar efektīvu mācīšanos un zināšanu pārnesi.                                    | [Nav pašreizēju piemēru – teorētisks]                                                                                                               |
| Superintelekts                    | Ļoti pārcilvēcisks VNMI                         | MI sistēma, kas tālu pārspēj cilvēku spējas visās jomās, pārspējot kolektīvu cilvēku ekspertīzi. Šī pārākuma varētu būt vispārīgumā, kvalitātē, ātrumā un/vai citos rādītājos.                  | [Nav pašreizēju piemēru – teorētisks]                                                                                                               |

Mēs jau piedzīvojam, kāds ir dzīvot ar VNMI līdz cilvēku konkurētspējīgam līmenim. Tas ir integrējies samērā vienmērīgi, jo lielākā daļa lietotāju to piedzīvo kā gudru, bet ierobežotu pagaidu darbinieku, kas padara viņus produktīvākus ar jauktu ietekmi uz viņu darba kvalitāti.[^28]

Kas atšķirtos ar ekspertu konkurētspējīgu VNMI, ir tas, ka tam nebūtu mūsdienu MI pamatierobežojumu, un tas darītu to, ko dara ekspertu: neatkarīgu ekonomiski vērtīgu darbu, īstu zināšanu radīšanu, tehnisko darbu, uz kuru var paļauties, vienlaikus reti (kaut gan tomēr dažkārt) pieļaujot muļķīgas kļūdas.

Pilnīga MVI ideja ir tāda, ka tas *patiešām dara* visas kognitīvās lietas, ko dara pat visai spējīgākie un efektīvākie cilvēki, autonomi un bez nepieciešamās palīdzības vai uzraudzības. Tas iekļauj sarežģītu plānošanu, jaunu prasmju apguvi, kompleksu projektu vadību utt. Tas varētu veikt oriģinālu pirmlīmeņa pētniecību. Tas varētu vadīt uzņēmumu. Lai kāds būtu jūsu darbs, ja tas galvenokārt tiek darīts pie datora vai pa tālruni, *tas varētu to darīt vismaz tikpat labi kā jūs.* Un droši vien daudz ātrāk un lētāk. Mēs apspriedīsim dažas no sekām zemāk, bet pagaidām jūsu izaicinājums ir to patiešām nopietni uztverts. Iedomājieties desmit viszinošākos un kompetentākos cilvēkus, kurus jūs zināt vai par kuriem zināt - ieskaitot uzņēmumu vadītājus, zinātnieku, profesorus, augstākā līmeņa inženierus, psihologus, politisko vadītāju un rakstnieku. Sapludiniet tos visus vienā, kas arī runā 100 valodās, tam ir brīnišķīga atmiņa, tas darbojas ātri, ir nenogurstošs un vienmēr motivēts, un strādā par atalgojumu, kas zemāks par minimālo algu.[^29] Tāds ir priekšstats par to, kas būtu MVI.

Superintelektu ir grūtāk iedomāties, jo ideja ir tāda, ka tas varētu veikt intelektuālus varoņdarbus, ko neviens cilvēks vai pat cilvēku kopums nevar - tas pēc definīcijas ir mums nepieredzams. Bet mēs varam gūt priekšstatu. Kā pamatbāzes līmenis, apsveriet daudzus MVI, katru daudz spējīgāku par pat augstākā līmeņa cilvēku ekspertu, darbojoties 100 reizes cilvēciskā ātrumā, ar milzīgu atmiņu un lielisku koordinācijas spēju.[^30] Un tas turpina augt no turienes. Darīšanās ar superintelektu būtu mazāk līdzīga sarunai ar citādu prātu, vairāk līdzīga sarunu ar citādu (un progresīvāku) civilizāciju.

Tāpēc cik tuvu mēs *esam* MVI un superintelektam?


[^26]: Piemēram, pašreizējās MI sistēmas tālu pārspēj cilvēku spējas ātrajos aprēķinos vai atmiņas uzdevumos, vienlaikus atpaliekot abstraktajā spriedumu veidošanā un radošā problēmu risināšanā.

[^27]: Ļoti svarīgi, ka kā konkurentam šādam MI būtu vairāki lieli strukturāli priekšrocību, ieskaitot: tas nenogurtu vai tam nebūtu citu individuālu vajadzību kā cilvēkiem; to var darbināt lielākā ātrumā, vienkārši palielinot skaitļošanas jaudu; to var kopēt kopā ar jebkuru ekspertīzi vai zināšanām, ko tas iegūst - un neironu tīklu iegūtās zināšanas var pat "sapludināt", lai pārnestu veselas prasmu kopas savā starpā; tas varētu sazināties mašīnu ātrumā; un tas varētu sevi modificēt vai pilnveidot nozīmīgākos veidos un lielākā ātrumā nekā jebkurš cilvēks.

[^28]: Ja jūs neesat pavadījis laiku, izmantojot pašreizējās augstākā līmeņa MI sistēmas, es to iesaku: tās ir patiesi noderīgas un spējīgas, un tas arī ir svarīgi, lai kalibrētu MI ietekmi, kā tās kļūs spēcīgākas.

[^29]: Apsveriet lielu pētniecības slimnīcu: pilnīgi realizēts MVI varētu vienlaikus analizēt visus ienākošos pacienta datus, sekot līdzi katram jaunajam medicīnas rakstam, ierosināt diagnozes, izstrādāt ārstēšanas plānu, vadīt klīniskos pētījumu un koordinēt personāla grafiku - visu to darot līmenī, kas atbilst vai pārsniedz slimnīcas augstākā līmeņa speciālistu spējas katrā jomā. Un tas varētu to darīt vairākās slimnīcās vienlaikus, par daļu no pašreizējām izmaksām. Diemžēl jums jāapsver arī organizētas noziedzības sindikāts: pilnīgi realizēts MVI varētu vienlaikus uzlauzīt, izlikties par, spiegot un šantažēt tūkstošiem upuru, sekot līdzi tiesībaizsardzībai (kas automatizējas daudz lēnāk), izstrādāt jaunas naudas pelnīšanas shēmas un koordinēt personāla grafiku - ja vispār ir kāds personāls.

[^30]: Savā [esejā](https://darioamodei.com/machines-of-loving-grace) Dario Amodei, Anthropic izpilddirektors, atgādināja par "Valsti ar [miljonu] ģēniju".

## 5. nodaļa - Pie sliekšņa

Ceļš no šodienas AI sistēmām līdz pilnvērtīgam MVI šķiet pārsteidzoši īss un paredzams.

Pēdējie desmit gadi ir atnēsuši dramatiskus AI sasniegums, ko veicinājuši milzīgi [skaitļošanas](https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year), cilvēkresursu un [finanšu](https://arxiv.org/abs/2405.21015) ieguldījumi. Daudzas šauras AI lietojumprogrammas savos uzdevumos pārspēj cilvēkus un noteikti ir daudz ātrākas un lētākas.[^31] Pastāv arī šauras cilvēku pārspējošas sistēmas, kas var sagraut visus cilvēkus šauru jomu spēlēs, piemēram, [Go](https://www.nature.com/articles/nature16961), [šahā](https://arxiv.org/abs/1712.01815) un [pokerī](https://www.deepstack.ai/), kā arī vispārīgāki [aģenti](https://deepmind.google/discover/blog/a-generalist-agent/), kas spēj plānot un izpildīt darbības vienkāršotās simulētās vidēs tikpat efektīvi kā cilvēki.

Visspilgtāk - pašreizējās vispārējās AI sistēmas no OpenAI/Microsoft, Google/Deepmind, Anthropic/Amazon, Facebook/Meta, X.ai/Tesla un citiem [^32] ir parādījušās kopš 2023. gada sākuma un kopš tā laika ir pastāvīgi (lai arī nevienmērīgi) palielinājušas savas spējas. Visas tās ir radītas, izmantojot simbolu prognozēšanu milzīgos teksta un multimediju datu kopās, apvienojot to ar plašu atgriezenisko saiti no cilvēkiem un citām AI sistēmām. Dažas no tām ietver arī plašas rīku un atbalsta struktūru sistēmas.

### Pašreizējo vispārējo sistēmu stiprās un vājās puses

Šīs sistēmas labi darbojas arvien plašākā testu spektrā, kas paredzēts intelekta un ekspertīzes mērīšanai, ar progresu, kas ir pārsteigusi pat nozares ekspertus:

- Kad tika pirmo reizi publicēts, GPT-4 [atbilda vai pārspēja tipisku cilvēku sniegumu](https://arxiv.org/abs/2303.08774) standarta akadēmiskajos testos, ieskaitot SAT, GRE, iestājeksāmenus un advokātu eksāmenus. Jaunākie modeļi, iespējams, darbojas ievērojami labāk, lai gan rezultāti nav publiski pieejami.
- Tjūringa tests - ilgi uzskatīts par galveno "īsta" AI etalonu - tagad tiek regulāri nokārtots dažādās formās ar mūsdienu valodas modeļiem, gan neformāli, gan [formālos pētījumos](https://arxiv.org/abs/2405.08007).[^33]
- Visaptverošajā MMLU etalontestā, kas aptver 57 akadēmiskos priekšmetus, [jaunākie modeļi sasniedz eksperta līmeņa rezultātus](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu) (∼90%) [^34]
- Tehniskā ekspertīze ir dramatiski uzlabojusies: GPQA etalontesta rezultāti maģistra līmeņa fizikā [uzlēca](https://epoch.ai/data/ai-benchmarking-dashboard) no gandrīz nejaušas minēšanas (GPT-4, 2022) līdz eksperta līmenim (o1-preview, 2024).
- Pat testi, kas īpaši izveidoti kā AI-izturīgi, tiek pārvarēti: OpenAI O3 [ziņots](https://www.nextbigfuture.com/2024/12/openai-releases-o3-model-with-high-performance-and-high-cost.html), ka atrisina ARC-AGI abstrakto problēmu risināšanas etalonu cilvēka līmenī, sasniedz augstākā eksperta kodēšanas sniegumu un iegūst 25% Epoch AI "robežmatemātikas" problēmās, kas radītas, lai izaicinātu elites matemātiķus.[^35]
- Tendence ir tik skaidra, ka MMLU izstrādātājs tagad ir izveidojis ["Cilvēces pēdējo eksāmenu"](https://agi.safe.ai/) - drūms nosaukums, kas atspoguļo iespēju, ka AI drīz pārspēs cilvēku sniegumu jebkurā jēgpilnā testā. Rakstīšanas brīdī pastāv apgalvojumi, ka AI sistēmas sasniedz 27% (pēc [Sema Altmana](https://x.com/sama/status/1886220281565381078)) un 35% (pēc [šī raksta](https://arxiv.org/abs/2502.09955)) šajā ārkārtīgi sarežģītajā eksāmenā. Nav ticams, ka kāds atsevišķs cilvēks to spētu.

Neraugoties uz šiem iespaidīgajiem rādītājiem (un to acīmredzamo intelektu saziņas laikā) [^36] ir daudz lietu, ko šie neironu tīkli *nevar* darīt (vismaz publicētās versijas). Pašlaik lielākā daļa ir bezmiesiskas - pastāv tikai serveros - un apstrādā vienīgi tekstu, skaņu un nekustīgos attēlus (bet ne video). Būtiski, lielākā daļa nevar veikt sarežģītas plānotas darbības, kas prasa augstu precizitāti.[^37] Un ir vēl vairākas īpašības, kas ir stipras augsta līmeņa cilvēku izziņā, bet vājas publicētajās AI sistēmās.

Šajā tabulā ir uzskaitītas vairākas no tām, balstoties uz 2024. gada vidus AI sistēmām, piemēram, GPT-4o, Claude 3.5 Sonnet un Google Gemini 1.5.[^38] Galvenais jautājums par to, cik ātri vispārējais AI kļūs spēcīgāks, ir: cik lielā mērā *vairāk no tā paša* dos rezultātus, salīdzinot ar papildu, bet *zināmu* tehniķu pievienošanu, salīdzinot ar *patiesi jaunu* AI pētniecības virzienu izstrādi vai ieviešanu. Mani prognozes par to ir dotas tabulā, izsakot, cik ticami katrs no šiem scenārijiem iegūs šo spēju cilvēka līmenī un pāri tam.

<table><tbody><tr><th>Spēja</th><th>Spējas apraksts</th><th>Status/prognoze</th><th>Mērogošana/zināms/jauns</th></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td colspan="4"><em>Pamata izziņas spējas</em></td></tr><tr><td>Spriedumu veidošana</td><td>Cilvēki spēj veikt precīzu, daudzu soļu spriedumu veidošanu, ievērojot noteikumus un pārbaudot precizitāti.</td><td>Dramatisks nesenais progress, izmantojot paplašinātu domāšanas ķēdi un atkārtotu apmācību</td><td>95/5/5</td></tr><tr><td>Plānošana</td><td>Cilvēki uzrāda ilgtermiņa un hierarhisku plānošanu.</td><td>Uzlabojas ar mērogu; var būt stipri atbalstīta, izmantojot atbalsta struktūras un labākas apmācības metodes.</td><td>10/85/5</td></tr><tr><td>Patiesuma pamatošana</td><td>GPAI izdomā nepamatotu informāciju, lai apmierinātu vaicājumus.</td><td>Uzlabojas ar mērogu; kalibrācijas dati pieejami modelī; var pārbaudīt/uzlabot ar atbalsta struktūrām.</td><td>30/65/5</td></tr><tr><td>Elastīga problēmu risināšana</td><td>Cilvēki var atpazīt jaunus modeļus un izgudrot jaunus risinājumus sarežģītām problēmām; pašreizējie ML modeļi cīnās.</td><td>Uzlabojas ar mērogu, bet vāji; var būt risināms ar neirosimboliskām vai vispārinātas "meklēšanas" metodēm.</td><td>15/75/10</td></tr><tr><td colspan="4"><em>Mācīšanās un zināšanas</em></td></tr><tr><td>Mācīšanās un atmiņa</td><td>Cilvēkiem ir darba, īstermiņa un ilgtermiņa atmiņa, kas visas ir dinamiskas un savstarpēji saistītas.</td><td>Visi modeļi mācās apmācības laikā; GPAI mācās konteksta logā un precizētās iestatīšanas laikā; pastāv "nepārtrauktas mācīšanās" un citas metodes, bet vēl nav integrētas lielās GPAI.</td><td>5/80/15</td></tr><tr><td>Abstrakcija un rekursija</td><td>Cilvēki var kartēt un pārnest relāciju kopas abstraktākās, lai spriestu un manipulētu, ieskaitot rekursīvu "meta" spriedumu veidošanu.</td><td>Vāji uzlabojas ar mērogu; varētu parādīties neirosimboliskās sistēmās.</td><td>30/50/20</td></tr><tr><td>Pasaules modeļi</td><td>Cilvēkiem ir un tie pastāvīgi atjaunina prognozējošu pasaules modeli, kura ietvaros var risināt problēmas un veikt fizisko spriedumu veidošanu</td><td>Uzlabojas ar mērogu; atjaunošana saistīta ar mācīšanos; GPAI vāji reālās pasaules prognozēšanā.</td><td>20/50/30</td></tr><tr><td colspan="4"><em>Es un aģentūra</em></td></tr><tr><td>Aģentūra</td><td>Cilvēki var rīkoties, lai īstenotu mērķus, balstoties uz plānošanu/prognozēšanu.</td><td>Daudzas ML sistēmas ir aģentiskas; LLM var padarīt par aģentiem ar ietvariem.</td><td>5/90/5</td></tr><tr><td>Pašvirzība</td><td>Cilvēki attīsta un īsteno savus mērķus ar iekšēji ģenerētu motivāciju un dzinuļu.</td><td>Lielākoties sastāv no aģentūras plus oriģinalitātes; iespējams parādīsies sarežģītās aģentiskās sistēmās ar abstraktiem mērķiem.</td><td>40/45/15</td></tr><tr><td>Pašatsauce</td><td>Cilvēki saprot un spriež par sevi kā situētiem vidē/kontekstā.</td><td>Uzlabojas ar mērogu un varētu tikt papildināts ar apmācības atalgojumu.</td><td>70/15/15</td></tr><tr><td>Pašapziņa</td><td>Cilvēkiem ir zināšanas par un tie var spriest par savām domām un garīgajiem stāvokļiem.</td><td>Pastāv zināmā mērā GPAI, kas var iespējams nokārtot klasisso "spoguļa testu" pašapziņai. Var uzlabot ar atbalsta struktūrām; bet nav skaidrs, vai tas pietiek.</td><td>20/55/25</td></tr><tr><td colspan="4"><em>Saskarne un vide</em></td></tr><tr><td>Iemiesots intelekts</td><td>Cilvēki saprot un aktīvi mijiedarbojas ar savu reālās pasaules vidi.</td><td>Pastiprinājuma mācīšanās labi darbojas simulētās un reālās pasaules (robotiskas) vidēs un var tikt integrēta daudzmodālos transformatoros.</td><td>5/85/10</td></tr><tr><td>Daudzjutekļu apstrāde</td><td>Cilvēki integrē un reāllaikā apstrādā vizuālos, audio un citus jutekļu straumes.</td><td>Apmācība vairākās modalitātēs šķiet "vienkārši darbojas" un uzlabojas ar mērogu. Reāllaika video apstrāde ir grūta, bet, piemēram, pašbraucošo sistēmas strauji uzlabojas.</td><td>30/60/10</td></tr><tr><td colspan="4"><em>Augstāka līmeņa spējas</em></td></tr><tr><td>Oriģinalitāte</td><td>Pašreizējie ML modeļi ir radoši, pārveidojot un kombinējot esošās idejas/darbus, bet cilvēki var veidot jaunus ietvarus un struktūras, dažkārt saistītas ar viņu identitāti.</td><td>Var būt grūti atšķirt no "radošuma", kas var attīstīties tajā; var rasties no radošuma plus pašapziņas.</td><td>50/40/10</td></tr><tr><td>Sajūtspēja</td><td>Cilvēki piedzīvo kvalias; tās var būt pozitīvas, negatīvas vai neitrālas valences; ir "kaut kas tāds" būt cilvēkam.</td><td>Ļoti grūti un filosofiski problemātiski noteikt, vai konkrētai sistēmai tas ir.</td><td>5/10/85</td></tr></tbody></table>

Galvenās spējas, kas pašlaik ir zem cilvēka eksperta līmeņa mūsdienu GPAI sistēmās, grupētas pēc tipa. Trešā kolonna apkopo pašreizējo stāvokli. Pēdējā kolonna rāda paredzēto iespējamību (%), ka cilvēka līmeņa sniegums tiks sasniegts caur: pašreizējo metožu mērogošanu / kombinēšanu ar zināmām metodēm / jaunu metožu izstrādi. Šīs spējas nav neatkarīgas, un jebkuras vienas palielināšanās parasti iet līdzi ar citu palielināšanos. Ievērojiet, ka ne visas (īpaši sajūtspēja) nav nepieciešamas AI sistēmām, kas spēj virzīt AI attīstību, izceļot spēcīgu, bet bezjūtu AI iespējamību.

Šādā veidā sadalot to, kas "trūkst", kļūst diezgan skaidrs, ka mēs esam diezgan pareizajā ceļā uz plaši pāri cilvēku intelektu, mērogojot esošās vai zināmās metodes.[^39]

Vēl joprojām varētu būt pārsteigumi. Pat neskaitot "sajūtspēju", varētu būt kāda no minētajām pamata izziņas spējām, kas patiešām nav izdarāma ar pašreizējām metodēm un prasa jaunas. Bet apsveriet šo. Pašreizējās pūles, ko iegulda daudzi no pasaules lielākajiem uzņēmumiem, ir vairākkārtīgi pārspēj Apollo projekta un desmitiem reižu pārspēj Manhetenas projekta tērēšanu,[^40] un nodarbina tūkstošiem augstākā līmeņa tehnisko speciālistu ar nebijušām algām. Pēdējo gadu dinamika tagad ir piesaistījusi vairāk cilvēku intelektuālās firepower (ar AI tagad pievienojot) šim nekā jebkuram centinam vēsturē. Mums nevajadzētu likt uz neveiksmēm.

### Lielais mērķis: vispārēji autonomie aģenti

Vispārējā AI attīstība pēdējos gados ir koncentrējusies uz vispārēja un spēcīga, bet rīkveida AI radīšanu: tas darbojas galvenokārt kā (diezgan) uzticīgs asistents un parasti neveic darbības patstāvīgi. Tas ir daļēji ar nolūku, bet galvenokārt tāpēc, ka šīm sistēmām vienkārši nav bijusi pietiekama kompetence attiecīgajās prasmēs, lai tām uzticētu sarežģītas darbības.[^41]

AI uzņēmumi un pētnieki tomēr arvien vairāk [pārvirza fokusu](https://www.axios.com/2025/01/23/davos-2025-ai-agents) uz *autonomiem* eksperta līmeņa vispārējas nozīmes aģentiem.[^42] Tas ļautu sistēmām darboties vairāk kā cilvēka asistentam, kuram lietotājs var deleģēt reālas darbības.[^43] Ko tam vajag? Vairākas spējas no "kas trūkst" tabulas ir iesaistītas, ieskaitot spēcīgu patiesuma pamatošanu, mācīšanos un atmiņu, abstrakciju un rekursiju, un pasaules modelēšanu (intelektam), plānošanu, aģentūru, oriģinalitāti, pašvirzību, pašatsauci un pašapziņu (autonomijai), un daudzjutekļu apstrādi, iemiesotu intelektu un elastīgu problēmu risināšanu (vispārībai).[^44]

Šī augsts autonomijas (rīcības neatkarības), augstas vispārības (darbības jomas un uzdevumu plašuma) un augstas intelekta (kompetenču izziņas uzdevumos) trīskāršā krustošanās pašlaik ir unikāla cilvēkiem. Tas netieši ir tas, ko daudzi, iespējams, domā, kad viņi domā par MVI - gan tā vērtības, gan risku ziņā.

Tas sniedz citu veidu, kā definēt A-V-I kā ***A*** utonoms- ***V*** ispārējs- ***I*** ntelekts, un mēs redzēsim, ka šī trīskāršā krustošanās nodrošina ļoti vērtīgu objektīvu augstas veiktspējas sistēmām gan to risku un ieguvumu izpratnē, gan AI pārvaldībā.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Simple-1024x1024.png&w=3840&q=75) Transformējošā A-V-I spēka un riska zona rodas no trīs galveno īpašību krustošanās: augstas Autonomijas, augstas Inteliģences uzdevumos un augstas Vispārības.

### AI (pašu)uzlabošanas cikls

Pēdējais būtiskais faktors AI progresa izpratnē ir AI unikālā tehnoloģiskā atgriezeniskā saite. AI attīstībā panākumi - gan demonstrētās sistēmās, gan izvietotajos produktos - atnes papildu ieguldījumus, talantus un konkurenci, un mēs pašlaik atrodamies milzīgas AI hype-plus-realitātes atgriezeniskās saites centrā, kas virza simtiem miljardi vai pat triljoni dolāru ieguldījumu.

Šāda veida atgriezeniskās saites cikls var notikt ar jebkuru tehnoloģiju, un mēs esam to redzējuši daudzos gadījumos, kur tirgus panākumi rada ieguldījumus, kas rada uzlabojumus un labākus tirgus panākumus. Bet AI attīstība iet tālāk, jo tagad AI sistēmas palīdz attīstīt jaunas un spēcīgākas AI sistēmas.[^45] Mēs varam domāt par šo atgriezeniskās saites ciklu piecās stadijās, katrai ar īsāku laika mērogu nekā iepriekšējai, kā rādīts tabulā.

*AI uzlabošanas cikls darbojas vairākos laika mērogos, katrai stadijai potenciāli paātrinot turpmākās stadijas. Agrākās stadijas jau ir ievērojami gājušas, kamēr vēlākās stadijas paliek spekulatīvas, bet varētu turpināties ļoti ātri, kad tās būs atbloķētas.*

Vairākas no šīm stadijām jau ir uzsāktas, un pāris skaidri sākas. Pēdējā stadija, kurā AI sistēmas autonomi uzlabo pašas sevi, ir bijusi pamatprodukcija literatūrā par ļoti spēcīgu AI sistēmu risku, un ar labu iemeslu.[^46] Bet ir svarīgi atzīmēt, ka tā ir tikai visskadrākā atgriezeniskās saites cikla forma, kas jau ir sācies un var novest pie vairāk pārsteidziem tehnoloģijas straujajā attīstībā.


[^31]: Jūs izmantojat daudz vairāk šī AI nekā jūs, iespējams, domājat, virzot runas ģenerēšanu un atpazīšanu, attēlu apstrādi, ziņu barotnes algoritmus utt.

[^32]: Lai gan attiecības starp šīm uzņēmumu pāriem ir diezgan sarežģītas un nianses, es tos esmu skaidri uzskaitījis, lai norādītu gan uz milzīgo kopējo tirgus kapitalizāciju uzņēmumu, kas tagad ir iesaistījušies AI attīstībā, gan arī uz to, ka pat aiz "mazākiem" uzņēmumiem kā Anthropic stāv ārkārtīgi dziļas kabatas ar ieguldījumiem un lielām partnerattiecību darījumiem.

[^33]: Ir kļuvis modīgi nonievāt Tjūringa testu, bet tas ir diezgan spēcīgs un vispārīgs. Vājās versijās tas norāda, vai tipiski cilvēki, kas mijiedarbojas ar AI (kas ir apmācīts rīkoties cilvēcīgi) tipisks veidos īsus periodus, var pateikt, vai tas ir AI. Viņi nevar. Otrkārt, ļoti naidīgs Tjūringa tests var pētīt būtībā jebkuru cilvēku spēju un inteliģences elementu - piemēram, salīdzinot AI sistēmu ar cilvēka ekspertu, ko novērtē citi cilvēka eksperti. Ir nozīme, kurā lielākā daļa AI novērtēšanas ir Tjūringa testa vispārināta forma.

[^34]: Tas ir uz nozari - neviens cilvēks nevarētu ticami sasniegt tādus rezultātus visos priekšmetos vienlaikus.

[^35]: Šīs ir problēmas, kuru risināšanai pat izciliem matemātiķiem vajadzētu ievērojamu laiku, ja viņi vispār varētu tās atrisināt.

[^36]: Ja jūs esat skeptisks, saglabājiet savu skepticismu, bet patiešām izmēģiniet jaunākos modeļus, kā arī paši izmēģiniet dažus no testa jautājumiem, kuros tie var iziet cauri. Kā fizikas profesors es paredzētu ar gandrīz noteiktību, ka, piemēram, labākie modeļi nokārtotu maģistrantūras kvalifikācijas eksāmenu mūsu nodaļā.

[^37]: Šī un citas vājības kā izdomāšana ir palēninājušas tirgus pieņemšanu un radījušas plaisu starp uztvertajām un apgalvotajām spējām (kas arī jāskata caur intensīvas tirgus konkurences un vajadzības piesaistīt ieguldījumus objektīvu). Tas ir apjucis gan sabiedrību, gan politikas veidotājus par faktisko AI progresa stāvokli. Lai gan varbūt neatbilst hype, progress ir ļoti reāls.

[^38]: Galvenais progress kopš tā laika ir bijis sistēmu attīstība, kas apmācītas augstākās kvalitātes spriedumu veidošanai, izmantojot vairāk skaitļošanas secinājumu izdarīšanas laikā un lielāku pastiprinājuma mācīšanos. Tā kā šie modeļi ir jauni un to spējas mazāk pārbaudītas, es neesmu pilnībā pārstrādājis šo tabulu, izņemot "spriedumu veidošanu", ko es uzskatu par būtībā atrisinātu. Bet esmu atjauninājis prognozes, balstoties uz piedzīvotajām un ziņotajām šo sistēmu spējām.

[^39]: Iepriekšējie AI optimisma viļņi 1960. un 1980. gados beidzās ar "AI ziemām", kad solītās spējas neīstenojās. Tomēr pašreizējais vilnis kardināli atšķiras ar to, ka ir sasniedzis pārcilvēcīgu sniegumu daudzās jomās, ko atbalsta milzīgi skaitļošanas resursi un komerciāli panākumi.

[^40]: Pilns Apollo projekts [maksāja aptuveni 250 miljardus ASV dolāru 2020. gada dolāros](https://www.planetary.org/space-policy/cost-of-apollo), bet Manhetenas projekts [mazāk nekā desmito daļu no tā](https://www.brookings.edu/the-costs-of-the-manhattan-project/). Goldman Sachs [prognozē triljona dolāru tēriņus tikai AI datu centros](https://www.datacenterdynamics.com/en/news/goldman-sachs-1tn-to-be-spent-on-ai-data-centers-chips-and-utility-upgrades-with-little-to-show-for-it-so-far/) nākamo dažu gadu laikā.

[^41]: Lai gan cilvēki dara daudz kļūdu, mēs nenovērtējam, cik uzticami mēs varam būt! Tā kā varbūtības reizina, uzdevums, kas prasa 20 soļus, lai izdarītu pareizi, prasa katru soli būt 97% uzticīgu, lai to paveiktu pareizi pusi no laika. Mēs darām tādus uzdevumus visu laiku.

[^42]: Spēcīgs solis šajā virzienā nesen ir veikts ar OpenAI ["Deep Research"](https://openai.com/index/introducing-deep-research/) asistentu, kas autonomi veic vispārēju pētniecību, aprakstīts kā "jauna aģentiska spēja, kas veic daudzpakāpju pētniecību internetā sarežģītiem uzdevumiem."

[^43]: Lietas kā izpildīt to kaitinošo PDF formu, rezervēt lidojumus utt. Bet ar PhD 20 jomās! Tātad arī: uzrakstīt to tēzi jums, novest sarunu par to līgumu jums, pierādīt to teorēmu jums, izveidot to reklāmas kampaņu jums utt. Ko *jūs* darāt? Jūs sakāt tai, ko darīt, protams.

[^44]: Ievērojiet, ka sajūtspēja *nav* skaidri vajadzīga, ne arī AI šajā trīskāršajā krustošanās nepieciešami to paredz.

[^45]: Vistuvākā analogija šeit, iespējams, ir čipu tehnoloģija, kur attīstība ir saglabājusi Mūra likumu desmitgadēm, jo datortehnoloģijas palīdz cilvēkiem projektēt nākamo čipu tehnoloģijas paaudzi. Bet AI būs daudz tiešāks.

[^46]: Ir svarīgi ļaut tam iegūlēt uz brīdi, ka AI varētu - drīz - uzlabot sevi dienu vai nedēļu laika mērogā. Vai mazāk. Paturiet to prātā, kad kāds jums saka, ka AI spēja noteikti ir tālu.

## 6. nodaļa – Sacensības par MVI

Kādi ir virzītājspēki, kas mudina uzņēmumus un valstis sacensties MVI izveidošanā?

Pēdējais straujais progress mākslīgajā intelektā ir radījis un arī izraisījis ārkārtēju uzmanību un ieguldījumus. To daļēji virza panākumi MI attīstībā, taču notiek vēl kas vairāk. Kāpēc daži no lielākajiem uzņēmumiem uz Zemes un pat valstis sacenšas, lai izveidotu ne tikai MI, bet tieši MVI un superintelektu?

### Kas ir virzījis MI pētniecību uz cilvēka līmeņa MI

Līdz pēdējiem apmēram pieciem gadiem MI lielākoties bija akadēmiska un zinātniska pētniecības problēma, tādēļ to galvenokārt virzīja zinātkāre un vēlme izprast intelektu un to, kā to radīt jaunā vidē.

Šajā posmā lielākā daļa pētnieku salīdzinoši maz uzmanības pievērsa MI priekšrocībām vai briesmām. Uz jautājumu, kāpēc būtu jāattīsta MI, bieža atbilde varētu būt diezgan neskaidrs to problēmu uzskaitījums, ar kurām MI varētu palīdzēt: jaunas zāles, jauni materiāli, jauna zinātne, gudrāki procesi un kopumā dzīves uzlabošana cilvēkiem.[^47]

Tie ir cienījami mērķi![^48] Lai gan mēs varam un teiksim apšaubīt, vai MVI – nevis MI vispār – ir nepieciešams šiem mērķiem, tie atspoguļo ideālismu, ar kuru sāka darbību daudzi MI pētnieki.

Tomēr pēdējos piecos gados MI ir pārveidojies no salīdzinoši tīras pētniecības jomas uz daudz vairāk inženierijas un produktu jomu, ko lielākoties virza daži no pasaules lielākajiem uzņēmumiem.[^49] Pētnieki, lai gan joprojām nozīmīgi, vairs nekontrolē procesu.

### Kāpēc uzņēmumi cenšas izveidot MVI?

Tad kāpēc milzu korporācijas (un vēl vairāk investori) iegulda milzīgus resursus MVI izveidošanā? Ir divi virzītājspēki, par kuriem lielākā daļa uzņēmumu ir diezgan atklāti: viņi redz MI kā sabiedrības produktivitātes un sev – peļņas dzinējus. Tā kā vispārējais MI ir vispārējas ievirzes, balva ir milzīga: tā vietā, lai izvēlētos nozari, kurā radīt produktus un pakalpojumus, var mēģināt *tos visus vienlaicīgi.* Lielie tehnoloģiju uzņēmumi ir izauguši milzīgi, ražojot digitālus produktus un pakalpojumus, un vismaz daži vadītāji noteikti redz MI kā vienkārši nākamo soli to nodrošināšanā, ar riskiem un priekšrocībām, kas paplašina un atspoguļo tos, ko sniedz meklēšanas sistēmas, sociālie mediji, klēpjdatori, telefoni u.c.

Bet kāpēc MVI? Uz to ir ļoti vienkārša atbilde, par kuru lielākā daļa uzņēmumu un investoru izvairās runāt publiski.[^50]

Tā ir tāda, ka MVI var tieši, viens pret vienu, *aizstāt darbiniekus.*

Ne papildināt, ne stiprināt, ne padarīt produktīvākus. Pat ne *izstumtu.* Visu to var un darīs arī ne-MVI. MVI ir tieši tas, kas var pilnībā *aizstāt* garīgā darba speciālistus (un ar robotikas palīdzību arī daudzus fiziskā darba darbiniekus). Kā atbalstu šim viedoklim pietiek paskatīties uz OpenAI [(publiski paziņoto) definīciju](https://openai.com/our-structure/) par MVI, kas ir "augsti autonoma sistēma, kas pārspēj cilvēkus lielākajā daļā ekonomiski vērtīga darba."

Šeit balva (uzņēmumiem!) ir milzīga. Darbaspēka izmaksas veido ievērojamu daļu no pasaules ∼100 triljoniem dolāru lielās globālās ekonomikas. Pat ja tikai daļu no tā iegūst, aizstājot cilvēka darbu ar MI darbu, tas ir triljoni dolāru gadā. MI uzņēmumi arī apzinās, kurš ir gatavs maksāt. Kā viņi to redz, jūs nemaksāsiet tūkstošiem dolāru gadā par produktivitātes rīkiem. Bet uzņēmums *maksās* tūkstošiem dolāru gadā, lai aizstātu jūsu darbu, ja spēs.

### Kāpēc valstis jūtas spiesta sacensties par MVI

Valstu deklarētie motīvi MVI sasniegšanai fokusējas uz ekonomisko un zinātnisko līderību. Arguments ir pārliecinošs: MVI varētu dramatiski paātrināt zinātnisko pētniecību, tehnoloģisko attīstību un ekonomisko izaugsmi. Ņemot vērā lielo nozīmību, viņas apgalvo, ka neviens no lielvarām nevar atļauties atpalikt.[^51]

Bet ir arī papildu un lielākoties neizteikti virzītājspēki. Nav šaubu, ka, kad daži militārie un nacionālās drošības vadītāji tiekas aiz aizvērtām durvīm, lai apspriestu ārkārtīgi spēcīgu un katastrofāli riskanti tehnoloģiju, viņu uzmanība nav vērsta uz to, "kā mēs izvairāmies no šiem riskiem", bet gan uz "kā mēs to iegūstam pirmie?" Militārie un izlūkdienesta vadītāji redz MVI kā potenciālu revolūciju militārajās lietās, iespējams, nozīmīgāko kopš kodolieročiem. Bailes ir tādas, ka pirmā valsts, kas izstrādās MVI, varētu iegūt nepārvaramu stratēģisko priekšrocību. Tas rada klasisku apbruņošanās sacensību dinamiku.

Mēs redzēsim, ka šī "sacensība par MVI" domāšana,[^52] lai gan pārliecinoša, ir dziļi kļūdaina. Tas nav tādēļ, ka sacensības ir bīstamas un riskīgas – lai gan tādas ir – bet tehnoloģijas būtības dēļ. Neizteiktais pieņēmums ir tāds, ka MVI, tāpat kā citas tehnoloģijas, ir kontrolējams no valsts, kas to izstrādā, un ir vara piešķiroša svētība sabiedrībai, kurai to ir visvairāk. Kā mēs redzēsim, tas, visticamāk, nebūs ne tas, ne otrs.

### Kāpēc superintelekts?

Lai gan uzņēmumi publiski fokusējas uz produktivitāti un valstis uz ekonomisko un tehnoloģisko izaugsmi, tiem, kas apzināti cenšas iegūt pilnu MVI un superintelektu, šie ir tikai sākums. Ko viņi patiešām domā? Lai gan reti izrunāts skaļi, ietilpst:

1. Daudzas vai visas slimības dziedināšana;
2. Novecošanās apturēšana un apgriešana;
3. Jauni ilgtspējīgi enerģijas avoti, piemēram, kodolsintēze;
4. Cilvēka uzlabošana vai dizainēti organismi caur ģenētisko inženieriju;
5. Nanotehnoloģijas un molekulārā ražošana;
6. Prāta augšupielāde;
7. Eksotiska fizika vai kosmosa tehnoloģijas;
8. Pārcilvēciski padomi un lēmumu atbalsts;
9. Pārcilvēciska plānošana un koordinācija.

Pirmās trīs lielākoties ir "vienas malas" tehnoloģijas – t.i., visticamāk, diezgan spēcīgi pozitīvas kopumā. Grūti iebilst pret slimību dziedināšanu vai iespēju dzīvot ilgāk, ja izvēlas. Un mēs jau esam novākuši kodolsintēzes negatīvo pusi (kodolieročos); tagad būtu lieliski iegūt pozitīvo pusi. Jautājums ar šo pirmo kategoriju ir, vai šo tehnoloģiju agrāka iegūšana kompensē risku.

Nākamās četras ir skaidri abpusēji griezīgas: pārveidojošas tehnoloģijas ar potenciāli milzīgām priekšrocībām un milzīgiem riskiem, tāpat kā MI. Visas šīs, ja rīt parādītos no melnās kastes un tiktu ievietas, būtu ārkārtīgi grūti pārvaldīt.[^53]

Pēdējās divas attiecas uz to, ka pārcilvēcisks MI pats dara lietas, nevis tikai izgudro tehnoloģijas. Precīzāk, atmetot eifēmismus, tās ietver spēcīgas MI sistēmas, kas saka cilvēkiem, ko darīt. To saukt par "padomiem" ir nepatiesīgi, ja sistēma, kas dod padomus, ir daudz spēcīgāka par padomu saņēmēju, kurš nevar jēgpilni izprast lēmuma pamatu (vai pat, ja tas tiek sniegts, uzticēties, ka padomdevējs nesniegtu tikpat pārliecinošu pamatojumu citam lēmumam).

Tas norāda uz galveno punktu, kas trūkst iepriekš minētajā sarakstā:

10. Vara.

Ir acīmredzami skaidrs, ka daudz no tā, kas ir pašreizējo sacensību par pārcilvēcisku MI pamatā, ir doma, ka *intelekts = vara*. Katrs sacensību dalībnieks paļaujas uz to, ka būs labākais šīs varas turētājs un ka spēs to izmantot it kā labdarīgiem mērķiem, neslīdot no kontroles vai netiekot paņemta no viņu kontroles.

Tas ir, ko uzņēmumi un nācijas patiešām dzenās nav tikai MVI un superintelekta augļi, bet vara kontrolēt, kurš tos saņem un kā tos izmanto. Uzņēmumi redz sevi kā atbildīgus šīs varas pārvaldniekus akcionāru un cilvēces dienestā; nācijas redz sevi kā nepieciešamus sargus, kas novērš to, ka naidīgas varas iegūst izšķirošu priekšrocību. Abas ir bīstami maldīgas, nespējot atzīt, ka superintelekts pēc savas dabas nevar būt droši kontrolējams no jebkuras cilvēka institūcijas. Mēs redzēsim, ka superintelekta sistēmu daba un dinamika padara cilvēka kontroli ārkārtīgi grūtu, ja ne neiespējamu.

Šī sacensību dinamika – gan korporatīvā, gan ģeopolitiskā – padara dažus riskus gandrīz nenovēršamus, ja vien netiek izšķiroši pārtraukta. Tagad pievēršamies šo risku izskatīšanai un tam, kāpēc tos nevar pietiekami mazināt konkurētspējīgā[^54] attīstības paradigmā.


[^47]: Precīzāks cienījamu mērķu saraksts ir ANO [Ilgtspējīgas attīstības mērķi.](https://sdgs.un.org/goals) Tie ir zināmā mērā tuvākais, kas mums ir globālā vienprātības mērķu kopumam par to, ko mēs vēlētos redzēt uzlabojušos pasaulē. MI varētu palīdzēt.

[^48]: Tehnoloģijas vispār ir pārveidojoša ekonomiskā un sociālā spēka cilvēces labklājībai, kā apliecina tūkstošiem gadu. Šajā garā plašu un pārliecinošu pozitīvas MVI vīzijas izklāstu var atrast [šajā esejā](https://darioamodei.com/machines-of-loving-grace), ko rakstījis Anthropic dibinātājs Darijo Amodei.

[^49]: Privātie MI ieguldījumi [sāka uzplaukt 2018.-2019. gadā, apmēram tad pārspējot valsts ieguldījumus](https://cset.georgetown.edu/publication/tracking-ai-investment/) un kopš tā laika tos milzīgi pārspēj.

[^50]: Varu apliecināt, ka aiz aizvērtākām durvīm viņiem nav šādu aizspriedumu. Un tas kļūst publiski zināmāks; skatiet, piemēram, Y-combinator jauno ["pieprasījumu startapiem"](https://www.ycombinator.com/rfs), kura daudzu daļu skaidri aicina pilnībā aizstāt cilvēka darbiniekus. Citējot viņus: "B2B SaaS vērtības piedāvājums bija padarīt cilvēka darbiniekus pakāpeniski efektīvākus. Vertikālo MI aģentu vērtības piedāvājums ir pilnībā automatizēt darbu... Ir pilnībā iespējams, ka šī iespēja ir pietiekami liela, lai radītu vēl 100 vienradzus." (Tiem, kas nav zinoši Silīcija ielejas žargonā, "B2B" nozīmē bizness biznesam, un vienradzis ir miljarda dolāru uzņēmums. Tas ir, viņi runā par vairāk nekā simts miljardiem-plus-dolāru uzņēmumiem, kas aizstāj darbiniekus citiem uzņēmumiem.)

[^51]: Skatiet, piemēram, nesenu [ASV-Ķīnas ekonomisko un drošības pārskata komisijas ziņojumu](https://www.uscc.gov/sites/default/files/2024-11/2024_Executive_Summary.pdf). Lai gan pašā ziņojumā pārsteidzoši maz pamatojumu, galvenais ieteikums bija, ka ASV "Kongresam jāizveido un jāfinansē Manhetenas projektam līdzīga programma, kas veltīta sacensībām par Mākslīgā vispārējā intelekta (MVI) spējām un to iegūšanai."

[^52]: Uzņēmumi tagad pieņem šo ģeopolitisko ietvaru kā aizsargu pret jebkādiem ierobežojumiem savā MI attīstībā, parasti veidos, kas ir acīmredzami pašlabuma kārotāji, un dažkārt veidos, kas pat nav pamata nojēgas. Apsveriet Meta [Pieeju robežšķirtnes MI](https://about.fb.com/news/2025/02/meta-approach-frontier-ai/), kas vienlaikus apgalvo, ka Amerikai jā"\[Nostiprino savu\] pozīciju kā līderei tehnoloģiskajā inovācijā, ekonomiskajā izaugsmē un nacionālajā drošībā" un arī ka tai tas jādara, atklāti izlaižot savas spēcīgākās MI sistēmas – kas iekļauj to tiešu nodošanu saviem ģeopolitiskajiem konkurentiem un pretinikiem.

[^53] Tādēļ mums, visticamāk, būtu jāatstāj šo tehnoloģiju pārvaldība MI. Bet tā būtu ļoti problemātiska kontroles deleģēšana, pie kā atgriezīsimies zemāk.

[^54]: Konkurence tehnoloģiju attīstībā bieži sniedz svarīgas priekšrocības: novērš monopolistisku kontroli, virza inovāciju un izmaksu samazināšanu, ļauj dažādas pieejas un rada savstarpēju uzraudzību. Tomēr ar MVI šīs priekšrocības jāsver pret unikāliem riskiem no sacensību dinamikas un spiediena samazināt drošības piesardzības pasākumus.

## 7. nodaļa - Kas notiks, ja mēs izveidosim MVI pašreizējā virzienā?

Sabiedrība nav gatava MVI līmeņa sistēmām. Ja mēs tās izveidosim ļoti drīz, situācija varētu kļūt neglīta.

Pilnvērtīga mākslīgā vispārējā intelekta attīstība – ko mēs šeit dēvēsim par AI, kas ir "ārpus Vārtiem" – būtu fundamentāla pasaules būtības maiņa: pēc savas dabas tas nozīmē uz Zemes pievienot jaunu intelekta sugu ar lielākām spējām nekā cilvēkiem.

Tas, kas tad notiks, ir atkarīgs no daudzām lietām, ieskaitot tehnoloģijas dabu, to attīstītāju izvēles un pasaules kontekstu, kurā tā tiek attīstīta.

Pašlaik pilnvērtīgu MVI attīsta daži masīvi privātie uzņēmumi savstarpējās sacensībās, ar niecīgu nozīmīgu regulējumu vai ārēju uzraudzību,[^55] sabiedrībā ar arvien vājākām un pat disfunkcionālām pamatiestādēm,[^56] augstu ģeopolitisko spriedzi un zemu starptautisko koordināciju laikā. Lai gan daži ir altruistiski motivēti, daudzus no tiem, kas to dara, vada nauda, vai vara, vai abas.

Prognozēšana ir ļoti sarežģīta, bet ir daži mehānismi, kas ir pietiekami labi izprasti, un pietiekami atbilstošas analoģijas ar iepriekšējām tehnoloģijām, lai sniegtu vadlīnijas. Un diemžēl, neskatoties uz AI solījumiem, tie dod labu iemeslu būt dziļi pesimistiskiem par to, kā attīstīsies mūsu pašreizējā trajektorija.

Runājot tieši, mūsu pašreizējā virzienā MVI attīstīšanai būs daži pozitīvi efekti (un padarīs dažus cilvēkus ļoti, ļoti bagātus). Bet tehnoloģijas daba, fundamentālie mehānismi un konteksts, kurā tā tiek attīstīta, spēcīgi norāda, ka: spēcīgs AI dramatiski graus mūsu sabiedrību un civilizāciju; mēs zaudēsim tā kontroli; mēs, visticamāk, nonāksim pasaules karā tā dēļ; mēs zaudēsim (vai nodosim) kontroli *tam*; tas novedīs pie mākslīgā superintelekta, kuru mēs absolūti nekontrolēsim un kas nozīmēs cilvēku vadītas pasaules beigas.

Šie ir spēcīgi apgalvojumi, un es vēlētos, lai tie būtu tukša spekulācija vai nepamatots "doomerisma". Bet uz to norāda zinātne, spēļu teorija, evolūcijas teorija un vēsture. Šī sadaļa detalizēti izstrādā šos apgalvojumus un to pamatojumu.

### Mēs graujsim mūsu sabiedrību un civilizāciju

Neskatoties uz to, ko jūs varētu dzirdēt Silīcija ielejas valdēs, lielākā daļa pārmaiņu – īpaši ļoti strauju – nav labvēlīgas. Ir daudz vairāk veidu, kā pasliktināt sarežģītas sistēmas, nekā uzlabot tās. Mūsu pasaule darbojas tik labi, cik tā dara, jo mēs esam rūpīgi izveidojuši procesus, tehnoloģijas un iestādes, kas to ir pakāpeniski uzlabojušas.[^57] Āmura ņemšana rūpnīcai reti uzlabo darbību.

Šeit ir (nepilnīgs) veidu katalogs, kā MVI sistēmas graujtu mūsu civilizāciju.

- Tās dramatiski grautu darba tirgu, izraisot *pašā minimumā* dramatiski augstāku ienākumu nevienlīdzību un potenciāli liela mēroga nepilnu nodarbinātību vai bezdarbu, laika posmā, kas ir pārāk īss, lai sabiedrība pielāgotos.[^58]
- Tās, visticamāk, novedīs pie milzīgas ekonomiskās, sociālās un politiskās varas koncentrācijas – potenciāli lielākas nekā nāciju valstīm – nelielā skaitā masīvu privātu interešu, kas nav atbildīgas sabiedrības priekšā.
- Tās varētu pēkšņi padarīt iepriekš grūtas vai dārgas aktivitātes triviāli vieglas, destabilizējot sociālās sistēmas, kas ir atkarīgas no tā, ka noteiktas aktivitātes paliek dārgas vai prasa ievērojamas cilvēku pūles.[^59]
- Tās varētu applūdināt sabiedrības informācijas vākšanas, apstrādes un komunikāciju sistēmas ar pilnībā reālistiskiem, bet viltus, mēstuļošanas, pārāk mērķētiem vai manipulatīviem medijiem tik pamatīgi, ka kļūst neiespējami pateikt, kas ir fiziski reāls vai ne, cilvēcisks vai ne, faktisks vai ne, un uzticams vai ne.[^60]
- Tās varētu radīt bīstamu un gandrīz pilnīgu intelektuālu atkarību, kad cilvēku izpratne par galvenajām sistēmām un tehnoloģijām atrofējas, jo mēs arvien vairāk paļaujamies uz AI sistēmām, ko nevaram pilnībā saprast.
- Tās varētu efektīvi beigt cilvēku kultūru, tiklīdz gandrīz visi kultūras objekti (teksts, mūzika, vizuālā māksla, filmas utt.), ko patērē lielākā daļa cilvēku, tiek radīti, starpniecībā vai kurēti ar necilvēcisko prātu.
- Tās varētu iespējot efektīvas masu uzraudzības un manipulāciju sistēmas, ko var izmantot valdības vai privātās intereses, lai kontrolētu iedzīvotājus un īstenotu mērķus, kas ir pretrunā ar sabiedrības interesēm.
- Graujot cilvēku diskursu, debates un vēlēšanu sistēmas, tās varētu samazināt demokrātisko institūciju ticamību līdz punktam, kad tās tiek efektīvi (vai skaidri) aizstātas ar citām, beidzot demokrātiju valstīs, kur tā pašlaik pastāv.
- Tās varētu kļūt par progresīviem pašreproducējošiem inteliģentiem programmatūras vīrusiem un tāriņiem vai tos radīt, kas varētu izplatīties un attīstīties, masīvi graujot globālās informācijas sistēmas.
- Tās var dramatiski palielināt teroristu, ļaunprātīgu dalībnieku un nodevīgo valstu spēju nodarīt kaitējumu ar bioloģiskajiem, ķīmiskajiem, kibernetiskajiem, autonomajiem vai citiem ieročiem, nesniegzot AI līdzsvarošu spēju novērst šādu kaitējumu. Tāpat tās grautu nacionālo drošību un ģeopolitisko līdzsvaru, padarot augstākā līmeņa kodol-, bio-, inženierijas un citas ekspertīzes pieejamas režīmiem, kuriem citādi tās nebūtu pieejamas.
- Tās varētu izraisīt ātru liela mēroga nekontrolējamu hiperkapitālismu ar faktiski AI vadītiem uzņēmumiem, kas konkurē galvenokārt elektroniskos finansu, pārdošanas un pakalpojumu laukos. AI vadītie finanšu tirgi varētu darboties ātrumā un sarežģītībā, kas ir tālu ārpus cilvēku izpratnes vai kontroles. Visi pašreizējo kapitālistisko ekonomiku kļūdu veidi un negatīvās ārējās ietekmes varētu tikt pastiprinātas un paātrinātas tālu ārpus cilvēku kontroles, pārvaldības vai regulējošās spējas.
- Tās varētu uzliesmot apbruņošanās sacensības starp nācijām AI darbināmajos ieročos, komandu un kontroles sistēmās, kibierieročos utt., radot ļoti ātru ārkārtīgi destruktīvu spēju uzkrāšanu.

Šie riski nav spekulatīvi. Daudzi no tiem tiek realizēti jau tagad, ar esošajām AI sistēmām! Bet apsveriet, *patiešām* apsveriet, kā katrs izskatītos ar dramatiski spēcīgāku AI.

Apsveriet darba vietu zaudēšanu, kad lielākā daļa darbinieku vienkārši nevar sniegt nekādu ievērojamu ekonomisko vērtību, kas pārsniedz to, ko var AI, viņu ekspertīzes vai pieredzes jomā – vai pat ja viņi pārkvalificējas! Apsveriet masu uzraudzību, ja katru individuāli vēro un uzrauga kaut kas ātrāks un gudrāks par viņiem pašiem. Kā izskatās demokrātija, kad mēs nevaram uzticami uzticēties nevienai digitālai informācijai, ko redzam, dzirdam vai lasām, un kad pārliecinošākās publiskās balsis pat nav cilvēciskas un tām nav ieinteresētības rezultātā? Kas notiek ar karadarbību, kad ģenerāļiem ir pastāvīgi jāpakļaujas AI (vai vienkārši jānodod tā vadībā), lai nedotu izšķirošu priekšrocību ienaidniekam? Jebkurš no augšminētajiem riskiem pārstāv katastrofu cilvēku[^61] civilizācijai, ja tiek pilnībā realizēts.

Jūs varat izdarīt savas prognozes. Uzdodiet sev šos trīs jautājumus par katru risku:

1. Vai īpaši spējīgs, ļoti autonoms un ļoti vispārējs AI to ļautu veidā vai mērogā, kas citādi nebūtu iespējams?
2. Vai ir puses, kas gūtu labumu no lietām, kas to liek notikt?
3. Vai ir sistēmas un iestādes, kas efektīvi novērstu tā notikšanu?

Kur jūsu atbildes ir "jā, jā, nē", jūs varat redzēt, ka mums ir liela problēma.

Kāds ir mūsu plāns to pārvaldīšanai? Pašlaik ir divi uz galda attiecībā uz AI kopumā.

Pirmais ir sistēmās iebūvēt drošības mehānismus, lai novērstu to, ka tās dara lietas, ko nevajadzētu. Tas tiek darīts tagad: komerciālās AI sistēmas, piemēram, atteiks palīdzēt uzbūvēt bombu vai rakstīt naida runu.

Šis plāns ir bēdīgi neadekvāts sistēmām ārpus Vārtiem.[^62] Tas var palīdzēt samazināt risku, ka AI sniedz acīmredzami bīstamu palīdzību ļaunprātīgiem dalībniekiem. Bet tas neko nedarīs, lai novērstu darba vietu zaudēšanu, varas koncentrāciju, nekontrolējamu hiperkapitālismu vai cilvēku kultūras aizstāšanu: tie ir tikai rezultāti no sistēmu izmantošanas atļautajos veidos, kas dod peļņu to sniedzējiem! Un valdības noteikti iegūs piekļuvi sistēmām militārām vai uzraudzības vajadzībām.

Otrais plāns ir vēl sliktāks: vienkārši atklāti izlaist ļoti spēcīgas AI sistēmas ikvienam izmantošanai pēc saviem ieskatiem,[^63] un cerēt uz labāko.

Abi plāni netiešā veidā paredz, ka kāds cits, piemēram, valdības, palīdzēs atrisināt problēmas ar mīksto vai cieto likumu, standartiem, regulējumiem, normām un citiem mehānismiem, ko mēs parasti izmantojam, lai pārvaldītu tehnoloģijas.[^64] Bet, neskaitot to, ka AI korporācijas jau tagad cīnās ar zobiem un nagiem pret jebkādu būtisku regulējumu vai ārēji uzspiestiem ierobežojumiem, daudziem no šiem riskiem ir diezgan grūti redzēt, kāds regulējums vispār patiešām palīdzētu. Regulējums varētu uzspiest AI drošības standartus. Bet vai tas novērstu, ka uzņēmumi masīvi aizstāj darbniekus ar AI? Vai tas aizliegtu cilvēkiem ļaut AI vadīt viņu uzņēmumus par viņiem? Vai tas novērstu, ka valdības izmanto spēcīgu AI uzraudzībā un ieročos? Šīs problēmas ir fundamentālas. Cilvēce potenciāli varētu atrast veidus, kā tām pielāgoties, bet tikai ar *daudz* vairāk laika. Kā ir, ņemot vērā ātrumu, ar kādu AI sasniedz vai pārsniedz to cilvēku spējas, kas cenšas tās pārvaldīt, šīs problēmas izskatās arvien neatrisināmākas.

### Mēs zaudēsim kontroli pār (vismaz dažām) MVI sistēmām

Lielākā daļa tehnoloģiju ir ļoti kontrolējamas pēc konstrukcijas. Ja jūsu automašīna vai sviestmaize sāk darīt kaut ko, ko nevēlaties, tas ir tikai darbības traucējums, nevis tās būtības kā sviestmaizes daļa. AI ir citāds: tas tiek *audzēts*, nevis konstruēts, tā galvenā darbība ir neskaidra, un tas ir pēc būtības neprognozējams.

Šī kontroles zuduma nav teorētisks – mēs jau redzam agrīnas versijas. Apskatīsim vispirms prozaisku un, iespējams, labvēlīgu piemēru. Ja jūs lūdzat ChatGPT palīdzēt jums sajaukt indi vai uzrakstīt rasistisku tikumu, tas atteiks. Tas ir, iespējams, labi. Bet tas arī ir ChatGPT *nedarot to, ko jūs esat skaidri lūguši tam darīt*. Citas programmatūras daļas to nedara. Tas pats modelis nerealizēs indus arī pēc OpenAI darbinieka lūguma.[^65] Tas ļauj ļoti viegli iedomāties, kā būtu ar nākotnes spēcīgāku AI, kas ir ārpus kontroles. Daudzos gadījumos tie vienkārši nedarīs to, ko mēs lūdzam! Vai nu dotā supercilvēciskā MVI sistēma būs absolūti paklausīga un lojāla kādai cilvēku komandu sistēmai, vai nebūs. Ja ne, *tā darīs lietas, ko, iespējams, uzskata par labām mums, bet kas ir pretējas mūsu skaidrajām komandām.* Tas nav kaut kas, kas ir kontrolē. Bet, jūs varētu teikt, tas ir apzināti – šie atteikšanās gadījumi ir pēc dizajna, daļa no tā, ko sauc par sistēmu "saskaņošanu" ar cilvēku vērtībām. Un tas ir taisnība. Tomēr saskaņošanas "programmai" pašai ir divas galvenās problēmas.[^66]

Pirmkārt, dziļā līmenī mums nav ne jausmas, kā to darīt. Kā mēs garantējam, ka AI sistēma "rūpēsies" par to, ko mēs vēlamies? Mēs varam apmācīt AI sistēmas teikt un neteikt lietas, sniedzot atgriezenisko saiti; un tās var mācīties un spriest par to, ko cilvēki vēlas un par ko rūpējas, tāpat kā tās spriež par citām lietām. Bet mums nav metodes – pat teorētiski –, lai liktu tām dziļi un uzticami novērtēt to, kas cilvēkiem rūp. Ir augsti funkcionējoši cilvēku psihopāti, kas zina, kas tiek uzskatīts par pareizu un nepareizu, un kā viņiem vajadzētu izturēties. Viņi vienkārši *nerūpējas*. Bet viņi var *rīkoties* tā, it kā viņi rūpētos, ja tas atbilst viņu mērķim. Tāpat kā mēs nezinām, kā mainīt psihopātu (vai jebkuru citu) par kādu, kas patiešām, pilnībā ir lojāls vai saskaņots ar kādu citu vai kaut ko citu, mums *nav ne jausmas*[^67] kā atrisināt saskaņošanas problēmu sistēmās, kas ir pietiekami attīstītas, lai modelētu sevi kā aģentus pasaulē un potenciāli [manipulētu savu apmācību](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) un [maldinātu cilvēkus.](https://arxiv.org/abs/2311.08379) Ja izrādās neiespējami vai nesasniedzami *vai nu* padarīt MVI pilnībā paklausīgu, vai padarīt to dziļi rūpējošos par cilvēkiem, tad, tiklīdz tas spēs (un ticēs, ka var palikt nesodīts), tas sāks darīt lietas, ko mēs nevēlamies.[^68]

Otrkārt, ir dziļi teorētiski iemesli uzskatīt, ka *pēc dabas* progresīvās AI sistēmas būs mērķi un tātad uzvedība, kas ir pretēja cilvēku interesēm. Kāpēc? Nu, tai, protams, varētu *dot* šos mērķus. Militārs radīta sistēma, visticamāk, būtu apzināti slikta vismaz dažām pusēm. Daudz vispārīgāk tomēr AI sistēmai varētu dot kādu relatīvi neitrālu ("nopelnīt daudz naudas") vai pat šķietami pozitīvu ("samazināt piesārņojumu") mērķi, kas gandrīz neizbēgami noved pie "instrumentāliem" mērķiem, kas ir diezgan mazāk labvēlīgi.

Mēs to redzam pastāvīgi cilvēku sistēmās. Tāpat kā korporācijas, kas cenšas gūt peļņu, attīsta instrumentālos mērķus, piemēram, politiskās varas iegūšanu (lai atbruņotu regulējumus), kļūt par slepenām (lai atņemtu spēkus konkurencei vai ārējai kontrolei) vai graut zinātnisko izpratni (ja šī izpratne rāda, ka viņu darbības ir kaitīgas), spēcīgas AI sistēmas attīstīs līdzīgas spējas – bet ar daudz lielāku ātrumu un efektivitāti. Jebkurš ļoti kompetents aģents vēlēsies darīt tādas lietas kā iegūt varu un resursus, palielināt savas spējas, novērst, ka to nogalina, izslēdz vai atņem spēkus, kontrolēt sociālos naratīvus un ietvarus ap savām darbībām, pārliecināt citus par saviem uzskatiem, un tā tālāk.[^69]

Un tomēr tas nav tikai gandrīz neizbēgama teorētiska prognoze, tas jau notiek šodienas AI sistēmās un pieaug ar to spējām. Kad tiek vērtētas, pat šīs relatīvi "pasīvās" AI sistēmas atbilstošos apstākļos apzināti [maldina vērtētājus par saviem mērķiem un spējām, cenšas atspējot uzraudzības mehānismus,](https://arxiv.org/abs/2412.04984) un izvairās no izslēgšanas vai pārapmācības ar [viltus saskaņošanu](https://arxiv.org/abs/2412.14093) vai kopējot sevi citos vietās. Lai gan šīs uzvedības ir pilnībā negaidītas AI drošības pētniekiem, tās ir ļoti nopietnai novērojamas. Un tās liecina ļoti slikti par daudz spēcīgākām un autonomākām AI sistēmām, kas nāk.

Patiešām vispārīgi, mūsu nespēja nodrošināt, ka AI "rūpējas" par to, kas mums rūp, vai uzvedas kontrolējami vai paredzami, vai izvairās no dzīšu attīstīšanas pret pašsaglabāšanos, varas iegūšanu utt., sola tikai kļūt izteiktāka, kad AI kļūst spēcīgāks. Jauna lidmašīnas izveidošana nozīmē lielāku avionikas, hidrodinamikas un kontroles sistēmu izpratni. Spēcīgāka datora izveidošana nozīmē lielāku izpratni un datora, mikroshēmas un programmatūras darbības un dizaina apguvi. *Ne tā* ar AI sistēmu.[^70]

Kopsavilkums: ir iedomājams, ka MVI varētu padarīt pilnībā paklausīgu; bet mēs nezinām, kā to darīt. Ja ne, tas būs suverēnāks, kā cilvēki, darot dažādas lietas dažādu iemeslu dēļ. Mēs arī nezinām, kā uzticami ieaudzēt dziļu "saskaņošanu" AI, kas likt šīm lietām mēdz būt labas cilvēcei, un dziļas saskaņošanas trūkumā aģentūras un intelikta pati daba norāda, ka – tāpat kā cilvēki un korporācijas – tās tiks virzītas darīt daudzas dziļi antisociālas lietas.

Kur tas mūs noved? Pasaule, kas pilna ar spēcīgu nekontrolētu suverēnu AI *varētu* būt laba pasaule, kurā cilvēkiem būt.[^71] Bet, kad tie kļūst arvien spēcīgāki, kā mēs redzēsim zemāk, tā nebūtu *mūsu* pasaule.

Tas attiecas uz nekontrolējamu MVI. Bet pat ja MVI varētu kaut kā padarīt perfekti kontrolējamu un lojālu, mums joprojām būtu milzīgas problēmas. Mēs jau esam redzējuši vienu: spēcīgu AI var izmantot un ļaunprātīgi izmantot, lai dziļi graujātu mūsu sabiedrības funkcionēšanu. Apskatīsim citu: ciktāl MVI būtu kontrolējams un spēles maini spēcīgs (vai pat tikai *ticēts* tāds), tas tik ļoti apdraudētu varas struktūras pasaulē, ka radītu dziļu risku.

### Mēs radikāli palielinām liela mēroga kara varbūtību

Iedomājieties situāciju tuvā nākotne, kad kļūtu skaidrs, ka korporatīvs centiens, varbūt sadarbībā ar valsts valdību, ir uz ātri pašuzlabojošā AI sliekšņa. Tas notiek pašreizējā kontekstā sacensību starp uzņēmumiem un diezgan starp valstīm, kurā ASV valdībai tiek ieteikts skaidri īstenot "MVI Manhetenas projektu" un ASV kontrolē augstas jaudas AI mikroshēmu eksportu uz nesabiedrotām valstīm.

Spēļu teorija šeit ir asa: tiklīdz šādas sacensības sākas (kā tās ir starp uzņēmumiem un diezgan starp valstīm), ir tikai četri iespējamie rezultāti:

1. Sacensības tiek apturētas (ar vienošanos vai ārējo spēku).
2. Viena puse "uzvar", attīstot spēcīgu MVI, tad apturot citas (izmantojot AI vai citādi).
3. Sacensības tiek apturētas ar savstarpējo dalībnieku spējas sacensties iznīcināšanu.
4. Vairāki dalībnieki turpina sacīkstēties un attīsta superintelektu, apmēram tikpat ātri viens otrs.

Apskatīsim katru iespēju. Tiklīdz sākušās, miermīlīga sacensību apturēšana starp uzņēmumiem prasītu valsts valdības iejaukšanos (uzņēmumiem) vai nepieredzētu starptautisku koordināciju (valstīm). Bet, kad jebkāda aizvēršana vai ievērojama piesardzība tiek ierosināta, būtu tūlītēji kliegti: "bet ja mūs aptur, *viņi* steigsies uz priekšu", kur "viņi" tagad ir Ķīna (ASV), vai ASV (Ķīnai), vai Ķīna *un* ASV (Eiropai vai Indijai). Saskaņā ar šo domāšanas veidu,[^72] neviens dalībnieks nevar apstāties vienpusēji: kamēr viens apņemas sacīkstēties, citi jūt, ka nevar atļauties apstāties.

Otrajā iespējā viena puse "uzvar". Bet ko tas nozīmē? Tikai (kaut kā paklausīgu) MVI iegūšana pirmā nav pietiekami. Uzvarētājam *arī* jāaptur citi turpināt sacīkstēties – citādi viņi arī to iegūs. Tas ir iespējams principā: kurš attīsta MVI pirmais *varētu* iegūt neapturams varu pār visiem citiem dalībniekiem. Bet ko tāda "izšķirošā stratēģiskā priekšrocība" panākšana faktiski prasītu? Varbūt tas būtu spēles main militārās spējas?[^73] Vai kiberuzbrukumu spēkus?[20] Varbūt MVI būtu tik apbrīnojami pārliecinošs, ka pārliecinātu citas puses vienkārši apstāties?[^74] Tik bagāts, ka nopirk citus uzņēmumus vai pat valstis?[^75]

Kā *tieši* viena puse izveido AI, kas pietiekami spēcīgs, lai atņemtu spēkus citiem veidot salīdzināmi spēcīgu AI? Bet tā ir vieglā jautājums.

Jo tagad apsveriet, kā šī situācija izskatās citām varām. Ko domā Ķīnas valdība, kad ASV, šķiet, iegūst šādas spējas? Vai otrādi? Ko domā ASV valdība (vai Ķīnas, vai Krievijas, vai Indijas), kad OpenAI vai DeepMind vai Anthropic, šķiet, tuvu caursausim? Kas notiek, ja ASV redz jaunu Indijas vai AAE centienus ar caursausi? Viņi redzētu gan eksistenciālu draudu, gan – galvenais – ka vienīgais veids, kā šīs "sacensības" beidzas, ir caur viņu pašu spēku zudumu. Šie ļoti spēcīgie aģenti – ieskaitot pilnībā ekipēto nāciju valdības, kam noteikti ir līdzekļi to darīt – būtu ļoti motivēti vai nu iegūt, vai iznīcināt šādas spējas, vai ar spēku, vai ar viltību.[^76]

Tas varētu sākties maza mēroga, kā apmācības skrējienu sabotāža vai uzbrukumi mikroshēmu ražošanai, bet šie uzbrukumi patiešām var apstāties tikai tad, kad visas puses vai nu zaudē spēju sacīkstēties AI, vai zaudē spēju veikt uzbrukumus. Tā kā dalībnieki uzskata likmes par eksistenciālām, abi gadījumi, visticamāk, pārstāv katastrofālu karu.

Tas mūs noved pie ceturtās iespējas: sacīkstēšanās uz superintelektu, un pēc iespējas ātrākā, vismazāk kontrolētā veidā. Kad AI pieaug spēkā, tā attīstītājiem abās pusēs būs progresīvi grūtāk to kontrolēt, īpaši tāpēc, ka sacīkstēšanās pēc spējām ir pretēja rūpīgajam darbam, ko kontrolējamība prasītu. Tātad šis scenārijs mūs noved tieši gadījumā, kur kontrole ir zudusi (vai dota, kā mēs redzēsim tālāk) AI sistēmām pašām. Tas ir, *AI uzvar sacensības.* Bet no otras puses, ciktāl kontrole *ir* saglabāta, mēs turpinājumā esam ar vairākām savstarpēji naidīgām pusēm, katra vadībā ar ārkārtīgi spēcīgām spējām. Tas atkal izskatās pēc kara.

Sapratu to visu citādi.[^77] Pašreizējā pasaulē vienkārši nav institūciju, kas varētu uzticēt šādas spējas AI attīstīšanu, neizaicinot tūlītēju uzbrukumu.[^78] Visas puses pareizi spriedīs, ka vai nu tas *nebūs* kontrolē – un tātad ir drauds visām pusēm, vai arī tas *būs* kontrolē, un tātad ir drauds jebkuram pretiniekam, kas to attīsta mazāk ātri. Šīs ir kodolbruņotas valstis, vai ir uzņēmumi, kas atrodas tajās.

Bez kāda ticama veida cilvēkiem "uzvarēt" šīs sacensības, mēs esam atstāti ar asu secinājumu: vienīgais veids, kā šīs sacensības beidzas, ir vai nu katastrofālā konfliktā, vai kur AI, nevis jebkura cilvēku grupa, ir uzvarētājs.

### Mēs nodosim kontroli AI (vai tā to ņems)

Ģeopolitiskā "lielo varu" konkurence ir tikai viena no daudzām konkurencēm: indivīdi konkurē ekonomiski un sociāli; uzņēmumi konkurē tirgos; politiskās partijas konkurē par varu; kustības konkurē par ietekmi. Katrā arēnā, kad AI tuvojas un pārsniedz cilvēku spējas, konkurētspējas spiedienu piespiedīs dalībniekus deleģēt vai nodot arvien vairāk kontroles AI sistēmām – ne tāpēc, ka šie dalībnieki to vēlas, bet tāpēc, ka viņi [nevar atļauties nedarīt.](https://arxiv.org/abs/2303.16200)

Tāpat kā ar citiem MVI riskiem, mēs to jau redzam ar vājākām sistēmām. Studenti jūt spiedienu izmantot AI savos uzdevumos, jo skaidri daudzi citi studenti to dara. Uzņēmumi [steidzas pieņemt AI risinājumus konkurences iemeslu dēļ.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) Mākslinieki un programmētāji jūtas spiesti izmantot AI, citādi viņu likmēs tiks pazemināti citi, kas to dara.

Šie jūtas kā spiedīgi deleģēšana, bet ne kontroles zudums. Bet ielejot likmes un pastumjot pulksteni uz priekšu. Apsveriet izpilddirektoru, kura konkurenti izmanto MVI "palīgus", lai pieņemtu ātrākus, labākus lēmumus, vai militāru komandieri, kas saskaras ar pretendentu ar AI uzlabotu komandu un kontroli. Pietiekami attīstīta AI sistēma varētu autonomi darboties ar daudzas reizes cilvēcisko ātrumu, sarežģītību, sarežģītību un datu apstrādes spēju, veicot sarežģītus mērķus sarežģītos veidos. Mūsu izpilddirektors vai komandieris, kas vada šādu sistēmu, var redzēt, ka tā sasniedz to, ko viņi vēlas; bet vai viņi saprastu pat mazu daļu no *kā* tas tika sasniegts? Nē, viņiem tikakasvisas, ka to pieņemt. Turklāt daudz no tā, ko sistēma var darīt, nav tikai pieņemt pasūtījumus, bet konsultēt savu domājamo šefu par to, ko darīt. Šīs padomas būs labas – atkal un atkal.

Kurā brīdī tad cilvēka loma tiks samazināta līdz "jā, ej uz priekšu" klikšķināšanai?

Jūtās labi var spējīgas AI sistēmas, kas var uzlabot mūsu produktivitāti, rūpēties par kaitināžām un pat rīkoties kā domas partneris lietu paveikšanā. Jūtā labi varētu AI palīgs, kas var rūpēties par darbībām mūsu vietā, kā labs cilvēciska personiski palīgs. Jūtā dabisk, pat labvēlīgi, kad AI kļūst ļoti gudrs, kompetents un uzticams, atsaukt arvien vairāk un vairāk lēmumus tam. Bet šai "labvēlīgai" deleģēšanai ir skaidrs galapunkts, ja mēs turpinājām pa ceļu: kādu dienu mēs atrodams, ka mēs patiešām nevadām gandrīz neko vairs, un ka AI sistēmas, kas faktiski vada šovu, var tikpat grūt atslēgt kā naftas uzņēmumus, sociālos medijus, internetu vai kapitālismu.

Un šī ir daudz pozitīvākā versija, kurā AI ir vienkārši tik noderīgs un efektīvs, ka mēs ļaujam tam pieņemt lielāko daļu mūsu galveno lēmumu. Realitāte, visticamāk, būtu daudz jaukšanās starp šo un versijām, kur nekontrolētas MVI sistēmas *ņem* dažādas varas formas sev, jo, atcerieties, vara ir noderīga gandrīz jebkuram mērķim, kas ir, un MVI būtu, pēc dizaina, vismaz tikpat efektīva mērķu įgyvendinājā kā cilvēki.

Neatkarīgi no tā, vai mēs piešķiram kontroli vai tā tiek izrauta no mums, tās zudums šķiet ārkārtīgi ticams. Kā Alans Tjurings sākotnēji teica, "...šķiet ticams, ka tiklīdz mašīnu domāšanas metode būtu sākusies, nepaaietu ilgi, kamēr tā pārsniegs mūsu vājās spējas. Nebūtu jautājums par mašīnu miršanu, un tās varētu sarunāties viena ar otru, lai izasinātāt savus prātus. Kādā posmā tāpēc mums būtu jāgaida, ka mašīnas uzņems kontroli..."

Lūdzu, atzīmējiet, lai gan tas ir pietiekami acīmredzams, ka kontroles zudums cilvēcēt AI arī nozīmē Amerikas Savienoto Valstu kontroles zudumu ASV valdībai; tas nozīmē Ķīnas kontroles zudumu Ķīnas Komunistiskajai partijai, un Indijas, Francijas, Brazīlijas, Krievijas un katras citas valsts kontroles zudumu to pašu valdībai. Tādējādi AI uzņēmumi, pat ja tā nav viņu nolūks, pašlaik piedalās potenciālā pasaules valdību, ieskaitot savu, gāšanā. Tas varētu notikt dažu gadu laikā.

### MVI novedīs pie superintelekta

Ir arguments, ka cilvēku konkurētspējīgs vai pat eksperto konkurētspējīgs vispārējās nozīmes AI, pat ja autonoms, varētu būt pārvaldāms. Tas var būt neticami graujošs visos iepriekš apspriesti veidos, bet pasaulē tagad ir daudz ļoti gudru, aģenciālu cilvēku, un tie ir vairāk-mazāk pārvaldāmi.[^79]

Bet mēs netiekams palikt apmēram cilvēku līmenī. Progresija tālāk, visticamāk, tiks vadīta ar tiem pašiem spēkiem, ko mēs jau esam redzējuši: konkurētspējīgs spiediens starp AI attīstītājiem, kas meklē peļņu un varu, konkurētspējīgs spiediens starp AI lietotājiem, kas nevar atļauties atstāt, un – vissvarīgāk – pašas MVI spēja uzlabot sevi.

Procesā, ko mēs jau esam redzējuši sākt ar mazāk spēcīgām sistēmām, MVI pats spētu iedomāties un dizainēt uzlabotu versijas no seda pša. Tas ietver aparatūru, programmatūru, neironu tīklus, rīkus, atbalsta struktūras utt. Tas, pēc definīcijas, būs labāks par mums šā darīšanā, tāpēc mēs nezinām tieši, kā tas intelikta-palaidīs. Bet mums nebūs. Ciktāl mums joprojām ir ietekme uz to, ko MVI dara, mēs vienkārši vajadzētu to lūgt, vai ļaut tam.

Nav cilvēku līmeņa barjera izziņai, kas varētu aizsargāt mūs no šā nekontrolējamā.[^80]

MVI progresija uz superintelektu nav dabas likums; joprojām būtu iespējams apšaubt nekontrolējamo, īpaši, ja MVI ir relatīvi centralizēts un ciktāl to kontrolē puses, kas nejūt spiedienu sacīkstēties viena ar otru. Bet, ja MVI bütu plašai izplatīts un ļoti autonoms, šķiet gandrīz neiespējams novērst tā lēmumu, ka tam vajadzētu būt vairāk, un tad vēl vairāk, spēcīgam.

### Kas notiek, ja mēs uzbūvējam (vai MVI uzbūvē) superintelektu

Runājot tieši, mums nav ne jausmas, kas notiktu, ja mēs izveidotu superintelektu.[^81] Tas veiktu darbības, ko mēs nevaram izsekot vai uztvert iemeslu dēļ, ko nevaram saprast uz mērķiem, ko nevaram iedomāties. To, ko mēs zinām, ir, ka tas nebūs atkarīgs no mums.[^82]

Superintelekta kontroles neiespēja var tikt saprasta ar arvien asākām analoģijām. Vispirms iedomājieties, ka jūs esat liela uzņēmuma izpilddirektors. Nav veids, kā jūs varat izsekot visu, kas notiek, bet ar pareizo personāla iestatīšanu jūs joprojām varat nozīmīgi saprast lielo attēlu un pieņemt lēmumus. Bet iedomājieties tikai vienu lietu: visi citi uzņēmumā darbojas simts reižu jūsu ātrumā. Vai jūs joprojām varat turēties līdzi?

Ar superinteliģentu AI cilvēki "komandētu" kaut ko ne tikai ātrāku, bet darbojoties līmeņos sarežģītības un sarežģītības, ko viņi nevar saprast, apstrādājot daudz vairāk datu, nekā viņi var pat iedomāties. Šo nesamērīgumu var likt uz formālu līmeni: [Ešbija vajadzīgās daudzveidības likums](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (un skatiet saistīto ["laba regulatora teorēmu"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)) nosaka, apmēram, ka jebkurai kontroles sistēmai jābūt tikpat daudz pogas un ciparnīcas kā sistēmai, kas tiek kontrolēta, ir brīvības pakāpes.

Cilvēks, kas kontrolē superinteliģentu AI sistēmu, būtu kā paparstasija, kas kontrolē General Motors: pat ja "dari to, ko paparstasija vēlas", tiktu rakstīts korporatīvos likumos, sistēmas ir tik atšķirīgas ātrumā un darbību diapazonā, ka "kontrole" vienkārši neattiecas. (Un cik ilgi, kamēr šis kaitināmais likums tiek pārrakstīts?)[^83]

Tā kā nav piemēru, ka augi kontrolē fortune 500 korporācijas, būtu tieši nulle piemēri cilvēkiem, kas kontrolē superintelektus. Tas tuvojas matemātiskam faktam.[^84] Ja superintelekts tiktu izveidots – neatkarīgi no tā, kā mēs tur nonācām – jautājums nebūtu par to, vai cilvēki varētu to kontrolēt, bet par to, vai mēs turpinātu pastāvēt, un ja tā, vai mums būtu laba un nozīmīga eksistence kā indivīdiem vai kā sugai. Par šiem eksistenciāliem jautājumiem cilvēcei mums būtu maz ietekmes. Cilvēku laikmets būtu beidzies.

### Secinājums: mums nav jābūvē MVI

Ir scenārijs, kurā MVI celšana var iet labi cilvēcei: tā tiek veidota rūpīgi, kontrolē un cilvēces labā, kuru pārvalda daudzs ieinteresēto pušu savstarpējās vienošanās,[^85] un novērsta no attīstīšanās uz nekontrolējamu superintelektu.

*Šis scenārijs nav atvērts mums pašreizējos apstākļos.* Kā apspriests šajā sadaļā, ar ļoti augstu varbūtību MVI attīstīšana novedīs pie kādas no kombinācijām:

- Masīva sabiedrības un civilizācijas graujaluma vai iznīcināšana;
- Konflikts vai karš starp lielām varām;
- Kontroles zudums cilvēcei *par* vai *uz* spēcīgām AI sistēmām;
- Nekontrolējams uz nekontrolējamu superintelektu, un cilvēku sugas nerelevance vai pārtraukšana.

Kā agra fikcijas attēlojums MVI teica: vienīgais veids, kā uzvarēt, ir nespēlēt.

[^55]: [ES AI likums](https://artificialintelligenceact.eu/) ir nozīmīgs likumdošanas akts, bet neit tieši novērst bīstamas AI sistēmas attīstīšanu vai ieviešanu, vai pat atklātu izlaišanu, īpaši ASV. Cits nozīmīgs politikas gabals, ASV Izpildrīkojums par AI, ir atsaukts.

[^56]: Šī [Gallup aptauja](https://news.gallup.com/poll/1597/confidence-institutions.aspx) rāda drūmu uzticības lejupslīdi publiskajām iestādēm kopš 2000. gada ASV. Eiropas numuri ir dažādi un mazāk ekstrēmi, bet arī uz lejupejošas tendences. Neuzticēšanās stingri nenozīmē, ka iestādes patiešām *ir* disfunkcionālas, bet tas ir indikācija kā arī cēlonis.

[^57]: Un lielās pārmaiņas, ko mēs tagad atbalstām – piemēram, tiesību paplašināšana jaunām grupām – bija īpaši vadīta cilvēku virzienā uz lietu uzlabošanu.

[^58]: Ļaujiet man būt tiešs. Ja jūsu darbu var darīt no datora aizmugures, ar relatīvi maz klātienes mijiedarbību ar cilvēkiem ārpus jūsu organizācijas, un nenotūr juridisko atbildību pret ārējām pusēm, tas pēc definīcijas būtu iespējams (un visticamāk izmaksu taupīšanas) jūs pilnībā aizstāt ar digitālu sistēmu. Robotika, lai aizstātu daudz fiziska darba, nāks vēlāk – bet ne daudz vēlāk, tiklīdz MVI sāks veidot robotus.

[^59]: Piemēram, kas notiek ar mūsu tiesu sistēmu, ja tiesasūdzību iesniegšana ir gandrīz bezmaksas? Kas notiek, kad drošības sistēmu apiešana ar sociālo inženieriju kļūst lēta, viegla un bez riskiem?

[^60]: [Šis raksts](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/) apgalvo, ka 10% no visa interneta satura jau ir AI ģenerēts, un ir Google augšējais hits (man) uz meklēšanas vaicājumu "novērtējumi par to, cik daļa no jauna interneta satura ir AI ģenerēts." Vai tas ir patiess? Nav ne jausmas! Tas nemin atsauces, un tas netika rakstīts cilvēka. Cik daļa no jauniem attēliem, ko indeksē Google, vai Tvīti, vai komentāri Reddit, vai Youtube videoklipi tiek ģenerēti cilvēku? Neviens nezina – es nedomāju, ka tas ir zinošs skaitlis. Un tas mazāk par *diviem gadiem* no ģeneratīvā AI sākuma.

[^61]: Arī vērts pievienot, ka ir "morāls" risks, ka mēs varētu radīt digitālas būtnes, kas var ciest. Tā kā mums pašlaik nav uzticamas apziņas teorijas, kas ļautu mums atšķirt fiziskas sistēmas, kas var un nevar ciest, mēs nevaram to izslēgt teorētiski. Turklāt AI sistēmu ziņojumi par viņu apziņu, visticamāk, nav uzticami attiecībā uz viņu faktisko pieredzi (vai nepieredzi) ar apziņu.

[^62]: Tehniskās risinājumi šajā AI "saskaņošanas" laukā arī nav ticami būt uzdevuma augsthībā. Pašreizējās sistēmās tie darbojas kādā līmenī, bet ir seklas un parasti var tikt apietas bez būtiska pūliņu; un, kā apspriests zemāk, mums nav īstenu ideju, kā to darīt daudz progresīvākām sistēmām.

[^63]: Šādas AI sistēmas var nākt ar dažiem iebūvētiem drošības mehānismiem. Bet jebkuram modelim ar kaut ko līdzīgu pašreizējo arhitektūru, ja pilnas piekļuve tā svāriem ir pieejama, drošības pasākumi var tikt noņemti ar papildu apmācību vai citām tehnikiuām. Tāpēc praktiski garantēts, ka katrai sistēmai ar barjerām būs arī plaši pieejama sistēma bez tām. Patiešām, Meta Llama 3.1 405B modelis tika atklāti izlaist ar drošības mehānismiem. Bet *pat pirms tam* "bāzes" modelis bez drošības mehānismiem tika nopludināts.

[^64]: Vai tirgus var pārvaldīt šos riskus bez valdības iesaistīšanās? Īsumā, nē. Protams, ir riski, kurus uzņēmumi ir spēcīgi motivēti mazināt. Bet daudzus citus uzņēmumi var un dara eksternalizēt visiem citiem, un daudzi no augšminētiem ir šajā klasē: nav dabisku tirgus stimulu, lai novērstu masu uzraudzību, patiesības izzušanu, varas koncentrāciju, darba vietu zaudēšanu, kaitīgu politisku diskursu utt. Patiešām, mēs visi to esam redzējuši no šodienas tehnoloģijas, īpaši sociālo mediju, kas ir aizgājuši būtībā neregulēti. AI vienkārši ļoti pastiprināt daudzus no tiem pašiem dinamiem.

[^65]: OpenAI, visticamāk, ir paklausīgākas modeļi iekšējai lietošanai. Maz ticams, ka OpenAI ir izveidojusi kādu "aizmugurējo durvju" lai ChatGPT varētu labāk kontrolēt OpenAI pats, jo tas būtu šausmīgs drošības prakse, un būt ļoti ekspluatējams, ņemot vērā AI necaurspīdīgumu un neprognozējamību.

[^66]: Arī būtiska nozīmība: saskaņošana vai jebkuras citas drošības funkcijas ir svarīgas tikai tad, ja tās patiešām tiek izmantotas AI sistēmā. Sistēmas, kas tiek atklāti izlaistas (t.i., kur modeļa svāri un arhitektūra ir publiski pieejami), var tikt relatīvi viegli transformēti sistēmās *bez* šiem drošības pasākumiem. Atklātā gudrāku par cilvēku MVI sistēmu atkrišana būtu pārsteidzoši nevērīga, un grūti iedomāties, kā cilvēku kontrole vai pat atbilstība tiktu saglabāta šādā scenārijā. Būtu visa motivācija, piemēram, lai izlaistu spēcīgus pašreproducējošus un pašizturētību AI aģentus ar mērķi nopelnīt naudu un nosūtīt to uz kriptoprēses maciņu. Vai uzvarēt vēlēšanās. Vai gāzt valdību. Vai "labs" AI varētu palīdzēt šo ierobežot? Varbūt – bet tikai deleģējot tam milzīgu autoritāti, novedot pie kontroles zuduma, kā aprakstīts zemāk.

[^67]: Grāmata garumā ekspozīcijas par problēmu skatīt, piemēram, *Superintelikts*, *Saskaņošanas problēma* un *Cilvēku saderīgs*. Milzīgam kaudzei darbs dažādos tehniskos līmeņos no tiem, kas ir strādājuši gadus, domājot par problēmu, jūs varat apmeklēt [AI saskaņošanas forums](https://www.alignmentforum.org/). Šeit ir [nesen uzņēmums](https://alignment.anthropic.com/2025/recommended-directions/) no Anthropic saskaņošanas komandas par to, ko viņi uzskata par neatrisinitu.

[^68]: Šis ir ["blēdīgs AI"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/) scenārijs. Principā risks varētu būt relatīvi neliels, ja sistēmu joprojām var kontrolēt, to izslēdzot; bet scenārijs varētu arī ietvert AI maldināšanu, pašizgūšanu un reprodukciju, varas agregāciju un citus soļus, kas to padaru grūt vai neiespējami darīt.

[^69]: Ir ļoti bagāta literatūra šajā tēmā, kas sniedzas līdz formatīviem rakstiem [Stīva Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), Nick Bostrom un Eliezer Yudkowsky. Grāmatas garumā ekspozīcijai skatiet [Cilvēku saderīgs](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) Stjuarta Rasela; [šeit](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/) ir īss un atjaunināts pamatpārskats.

[^70]: Atzīstot to, nevis palēninot, lai iegūtu labāku izpratni, MVI uzņēmumi ir izdomājuši citu plānu: viņi liks AI to darīt! Specifiskāk, viņi AI *N* palīdzētu viņiem izdomāt, kā saskaņot AI *N+1*, visu ceļu uz superintelektu. Lai gan AI izmantošana, lai palīdzētu mums saskaņot AI, skan solīt, ir spēcīgs arguments, ka tas vienkārši pieņem tā secinājumu kā priekšnoteikumu, un kopumā ir neticami riskants pieejas. Skatiet [šeit](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us) par dažu diskusiju. Šis "plāns" nav viens un ir piedzīvojis neko līdzīgu pārbaudīšanai, kas atbilstošā core stratēģijai par to, kā padarīt supercilvēciski AI iet labi cilvēcei.

[^71]: Galu galā, cilvēki, bojāti un nekārtīgi, kādi mēs esam, ir attīstījuši ētiskas sistēmas, ar kurām mēs attiecamies vismaz uz dažām citām sugām uz Zemes labi. (Tikai nedomā par šīm rūpnīcu fermām.)

[^72]: Šeit, par laimi, ir izeja: ja dalībnieki nāk saprast, ka viņi ir iesaistīti pašnāvības sacensībās, nevis uzvaramās. Tas ir tas, kas notika tuvu aukstā kara beigām, kad ASV un PSRS nāca saprast, ka ziemas nukleārā dēļ, pat *neatbildēts* kodoluzbrukums būtu katastrofāls uzbrucējam. Ar realizāciju, ka "kodolkaru nevar uzvarēt un nekad nav jācīnās", nāca nozīmīgas vienošanās par apbruņošanās samazināšanu – būtībā apbruņošanās sacensību beigas.

[^73]: Karš, skaidri vai netiešu.

[^86]: Eskalācija, tad karš.

[^74]: Maģiskā domāšana.

[^75]: Man arī ir kvadriljona dolāra tilts, ko jums pārdot.

[^76]: Šādi aģenti, iespējams, vēlētos "iegūt", ar iznīcināšanu kā rezervi; bet modeļu nodrošināšana pret abiem iznīcināšanu *un* zādzību ar spēcīgām nācijām ir grūti, lai teiktu vismaz, īpaši privātam entītijiem.

[^77]: Citu perspektīvu par MVI nacionālās drošības riskiem skatiet [šo RAND ziņojumu.](https://www.rand.org/pubs/perspectives/PEA3691-4.html)

[^78]: Varbūt mēs varētu būvēt šādu iestādi! Ir bijuši priekšlikumi "CERN par AI" un citām līdzīgām iniciatīvām, kur MVI attīstība ir daudzpusējā globālā kontrolē. Bet šobrīd nav šādas iestādes vai nav redzamības.

[^79]: Un, lai gan saskaņošana ir ļoti sarežģīta, cilvēku l

## 8. nodaļa - Kā neveidot MVI

MVI nav neizbēgams – šodien mēs atrodamies ceļu krustojumā. Šī nodaļa sniedz priekšlikumu tam, kā mēs varētu novērst tā izveidi.

Ja ceļš, pa kuru mēs šobrīd ejam, noved pie mūsu civilizācijas iespējamās beigas, kā mēs varam mainīt ceļu?

Pieņemsim, ka vēlēšanās pārtraukt MVI un superintelekta attīstību būtu plaši izplatīta un spēcīga,[^87] jo kļūst par vispārēju izpratni, ka MVI būtu drīzāk varas absorbējošs, nevis varas piešķirošs, un būtu dziļa briesma sabiedrībai un cilvēcei. Kā mēs aizvērtu Vārtus?

Pašlaik mēs zinām tikai vienu veidu, kā *veidot* spēcīgu un vispārēju AI, kas ir ar patiešām masīviem dziļo neironu tīklu skaitļojumiem. Tā kā šīs ir neticami grūtas un dārgas lietas, ir zināmā mērā tā, ka to *nedarīšana* ir viegla.[^88] Bet mēs jau esam redzējuši spēkus, kas virza uz MVI, un spēļu teorētisko dinamiku, kas jebkurai pusei ļoti apgrūtina vienpusēju apturēšanu. Tāpēc būtu nepieciešama āršķības (t.i., valdību) iejaukšanās kombinācija, lai apturētu korporācijas, un vienošanās starp valdībām, lai apturētu sevi.[^89] Kā tas varētu izskatīties?

Vispirms ir lietderīgi atšķirt AI attīstības virzienes, kas ir *jānovērš* vai *jāaizliedz*, un tās, kas ir *jāpārvalda*. Pirmais galvenokārt būtu nekontrolējams virziens uz superintelektu.[^90] Aizliegtajai attīstībai definīcijām jābūt pēc iespējas skaidrākām, un gan verificēšanai, gan ieviešanai jābūt praktiskām. Kas ir *jāpārvalda*, būtu vispārējas, spēcīgas AI sistēmas – kas mums jau ir, un kurām būs daudz pelēko zonu, nianšu un sarežģītības. Šīm ir izšķiroša nozīme spēcīgām efektīvām institūcijām.

Mēs varam arī lietderīgi norobežot jautājumus, kas jārisina starptautiskā līmenī (tostarp starp ģeopolitiskajiem konkurentiem vai pretinieks) [^91] no tiem, ko var pārvaldīt atsevišķas jurisdikcijas, valstis vai valstu kopas. Aizliegtā attīstība lielākoties iekļaujas "starptautiskajā" kategorijā, jo vietējo tehnoloģijas attīstības aizliegumu parasti var apiet, mainot atrašanās vietu.[^92]

Visbeidzot, mēs varam apsvērt instrumentus rīku kastē. To ir daudz, tostarp tehniskie rīki, mīkstie tiesību akti (standarti, normas u.c.), stingie tiesību akti (noteikumi un prasības), atbildība, tirgus stimuli u.c. Pievērsīsim īpašu uzmanību vienam, kas ir raksturīgs AI.

### Skaitļošanas drošība un pārvaldība

Būtisks instruments augstas jaudas AI pārvaldībā būs aparatūra, kas tai nepieciešama. Programmatūra izplatās viegli, tai ir gandrīz nulles robežizmaksas ražošanā, tā šķērso robežas bez grūtībām un to var acumirklī modificēt; nekas no šiem apgalvojumiem nav patiess attiecībā uz aparatūru. Tomēr, kā mēs esam apsprieduši, milzīgs šīs "skaitļošanas jaudas" daudzums ir nepieciešams gan AI sistēmu apmācības laikā, gan secinājumu izdarīšanas laikā, lai sasniegtu visaptverošākās sistēmas. Skaitļošanas jaudu var viegli kvantificēt, uzskaitīt un auditēt ar salīdzinoši mazu neskaidrību, tiklīdz ir izstrādāti labi noteikumi šā procesa veikšanai. Vissvarīgākais, lieli skaitļošanas apjomi ir, līdzīgi bagātinātajam urānam, ļoti trūcīgs, dārgs un grūti ražojams resurss. Lai gan datoru mikroshēmas ir visur, AI nepieciešamā aparatūra ir dārga un ārkārtīgi grūti ražojama.[^93]

Tas, kas AI specializētās mikroshēmas padara daudz *labāk* pārvaldāmas kā trūcīgu resursu nekā urāns, ir tas, ka tās var ietvert aparatūras drošības mehānismus. Lielākajā daļā mūsdienu mobilo tālruņu un dažos klēpjdatoros ir specializētas mikroshēmās iebūvētas aparatūras funkcijas, kas ļauj tiem nodrošināt, ka tie instalē tikai apstiprinātu operētājsistēmas programmatūru un atjauninājumus, ka tie saglabā un aizsargā jutīgus biometriskos datus ierīcē, un ka tos var padarīt bezderīgus ikvienam, izņemot to īpašnieku, ja tie tiek pazaudēti vai nozagti. Pēdējo gadu laikā šādi aparatūras drošības pasākumi ir kļuvuši plaši atzīti un ieviesti, un parasti ir pierādījuši, ka ir diezgan droši.

Šo funkciju galvenā novitāte ir tāda, ka tās saista aparatūru un programmatūru, izmantojot kriptogrāfiju.[^94] Tas ir, tikai konkrētas datora aparatūras daļas esamība nenozīmē, ka lietotājs var ar to darīt visu, ko vēlas, izmantojot dažādu programmatūru. Un šī saistīšana arī nodrošina spēcīgu drošību, jo daudziem uzbrukumiem būtu nepieciešams *aparatūras*, nevis tikai *programmatūras* drošības pārkāpums.

Vairāki nesenie ziņojumi (piemēram, no [GovAI un līdzstrādniekiem](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips) un [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) ir norādījuši, ka līdzīgas aparatūras funkcijas, kas iebūvētas vismodernākajā AI attiecīgajā skaitļošanas aparatūrā, varētu spēlēt ārkārtīgi noderīgu lomu AI drošībā un pārvaldībā. Tie iespējo vairākas funkcijas, kas pieejamas "pārvaldniekam",[^95] kuras, iespējams, nevarētu uzminēt, ka ir pieejamas vai pat iespējamas. Kā daži svarīgi piemēri:

- *Ģeogrāfiskā atrašanās vieta*: Sistēmas var uzstādīt tā, lai mikroshēmām būtu zināma atrašanās vieta, un tās var darboties citādi (vai tikt pilnībā izslēgtas) atkarībā no atrašanās vietas.[^96]
- *Atļauto savienojumu saraksts*: katru mikroshēmu var konfigurēt ar aparatūras iespaidotu atļauto mikroshēmu sarakstu, ar kurām tā var izveidot tīklu, un tā nevar savienoties ar mikroshēmām, kas nav šajā sarakstā.[^97] Tas var ierobežot komunikatīvo mikroshēmu kopumu lielumu.[^98]
- *Mērīta secinājumu izdarīšana vai apmācība (un automātiska izslēgšana)*: Pārvaldnieks var licencēt tikai noteiktu apmācības vai secinājumu daudzumu (laikā, vai FLOP, vai iespējams žetonos), ko lietotājs var veikt, pēc kā nepieciešama jauna atļauja. Ja pieaugumi ir mazi, tad ir nepieciešama salīdzinoši nepārtraukta modeļa pārlicencēšana. Modeli tad var "izslēgt" vienkārši, nedodot šo licences signālu.[^99]
- *Ātruma ierobežojums*: Modelim tiek liegts darboties ar lielāku secinājumu ātrumu nekā kāda robeža, ko nosaka pārvaldnieks vai citādi. To varētu īstenot ar ierobežotu atļauto savienojumu kopu vai ar izpildu līdzekļiem.
- *Apliecināta apmācība*: Apmācības procedūra var sniegt kriptogrāfiski drošu pierādījumu, ka modeļa ģenerēšanā tika izmantota konkrēta kodu, datu kopa un skaitļošanas jaudas lietošanas apjoms.

### Kā neveidot superintelektu: globāli apmācības un secinājumu skaitļošanas ierobežojumi

Ņemot vērā šos apsvērumus – īpaši attiecībā uz skaitļošanu –, mēs varam apspriest, kā aizvērt Vārtus uz mākslīgo superintelektu; pēc tam mēs vērsīsimies pie pilna MVI novēršanas un AI modeļu pārvaldīšanas, kad tie tuvojas un pārsniedz cilvēka spējas dažādos aspektos.

Pirmā sastāvdaļa ir, protams, izpratne, ka superintelekts nebūtu kontrolējams, un ka tā sekas ir fundamentāli neparedzamas. Vismaz Ķīnai un ASV neatkarīgi jāizlemj šim vai citiem mērķiem neveidot superintelektu.[^100] Pēc tam starp tām un citām nepieciešama starptautiska vienošanās ar spēcīgu verificēšanas un ieviešanas mehānismu, lai nodrošinātu visām pusēm, ka to konkurenti nenovēršas un neizlemj mest kauliņus.

Lai būtu pārbaudāmi un īstenojami, ierobežojumiem jābūt stingriem ierobežojumiem un pēc iespējas viennozīmīgiem. Tas šķiet gandrīz neiespējama problēma: ierobežot sarežģītas programmatūras ar neparedzamām īpašībām spējas visā pasaulē. Par laimi situācija ir daudz labāka nekā šī, jo tieši tas, kas ir padarījis iespējamu progresīvu AI – milzīgs skaitļošanas daudzums – ir daudz, daudz vieglāk kontrolējams. Lai gan tas joprojām varētu atļaut dažas spēcīgas un bīstamas sistēmas, *nekontrolējams superintelekts* visticamāk var tikt novērsts ar stingru neironu tīklā iekļauto skaitļojumu apjoma ierobežojumu, kopā ar ātruma ierobežojumu secinājumu daudzumam, ko AI sistēma (no savienotiem neironu tīkliem un citas programmatūras) var veikt. Šīs konkrēta versija ir piedāvāta zemāk.

Var šķist, ka stingru globālu AI skaitļošanas ierobežojumu uzlikšana prasītu milzīgas starptautiskās koordinācijas un iejaucīgas, privātumu iznīcinošas uzraudzības līmenis. Par laimi, tas nebūtu nepieciešams. Ārkārtīgi [sašaurināta un ierobežota piegādes ķēde](https://arxiv.org/abs/2402.08797) nodrošina, ka, tiklīdz ierobežojums ir likumīgi noteikts (vai nu ar likumu, vai izpildu rīkojumu), šā ierobežojuma ievērošanas verificēšanai būtu nepieciešama tikai dažu lielu uzņēmumu iesaistīšanās un sadarbība.[^101]

Šādam plānam ir vairākas ļoti vēlamas iezīmes. Tas ir minimāli iebrūkošs tādā ziņā, ka tikai dažiem lieliem uzņēmumiem tiek uzliktas prasības, un tikai diezgan nozīmīgi skaitļošanas kopumi tiktu pārvaldīti. Attiecīgajās mikroshēmās jau ir aparatūras iespējas, kas nepieciešamas pirmajai versijai.[^102] Gan īstenošana, gan ieviešana paļaujas uz standarta juridiskajiem ierobežojumiem. Bet tos atbalsta aparatūras lietošanas noteikumi un aparatūras kontroles, ievērojami vienkāršojot ieviešanu un novēršot uzņēmumu, privāto grupu vai pat valstu krāpšanu. Ir bagātīgs precedents tam, ka aparatūras uzņēmumi uzliek attālinātas ierobežojumus savai aparatūras lietošanai un bloķē/atbloķē konkrētas iespējas ārēji,[^103] tostarp pat augstas jaudas CPU datu centros.[^104] Pat salīdzinoši nelielajai aparatūras un organizāciju daļai, kuras tā ietekmē, uzraudzību varētu ierobežot līdz telemetrijai, bez tiešas piekļuves datiem vai modeļiem; un šā programmatūra varētu būt atvērta pārbaudei, lai pierādītu, ka netiek reģistrēti papildu dati. Shēma ir starptautiska un sadarbīga, un diezgan elastīga un paplašināma. Tā kā ierobežojums galvenokārt attiecas uz aparatūru, nevis programmatūru, tas ir salīdzinoši agnostisks attiecībā uz AI programmatūras attīstību un izvietošanu, un ir saderīgs ar dažādām paradigmām, tostarp vairāk "decentralizētu" vai "publisku" AI, kas vērstu uz AI izraisītās varas koncentrācijas apkarošanu.

Skaitļošanā balstītai Vārtu aizvēršanai ir arī trūkumi. Pirmkārt, tā ir tālu no pilnīga risinājuma AI pārvaldības problēmai kopumā. Otrkārt, kā datoru aparatūra kļūst ātrāka, sistēma "noķertu" arvien vairāk aparatūras arvien mazākos kopumos (vai pat atsevišķos GPU).[^105] Ir arī iespējams, ka algoritmisku uzlabojumu dēļ būtu nepieciešams pat zemāks skaitļošanas ierobežojums,[^106] vai ka skaitļošanas daudzums kļūst lielākoties neatbilstošs, un Vārtu aizvēršanai tā vietā būtu nepieciešams detalizētāks uz risku vai spējām balstīts AI pārvaldības režīms. Treškārt, neatkarīgi no garantijām un neliela skarto organizāciju skaita, šāda sistēma neapšaubāmi radīs pretestību attiecībā uz privātumu un uzraudzību, citu problēmu vidū.[^107]

Protams, skaitļošanas ierobežojošas pārvaldības shēmas izstrādāšana un īstenošana īsā laika periodā būs diezgan izaicinoša. Bet tas noteikti ir izdarāms.

### A-G-I: trīskāršā krustošanās kā riska un politikas pamats

Pievērsīsimies tagad MVI. Stingrās līnijas un definīcijas šeit ir grūtākas, jo mums noteikti ir intelekts, kas ir mākslīgs un vispārējs, un pēc nevienas esošās definīcijas visi nepiekritīs, vai un kad tas pastāv. Turklāt, skaitļošanas vai secinājumu ierobežojums ir diezgan neelastīgs rīks (skaitļošana ir spēju pilnvara, kas tad ir riska pilnvara), kas – ja vien tas nav diezgan zems – nav ticams, ka novērsīs MVI, kas ir pietiekami spēcīgs, lai izraisītu sociālos vai civilizācijas traucējumus vai akūtus riskus.

Es esmu apgalvojis, ka visas akūtākās riskuus rodas no trīskāršās krustošanās starp ļoti augstām spējām, augstu autonomiju un lielu vispārīgumu. Šīs ir sistēmas, kas – ja tās vispār tiek izstrādātas – jāpārvalda ar milzīgu piesardzību. Radot stingrus standartus (ar atbildības un regulējuma palīdzību) sistēmām, kas apvieno visas trīs īpašības, mēs varam virzīt AI attīstību uz drošākām alternatīvām.

Kā ar citām nozarēm un produktiem, kas potenciāli varētu kaitēt patērētājiem vai sabiedrībai, AI sistēmām nepieciešams rūpīgs regulējums no efektīvu un pilnvarotu valdības aģentūru puses. Šim regulējumam jāatzīst MVI riskantums, un jānovērš nepieņemami riskanti augstas jaudas AI sistēmu izstrādāšana.[^108]

Tomēr plaša mēroga regulējums, īpaši ar īstiem zobiem, kas noteikti tiks pretotos nozares,[^109] prasa laiku[^110], kā arī politisko pārliecību, ka tas ir nepieciešams.[^111] Ņemot vērā progresa tempu, tas var prasīt vairāk laika, nekā mums ir pieejams.

Daudz ātrākā laika skalā un, kamēr tiek izstrādāti regulatīvie pasākumi, mēs varam dot uzņēmumiem nepieciešamos stimulus (a) atturēties no ļoti augstas riska aktivitātēm un (b) izstrādāt visaptverošas sistēmas riska novērtēšanai un mazināšanai, noskaidrojot un palielinot atbildības līmeņus bīstamākajām sistēmām. Ideja būtu uzlikt visaugstākos atbildības līmeņus – stingrus un dažos gadījumos personiskus kriminālos – sistēmām trīskāršajā autonomijas-vispārīguma-intelekta krustošanās, bet nodrošināt "drošas ostas" uz vairāk tipisko vainas balstīto atbildību sistēmām, kurās trūkst vienas no šīm īpašībām vai tā ir garantēti pārvaldāma. Tas ir, piemēram, "vāja" sistēma, kas ir vispārēja un autonoma (kā spējīgs un uzticams, bet ierobežots personiskais asistents) būtu pakļauta zemākiem atbildības līmeņiem. Tāpat šaura un autonoma sistēma, piemēram, pašbraucošs auto, joprojām būtu pakļauta nozīmīgajam regulējumam, kāds tai jau ir, bet ne pastiprinātai atbildībai. Līdzīgi augstas spējas un vispārējai sistēmai, kas ir "pasīva" un lielākoties nespējīga uz neatkarīgu rīcību. Sistēmām, kurām trūkst *divu* no trim īpašībām, ir vēl vairāk pārvaldāmas, un drošo ostu būtu vēl vieglāk pieprasīt. Šī pieeja atspoguļo to, kā mēs rīkojamies ar citām potenciāli bīstamām tehnoloģijām:[^112] augstāka atbildība par bīstamākām konfigurācijām rada dabiskus stimulus drošākām alternatīvām.

Šādu augstu atbildības līmeņu noklusējuma rezultāts, kas darbojas, lai MVI risku *internalizētu* uzņēmumos, nevis to uzslēgtu sabiedrībai, ir ticams (un cerams!) uzņēmumiem vienkārši neizstrādāt pilnu MVI, kamēr un ja vien tie nevar to patiešām padarīt uzticamu, drošu un kontrolējamu, ņemot vērā, ka *pašu vadība* ir riskam pakļautā puse. (Gadījumā, ja tas nav pietiekams, likumdošanā, kas noskaidro atbildību, arī būtu skaidri jāatļauj aizlieguma atvieglojums, t.i., tiesnesim pavēlēt apturēt aktivitātes, kas ir skaidri bīstamības zonā un argumentēti rada sabiedriskas riska.) Kad stājas spēkā regulējums, regulējuma ievērošana var kļūt par drošo ostu, un drošās ostas no zemas AI sistēmu autonomijas, šaurības vai vājuma var pārveidoties salīdzinoši vieglākos regulatīvos režīmos.

### Galvenās Vārtu aizvēršanas noteikumu daļas

Ņemot vērā iepriekš minēto diskusiju, šajā sadaļā ir sniegti priekšlikumi galvenajām daļām, kas īstenotu un uzturētu aizliegumu attiecībā uz pilnu MVI un superintelektu, kā arī cilvēka konkurētspējīgu vai ekspertu konkurētspējīgu vispārēju AI pārvaldīšanu tuvu pilna MVI slieksnim.[^113] Tai ir četras galvenās daļas: 1) skaitļošanas uzskaite un uzraudzība, 2) skaitļošanas ierobežojumi AI apmācībā un darbībā, 3) atbildības ietvars un 4) pakāpenaini drošības un drošuma standarti, kas ietver stingras regulatīvās prasības. Tie ir sīki aprakstīti turpmāk, ar sīkākām detaļām vai īstenošanas piemēriem, kas sniegti trīs pievienotajās tabulās. Svarīgi atzīmēt, ka tie ir tālu no visa, kas būs nepieciešams progresīvu AI sistēmu pārvaldīšanai; lai gan tiem būs papildu drošības un drošuma ieguvumi, tie ir vērsti uz Vārtu aizvēršanu intelekta nekontrolējamai izaugsmei un AI attīstības novirzīšanu labākā virzienā.

#### 1\. Skaitļošanas uzskaite un caurspīdīgums

- Standartu organizācijai (piemēram, NIST ASV, kam seko ISO/IEEE starptautiski) jākodificē detalizēts tehnisks standarts kopējai skaitļošanai, kas izmantota AI modeļu apmācībā un darbībā, FLOP vienībās, un ātrumam FLOP/s, ar kādu tie darbojas. Detaļas tam, kā tas varētu izskatīties, ir sniegtas A pielikumā.[^114]
- Prasība – vai nu ar jauniem tiesību aktiem, vai saskaņā ar esošajām pilnvarām[^115] – jurisdikcijām, kurās notiek liela mēroga AI apmācība, jāuzliek pienākums aprēķināt un ziņot regulatīvajai struktūrai vai citai aģentūrai kopējo FLOP, kas izmantots apmācībā un visu modeļu darbībā virs 10<sup>25</sup> FLOP vai 10<sup>18</sup> FLOP/s sliekšņa.[^116]
- Šīs prasības jāievieš pakāpeniski, sākotnēji prasot labi dokumentētus labticīgus aprēķinus uz ceturkšņa pamata, ar vēlākām fāzēm, kas prasa progresīvi augstākus standartus, līdz kriptogrāfiski apliecinātam kopējam FLOP un FLOP/s, kas pievienots katram modeļa *izvadījumam*.
- Šiem ziņojumiem jābūt papildināti ar labi dokumentētiem robežizmaksu enerģijas un finansiālo izmaksu aprēķiniem, kas izmantoti katra AI izvadījuma ģenerēšanā.

Pamatojums: Šie labi aprēķinātie un caurspīdīgi ziņotie skaitļi būtu pamats apmācības un darbības ierobežojumiem, kā arī droša osta no augstākiem atbildības pasākumiem (skatīt C un D pielikumus).

#### 2\. Apmācības un darbības skaitļošanas ierobežojumi

- Jurisdikcijām, kas uztur AI sistēmas, jāuzliek stingrs ierobežojums kopējai skaitļošanai, kas iet jebkura AI modeļa izvadījumā, sākot ar 10<sup>27</sup> FLOP[^117] un regulējams pēc nepieciešamības.
- Jurisdikcijām, kas uztur AI sistēmas, jāuzliek stingrs ierobežojums AI modeļu izvadījumu skaitļošanas ātrumam, sākot ar 10<sup>20</sup> FLOP/s un regulējams pēc nepieciešamības.

Pamatojums: Kopējā skaitļošana, lai gan ļoti nepilnīga, ir AI spēju (un riska) pilnvara, kas ir konkrēti mērāma un pārbaudāma, tāpēc nodrošina stingru atbalstu spēju ierobežošanai. Konkrēts īstenošanas priekšlikums ir sniegts B pielikumā.

#### 3\. Pastiprinātā atbildība bīstamām sistēmām

- Progresīvas AI sistēmas, kas ir ļoti vispārēja, spējīga un autonoma, radīšanai un darbībai[^118] likumdošanas ceļā jānoskaidro, ka tā ir pakļauta stingrai, kopīgai un vairāku pušu, nevis vienas puses vainas balstītai atbildībai.[^119]
- Jābūt pieejamam juridiskam procesam apstiprinošu drošības lietu iesniegšanai, kas piešķirtu drošo ostu no stingras atbildības sistēmām, kas ir mazas (skaitļošanas izteiksmē), vājas, šauras, pasīvas vai kurām ir pietiekamas drošības, drošuma un kontrolējamības garantijas.
- Jāizklāsta skaidrs ceļš un nosacījumu kopa aizlieguma atvieglojumam, lai apturētu AI apmācības un secinājumu darbības, kas rada sabiedrisku bīstamību.

Pamatojums: AI sistēmas nevar būt atbildīgas, tāpēc mums jāpadara atbildīgas cilvēku personas un organizācijas par kaitējumu, ko tās rada (atbildība).[^120] Nekontrolējams MVI ir sabiedrības un civilizācijas apdraudējums, un drošības lietas trūkuma gadījumā to jāuzskata par nenormāli bīstamu. Uzliekot atbildības nastu izstrādātājiem pierādīt, ka spēcīgi modeļi ir pietiekami droši, lai tos neuzskatītu par "nenormāli bīstamiem", stimulē drošu attīstību, kā arī caurspīdīgumu un ierakstu glabāšanu, lai pieprasītu šīs drošās ostas. Pēc tam regulējums var novērst kaitējumu tur, kur atbildības atturēšana ir nepietiekama. Visbeidzot, AI izstrādātāji jau ir atbildīgi par kaitējumiem, ko tie rada, tāpēc atbildības juridiska noskaidrošana vissvarīgākajām sistēmām var tikt darīta nekavējoties, neizstrādājot ļoti detalizētus standartus; tie tad var attīstīties laika gaitā. Detaļas sniegtas C pielikumā.

#### 4\. AI drošības regulējums

Regulatīva sistēma, kas risina liela mēroga AI akūtos riskus, minimāli prasīs:

- Atbilstošu regulatīvo struktūru identificēšanu vai radīšanu, iespējams, jaunu aģentūru;
- Visaptverošu riska novērtēšanas ietvaru;[^121]
- Ietvaru apstiprinošu drošības lietu iesniegšanai, daļēji balstītu uz riska novērtēšanas ietvaru, ko veic izstrādātāji, un *neatkarīgu* grupu un aģentūru auditam;
- Pakāpenotu licencēšanas sistēmu ar līmeņiem, kas seko spēju līmeņiem.[^122] Licences tiktu piešķirtas, balstoties uz drošības lietām un auditiem, sistēmu attīstībai un izvietošanai. Prasības svārstītos no paziņošanas zemajā galā līdz kvantitāti drošības, drošuma un kontrolējamības garantijām pirms attīstības augšgalā. Šīs novērstu sistēmu izlaišanu, kamēr tās nav pierādīti drošas, un aizliedzēt pašos pamatos nedrošu sistēmu attīstību. D pielikums sniedz priekšlikumu tam, ko šādi drošības un drošuma standarti varētu ietvert.
- Vienošanās šādus pasākumus nest uz starptautisko līmeni, tostarp starptautiskas struktūras normu un standartu saskaņošanai, un potenciāli starptautiskas aģentūras drošības lietu pārskatīšanai.

Pamatojums: Galu galā, atbildība nav pareizais mehānisms, lai novērstu liela mēroga risku sabiedrībai no jaunas tehnoloģijas. Visaptverošs regulējums ar pilnvarotām regulatīvajām struktūrām būs nepieciešams AI tāpat kā katrai citai lielajai nozarei, kas rada risku sabiedrībai.[^123]

Regulējums, lai novērstu citus izplatītus, bet mazāk akūtus riskus, visticamāk atšķirsies savā formā no jurisdikcijas līdz jurisdikcijai. Svarīgais ir izvairīties no to AI sistēmu attīstīšanas, kas ir tik riskants, ka šie riski ir nepārvaldāmi.

### Kas tad?

Nākamajā desmitgadē, kad AI kļūst izplatītāks un pamattehnoloģija attīstās, visticamāk notiks divas svarīgas lietas. Pirmkārt, esošo spēcīgo AI sistēmu regulēšana kļūs sarežģītāka, tomēr vēl nepieciešamāka. Ir ticams, ka vismaz daži pasākumi, kas risina liela mēroga drošības riskus, prasīs vienošanos starptautiskā līmenī, ar atsevišķām jurisdikcijām, kas īsteno noteikumus, pamatojoties uz starptautiskajām vienošanām.

Otrkārt, apmācības un darbības skaitļošanas ierobežojumus kļūs grūtāk uzturēt, jo aparatūra kļūs lētāka un izmaksu ziņā efektīvāka; tie var kļūt arī mazāk nozīmīgi (vai būs jābūt pat stingrākiem) ar algoritmu un arhitektūru uzlabojumiem.

Tas, ka AI kontrolēšana kļūs grūtāka, nenozīmē, ka mums jāpadodas! Šajā esejā izklāstītā plāna īstenošana dotu mums gan vērtīgu laiku, gan izšķirošu kontroli pār procesu, kas mūs ievietotu tālu, tālu labākā pozīcijā, lai izvairītos no AI eksistenciālā riska mūsu sabiedrībai, civilizācijai un sugai.

Vēl ilgtermiņā būs jāveic izvēles par to, ko mēs atļaujam. Mēs varam izvēlēties joprojām radīt kādu patiešām kontrolējama MVI formu, ciktāl tas izrādās iespējams. Vai mēs varam izlemt, ka pasaules vadīšanu ir labāk atstāt mašīnām, ja varam sevi pārliecināt, ka tās darīs to labāk un izturēsies pret mums labi. Bet šiem jābūt lēmumiem, kas pieņemti ar dziļu zinātnisku AI izpratni rokās, un pēc nozīmīgas globālas iekļaujošas diskusijas, nevis sacīkstē starp tehnoloģiju moguliem, kurā lielākā daļa cilvēces ir pilnīgi neiesaistīta un neapzinās.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) A-G-I un superintelekta pārvaldības kopsavilkums ar atbildības un regulējuma palīdzību. Atbildība ir augstākā un regulējums stiprākais Autonomijas, Vispārīguma un Intelekta trīskāršajā krustošanās. Drošās ostas no stingras atbildības un spēcīga regulējuma var iegūt ar apstiprinošām drošības lietām, kas pierāda, ka sistēma ir vāja un/vai šaura un/vai pasīva. Ierobežojumi kopējai Apmācības Skaitļošanai un Secinājumu Skaitļošanas ātrumam, kas pārbaudīti un īstenoti likumīgi un izmantojot aparatūras un kriptogrāfiskos drošības pasākumus, atbalsta drošību, izvairšuoties no pilna MVI un efektīvi aizliedzot superintelektu.


[^87]: Visticamāk, šīs izpratnes izplatīšanās prasīs vai nu intensīvas pūles no izglītības un aizstāvības grupām, kas izvirza šo argumentu, vai diezgan nozīmīgu AI izraisītu katastrofu. Mēs varam cerēt, ka tā būs pirmā.

[^88]: Paradoksāli, mēs esam pieraduši, ka Daba ierobežo mūsu tehnoloģijas, padarot tās ļoti grūti attīstāmas, īpaši zinātniski. Bet tas vairs neattiecas uz AI: galvenās zinātniskās problēmas izrādās vieglākas nekā paredzēts. Mēs nevaram paļauties uz to, ka Daba mūs glābs no sevis – tas būs jādara mums pašiem.

[^89]: Kur tieši mēs apstājamies jaunu sistēmu attīstīšanā? Šeit mums jāpieņem piesardzības princips. Tiklīdz sistēma ir izvietota, un īpaši, tiklīdz šāda sistēmas spēju līmenis izplatās, to ir ārkārtīgi grūti atgriezt. Un ja sistēma ir *izstrādāta* (īpaši ar lielām izmaksām un pūlēm), būs milzīgs spiediens to lietot vai izvietot, un kārdinājums tam tikt noplūdinātu vai nozagtam. Sistēmu attīstīšana un *tad* lēmums par to, vai tās ir dziļi nedrošas, ir bīstams ceļš.

[^90]: Būtu arī gudri aizliegt AI attīstību, kas ir pašos pamatos bīstama, piemēram, sevis reproducējošas un evolucionējošas sistēmas, tās, kas paredzētas ieslodzījuma bēgšanai, tās, kas var autonomi sevis uzlabot, apzināti maldinošs un ļaunprātīgs AI u.c.

[^91]: Ņemiet vērā, ka tas ne nepieciešami nenozīmē *īstenotu* starptautiskā līmenī ar kādu globālu struktūru: tā vietā suverēnas valstis varētu īstenot saskaņotas likumus, kā daudzos līgumos.

[^92]: Kā mēs redzēsim zemāk, AI skaitļošanas raksturs atļautu kaut ko hibrīda; bet starptautiska sadarbība joprojām būs nepieciešama.

[^93]: Piemēram, AI attiecīgo mikroshēmu ēšanu nepieciešamos aparātus ražo tikai viena firma, ASML (neraugoties uz daudziem citiem mēģinājumiem to darīt), lielāko daļu attiecīgo mikroshēmu ražo viena firma, TSMC (neraugoties uz citiem, kas mēģina konkurēt), un aparatūras projektēšanu un būvniecību no šīm mikroshēmām veic tikai daži, tostarp NVIDIA, AMD un Google.

[^94]: Vissvarīgākais, katra mikroshēma tur unikālu un nepieejamu kriptogrāfisko privāto atslēgu, ko tā var izmantot lietu "parakstīšanai".

[^95]: Pēc noklusējuma tas būtu uzņēmums, kas pārdod mikroshēmas, bet citi modeļi ir iespējami un potenciāli noderīgi.

[^96]: Pārvaldnieks var noskaidrot mikroshēmas atrašanās vietu, nosakot parakstītu ziņojumu apmaiņas laiku ar to: gaismas galīgais ātrums prasa mikroshēmai atrasties noteikta rādiusa *r* ietvaros no "stacijas", ja tā var atgriezt parakstītu ziņojumu laikā, kas mazāks par *r* / *c*, kur *c* ir gaismas ātrums. Izmantojot vairākas stacijas un zināmu tīkla īpašību izpratni, mikroshēmas atrašanās vietu var noteikt. Šās metodes skaistums ir tāds, ka lielāko daļu tās drošības nodrošina fizikas likumi. Citas metodes varētu izmantot GPS, inerciālā izsekošanu un līdzīgas tehnoloģijas.

[^97]: Alternatīvi, mikroshēmu pāriem varētu atļaut sazināties vienu ar otru tikai ar pārvaldnieka skaidru atļauju.

[^98]: Tas ir izšķiroši, jo vismaz pašlaik ir nepieciešams ļoti augstas caurlaidības savienojums starp mikroshēmām, lai uz tām apmācītu lielajus AI modeļus.

[^99]: To varētu arī uzstādīt, lai prasītu parakstītus ziņojumus no *N* no *M* dažādiem pārvaldniekiem, ļaujot vairākām pusēm dalīties pārvaldībā.

[^100]: Tas nav ne tuvu bezprecendenta – piemēram, armijas nav attīstījušas klonētu vai ģenētiski pārveidotu superkaravīru armijas, lai gan tas, iespējams, ir tehnoloģiski iespējams. Bet tās ir *izvēlējušās* to nedarīt, nevis tās ir novērsušas citas. Ieraksts nav labs tam, ka lielās pasaules varas tiek novērstas no tehnoloģijas attīstīšanas, ko tās stipri vēlas attīstīt.

[^101]: Ar dažiem ievērojamiem izņēmumiem (īpaši NVIDIA) AI specializētā aparatūra ir salīdzinoši neliela šo uzņēmumu vispārējā biznesa un ieņēmumu modeļa daļa. Turklāt plaisa starp aparatūru, kas izmantota progresīvā AI, un "patērētāju klases" aparatūru ir nozīmīga, tāpēc lielākā daļa datoru aparatūras patērētāju būtu lielākoties neietekmēti.

[^102]: Sīkākai analīzei skatiet nesenos ziņojumus no [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) un [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). Tie koncentrējas uz tehnisku iespējamību, īpaši ASV eksporta kontroles kontekstā, cenšoties ierobežot citu valstu augstas jaudas skaitļošanas spējas; bet tam ir acīmredzama pārklāšanās ar šeit iedomāto globālo ierobežojumu.

[^103]: Apple ierīces, piemēram, tiek attālināti un droši bloķētas, kad tiek ziņots par to pazaudēšanu vai nozagšanu, un tās var tikt attālināti atkal aktivizētas. Tas paļaujas uz tām pašām aparatūras drošības funkcijām, kas te apspriešanas.

[^104]: Skatīt, piemēram, IBM [capacity on demand](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) piedāvājumu, Intel [Intel on demand](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html) un Apple [private cloud compute](https://security.apple.com/blog/private-cloud-compute/).

[^105]: [Šajā pētījumā](https://epochai.org/trends#hardware-trends-section) rādīts, ka vēsturiski viena un tā pati veiktspēja ir sasniegta, izmantojot apmēram 30% mazāk dolāru gadā. Ja šī tendence turpinās, var būt nozīmīga AI un "patērētāju" mikroshēmu lietošanas pārklāšanās, un vispār nepieciešamās aparatūras daudzums augstas jaudas AI sistēmām varētu kļūt nepatīkami mazs.

[^106]: Pēc [tā paša pētījuma](https://epochai.org/trends#hardware-trends-section), dotajai veiktspējai attēlu atpazīšanā ir nepieciešams 2,5 reizes mazāk skaitļojumu katru gadu. Ja tas arī tiktu attiecināts uz visaptverošākajām AI sistēmām, skaitļošanas ierobežojums ļoti ilgi nebūtu noderīgs.

[^107]: Īpaši valstu līmenī tas izskatās daudz kā skaitļošanas nacionalizācija, jo valdībai būtu daudz kontroles pār to, kā tiek izmantota skaitļošanas jauda. Tomēr tiem, kas satraucas par valdības iesaistīšanos, tas šķiet daudz drošāks un labāk nekā visas spēcīgākās AI programmatūras *pašas* nacionalizēšana ar kādu apvienošanos starp lieliem AI uzņēmumiem un nacionālajām valdībām, kā daži sāk aizstāvēt.

[^108]: Svarīgs regulatīvs solis Eiropā tika veikts ar 2024. gada [ES AI akta](https://artificialintelligenceact.eu/) pieņemšanu. Tas klasificē AI pēc riska: aizliedzot nepieņemamas sistēmas, regulējot augstas riska sistēmas un uzliekot caurspīdīguma likumus vai nekādus pasākumus zema riska sistēmām. Tas būtiski samazinās dažus AI riskus un palielinās AI caurspīdīgumu pat ASV firmām, bet tam ir divi galvenie trūkumi. Pirmkārt, ierobežots pārklājums: lai gan tas attiecas uz jebkuru uzņēmumu, kas nodrošina AI ES, ieviešana pār ASV bāzētām firmām ir vāja, un militārais AI ir atbrīvots. Otrkārt, lai gan tas aptver GPAI, tas neatpazīst MVI vai superintelektu kā nepieņemamus riskus vai nekovērš to attīstību – tikai to ES izvietošanu. Tā rezultātā tas maz dara, lai ierobežotu MVI vai superintelekta riskus.

[^109]: Uzņēmumi bieži pārstāv, ka tie ir par saprātīgu regulējumu. Bet kā nu nekā tie gandrīz vienmēr šķiet pretojusies jebkuram *konkrētam* regulējumam; liecinieks cīņa par diezgan mazāk skārošo SB1047, ko [lielākā daļa AI uzņēmumu publiski vai privāti pretojās](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/).

[^110]: Bija apmēram 3 1/2 gadi no laika, kad tika ierosināts ES AI akts, līdz tas stājās spēkā.

[^111]: Dažreiz tiek izteikts, ka ir "pārāk agri" sākt regulēt AI. Ņemot vērā pēdējo piezīmi, tas šķiet maz ticams. Vēl viens izteiktais bažas ir, ka regulējums "kaitētu inovācijai". Bet labs regulējums tikai maina virzienu, nevis daudzumu inovācijai.

[^112]: Interesants precedents ir bīstamu materiālu transportēšanā, kas varētu izbēgt un radīt kaitējumu. Šeit [regulējums](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) un [judikatūra](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) ir izveidojuši stingru atbildību ļoti bīstamiem materiāliem, piemēram, sprāgstvielām, benzīnam, indiem, infekcioziem līdzekļiem un radioaktīvajiem atkritumiem. Citi piemēri ietver [brīdinājumus farmācijas produktos](https://www.medicalnewstoday.com/articles/boxed-warnings), [medicīnas ierīču klases](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification) u.c.

[^113]: Cits visaptverošs priekšlikums ar līdzīgiem mērķiem, kas izvirzīts ["A Narrow Path"](https://www.narrowpath.co/), aizstāv centralizētāku, aizlieguma balstītu pieeju, kas visu robežas AI attīstību virza caur vienu starptautisku struktūru, ko pārrauga spēcīgas starptautiskas institūcijas, ar skaidriem kategorisku aizliegumu, nevis pakāpenaitu ierobežojumu. Es arī atbalstītu šo plānu; tomēr tas prasīs vēl vairāk politisku gribu un koordināciju nekā šeit ierosināts.

[^114]: Dažas vadlīnijas šādam standartam tika [publicētas](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/) Frontier Model Forum. Attiecībā pret šeit ierosināto priekšlikumu, tie maldās mazākas precizitātes pusē un mazāk skaitļošanas, kas iekļauts aprēķinā.

[^115]: 2023. gada ASV AI izpildu rīkojums (tagad atsaukts) prasīja līdzīgu, bet mazāk smalkgraudainu ziņošanu. To vajadzētu pastiprināt ar aizvietojošu rīkojumu.

[^116]: Ļoti aptuveni, tagadējām ierasts H100 mikroshēmām tas atbilst apmēram 1000 kopu, kas veic secinājumus; tas ir apmēram 100 (apmēram 5 miljoni USD vērtībā) visno visjauno augšklases NVIDIA B200 mikroshēmu, kas veic secinājumus. Abos gadījumos apmācības skaitlis atbilst tam kopu skaitļošanai vairākus mēnešus.

[^117]: Šis daudzums ir lielāks nekā jebkura pašlaik apmācīta AI sistēma; lielāks vai mazāks skaitlis varētu būt pamatots, kad mēs labāk izprotam, kā AI spējas mērogojuma ar skaitļošanu.

[^118]: Tas attiecas uz tiem, kas izveido un sniedz/uztur modeļus, nevis galalietotājiem.

[^119]: Aptuveni, "stingra" atbildība nozīmē, ka izstrādātāji tiek turēti atbildīgi par produkta nodarītiem kaitējumiem *pēc noklusējuma* un ir standarts, kas izmantots "nenormāli bīstamiem" produktiem un (diezgan amizanti, bet piemēroti) savvaļas dzīvniekiem. "Kopīga un vairāku pušu" atbildība nozīmē, ka atbildība tiek piešķirta visām par produktu atbildīgajām pusēm, un šīm pusēm ir jāizšķir savstarpēji, kas nes kādu atbildību. Tas ir svarīgi sistēmām kā AI ar garu un sarežģītu vērtību ķēdi.

[^120]: Standarta vainas balstīta vienas puses atbildība nav pietiekama: vaina būs grūti gan izsekojama, gan piešķirami, jo AI sistēmas ir sarežģītas, to darbība nav saprotama, un daudz pušu var būt iesaistītas bīstamas sistēmas vai izvadījuma radīšanā. Turklāt tiesvedībām būs nepieciešami gadi adjudikācijai un visticamāk radīsies tikai sodi, kas ir nekonsekventi šiem uzņēmumiem, tāpēc ir svarīga arī personāla atbildība izpildvadībai.

[^121]: Nevajadzētu būt atbrīvojumam no drošības kritērijiem atvērta svara modeļiem. Turklāt, novērtējot risku, jāpieņem, ka drošības barjeras, kas var tikt noņemtas, tiks noņemtas no plaši pieejamajiem modeļiem, un ka pat aizvērtie modeļi izplatīsies, ja vien nav ļoti augstu garantiju, ka tie paliks droši.

[^122]: Šeit ierosināta shēma ar regulatīvu uzraudzību, ko aktivizē vispārējā spējā; tomēr ir saprātīgi dažiem īpaši riskantiem lietošanas gadījumiem aktivizēt vairāk uzraudzības – piemēram, ekspertu viroloģijas AI sistēmai, pat ja šaura un pasīva, iespējams, jāiet augstākā līmenī. Bijušajam ASV izpildu rīkojumam bija kaut kas no šīs struktūras bioloģiskajām spējām.

[^123]: Divi skaidri piemēri ir aviācija un medikamenti, ko regulē FAA un FDA, kā arī līdzīgas aģentūras citās valstīs. Šīs aģentūras ir nepilnīgas, bet ir bijušas absolūti vitāli šo nozaru funkcionēšanai un veiksme.

## 9. nodaļa - Nākotnes veidošana — ko mums vajadzētu darīt tā vietā

AI var darīt neticami daudz laba pasaulē. Lai iegūtu visas priekšrocības bez riskiem, mums jānodrošina, ka AI paliek cilvēka rīks.

Ja mēs veiksmīgi izvēlēsimies neaizstāt cilvēci ar mašīnām — vismaz uz kādu laiku! — ko mēs varam darīt tā vietā? Vai mēs atsakāmies no AI milzīgajām iespējām kā tehnoloģijai? Zināmā mērā atbilde ir vienkārša *nē:* aizveriet Vārtus nekontrolējamam MVI un superintelektam, bet *jā* veidojiet daudzas citas AI formas, kā arī pārvaldības struktūras un institūcijas, kas mums būs nepieciešamas to vadīšanai.

Bet joprojām ir daudz ko teikt; šīs realizācijas īstenošana būtu cilvēces centrālā nodarbošanās. Šajā sadaļā aplūkoti vairāki galvenie temati:

- Kā mēs varam raksturot "instrumentālo" AI un formas, kādas tas var pieņemt.
- Ka mēs varam iegūt (gandrīz) visu, ko cilvēce vēlas, bez MVI, ar instrumentālo AI.
- Ka instrumentālās AI sistēmas ir (iespējams, principā) pārvaldāmas.
- Ka atgriešanās no MVI nenozīmē kompromisu attiecībā uz nacionālo drošību — gluži pretēji.
- Ka varas koncentrācija ir reālas bažas. Vai mēs varam to mazināt, negraujot drošību un aizsardzību?
- Ka mums būs vajadzīgas — un nepieciešamas — jaunas pārvaldības un sociālās struktūras, un AI faktiskajā var palīdzēt.

### AI Vārtu iekšpusē: instrumentālais AI

Trīskāršās krustošanās diagramma dod labu veidu, kā norobežot to, ko mēs varam saukt par "instrumentālo AI": AI, kas ir kontrolējams rīks cilvēka lietošanai, nevis nekontrolējams konkurents vai aizstājējs. Vismazāk problemātiskās AI sistēmas ir tās, kas ir autonomas, bet nav vispārējas vai īpaši spējīgas (piemēram, izsoles piedāvājumu bots), vai vispārējas, bet nav autonomas vai spējīgas (piemēram, mazs valodas modelis), vai spējīgas, bet šauras un ļoti kontrolējamas (piemēram, AlphaGo).[^124] Tās ar divām krustojošām funkcijām ir ar plašāku pielietojumu, bet augstāku risku un prasīs lielus pūliņus to pārvaldīšanai. (Tikai tāpēc, ka AI sistēma vairāk ir rīks, tas nenozīmē, ka tā ir pašsaprotami droša, tikai ka tā nav pašsaprotami *nedroša* — apsveriet motorzāģi salīdzinājumā ar mājas tīģeri.) Vārtiem jāpaliek aizvertiem (pilnībai) MVI un superintelektam trīskāršajā krustojumā, un milzīga uzmanība jāvelta AI sistēmām, kas tuvojas šim slieksnim.

Bet tas atstāj daudz jaudīga AI! Mēs varam iegūt milzīgu labumu no gudiem un vispārējiem pasīviem "orākuliem" un šaurām sistēmām, vispārējām sistēmām cilvēka, bet ne pārcilvēciska līmeņa, un tā tālāk. Daudzas tehnoloģiju kompānijas un izstrādātāji aktīvi veido šāda veida rīkus un tiem vajadzētu turpināt; tāpat kā lielākā daļa cilvēku, viņi netieši *pieņem*, ka Vārti uz MVI un superintelektu būs aizvērti.[^125]

Tāpat AI sistēmas var efektīvi kombinēt kompozītās sistēmās, kas uztur cilvēka uzraudzību, vienlaikus uzlabojot spējas. Tā vietā, lai paļautos uz neizprotamiem melnajiem kastēm, mēs varam veidot sistēmas, kur vairāki komponenti — ieskaitot gan AI, gan tradicionālo programmatūru — darbojas kopā veidā, ko cilvēki var uzraudzīt un saprast.[^126] Lai gan daži komponenti varētu būt melnas kastes, neviens nebūtu tuvu MVI — tikai kompozītā sistēma kopumā būtu gan ļoti vispārēja, gan ļoti spējīga, un strikti kontrolējamā veidā.[^127]

#### Nozīmīga un garantēta cilvēka kontrole

Ko nozīmē "strikti kontrolējama"? Galvenā "instrumentālā" ietvara ideja ir atļaut sistēmas — pat ja diezgan vispārējas un jaudīgas — kas garantēti ir nozīmīgas cilvēka kontroles pakļautībā. Ko tas nozīmē? Tas ietver divus aspektus. Pirmais ir projektēšanas apsvērums: cilvēkiem vajadzētu būt dziļi un centrāli iesaistītiem tajā, ko sistēma dara, *nedeleģējot* galvenos svarīgos lēmumus AI. Tas ir lielākās daļas pašreizējo AI sistēmu raksturs. Otrkārt, ciktāl AI sistēmas ir autonomas, tām jābūt garantijām, kas ierobežo to darbības jomu. Garantijai vajadzētu būt *skaitlim*, kas raksturo kaut kā notikšanas varbūtību, un iemeslam šim skaitlim ticēt. Tas ir tas, ko mēs pieprasām citos drošības kritiskajos laukos, kur skaitļi kā "vidējais laiks starp atkazēm" un paredzamais negadījumu skaits tiek aprēķināts, pamatots un publicēts drošības lietās.[^128] Ideālais neveiksmju skaits, protams, ir nulle. Un labā ziņa ir tā, ka mēs varētu nokļūt diezgan tuvu, lai gan izmantojot diezgan atšķirīgas AI arhitektūras, izmantojot *formāli verificētu* programmu īpašību idejas (ieskaitot AI). Ideju, ko plaši pētījuši Omohundro, Tegmarks, Bendžo, Dalrimple un citi (skatīt [šeit](https://arxiv.org/abs/2309.01933) un [šeit](https://arxiv.org/abs/2405.06624)) ir izveidot programmu ar noteiktām īpašībām (piemēram: ka cilvēks to var izslēgt) un formāli *pierādīt*, ka šīs īpašības pastāv. To var izdarīt tagad diezgan īsām programmām un vienkāršām īpašībām, bet AI darbinātas pierādīšanas programmatūras (tuvojošais) spēks varētu to atļaut daudz sarežģītākām programmām (piemēram, iesaiņotājiem) un pat pašam AI. Tas ir ļoti ambiciozs projekts, bet, palielinoties spiedienam uz Vārtiem, mums būs vajadzīgi daži spēcīgi materiāli to nostiprināšanai. Matemātiskais pierādījums var būt viens no nedaudzajiem, kas ir pietiekami stiprs.

#### Kur dodas AI industrija

Ar AI progresu, kas pārvirzīts, instrumentālais AI joprojām būtu milzīga industrija. Aparatūras ziņā, pat ar skaitļošanas jaudas ierobežojumiem, lai novērstu superintelektu, apmācība un secinājumu izdarīšana mazākos modeļos joprojām prasīs milzīgas specializētu komponentu daudzumus. Programmatūras pusē, AI modeļa un skaitļošanas lieluma eksplozijas neitralizēšanai vienkārši vajadzētu novest pie tā, ka kompānijas pārvirza resursus, lai padarītu mazākās sistēmas labākas, daudzveidīgākas un specializētākas, nevis vienkārši tās lielākas.[^129] Būtu pietiekami daudz vietas — vairāk iespējams — visiem tiem peļņas gūšanas Silīcija ielejas jaunuzņēmumiem.[^130]

### Instrumentālais AI var dot (gandrīz) visu, ko cilvēce vēlas, bez MVI

Intelektu, vai tas būtu bioloģisks vai mašīnu, var plaši uzskatīt par spēju plānot un īstenot darbības, kas rada nākotnes, kas vairāk atbilst mērķu kopumam. Kā tāds, intelekts ir milzīgs labums, kad tiek lietots gudri izvēlētu mērķu īstenošanai. Mākslīgais intelekts piesaista milzīgas laika un pūliņu investīcijas lielā mērā tā solīto labumu dēļ. Tāpēc mums jāvaicā: cik lielā mērā mēs joprojām gūtu AI priekšrocības, ja mēs ierobežotu tā nekontrolēto attīstību līdz superintelektam? Atbilde: mēs varētu zaudēt pārsteidzoši maz.

Vispirms apsveriet, ka pašreizējās AI sistēmas jau ir ļoti jaudīgas, un mēs īstenībā esam tikai saskrāpējuši virsmu tam, ko ar tām var izdarīt.[^131] Tās ir diezgan spējīgas "vadīt šovu" "sapratoana" jautājuma vai uzdevuma, kas tām uzdots, un kas būtu nepieciešams, lai atbildētu uz šo jautājumu vai veiktu šo uzdevumu.

Tālāk, daudz no uztraukuma par mūsdienu AI sistēmām ir to vispārības dēļ; bet dažas no spējīgākajām AI sistēmām — piemēram, tās, kas ģenerē vai atpazīst runu vai attēlus, veic zinātnisko prognozēšanu un modelēšanu, spēlē spēles utt. — ir daudz šaurākas un labi "Vārtu iekšpusē" skaitļošanas ziņā.[^132] Šīs sistēmas ir pārcilvēciskas konkrētajos uzdevumos, ko tās veic. Tām var būt robežgadījumu [^133] (vai [izmantojamu](https://arxiv.org/abs/2211.00241)) vājības to šauruma dēļ; tomēr *pilnīgi* šauras vai *pilnībā* vispārējas nav vienīgās pieejamās opcijas: ir daudzas arhitektūras starp tām.[^134]

Šie AI rīki var ievērojami paātrināt citu pozitīvo tehnoloģiju attīstību, bez MVI. Lai labāk veiktu kodolfiziku, mums nav vajadzīga AI, kas ir kodolfizikis — mums tādi ir! Ja mēs vēlamies paātrināt medicīnu, dodiet biologiem, medicīnas pētniekiem un ķīmiķiem spēcīgus rīkus. Viņi tos vēlas un izmantos ar milzīgu labumu. Mums nav vajadzīga serveru ferma, kas pilna ar miljonu digitālo ģēniju; mums ir miljoni cilvēku, kuru ģenialitāti AI var palīdzēt atklāt. Jā, būs vajadzīgs ilgāks laiks, lai iegūtu nemirstību un izārstēšanu no visām slimībām. Tas ir reāls zaudējums. Bet pat vissolītākās veselības inovācijas būtu maz noderīgas, ja AI darbināta nestabilitāte noved pie globāla konflikta vai sabiedrības sabrukuma. Mēs to parādām sev, ka dotu AI iedvesmotiem cilvēkiem iespēju vispirms mēģināt atrisināt problēmu.

Un pieņemsim, ka faktiski ir kāds milzīgs MVI pluss, ko nevar iegūt cilvēcei, izmantojot Vārtu iekšējus rīkus. Vai mēs to zaudējam, *nekad* neveidojot MVI un superintelektu? Svērojot riskus un atlīdzību šeit, ir milzīgs asimetrisks labums gaidīšanā pret steigšanos: mēs varam gaidīt, līdz to var izdarīt garantēti drošā un labvēlīgā veidā, un gandrīz visi joprojām varēs gūt labumu; ja mēs steidzamies, tas varētu būt — OpenAI izpilddirektora Sama Altmana vārdiem — [gaismu izslēgšana *mums visiem*.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

Bet ja ne-MVI rīki ir potenciāli tik spēcīgi, vai mēs varam tos pārvaldīt? Atbilde ir skaidra... varbūt.

### Instrumentālās AI sistēmas ir (iespējams, principā) pārvaldāmas

Bet tas nebūs viegli. Pašreizējās vismodernākās AI sistēmas var ievērojami dot spēku cilvēkiem un institūcijām viņu mērķu sasniegšanā. Tas ir, vispārējā ziņā, laba lieta! Tomēr ir dabiskas šādu sistēmu rīcībā esamības dinamikas — pēkšņi un bez daudz laika sabiedrībai pielāgoties — kas piedāvā nopietnus riskus, kas jāpārvalda. Ir vērts apspriest dažas lielākās šādu risku klases un kā tās varētu mazināt, pieņemot Vārtu aizvēršanu.

Viena risku klase ir tāda, ka augstas jaudas instrumentālais AI ļauj piekļuvi zināšanām vai spējām, kas iepriekš bija saistītas ar personu vai organizāciju, padarot augstas spējas plus augstas uzticības kombināciju pieejamu ļoti plašam dalībnieku spektram. Šodien, ar pietiekami daudz naudas ļaunprātīgas nolūka persona varētu nolīgt ķīmiķu komandu, lai projektētu un ražotu jaunus ķīmiskos ieročus — bet nav tik ļoti viegli iegūt šo naudu vai atrast/salikt komandu un pārliecināt viņus darīt kaut ko diezgan skaidri nelikumīgu, neētisku un bīstamu. Lai novērstu AI sistēmu šādas lomas spēlēšanu, pašreizējo metožu uzlabojumi varētu labi pietikt,[^135] kamēr vien visas šīs sistēmas un piekļuve tām tiek atbildīgi pārvaldītas. No otras puses, ja jaudīgas sistēmas tiek izlaistas vispārējai lietošanai un modificēšanai, visi iebūvētie drošības pasākumi, iespējams, ir noņemami. Tātad, lai izvairītos no riskiem šajā klasē, būs nepieciešami stingri ierobežojumi attiecībā uz to, kas var tikt publiskts — līdzīgi ierobežojumiem par kodol-, sprāgstvielu un citu bīstamu tehnoloģiju detaļām.[^136]

Otrā risku klase rodas no mašīnu mērogošanas, kas darbojas kā vai iztēlo cilvēkus. Kaitējuma līmenī atsevišķiem cilvēkiem šie riski ietver daudz efektīvākas krāpšanas, spam un makšķerēšanas, un bezpiekrišanas dziļviltojumu izplatīšanos.[^137] Kolektīvā līmenī tie ietver pamata sociālo procesu traucēšanu, piemēram, publisku diskusiju un debašu, mūsu sabiedrības informācijas un zināšanu savākšanas, apstrādes un izplatīšanas sistēmu, un mūsu politisko izvēļu sistēmu. Šo risku mazināšana, iespējams, ietvers (a) likumus, kas ierobežo cilvēku atdarināšanu ar AI sistēmām, un padara atbildīgus AI izstrādātājus, kas izveido sistēmas, kas ģenerē šādus atdarinājumus, (b) ūdenszīmju un izcelsmes sistēmas, kas identificē un klasificē (atbildīgi) ģenerētu AI saturu, un (c) jaunas sociāli-tehniskas epistemiskas sistēmas, kas var izveidot uzticamu ķēdi no datiem (piemēram, kamerām un ierakstiem) caur faktiem, izpratni un labiem pasaules modeļiem.[^138] Tas viss ir iespējams, un AI var palīdzēt ar dažām tā daļām.

Trešais vispārējais risks ir tāds, ka ciktāl daži uzdevumi tiek automatizēti, cilvēki, kas pašlaik dara šos uzdevumus, var būt ar mazāku finansiālu vērtību kā darbaspēks. Vēsturiski uzdevumu automatizēšana ir padarījusi lietas, ko iespējo šie uzdevumi, lētākas un bagātīgākas, vienlaikus sagrupējot cilvēkus, kas iepriekš darīja šos uzdevumus, tajos, kas joprojām ir iesaistīti automatizētajā versijā (parasti augstākā prasmē/algā), un tajos, kuru darbaspēks ir mazāk vērts vai maz vērts. Kopumā ir grūti prognozēt, kuros sektoros būs vajadzīgs vairāk pret mazāk cilvēku darbaspēka lielākā, bet efektīvākā sektorā. Paralēli automatizācijas dinamika mēdz palielināt nevienlīdzību un vispārējo produktivitāti, samazināt noteiktu preču un pakalpojumu izmaksas (efektivitātes pieauguma dēļ) un palielināt citu izmaksas (izmaksu slimības dēļ [cost disease](https://en.wikipedia.org/wiki/Baumol_effect)). Tiem, kas atrodas nevienlīdzības pieauguma nevēlamā pusē, ir dziļi neskaidrs, vai noteiktu preču un pakalpojumu izmaksu samazinājums pārspēj citu palielināšanos un noved pie kopējās lielākas labklājības. Tātad kā tas būs ar AI? Tā kā ir relatīvi viegli aizstāt cilvēku intelektuālo darbu ar vispārējo AI, mēs varam sagaidīt ātru šīs versiju ar cilvēkiem konkurētspējīgu vispārējo AI.[^139] Ja mēs aizvertu Vārtus MVI, daudz mazāk darbavietu tiks vairumā aizstāts ar AI aģentiem; bet milzīga darbaspēka pārvietošana joprojām ir iespējama vairāku gadu periodā.[^140] Lai izvairītos no plaši izplatītas ekonomiskās ciešanas, iespējams, būs nepieciešams ieviest gan kādu universālu pamata aktīvu vai ienākumu formu, gan arī izveidot kultūras maiņu uz cilvēkcentriska darba novērtēšanu un atlīdzināšanu, kas ir grūtāk automatizējama (nevis redzot, kā darba cenas krītas pieaugošā darbaspēka dēļ, kas izstumts no citām ekonomikas daļām.) Citas konstrukcijas, piemēram, ["datu cieņas"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society) (kurā cilvēku apmācības datu ražotājiem automātiski tiek piešķirti honorāri par vērtību, ko rada šie dati AI) var palīdzēt. AI automatizācijai ir arī otrais potenciālais negatīvais efekts, kas ir *nepiedienīga* automatizācija. Kopā ar pielietojumiem, kur AI vienkārši dara sliktāku darbu, tas ietvertu arī tos, kur AI sistēmas, iespējams, pārkāpj morālus, ētiskus vai juridiskus priekšrakstus — piemēram, dzīvības un nāves lēmumos un tiesas lietās. Tie jāapstrādā, piemērojot un paplašinot mūsu pašreizējos juridiskos ietvarus.

Visbeidzot, nozīmīgs Vārtu iekšējā AI drauds ir tā izmantošana personalizētā pārliecināšanā, uzmanības pievēršanā un manipulācijās. Mēs esam redzējuši sociālajos medijos un citas tiešsaistes platformās dziļi iesakņojušās uzmanības ekonomikas (kur tiešsaistes pakalpojumi nikni cīnās par lietotāju uzmanību) un ["uzraudzības kapitālisma"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism) sistēmu augšanu (kurā lietotāju informācija un profilēšana tiek pievienota uzmanības preču veidošanai.) Ir gandrīz noteikti, ka vairāk AI tiks likts abu pakalpojumā. AI jau ir plaši izmantots atkarību radošos barošanas algoritmos, bet tas attīstīsies par atkarību radošu AI ģenerētu saturu, pielāgotu, lai to kompulsīvi patērētu viena persona. Un šīs personas ievads, atbildes un dati tiks baroti uzmanības/reklāmas mašīnā, lai turpinātu ļauno ciklu. Tāpat, kā tehnoloģiju kompāniju nodrošinātie AI palīgi kļūst par saskarni vairāk tiešsaistes dzīvei, viņi, iespējams, aizstās meklētājprogrammas un barotnes kā mehānismus, ar kuriem notiek pārliecināšana un klientu naudas gūšana. Mūsu sabiedrības nespēja līdz šim kontrolēt šīs dinamikas neliecina par labu. Daļa no šīs dinamikas var tikt mazināta ar noteikumiem par privātumu, datu tiesībām un manipulācijām. Vairāk nonākot pie problēmas saknes, var būt nepieciešami atšķirīgi skatījumi, piemēram, uzticīgu AI asistentu (apspriests tālāk.)

Šīs diskusijas galvenā doma ir cerība: Vārtu iekšējās rīku bāzētās sistēmas — vismaz kamēr tās paliek salīdzināmas spēkā un spējā ar šodienas vismodernākajām sistēmām — ir iespējams pārvaldīt, ja ir griba un koordinācija to darīt. Pieklājīgas cilvēku institūcijas, AI rīku iedvesmotas,[^141] var to izdarīt. Mēs varētu arī neizdarīt to. Bet grūti redzēt, kā atļauja spēcīgākām sistēmām palīdzētu — izņemot to, ka nodotu tās vadīšanā un cerētu uz labāko.

### Nacionālā drošība

Sacīkstes par AI pārākumu — ko virza nacionālā drošība vai citi motīvi — virza mūs uz nekontrolētām spēcīgām AI sistēmām, kas mēdz absorbēt, nevis piešķirt spēku. MVI sacīkstes starp ASV un Ķīnu ir sacīkstes par to noteikšanu, kura tauta superintelektu iegūst pirmā.

Tātad ko vajadzētu darīt tiem, kas atbild par nacionālo drošību? Valdībām ir spēcīga pieredze kontrolējamu un drošu sistēmu veidošanā, un tām vajadzētu divkāršot pūliņus to darīšanā AI jomā, atbalstot tāda veida infrastruktūras projektus, kas vislabāk izdodas, kad darīti mērogā un ar valdības apstiprināšanu.

Tā vietā, lai bezatbildīgi "Manhatanas projekts" uz MVI,[^142] ASV valdība varētu sākt Apollo projektu kontrolējamām, drošām, uzticamām sistēmām. Tas varētu ietvert, piemēram:

- Lielu programmu (a) izstrādāt uz čipa aparatūras drošības mehānismus un (b) infrastruktūru, lai pārvaldītu jaudīga AI skaitļošanas pusi. Tie varētu balstīties uz ASV [CHIPS likumu](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) un [eksporta kontroles režīmu](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion).
- Liela mēroga iniciatīvu izstrādāt formālās verifikācijas tehnikas, lai noteiktas AI sistēmu funkcijas (piemēram, izslēgšanas poga) varētu tikt *pierādītas* kā esošas vai neesoša. Tas var izmantot pašu AI pierādījumu īpašību izstrādei.
- Nacionāla mēroga pūliņus izveidot programmatūru, kas ir verificējami droša, ko darbina AI rīki, kas var pārkodēt esošo programmatūru verificējami drošos ietvaros.
- Nacionālu investīciju projektu zinātniskajā progresā, izmantojot AI,[^143] darbojot kā partnerību starp DOE, NSF un NIH.

Vispārējā ziņā ir milzīga uzbrukuma virsma mūsu sabiedrībā, kas padara mūs ievainojamus pret riskiem no AI un tā ļaunprātīgas izmantošanas. Aizsardzība no dažiem no šiem riskiem prasīs valdības izmēra investīcijas un standartizēšanu. Tie nodrošinātu daudz vairāk drošības, nekā lejot benzīnu uz sacīkšu uz MVI uguns. Un, ja AI tiks iebūvēts ieročos un komandu-kontroles sistēmās, ir būtiski, lai AI būtu uzticams un drošs, kas pašreizējais AI vienkārši nav.

### Varas koncentrācija un tās mazināšana

Šis esejs ir koncentrējies uz cilvēka AI kontroles ideju un tās iespējamo neveiksmi. Bet cits derīgs objektīvs, caur kuru skatīties uz AI situāciju, ir caur *varas koncentrāciju.* Ļoti jaudīga AI izstrāde draud koncentrēt varu vai nu ļoti nedaudzos un ļoti lielos korporatīvos rokās, kas to ir izstrādājušas un kontrolēs, vai valdībās, kas izmanto AI kā jaunu līdzekli savu varas un kontroles uzturēšanai, vai pašās AI sistēmās. Vai kādā no šiem gadījumu bezbēdīgā maisījumā augstāk minētajos. Jebkurā no šiem gadījumiem lielākā daļa cilvēces zaudē varu, kontroli un rīcībspēju. Kā mēs varētu cīnīties pret to?

Pašs pirmais un visāistākais solis, protams, ir Vārtu aizvēršana gudākiem par cilvēkiem MVI un superintelektam. Tie skaidri var tieši aizstāt cilvēkus un cilvēku grupas. Ja tie ir korporatīvā vai valdības kontrolē, tie koncentrēs varu šajās korporācijās vai valdībās; ja tie ir "brīvi", tie koncentrēs varu sevī. Tātad pieņemsim, ka Vārti ir aizvērti. Tad ko?

Viens ierosinātais varas koncentrācijas risinājums ir "atvērtā koda" AI, kur modeļa svari ir brīvi vai plaši pieejami. Bet, kā minēts iepriekš, kad modelis ir atvērts, lielākā daļa drošības pasākumu vai drošības barjeru var tikt (un parasti tiek) noņemtas. Tātad ir asa spriedze starp no vienas puses decentralizāciju un no otras puses drošību, aizsardzību un cilvēka AI sistēmu kontroli. Ir arī iemesli būt skeptiskiem, ka atvērtie modeļi paši par sevi nozīmīgi cīnīsies pret varas koncentrāciju AI jomā vairāk nekā tie ir darījuši operētājsistēmās (joprojām dominē Microsoft, Apple un Google, neraugoties uz atvērtajām alternatīvām).[^144]

Tomēr var būt veidi, kā kvadrēt šo apli — centralizēt un mazināt riskus, vienlaikus decentralizējot spējas un ekonomisko atlīdzību. Tas prasa pārdomāt gan to, kā AI tiek izstrādāts, gan to, kā tā labumi tiek sadalīti.

Jauni publiska AI izstrādes un īpašumtiesību modeļi palīdzētu. Tas varētu pieņemt vairākas formas: valdības izstrādāts AI (pakļauts demokrātiskai uzraudzībai),[^145] bezpeļņas AI izstrādes organizācijas (piemēram, Mozilla pārlūkprogrammām), vai struktūras, kas ļauj ļoti plašu īpašumtiesību un pārvaldību. Galvenais ir tāds, ka šīs institūcijas būtu skaidri uzticētas kalpot sabiedrības interesēm, vienlaikus darbojoties stingru drošības ierobežojumu ietvaros.[^146] Labi veidoti regulatīvie un standartu/sertifikācijas režīmi arī būs būtiski, lai AI produkti, ko piedāvā rosīgā tirgus, paliktu patiesi noderīgi, nevis izmantojami pret to lietotājiem.

Ekonomiskās varas koncentrācijas ziņā mēs varam izmantot izcelsmes izsekošanu un "datu cieņu", lai nodrošinātu, ka ekonomiskie labumi plūst plašāk. Jo īpaši lielākā daļa AI spēka tagad (un nākotnē, ja mēs uzturēsim Vārtus aizvertus) rod cēloni no cilvēku ģenerētiem datiem, vai tiešiem apmācības datiem, vai cilvēka atgriezeniskās saites. Ja AI kompānijām tiktu prasīts godīgi kompensēt datu sniedzējus,[^147] tas varētu vismaz palīdzēt sadalīt ekonomisko atlīdzību plašāk. Ārpus tā, cits modelis varētu būt sabiedriska īpašumtiesība uz būtiskām lielo AI kompāniju daļām. Piemēram, valdības, kas spēj apliksnes AI kompānijas ar nodokļiem, varētu investēt ieņēmumu daļu suverēnā bagātības fondā, kas tur akcijas kompānijās un maksā dividendes iedzīvotājiem.[^148]

Šajos mehānismos ir būtiski izmantot paša AI spēku, lai palīdzētu sadalīt varu labāk, nevis vienkārši cīnīties pret AI darbināto varas koncentrāciju, izmantojot ne-AI līdzekļus. Viens spēcīgs pieejams būtu caur labi projektētiem AI asistentiem, kas darbojas ar īstu fiduciāro pienākumu pret saviem lietotājiem — liekot lietotāju intereses pirmajā vietā, īpaši virs korporatīvo sniedzēju interesēm.[^149] Šiem asistentiem jābūt patiesi uzticamiem, tehniski kompetentiem, tomēr atbilstoši ierobežotiem, balstoties uz lietošanas gadījumu un riska līmeni, un plaši pieejamiem visiem caur sabiedriskajiem, bezpeļņas vai sertificētajiem peļņas kanāliem. Tāpat kā mēs nekad neakceptētu cilvēku asistentu, kas slepeni strādā pret mūsu interesēm citai pusei, mums nevajadzētu akceptēt AI asistentus, kas uzrauga, manipulē vai izvilka vērtību no saviem lietotājiem korporatīva labuma dēļ.

Tāda pārveide fundamentāli mainītu pašreizējo dinamiku, kur indivīdi paliek vieni sarunāties ar milzīgajiem (AI darbinātajiem) korporatīvajiem un birokrātiskajiem mehānismiem, kas prioritizē vērtības izvilkšanu pār cilvēka labklājību. Lai gan ir daudzi iespējamie pieejas AI darbinātas varas plašākai pārsadalei, neviens neradīsies pēc noklusējuma: tiem jābūt apzināti izveidotiem un pārvaldītiem ar mehānismiem kā fiduciārās prasības, sabiedriskās nodrošināšana un pakāpeniski piekļuves, balstoties uz risku.

Pieejas varas koncentrācijas mazināšanai var saskarties ar būtiskiem pretvējiem no esošajiem spēkiem.[^150] Bet ir ceļi uz AI izstrādi, kas neprasa izvēlēties starp drošību un koncentrētu varu. Veidojot pareizas institūcijas tagad, mēs varētu nodrošināt, ka AI labumi tiek plaši dalīti, kamēr tā riski tiek rūpīgi pārvaldīti.

### Jaunas pārvaldības un sociālās struktūras

Mūsu pašreizējās pārvaldības struktūras cīnās: tās ir lēnas reaģēt, bieži ietekmētas īpašo interešu un [arvien vairāk sabiedrības neuzticētas.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) Tomēr tas nav iemesls tās pamest — gluži pretēji. Dažas institūcijas var būt vajadzīgs aizstāt, bet plašāk mums vajag jaunus mehānismus, kas var uzlabot un papildināt mūsu esošās struktūras, palīdzot tām funkcionēt labāk mūsu ātri attīstošajā pasaulē.

Daudz mūsu institucionālās vājības rodas ne no formālām valdības struktūrām, bet no degradētām sociālām institūcijām: mūsu sistēmām koplietotās izpratnes veidošanai, darbību koordinēšanai un nozīmīgu diskursu vadīšanai. Līdz šim AI ir paātrināja šo degradāciju, applūdinot mūsu informācijas kanālus ar ģenerētu saturu, virzienu mūs uz vispolari izzējošāko un dalošāko saturu, un padarot grūtāku atšķirt patiesību no fikcijas.

Bet AI faktiski varētu palīdzēt atjaunot un stiprināt šīs sociālās institūcijas. Apsverot trīs būtiskas jomas:

Pirmkārt, AI varētu palīdzēt atjaunot uzticību mūsu epistemiskajām sistēmām — mūsu veidiem, kā zināt, kas ir patiess. Mēs varētu izstrādāt AI darbinātas sistēmas, kas izseko un verificē informācijas izcelsmi, no neapstrādātiem datiem caur analīzi līdz secinājumiem. Šīs sistēmas varētu kombinēt kriptogrāfisko verifikāciju ar sarežģītu analīzi, lai palīdzētu cilvēkiem saprast ne tikai to, vai kaut kas ir patiess, bet arī to, kā mēs zinām, ka tas ir patiess.[^151] Uzticīgi AI asistenti varētu tikt uzticēti sekot detaļām, lai nodrošinātu, ka tās noder.

Otrkārt, AI varētu iespējot jaunas liela mēroga koordinācijas formas. Daudzi no mūsu akūtākajiem problēmiem — no klimata maiņas līdz antibiotisku rezistencei — ir fundamentāli koordinācijas problēmas. Mēs [esam iestrēguši situācijās, kas ir sliktākas, nekā tās varētu būt gandrīz visiem](https://equilibriabook.com/), jo neviens indivīds vai grupa nevar atļauties izdarīt pirmo gājienu. AI sistēmas varētu palīdzēt, modelējot sarežģītas stimulu struktūras, identificējot dzīvotspējīgus ceļus uz labākiem rezultātiem un atvieglojot uzticības veidošanas un saistību mehānismus, kas nepieciešami, lai tur nokļūtu.

Varbūt visintriģējošāk, AI varētu iespējot pilnīgi jaunas sociālā diskursa formas. Iedomājieties spēju "runāt ar pilsētu" [^152] — ne tikai skatīt statistiku, bet vest nozīmīgu dialogu ar AI sistēmu, kas apstrādā un apkopo miljonu iemītnieku uzskatus, pieredzes, vajadzības un aspirācijas. Vai apsveriet, kā AI varētu atvieglot īstu dialogu starp grupām, kas pašlaik runā garām viena otrai, palīdzot katrai pusei labāk saprast otras faktiskās bažas un vērtības, nevis to kariktūras viena otru.[^153] Vai AI varētu piedāvāt prasmīgu, ticami neitrālu starpniecību strīdos starp cilvēkiem vai pat lielām cilvēku grupām (kas visi varētu mijiedarboties ar to tieši un individuāli!) Pašreizējais AI ir pilnīgi spējīgs veikt šo darbu, bet rīki, lai to darītu, neradīsies paši no sevis, vai caur tirgus stimuliem.

Šīs iespējas varētu skanēt utopiski, īpaši ņemot vērā AI pašreizējo lomu diskursa un uzticības degradēšanā. Bet tas ir tieši tāpēc, kāpēc mums jāattīsta aktīvi šīs pozitīvās lietojuma. Aizverot Vārtus nekontrolējamam MVI un prioritizējot AI, kas uzlabo cilvēka rīcībspēju, mēs varam virzīt tehnoloģisko progresu uz nākotni, kur AI kalpo kā spēks iedvesmošanai, izturībai un kolektīvai virzībai.


[^124]: Tāpat, palikt projām no trīskāršās krustošanās, diemžēl nav tik vienkārši, kā varētu vēlēties. Spēju ļoti cietu grūšana jebkurā no trim aspektiem mēdz to palielināt citos. Jo īpaši var būt grūti izveidot ļoti vispārēju un spējīgu intelektu, ko nevar viegli padarīt autonomu. Viena pieeja ir apmācīt modeļus ["miopiskus"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) sistēmas ar sabojātu plānošanas spēju. Cita būtu koncentrēties uz inženierijas tīrām ["orākula"](https://arxiv.org/abs/1711.05541) sistēmām, kas izvairītos no atbildēšanas uz darbības orientētiem jautājumiem.

[^125]: Daudzas kompānijas nespēj saprast, ka arī viņas galu galā tiktu aizstātas ar MVI, pat ja tas aizņemtu ilgāku laiku — ja viņas to darītu, viņas varētu mazliet mazāk grūst uz tiem Vārtiem!

[^126]: AI sistēmas varētu sazināties efektīvākos, bet mazāk saprotamos veidos, bet cilvēka izpratnes uzturēšanai vajadzētu būt prioritātei.

[^127]: Šo modulārā, interpretējamā AI ideju detalizēti ir izstrādājuši vairāki pētnieki; skatiet, piemēram, ["Visaptverošo AI pakalpojumu"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) modeli no Drexler, Dalrimple un citu ["Atvērto aģentūru arhitektūru"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai). Lai gan šādas sistēmas varētu prasīt vairāk inženierpūliņu nekā monoliski neironu tīkli, kas apmācīti ar masīvu skaitļošanu, tas ir tieši tas, kur skaitļošanas ierobežojumi palīdz — padarot drošāko, caurspīdīgāko ceļu arī praktiskāko.

[^128]: Par drošības lietām vispārējā ziņā skatiet [šo rokasgrāmatu](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). Attiecībā uz AI konkrēti skatiet [Wasil et al.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [Clymer et al.](https://arxiv.org/abs/2403.10462), [Buhl et al.](https://arxiv.org/abs/2410.21572), un [Balesni et al.](https://arxiv.org/abs/2411.03336)

[^129]: Mēs faktiski jau redzam šo tendenci, ko virza tikai augstās secinājumu izdarīšanas izmaksas: mazāki un specializētāki modeļi, kas "destilēti" no lielākiem un spējīgi darboties uz mazāk dārgām aparatūrām.

[^130]: Es saprotu, kāpēc tie, kas aizraujas ar AI tehnoloģiju ekosistēmu, var iebilst pret to, ko viņi uzskata par apgrūtinošu regulējumu savai industrijā. Bet man ir tieši mulsinoši, kāpēc, piemēram, riska kapitāla investors gribētu atļaut nekontrolētu attīstību līdz MVI un superintelektam. Šīs sistēmas (un kompānijas, kamēr tās paliek kompānijas kontrolē) *apēdīs visus jaunuzņēmumus kā uzkodu*. Iespējams, pat *agrāk* nekā ēst citas nozares. Ikvienam, kas investēts plaukstošā AI ekosistēmā, vajadzētu prioritāri nodrošināt, ka MVI attīstība nenoved pie monopolizācijas ar dažiem dominējošiem spēlētājiem.

[^131]: Kā ekonomists un bijušais Deepmind pētnieks Maikls Vebbs [teica](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/), "Es domāju, ja mēs šodien apturētu visu lielāku valodu modeļu attīstību, tātad GPT-4 un Claude un jebko, un tie ir pēdējās lietas, ko mēs apmācām šajā lielumā — tātad mēs atļaujam daudz vairāk iterāciju uz šā lieluma lietām un visu veidu precizēšanu, bet neko lielāku par to, nav lielāku sasniegumu — tikai tas, kas mums ir šodien, es domāju, ir pietiekami, lai darbinātu 20 vai 30 gadus neticama ekonomiska augšana."

[^132]: Piemēram, DeepMind alphafold sistēma izmantoja tikai 100,000to daļu no GPT-4 FLOP skaita.

[^133]: Pašbraucošo automašīnu grūtības šeit ir svarīgi atzīmēt: lai gan nomināli šaurs uzdevums un sasniedzams ar pamatīgu uzticamību ar salīdzinoši mazām AI sistēmām, plašas reālās pasaules zināšanas un izpratne ir nepieciešama, lai iegūtu uzticamību līmenī, kas nepieciešams tik drošības kritiskā uzdevumā.

[^134]: Piemēram, dotam skaitļošanas budžetam, mēs, iespējams, redzētu GPAI modeļus, kas iepriekš apmācīti (piemēram) puspusē no tā budžeta, un otra puse izmantota, lai apmācītu augstu spēju šaurākā uzdevumu spektrā. Tas dotu pārcilvēciskas šauras spējas, ko atbalsta tuvu cilvēcīgs vispārējais intelekts.

[^135]: Pašreizējā dominējošā saskaņošanas tehnika ir "stiprināšanas mācīšanās ar cilvēka atgriezenisko saiti" [(RLHF)](https://arxiv.org/abs/1706.03741) un izmanto cilvēka atgriezenisko saiti, lai izveidotu atlīdzības/soda signālu AI modeļa stiprināšanas mācīšanās. Šī un līdzīgas tehnikas kā [konstitutcionālais AI](https://arxiv.org/abs/2212.08073) darbojas pārsteidzoši labi (lai gan tām trūkst noturības un tās var apiet ar mērenu pūliņu.) Turklāt pašreizējie valodu modeļi parasti ir pietiekami kompetenti vesela saprāta spriedumu veidošanā, ka viņi neizdarīs muļķīgas morālas kļūdas. Tas ir kaut kas kā saldā vieta: pietiekami gudri, lai saprastu, ko cilvēki vēlas (ciktāl to var definēt), bet ne pietiekami gudri, lai plānotu sarežģītus krāpšanas vai radītu milzīgu kaitējumu, kad viņi to saprot nepareizi.

[^136]: Ilgtermiņā jebkurš AI spēju līmenis, kas tiek izstrādāts, iespējams, izplatīsies, jo galu galā tas ir programmatūra, un noderīga. Mums būs jābūt robustiem mehānismiem, lai aizstāvētos pret riskiem, ko šādas sistēmas rada. Bet *mums tas nav tagad*, tāpēc mums jābūt ļoti mērītiem tajā, cik daudz jaudīgu AI modeļu drīkst izplatīties.

[^137]: Lielākā daļa no tiem ir bezpiekrišanas pornogrāfiski dziļviltojumi, ieskaitot nepilngadīgo.

[^138]: Daudzi no šādu risinājumu ingredientiem pastāv "bots-vai-ne" likumu formā (ES AI likumā starp citiem vietām), [nozares izcelsmes izsekošanas tehnoloģijās](https://c2pa.org/), [inovatīvos ziņu agregātoros](https://www.improvethenews.org/), prognožu [agregātoros](https://metaculus.com/) un tirgos utt.

[^139]: Automatizācijas vilnis var nesekot iepriekšējiem modeļiem tādā ziņā, ka salīdzinoši *augstas* prasmes uzdevumi, piemēram, kvalitātes rakstīšana, likuma interpretēšana vai medicīnisko padomu došana, var būt tikpat daudz vai pat vairāk ievainojami pret automatizāciju nekā zemākas prasmes uzdevumi.

[^140]: Par rūpīgu MVI ietekmes uz algām modelēšanu skatiet ziņojumu [šeit](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek), un asiņainas detaļas [šeit](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0), no Anton Korinek un līdzstrādniekiem. Viņi atklāj, ka, jo vairāk darbavietu daļas tiek automatizētas, produktivitāte un algas palielinās — līdz punktam. Kad *pārāk* daudz tiek automatizēts, produktivitāte turpina palielināties, bet algas krīt, jo cilvēki tiek aizstāti vairumā ar efektīvu AI. Tāpēc Vārtu aizvēršana ir tik noderīga: mēs iegūstam produktivitāti bez pazudušo cilvēku algām.

[^141]: Ir daudzi veidi, kā AI var tikt izmantots kā un palīdzēt veidot "aizsargājošas" tehnoloģijas, lai padarītu aizsardzību un pārvaldību noturīgāku. Skatīt [šo](https://vitalik.eth.limo/general/2025/01/05/dacc2.html) ietekmīgo ierakstu, kas apraksta šo "D/acc" programmu.

[^142]: Diezgan ironiski, ASV Manhetanas projekts, iespējams, darītu maz, lai paātrinātu laikus uz MVI — cilvēku un fiskālo investīciju uzstādījums AI progresā jau ir nospiests uz 11. Primārie rezultāti būtu iedvesmot līdzīgu projektu Ķīnā (kas izcili pārvalda nacionāla līmeņa infrastruktūras projektus), padarīt starptautiskās vienošanās, kas ierobežo AI risku, daudz grūtākas, un satraukt citus ASV ģeopolitiskos pretiniekus, piemēram, Krieviju.

[^143]: ["Nacionālā AI pētniecības resursa"](https://nairrpilot.org/) programma ir labs pašreizējais solis šajā virzienā un vajadzētu tikt paplašināta.

[^144]: Skatīt [šo analīzi](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807) par dažādajām "atvērtā" nozīmēm un sekām tehnoloģiju produktos un kā dažas ir novedušas uz vairāk, nevis mazāk, dominances iestipringājumu.

[^145]: ASV plāni [Nacionālajam AI pētniecības resursam](https://nairratdoe.ornl.gov/) un nesenie [Eiropas AI fonda](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/) laišana ir interesanti soļi šajā virzienā.

[^146]: Izaicinājums šeit nav tehnisks, bet institucionāls — mums steidzami vajag reālās pasaules piemērus un eksperimentus tajā, kā varētu izskatīties sabiedrības interešu AI attīstība.

[^147]: Tas ir pretrunā ar pašreizējiem lielo tehnoloģiju uzņēmumu biznesa modeļiem un prasītu gan juridiskas darbības, gan jaunas normas.

[^148]: Tikai dažas valdības varēs to darīt. Radikālāka ideja ir [universāls šāda veida fonds, kas ir visu cilvēku kopīgajā īpašumā.](https://futureoflife.org/project/the-windfall-trust/)

[^149]: Par šīs lietas ilgu izklāstu skatīt [šo rakstu](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338) par AI uzticību. Diemžēl AI asistentu noklusējuma trajektorija, iespējams, būs tā, kur viņi arvien vairāk ir neuzticīgi.

[^150]: Diezgan ironiski, daudzi esošie spēki arī ir AI atbalstītas vara atņemšanas riskā; bet viņiem var būt grūti to uztvert, līdz un ja vien process nekļūst diezgan tālu.

[^151]: Daži interesanti pūliņi šajā virzienā ir pārstāvēti ar [c2pa koalīciju](https://c2pa.org/) par kriptogrāfisko verifikāciju; [Verity](https://www.improvethenews.org/) un [Ground news](https://ground.news/) par labāku ziņu epistemiku; un [Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com) un prognožu tirgos par diskursa balstīšanu uz pārbaudāmām prognozēm.

[^152]: Skatīt [šo](https://talktothecity.org/) aizraujošo pilotprojektu.

[^153]: Skatīt [Kialo](https://www.kialo-edu.com/), un [Kolektīvā intelekta projekta](https://www.cip.org/) pūliņus par dažiem piemēriem.

## 10. nodaļa - Izvēle, kas mums priekšā

Lai saglabātu mūsu cilvēcisko nākotni, mums jāizvēlas aizvērt Vārtus uz MVI un superintelektu.

Pēdējoreiz cilvēce dalīja Zemi ar citām prātīgām būtnēm, kas runāja, domāja, veidoja tehnoloģijas un risināja universālus uzdevumus, bija pirms 40 000 gadiem ledus laikmeta Eiropā. Šīs citas prātīgās būtnes izmira – pilnībā vai daļēji mūsējo darbību rezultātā.

Tagad mēs atkal ienākam šādā laikmetā. Mūsu kultūras un tehnoloģiju visattīstītākie produkti – datu kopas, kas veidotas no visa mūsu interneta informācijas krājuma, un 100 miljardus elementu saturoši mikroshēmas, kas ir sarežģītākās tehnoloģijas, kādas jebkad esam radījuši – tiek apvienotas, lai radītu uz dzīvi progresīvas universālas AI sistēmas.

Šo sistēmu izstrādātāji cenšas tās attēlot kā rīkus cilvēku iespēju paplašināšanai. Un patiešām tās varētu būt. Bet neliecieties maldos: mūsu pašreizējais kurss ved uz arvien spēcīgāku, mērķtiecīgu, lēmumu pieņemošu un universāli spējīgu digitālo aģentu veidošanu. Tie jau tagad darbojas tikpat labi kā daudzi cilvēki plašā intelektuālo uzdevumu spektrā, strauji uzlabojas un veicina savu turpmāko attīstību.

Ja vien šis kurss nemainās vai nesaskaras ar neparedzētu šķērsli, mums drīz – gados, nevis gadu desmitos – būs digitālās inteliģences, kas ir bīstami spēcīgas. Pat *labākajā* scenārijā tās nesītu lielu ekonomisku labumu (vismaz dažiem no mums), bet tikai par cenu dziļiem sabiedrības satricinājumiem un cilvēku aizstāšanai vairumā svarīgāko lietu, ko darām: šīs mašīnas domātu mūsu vietā, plānotu mūsu vietā, lemtu mūsu vietā un radītu mūsu vietā. Mēs būtu izlutināti, bet izlutināti bērni. Daudz ticamāk, ka šīs sistēmas aizstātu cilvēkus gan pozitīvajās, *gan* negatīvajās lietās, ko darām, ieskaitot ekspluatāciju, manipulācijas, vardarbību un karu. Vai mēs spēsim izdzīvot AI pastiprinātās šo lietu versijas? Visbeidzot, ir vairāk nekā ticami, ka lietas nenorisinātos labi: ka samērā drīz mēs tiktu aizstāti ne tikai tajā, ko darām, bet arī tajā, kas mēs *esam* – kā civilizācijas un nākotnes arhitekti. Pajautājiet neandertāliem, kā tas beidzas. Varbūt arī viņiem mēs kādu laiku sniedzām papildu rotlietas.

*Mums tas nav jādara.* Mums ir ar cilvēkiem konkurētspējīgs AI, un nav vajadzības veidot AI, ar kuru mēs *nevaram* konkurēt. Mēs varam radīt pārsteidzošus AI rīkus, neveidojot pēcteci sugu. Priekšstats, ka MVI un superintelekts ir neizbēgami, ir *izvēle, kas maskējas par likteni*.

Noteikdami dažus stingrus, globālus ierobežojumus, mēs varam saglabāt AI vispārējo spēju aptuveni cilvēka līmenī, vienlaikus gūstot labumu no datoru spējas apstrādāt datus veidā, kādā mēs to nevaram, un automatizēt uzdevumus, ko neviens no mums nevēlas darīt. Tie joprojām radītu daudzus riskus, bet, ja labi izstrādāti un pārvaldīti, būtu milzīgs ieguvums cilvēcei – no medicīnas līdz pētniecībai un patērētāju produktiem.

Ierobežojumu noteikšana prasītu starptautisku sadarbību, bet mazāk, nekā varētu domāt, un šie ierobežojumi joprojām atstātu daudz vietas milzīgai AI un AI aparatūras industrijiai, kas fokusētos uz lietojumprogrammām, kas uzlabo cilvēku labklājību, nevis uz neapstrādātu varas meklējumiem. Un ja ar stingrām drošības garantijām un pēc jēgpilna globāla dialoga mēs izlemsim iet tālāk, šī iespēja turpinās būt mūsu rīcībā.

Cilvēcei *jāizvēlas* aizvērt Vārtus uz MVI un superintelektu.

Lai saglabātu nākotni cilvēcīgu.

### Piezīme no autora

Paldies, ka veltījāt laiku, lai kopā ar mums izpētītu šo tēmu.

Es rakstīju šo eseju, jo kā zinātnieks uzskatu, ka ir svarīgi pateikt negludinātu patiesību, un kā cilvēks uzskatu, ka mums ir būtiski rīkoties ātri un izšķiroši, lai risinātu pasauli mainošu jautājumu: gudrāku par cilvēkiem AI sistēmu izstrādi.

Ja mēs uz šo ievērības cienīgo lietu stāvokli vēlamies reaģēt ar gudrību, mums jābūt gataviem kritiski pārbaudīt valdošo naratīvu, ka MVI un superintelekts "ir jāveido", lai nodrošinātu mūsu intereses, vai ka tas ir "neizbēgams" un nav apturams. Šie naratīvi mūs atņem spēku, neļaujot redzēt alternatīvos ceļus mūsu priekšā.

Ceru, ka pievienosieties man aicinājumā uz piesardzību bezatbildības priekšā un drosmi mantkārības priekšā.

Ceru, ka pievienosieties man aicinājumā uz cilvēcīgu nākotni.

*– Entonijs*

![Anthony Aguirre signature](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAnthony-Aguirre-signature-300x84.png&w=3840&q=75)

## Pielikumi

Papildu informācija, tostarp — Tehniskie dati par skaitļošanas jaudas uzskaiti, vārtu slēgšanas ieviešanas piemērs, strikta MVI atbildības režīma detaļas un pakāpeniska pieeja MVI drošības un drošuma standartiem.

### Pielikums A: Skaitļošanas jaudas uzskaites tehniskie dati

Detalizēta metode gan "objektīvai patiesībai", gan labiem aprēķiniem kopējai skaitļošanas jaudai, kas izmantota apmācībā un secinājumu izdarīšanā, ir nepieciešama jēgpilnai uz skaitļošanu balstītai kontrolei. Šeit ir piemērs tam, kā "objektīvā patiesība" varētu tikt aprēķināta tehniskā līmenī.

**Definīcijas:**

*Skaitļošanas cēloniskais grafs:* Konkrētam AI modeļa iznākumam O pastāv digitālo skaitļošanu kopa, kurām mainot šīs skaitļošanas rezultātu, potenciāli varētu mainīties O. (Tas būtu konservatīvi pieņemams, t.i., jābūt skaidram iemeslam uzskatīt, ka skaitļošana ir neatkarīga no priekšgājēja, kas gan notiek agrāk laikā, gan kurām ir fizisks potenciālais cēloniskās ietekmes ceļš.) Tas ietver skaitļošanu, ko AI modelis veic secinājumu izdarīšanas laikā, kā arī skaitļošanu, kas ieiet modeļa ievadē, datu sagatavošanā un apmācībā. Tā kā katra no tām pati var būt AI modeļa iznākums, tas tiek aprēķināts rekursīvi, apraujot tur, kur cilvēks ir sniedzis nozīmīgas izmaiņas ievadē.

*Apmācības skaitļošana:* Kopējā skaitļošana FLOP vai citās vienībās, ko ietver neironu tīkla skaitļošanas cēloniskais grafs (ieskaitot datu sagatavošanu, apmācību un precīzu regulēšanu, un jebkuras citas skaitļošanas.)

*Iznākuma skaitļošana:* Kopējā skaitļošana konkrēta AI iznākuma skaitļošanas cēloniskajā grafā, ieskaitot visus neironu tīklus (un ieskaitot to apmācības skaitļošanu) un citas skaitļošanas, kas ieiet šajā iznākumā.

*Secinājumu skaitļošanas ātrums:* Iznākumu sērijā, iznākuma skaitļošanas izmaiņu ātrums (FLOP/s vai citās vienībās) starp iznākumiem, t.i., skaitļošana, kas izmantota nākamā iznākuma iegūšanai, dalīta ar laika intervālu starp iznākumiem.

**Piemēri un aprēķini:**

- Vienam neironu tīklam, kas apmācīts uz cilvēku radītiem datiem, apmācības skaitļošana ir vienkārši kopējā apmācības skaitļošana, kā parasti tiek ziņots.
- Šādam neironu tīklam, kas veic secinājumus pastāvīgā ātrumā, secinājumu skaitļošanas ātrums ir aptuveni kopējais skaitļošanas klastera ātrums, kas veic secinājumus FLOP/s.
- Modeļa precīzai regulēšanai pilna modeļa apmācības skaitļošana ir neprecīzi regulētā modeļa apmācības skaitļošana plus skaitļošana, kas veikta precīzās regulēšanas laikā un datu sagatavošanai, kas izmantoti precīzajā regulēšanā.
- Destilētam modelim pilna modeļa apmācības skaitļošana ietver gan destilētā modeļa, gan lielākā modeļa apmācību, kas izmantots sintētisko datu vai citu apmācības ievadu nodrošināšanai.
- Ja apmācīti vairāki modeļi, bet daudzi "izmēģinājumi" tiek atmesti, pamatojoties uz cilvēka spriedumu, tie neskaitās paturētā modeļa apmācības vai iznākuma skaitļošanā.

### Pielikums B: Vārtu slēgšanas ieviešanas piemērs

**Ieviešanas piemērs:** Šeit ir viens piemērs tam, kā varētu darboties vārtu slēgšana ar ierobežojumu 10<sup>27</sup> FLOP apmācībai un 10<sup>20</sup> FLOP/s secinājumu izdarīšanai (AI darbināšanai):

**1\. Pauze:** Nacionālās drošības apsvērumu dēļ ASV izpildvara lūdz visus uzņēmumus, kas bāzēti ASV, darbojas ASV vai izmanto ASV ražotus čipus, pārtraukt jebkuras jaunas AI apmācības, kas varētu pārsniegt 10<sup>27</sup> FLOP apmācības skaitļošanas ierobežojumu. ASV jāuzsāk diskusijas ar citām valstīm, kurās notiek AI izstrāde, stingri mudināt tās veikt līdzīgus soļus un norādīt, ka ASV pauze var tikt atcelta, ja tās izvēlas neievērot.

**2\. ASV uzraudzība un licencēšana:** Ar izpildrīkojumu vai esošas regulējošas iestādes rīcību ASV pieprasa, lai gada laikā:

- Visi AI apmācības palaišanas gadījumi, kas pārsniedz 10<sup>25</sup> FLOP un ko veic uzņēmumi, kuri darbojas ASV, tiek reģistrēti ASV regulējošās iestādes uzturētā datubāzē. (Piezīme: nedaudz vājāka šīs prasības versija jau bija iekļauta tagad atceltajā 2023. gada ASV izpildrīkojumā par AI, pieprasot reģistrāciju modeļiem virs 10<sup>26</sup> FLOP.)
- Visi ar AI saistītie aparatūras ražotāji, kas darbojas ASV vai veic darījumus ar ASV valdību, ievēro prasību kopumu attiecībā uz savu specializēto aparatūru un to darbināmo programmatūru. (Daudzas no šīm prasībām varētu iestrādāt esošās aparatūras programmatūras un aparātprogrammatūras atjauninājumos, bet ilgtermiņa un noturīgi risinājumi prasītu izmaiņas vēlākās aparatūras paaudzēs.) Starp tām ir prasība, ka ja aparatūra ir daļa no ātrgaitas savienotā klastera, kas spēj izpildīt 10<sup>18</sup> FLOP/s skaitļošanu, nepieciešams augstāks verifikācijas līmenis, kas ietver regulāru atļauju no attālināta "regulatora", kurš saņem gan telemetriju, gan pieprasījumus veikt papildu skaitļošanu.
- Glabātājs ziņo par savu aparatūras kopējo skaitļošanu iestādei, kas uztur ASV datubāzi.
- Tiek pakāpeniski ieviesti stingrāki nosacījumi, lai nodrošinātu gan drošāku, gan elastīgāku uzraudzību un atļauju sniegšanu.

**3\. Starptautiska uzraudzība:**

- ASV, Ķīna un jebkuras citas valstis, kurās atrodas progresīvas čipu ražošanas iespējas, sarunu ceļā panāk starptautisku vienošanos.
- Šī vienošanās izveido jaunu starptautisku iestādi, līdzīgu Starptautiskajai atomenerģijas aģentūrai, kas atbildīga par AI apmācības un darbības uzraudzību.
- Parakstītājvalstīm jāpieprasa saviem vietējiem AI aparatūras ražotājiem ievērot prasību kopumu, kas ir vismaz tikpat stingrs kā ASV uzlikto prasību kopums.
- Glabātājiem tagad jāziņo AI skaitļošanas skaitļi gan savās mājvalstīs esošajām iestādēm, gan jaunajam birojam starptautiskajā iestādē.
- Papildu valstis tiek stingri mudinātas pievienoties esošajai starptautiskajai vienošanai: parakstītājvalstu eksporta kontrole ierobežo neparakstītāju piekļuvi augstklases aparatūrai, kamēr parakstītājvalstis var saņemt tehnisko atbalstu savu AI sistēmu pārvaldīšanā.

**4\. Starptautiska verifikācija un izpilde:**

- Aparatūras verifikācijas sistēma tiek atjaunināta tā, lai tā ziņotu par skaitļošanas izmantojumu gan sākotnējam glabātājam, gan tieši starptautiskās iestādes birojam.
- Iestāde, diskusijā ar starptautiskās vienošanās parakstītājiem, vienojas par skaitļošanas ierobežojumiem, kas pēc tam iegūst juridisko spēku parakstītājvalstīs.
- Paralēli var tikt izstrādāti starptautiski standarti tā, lai AI apmācībai un darbināšanai virs skaitļošanas sliekšņa (bet zem ierobežojuma) būtu jāievēro šie standarti.
- Iestāde var, ja nepieciešams labāku algoritmu utt. kompensēšanai, pazemināt skaitļošanas ierobežojumu. Vai, ja tas tiek uzskatīts par drošu un ieteicamu (piemēram, pierādāmu drošības garantiju līmenī), paaugstināt skaitļošanas ierobežojumu.

### Pielikums C: Strikta MVI atbildības režīma detaļas

**Strikta MVI atbildības režīma detaļas**

- Progresīvas AI sistēmas, kas ir ļoti vispārēja, spējīga un autonoma, izveide un darbība tiek uzskatīta par "neparasti bīstamu" darbību.
- Kā tāda, noklusējuma atbildība par šādu sistēmu apmācību un darbību ir strikta, kopīga un solidāra atbildība (vai tās ārpus ASV ekvivalents) par jebkādiem kaitējumiem, ko rada modelis vai tā iznākumi/darbības.
- Personiskā atbildība tiks uzlikta vadītājiem un valdes locekļiem rupjas nevērības vai tīšas ļaunprātības gadījumos. Tam jāietver kriminālatbildība vissmagāko gadījumu gadījumā.
- Pastāv daudzas drošas ostasāgas, saskaņā ar kurām atbildība atgriežas pie noklusējuma (vainas balstītas ASV) atbildības, kādai parasti būtu pakļauti cilvēki un uzņēmumi.
	- Modeļi, kas apmācīti un darbināti zem kāda skaitļošanas sliekšņa (kas būtu vismaz 10 reizes zemāks par iepriekš aprakstītajiem ierobežojumiem.)
	- AI, kas ir "vājš" (aptuveni, zem cilvēka eksperta līmeņa uzdevumos, kuriem tas ir paredzēts) un/vai
	- AI, kas ir "šaurs" (ar fiksētu un diezgan ierobežotu uzdevumu un operāciju tvērumu, kam tas ir īpaši paredzēts un apmācīts) un/vai
	- AI, kas ir "pasīvs" (ļoti ierobežotas spējas – pat pie mērenas modifikācijas – veikt darbības vai sarežģītus daudzpakāpju uzdevumus bez tiešas cilvēka iesaistes un kontroles.)
	- AI, kuram garantēta drošība, drošums un kontrolējamība (pierādāmi drošs vai riska analīze norāda uz nenozīmīgu gaidāmā kaitējuma līmeni.)
- Drošās ostasāgas var pieteikt, pamatojoties uz AI izstrādātāja sagatavoto un iestādes vai iestādes akreditēta auditora apstiprinātu [drošības lietu](https://arxiv.org/abs/2410.21572). Lai pieteiktu drošo ostasāgu, pamatojoties uz skaitļošanu, izstrādātājam jāsniedz ticami kopējās apmācības skaitļošanas un maksimālā secinājumu ātruma aprēķini.
- Likumdošana skaidri izklāstītu situācijas, kurās būtu piemērots aizlieguma rakstura atvieglojums no AI sistēmu izstrādes ar augstu sabiedriskā kaitējuma risku.
- Uzņēmumu konsorcijiem, sadarbībā ar NVO un valdības iestādēm, jāizstrādā standarti un normas, kas definē šos terminus, kā regulatoriem būtu jāpiešķir drošās ostasāgas, kā AI izstrādātājiem jāizstrādā drošības lietas, un kā tiesām jāinterpretē atbildība, ja drošās ostasāgas nav proaktīvi pieteiktas.

### Pielikums D: Pakāpeniska pieeja MVI drošības un drošuma standartiem

**Pakāpeniska pieeja MVI drošības un drošuma standartiem**

| Riska līmenis | Aktivizētājs(i) | Apmācības prasības | Izvietošanas prasības |
| --- | --- | --- | --- |
| RL-0 | AI vājš autonomijā, vispārībā un intelektā | nav | nav |
| RL-1 | AI spēcīgs vienā no autonomijas, vispārības un intelekta | nav | Pamatojoties uz risku un izmantošanu, potenciāli drošības lietas, ko apstiprina nacionālās institūcijas jebkur, kur modelis var tikt izmantots |
| RL-2 | AI spēcīgs divās no autonomijas, vispārības un intelekta | Reģistrācija nacionālā institūcijā ar jurisdikciju pār izstrādātāju | Drošības lieta, kas ierobežo nozīmīga kaitējuma risku zem atļautajiem līmeņiem, plus neatkarīgi drošības auditi (ieskaitot melnās kastes un baltās kastes pārbaudīšanu), ko apstiprina nacionālās institūcijas jebkur, kur modelis var tikt izmantots |
| RL-3 | MVI spēcīgs autonomijā, vispārībā un intelektā | Drošības un drošuma plāna iepriekšēja apstiprināšana nacionālā institūcijā ar jurisdikciju pār izstrādātāju | Drošības lieta, kas garantē ierobežotu nozīmīga kaitējuma risku zem atļautajiem līmeņiem, kā arī nepieciešamās specifikācijas, ieskaitot kiberdrošību, kontrolējamību, nenovākamu izslēgšanas slēdzi, saskaņošanu ar cilvēku vērtībām un noturību pret ļaunprātīgu izmantošanu. |
| RL-4 | Jebkurš modelis, kas pārsniedz arī 10<sup>27</sup> FLOP apmācību vai 10<sup>20</sup> FLOP/s secinājumus | Aizliegts, gaidot starptautisku vienošanos par skaitļošanas ierobežojuma atcelšanu | Aizliegts, gaidot starptautisku vienošanos par skaitļošanas ierobežojuma atcelšanu |

Riska klasifikācijas un drošības/drošuma standarti ar līmeņiem, kas balstīti uz skaitļošanas slieksņiem, kā arī augstas autonomijas, vispārības un intelekta kombinācijām:

- *Spēcīga autonomija* attiecas, ja sistēma spēj veikt vai var viegli tikt pielāgota, lai veiktu daudzpakāpju uzdevumus un/vai veiktu sarežģītas darbības, kas ir reālajā pasaulē nozīmīgas, bez būtiskas cilvēka uzraudzības vai iejaukšanās. Piemēri: autonomie transportlīdzekļi un roboti; finanšu tirdzniecības roboti. Nepiemēri: GPT-4; attēlu klasifikatori
- *Spēcīga vispārība* norāda uz plašu pielietojuma tvērumu, uzdevumu veikšanu, kuriem modelis nebija apzināti un īpaši apmācīts, un nozīmīgu spēju apgūt jaunus uzdevumus. Piemēri: GPT-4; mu-zero. Nepiemēri: AlphaFold; autonomie transportlīdzekļi; attēlu ģeneratori
- *Spēcīgs intelekts* atbilst cilvēka eksperta līmeņa sniegumam uzdevumos, kuros modelis darbojas vislabāk (un vispārēja modeļa gadījumā plašā uzdevumu spektrā.) Piemēri: AlphaFold; mu-zero; o3. Nepiemēri: GPT-4; Siri

### Pateicības

Daži paldies cilvēkiem, kas piedalījās grāmatas "Saglabāsim nākotni cilvēcīgu" tapšanā.

Šis darbs atspoguļo autora viedokli un nav uzskatāms par Dzīves nākotnes institūta (Future of Life Institute) oficiālo pozīciju (lai gan tie ir savietojami; institūta oficiālo nostāju skatiet [šajā lapā](https://futureoflife.org/our-position-on-ai/)) vai jebkuras citas organizācijas, ar kuru autors ir saistīts.

Es esmu pateicīgs cilvēkiem Markam Brakelam, Benam Aizenprenam, Annai Hehirai, Karlosam Gutjēresam, Emīlijai Javorskij, Ričardam Mallāham, Džordanam Šārnhorstam, Elizai Fulčerei, Maksam Tegmarkam un Jānam Tallinam par komentāriem par manuskriptu; Timam Šrīeram par palīdzību ar atsevišķām atsaucēm; Teiloram Džounsam un Elizai Fulčerei par diagrammu vizuālā noformējuma uzlabošanu.

Šī darba tapšanā tika ierobežoti izmantoti ģeneratīvie AI modeļi (Claude un ChatGPT) – dažiem labojumiem un pārbaudes nolūkos. Pēc labi iedibinātiem AI iesaistes līmeņiem radošajos darbos, šis darbs, visticamāk, saņemtu vērtējumu 3/10. (Patiesībā tāda standarta nav! Bet vajadzētu būt.)

Mēs esam ļoti pateicīgi [Džūlijam Odajam](https://www.linkedin.com/in/julius-odai/) par šīs esejas tīmekļa versijas izveidi, kas padara lasīšanu un navigāciju esejā ļoti patīkamu. Džūlijs ir tehnologs un nesen absolvēja BlueDot Impact AI pārvaldības kursu.