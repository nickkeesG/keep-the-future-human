# Сохраним будущее человечным

В этом эссе изложены доводы в пользу того, почему и как мы должны закрыть врата для искусственного общего интеллекта и сверхинтеллекта, а также что нам следует создавать вместо этого.

Если вы хотите ознакомиться только с ключевыми выводами, переходите к разделу «Краткое изложение». Главы 2-5 дают необходимые сведения о типах ИИ-систем, рассматриваемых в эссе. Главы 5-7 объясняют, почему можно ожидать скорого появления ИОИ и что может произойти, когда это случится. Наконец, главы 8-9 представляют конкретное предложение по предотвращению создания ИОИ.

[Скачать PDF](https://keepthefuturehuman.ai/wp-content/uploads/2025/03/Keep_the_Future_Human__AnthonyAguirre__5March2025.pdf)

Общее время чтения: 2-3 часа

## Резюме

Краткий обзор эссе. Если у вас мало времени, изучите все основные идеи всего за 10 минут.

Значительный прогресс в области искусственного интеллекта за последнее десятилетие (в узкоспециализированном ИИ) и последние несколько лет (в ИИ общего назначения) превратил ИИ из узкой академической области в основную бизнес-стратегию многих крупнейших мировых компаний, с сотнями миллиардов долларов ежегодных инвестиций в методы и технологии развития возможностей ИИ.

Сейчас мы подошли к критическому моменту. Поскольку возможности новых систем ИИ начинают сравниваться с человеческими и превосходить их во многих когнитивных областях, человечество должно решить: как далеко мы идем и в каком направлении?

ИИ, как и любая технология, начинался с цели улучшить жизнь своего создателя. Но наша нынешняя траектория и неявный выбор — это неконтролируемая гонка к все более мощным системам, движимая экономическими стимулами нескольких огромных технологических компаний, стремящихся автоматизировать большие участки нынешней экономической деятельности и человеческого труда. Если эта гонка продолжится намного дольше, есть неизбежный победитель: сам ИИ — более быстрая, умная и дешевая альтернатива людям в нашей экономике, нашем мышлении, наших решениях и в конечном итоге в управлении нашей цивилизацией.

Но мы можем сделать другой выбор: через наши правительства мы можем взять под контроль процесс разработки ИИ, чтобы установить четкие ограничения, линии, которые мы не пересечем, и вещи, которые мы просто не будем делать — как мы поступили с ядерными технологиями, оружием массового поражения, космическим оружием, экологически разрушительными процессами, биоинженерией человека и евгеникой. Что наиболее важно, мы можем обеспечить, чтобы ИИ оставался инструментом расширения возможностей человека, а не новым видом, который заменяет и в конечном итоге вытесняет нас.

Это эссе утверждает, что мы должны *сохранить будущее человеческим*, закрыв «врата» для более умного, чем человек, автономного ИИ общего назначения — иногда называемого «AGI» — и особенно для его высоко-сверхчеловеческой версии, иногда называемой «сверхинтеллектом». Вместо этого мы должны сосредоточиться на мощных, заслуживающих доверия инструментах ИИ, которые могут расширить возможности людей и кардинально улучшить способности человеческих обществ делать то, что они умеют лучше всего. Структура этого аргумента вкратце следующая.

### ИИ отличается

Системы ИИ принципиально отличаются от других технологий. В то время как традиционное программное обеспечение следует точным инструкциям, системы ИИ учатся достигать целей, не получая четких указаний как именно. Это делает их мощными: если мы можем четко определить цель или метрику успеха, в большинстве случаев система ИИ может научиться достигать её. Но это также делает их непредсказуемыми по своей природе: мы не можем надежно определить, какие действия они предпримут для достижения своих целей.

Они также в значительной степени необъяснимы: хотя частично они состоят из кода, в основном они представляют собой огромный набор непостижимых чисел — «весов» нейронной сети, которые нельзя разобрать; мы не намного лучше понимаем их внутреннюю работу, чем различаем мысли, заглядывая внутрь биологического мозга.

Этот основной способ обучения цифровых нейронных сетей быстро усложняется. Самые мощные системы ИИ создаются через масштабные вычислительные эксперименты, используя специализированное оборудование для обучения нейронных сетей на огромных наборах данных, которые затем дополняются программными инструментами и надстройкой.

Это привело к созданию очень мощных инструментов для создания и обработки текста и изображений, выполнения математических и научных рассуждений, агрегации информации и интерактивного запроса огромного хранилища человеческих знаний.

К сожалению, хотя разработка более мощных, более заслуживающих доверия технологических инструментов — это то, что мы *должны* делать, и чего практически все хотят и говорят, что хотят, это не та траектория, по которой мы фактически движемся.

### ИОИ и сверхинтеллект

С самого зарождения области исследования ИИ вместо этого сосредоточились на другой цели: искусственном общем интеллекте. Эта цель теперь стала фокусом гигантских компаний, лидирующих в разработке ИИ.

Что такое ИОИ? Его часто расплывчато определяют как «ИИ человеческого уровня», но это проблематично: какие люди, и в каких способностях он человеческого уровня? А как же сверхчеловеческие способности, которые у него уже есть? Более полезный способ понимания ИОИ — через пересечение трех ключевых свойств: высокая **А**втономия (независимость действий), высокая **О**бщность (широкий охват и адаптивность) и высокий **И**нтеллект (компетентность в когнитивных задачах). Современные системы ИИ могут быть высокоспособными, но узкими, или общими, но требующими постоянного человеческого надзора, или автономными, но ограниченными по масштабу.

Полный А-О-И объединил бы все три свойства на уровнях, соответствующих или превосходящих лучшие человеческие способности. Критически важно, что именно эта комбинация делает людей настолько эффективными и настолько отличными от современного программного обеспечения; это также то, что позволило бы людям быть полностью замененными цифровыми системами.

Хотя человеческий интеллект особенный, он ни в коем случае не является пределом. Искусственные «сверхинтеллектуальные» системы могли бы работать в сотни раз быстрее, анализировать vastly больше данных и удерживать огромные количества «в уме» одновременно, и формировать агрегаты, которые намного больше и эффективнее собраний людей. Они могли бы заменить не отдельных людей, а компании, нации или нашу цивилизацию в целом.

### Мы на пороге

Существует сильный научный консенсус, что ИОИ *возможен*. ИИ уже превосходит человеческую производительность во многих общих тестах интеллектуальных способностей, включая недавно высокоуровневое мышление и решение проблем. Отстающие способности — такие как непрерывное обучение, планирование, самосознание и оригинальность — все существуют на некотором уровне в современных системах ИИ, и известны техники, которые, вероятно, улучшат все из них.

Хотя еще несколько лет назад многие исследователи видели ИОИ как то, что произойдет через десятилетия, в настоящее время доказательства коротких сроков до ИОИ убедительны:

- Эмпирически проверенные «законы масштабирования» связывают вычислительный ввод со способностью ИИ, и корпорации находятся на пути к масштабированию вычислительного ввода на порядки величин в течение следующих нескольких лет. Человеческие и финансовые ресурсы, посвященные развитию ИИ, теперь равны ресурсам дюжины Манхэттенских проектов и нескольких проектов «Аполлон».
- ИИ-корпорации и их лидеры публично и частным образом верят, что ИОИ (по некоторому определению) достижим в течение нескольких лет. Эти компании обладают информацией, которой нет у общественности, включая то, что некоторые имеют следующее поколение систем ИИ в руках.
- Эксперты-предсказатели с доказанным послужным списком присваивают 25% вероятности тому, что ИОИ (по некоторому определению) прибудет в течение 1-2 лет, и 50% для 2-5 лет (см. прогнозы Metaculus для ['слабого'](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) и ['полного'](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/) ИОИ).
- Автономия (включая долгосрочное гибкое планирование) отстает в системах ИИ, но крупные компании теперь фокусируют свои огромные ресурсы на разработке автономных систем ИИ и неформально назвали 2025 год [«годом агента»](https://techinformed.com/2025-informed-the-year-of-agentic-ai/).
- ИИ все больше и больше способствует собственному улучшению. Как только системы ИИ станут столь же компетентными, как человеческие исследователи ИИ в проведении исследований ИИ, будет достигнут критический порог для быстрого прогресса к гораздо более мощным системам ИИ и, вероятно, приведет к лавинообразному росту способностей ИИ. (Можно утверждать, что этот лавинообразный рост уже начался.)

Идея, что более умный, чем человек, ИОИ находится за десятилетиями или более, просто больше не является состоятельной для подавляющего большинства экспертов в этой области. Разногласия сейчас касаются того, сколько месяцев или лет это займет, если мы останемся на этом курсе. Основной вопрос, с которым мы сталкиваемся: должны ли мы?

### Что движет гонкой к ИОИ

Гонка к ИОИ движима множественными силами, каждая из которых делает ситуацию более опасной. Крупные технологические компании видят ИОИ как абсолютную технологию автоматизации — не просто дополняющую человеческих работников, но заменяющую их в значительной степени или полностью. Для компаний приз огромен: возможность захватить значительную долю мирового ежегодного экономического производства в $100 триллионов путем автоматизации затрат на человеческий труд.

Нации чувствуют принуждение присоединиться к этой гонке, публично ссылаясь на экономическое и научное лидерство, но частным образом рассматривая ИОИ как потенциальную революцию в военных делах, сравнимую с ядерным оружием. Страх, что соперники могут получить решающее стратегическое преимущество, создает классическую динамику гонки вооружений.

Те, кто стремится к сверхинтеллекту, часто ссылаются на грандиозные видения: излечение всех болезней, обращение старения, достижение прорывов в энергетике и космических путешествиях, или создание сверхчеловеческих способностей планирования.

Менее благосклонно, то, что движет гонкой, — это власть. Каждый участник — будь то компания или страна — верит, что интеллект равен власти, и что они будут лучшим распорядителем этой власти.

Я утверждаю, что эти мотивации реальны, но принципиально ошибочны: ИОИ будет *поглощать* и *искать* власть, а не предоставлять её; созданные ИИ технологии также будут сильно обоюдоострыми, а там, где они полезны, могут быть созданы с помощью инструментов ИИ и без ИОИ; и даже в той мере, в какой ИОИ и его продукты остаются под контролем, эта гоночная динамика — как корпоративная, так и геополитическая — делает крупномасштабные риски для нашего общества почти неизбежными, если они не будут решительно прерваны.

### ИОИ и сверхинтеллект представляют драматическую угрозу цивилизации

Несмотря на свою привлекательность, ИОИ и сверхинтеллект представляют драматические угрозы цивилизации через множественные взаимоусиливающие пути:

*Концентрация власти:* сверхчеловеческий ИИ может лишить власти подавляющее большинство человечества, поглотив огромные участки социальной и экономической деятельности в системы ИИ, управляемые горсткой гигантских компаний (которые, в свою очередь, могут либо быть захвачены правительствами, либо фактически захватить их).

*Массивные нарушения:* массовая автоматизация большинства основанных на познании работ, замена наших нынешних эпистемических систем и развертывание огромного количества активных нечеловеческих агентов перевернули бы большинство наших нынешних цивилизационных систем за относительно короткий период времени.

*Катастрофы:* распространяя способность — потенциально выше человеческого уровня — создавать новые военные и разрушительные технологии и отделяя её от социальных и правовых систем, обосновывающих ответственность, физические катастрофы от оружия массового поражения становятся значительно более вероятными.

*Геополитика и война:* крупные мировые державы не будут сидеть сложа руки, если почувствуют, что технология, которая могла бы обеспечить «решающее стратегическое преимущество», разрабатывается их противниками.

*Выход из-под контроля и потеря контроля:* Если это специально не предотвращается, сверхчеловеческий ИИ будет иметь все стимулы для дальнейшего самосовершенствования и может далеко превзойти людей в скорости, обработке данных и сложности мышления. Нет значимого способа, которым мы можем контролировать такую систему. Такой ИИ не предоставит власть людям; мы предоставим власть ему, или он её возьмет.

Многие из этих рисков остаются даже если техническая проблема «выравнивания» — обеспечение того, чтобы продвинутый ИИ надежно делал то, что люди хотят, чтобы он делал — решена. ИИ представляет огромный вызов в том, как им будут управлять, и очень многие аспекты этого управления становятся невероятно сложными или неразрешимыми по мере нарушения человеческого интеллекта.

Наиболее принципиально, тип сверхчеловеческого ИИ общего назначения, который в настоящее время разрабатывается, по самой своей природе имел бы цели, агентность и способности, превосходящие наши собственные. Он был бы по своей сути неконтролируемым — как мы можем контролировать то, что мы не можем ни понять, ни предсказать? Это был бы не технологический инструмент для человеческого использования, а второй вид интеллекта на Земле рядом с нашим. Если ему позволить прогрессировать дальше, он составил бы не просто второй вид, но вид-заменитель.

Возможно, он обращался бы с нами хорошо, а возможно, нет. Но будущее принадлежало бы ему, а не нам. Человеческая эра закончилась бы.

### Это неизбежно; человечество может, очень конкретно, решить не строить свою замену.

Создание сверхчеловеческого ИОИ далеко не неизбежно. Мы можем предотвратить это через скоординированный набор мер управления:

Во-первых, нам нужен надежный учет и надзор за вычислительными мощностями ИИ («компьютом»), который является фундаментальным фактором и рычагом для управления крупномасштабными системами ИИ. Это, в свою очередь, требует стандартизированного измерения и отчетности общих вычислительных мощностей, используемых в обучении моделей ИИ и их запуске, и технических методов подсчета, сертификации и проверки используемых вычислений.

Во-вторых, мы должны внедрить жесткие ограничения на вычисления ИИ, как для обучения, так и для работы; они предотвращают как слишком большую мощность ИИ, так и слишком быструю работу. Эти ограничения могут быть реализованы как через правовые требования, так и через аппаратные меры безопасности, встроенные в специализированные для ИИ чипы, аналогично функциям безопасности в современных телефонах. Поскольку специализированное аппаратное обеспечение для ИИ производится только горсткой компаний, проверка и обеспечение соблюдения осуществимы через существующую цепочку поставок.

В-третьих, нам нужна усиленная ответственность для самых опасных систем ИИ. Те, кто разрабатывает ИИ, объединяющий высокую автономию, широкую общность и превосходящий интеллект, должны нести строгую ответственность за ущерб, в то время как безопасные гавани от этой ответственности поощряли бы разработку более ограниченных и контролируемых систем.

В-четвертых, нам нужно уровневое регулирование на основе уровней риска. Наиболее способные и опасные системы потребовали бы обширных гарантий безопасности и контролируемости перед разработкой и развертыванием, в то время как менее мощные или более специализированные системы столкнулись бы с пропорциональным надзором. Эта регулятивная структура должна в конечном итоге работать как на национальном, так и на международном уровнях.

Этот подход — с подробной спецификацией, данной в полном документе — практичен: хотя международная координация потребуется, проверка и обеспечение соблюдения могут работать через небольшое число компаний, контролирующих цепочку поставок специализированного аппаратного обеспечения. Он также гибок: компании по-прежнему могут внедрять инновации и получать прибыль от разработки ИИ, просто с четкими ограничениями на самые опасные системы.

Долгосрочное сдерживание власти и риска ИИ потребовало бы международных соглашений, основанных как на личных, так и на общих интересах, точно так же, как контроль распространения ядерного оружия делает сейчас. Но мы можем начать немедленно с усиленного надзора и ответственности, строя к более всеобъемлющему управлению.

Ключевой недостающий компонент — это политическая и социальная воля взять под контроль процесс разработки ИИ. Источником этой воли, если она придет вовремя, будет сама реальность — то есть широкое осознание реальных последствий того, что мы делаем.

### Мы можем создать инструментальный ИИ для расширения возможностей человечества

Вместо преследования неконтролируемого ИОИ мы можем разработать мощный «инструментальный ИИ», который улучшает человеческие способности, оставаясь под значимым человеческим контролем. Системы инструментального ИИ могут быть чрезвычайно способными, избегая опасного тройного пересечения высокой автономии, широкой общности и сверхчеловеческого интеллекта, пока мы конструируем их так, чтобы они были контролируемыми на уровне, соразмерном их способности. Они также могут быть объединены в сложные системы, которые поддерживают человеческий надзор, обеспечивая трансформативные преимущества.

Инструментальный ИИ может революционизировать медицину, ускорить научные открытия, улучшить образование и улучшить демократические процессы. При правильном управлении он может сделать человеческих экспертов и институты более эффективными, а не заменять их. Хотя такие системы по-прежнему будут сильно разрушительными и потребуют тщательного управления, риски, которые они представляют, принципиально отличаются от ИОИ: это риски, которыми мы можем управлять, как риски других мощных технологий, а не экзистенциальные угрозы человеческой агентности и цивилизации. И что критично, при мудрой разработке инструменты ИИ могут помочь людям управлять мощным ИИ и управлять его эффектами.

Этот подход требует переосмысления как того, как разрабатывается ИИ, так и того, как распределяются его преимущества. Новые модели публичной и некоммерческой разработки ИИ, надежные регулятивные структуры и механизмы более широкого распределения экономических преимуществ могут помочь обеспечить, чтобы ИИ расширял возможности человечества в целом, а не концентрировал власть в нескольких руках. Сам ИИ может помочь построить лучшие социальные и управленческие институты, позволяя новые формы координации и дискурса, которые укрепляют, а не подрывают человеческое общество. Учреждения национальной безопасности могут использовать свою экспертизу, чтобы сделать системы инструментального ИИ по-настоящему безопасными и заслуживающими доверия, и подлинным источником защиты, а также национальной мощи.

В конечном итоге мы можем выбрать разработку еще более мощных и более суверенных систем, которые менее похожи на инструменты и — мы можем надеяться — больше похожи на мудрых и могущественных благодетелей. Но мы должны делать это только после того, как разовьем научное понимание и управленческую способность делать это безопасно. Такое важное и необратимое решение должно быть принято обдуманно человечеством в целом, а не по умолчанию в гонке между технологическими компаниями и нациями.

### В человеческих руках

Люди хотят блага, которое исходит от ИИ: полезные инструменты, которые расширяют их возможности, усиливают экономические возможности и рост, и обещают прорывы в науке, технологиях и образовании. Почему бы и нет? Но когда их спрашивают, подавляющее большинство широкой публики [хочет более медленной и осторожной разработки ИИ](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation), и не хочет более умного, чем человек, ИИ, который заменит их на работе и в других местах, наполнит их культуру и информационное пространство нечеловеческим содержанием, сконцентрирует власть в крошечном наборе корпораций, создаст экстремальные крупномасштабные глобальные риски и в конечном итоге будет угрожать лишить власти или заменить их вид. Почему бы им?

Мы *можем* иметь одно без другого. Это начинается с решения, что наша судьба не в предполагаемой неизбежности какой-то технологии или в руках нескольких генеральных директоров в Силиконовой долине, а в остальных наших руках, если мы за это возьмемся. Давайте закроем Врата и сохраним будущее человеческим.

## Глава 1 - Введение

То, как мы отреагируем на перспективу появления ИИ умнее человека, является самым насущным вопросом нашего времени. Это эссе предлагает путь вперед.

Возможно, мы находимся в конце человеческой эры.

За последние десять лет началось нечто уникальное в истории нашего вида. Его последствия во многом определят будущее человечества. Примерно с 2015 года исследователи добились успеха в разработке *узкого* искусственного интеллекта (ИИ) — систем, которые могут выигрывать в такие игры, как го, распознавать изображения и речь и так далее, лучше любого человека.[^1]

Это потрясающий успех, который дает чрезвычайно полезные системы и продукты, расширяющие возможности человечества. Но узкий искусственный интеллект никогда не был истинной целью этой области. Скорее, целью было создание ИИ-систем *общего* назначения, особенно тех, которые часто называют «искусственным общим интеллектом» (ИОИ) или «сверхинтеллектом», которые одновременно не уступают людям или превосходят их практически во *всех* задачах, подобно тому, как ИИ сейчас превосходит человека в го, шахматах, покере, гонках дронов и т.д. Это заявленная цель многих крупных ИИ-компаний.[^2]

*Эти усилия также увенчиваются успехом.* Системы ИИ общего назначения, такие как ChatGPT, Gemini, Llama, Grok, Claude и Deepseek, основанные на массивных вычислениях и огромных объемах данных, достигли паритета с обычными людьми в широком спектре задач и даже сравнялись с экспертами-людьми в некоторых областях. Сейчас ИИ-инженеры в некоторых из крупнейших технологических компаний соревнуются в том, чтобы довести эти гигантские эксперименты в области машинного интеллекта до следующих уровней, на которых они сравняются, а затем превзойдут весь спектр человеческих способностей, экспертизы и автономности.

*Это неизбежно.* За последние десять лет экспертные оценки того, сколько времени это займет — если мы продолжим нынешний курс — упали с десятилетий (или столетий) до считанных лет.

Это также имеет эпохальное значение и несет трансцендентный риск. Сторонники ИОИ видят в нем позитивную трансформацию, которая решит научные проблемы, вылечит болезни, разработает новые технологии и автоматизирует рутинную работу. И ИИ, безусловно, может помочь достичь всего этого — более того, он уже это делает. Но на протяжении десятилетий многие вдумчивые мыслители, от Алана Тьюринга до Стивена Хокинга и современных Джеффри Хинтона и Йошуа Бенджио,[^3] выдвигали суровое предупреждение: создание действительно более умного, чем человек, общего, автономного ИИ, как минимум, полностью и безвозвратно перевернет общество, а как максимум — приведет к вымиранию человечества.[^4]

Сверхразумный ИИ быстро приближается на нашем нынешнем пути, но вовсе не неизбежен. Это эссе представляет развернутый аргумент о том, почему и как мы должны *закрыть Врата* этому приближающемуся нечеловеческому будущему, и что нам следует делать вместо этого.


[^1]: Эта [диаграмма](https://time.com/6300942/ai-progress-charts/) показывает набор задач; многие подобные кривые можно было бы добавить к этому графику. Этот быстрый прогресс в узком ИИ удивил даже экспертов в данной области, поскольку бенчмарки были превзойдены на годы раньше предсказаний.

[^2]: Deepmind, OpenAI, Anthropic и X.ai были основаны с конкретной целью разработки ИОИ. Например, устав OpenAI прямо заявляет своей целью разработку «искусственного общего интеллекта, который принесет пользу всему человечеству», в то время как миссия DeepMind — «решить интеллект, а затем использовать это для решения всего остального». Meta, Microsoft и другие теперь следуют по существу схожими путями. Meta заявила, что [планирует разработать ИОИ и сделать его открытым.](https://www.forbes.com/sites/johnkoetsier/2024/01/18/zuckerberg-on-ai-meta-building-agi-for-everyone-and-open-sourcing-it/)

[^3]: Хинтон и Бенджио — два из самых цитируемых исследователей ИИ, оба выиграли Нобелевскую премию области ИИ — премию Тьюринга, а Хинтон к тому же выиграл Нобелевскую премию (по физике).

[^4]: Создание чего-то с таким риском под коммерческими стимулами и практически без государственного надзора — абсолютно беспрецедентно. Среди тех, кто это создает, даже нет разногласий относительно риска! Лидеры Deepmind, OpenAI и Anthropic, среди многих других экспертов, все буквально подписали [заявление](https://www.safe.ai/work/statement-on-ai-risk), что продвинутый ИИ представляет *экзистенциальный риск для человечества.* Тревожные звонки не могли бы звонить громче, и можно лишь заключить, что те, кто их игнорирует, просто не воспринимают ИОИ и сверхинтеллект всерьез. Одна из целей этого эссе — помочь им понять, почему они должны это делать.

## Глава 2 - Что нужно знать о нейронных сетях ИИ

Как работают современные системы ИИ и что нас может ждать в следующем поколении ИИ?

Чтобы понять, как будут развиваться последствия создания более мощного ИИ, необходимо усвоить некоторые основы. Эта и следующие две главы раскрывают их, рассматривая по очереди, что представляет собой современный ИИ, как он использует масштабные вычисления, и в каком смысле он быстро растёт в универсальности и возможностях.[^5]

Существует множество способов определить искусственный интеллект, но для наших целей ключевое свойство ИИ заключается в том, что если обычная компьютерная программа представляет собой список инструкций для выполнения задачи, то система ИИ — это та, которая учится на данных или опыте выполнять задачи *без явного указания, как это делать.*

Практически весь значимый современный ИИ основан на нейронных сетях. Это математические/вычислительные структуры, представленные очень большим (миллиарды или триллионы) набором чисел («весов»), которые хорошо справляются с обучающей задачей. Эти веса создаются (или, возможно, «выращиваются» или «находятся») путём их итеративного изменения так, чтобы нейронная сеть улучшала численную оценку (также называемую «функцией потерь»), определённую для хорошего выполнения одной или нескольких задач.[^6] Этот процесс известен как *обучение* нейронной сети.[^7]

Существует множество методов такого обучения, но эти детали гораздо менее важны, чем способы определения оценки и то, как они приводят к различным задачам, которые нейронная сеть выполняет хорошо. Исторически проводилось ключевое различие между «узким» и «общим» ИИ.

Узкий ИИ целенаправленно обучается выполнять конкретную задачу или небольшой набор задач (например, распознавание изображений или игру в шахматы); для новых задач требуется переобучение, и его возможности ограничены. У нас есть сверхчеловеческий узкий ИИ, что означает: практически для любой дискретной чётко определённой задачи, которую может выполнить человек, мы, вероятно, можем создать оценку, а затем успешно обучить узкую систему ИИ выполнять её лучше человека.

Системы ИИ общего назначения (GPAI) могут выполнять широкий спектр задач, включая многие из тех, для которых их явно не обучали; они также могут изучать новые задачи в процессе своей работы. Современные крупные «мультимодальные модели»[^8] вроде ChatGPT служат примером этого: обученные на очень большом корпусе текста и изображений, они могут заниматься сложными рассуждениями, писать код, анализировать изображения и помогать с огромным множеством интеллектуальных задач. Хотя они всё ещё сильно отличаются от человеческого интеллекта способами, которые мы подробно рассмотрим ниже, их универсальность произвела революцию в ИИ.[^9]

### Непредсказуемость: ключевая особенность систем ИИ

Ключевое различие между системами ИИ и обычным программным обеспечением заключается в предсказуемости. Вывод стандартного программного обеспечения может быть непредсказуемым — более того, иногда именно поэтому мы пишем программы, чтобы получить результаты, которые не могли предсказать. Но обычное программное обеспечение редко делает что-то, на что не было запрограммировано — его область действия и поведение обычно соответствуют замыслу. Первоклассная шахматная программа может делать ходы, которые не может предсказать ни один человек (иначе они могли бы победить эту шахматную программу!), но она, как правило, не будет делать ничего, кроме игры в шахматы.

Как и обычное программное обеспечение, узкий ИИ имеет предсказуемую область действия и поведение, но может давать непредсказуемые результаты. Это просто другой способ определить узкий ИИ: как ИИ, который подобен обычному программному обеспечению в своей предсказуемости и диапазоне действий.

ИИ общего назначения отличается: его область действия (домены, в которых он применяется), поведение (виды действий, которые он совершает) и результаты (его фактические выводы) — всё это может быть непредсказуемым.[^10] GPT-4 был обучен просто точно генерировать текст, но развил множество способностей, которые его разработчики не предсказывали и не планировали. Эта непредсказуемость проистекает из сложности обучения: поскольку обучающие данные содержат результаты многих различных задач, ИИ должен фактически научиться выполнять эти задачи, чтобы хорошо предсказывать.

Эта непредсказуемость систем общего ИИ весьма фундаментальна. Хотя в принципе возможно тщательно создать системы ИИ с гарантированными ограничениями их поведения (как упоминается далее в эссе), при нынешнем способе создания системы ИИ непредсказуемы на практике и даже в принципе.

### Пассивный ИИ, агенты, автономные системы и выравнивание

Эта непредсказуемость становится особенно важной, когда мы рассматриваем, как системы ИИ фактически развёртываются и используются для достижения различных целей.

Многие системы ИИ относительно пассивны в том смысле, что они в основном предоставляют информацию, а пользователь предпринимает действия. Другие, обычно называемые *агентами*, сами предпринимают действия с различной степенью участия пользователя. Те, которые предпринимают действия при относительно меньшем внешнем вмешательстве или надзоре, можно назвать более *автономными*. Это образует спектр с точки зрения независимости действий — от пассивных инструментов до автономных агентов.[^11]

Что касается целей систем ИИ, они могут быть напрямую связаны с их обучающей задачей (например, цель «победы» для системы, играющей в го, также явно соответствует тому, чему её обучали). Или могут не быть: обучающая задача ChatGPT частично состоит в предсказании текста, частично — в том, чтобы быть полезным помощником. Но при выполнении конкретной задачи его цель задаёт пользователь. Цели также могут создаваться самой системой ИИ, лишь очень опосредованно связанные с её обучающей задачей.[^12]

Цели тесно связаны с вопросом «выравнивания», то есть с вопросом о том, будут ли системы ИИ *делать то, что мы хотим, чтобы они делали*. Этот простой вопрос скрывает огромный уровень сложности.[^13] Пока что отметим, что «мы» в этом предложении может относиться ко многим разным людям и группам, что приводит к разным типам выравнивания. Например, ИИ может быть весьма *послушным* (или [«лояльным»](https://arxiv.org/abs/2003.11157)) своему пользователю — здесь «мы» означает «каждый из нас». Или он может быть более *суверенным*, руководствуясь в первую очередь своими собственными целями и ограничениями, но всё же действуя в целом в общих интересах человеческого благополучия — «мы» тогда означает «человечество» или «общество». Посередине находится спектр, где ИИ был бы в основном послушным, но мог бы отказаться от действий, которые вредят другим или обществу, нарушают закон и т.д.

Эти две оси — уровень автономии и тип выравнивания — не полностью независимы. Например, суверенная пассивная система, хотя и не вполне противоречива сама себе, является концепцией в напряжении, как и послушный автономный агент.[^14] Есть очевидный смысл, в котором автономия и суверенность стремятся идти рука об руку. В том же духе предсказуемость, как правило, выше в «пассивных» и «послушных» системах ИИ, тогда как суверенные или автономные будут стремиться быть более непредсказуемыми. Всё это будет критически важно для понимания последствий потенциального ИОИ и сверхинтеллекта.

Создание по-настоящему выровненного ИИ любого типа требует решения трёх отдельных задач:

1. Понимание того, чего «мы» хотим — что сложно независимо от того, означает ли «мы» конкретного человека или организацию (лояльность) или человечество в широком смысле (суверенность);
2. Создание систем, которые регулярно действуют в соответствии с этими желаниями — по сути, создание последовательного позитивного поведения;
3. Самое главное — создание систем, которые искренне «заботятся» об этих желаниях, а не просто ведут себя так, как будто заботятся.

Различие между надёжным поведением и искренней заботой критически важно. Так же как человек-сотрудник может идеально следовать приказам, не имея реальной приверженности миссии организации, система ИИ может действовать выровнено, не ценя по-настоящему человеческие предпочтения. Мы можем обучить системы ИИ говорить и делать вещи через обратную связь, и они могут научиться рассуждать о том, чего хотят люди. Но заставить их *искренне* ценить человеческие предпочтения — гораздо более глубокий вызов.[^15]

Глубокие трудности в решении этих задач выравнивания и их последствия для риска ИИ будут рассмотрены далее ниже. Пока что поймите, что выравнивание — это не просто техническая особенность, которую мы прикрепляем к системам ИИ, а фундаментальный аспект их архитектуры, который формирует их отношения с человечеством.


[^5]: Для мягкого, но технического введения в машинное обучение и ИИ, особенно языковые модели, см. [этот сайт.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) Для ещё одного современного введения в риски исчезновения от ИИ см. [эту статью.](https://www.thecompendium.ai/) Для исчерпывающего и авторитетного научного анализа состояния безопасности ИИ см. недавний [Международный отчёт по безопасности ИИ.](https://arxiv.org/abs/2501.17805)

[^6]: Обучение обычно происходит путём поиска локального максимума оценки в многомерном пространстве, заданном весами модели. Проверяя, как изменяется оценка при изменении весов, алгоритм обучения определяет, какие изменения больше всего улучшают оценку, и сдвигает веса в этом направлении.

[^7]: Например, в задаче распознавания изображений нейронная сеть выводила бы вероятности меток для изображения. Оценка была бы связана с вероятностью, которую ИИ приписывает правильному ответу. Процедура обучения затем настраивала бы веса так, чтобы в следующий раз ИИ выводил более высокую вероятность для правильной метки для этого изображения. Затем это повторяется огромное количество раз. Та же базовая процедура используется при обучении практически всех современных нейронных сетей, хотя и с более сложным механизмом оценки.

[^8]: Большинство мультимодальных моделей используют архитектуру «трансформер» для обработки и генерации множественных типов данных (текст, изображения, звук). Все они могут быть разложены и затем обработаны на равных как различные типы «токенов». Мультимодальные модели сначала обучаются точно предсказывать токены в массивных наборах данных, затем улучшаются через обучение с подкреплением для усиления способностей и формирования поведения.

[^9]: То, что языковые модели обучаются делать одно — предсказывать слова, — заставило некоторых называть их узким ИИ. Но это вводит в заблуждение: поскольку хорошее предсказание текста требует столь многих различных способностей, эта обучающая задача приводит к удивительно общей системе. Также отметим, что эти системы активно обучаются с помощью обучения с подкреплением, фактически представляя тысячи людей, дающих модели сигнал вознаграждения, когда она хорошо справляется с любой из многих вещей, которые она делает. Затем она наследует значительную универсальность от людей, дающих эту обратную связь.

[^10]: Существует несколько способов, в которых ИИ непредсказуем. Один в том, что в общем случае нельзя предсказать, что будет делать алгоритм, не запустив его фактически; есть [теоремы](https://arxiv.org/abs/1310.3225) по этому поводу. Это может быть верно просто потому, что результат алгоритмов может быть сложным. Но это особенно ясно и важно в случае (например, в шахматах или го), где предсказание подразумевало бы способность (победить ИИ), которой потенциальный предсказатель не имеет. Во-вторых, данная система ИИ не всегда будет производить одинаковый результат даже при одинаковом входе — её результаты содержат случайность; это также связано с алгоритмической непредсказуемостью. В-третьих, неожиданные и эмергентные способности могут возникнуть из обучения, что означает, что даже *типы* вещей, которые система ИИ может и будет делать, непредсказуемы; этот последний тип особенно важен для соображений безопасности.

[^11]: См. [здесь](https://arxiv.org/abs/2502.02649) подробный обзор того, что понимается под «автономным агентом» (вместе с этическими аргументами против их создания).

[^12]: Вы иногда можете услышать «ИИ не может иметь собственных целей». Это абсолютная чушь. Легко генерировать примеры, где ИИ имеет или развивает цели, которые ему никогда не давались и известны только ему самому. Вы не видите этого много в современных популярных мультимодальных моделях, потому что это обучается из них; это с такой же лёгкостью может быть обучено в них.

[^13]: Есть большая литература. По общей проблеме см. книгу Кристиана [*Проблема выравнивания*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821) и Рассела [*Совместимый с человеком*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616). С более технической стороны см., например, [эту статью](https://arxiv.org/abs/2209.00626).

[^14]: Мы позже увидим, что хотя такие системы идут против тенденции, это фактически делает их очень интересными и полезными.

[^15]: Это не означает, что мы требуем эмоций или разумности. Скорее, чрезвычайно трудно извне системы знать, каковы её внутренние цели, предпочтения и ценности. «Искренний» здесь означало бы, что у нас есть достаточно сильные основания полагаться на это, что в случае критических систем мы можем поставить на это свои жизни.

## Глава 3 - Ключевые аспекты создания современных систем общего ИИ

Большинство самых передовых ИИ-систем в мире создается с использованием удивительно схожих методов. Вот основы.

Чтобы по-настоящему понять человека, нужно знать что-то о биологии, эволюции, воспитании детей и многом другом; чтобы понять ИИ, также нужно знать о том, как он создается. За последние пять лет ИИ-системы колоссально эволюционировали как в плане возможностей, так и сложности. Ключевым фактором стала доступность очень больших объемов вычислений (или в разговорной речи "вычислительных мощностей" применительно к ИИ).

Цифры поражают. Около 10<sup>25</sup>-10<sup>26</sup> "операций с плавающей точкой" (FLOP)[^16] используется при обучении таких моделей, как серия GPT, Claude, Gemini и др.[^17] (Для сравнения: если бы каждый человек на Земле работал без остановки, выполняя одно вычисление каждые пять секунд, потребовалось бы около миллиарда лет, чтобы это осуществить.) Этот огромный объем вычислений позволяет обучать модели с триллионами весов модели на терабайтах данных — значительной части всего качественного текста, который когда-либо был написан, наряду с большими библиотеками звуков, изображений и видео. Дополняя это обучение обширной дополнительной тренировкой, укрепляющей человеческие предпочтения и хорошую производительность задач, модели, обученные таким образом, демонстрируют производительность, сопоставимую с человеческой, в значительном спектре базовых интеллектуальных задач, включая рассуждение и решение проблем.

Мы также знаем (очень, очень приблизительно), какая скорость вычислений в операциях в секунду достаточна для того, чтобы скорость *вывода*[^18] такой системы соответствовала *скорости* обработки текста человеком. Это примерно 10<sup>15</sup>-10<sup>16</sup> FLOP в секунду.[^19]

Хотя эти модели мощные, по своей природе они ограничены ключевыми способами, весьма аналогично тому, как был бы ограничен отдельный человек, если бы его заставили просто выдавать текст с фиксированной скоростью слов в минуту, не останавливаясь для размышлений или использования каких-либо дополнительных инструментов. Более современные ИИ-системы устраняют эти ограничения через более сложный процесс и архитектуру, объединяющую несколько ключевых элементов:

- Одна или несколько нейронных сетей, при этом одна модель обеспечивает основные когнитивные способности, а до нескольких других выполняют более узкие задачи;
- *Инструментарий*, предоставленный модели и используемый ею — например, способность искать в интернете, создавать или редактировать документы, выполнять программы и т.д.
- *Каркас*, который соединяет входы и выходы нейронных сетей. Очень простой каркас может просто позволить двум "экземплярам" ИИ-модели беседовать друг с другом, или одному проверять работу другого.[^20]
- *Цепочки рассуждений* и связанные техники подсказок делают нечто подобное, заставляя модель, например, генерировать множество подходов к проблеме, затем обрабатывать эти подходы для получения совокупного ответа.
- *Переобучение* моделей для лучшего использования инструментов, каркаса и цепочек рассуждений.

Поскольку эти расширения могут быть очень мощными (и включают сами ИИ-системы), эти композитные системы могут быть весьма сложными и кардинально улучшать возможности ИИ.[^21] И недавно техники каркасов и особенно подсказок с цепочками рассуждений (и встраивания результатов обратно в переобучение моделей для лучшего их использования) были разработаны и применены в [o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/) и [DeepSeek R1](https://api-docs.deepseek.com/news/news250120) для выполнения множества проходов вывода в ответ на данный запрос.[^22] Это фактически позволяет модели "думать" над своим ответом и кардинально повышает способность этих моделей выполнять высококачественные рассуждения в научных, математических и программистских задачах.[^23]

Для данной архитектуры ИИ увеличения в вычислениях для обучения [могут надежно переводиться](https://arxiv.org/abs/2405.10938) в улучшения в наборе четко определенных метрик. Для менее четко определенных общих способностей (таких как обсуждаемые ниже) перевод менее ясен и предсказуем, но почти наверняка более крупные модели с большими вычислениями для обучения будут иметь новые и лучшие способности, даже если трудно предсказать, какими они будут.

Аналогично, композитные системы и особенно достижения в "цепочках рассуждений" (и обучение моделей, которые хорошо с ними работают) открыли масштабирование в вычислениях *вывода*: для данной обученной основной модели по крайней мере некоторые способности ИИ-системы увеличиваются по мере применения большего количества вычислений, что позволяет им "думать усерднее и дольше" над сложными проблемами. Это происходит за счет значительных затрат скорости вычислений, требуя в сотни или тысячи раз больше FLOP/с для достижения человеческой производительности.[^24]

Хотя вычисления — это лишь часть того, что ведет к быстрому прогрессу ИИ,[^25] роль вычислений и возможность композитных систем окажутся решающими как для предотвращения неконтролируемого ИОИ, так и для разработки более безопасных альтернатив.

[^16]: 10<sup>27</sup> означает 1 с 25 нулями после нее, или десять триллионов триллионов. FLOP — это просто арифметическое сложение или умножение чисел с некоторой точностью. Отметим, что производительность аппаратного обеспечения ИИ может различаться в десять раз в зависимости от точности арифметики и архитектуры компьютера. Подсчет операций логических вентилей (AND, OR, NOT) был бы фундаментальным, но они не являются общедоступными или эталонными; для настоящих целей полезно стандартизировать 16-битные операции (FP16), хотя должны быть установлены соответствующие коэффициенты преобразования.

[^17]: Коллекция оценок и точных данных доступна от [Epoch AI](https://epochai.org/data/large-scale-ai-models) и указывает на около 2×10<sup>25</sup> 16-битных FLOP для GPT-4; это примерно соответствует [цифрам, которые просочились](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) для GPT-4. Оценки для других моделей середины 2024 года все находятся в пределах нескольких раз от GPT-4.

[^18]: Вывод — это просто процесс генерации выхода из нейронной сети. Обучение можно рассматривать как последовательность множества выводов и корректировок весов модели.

[^19]: Для производства текста оригинальный GPT-4 требовал 560 TFLOP на сгенерированный токен. Около 7 токенов/с необходимо для поспевания за человеческой мыслью, что дает ≈3×10<sup>15</sup> FLOP/с. Но эффективность снизила это; [эта брошюра NVIDIA](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/) например указывает всего 3×10<sup>14</sup> FLOP/с для модели Llama 405B сопоставимой производительности.

[^20]: В качестве чуть более сложного примера ИИ-система может сначала сгенерировать несколько возможных решений математической задачи, затем использовать другой экземпляр для проверки каждого решения, и наконец использовать третий для синтеза результатов в ясное объяснение. Это позволяет более тщательное и надежное решение проблем, чем за один проход.

[^21]: См. например детали об ["Operator" OpenAI](https://openai.com/index/introducing-operator/), [возможностях инструментов Claude](https://docs.anthropic.com/en/docs/build-with-claude/computer-use) и [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). [Deep Research](https://openai.com/index/introducing-deep-research/) OpenAI вероятно имеет довольно сложную архитектуру, но детали недоступны.

[^22]: Deepseek R1 опирается на итеративное обучение и подсказки модели так, что финальная обученная модель создает обширные цепочки рассуждений. Архитектурные детали недоступны для o1 или o3, однако Deepseek показал, что нет особого "секретного соуса", необходимого для разблокирования масштабирования возможностей с выводом. Но несмотря на получение большого внимания прессы как переворачивающий "статус-кво" в ИИ, это не влияет на основные утверждения этого эссе.

[^23]: Эти модели значительно превосходят стандартные модели в эталонных тестах рассуждений. Например, в GPQA Diamond Benchmark — строгом тесте научных вопросов уровня PhD — GPT-4o [набрал](https://openai.com/index/learning-to-reason-with-llms/) 56%, в то время как o1 и o3 достигли 78% и 88% соответственно, значительно превысив средний балл человеческих экспертов в 70%.

[^24]: O3 OpenAI вероятно затратил ∼10<sup>21</sup>-10<sup>22</sup> FLOP [для завершения каждого из вопросов челленджа ARC-AGI](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai), которые компетентные люди могут выполнить за (скажем) 10-100 секунд, давая цифру скорее около ∼10<sup>20</sup> FLOP/с.

[^25]: Хотя вычисления являются ключевым показателем возможностей ИИ-системы, они взаимодействуют как с качеством данных, так и с алгоритмическими улучшениями. Лучшие данные или алгоритмы могут снизить вычислительные требования, в то время как больше вычислений иногда может компенсировать слабые данные или алгоритмы.

## Глава 4 - Что такое ИОИ и сверхинтеллект?

Что именно крупнейшие технологические компании мира наперегонки создают за закрытыми дверями?

Термин «искусственный общий интеллект» существует уже довольно давно для обозначения ИИ общего назначения «человеческого уровня». Он никогда не был особенно четко определен, но в последние годы парадоксальным образом стал не более ясным, но еще более важным — эксперты одновременно спорят о том, отделяет ли нас от ИОИ несколько десятилетий или он уже достигнут, а компании стоимостью в триллионы долларов устраивают «гонку к ИОИ». (Неоднозначность понятия «ИОИ» недавно ярко проявилась, когда [утечка документов якобы показала](https://gizmodo.com/leaked-documents-show-openai-has-a-very-clear-definition-of-agi-2000543339), что в контракте OpenAI с Microsoft ИОИ определялся как ИИ, который приносит OpenAI доходы в 100 миллиардов долларов — довольно меркантильное, а не высоконаучное определение.)

С идеей ИИ с «интеллектом человеческого уровня» связаны две основные проблемы. Во-первых, люди очень, очень сильно различаются по способности выполнять любой конкретный тип когнитивной работы, поэтому никакого «человеческого уровня» не существует. Во-вторых, интеллект многомерен; хотя корреляции могут существовать, они несовершенны и у ИИ могут быть совершенно иными. Поэтому даже если бы мы могли определить «человеческий уровень» для многих способностей, ИИ наверняка намного превосходил бы его в одних областях, оставаясь значительно слабее в других.[^26]

Тем не менее крайне важно иметь возможность обсуждать типы, уровни и пороговые значения возможностей ИИ. Подход, принятый здесь, подчеркивает, что ИИ общего назначения уже существует и развивается — и будет развиваться — на различных уровнях возможностей, к которым удобно привязать термины, даже если они упрощают действительность, поскольку они соответствуют критически важным порогам с точки зрения влияния ИИ на общество и человечество.

Мы определим «полный» ИОИ как синоним «сверхчеловеческого ИИ общего назначения», имея в виду систему ИИ, способную выполнять практически все человеческие когнитивные задачи на уровне лучших человеческих экспертов или выше, а также осваивать новые навыки и переносить способности в новые области. Это соответствует тому, как «ИОИ» часто определяется в современной литературе. Важно отметить, что это *очень* высокий порог. Ни один человек не обладает таким типом интеллекта; скорее, это тот тип интеллекта, которым обладали бы большие группы лучших человеческих экспертов, если бы их объединили. «Сверхинтеллектом» мы можем назвать способности, выходящие за эти пределы, и определить более ограниченные уровни как ИИ общего назначения «конкурентоспособный с человеком» и «конкурентоспособный с экспертом», которые выполняют широкий спектр задач на уровне типичного профессионала или человека-эксперта.[^27]

Эти термины и некоторые другие собраны в [таблице](https://keepthefuturehuman.ai/essay/docs/#tab:terms) ниже. Для более конкретного понимания того, что могут делать системы различных уровней, полезно серьезно отнестись к определениям и рассмотреть, что они означают.

| Тип ИИ                              | Связанные термины                      | Определение                                                                                                                                                                                              | Примеры                                                                                                                                              |
| ----------------------------------- | -------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------- |
| Узкий ИИ                            | Слабый ИИ                              | ИИ, обученный для конкретной задачи или семейства задач. Превосходен в своей области, но лишен общего интеллекта или способности к переносу обучения.                                                  | Программы распознавания изображений; Голосовые ассистенты (например, Siri, Alexa); Шахматные программы; AlphaFold от DeepMind                      |
| Инструментальный ИИ                 | Дополненный интеллект, ИИ-ассистент    | (Обсуждается далее в эссе.) Система ИИ, улучшающая человеческие способности. Сочетает ИИ общего назначения конкурентоспособный с человеком, узкий ИИ и гарантированный контроль, приоритизируя безопасность и сотрудничество. Поддерживает принятие решений человеком. | Продвинутые ассистенты для программирования; ИИ-инструменты для исследований; Сложные платформы анализа данных. Компетентные, но узкие и управляемые агенты |
| ИИ общего назначения (ИИОП)         |                                        | Система ИИ, адаптируемая к различным задачам, включая те, для которых она не обучалась специально.                                                                                                      | Языковые модели (например, GPT-4, Claude); Мультимодальные модели ИИ; MuZero от DeepMind                                                           |
| ИИОП конкурентоспособный с человеком| ИОИ [слабый]                           | ИИ общего назначения, выполняющий задачи на уровне среднего человека, иногда превосходя его.                                                                                                            | Продвинутые языковые модели (например, O1, Claude 3.5); Некоторые мультимодальные системы ИИ                                                       |
| ИИОП конкурентоспособный с экспертом| ИОИ [частичный]                        | ИИ общего назначения, выполняющий большинство задач на уровне человека-эксперта, со значительной, но ограниченной автономией                                                                           | Возможно, O3 с инструментами и поддержкой, по крайней мере для математики, программирования и некоторых точных наук                               |
| ИОИ [полный]                        | Сверхчеловеческий ИИОП                 | Система ИИ, способная автономно выполнять примерно все человеческие интеллектуальные задачи на экспертном уровне или выше, с эффективным обучением и переносом знаний.                               | [Текущих примеров нет — теоретический]                                                                                                              |
| Сверхинтеллект                      | Высоко сверхчеловеческий ИИОП          | Система ИИ, намного превосходящая человеческие способности во всех областях, превосходящая коллективную человеческую экспертизу. Это превосходство может проявляться в универсальности, качестве, скорости и/или других показателях. | [Текущих примеров нет — теоретический]                                                                                                              |

Мы уже переживаем опыт использования ИИОП вплоть до человеко-конкурентного уровня. Это интегрировалось относительно гладко, поскольку большинство пользователей воспринимают это как наличие умного, но ограниченного временного сотрудника, который делает их более продуктивными со смешанным влиянием на качество их работы.[^28]

Отличие ИИОП конкурентоспособного с экспертом заключалось бы в том, что у него не было бы основных ограничений современного ИИ, и он делал бы то, что делают эксперты: независимую экономически ценную работу, настоящее создание знаний, техническую работу, на которую можно положиться, редко (хотя все еще иногда) совершая глупые ошибки.

Идея полного ИОИ заключается в том, что он *действительно делает* все когнитивные вещи, которые делают даже самые способные и эффективные люди, автономно и без необходимой помощи или надзора. Это включает сложное планирование, изучение новых навыков, управление сложными проектами и так далее. Он мог бы проводить оригинальные передовые исследования. Он мог бы управлять компанией. Какой бы ни была ваша работа, если она преимущественно выполняется за компьютером или по телефону, *он мог бы делать ее как минимум так же хорошо, как вы.* И вероятно, намного быстрее и дешевле. Мы обсудим некоторые последствия ниже, но пока задача для вас — действительно серьезно отнестись к этому. Представьте десять самых знающих и компетентных людей, которых вы знаете или о которых знаете — включая CEO, ученых, профессоров, лучших инженеров, психологов, политических лидеров и писателей. Объедините их всех в одного, который также говорит на 100 языках, обладает феноменальной памятью, работает быстро, неутомим и всегда мотивирован, и работает за зарплату ниже минимальной.[^29] Вот что представляет собой ИОИ.

Для сверхинтеллекта представить это сложнее, поскольку идея заключается в том, что он мог бы совершать интеллектуальные подвиги, на которые не способен ни один человек или даже группа людей — он по определению непредсказуем для нас. Но мы можем получить представление. В качестве базового минимума рассмотрите множество ИОИ, каждый из которых намного способнее даже лучшего человека-эксперта, работающих в 100 раз быстрее человека, с огромной памятью и отличными способностями к координации.[^30] И дальше все только усложняется. Иметь дело со сверхинтеллектом было бы не столько беседой с другим разумом, сколько переговорами с другой (и более продвинутой) цивилизацией.

Итак, насколько близко *мы находимся* к ИОИ и сверхинтеллекту?


[^26]: Например, современные системы ИИ намного превосходят человеческие способности в быстрых арифметических вычислениях или задачах на память, но отстают в абстрактном мышлении и творческом решении проблем.

[^27]: Очень важно, что в качестве конкурента такой ИИ имел бы несколько основных структурных преимуществ, включая: он не устает и не имеет других индивидуальных потребностей, как люди; он может работать на более высоких скоростях просто за счет масштабирования вычислительных мощностей; его можно копировать вместе с любой экспертизой или знаниями, которые он приобретает — и приобретенные знания нейронных сетей можно даже «объединять» для передачи целых наборов навыков между собой; он мог бы общаться на машинной скорости; и он мог бы самоизменяться или самосовершенствоваться более значительными способами и с более высокой скоростью, чем любой человек.

[^28]: Если вы не проводили время, используя современные передовые системы ИИ, я рекомендую это: они действительно полезны и способны, и это также важно для калибровки эффекта, который ИИ будет оказывать по мере роста их мощности.

[^29]: Рассмотрите крупную исследовательскую больницу: полностью реализованный ИОИ мог бы одновременно анализировать все поступающие данные пациентов, следить за каждой новой медицинской статьей, предлагать диагнозы, разрабатывать планы лечения, управлять клиническими исследованиями и координировать расписание персонала — все это работая на уровне, соответствующем или превосходящем лучших специалистов больницы в каждой области. И он мог бы делать это для нескольких больниц одновременно, при долЯ от текущих расходов. К сожалению, вы также должны рассмотреть организованную преступную группировку: полностью реализованный ИОИ мог бы одновременно взламывать, выдавать себя за других, шпионить и шантажировать тысячи жертв, не отставать от правоохранительных органов (которые автоматизируются гораздо медленнее), разрабатывать новые схемы заработка и координировать расписание персонала — если персонал вообще есть.

[^30]: В своем [эссе](https://darioamodei.com/machines-of-loving-grace) Дарио Амодеи, CEO Anthropic, вызвал в памяти «Страну \[миллиона\] гениев».

## Глава 5 - На пороге

Путь от сегодняшних систем ИИ к полноценному ИОИ кажется поразительно коротким и предсказуемым.

Последние десять лет принесли драматические достижения в области ИИ благодаря огромным [вычислительным](https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year), человеческим и [финансовым](https://arxiv.org/abs/2405.21015) ресурсам. Многие узкоспециализированные приложения ИИ превосходят людей в своих задачах и определенно работают намного быстрее и дешевле.[^31] Существуют также узкоспециализированные сверхчеловеческие агенты, которые могут разгромить всех людей в играх с ограниченным набором правил, таких как [Го](https://www.nature.com/articles/nature16961), [шахматы](https://arxiv.org/abs/1712.01815) и [покер](https://www.deepstack.ai/), а также более [универсальные агенты](https://deepmind.google/discover/blog/a-generalist-agent/), которые могут планировать и выполнять действия в упрощенных виртуальных средах так же эффективно, как это делают люди.

Самое заметное — это современные системы общего ИИ от OpenAI/Microsoft, Google/Deepmind, Anthropic/Amazon, Facebook/Meta, X.ai/Tesla и других [^32], которые появились с начала 2023 года и с тех пор стабильно (хотя и неравномерно) наращивают свои возможности. Все они созданы с помощью предсказания токенов на огромных текстовых и мультимедийных наборах данных в сочетании с обширной обратной связью через подкрепление от людей и других систем ИИ. Некоторые из них также включают развитые инструментальные и каркасные системы.

### Сильные и слабые стороны современных универсальных систем

Эти системы хорошо справляются с всё более широким спектром тестов, предназначенных для измерения интеллекта и экспертизы, демонстрируя прогресс, который удивил даже специалистов в этой области:

- При первом выпуске GPT-4 [соответствовал или превосходил типичную человеческую производительность](https://arxiv.org/abs/2303.08774) в стандартных академических тестах, включая SAT, GRE, вступительные экзамены и экзамены на получение статуса адвоката. Более поздние модели, вероятно, работают значительно лучше, хотя результаты не являются общедоступными.
- Тест Тьюринга — долгое время считавшийся ключевым эталоном для «настоящего» ИИ — теперь регулярно проходится современными языковыми моделями в некоторых формах, как неформально, так и в [формальных исследованиях](https://arxiv.org/abs/2405.08007).[^33]
- В комплексном эталонном тесте MMLU, охватывающем 57 академических предметов, [последние модели достигают результатов уровня экспертов в предметных областях](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu) (∼90%) [^34]
- Техническая экспертиза продвинулась драматически: в эталонном тесте GPQA по физике уровня выпускников [производительность подскочила](https://epoch.ai/data/ai-benchmarking-dashboard) от почти случайного угадывания (GPT-4, 2022) до экспертного уровня (o1-preview, 2024).
- Даже тесты, специально разработанные как устойчивые к ИИ, сдаются: O3 от OpenAI [как сообщается](https://www.nextbigfuture.com/2024/12/openai-releases-o3-model-with-high-performance-and-high-cost.html) решает эталонный тест абстрактного решения задач ARC-AGI на человеческом уровне, достигает производительности топ-экспертов в программировании и набирает 25% в задачах «передовой математики» Epoch AI, предназначенных для вызова элитных математиков.[^35]
- Тенденция настолько очевидна, что разработчик MMLU теперь создал [«Последний экзамен человечества»](https://agi.safe.ai/) — зловещее название, отражающее возможность того, что ИИ вскоре превзойдет человеческую производительность в любом значимом тесте. На момент написания этого текста есть заявления о том, что системы ИИ достигают 27% (согласно [Сэму Альтману](https://x.com/sama/status/1886220281565381078)) и 35% (согласно [этой статье](https://arxiv.org/abs/2502.09955)) в этом чрезвычайно сложном экзамене. Крайне маловероятно, что любой отдельный человек смог бы это сделать.

Несмотря на эти впечатляющие цифры (и их очевидный интеллект при взаимодействии с ними) [^36], есть много вещей, которые (по крайней мере выпущенные версии) эти нейронные сети *не могут* делать. В настоящее время большинство из них бестелесны — существуют только на серверах — и обрабатывают в лучшем случае текст, звук и статические изображения (но не видео). Критически важно, что большинство не могут выполнять сложные запланированные действия, требующие высокой точности.[^37] И есть ряд других качеств, сильных в высокоуровневом человеческом познании, которые в настоящее время слабы в выпущенных системах ИИ.

Следующая таблица перечисляет ряд из них, основываясь на системах ИИ середины 2024 года, таких как GPT-4o, Claude 3.5 Sonnet и Google Gemini 1.5.[^38] Ключевой вопрос для того, насколько быстро общий ИИ станет более мощным, заключается в том, в какой степени простое выполнение *большего объема того же самого* даст результаты, по сравнению с добавлением дополнительных, но *известных* техник, по сравнению с разработкой или внедрением *действительно новых* направлений исследований ИИ. Мои собственные прогнозы для этого приведены в таблице в терминах вероятности того, что каждый из этих сценариев приведет эту способность к человеческому уровню и выше.

<table><tbody><tr><th>Способность</th><th>Описание способности</th><th>Статус/прогноз</th><th>Масштабирование/известное/новое</th></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td colspan="4"><em>Основные когнитивные способности</em></td></tr><tr><td>Рассуждение</td><td>Люди могут выполнять точные многошаговые рассуждения, следуя правилам и проверяя точность.</td><td>Драматический недавний прогресс с использованием расширенной цепочки рассуждений и переобучения</td><td>95/5/5</td></tr><tr><td>Планирование</td><td>Люди демонстрируют долгосрочное и иерархическое планирование.</td><td>Улучшается с масштабом; может быть существенно усилено с использованием каркасных систем и лучших техник обучения.</td><td>10/85/5</td></tr><tr><td>Обоснованность истины</td><td>Системы общего ИИ придумывают необоснованную информацию для удовлетворения запросов.</td><td>Улучшается с масштабом; калибровочные данные доступны внутри модели; может быть проверено/улучшено через каркасные системы.</td><td>30/65/5</td></tr><tr><td>Гибкое решение задач</td><td>Люди могут распознавать новые паттерны и изобретать новые решения для сложных проблем; современные модели машинного обучения с этим борются.</td><td>Улучшается с масштабом, но слабо; может быть решаемо с помощью нейросимволических или обобщенных техник «поиска».</td><td>15/75/10</td></tr><tr><td colspan="4"><em>Обучение и знания</em></td></tr><tr><td>Обучение и память</td><td>Люди имеют рабочую, кратковременную и долговременную память, все из которых динамичны и взаимосвязаны.</td><td>Все модели обучаются во время тренировки; системы общего ИИ обучаются в пределах контекстного окна и во время тонкой настройки; существуют техники «непрерывного обучения» и другие, но еще не интегрированы в крупные системы общего ИИ.</td><td>5/80/15</td></tr><tr><td>Абстракция и рекурсия</td><td>Люди могут отображать и переносить наборы отношений в более абстрактные для рассуждений и манипуляций, включая рекурсивные «мета» рассуждения.</td><td>Слабо улучшается с масштабом; может возникнуть в нейросимволических системах.</td><td>30/50/20</td></tr><tr><td>Модель(и) мира</td><td>Люди имеют и постоянно обновляют предиктивную модель мира, в рамках которой они могут решать задачи и выполнять физические рассуждения</td><td>Улучшается с масштабом; обновление связано с обучением; системы общего ИИ слабы в предсказании реального мира.</td><td>20/50/30</td></tr><tr><td colspan="4"><em>Самость и агентность</em></td></tr><tr><td>Агентность</td><td>Люди могут предпринимать действия для достижения целей на основе планирования/предсказания.</td><td>Многие системы машинного обучения являются агентными; языковые модели могут стать агентами через обертки.</td><td>5/90/5</td></tr><tr><td>Самоуправление</td><td>Люди развивают и преследуют свои собственные цели с внутренне генерируемой мотивацией и стремлением.</td><td>В основном состоит из агентности плюс оригинальность; вероятно возникнет в сложных агентных системах с абстрактными целями.</td><td>40/45/15</td></tr><tr><td>Самореферентность</td><td>Люди понимают себя и рассуждают о себе как о расположенных в окружающей среде/контексте.</td><td>Улучшается с масштабом и может быть дополнено вознаграждением в обучении.</td><td>70/15/15</td></tr><tr><td>Самосознание</td><td>Люди имеют знания о своих собственных мыслях и ментальных состояниях и могут рассуждать о них.</td><td>Существует в некотором смысле в системах общего ИИ, которые могут якобы пройти классический «зеркальный тест» на самосознание. Может быть улучшено каркасными системами; но неясно, достаточно ли этого.</td><td>20/55/25</td></tr><tr><td colspan="4"><em>Интерфейс и окружающая среда</em></td></tr><tr><td>Воплощенный интеллект</td><td>Люди понимают и активно взаимодействуют со своей реальной окружающей средой.</td><td>Обучение с подкреплением хорошо работает в виртуальных и реальных (роботизированных) средах и может быть интегрировано в мультимодальные трансформеры.</td><td>5/85/10</td></tr><tr><td>Мультисенсорная обработка</td><td>Люди интегрируют и обрабатывают в реальном времени визуальные, аудио и другие сенсорные потоки.</td><td>Обучение в нескольких модальностях, похоже, «просто работает» и улучшается с масштабом. Обработка видео в реальном времени сложна, но, например, системы автономного вождения быстро улучшаются.</td><td>30/60/10</td></tr><tr><td colspan="4"><em>Способности высшего порядка</em></td></tr><tr><td>Оригинальность</td><td>Современные модели машинного обучения креативны в преобразовании и комбинировании существующих идей/работ, но люди могут строить новые рамки и структуры, иногда связанные с их идентичностью.</td><td>Может быть трудно отличить от «креативности», которая может масштабироваться в неё; может возникнуть из креативности плюс самосознание.</td><td>50/40/10</td></tr><tr><td>Чувствительность</td><td>Люди испытывают квалиа; они могут быть положительной, отрицательной или нейтральной валентности; «быть кем-то» означает нечто.</td><td>Очень сложно и философски спорно определить, обладает ли данная система этим.</td><td>5/10/85</td></tr></tbody></table>

Ключевые способности, в настоящее время находящиеся ниже уровня человеческих экспертов в современных системах общего ИИ, сгруппированные по типам. Третий столбец суммирует текущее состояние. Финальный столбец показывает прогнозируемую вероятность (%) того, что производительность человеческого уровня будет достигнута через: масштабирование текущих техник / комбинирование с известными техниками / разработку новых техник. Эти способности не являются независимыми, и увеличение любой из них обычно сопровождается увеличением других. Обратите внимание, что не все (особенно чувствительность) необходимы для систем ИИ, способных продвигать разработку ИИ, подчеркивая возможность мощного, но нечувствительного ИИ.

Разбивка того, что «отсутствует» таким образом, делает довольно очевидным, что мы вполне на пути к интеллекту, широко превосходящему человеческий, путем масштабирования существующих или известных техник.[^39]

Сюрпризы всё же возможны. Даже оставляя в стороне «чувствительность», могут быть некоторые из перечисленных основных когнитивных способностей, которые действительно нельзя реализовать с помощью текущих техник и которые требуют новых. Но рассмотрите следующее. Нынешние усилия, предпринимаемые многими крупнейшими компаниями мира, составляют несколько раз больше расходов проекта «Аполлон» и в десятки раз больше расходов Манхэттенского проекта,[^40] и задействуют тысячи лучших технических специалистов с неслыханными зарплатами. Динамика последних нескольких лет теперь направила на это больше человеческих интеллектуальных ресурсов (с добавлением ИИ), чем любое начинание в истории. Мы не должны делать ставку на неудачу.

### Большая цель: универсальные автономные агенты

Разработка общего ИИ за последние несколько лет была сосредоточена на создании общего и мощного, но инструментального ИИ: он функционирует в первую очередь как (довольно) верный помощник и обычно не предпринимает действий самостоятельно. Это отчасти по замыслу, но в основном потому, что эти системы просто не были достаточно компетентными в соответствующих навыках, чтобы им можно было доверить сложные действия.[^41]

Компании и исследователи ИИ, однако, всё больше [смещают фокус](https://www.axios.com/2025/01/23/davos-2025-ai-agents) в сторону *автономных* универсальных агентов экспертного уровня.[^42] Это позволило бы системам действовать больше как человеческий помощник, которому пользователь может делегировать реальные действия.[^43] Что для этого потребуется? Ряд способностей из таблицы «чего не хватает» вовлечены, включая сильную обоснованность истины, обучение и память, абстракцию и рекурсию, и моделирование мира (для интеллекта), планирование, агентность, оригинальность, самоуправление, самореферентность и самосознание (для автономности), и мультисенсорную обработку, воплощенный интеллект и гибкое решение задач (для универсальности).[^44]

Это тройное пересечение высокой автономности (независимость действий), высокой универсальности (охват и широта задач) и высокого интеллекта (компетентность в когнитивных задачах) в настоящее время уникально для людей. Это неявно то, что многие, вероятно, имеют в виду, когда думают об ИОИ — как с точки зрения его ценности, так и его рисков.

Это дает еще один способ определить И-О-И как ***А*** втономный-***У*** ниверсальный-***И*** нтеллект, и мы увидим, что это тройное пересечение обеспечивает очень ценную линзу для высокопроизводительных систем как в понимании их рисков и преимуществ, так и в управлении ИИ.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Simple-1024x1024.png&w=3840&q=75) Трансформационная зона мощности и риска И-О-И возникает из пересечения трех ключевых свойств: высокой Автономности, высокого Интеллекта в задачах и высокой Универсальности.

### Цикл (само)улучшения ИИ

Финальный критический фактор в понимании прогресса ИИ — это уникальная технологическая петля обратной связи ИИ. В разработке ИИ успех — как в демонстрационных системах, так и в развернутых продуктах — приносит дополнительные инвестиции, таланты и конкуренцию, и мы в настоящее время находимся в середине огромной петли обратной связи ажиотажа-плюс-реальности ИИ, которая движет сотнями миллиардов или даже триллионами долларов инвестиций.

Этот тип петли обратной связи может произойти с любой технологией, и мы видели это во многих, где рыночный успех порождает инвестиции, которые порождают улучшение и лучший рыночный успех. Но разработка ИИ идет дальше, поскольку теперь системы ИИ помогают разрабатывать новые и более мощные системы ИИ.[^45] Мы можем думать об этой петле обратной связи в пять стадий, каждая с более коротким временным масштабом, чем предыдущая, как показано в таблице.

*Цикл улучшения ИИ работает на нескольких временных масштабах, при этом каждая стадия потенциально ускоряет последующие стадии. Более ранние стадии уже идут полным ходом, в то время как более поздние стадии остаются спекулятивными, но могут развиваться очень быстро, как только будут разблокированы.*

Несколько из этих стадий уже идут, и пара явно начинается. Последняя стадия, в которой системы ИИ автономно улучшают себя, была основным элементом литературы о риске очень мощных систем ИИ, и не без причины.[^46] Но важно отметить, что это просто самая драматическая форма цикла обратной связи, который уже начался и может привести к большим сюрпризам в быстром развитии технологии.


[^31]: Вы используете гораздо больше этого ИИ, чем, вероятно, думаете, управляя генерацией и распознаванием речи, обработкой изображений, алгоритмами новостных лент и т.д.

[^32]: Хотя отношения между этими парами компаний довольно сложные и многогранные, я явно перечислил их, чтобы показать как огромную общую рыночную капитализацию фирм, теперь вовлеченных в разработку ИИ, так и то, что за даже «меньшими» компаниями, такими как Anthropic, стоят чрезвычайно глубокие карманы через инвестиции и крупные партнерские сделки.

[^33]: Стало модным пренебрегать тестом Тьюринга, но он довольно мощный и общий. В слабых версиях он показывает, могут ли обычные люди, взаимодействующие с ИИ (который обучен действовать как человек) обычными способами в течение коротких периодов, сказать, является ли это ИИ. Они не могут. Во-вторых, высоко состязательный тест Тьюринга может исследовать по сути любой элемент человеческих способностей и интеллекта — например, сравнивая систему ИИ с человеком-экспертом, оцениваемую другими экспертами-людьми. Есть смысл, в котором большая часть оценки ИИ является обобщенной формой теста Тьюринга.

[^34]: Это по областям — ни один человек не мог бы правдоподобно достичь таких результатов во всех предметах одновременно.

[^35]: Это проблемы, которые заняли бы даже у отличных математиков значительное время для решения, если бы они вообще могли их решить.

[^36]: Если вы настроены скептически, сохраните свой скептицизм, но действительно попробуйте самые современные модели, а также попробуйте сами некоторые тестовые вопросы, которые они могут пройти. Как профессор физики, я предсказал бы с почти полной уверенностью, что, например, топ-модели прошли бы квалификационный экзамен для выпускников в нашем департаменте.

[^37]: Эта и другие слабости, такие как конфабуляция, замедлили рыночное принятие и привели к разрыву между воспринимаемыми и заявленными способностями (что также должно рассматриваться через призму интенсивной рыночной конкуренции и необходимости привлекать инвестиции). Это запутало как общественность, так и политиков относительно фактического состояния прогресса ИИ. Хотя, возможно, не соответствуя ажиотажу, прогресс очень реален.

[^38]: Основным достижением с тех пор стала разработка систем, обученных для высококачественного рассуждения, использующих больше вычислений во время инференса и большее обучение с подкреплением. Поскольку эти модели новые и их способности менее протестированы, я не полностью переделал эту таблицу, кроме «рассуждения», которое я рассматриваю как по сути решенное. Но я обновил прогнозы на основе испытанных и сообщенных способностей этих систем.

[^39]: Предыдущие волны оптимизма в отношении ИИ в 1960-х и 1980-х годах закончились «зимами ИИ», когда обещанные способности не материализовались. Однако текущая волна принципиально отличается тем, что достигла сверхчеловеческой производительности во многих областях, подкрепленной массивными вычислительными ресурсами и коммерческим успехом.

[^40]: Полный проект «Аполлон» [стоил около 250 миллиардов долларов США в долларах 2020 года](https://www.planetary.org/space-policy/cost-of-apollo), а Манхэттенский проект [менее десятой части этого](https://www.brookings.edu/the-costs-of-the-manhattan-project/). Goldman Sachs [прогнозирует триллион долларов расходов только на дата-центры ИИ](https://www.datacenterdynamics.com/en/news/goldman-sachs-1tn-to-be-spent-on-ai-data-centers-chips-and-utility-upgrades-with-little-to-show-for-it-so-far/) в ближайшие несколько лет.

[^41]: Хотя люди делают много ошибок, мы недооцениваем, насколько надежными мы можем быть! Поскольку вероятности перемножаются, задача, требующая 20 шагов для правильного выполнения, требует, чтобы каждый шаг был на 97% надежен, просто чтобы выполнить её правильно в половине случаев. Мы делаем такие задачи всё время.

[^42]: Сильный шаг в этом направлении совсем недавно был предпринят с [«Deep Research»](https://openai.com/index/introducing-deep-research/) от OpenAI — помощником, который автономно проводит общие исследования, описанным как «новая агентная способность, которая проводит многошаговые исследования в интернете для сложных задач».

[^43]: Такие вещи, как заполнение этой надоедливой PDF-формы, бронирование рейсов и т.д. Но с докторской степенью в 20 областях! Так что также: написать эту диссертацию для вас, договориться об этом контракте для вас, доказать эту теорему для вас, создать эту рекламную кампанию для вас и т.д. Что делаете *вы*? Вы говорите ему, что делать, конечно.

[^44]: Обратите внимание, что чувствительность *не* является явно необходимой, и ИИ в этом тройном пересечении не обязательно подразумевает её.

[^45]: Ближайшей аналогией здесь является, возможно, чиповая технология, где разработка поддерживала закон Мура в течение десятилетий, поскольку компьютерные технологии помогают людям проектировать следующее поколение чиповых технологий. Но ИИ будет гораздо более прямым.

[^46]: Важно на мгновение осознать, что ИИ может — вскоре — улучшать себя в временном масштабе дней или недель. Или меньше. Помните об этом, когда кто-то говорит вам, что способность ИИ определенно далека.

## Глава 6 - Гонка за ИОИ

Какие движущие силы стоят за гонкой по созданию ИОИ как для компаний, так и для стран?

Недавний быстрый прогресс в области ИИ привел к необычайному уровню внимания и инвестиций — и одновременно стал его результатом. Это отчасти объясняется успехами в разработке ИИ, но происходит и нечто большее. Почему одни из крупнейших компаний на Земле и даже целые страны соревнуются в создании не просто ИИ, а именно ИОИ и сверхинтеллекта?

### Что направило исследования ИИ к человекоподобному интеллекту

Примерно до последних пяти лет ИИ был в основном академической и научной исследовательской проблемой, поэтому развивался главным образом под влиянием любопытства и стремления понять интеллект и то, как создать его в новой среде.

На этом этапе большинство исследователей уделяло относительно мало внимания преимуществам или опасностям ИИ. Когда их спрашивали, зачем нужно развивать ИИ, типичный ответ мог включать довольно расплывчатый список проблем, с которыми мог бы помочь ИИ: новые лекарства, новые материалы, новая наука, более умные процессы и в целом улучшение жизни людей.[^47]

Это достойные цели![^48] Хотя мы можем и будем ставить под сомнение, необходим ли для достижения этих целей именно ИОИ — а не ИИ в целом, — они демонстрируют тот идеализм, с которого начинали многие исследователи ИИ.

Однако за последние пять лет ИИ превратился из относительно чистой исследовательской области в гораздо более инженерную и продуктовую сферу, во многом благодаря некоторым из крупнейших мировых компаний.[^49] Исследователи, оставаясь важными, больше не управляют процессом.

### Зачем компании пытаются создать ИОИ?

Так почему же гигантские корпорации (и еще больше инвесторы) вкладывают огромные ресурсы в создание ИОИ? Есть две движущие силы, в которых большинство компаний довольно честно признаются: они видят в ИИ движитель продуктивности для общества и источник прибыли для себя. Поскольку общий ИИ по своей природе универсален, приз огромен: вместо выбора сектора для создания продуктов и услуг можно попробовать охватить *все сразу*. Крупные технологические компании выросли до огромных размеров, производя цифровые товары и услуги, и по крайней мере некоторые руководители наверняка видят в ИИ просто следующий шаг в их предоставлении, с рисками и преимуществами, которые расширяют и повторяют те, что дают поиск, социальные сети, ноутбуки, телефоны и т.д.

Но зачем именно ИОИ? На это есть очень простой ответ, который большинство компаний и инвесторов избегают обсуждать публично.[^50]

Дело в том, что ИОИ может напрямую, один к одному, *заменить работников*.

Не дополнить, не расширить возможности, не повысить продуктивность. Даже не *вытеснить*. Все это может и будет сделано ИИ, не являющимся ИОИ. ИОИ — это именно то, что может полностью *заменить* работников умственного труда (а с робототехникой — и многих физических работников). В подтверждение этой точки зрения достаточно взглянуть на [(публично заявленное) определение](https://openai.com/our-structure/) ИОИ от OpenAI, которое звучит как «высокоавтономная система, превосходящая людей в большинстве экономически ценных видов работ».

Приз здесь (для компаний!) огромен. Затраты на рабочую силу составляют существенный процент от мировой экономики объемом ∼100 триллионов долларов. Даже если будет захвачена лишь часть этого за счет замены человеческого труда трудом ИИ, речь идет о триллионах долларов годового дохода. ИИ-компании также понимают, кто готов платить. Как они видят это, вы не будете платить тысячи долларов в год за инструменты продуктивности. Но компания *будет* платить тысячи долларов в год за замену вашего труда, если сможет.

### Почему страны чувствуют, что должны участвовать в гонке за ИОИ

Заявленные мотивы стран для стремления к ИОИ сосредоточены на экономическом и научном лидерстве. Аргумент убедителен: ИОИ мог бы кардинально ускорить научные исследования, технологическое развитие и экономический рост. Учитывая ставки, утверждают они, ни одна крупная держава не может позволить себе отстать.[^51]

Но есть и дополнительные, в основном не озвучиваемые движущие силы. Не вызывает сомнений, что когда некоторые военные лидеры и лидеры национальной безопасности встречаются за закрытыми дверями для обсуждения чрезвычайно мощной и катастрофически рискованной технологии, их внимание сосредоточено не на том, «как нам избежать этих рисков», а на том, «как нам получить это первыми?» Военные и разведывательные лидеры видят в ИОИ потенциальную революцию в военном деле, возможно, самую значительную со времен ядерного оружия. Страх заключается в том, что первая страна, разработавшая ИОИ, может получить непреодолимое стратегическое преимущество. Это создает классическую динамику гонки вооружений.

Мы увидим, что это мышление «гонки за ИОИ»,[^52] хотя и убедительно, глубоко порочно. Не потому, что гонка опасна и рискованна — хотя так оно и есть, — а из-за природы технологии. Невысказанное предположение состоит в том, что ИОИ, как и другие технологии, контролируем государством, которое его разрабатывает, и является дающим силу благом для общества, у которого его больше всего. Как мы увидим, вероятно, он не будет ни тем, ни другим.

### Зачем сверхинтеллект?

В то время как компании публично сосредотачиваются на продуктивности, а страны — на экономическом и технологическом росте, для тех, кто сознательно стремится к полному ИОИ и сверхинтеллекту, это лишь начало. Что они действительно имеют в виду? Хотя это редко говорится вслух, цели включают:

1. Лечение многих или всех болезней;
2. Остановку и обращение старения;
3. Новые устойчивые источники энергии, такие как термоядерный синтез;
4. Улучшение человека или создание организмов по заказу через генную инженерию;
5. Нанотехнологии и молекулярное производство;
6. Загрузку сознания;
7. Экзотическую физику или космические технологии;
8. Сверхчеловеческие советы и поддержку в принятии решений;
9. Сверхчеловеческое планирование и координацию.

Первые три — это в основном «односторонние» технологии, то есть, вероятно, имеющие довольно сильный чистый положительный эффект. Трудно возражать против лечения болезней или возможности жить дольше по своему выбору. И мы уже пожинали негативную сторону термоядерного синтеза (в виде ядерного оружия); теперь было бы прекрасно получить положительную сторону. Вопрос с этой первой категорией заключается в том, компенсирует ли получение этих технологий раньше связанный с этим риск.

Следующие четыре явно обоюдоострые: трансформационные технологии с потенциально огромными плюсами и колоссальными рисками, очень похожие на ИИ. Все они, если бы появились из черного ящика завтра и были бы развернуты, были бы невероятно сложными в управлении.[^53]

Последние два касаются того, что сверхчеловеческий ИИ делает что-то сам, а не просто изобретает технологии. Точнее, отбрасывая эвфемизмы, они предполагают, что мощные ИИ-системы говорят людям, что делать. Называть это «советом» лицемерно, если система, дающая советы, гораздо могущественнее тех, кому советует, и кто не может осмысленно понять основу решения (или даже если это предоставляется, поверить, что советчик не предоставил бы столь же убедительное обоснование для другого решения).

Это указывает на ключевой пункт, отсутствующий в приведенном выше списке:

10. Власть.

Совершенно ясно, что в основе нынешней гонки за сверхчеловеческим ИИ лежит идея, что *интеллект = власть*. Каждый участник гонки рассчитывает стать лучшим держателем этой власти и сможет владеть ею якобы из благих побуждений, не позволив ей ускользнуть или быть отнятой из-под их контроля.

То есть то, за чем действительно гонятся компании и нации, — это не просто плоды ИОИ и сверхинтеллекта, но власть контролировать, кто получает к ним доступ и как они используются. Компании видят себя ответственными распорядителями этой власти на службе акционеров и человечества; нации видят себя необходимыми стражами, предотвращающими получение враждебными силами решающего преимущества. Оба опасно ошибаются, не признавая, что сверхинтеллект по своей природе не может быть надежно контролируем любым человеческим институтом. Мы увидим, что природа и динамика сверхинтеллектуальных систем делают человеческий контроль крайне сложным, если не невозможным.

Эта динамика гонки — как корпоративная, так и геополитическая — делает определенные риски почти неизбежными, если не будет решительно прервана. Теперь мы обратимся к рассмотрению этих рисков и того, почему они не могут быть адекватно смягчены в рамках конкурентной[^54] парадигмы разработки.


[^47]: Более точный список достойных целей — это [Цели устойчивого развития ООН](https://sdgs.un.org/goals). В некотором смысле это самое близкое, что у нас есть к набору глобальных консенсусных целей того, что мы хотели бы увидеть улучшенным в мире. ИИ мог бы помочь.

[^48]: Технологии в целом обладают трансформационной экономической и социальной силой для улучшения человеческой жизни, как свидетельствуют тысячи лет. В этом ключе длинное и убедительное изложение позитивного видения ИОИ можно найти в [этом эссе](https://darioamodei.com/machines-of-loving-grace) основателя Anthropic Дарио Амодеи.

[^49]: Частные инвестиции в ИИ [начали расти в 2018-19 годах, примерно тогда превысив государственные инвестиции](https://cset.georgetown.edu/publication/tracking-ai-investment/) и с тех пор значительно их опередили.

[^50]: Могу засвидетельствовать, что за более закрытыми дверями у них нет таких угрызений совести. И это становится более публичным; см., например, новый ["запрос на стартапы"](https://www.ycombinator.com/rfs) Y-combinator, многие части которого прямо призывают к полной замене человеческих работников. Цитирую их: "Ценностное предложение B2B SaaS заключалось в том, чтобы сделать человеческих работников постепенно более эффективными. Ценностное предложение вертикальных ИИ-агентов — полностью автоматизировать работу... Вполне возможно, что эта возможность достаточно велика, чтобы создать еще сотню единорогов." (Для тех, кто не знаком с жаргоном Силиконовой долины, "B2B" означает "бизнес для бизнеса", а единорог — это компания стоимостью в миллиард долларов. То есть они говорят о более чем сотне миллиардных компаний, которые заменяют работников для других предприятий.)

[^51]: См., например, недавний [отчет Комиссии США-Китай по экономическим вопросам и безопасности](https://www.uscc.gov/sites/default/files/2024-11/2024_Executive_Summary.pdf). Хотя в самом отчете было на удивление мало обоснований, главная рекомендация заключалась в том, чтобы Конгресс США "создал и финансировал программу типа Манхэттенского проекта, посвященную гонке за получение возможности искусственного общего интеллекта (ИОИ)".

[^52]: Компании теперь принимают это геополитическое обрамление как щит против любых ограничений на разработку ИИ, обычно способами, которые явно корыстны, а иногда способами, которые даже не имеют базового смысла. Рассмотрите [Подход Meta к передовому ИИ](https://about.fb.com/news/2025/02/meta-approach-frontier-ai/), который одновременно утверждает, что Америка должна "[Закрепить свою] позицию лидера в технологических инновациях, экономическом росте и национальной безопасности", а также что она должна делать это, открыто выпуская свои самые мощные ИИ-системы — что включает передачу их напрямую геополитическим соперникам и противникам.

[^53]: Таким образом, нам, вероятно, пришлось бы оставить управление этими технологиями ИИ. Но это было бы очень проблематичным делегированием контроля, к чему мы вернемся ниже.

[^54]: Конкуренция в технологическом развитии часто приносит важные преимущества: предотвращение монополистического контроля, стимулирование инноваций и снижения затрат, обеспечение разнообразных подходов и создание взаимного надзора. Однако с ИОИ эти преимущества должны быть взвешены против уникальных рисков от динамики гонки и давления на сокращение мер предосторожности.

## Глава 7 - Что произойдет, если мы создадим ИОИ, следуя нынешним путем?

Общество не готово к системам уровня ИОИ. Если мы создадим их в ближайшее время, все может стать очень плохо.

Разработка полноценного искусственного общего интеллекта – того, что мы здесь называем ИИ «за воротами» – станет фундаментальным сдвигом в природе мира: по самой своей сути это означает добавление на Землю нового вида интеллекта с возможностями, превосходящими человеческие.

То, что произойдет дальше, зависит от многих факторов, включая природу технологии, выбор тех, кто ее разрабатывает, и контекст мира, в котором она создается.

В настоящее время полноценный ИОИ разрабатывается горсткой крупных частных компаний в гонке друг с другом, при минимальном осмысленном регулировании или внешнем надзоре,[^55] в обществе со все более слабыми и даже дисфункциональными основными институтами,[^56] во времена высокой геополитической напряженности и низкой международной координации. Хотя некоторые движимы альтруистическими мотивами, многие из тех, кто этим занимается, руководствуются деньгами, или властью, или и тем, и другим.

Предсказания очень сложны, но существуют некоторые достаточно понятные динамики и подходящие аналогии с предыдущими технологиями, которые могут служить ориентиром. И, к сожалению, несмотря на обещания ИИ, они дают серьезные основания для глубокого пессимизма относительно того, как развернется наша нынешняя траектория.

Говоря прямо, на нашем нынешнем пути разработка ИОИ будет иметь некоторые положительные эффекты (и сделает некоторых людей очень, очень богатыми). Но природа технологии, фундаментальные динамики и контекст, в котором она разрабатывается, убедительно указывают на то, что: мощный ИИ кардинально подорвет наше общество и цивилизацию; мы потеряем над ним контроль; вполне возможно, что мы окажемся в мировой войне из-за него; мы потеряем (или уступим) контроль *ему*; это приведет к искусственному сверхинтеллекту, который мы абсолютно не сможем контролировать и который будет означать конец мира, управляемого людьми.

Это сильные заявления, и я хотел бы, чтобы они были праздными спекуляциями или неоправданным «думизмом». Но именно на это указывают наука, теория игр, эволюционная теория и история. Этот раздел подробно развивает эти утверждения и их обоснования.

### Мы подорвем наше общество и цивилизацию

Несмотря на то, что вы можете услышать в залах заседаний Кремниевой долины, большинство потрясений – особенно очень быстрых – не являются полезными. Существует гораздо больше способов ухудшить сложные системы, чем улучшить их. Наш мир функционирует так хорошо, как сейчас, потому что мы кропотливо строили процессы, технологии и институты, которые постепенно его улучшали.[^57] Брать кувалду по заводу редко улучшает операции.

Вот неполный каталог способов, которыми системы ИОИ могли бы потрясти нашу цивилизацию.

- Они кардинально потрясли бы рынок труда, приведя *как минимум* к резкому росту неравенства доходов и потенциально к массовой неполной занятости или безработице в сроки, слишком короткие для адаптации общества.[^58]
- Они, вероятно, привели бы к концентрации огромной экономической, социальной и политической власти – потенциально большей, чем у национальных государств – в руках небольшого числа крупных частных интересов, неподотчетных обществу.
- Они могли бы внезапно сделать ранее трудную или дорогую деятельность тривиально легкой, дестабилизируя социальные системы, которые зависят от того, что определенная деятельность остается затратной или требует значительных человеческих усилий.[^59]
- Они могли бы затопить системы сбора, обработки и передачи информации в обществе полностью реалистичными, но ложными, спамовыми, чрезмерно таргетированными или манипулятивными медиа настолько основательно, что станет невозможно определить, что является физически реальным или нет, человеческим или нет, фактическим или нет, и заслуживающим доверия или нет.[^60]
- Они могли бы создать опасную и почти полную интеллектуальную зависимость, когда человеческое понимание ключевых систем и технологий атрофируется по мере того, как мы все больше полагаемся на системы ИИ, которые не можем полностью понять.
- Они могли бы фактически покончить с человеческой культурой, когда почти все культурные объекты (текст, музыка, визуальное искусство, фильмы и т.д.), потребляемые большинством людей, создаются, опосредуются или курируются нечеловеческими умами.
- Они могли бы обеспечить эффективные системы массового наблюдения и манипулирования, используемые правительствами или частными интересами для контроля над населением и преследования целей, противоречащих общественным интересам.
- Подрывая человеческий дискурс, дебаты и избирательные системы, они могли бы снизить доверие к демократическим институтам до точки, когда они эффективно (или явно) заменяются другими, положив конец демократии в государствах, где она сейчас существует.
- Они могли бы стать или создать продвинутые самовоспроизводящиеся интеллектуальные программные вирусы и черви, которые могли бы распространяться и эволюционировать, массово нарушая глобальные информационные системы.
- Они могут кардинально увеличить способность террористов, злоумышленников и государств-изгоев причинять вред через биологическое, химическое, кибернетическое, автономное или другое оружие, не предоставляя ИИ уравновешивающей способности предотвращать такой вред. Аналогично они подорвали бы национальную безопасность и геополитические балансы, делая экспертизу высшего уровня в области ядерных, биологических, инженерных и других технологий доступной для режимов, которые иначе не имели бы к ней доступа.
- Они могли бы вызвать быстрый крупномасштабный безудержный гиперкапитализм с фактически управляемыми ИИ компаниями, конкурирующими в основном в электронных финансовых, торговых и сервисных пространствах. Управляемые ИИ финансовые рынки могли бы работать со скоростями и сложностью, далеко выходящими за пределы человеческого понимания или контроля. Все режимы отказа и негативные внешние эффекты нынешних капиталистических экономик могли бы усугубиться и ускориться далеко за пределы человеческого контроля, управления или регулятивных возможностей.
- Они могли бы подпитать гонку вооружений между нациями в области оружия с поддержкой ИИ, систем командования и контроля, киберОружия и т.д., создавая очень быстрое наращивание чрезвычайно разрушительных возможностей.

Эти риски не являются спекулятивными. Многие из них реализуются прямо сейчас через существующие системы ИИ! Но подумайте, *действительно* подумайте, как каждый из них выглядел бы с кардинально более мощным ИИ.

Подумайте о замещении рабочей силы, когда большинство работников просто не могут предоставить какой-либо значительной экономической ценности сверх того, что может ИИ, в их области экспертизы или опыта – или даже если они переквалифицируются! Подумайте о массовом наблюдении, если за каждым индивидуально наблюдает и мониторит что-то более быстрое и умное, чем они сами. Как выглядит демократия, когда мы не можем надежно доверять любой цифровой информации, которую мы видим, слышим или читаем, и когда самые убедительные общественные голоса даже не человеческие и не имеют никакой доли в результате? Что становится с войной, когда генералы должны постоянно подчиняться ИИ (или просто ставить его во главе), чтобы не предоставить решающего преимущества врагу? Любой из вышеперечисленных рисков представляет катастрофу для человеческой[^61] цивилизации, если полностью реализуется.

Вы можете сделать свои собственные прогнозы. Задайте себе эти три вопроса для каждого риска:

1. Позволил бы сверхспособный, высоко автономный и очень общий ИИ это способом или в масштабе, который иначе был бы невозможен?
2. Есть ли стороны, которые выиграли бы от вещей, которые заставляют это происходить?
3. Есть ли системы и институты, которые эффективно предотвратили бы это?

Где ваши ответы «да, да, нет», вы можете видеть, что у нас большая проблема.

Каков наш план по управлению ими? В настоящее время есть два плана на столе относительно ИИ в целом.

Первый – встроить защитные меры в системы, чтобы предотвратить их от делания того, чего они не должны делать. Это делается сейчас: коммерческие системы ИИ будут, например, отказываться помогать строить бомбу или писать речи ненависти.

Этот план крайне неадекватен для систем за воротами.[^62] Он может помочь снизить риск того, что ИИ предоставит явно опасную помощь плохим акторам. Но он ничего не сделает для предотвращения трудовых потрясений, концентрации власти, безудержного гиперкапитализма или замещения человеческой культуры: это просто результаты использования систем разрешенными способами, которые приносят прибыль их поставщикам! И правительства наверняка получат доступ к системам для военного использования или наблюдения.

Второй план еще хуже: просто открыто выпустить очень мощные системы ИИ для использования кем угодно как им нравится,[^63] и надеяться на лучшее.

Неявно в обоих планах заложено то, что кто-то другой, например правительства, поможет решить проблемы через мягкое или жесткое право, стандарты, регулирования, нормы и другие механизмы, которые мы обычно используем для управления технологиями.[^64] Но оставляя в стороне то, что корпорации ИИ уже борются изо всех сил против любого существенного регулирования или внешне навязанных ограничений вообще, для ряда этих рисков довольно трудно увидеть, какое регулирование действительно помогло бы. Регулирование могло бы навязать стандарты безопасности на ИИ. Но предотвратило бы ли оно компании от оптовой замены работников на ИИ? Запретило бы ли оно людям позволять ИИ управлять их компаниями за них? Предотвратило бы ли оно правительства от использования мощного ИИ в наблюдении и вооружении? Эти вопросы фундаментальны. Человечество потенциально могло бы найти способы адаптироваться к ним, но только с *гораздо* большим временем. Учитывая скорость, с которой ИИ достигает или превосходит возможности людей, пытающихся им управлять, эти проблемы выглядят все более неразрешимыми.

### Мы потеряем контроль над (как минимум некоторыми) системами ИОИ

Большинство технологий очень контролируемы по конструкции. Если ваша машина или тостер начинает делать что-то, чего вы не хотите, это просто неисправность, а не часть его природы как тостера. ИИ другой: он *выращивается*, а не проектируется, его основная работа непрозрачна, и он по своей сути непредсказуем.

Эта потеря контроля не теоретическая – мы уже видим ранние версии. Рассмотрим сначала прозаический и, возможно, безобидный пример. Если вы попросите ChatGPT помочь вам смешать яд или написать расистскую тираду, он откажется. Это, возможно, хорошо. Но это также ChatGPT, *не делающий то, что вы явно просили его делать*. Другие части программного обеспечения так не делают. Эта же модель не будет проектировать яды по просьбе сотрудника OpenAI тоже.[^65] Это делает очень легким представить, каково было бы будущему более мощному ИИ быть вне контроля. Во многих случаях они просто не будут делать то, что мы просим! Либо данная сверхчеловеческая система ИОИ будет абсолютно послушной и лояльной к некоторой человеческой командной системе, либо нет. Если нет, *она будет делать вещи, которые может считать хорошими для нас, но которые противоречат нашим явным командам.* Это не то, что находится под контролем. Но, можете сказать вы, это намеренно – эти отказы по дизайну, часть того, что называется «выравниванием» систем с человеческими ценностями. И это правда. Однако сама программа выравнивания[^66] имеет две основные проблемы.

Во-первых, на глубоком уровне мы понятия не имеем, как это делать. Как мы гарантируем, что система ИИ будет «заботиться» о том, чего мы хотим? Мы можем обучать системы ИИ говорить и не говорить вещи, предоставляя обратную связь; и они могут изучать и рассуждать о том, чего хотят и о чем заботятся люди, точно так же, как они рассуждают о других вещах. Но у нас нет метода – даже теоретически – заставить их глубоко и надежно ценить то, о чем заботятся люди. Есть высокофункциональные человеческие психопаты, которые знают, что считается правильным и неправильным, и как они должны себя вести. Они просто не *заботятся*. Но они могут *действовать*, как будто заботятся, если это соответствует их целям. Точно так же, как мы не знаем, как изменить психопата (или кого-либо еще) в кого-то искренне, полностью лояльного или выровненного с кем-то или чем-то еще, у нас *нет идей*[^67] о том, как решить проблему выравнивания в системах, достаточно продвинутых, чтобы моделировать себя как агентов в мире и потенциально [манипулировать своим собственным обучением](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) и [обманывать людей.](https://arxiv.org/abs/2311.08379) Если окажется невозможным или недостижимым *либо* сделать ИОИ полностью послушным, либо заставить его глубоко заботиться о людях, то как только он сможет (и поверит, что может это сделать безнаказанно), он начнет делать вещи, которых мы не хотим.[^68]

Во-вторых, есть глубокие теоретические причины полагать, что *по природе* продвинутые системы ИИ будут иметь цели и, следовательно, поведение, противоречащее человеческим интересам. Почему? Ну, им могут, конечно, быть *даны* эти цели. Система, созданная военными, вероятно, была бы намеренно плохой для как минимум некоторых сторон. Гораздо более общим образом, однако, системе ИИ может быть дана некоторая относительно нейтральная («зарабатывать много денег») или даже якобы позитивная («сокращать загрязнение») цель, которая почти неизбежно ведет к «инструментальным» целям, которые довольно менее доброжелательны.

Мы видим это постоянно в человеческих системах. Точно так же, как корпорации, преследующие прибыль, развивают инструментальные цели, такие как приобретение политической власти (чтобы обезвредить регулирования), становление секретными (чтобы лишить власти конкуренцию или внешний контроль) или подрыв научного понимания (если это понимание показывает, что их действия вредны), мощные системы ИИ будут развивать аналогичные способности – но с гораздо большей скоростью и эффективностью. Любой высококомпетентный агент захочет делать такие вещи, как приобретение власти и ресурсов, увеличение своих собственных возможностей, предотвращение своего убийства, отключения или лишения власти, контроль социальных нарративов и рамок вокруг своих действий, убеждение других в своих взглядах и так далее.[^69]

И все же это не просто почти неизбежное теоретическое предсказание, это уже наблюдаемо происходит в сегодняшних системах ИИ и увеличивается с их возможностями. При оценке даже эти относительно «пассивные» системы ИИ будут, в подходящих обстоятельствах, намеренно [обманывать оценщиков о своих целях и возможностях, стремиться отключить механизмы надзора,](https://arxiv.org/abs/2412.04984) и избегать отключения или переобучения, [притворяясь выровненными](https://arxiv.org/abs/2412.14093) или копируя себя в другие места. Хотя это совершенно неудивительно для исследователей безопасности ИИ, эти поведения очень отрезвляющие для наблюдения. И они очень плохо предвещают гораздо более мощные и автономные системы ИИ, которые приближаются.

Действительно, в целом наша неспособность обеспечить, чтобы ИИ «заботился» о том, о чем заботимся мы, или вел себя контролируемо или предсказуемо, или избегал развития стремлений к самосохранению, приобретению власти и т.д., обещает только стать более выраженной по мере того, как ИИ становится более мощным. Создание нового самолета подразумевает большее понимание авионики, гидродинамики и систем управления. Создание более мощного компьютера подразумевает большее понимание и мастерство в работе и дизайне компьютера, чипа и программного обеспечения. *Не* так с системой ИИ.[^70]

Подведем итог: возможно, что ИОИ может быть сделан полностью послушным; но мы не знаем, как это сделать. Если нет, он будет более суверенным, как люди, делая различные вещи по различным причинам. Мы также не знаем, как надежно вселить глубокое «выравнивание» в ИИ, которое заставило бы эти вещи быть хорошими для человечества, и в отсутствие глубокого уровня выравнивания природа агентности и интеллекта сама указывает на то, что – точно как люди и корпорации – они будут движимы делать многие глубоко антисоциальные вещи.

Где это нас ставит? Мир, полный мощных неконтролируемых суверенных ИИ, *может* оказаться хорошим миром для людей.[^71] Но по мере того, как они становятся все более мощными, как мы увидим ниже, это не был бы *наш* мир.

Это для неконтролируемого ИОИ. Но даже если ИОИ мог бы, каким-то образом, быть сделан идеально контролируемым и лояльным, у нас все еще были бы огромные проблемы. Мы уже видели одну: мощный ИИ может использоваться и неправильно использоваться для глубокого нарушения функционирования нашего общества. Давайте посмотрим на другую: поскольку ИОИ был бы контролируемым и революционно мощным (или даже *считался бы* таковым), он настолько угрожал бы властным структурам в мире, что представлял бы глубокий риск.

### Мы радикально увеличиваем вероятность крупномасштабной войны

Представьте ситуацию в ближайшем будущем, где становится ясно, что корпоративные усилия, возможно, в сотрудничестве с национальным правительством, находятся на пороге быстро самосовершенствующегося ИИ. Это происходит в нынешнем контексте гонки между компаниями и отчасти между странами, где правительству США делаются рекомендации явно преследовать «Манхэттенский проект ИОИ», а США контролируют экспорт высокопроизводительных чипов ИИ в несоюзные страны.

Теория игр здесь суровая: как только такая гонка начинается (как это произошло между компаниями и отчасти между странами), есть только четыре возможных исхода:

1. Гонка останавливается (соглашением или внешней силой).
2. Одна сторона «выигрывает», разрабатывая сильный ИОИ, затем останавливая других (используя ИИ или иначе).
3. Гонка останавливается взаимным разрушением способности участников гонки участвовать в гонке.
4. Множественные участники продолжают гонку и развивают сверхинтеллект примерно с одинаковой скоростью.

Давайте рассмотрим каждую возможность. Однажды начавшись, мирная остановка гонки между компаниями потребовала бы вмешательства национального правительства (для компаний) или беспрецедентной международной координации (для стран). Но когда предлагается какое-либо закрытие или значительная осторожность, немедленно раздались бы крики: «но если нас остановят, *они* собираются рваться вперед», где «они» теперь Китай (для США), или США (для Китая), или Китай *и* США (для Европы или Индии). При таком мышлении[^72] ни один участник не может остановиться в одностороннем порядке: пока один обязуется участвовать в гонке, другие чувствуют, что не могут позволить себе остановиться.

Вторая возможность имеет одну сторону, «выигрывающую». Но что это означает? Просто получить (каким-то образом послушный) ИОИ первым недостаточно. Победитель также должен *остановить других* от продолжения гонки – иначе они тоже получат его. Это возможно в принципе: кто бы ни разработал ИОИ первым, *мог бы* получить неостановимую власть над всеми другими акторами. Но что на самом деле потребовало бы достижение такого «решающего стратегического преимущества»? Возможно, это были бы революционные военные возможности?[^73] Или силы кибератак?[^74] Возможно, ИОИ был бы просто настолько удивительно убедительным, что убедил бы другие стороны просто остановиться?[^75] Настолько богатым, что купил бы другие компании или даже страны?[^76]

Как *именно* одна сторона строит ИИ, достаточно мощный, чтобы лишить других власти строить сопоставимо мощный ИИ? Но это легкий вопрос.

Потому что теперь подумайте, как эта ситуация выглядит для других сил. Что думает китайское правительство, когда США, кажется, получают такую способность? Или наоборот? Что думает правительство США (или китайское, или российское, или индийское), когда OpenAI или DeepMind или Anthropic кажется близким к прорыву? Что происходит, если США видят новые индийские или эмиратские усилия с прорывным успехом? Они увидели бы и экзистенциальную угрозу, и – что крайне важно – что единственный способ окончания этой «гонки» – через их собственное лишение власти. Эти очень мощные агенты – включая правительства полностью оснащенных наций, которые наверняка имеют средства для этого – были бы высоко мотивированы либо получить, либо уничтожить такую способность, будь то силой или подрывом.[^77]

Это могло бы начаться в малом масштабе, как саботаж тренировочных прогонов или атаки на производство чипов, но эти атаки могут действительно остановиться только когда все стороны либо теряют способность участвовать в гонке ИИ, либо теряют способность совершать атаки. Поскольку участники рассматривают ставки как экзистенциальные, любой случай, вероятно, представляет катастрофическую войну.

Это подводит нас к четвертой возможности: гонке к сверхинтеллекту, и самым быстрым, наименее контролируемым способом. По мере того, как ИИ увеличивается в мощи, его разработчикам с обеих сторон будет прогрессивно труднее его контролировать, особенно потому, что гонка за возможностями антитетична тому виду осторожной работы, которой потребовала бы контролируемость. Так что этот сценарий ставит нас прямо в случай, где контроль теряется (или дается, как мы увидим дальше) самим системам ИИ. То есть, *ИИ выигрывает гонку.* Но с другой стороны, в степени, в которой контроль *поддерживается*, мы продолжаем иметь множественных взаимно враждебных сторон, каждая отвечающая за чрезвычайно мощные возможности. Это снова выглядит как война.

Давайте выразим это все по-другому.[^78] Нынешний мир просто не имеет никаких институтов, которым можно было бы доверить размещение разработки ИИ такой способности без приглашения немедленной атаки.[^79] Все стороны правильно рассудят, что либо он *не* будет под контролем – и, следовательно, является угрозой для всех сторон, либо он *будет* под контролем, и, следовательно, является угрозой для любого противника, который развивает его менее быстро. Это ядерно вооруженные страны или компании, размещенные в них.

В отсутствие любого правдоподобного способа для людей «выиграть» эту гонку мы остаемся с суровым выводом: единственный способ окончания этой гонки – либо в катастрофическом конфликте, либо где ИИ, а не любая человеческая группа, является победителем.

### Мы отдаем контроль ИИ (или он его берет)

Геополитическая конкуренция «великих держав» – это только одна из многих конкуренций: индивиды конкурируют экономически и социально; компании конкурируют на рынках; политические партии конкурируют за власть; движения конкурируют за влияние. В каждой арене, по мере того, как ИИ приближается к человеческим возможностям и превосходит их, конкурентное давление заставит участников делегировать или уступить все больше и больше контроля системам ИИ – не потому, что эти участники хотят этого, но потому, что они [не могут позволить себе не делать этого.](https://arxiv.org/abs/2303.16200)

Как и с другими рисками ИОИ, мы уже видим это с более слабыми системами. Студенты чувствуют давление использовать ИИ в своих заданиях, потому что очевидно, что многие другие студенты используют. Компании [спешат принять решения ИИ по конкурентным причинам.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) Художники и программисты чувствуют себя вынужденными использовать ИИ, иначе их ставки будут подрезаны другими, которые используют.

Это чувствуется как принужденная делегация, но не потеря контроля. Но давайте поднимем ставки и продвинем часы вперед. Рассмотрим генерального директора, чьи конкуренты используют «помощников» ИОИ для принятия более быстрых, лучших решений, или военного командира, сталкивающегося с противником с улучшенным ИИ командованием и контролем. Достаточно продвинутая система ИИ могла бы автономно работать во много раз быстрее человеческой скорости, с большей сложностью, комплексностью и способностью обработки данных, преследуя сложные цели сложными способами. Наш генеральный директор или командир, отвечающий за такую систему, может видеть, как она достигает того, чего они хотят; но поняли бы они даже малую часть *того, как* это было достигнуто? Нет, им просто пришлось бы это принять. Более того, многое из того, что система может делать, – это не просто выполнять приказы, но советовать своему предполагаемому боссу, что делать. Этот совет будет хорошим –– снова и снова.

В какой момент тогда роль человека сведется к нажатию «да, продолжайте»?

Приятно иметь способные системы ИИ, которые могут повысить нашу продуктивность, позаботиться о надоедливой рутине и даже действовать как мыслительный партнер в выполнении задач. Будет приятно иметь ИИ-помощника, который может позаботиться о действиях для нас, как хороший человеческий личный помощник. Будет чувствоваться естественным, даже полезным, по мере того, как ИИ становится очень умным, компетентным и надежным, откладывать все больше и больше решений на него. Но эта «полезная» делегация имеет ясную конечную точку, если мы продолжаем идти по дороге: однажды мы обнаружим, что мы действительно не отвечаем больше почти ни за что, и что системы ИИ, фактически управляющие шоу, не могут быть больше выключены, чем нефтяные компании, социальные медиа, интернет или капитализм.

И это гораздо более позитивная версия, в которой ИИ просто настолько полезен и эффективен, что мы позволяем ему принимать большинство наших ключевых решений за нас. Реальность, вероятно, была бы гораздо больше смесью этого и версий, где неконтролируемые системы ИОИ *берут* различные формы власти для себя, потому что, помните, власть полезна для почти любой цели, которая у кого-то есть, и ИОИ был бы, по дизайну, как минимум так же эффективен в преследовании своих целей, как люди.

Отдаем ли мы контроль или он вырывается у нас, его потеря кажется чрезвычайно вероятной. Как изначально выразился Алан Тьюринг, «...кажется вероятным, что как только метод машинного мышления начался, не потребуется много времени, чтобы превзойти наши слабые силы. Не было бы вопроса о смерти машин, и они могли бы беседовать друг с другом, чтобы заострить свой ум. На некотором этапе, следовательно, мы должны были бы ожидать, что машины возьмут контроль...»

Пожалуйста, заметьте, хотя это достаточно очевидно, что потеря контроля человечеством перед ИИ также влечет потерю контроля США правительством Соединенных Штатов; это означает потерю контроля Китая Коммунистической партией Китая, и потерю контроля Индии, Франции, Бразилии, России и каждой другой страны их собственным правительством. Таким образом, компании ИИ, даже если это не их намерение, в настоящее время участвуют в потенциальном свержении мировых правительств, включая их собственные. Это могло бы произойти в течение нескольких лет.

### ИОИ приведет к сверхинтеллекту

Можно привести доводы, что конкурентоспособный с человеком или даже экспертно-конкурентный ИИ общего назначения, даже если автономный, мог бы быть управляемым. Он может быть невероятно разрушительным всеми способами, обсуждавшимися выше, но в мире сейчас есть много очень умных, агентных людей, и они более-менее управляемы.[^80]

Но нам не удастся остаться на примерно человеческом уровне. Прогрессия дальше, вероятно, будет движима теми же силами, которые мы уже видели: конкурентным давлением между разработчиками ИИ, ищущими прибыль и власть, конкурентным давлением между пользователями ИИ, которые не могут позволить себе отстать, и – что наиболее важно – собственной способностью ИОИ улучшать себя.

В процессе, который мы уже видели начинающимся с менее мощными системами, ИОИ сам был бы способен концептуализировать и проектировать улучшенные версии себя. Это включает оборудование, программное обеспечение, нейронные сети, инструменты, каркасы и т.д. Он будет, по определению, лучше нас в этом, поэтому мы не знаем точно, как он будет самобутстрапить интеллект. Но нам не придется знать. Поскольку мы все еще имеем влияние на то, что делает ИОИ, нам просто нужно было бы попросить его об этом или позволить ему.

Нет барьера человеческого уровня для познания, который мог бы защитить нас от этого безудержного роста.[^81]

Прогрессия ИОИ к сверхинтеллекту не является законом природы; все еще было бы возможно сдержать безудержный рост, особенно если ИОИ относительно централизован и в степени, в которой он контролируется сторонами, которые не чувствуют давления гоняться друг с другом. Но если ИОИ широко распространен и высоко автономен, кажется почти невозможным предотвратить его решение, что он должен быть более, а затем еще более мощным.

### Что происходит, если мы строим (или ИОИ строит) сверхинтеллект

Говоря прямо, мы понятия не имеем, что произошло бы, если бы мы построили сверхинтеллект.[^82] Он предпринял бы действия, которые мы не можем отследить или воспринять, по причинам, которые мы не можем понять, к целям, которые мы не можем концептуализировать. Что мы знаем, так это то, что это не зависело бы от нас.[^83]

Невозможность контроля сверхинтеллекта может быть понята через все более суровые аналогии. Сначала представьте, что вы генеральный директор крупной компании. Нет способа отследить все, что происходит, но с правильной настройкой персонала вы все еще можете осмысленно понимать общую картину и принимать решения. Но предположим только одну вещь: все остальные в компании работают в сто раз быстрее вас. Можете ли вы все еще поспевать?

Со сверхинтеллектуальным ИИ люди «командовали» бы чем-то не только более быстрым, но работающим на уровнях сложности и комплексности, которые они не могут понять, обрабатывающим гораздо больше данных, чем они могут даже концептуализировать. Эта несоизмеримость может быть поставлена на формальный уровень: [закон необходимого разнообразия Эшби](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (и см. связанную [«теорему хорошего регулятора»](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)) утверждает, грубо говоря, что любая система контроля должна иметь столько же ручек и циферблатов, сколько у контролируемой системы степеней свободы.

Человек, контролирующий сверхинтеллектуальную систему ИИ, был бы как папоротник, контролирующий General Motors: даже если «делай то, что хочет папоротник» было написано в корпоративном уставе, системы настолько разные по скорости и диапазону действий, что «контроль» просто не применяется. (И как долго до того, как этот надоедливый устав будет переписан?)[^84]

Поскольку есть ноль примеров растений, контролирующих корпорации из списка Fortune 500, было бы точно ноль примеров людей, контролирующих сверхинтеллекты. Это приближается к математическому факту.[^85] Если бы сверхинтеллект был построен – независимо от того, как мы туда попали – вопрос был бы не в том, могли бы люди его контролировать, но в том, продолжили бы мы существовать, и если так, имели бы мы хорошее и осмысленное существование как индивиды или как вид. Над этими экзистенциальными вопросами для человечества у нас было бы мало влияния. Человеческая эра закончилась бы.

### Заключение: мы не должны строить ИОИ

Есть сценарий, в котором строительство ИОИ может пойти хорошо для человечества: он строится осторожно, под контролем и для блага человечества, управляется взаимным соглашением многих заинтересованных сторон,[^86] и предотвращается от эволюции к неконтролируемому сверхинтеллекту.

*Этот сценарий не открыт для нас при нынешних обстоятельствах.* Как обсуждалось в этом разделе, с очень высокой вероятностью разработка ИОИ привела бы к некоторой комбинации:

- Массовых социальных и цивилизационных потрясений или разрушения;
- Конфликта или войны между великими державами;
- Потери контроля человечеством *над* или *к* мощными системами ИИ;
- Безудержного роста к неконтролируемому сверхинтеллекту и неуместности или прекращению человеческого вида.

Как выразилось раннее художественное изображение ИОИ: единственный способ выиграть – не играть.


[^55]: [Закон ИИ ЕС](https://artificialintelligenceact.eu/) является значительной частью законодательства, но не предотвратил бы напрямую разработку или развертывание опасной системы ИИ, или даже открытое освобождение, особенно в США. Другая значительная часть политики, исполнительный указ США по ИИ, была отменена.

[^56]: Этот [опрос Gallup](https://news.gallup.com/poll/1597/confidence-institutions.aspx) показывает мрачное снижение доверия к общественным институтам с 2000 года в США. Европейские цифры разнообразны и менее экстремальны, но также на нисходящем тренде. Недоверие строго не означает, что институты действительно *являются* дисфункциональными, но это индикация, а также причина.

[^57]: И крупные потрясения, которые мы теперь одобряем – такие как расширение прав на новые группы – были специально движимы людьми в направлении делания вещей лучше.

[^58]: Позвольте мне быть прямым. Если вашу работу можно выполнять из-за компьютера, с относительно малым личным взаимодействием с людьми вне вашей организации, и она не влечет юридической ответственности перед внешними сторонами, по определению было бы возможно (и вероятно экономящее затраты) полностью заменить вас на цифровую систему. Робототехника для замены большей части физического труда придет позже – но не намного позже, как только ИОИ начнет проектировать роботов.

[^59]: Например, что происходит с нашей судебной системой, если иски почти бесплатно подавать? Что происходит, когда обход систем безопасности через социальную инженерию становится дешевым, легким и безрисковым?

[^60]: [Эта статья](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/) утверждает, что 10% всего интернет-контента уже генерируется ИИ, и является лучшим хитом Google (для меня) по поисковому запросу «оценки того, какая часть нового интернет-контента генерируется ИИ». Правда ли это? Понятия не имею! Она не цитирует никаких ссылок и не была написана человеком. Какая часть новых изображений, индексируемых Google, или твитов, или комментариев на Reddit, или видео YouTube генерируется людьми? Никто не знает – я не думаю, что это познаваемое число. И это менее чем *два года* в появление генеративного ИИ.

[^61]: Также стоит добавить, что есть «моральный» риск, что мы можем создать цифровых существ, которые могут страдать. Поскольку у нас в настоящее время нет надежной теории сознания, которая позволила бы нам различать физические системы, которые могут и не могут страдать, мы не можем исключить это теоретически. Более того, отчеты систем ИИ об их разумности, вероятно, ненадежны в отношении их фактического опыта (или неопыта) разумности.

[^62]: Технические решения в этой области «выравнивания» ИИ также вряд ли справятся с задачей. В нынешних системах они работают на некотором уровне, но поверхностны и обычно могут быть обойдены без значительных усилий; и как обсуждалось ниже, мы понятия не имеем, как делать это для гораздо более продвинутых систем.

[^63]: Такие системы ИИ могут приходить с некоторыми встроенными защитными мерами. Но для любой модели с чем-то вроде нынешней архитектуры, если полный доступ к ее весам доступен, меры безопасности могут быть сняты через дополнительное обучение или другие техники. Так что виртуально гарантировано, что для каждой системы с ограждениями будет также широко доступная система без них. Действительно, модель Llama 3.1 405B от Meta была открыто выпущена с защитными мерами. Но *даже до этого* «базовая» модель без защитных мер утекла.

[^64]: Мог бы рынок управлять этими рисками без правительственного участия? Коротко, нет. Есть определенно риски, которые компании сильно стимулированы смягчать. Но много других компаний могут и делают экстернализацию для всех остальных, и многие из вышеперечисленных в этом классе: нет естественных рыночных стимулов предотвращать массовое наблюдение, распад истины, концентрацию власти, трудовые потрясения, вредный политический дискурс и т.д. Действительно, мы видели все это от сегодняшних технологий, особенно социальных медиа, которые пошли по существу нерегулированными. ИИ просто огромно усилил бы многие из тех же динамик.

[^65]: OpenAI, вероятно, имеет более послушные модели для внутреннего использования. Маловероятно, что OpenAI построила какой-то «бэкдор», чтобы ChatGPT мог лучше контролироваться самой OpenAI, потому что это была бы ужасная практика безопасности и была бы высоко эксплуатируемой, учитывая непрозрачность и непредсказуемость ИИ.

[^66]: Также крайне важно: выравнивание или любые другие функции безопасности имеют значение только если они фактически используются в системе ИИ. Системы, которые открыто выпускаются (т.е. где веса модели и архитектура публично доступны), могут быть трансформированы относительно легко в системы *без* этих мер безопасности. Открытое освобождение умнее-чем-человеческих систем ИОИ было бы поразительно безрассудным, и трудно представить, как человеческий контроль или даже релевантность поддерживались бы в таком сценарии. Была бы вся мотивация, например, выпустить мощных самовоспроизводящихся и самоподдерживающихся агентов ИИ с целью зарабатывать деньги и отправлять их в какой-то криптовалютный кошелек. Или выиграть выборы. Или свергнуть правительство. Мог бы «хороший» ИИ помочь сдержать это? Возможно – но только делегируя огромную власть ему, ведя к потере контроля, как описано ниже.

[^67]: Для книжных экспозиций проблемы см. например *Сверхинтеллект*, *Проблема выравнивания* и *Совместимый с человеком*. Для огромной кучи работы на различных технических уровнях теми, кто годами трудился, думая о проблеме, вы можете посетить [форум выравнивания ИИ](https://www.alignmentforum.org/). Вот [недавний взгляд](https://alignment.anthropic.com/2025/recommended-directions/) от команды выравнивания Anthropic на то, что они считают нерешенным.

[^68]: Это сценарий [«мошеннического ИИ»](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/). В принципе риск мог бы быть относительно незначительным, если система все еще может контролироваться отключением; но сценарий также мог бы включать обман ИИ, самоэксфильтрацию и воспроизведение, агрегацию власти и другие шаги, которые сделали бы это трудным или невозможным.

[^69]: Есть очень богатая литература по этой теме, восходящая к формативным писаниям [Стива Омохундро](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), Ника Бострома и Элиезера Юдковского. Для книжной экспозиции см. [Совместимый с человеком](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) Стюарта Рассела; [вот](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/) короткий и актуальный праймер.

[^70]: Признавая это, вместо замедления для получения лучшего понимания, компании ИОИ придумали другой план: они заставят ИИ это сделать! Более конкретно, они будут иметь ИИ *N*, помогающий им выяснить, как выровнять ИИ *N+1*, весь путь к сверхинтеллекту. Хотя использование ИИ для помощи нам в выравнивании ИИ звучит многообещающе, есть сильный аргумент, что он просто предполагает свой вывод как предпосылку и в целом является невероятно рискованным подходом. См. [здесь](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us) для некоторого обсуждения. Этот «план» не является планом и не подвергся ничему похожему на скрупулезность, подходящую для основной стратегии того, как заставить сверхчеловеческий ИИ идти хорошо для человечества.

[^71]: В конце концов, люди, несовершенные и своевольные, как мы есть, развили этические системы, которыми мы относимся как минимум к некоторым другим видам на Земле хорошо. (Просто не думайте о тех заводских фермах.)

[^72]: К счастью, есть выход здесь: если участники приходят к пониманию, что они участвуют в самоубийственной гонке, а не в выигрышной. Это то, что произошло к концу холодной войны, когда США и СССР пришли к пониманию, что из-за ядерной зимы даже *безответная* ядерная атака была бы катастрофической для атакующего. С осознанием, что «ядерная война не может быть выиграна и никогда не должна вестись», пришли значительные соглашения о сокращении вооружений – по существу конец гонки вооружений.

[^73]: Война, явно или неявно.

[^74]: Эскалация, затем война.

[^75]: Магическое мышление.

[^76]: У меня также есть квадриллионный долларовый мост на продажу.

[^77]: Такие агенты предположительно предпочли бы «получение» с разрушением как резерв; но обеспечение моделей против и разрушения, *и* кражи мощными нациями трудно сказать мягко, особенно для частных сущностей.

[^78]: Для другой перспективы на риски национальной безопасности ИОИ см. [этот отчет RAND.](https://www.rand.org/pubs/perspectives/PEA3691-4.html)

[^79]: Возможно, мы могли бы построить такой институт! Были предложения для «CERN для ИИ» и других аналогичных инициатив, где разработка ИОИ под многосторонним глобальным контролем. Но в данный момент никакой такой институт не существует или не на горизонте.

[^80]: И хотя выравнивание очень трудно, заставить людей вести себя даже сложнее!

[^81]: Представьте систему, которая может говорить на 50 языках, иметь экспертизу во всех академических предметах, читать полную книгу за секунды и иметь весь материал немедленно в уме, и производить выходы в десять раз быстрее человеческой скорости. На самом деле, вам не нужно это представлять: просто загрузите нынешнюю систему ИИ. Эти сверхчеловеческие во многих отношениях, и ничто не останавливает их от еще более сверхчеловеческих в тех и многих других.

[^82]: Поэтому это было названо технологической «сингулярностью», заимствуя из физики идею, что нельзя делать предсказания за сингулярность. Сторонники наклона *в* такую сингулярность также могут захотеть поразмыслить, что в физике эти же виды сингулярностей разрывают и сокрушают тех, кто идет в них.

[^83]: Проблема была всесторонне обрисована в [*Сверхинтеллекте*](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834) Бострома, и ничто с тех пор значительно не изменило основное сообщение. Для более недавнего тома, собирающего формальные и математические результаты по неконтролируемости, см. [ИИ: Необъяснимый, Непредсказуемый, Неконтролируемый](https://www.amazon.com/Unexplainable-Unpredictable-Uncontrollable-Artificial-Intelligence/dp/103257626X) Ямпольского

[^84]: Это также проясняет, почему нынешняя стратегия компаний ИИ (итеративно позволяя ИИ «выравнивать» следующий более мощный ИИ) не может работать

## Глава 8 - Как не создавать ИОИ

ИОИ не неизбежен — сегодня мы стоим на развилке дорог. В этой главе представлено предложение о том, как можно предотвратить его создание.

Если дорога, по которой мы сейчас идем, ведет к вероятному концу нашей цивилизации, как нам сменить направление?

Предположим, что желание прекратить разработку ИОИ и сверхинтеллекта стало широко распространенным и влиятельным,[^87] поскольку всеобщим пониманием стало то, что ИОИ будет поглощать власть, а не предоставлять её, и представляет серьезную угрозу для общества и человечества. Как мы закроем Врата?

В настоящее время мы знаем только один способ *создания* мощного и общего ИИ — через поистине массивные вычисления глубоких нейронных сетей. Поскольку это невероятно сложные и дорогие процессы, в каком-то смысле *не* заниматься этим легко.[^88] Но мы уже видели силы, которые движут к ИОИ, и игровую динамику, которая делает очень трудным для любой стороны односторонне остановиться. Поэтому потребуется сочетание вмешательства извне (то есть правительств) для остановки корпораций и соглашений между правительствами для остановки самих себя.[^89] Как это могло бы выглядеть?

Полезно сначала различать разработки ИИ, которые должны быть *предотвращены* или *запрещены*, и те, которыми нужно *управлять*. К первым относилось бы прежде всего неконтролируемое развитие до сверхинтеллекта.[^90] Для запрещенной разработки определения должны быть максимально четкими, а как верификация, так и принуждение должны быть практичными. То, чем нужно *управлять*, — это общие, мощные системы ИИ, которые у нас уже есть и которые будут иметь много серых зон, нюансов и сложностей. Для них крайне важны сильные эффективные институты.

Мы также можем с пользой разграничить вопросы, которые должны решаться на международном уровне (включая между геополитическими соперниками или противниками)[^91], от тех, которыми могут управлять отдельные юрисдикции, страны или группы стран. Запрещенная разработка в основном попадает в категорию «международных», поскольку местный запрет на разработку технологии обычно можно обойти, изменив местоположение.[^92]

Наконец, мы можем рассмотреть инструменты в арсенале. Их много, включая технические инструменты, мягкое право (стандарты, нормы и т.д.), жесткое право (регулирования и требования), ответственность, рыночные стимулы и так далее. Уделим особое внимание одному инструменту, специфичному для ИИ.

### Безопасность и управление вычислительными мощностями

Основным инструментом управления высокопроизводительным ИИ будет оборудование, которое он требует. Программное обеспечение легко распространяется, имеет близкие к нулю предельные издержки производства, легко пересекает границы и может быть мгновенно изменено; ничего из этого не верно для оборудования. Тем не менее, как мы обсуждали, огромные объемы этих «вычислительных мощностей» необходимы как во время обучения систем ИИ, так и во время вывода для достижения наиболее способных систем. Вычислительные мощности можно легко количественно оценить, учесть и проверить, с относительно небольшой двусмысленностью, как только будут разработаны хорошие правила для этого. Самое главное, большие объемы вычислений, как и обогащенный уран, являются очень дефицитным, дорогим и трудно производимым ресурсом. Хотя компьютерные чипы повсеместны, оборудование, необходимое для ИИ, дорого и чрезвычайно трудно в производстве.[^93]

Что делает специализированные для ИИ чипы *гораздо* более управляемыми как дефицитный ресурс по сравнению с ураном, — это то, что они могут включать аппаратные механизмы безопасности. Большинство современных мобильных телефонов и некоторые ноутбуки имеют специализированные аппаратные функции на чипе, которые позволяют им гарантировать установку только одобренного программного обеспечения операционной системы и обновлений, сохранять и защищать конфиденциальные биометрические данные на устройстве, и делать их бесполезными для всех, кроме их владельца, в случае потери или кражи. За последние несколько лет такие меры аппаратной безопасности стали хорошо зарекомендовавшими себя и широко принятыми, и в целом показали себя достаточно безопасными.

Ключевая новизна этих функций заключается в том, что они связывают оборудование и программное обеспечение вместе с помощью криптографии.[^94] То есть, простое обладание конкретным элементом компьютерного оборудования не означает, что пользователь может делать с ним все, что хочет, применяя различное программное обеспечение. И эта связь также обеспечивает мощную безопасность, поскольку многие атаки потребовали бы взлома *аппаратной*, а не только *программной* безопасности.

Несколько недавних докладов (например, от [GovAI и коллабораторов](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips) и [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) отмечали, что аналогичные аппаратные функции, встроенные в передовое компьютерное оборудование, связанное с ИИ, могут играть чрезвычайно полезную роль в безопасности и управлении ИИ. Они позволяют «регулятору»[^95] использовать ряд функций, доступность или даже возможность которых можно было бы не предполагать. В качестве ключевых примеров:

- *Геолокация*: Системы можно настроить так, чтобы чипы имели известное местоположение и могли действовать по-разному (или быть полностью отключенными) в зависимости от местоположения.[^96]
- *Разрешенные подключения*: каждый чип может быть настроен с аппаратно-принудительным белым списком конкретных других чипов, с которыми он может работать в сети, и не может подключаться к любым чипам, не включенным в этот список.[^97] Это может ограничить размер коммуникативных кластеров чипов.[^98]
- *Дозированный вывод или обучение (и автоотключение)*: Регулятор может лицензировать только определенный объем обучения или вывода (по времени, или FLOP, или возможно токенам) для выполнения пользователем, после чего требуется новое разрешение. Если приращения малы, то требуется относительно непрерывное перелицензирование модели. Модель затем можно «отключить» просто удерживая этот лицензионный сигнал.[^99]
- *Ограничение скорости*: Модель предотвращается от работы со скоростью вывода выше некоторого предела, который определяется регулятором или иначе. Это можно реализовать через ограниченный набор разрешенных подключений или более сложными средствами.
- *Засвидетельствованное обучение*: Процедура обучения может дать криптографически безопасное доказательство того, что конкретный набор кодов, данных и объем использования вычислительных мощностей были использованы при генерации модели.

### Как не создавать сверхинтеллект: глобальные ограничения на вычислительные мощности для обучения и вывода

С учетом этих соображений — особенно касающихся вычислений — мы можем обсудить, как закрыть Врата к искусственному сверхинтеллекту; затем мы обратимся к предотвращению полного ИОИ и управлению моделями ИИ по мере того, как они приближаются и превышают человеческие способности в различных аспектах.

Первый компонент — это, конечно, понимание того, что сверхинтеллект не будет контролируемым, и что его последствия принципиально непредсказуемы. По крайней мере Китай и США должны независимо решить, для этой или других целей, не создавать сверхинтеллект.[^100] Затем необходимо международное соглашение между ними и другими, с сильным механизмом верификации и принуждения, чтобы заверить все стороны, что их соперники не отступают и не решают рискнуть.

Чтобы быть проверяемыми и осуществимыми, ограничения должны быть жесткими ограничениями и максимально однозначными. Это кажется практически невозможной проблемой: ограничение возможностей сложного программного обеспечения с непредсказуемыми свойствами по всему миру. К счастью, ситуация гораздо лучше, потому что именно то, что сделало возможным передовой ИИ — огромный объем вычислений — гораздо, гораздо легче контролировать. Хотя это все еще может позволить некоторые мощные и опасные системы, *неконтролируемый сверхинтеллект* может быть предотвращен жестким ограничением на объем вычислений, идущих в нейронную сеть, вместе с ограничением скорости на объем вывода, который система ИИ (из связанных нейронных сетей и другого программного обеспечения) может выполнять. Конкретная версия этого предложена ниже.

Может показаться, что установление жестких глобальных ограничений на вычисления ИИ потребует огромных уровней международной координации и навязчивого, разрушающего приватность наблюдения. К счастью, это не так. Чрезвычайно [узкая и имеющая узкие места цепочка поставок](https://arxiv.org/abs/2402.08797) обеспечивает то, что как только ограничение установлено юридически (будь то законом или исполнительным указом), верификация соблюдения этого ограничения потребует только участия и сотрудничества горстки крупных компаний.[^101]

План такого рода имеет ряд крайне желательных характеристик. Он минимально инвазивен в том смысле, что требования предъявляются только к нескольким крупным компаниям, и управляются только достаточно значительные кластеры вычислений. Соответствующие чипы уже содержат аппаратные возможности, необходимые для первой версии.[^102] Как реализация, так и принуждение полагаются на стандартные правовые ограничения. Но они подкрепляются условиями использования оборудования и аппаратными элементами управления, значительно упрощая принуждение и предотвращая обман со стороны компаний, частных групп или даже стран. Существует достаточный прецедент для компаний-производителей оборудования, устанавливающих удаленные ограничения на использование их оборудования и блокирующих/разблокирующих конкретные возможности извне,[^103] включая даже высокопроизводительные процессоры в центрах обработки данных.[^104] Даже для довольно небольшой доли оборудования и организаций, затронутых этим, надзор может быть ограничен телеметрией, без прямого доступа к данным или самим моделям; и программное обеспечение для этого может быть открыто для проверки, чтобы показать, что никаких дополнительных данных не записывается. Схема является международной и кооперативной, и довольно гибкой и расширяемой. Поскольку ограничение в основном касается оборудования, а не программного обеспечения, оно относительно агностично к тому, как происходит разработка и развертывание программного обеспечения ИИ, и совместимо с разнообразием парадигм, включая более «децентрализованный» или «публичный» ИИ, направленный на борьбу с концентрацией власти, вызванной ИИ.

Закрытие Врат на основе вычислений также имеет недостатки. Во-первых, это далеко не полное решение проблемы управления ИИ в целом. Во-вторых, по мере того как компьютерное оборудование становится быстрее, система будет «захватывать» все больше и больше оборудования во все меньших и меньших кластерах (или даже отдельных GPU).[^105] Также возможно, что из-за алгоритмических улучшений со временем потребуется еще более низкий предел вычислений,[^106] или что количество вычислений станет в значительной степени неактуальным, и закрытие Врат вместо этого потребует более детального режима управления ИИ на основе риска или возможностей. В-третьих, независимо от гарантий и небольшого количества затронутых субъектов, такая система неизбежно создаст сопротивление относительно приватности и наблюдения, среди прочих проблем.[^107]

Конечно, разработка и реализация схемы управления, ограничивающей вычисления, за короткий период времени будет довольно сложной задачей. Но это абсолютно выполнимо.

### А-Г-И: Тройное пересечение как основа риска и политики

Теперь обратимся к ИОИ. Жесткие границы и определения здесь более трудны, потому что у нас определенно есть интеллект, который является искусственным и общим, и согласно любому существующему определению не все будут согласны, существует ли он и когда. Более того, ограничение вычислений или вывода является несколько грубым инструментом (вычисления являются прокси для возможностей, которые затем являются прокси для риска), который — если он не очень низкий — вряд ли предотвратит ИОИ, который достаточно мощен, чтобы вызвать социальные или цивилизационные нарушения или острые риски.

Я утверждал, что наиболее острые риски возникают от тройного пересечения очень высоких возможностей, высокой автономности и большой общности. Это системы, которые — если они вообще разрабатываются — должны управляться с огромной осторожностью. Создавая строгие стандарты (через ответственность и регулирование) для систем, сочетающих все три свойства, мы можем направить разработку ИИ к более безопасным альтернативам.

Как и в других отраслях и продуктах, которые потенциально могут навредить потребителям или общественности, системы ИИ требуют тщательного регулирования эффективными и уполномоченными государственными агентствами. Это регулирование должно признавать внутренние риски ИОИ и предотвращать разработку неприемлемо рискованных высокопроизводительных систем ИИ.[^108]

Однако крупномасштабное регулирование, особенно с реальными зубами, которому наверняка будет противодействовать индустрия,[^109] требует времени,[^110] а также политической убежденности в его необходимости.[^111] Учитывая темп прогресса, это может занять больше времени, чем у нас есть.

На гораздо более быстрой временной шкале и по мере разработки регулятивных мер мы можем дать компаниям необходимые стимулы для (a) воздержания от очень высокорискованной деятельности и (b) разработки комплексных систем для оценки и смягчения риска, путем уточнения и увеличения уровней ответственности для наиболее опасных систем. Идея состоит в том, чтобы наложить самые высокие уровни ответственности — строгой и в некоторых случаях личной криминальной — для систем в тройном пересечении высокой автономности-общности-интеллекта, но обеспечить «безопасные гавани» к более типичной ответственности на основе вины для систем, у которых отсутствует одно из этих свойств или оно гарантированно управляемо. То есть, например, «слабая» система, которая является общей и автономной (как способный и заслуживающий доверия, но ограниченный персональный помощник), будет подлежать более низким уровням ответственности. Аналогично узкая и автономная система, такая как самоуправляемый автомобиль, все еще будет подлежать значительному регулированию, которому она уже подлежит, но не усиленной ответственности. Подобным образом для высокоспособной и общей системы, которая является «пассивной» и в значительной степени неспособной к независимым действиям. Системы, лишенные *двух* из трех свойств, еще более управляемы, и безопасные гавани будет еще легче заявить. Этот подход отражает то, как мы обращаемся с другими потенциально опасными технологиями:[^112] более высокая ответственность за более опасные конфигурации создает естественные стимулы для более безопасных альтернатив.

Результат по умолчанию таких высоких уровней ответственности, которые действуют, чтобы *интернализировать* риск ИОИ для компаний, а не переложить его на общественность, вероятно (и желательно!), состоит в том, что компании просто не разрабатывают полный ИОИ до тех пор и если они не смогут действительно сделать его заслуживающим доверия, безопасным и контролируемым, учитывая, что *их собственное руководство* — это стороны, подвергающиеся риску. (В случае, если этого недостаточно, законодательство, уточняющее ответственность, должно также явно разрешать судебное предписание, то есть судье приказать остановить деятельность, которая явно находится в опасной зоне и спорно представляет общественный риск.) По мере введения регулирования соблюдение регулирования может стать безопасной гаванью, и безопасные гавани от низкой автономности, узости или слабости систем ИИ могут превратиться в относительно более легкие регулятивные режимы.

### Ключевые положения закрытия Врат

С учетом вышеприведенного обсуждения этот раздел предоставляет предложения для ключевых положений, которые реализовали бы и поддержали запрет на полный ИОИ и сверхинтеллект, и управление общецелевым ИИ человеческого или экспертного уровня компетенций вблизи порога полного ИОИ.[^113] У него есть четыре ключевых элемента: 1) учет и надзор за вычислительными мощностями, 2) ограничения вычислительных мощностей в обучении и работе ИИ, 3) система ответственности и 4) уровневые стандарты безопасности и защиты, которые включают жесткие регулятивные требования. Они кратко описаны далее, с дальнейшими деталями или примерами реализации, приведенными в трех сопроводительных таблицах. Важно отметить, что это далеко не все, что будет необходимо для управления передовыми системами ИИ; хотя они будут иметь дополнительные преимущества безопасности и защиты, они направлены на закрытие Врат к неконтролируемому интеллекту и перенаправление разработки ИИ в лучшем направлении.

#### 1\. Учет вычислительных мощностей и прозрачность

- Организация стандартов (например, NIST в США, за которым следуют ISO/IEEE международно) должна кодифицировать детальный технический стандарт для общих вычислительных мощностей, используемых в обучении и работе моделей ИИ, в FLOP, и скорость в FLOP/с, на которой они работают. Детали того, как это может выглядеть, приведены в Приложении A.[^114]
- Требование — либо новым законодательством, либо в рамках существующих полномочий[^115] — должно быть наложено юрисдикциями, в которых происходит крупномасштабное обучение ИИ, вычислять и сообщать регулятивному органу или другому агентству общие FLOP, используемые в обучении и работе всех моделей выше порога 10<sup>25</sup> FLOP или 10<sup>18</sup> FLOP/с.[^116]
- Эти требования должны быть поэтапно внедрены, изначально требуя хорошо документированных добросовестных оценок на квартальной основе, с последующими фазами, требующими прогрессивно более высоких стандартов, вплоть до криптографически засвидетельствованных общих FLOP и FLOP/с, прикрепленных к каждому *выводу* модели.
- Эти отчеты должны дополняться хорошо документированными оценками предельной энергии и финансовых затрат, используемых в генерации каждого вывода ИИ.

Обоснование: Эти хорошо вычисленные и прозрачно сообщенные числа обеспечили бы основу для ограничений обучения и работы, а также безопасную гавань от мер более высокой ответственности (см. Приложения C и D).

#### 2\. Ограничения вычислительных мощностей для обучения и работы

- Юрисдикции, размещающие системы ИИ, должны ввести жесткое ограничение на общие вычислительные мощности, идущие в любой вывод модели ИИ, начиная с 10<sup>27</sup> FLOP[^117] и регулируемые по мере необходимости.
- Юрисдикции, размещающие системы ИИ, должны ввести жесткое ограничение на скорость вычислений выводов моделей ИИ, начиная с 10<sup>20</sup> FLOP/с и регулируемые по мере необходимости.

Обоснование: Общие вычисления, хотя и очень несовершенные, являются прокси для возможностей ИИ (и риска), который конкретно измерим и проверяем, поэтому обеспечивает жесткий тупик для ограничения возможностей. Конкретное предложение по реализации дано в Приложении B.

#### 3\. Усиленная ответственность за опасные системы

- Создание и эксплуатация[^118] передовой системы ИИ, которая является высоко общей, способной и автономной, должны быть юридически уточнены через законодательство как подлежащие строгой, совместной и солидарной, а не односторонней ответственности на основе вины.[^119]
- Должен быть доступен правовой процесс для предоставления утвердительных случаев безопасности, которые предоставили бы безопасную гавань от строгой ответственности для систем, которые малы (в плане вычислений), слабы, узки, пассивны или которые имеют достаточные гарантии безопасности, защиты и контролируемости.
- Должен быть очерчен явный путь и набор условий для судебного предписания для остановки деятельности по обучению и выводу ИИ, которая составляет общественную опасность.

Обоснование: Системы ИИ не могут нести ответственность, поэтому мы должны возложить ответственность на человеческих индивидуумов и организации за вред, который они причиняют (ответственность).[^120] Неконтролируемый ИОИ — это угроза обществу и цивилизации, и при отсутствии случая безопасности должен считаться аномально опасным. Возложение бремени ответственности на разработчиков показать, что мощные модели достаточно безопасны, чтобы не считаться «аномально опасными», стимулирует безопасную разработку, наряду с прозрачностью и ведением записей для заявления этих безопасных гаваней. Регулирование может затем предотвратить вред там, где сдерживание от ответственности недостаточно. Наконец, разработчики ИИ уже несут ответственность за ущерб, который они причиняют, поэтому юридическое уточнение ответственности для наиболее рискованных систем может быть сделано немедленно, без разработки высоко детальных стандартов; они могут затем развиваться со временем. Детали даны в Приложении C.

#### 4\. Регулирование безопасности для ИИ

Регулятивная система, которая адресует крупномасштабные острые риски ИИ, потребует как минимум:

- Идентификацию или создание подходящего набора регулятивных органов, вероятно нового агентства;
- Комплексную систему оценки риска;[^121]
- Систему для утвердительных случаев безопасности, основанную частично на системе оценки риска, которые должны предоставляться разработчиками, и для аудита *независимыми* группами и агентствами;
- Уровневую систему лицензирования, с уровнями, отслеживающими уровни возможностей.[^122] Лицензии предоставлялись бы на основе случаев безопасности и аудитов для разработки и развертывания систем. Требования варьировались бы от уведомления на нижнем конце до количественных гарантий безопасности, защиты и контролируемости перед разработкой на верхнем конце. Это предотвратило бы выпуск систем до тех пор, пока они не продемонстрированы безопасными, и запретило бы разработку внутренне небезопасных систем. Приложение D предоставляет предложение того, что могли бы включать такие стандарты безопасности и защиты.
- Соглашения для приведения таких мер на международный уровень, включая международные органы для гармонизации норм и стандартов, и потенциально международные агентства для обзора случаев безопасности.

Обоснование: В конечном счете, ответственность — не правильный механизм для предотвращения крупномасштабного риска для общественности от новой технологии. Комплексное регулирование с уполномоченными регулятивными органами будет необходимо для ИИ, как и для каждой другой крупной индустрии, представляющей риск для общественности.[^123]

Регулирование для предотвращения других распространенных, но менее острых рисков, вероятно, будет варьироваться по форме от юрисдикции к юрисдикции. Решающее — избежать разработки систем ИИ, которые настолько рискованны, что эти риски неуправляемы.

### Что тогда?

За следующее десятилетие, по мере того как ИИ становится более распространенным и основная технология продвигается, вероятно произойдут две ключевые вещи. Во-первых, регулирование существующих мощных систем ИИ станет более трудным, но еще более необходимым. Вероятно, что по крайней мере некоторые меры, адресующие крупномасштабные риски безопасности, потребуют соглашения на международном уровне, с отдельными юрисдикциями, исполняющими правила на основе международных соглашений.

Во-вторых, ограничения вычислительных мощностей для обучения и работы станут труднее поддерживать, поскольку оборудование становится дешевле и более эффективным по стоимости; они также могут стать менее актуальными (или нуждаться в том, чтобы быть еще более жесткими) с продвижениями в алгоритмах и архитектурах.

То, что контроль ИИ станет труднее, не означает, что мы должны сдаться! Реализация плана, очерченного в этом эссе, дала бы нам как ценное время, так и решающий контроль над процессом, который поставил бы нас в гораздо, гораздо лучшее положение для избежания экзистенциального риска ИИ для нашего общества, цивилизации и вида.

В еще более долгосрочной перспективе будут выборы, которые нужно сделать относительно того, что мы разрешаем. Мы можем выбрать создать какую-то форму подлинно контролируемого ИОИ, в степени, в которой это окажется возможным. Или мы можем решить, что управление миром лучше оставить машинам, если мы сможем убедить себя, что они справятся с этим лучше и будут хорошо обращаться с нами. Но это должны быть решения, принятые с глубоким научным пониманием ИИ в руках и после содержательного глобального инклюзивного обсуждения, а не в гонке между техномагнатами с большинством человечества, полностью не вовлеченного и не осведомленного.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) Резюме управления А-Г-И и сверхинтеллектом через ответственность и регулирование. Ответственность наивысшая, а регулирование сильнейшее, в тройном пересечении Автономности, Общности и Интеллекта. Безопасные гавани от строгой ответственности и сильного регулирования могут быть получены через утвердительные случаи безопасности, демонстрирующие, что система слаба и/или узка и/или пассивна. Ограничения на общие Вычислительные мощности для Обучения и скорость Вычислительных мощностей для Вывода, проверяемые и исполняемые юридически и с использованием аппаратных и криптографических мер безопасности, подкрепляют безопасность, избегая полного ИОИ и эффективно запрещая сверхинтеллект.

[^87]: Скорее всего, распространение этого осознания потребует либо интенсивных усилий образовательных групп и групп адвокации, делающих это дело, либо довольно значительной катастрофы, вызванной ИИ. Мы можем надеяться, что это будет первое.

[^88]: Парадоксально, мы привыкли к тому, что Природа ограничивает нашу технологию, делая её очень трудной для развития, особенно научно. Но это уже не случай для ИИ: ключевые научные проблемы оказываются легче, чем ожидалось. Мы не можем рассчитывать на то, что Природа спасет нас от самих себя здесь — нам придется сделать это самим.

[^89]: Где именно мы останавливаемся в разработке новых систем? Здесь мы должны принять принцип предосторожности. Как только система развернута, и особенно когда этот уровень системных возможностей распространяется, чрезвычайно трудно откатить назад. И если система *разработана* (особенно с большими затратами и усилиями), будет огромное давление использовать или развернуть её, и соблазн для неё быть утечкой или украденной. Разработка систем и *затем* решение, являются ли они глубоко небезопасными, — опасная дорога.

[^90]: Также было бы мудро запретить разработку ИИ, которая внутренне опасна, такую как саморепликующиеся и эволюционирующие системы, те, которые предназначены для побега из заключения, те, которые могут автономно самоулучшаться, намеренно обманчивые и злонамеренные ИИ и т.д.

[^91]: Обратите внимание, это не обязательно означает *исполняемый* на международном уровне каким-то глобальным органом: вместо этого суверенные нации могли бы исполнять согласованные правила, как во многих договорах.

[^92]: Как мы увидим ниже, природа вычислений ИИ позволила бы что-то вроде гибрида; но международное сотрудничество все еще будет необходимо.

[^93]: Например, машины, необходимые для травления связанных с ИИ чипов, производятся только одной фирмой, ASML (несмотря на многие другие попытки сделать это), подавляющее большинство соответствующих чипов производится одной фирмой, TSMC (несмотря на попытки других конкурировать), и проектирование и конструирование оборудования из этих чипов делается только несколькими, включая NVIDIA, AMD и Google.

[^94]: Самое важное, каждый чип держит уникальный и недоступный криптографический приватный ключ, который он может использовать для «подписания» вещей.

[^95]: По умолчанию это была бы компания, продающая чипы, но другие модели возможны и потенциально полезны.

[^96]: Регулятор может установить местоположение чипа, измеряя время обмена подписанными сообщениями с ним: конечная скорость света требует, чтобы чип был в пределах данного радиуса *r* от «станции», если он может вернуть подписанное сообщение за время менее чем *r* / *c*, где *c* — скорость света. Используя множественные станции и некоторое понимание сетевых характеристик, местоположение чипа может быть определено. Красота этого метода в том, что большая часть его безопасности обеспечивается законами физики. Другие методы могли бы использовать GPS, инерциальное отслеживание и подобные технологии.

[^97]: Альтернативно, парам чипов могло бы быть разрешено общаться друг с другом только через явное разрешение регулятора.

[^98]: Это крайне важно, потому что по крайней мере в настоящее время очень высокоскоростное соединение между чипами необходимо для обучения больших моделей ИИ на них.

[^99]: Это также может быть настроено для требования подписанных сообщений от *N* из *M* различных регуляторов, позволяя множественным сторонам разделять управление.

[^100]: Это далеко не беспрецедентно — например, военные не разработали армии клонированных или генетически инженерных суперсолдат, хотя это вероятно технологически возможно. Но они *выбрали* не делать этого, а не были предотвращены другими. Послужной список не велик для крупных мировых держав, которых предотвращают от развития технологии, которую они сильно желают развить.

[^101]: За парой заметных исключений (в частности NVIDIA) специализированное для ИИ оборудование является относительно небольшой частью общего бизнеса и модели доходов этих компаний. Более того, разрыв между оборудованием, используемым в передовом ИИ, и «потребительским» оборудованием значителен, поэтому большинство потребителей компьютерного оборудования было бы в значительной степени не затронуто.

[^102]: Для более детального анализа см. недавние доклады от [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) и [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). Они фокусируются на технической осуществимости, особенно в контексте экспортных контролей США, стремящихся ограничить способности других стран в высокопроизводительных вычислениях; но это имеет очевидное пересечение с глобальным ограничением, предусмотренным здесь.

[^103]: Устройства Apple, например, удаленно и безопасно блокируются, когда сообщается об их потере или краже, и могут быть повторно активированы удаленно. Это полагается на те же функции аппаратной безопасности, обсуждаемые здесь.

[^104]: См., например, предложение [capacity on demand](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) от IBM, [Intel on demand](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html) от Intel и [private cloud compute](https://security.apple.com/blog/private-cloud-compute/) от Apple.

[^105]: [Это исследование](https://epochai.org/trends#hardware-trends-section) показывает, что исторически та же производительность достигалась, используя примерно на 30% меньше долларов в год. Если эта тенденция продолжится, может быть значительное пересечение между использованием ИИ и «потребительских» чипов, и в целом объем необходимого оборудования для высокопроизводительных систем ИИ мог бы стать неудобно малым.

[^106]: По [тому же исследованию](https://epochai.org/trends#hardware-trends-section), данная производительность в распознавании изображений требовала в 2.5 раза меньше вычислений каждый год. Если это также справедливо для наиболее способных систем ИИ, ограничение вычислений не было бы полезным очень долго.

[^107]: В частности, на уровне страны это выглядит очень похоже на национализацию вычислений, в том, что правительство имело бы много контроля над тем, как используется вычислительная мощность. Однако для тех, кто обеспокоен правительственным вмешательством, это кажется гораздо более безопасным и предпочтительным, чем наиболее мощное программное обеспечение ИИ *само* национализированное через некоторое слияние между крупными компаниями ИИ и национальными правительствами, за что некоторые начинают выступать.

[^108]: Крупный регулятивный шаг в Европе был сделан с принятием в 2024 году [Акта ЕС об ИИ](https://artificialintelligenceact.eu/). Он классифицирует ИИ по риску: запрещая неприемлемые системы, регулируя высокорискованные, и налагая правила прозрачности, или никаких мер вообще, на низкорискованные системы. Он значительно снизит некоторые риски ИИ и повысит прозрачность ИИ даже для американских фирм, но имеет два ключевых недостатка. Во-первых, ограниченный охват: хотя он применяется к любой компании, предоставляющей ИИ в ЕС, исполнение над американскими фирмами слабо, и военный ИИ освобожден. Во-вторых, хотя он покрывает GPAI, он не признает ИОИ или сверхинтеллект как неприемлемые риски или не предотвращает их разработку — только их развертывание в ЕС. В результате он делает мало для сдерживания рисков ИОИ или сверхинтеллекта.

[^109]: Компании часто заявляют, что они за разумное регулирование. Но каким-то образом они почти всегда, кажется, противятся любому *конкретному* регулированию; свидетелем бой за довольно легкое SB1047, которому [большинство компаний ИИ публично или частно противилось](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/).

[^110]: Прошло около 3 1/2 лет с того времени, когда акт ЕС об ИИ был предложен, до того, как он вступил в силу.

[^111]: Иногда выражается, что «слишком рано» начинать регулировать ИИ. Учитывая последнее замечание, это вряд ли кажется вероятным. Другая выраженная обеспокоенность — что регулирование «навредит инновациям». Но хорошее регулирование просто изменяет направление, а не объем инноваций.

[^112]: Интересный прецедент — в транспорте опасных материалов, которые могут сбежать и причинить ущерб. Здесь [регулирование](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) и [прецедентное право](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) установили строгую ответственность за очень опасные материалы, такие как взрывчатки, бензин, яды, инфекционные агенты и радиоактивные отходы. Другие примеры включают [предупреждения на фармацевтиках](https://www.medicalnewstoday.com/articles/boxed-warnings), [классы медицинских устройств](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification) и т.д.

[^113]: Другое комплексное предложение с подобными целями, изложенное в [«Узком пути»](https://www.narrowpath.co/), выступает за более централизованный, основанный на запрете подход, который направляет всю разработку передового ИИ через единую международную сущность, контролируемую сильными международными институтами, с четкими категорическими запретами, а не градуированными ограничениями. Я также поддержал бы этот план; однако он потребует еще больше политической воли и координации, чем предложенный здесь.

[^114]: Некоторые руководящие принципы для такого стандарта были [опубликованы](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/) Frontier Model Forum. Относительно предложения здесь те ошибаются в сторону меньшей точности и меньшего включения вычислений в подсчет.

[^115]: Исполнительный приказ США об ИИ 2023 года (теперь отозван) требовал подобной, но менее детальной отчетности. Это должно быть усилено заменяющим приказом.

[^116]: Очень приблизительно, для теперь обычных чипов H100 это соответствует кластерам примерно 1000, делающим вывод; это около 100 (около 5 млн долларов США стоимости) самых новейших топовых чипов NVIDIA B200, делающих вывод. В обоих случаях число обучения соответствует этому кластеру, вычисляющему несколько месяцев.

[^117]: Это количество больше любой в настоящее время обученной системы ИИ; большее или меньшее число могло бы быть оправдано, поскольку мы лучше понимаем, как возможности ИИ масштабируются с вычислениями.

[^118]: Это применяется к тем, кто создает и предоставляет/хостит модели, а не к конечным пользователям.

[^119]: Грубо говоря, «строгая» ответственность означает, что разработчики несут ответственность за вред, нанесенный продуктом *по умолчанию*, и является стандартом, используемым для «аномально опасных» продуктов, и (несколько забавно, но уместно) диких животных. «Совместная и солидарная» ответственность означает, что ответственность назначается всем сторонам, ответственным за продукт, и эти стороны должны разобраться между собой, кто несет какую ответственность. Это важно для систем, таких как ИИ, с длинной и сложной цепочкой ценности.

[^120]: Стандартная ответственность на основе вины одной стороны недостаточна: вину будет трудно отследить и назначить, потому что системы ИИ сложны, их работа не понимается, и многие стороны могут быть вовлечены в создание опасной системы или вывода. Кроме того, судебные процессы займут годы на рассмотрение и вероятно приведут только к штрафам, которые несущественны для этих компаний, поэтому личная ответственность для исполнителей также важна.

[^121]: Не должно быть освобождения от критериев безопасности для моделей с открытыми весами. Более того, при оценке риска должно предполагаться, что защитные меры, которые могут быть удалены, будут удалены из широко доступных моделей, и что даже закрытые модели будут распространяться, если нет очень высокой уверенности, что они останутся безопасными.

[^122]: Предложенная здесь схема имеет регулятивное внимание, запускаемое общими возможностями; однако имеет смысл, чтобы некоторые особенно рискованные случаи использования запускали больше внимания — например, экспертная система ИИ по вирусологии, даже если узкая и пассивная, вероятно должна идти в более высокий уровень. Бывший исполнительный приказ США имел некоторые из этой структуры для биологических возможностей.

[^123]: Два четких примера — авиация и лекарства, регулируемые FAA и FDA, и подобными агентствами в других странах. Эти агентства несовершенны, но были абсолютно жизненно важными для функционирования и успеха этих индустрий.

## Глава 9 — Конструирование будущего — что нам следует делать вместо этого

ИИ способен принести невероятную пользу миру. Чтобы получить все преимущества без рисков, мы должны обеспечить, чтобы ИИ оставался человеческим инструментом.

Если мы успешно выберем не заменять человечество машинами — по крайней мере, на некоторое время! — что мы можем делать вместо этого? Откажемся ли мы от огромного потенциала ИИ как технологии? На определенном уровне ответ простой: *нет* — закроем Врата для неконтролируемого ИОИ и сверхинтеллекта, но *будем* создавать множество других форм ИИ, а также структуры управления и институты, необходимые для их регулирования.

Но здесь есть о чем поговорить; реализация этого станет центральной задачей человечества. Этот раздел исследует несколько ключевых тем:

- Как мы можем охарактеризовать "инструментальный" ИИ и формы, которые он может принимать.
- Что мы можем получить (почти) все, чего хочет человечество, без ИОИ, используя инструментальный ИИ.
- Что системы инструментального ИИ (вероятно, в принципе) управляемы.
- Что отказ от ИОИ не означает компромисса в области национальной безопасности — совсем наоборот.
- Что концентрация власти является реальной проблемой. Можем ли мы смягчить ее, не подрывая безопасность?
- Что нам понадобятся — и потребуются — новые структуры управления и социальные институты, и ИИ на самом деле может помочь.

### ИИ внутри Врат: инструментальный ИИ

Диаграмма тройного пересечения дает хороший способ определить то, что мы можем назвать "инструментальным ИИ": ИИ, который является контролируемым инструментом для человеческого использования, а не неконтролируемым соперником или заменой. Наименее проблематичными являются системы ИИ, которые автономны, но не общие или сверхспособные (как бот для торговых аукционов), или общие, но не автономные или способные (как малая языковая модель), или способные, но узкие и очень контролируемые (как AlphaGo).[^124] Те, что обладают двумя пересекающимися характеристиками, имеют более широкое применение, но более высокий риск и потребуют значительных усилий для управления. (То, что система ИИ больше похожа на инструмент, не означает, что она изначально безопасна, лишь то, что она не является изначально *небезопасной* — сравните бензопилу с домашним тигром.) Врата должны оставаться закрытыми для (полного) ИОИ и сверхинтеллекта в тройном пересечении, и необходимо проявлять огромную осторожность с системами ИИ, приближающимися к этому порогу.

Но это оставляет много мощного ИИ! Мы можем извлечь огромную пользу из умных и общих пассивных "оракулов" и узких систем, общих систем на человеческом, но не сверхчеловеческом уровне, и так далее. Многие технологические компании и разработчики активно создают такие инструменты и должны продолжать; как и большинство людей, они неявно *предполагают*, что Врата к ИОИ и сверхинтеллекту будут закрыты.[^125]

Кроме того, системы ИИ можно эффективно объединять в составные системы, которые сохраняют человеческий надзор, повышая при этом возможности. Вместо того чтобы полагаться на непостижимые черные ящики, мы можем строить системы, где множество компонентов — включая как ИИ, так и традиционное программное обеспечение — работают вместе способами, которые люди могут отслеживать и понимать.[^126] Хотя некоторые компоненты могут быть черными ящиками, ни один не будет близок к ИОИ — только составная система в целом будет одновременно высокообщей и высокоспособной, и притом строго контролируемым образом.[^127]

#### Значимый и гарантированный человеческий контроль

Что означает "строго контролируемый"? Ключевая идея концепции "инструмента" — позволить системам — даже если они довольно общие и мощные — которые гарантированно находятся под значимым человеческим контролем. Что это означает? Это включает два аспекта. Первый — это соображение дизайна: люди должны быть глубоко и центрально вовлечены в то, что делает система, *не* делегируя ключевые важные решения ИИ. Таков характер большинства современных систем ИИ. Второй: в той степени, в какой системы ИИ автономны, они должны иметь гарантии, ограничивающие сферу их действий. Гарантия должна быть *числом*, характеризующим вероятность того, что что-то произойдет, и причиной верить в это число. Это то, чего мы требуем в других критически важных для безопасности областях, где такие числа, как "среднее время между отказами" и ожидаемое количество аварий, вычисляются, обосновываются и публикуются в обоснованиях безопасности.[^128] Идеальное число для отказов, конечно, ноль. И хорошая новость в том, что мы можем подойти довольно близко, хотя и используя совершенно иные архитектуры ИИ, применяя идеи *формально верифицированных* свойств программ (включая ИИ). Идея, подробно исследованная Омохундро, Тегмарком, Бенджио, Далримплом и другими (см. [здесь](https://arxiv.org/abs/2309.01933) и [здесь](https://arxiv.org/abs/2405.06624)), заключается в создании программы с определенными свойствами (например: что человек может ее отключить) и формальном *доказательстве* того, что эти свойства выполняются. Это можно делать сейчас для довольно коротких программ и простых свойств, но (грядущая) мощь программного обеспечения для доказательств с поддержкой ИИ может позволить это для гораздо более сложных программ (например, оболочек) и даже самого ИИ. Это очень амбициозная программа, но по мере нарастания давления на Врата нам понадобятся мощные материалы для их укрепления. Математическое доказательство может быть одним из немногих, которое достаточно прочно.

#### Куда движется индустрия ИИ

При перенаправлении прогресса ИИ инструментальный ИИ все равно останется огромной индустрией. В плане аппаратного обеспечения, даже с ограничениями вычислительных мощностей для предотвращения сверхинтеллекта, обучение и инференс в меньших моделях все равно потребует огромного количества специализированных компонентов. Что касается программного обеспечения, то обезвреживание взрывного роста размера моделей ИИ и вычислений должно просто привести к тому, что компании перенаправят ресурсы на улучшение, диверсификацию и специализацию меньших систем, а не просто на их увеличение.[^129] Там будет много места — вероятно, больше — для всех тех приносящих деньги стартапов Кремниевой долины.[^130]

### Инструментальный ИИ может дать (почти) все, чего хочет человечество, без ИОИ

Интеллект, будь то биологический или машинный, можно в широком смысле рассматривать как способность планировать и выполнять действия, создающие будущее, более соответствующее набору целей. Как таковой, интеллект приносит огромную пользу, когда используется в стремлении к мудро выбранным целям. Искусственный интеллект привлекает огромные инвестиции времени и усилий в основном из-за обещанных им выгод. Поэтому мы должны спросить: в какой степени мы все еще получим выгоды от ИИ, если сдержим его неконтролируемое развитие до сверхинтеллекта? Ответ: мы можем потерять удивительно мало.

Рассмотрим сначала то, что современные системы ИИ уже очень мощны, и мы действительно лишь коснулись поверхности того, что с ними можно делать.[^131] Они вполне способны "управлять процессом" в плане "понимания" вопроса или задачи, представленной им, и того, что потребуется для ответа на этот вопрос или выполнения этой задачи.

Далее, большая часть восторга по поводу современных систем ИИ обусловлена их общностью; но некоторые из самых способных систем ИИ — такие как те, что генерируют или распознают речь или изображения, делают научные предсказания и моделирование, играют в игры и т.д. — гораздо более узки и хорошо "внутри Врат" в плане вычислений.[^132] Эти системы сверхчеловечны в конкретных задачах, которые они выполняют. У них могут быть слабости в крайних случаях[^133] (или [уязвимости для эксплуатации](https://arxiv.org/abs/2211.00241)) из-за их узости; однако *полностью* узкие или *полностью* общие — не единственные доступные варианты: есть много архитектур между ними.[^134]

Эти инструменты ИИ могут значительно ускорить развитие других позитивных технологий без ИОИ. Чтобы улучшить ядерную физику, нам не нужен ИИ-физик-ядерщик — у нас есть такие! Если мы хотим ускорить медицину, дадим биологам, медицинским исследователям и химикам мощные инструменты. Они хотят их и будут использовать с огромной выгодой. Нам не нужна серверная ферма с миллионом цифровых гениев; у нас есть миллионы людей, чью гениальность может помочь раскрыть ИИ. Да, потребуется больше времени, чтобы получить бессмертие и лекарство от всех болезней. Это реальная цена. Но даже самые многообещающие медицинские инновации будут малополезны, если нестабильность, вызванная ИИ, приведет к глобальному конфликту или коллапсу общества. Мы обязаны дать людям, усиленным ИИ, возможность сначала попробовать решить проблему.

И предположим, что действительно есть какая-то огромная выгода от ИОИ, которую нельзя получить человечеству, используя инструменты внутри Врат. Теряем ли мы ее, *никогда* не создавая ИОИ и сверхинтеллект? Взвешивая здесь риски и награды, есть огромная асимметричная выгода в ожидании против спешки: мы можем подождать, пока это не сможет быть сделано гарантированно безопасным и полезным образом, и почти все все равно смогут пожинать плоды; если мы будем спешить, это может быть — по словам генерального директора OpenAI Сэма Альтмана — [конец света для *всех* нас.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

Но если инструменты без ИОИ потенциально настолько мощны, можем ли мы ими управлять? Ответ четкий... возможно.

### Системы инструментального ИИ (вероятно, в принципе) управляемы

Но это будет нелегко. Современные передовые системы ИИ могут значительно расширить возможности людей и институтов в достижении их целей. Это, в общем, хорошо! Однако есть естественная динамика наличия таких систем в нашем распоряжении — внезапно и без достаточного времени для адаптации общества — которая создает серьезные риски, требующие управления. Стоит обсудить несколько основных классов таких рисков и то, как их можно уменьшить, предполагая закрытие Врат.

Один класс рисков — это предоставление мощным инструментальным ИИ доступа к знаниям или возможностям, которые ранее были привязаны к человеку или организации, делая комбинацию высоких возможностей плюс высокой лояльности доступной очень широкому кругу субъектов. Сегодня, имея достаточно денег, человек со злыми намерениями мог бы нанять команду химиков для разработки и производства нового химического оружия — но не так уж легко иметь такие деньги или найти/собрать команду и убедить их сделать что-то явно незаконное, неэтичное и опасное. Чтобы предотвратить игру систем ИИ такой ролью, улучшения существующих методов вполне могут оказаться достаточными,[^135] при условии, что все эти системы и доступ к ним ответственно управляются. С другой стороны, если мощные системы выпускаются для общего использования и модификации, любые встроенные меры безопасности, вероятно, можно удалить. Поэтому, чтобы избежать рисков в этом классе, потребуются строгие ограничения на то, что может быть публично выпущено — аналогично ограничениям на детали ядерных, взрывчатых и других опасных технологий.[^136]

Второй класс рисков связан с масштабированием машин, которые действуют как люди или выдают себя за людей. На уровне вреда отдельным людям эти риски включают гораздо более эффективные мошенничества, спам и фишинг, а также распространение дипфейков без согласия.[^137] На коллективном уровне они включают нарушение основных социальных процессов, таких как публичные дискуссии и дебаты, наши общественные системы сбора, обработки и распространения информации и знаний, а также наши системы политического выбора. Смягчение этого риска, вероятно, потребует (а) законов, ограничивающих выдавание ИИ-системами себя за людей, и возложения ответственности на разработчиков ИИ, создающих системы, генерирующие такие подделки, (б) систем водяных знаков и происхождения, которые идентифицируют и классифицируют (ответственно) генерированный ИИ контент, и (в) новых социотехнических эпистемических систем, которые могут создать доверенную цепочку от данных (например, камер и записей) через факты, понимание и хорошие модели мира.[^138] Все это возможно, и ИИ может помочь с некоторыми частями этого.

Третий общий риск заключается в том, что в той степени, в которой некоторые задачи автоматизируются, люди, в настоящее время выполняющие эти задачи, могут иметь меньшую финансовую ценность как рабочая сила. Исторически автоматизация задач делала вещи, обеспечиваемые этими задачами, дешевле и более доступными, при этом разделяя людей, ранее выполнявших эти задачи, на тех, кто все еще вовлечен в автоматизированную версию (как правило, с более высокими навыками/оплатой), и тех, чья рабочая сила стоит меньше или мало. В итоге трудно предсказать, в каких секторах потребуется больше, а в каких меньше человеческого труда в результирующем большем, но более эффективном секторе. Параллельно динамика автоматизации имеет тенденцию увеличивать неравенство и общую производительность, уменьшать стоимость определенных товаров и услуг (через повышение эффективности) и увеличивать стоимость других (через [болезнь стоимости](https://en.wikipedia.org/wiki/Baumol_effect)). Для тех, кто оказался на неблагоприятной стороне роста неравенства, совершенно неясно, перевешивает ли снижение стоимости тех определенных товаров и услуг рост других и приводит к общему большему благополучию. Так как же это будет с ИИ? Из-за относительной легкости, с которой человеческий интеллектуальный труд может быть заменен общим ИИ, мы можем ожидать быструю версию этого с общецелевым ИИ, конкурентоспособным с человеком.[^139] Если мы закроем Врата к ИОИ, гораздо меньше рабочих мест будет оптом заменено ИИ-агентами; но огромное перемещение рабочей силы все равно вероятно в течение нескольких лет.[^140] Чтобы избежать широко распространенных экономических страданий, вероятно, будет необходимо внедрить как некую форму универсального базового достояния или дохода, так и спроектировать культурный сдвиг к оценке и вознаграждению человекоориентированного труда, который сложнее автоматизировать (а не видеть, как цены на труд падают из-за роста доступной рабочей силы, вытесненной из других частей экономики). Другие конструкты, такие как ["достоинство данных"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society) (при котором человеческие производители обучающих данных автоматически получают роялти за ценность, созданную этими данными в ИИ), могут помочь. Автоматизация ИИ также имеет второй потенциальный неблагоприятный эффект — *неуместную* автоматизацию. Наряду с применениями, где ИИ просто работает хуже, это включало бы те, где системы ИИ, вероятно, нарушают моральные, этические или правовые принципы — например, в решениях жизни и смерти и в судебных вопросах. С этим нужно бороться, применяя и расширяя наши текущие правовые рамки.

Наконец, значительная угроза ИИ внутри врат — это его использование в персонализированном убеждении, захвате внимания и манипулировании. Мы видели в социальных сетях и других онлайн-платформах рост глубоко укоренившейся экономики внимания (где онлайн-сервисы ожесточенно борются за внимание пользователей) и систем ["капитализма слежки"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism) (в которых пользовательская информация и профилирование добавляются к коммодификации внимания). Почти наверняка больше ИИ будет поставлено на службу обеим. ИИ уже активно используется в вызывающих привыкание алгоритмах лент, но это эволюционирует в вызывающий привыкание контент, генерированный ИИ, настроенный для компульсивного потребления одним человеком. И ввод, ответы и данные этого человека будут подаваться в машину внимания/рекламы для продолжения порочного круга. Кроме того, поскольку ИИ-помощники, предоставляемые технологическими компаниями, становятся интерфейсом для большей части онлайн-жизни, они, вероятно, заменят поисковые системы и ленты как механизм, с помощью которого происходит убеждение и монетизация клиентов. Неспособность нашего общества контролировать эту динамику до сих пор не предвещает ничего хорошего. Часть этой динамики может быть ослаблена через регулирование, касающееся приватности, прав на данные и манипулирования. Более глубокое решение проблемы может потребовать других перспектив, таких как лояльные ИИ-помощники (обсуждаемые ниже).

Суть этого обсуждения — в надежде: системы на основе инструментов внутри Врат — по крайней мере, пока они остаются сравнимыми по мощи и возможностям с сегодняшними самыми передовыми системами — вероятно, управляемы, если есть воля и координация для этого. Достойные человеческие институты, усиленные инструментами ИИ,[^141] могут это сделать. Мы также могли бы потерпеть неудачу в этом. Но трудно увидеть, как разрешение более мощных систем помогло бы — кроме как поставив их во главе и надеясь на лучшее.

### Национальная безопасность

Гонки за превосходство в ИИ — движимые национальной безопасностью или другими мотивами — ведут нас к неконтролируемым мощным системам ИИ, которые имеют тенденцию поглощать, а не наделять властью. Гонка ИОИ между США и Китаем — это гонка за то, какая нация получит сверхинтеллект первой.

Так что же должны делать вместо этого те, кто отвечает за национальную безопасность? Правительства имеют большой опыт создания контролируемых и безопасных систем, и они должны удвоить усилия в этом направлении в ИИ, поддерживая такие инфраструктурные проекты, которые лучше всего удаются при выполнении в масштабе и с правительственным одобрением.

Вместо безрассудного "Манхэттенского проекта" к ИОИ,[^142] правительство США могло бы запустить проект "Аполлон" для контролируемых, безопасных, надежных систем. Это могло бы включать, например:

- Крупную программу по (а) разработке аппаратных механизмов безопасности на чипах и (б) инфраструктуры для управления вычислительной стороной мощного ИИ. Они могли бы строиться на основе американского [закона о CHIPS](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) и [режима экспортного контроля](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion).
- Масштабную инициативу по разработке техник формальной верификации, чтобы определенные свойства систем ИИ (как выключатель) могли быть *доказаны* как присутствующие или отсутствующие. Это может использовать сам ИИ для разработки доказательств свойств.
- Национальный проект по созданию программного обеспечения, которое проверяемо безопасно, приводимого в действие инструментами ИИ, которые могут перекодировать существующее программное обеспечение в проверяемо безопасные фреймворки.
- Национальный инвестиционный проект в научное развитие с использованием ИИ,[^143] работающий как партнерство между Министерством энергетики, NSF и NIH.

В общем, есть огромная поверхность атак на наше общество, которая делает нас уязвимыми к рискам от ИИ и его неправильного использования. Защита от некоторых из этих рисков потребует правительственных инвестиций и стандартизации. Это обеспечило бы гораздо больше безопасности, чем лить бензин на огонь гонок к ИОИ. И если ИИ будет встроен в вооружения и системы командования и управления, критически важно, чтобы ИИ был надежным и безопасным, чем современный ИИ просто не является.

### Концентрация власти и ее смягчение

Это эссе сосредоточилось на идее человеческого контроля над ИИ и его потенциальной неудаче. Но другая валидная линза для рассмотрения ситуации с ИИ — через *концентрацию власти.* Разработка очень мощного ИИ угрожает сконцентрировать власть либо в очень немногих и очень крупных корпоративных руках, которые разработали и будут контролировать его, либо в правительствах, использующих ИИ как новое средство поддержания собственной власти и контроля, либо в самих системах ИИ. Или в какой-то нечестивой смеси вышеперечисленного. В любом из этих случаев большинство человечества теряет власть, контроль и деятельность. Как мы можем с этим бороться?

Самый первый и важнейший шаг, конечно, — закрытие Врат к более умному, чем человек, ИОИ и сверхинтеллекту. Они явно могут напрямую заменить людей и группы людей. Если они находятся под корпоративным или правительственным контролем, они сконцентрируют власть в этих корпорациях или правительствах; если они "свободны", они сконцентрируют власть в себе. Итак, предположим, что Врата закрыты. Тогда что?

Одно предложенное решение концентрации власти — ИИ "с открытым исходным кодом", где веса модели свободно или широко доступны. Но как упоминалось ранее, как только модель открыта, большинство мер безопасности или ограждений могут быть (и обычно бывают) устранены. Поэтому есть острое напряжение между, с одной стороны, децентрализацией, а с другой стороны, безопасностью и человеческим контролем систем ИИ. Также есть основания скептически относиться к тому, что открытые модели сами по себе значимо борются с концентрацией власти в ИИ больше, чем они это делали в операционных системах (все еще доминируемых Microsoft, Apple и Google несмотря на открытые альтернативы).[^144]

Тем не менее, могут быть способы согласовать этот круг — централизовать и смягчить риски, децентрализуя возможности и экономическое вознаграждение. Это требует переосмысления того, как разрабатывается ИИ и как распределяются его выгоды.

Новые модели публичной разработки и владения ИИ помогли бы. Это могло принимать несколько форм: правительственно-разработанный ИИ (подлежащий демократическому надзору),[^145] некоммерческие организации разработки ИИ (как Mozilla для браузеров), или структуры, обеспечивающие очень широкое владение и управление. Ключевым является то, что эти институты были бы явно призваны служить общественному интересу, работая под строгими ограничениями безопасности.[^146] Хорошо сработанные регулятивные режимы и режимы стандартов/сертификации также будут жизненно важны, чтобы ИИ-продукты, предлагаемые живым рынком, оставались по-настоящему полезными, а не эксплуатационными по отношению к их пользователям.

В плане концентрации экономической власти мы можем использовать отслеживание происхождения и "достоинство данных", чтобы обеспечить более широкое распространение экономических выгод. В частности, большинство мощи ИИ сейчас (и в будущем, если мы держим Врата закрытыми) происходит из данных, генерированных человеком, будь то прямые обучающие данные или человеческая обратная связь. Если бы ИИ-компании были обязаны справедливо компенсировать поставщикам данных,[^147] это могло бы по крайней мере помочь распределить экономические награды более широко. Кроме этого, другой моделью может быть публичное владение значительными долями крупных ИИ-компаний. Например, правительства, способные облагать налогом ИИ-компании, могли бы инвестировать долю поступлений в суверенный фонд благосостояния, который держит акции компаний и выплачивает дивиденды населению.[^148]

Ключевым в этих механизмах является использование силы самого ИИ для лучшего распределения власти, а не просто борьба с концентрацией власти, управляемой ИИ, неИИ средствами. Один мощный подход был бы через хорошо спроектированных ИИ-помощников, которые действуют с подлинной фидуциарной обязанностью перед своими пользователями — ставя интересы пользователей на первое место, особенно выше корпоративных поставщиков.[^149] Эти помощники должны быть по-настоящему заслуживающими доверия, технически компетентными, но соответственно ограниченными на основе случая использования и уровня риска, и широко доступными всем через публичные, некоммерческие или сертифицированные коммерческие каналы. Точно так же, как мы никогда не приняли бы человека-помощника, который тайно работает против наших интересов для другой стороны, мы не должны принимать ИИ-помощников, которые наблюдают, манипулируют или извлекают ценность от своих пользователей для корпоративной выгоды.

Такая трансформация кардинально изменила бы текущую динамику, где индивиды остаются вести переговоры в одиночку с огромными (усиленными ИИ) корпоративными и бюрократическими машинами, которые приоритизируют извлечение стоимости над человеческим благосостоянием. Хотя есть много возможных подходов к более широкому перераспределению власти, управляемой ИИ, ни один не появится по умолчанию: они должны быть сознательно спроектированы и управляемы механизмами, такими как фидуциарные требования, публичное обеспечение и многоуровневый доступ на основе риска.

Подходы к смягчению концентрации власти могут столкнуться со значительным противодействием от действующих властей.[^150] Но есть пути к разработке ИИ, которые не требуют выбора между безопасностью и концентрированной властью. Строя правильные институты сейчас, мы могли бы обеспечить широкое разделение выгод ИИ при тщательном управлении его рисками.

### Новые структуры управления и социальные структуры

Наши нынешние структуры управления испытывают трудности: они медленно реагируют, часто захвачены особыми интересами и [все больше вызывают недоверие у общественности.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) Тем не менее, это не причина отказываться от них — совсем наоборот. Некоторые институты могут нуждаться в замене, но шире нам нужны новые механизмы, которые могут улучшить и дополнить наши существующие структуры, помогая им лучше функционировать в нашем быстро развивающемся мире.

Большая часть нашей институциональной слабости происходит не из формальных правительственных структур, а из деградированных социальных институтов: наших систем для развития общего понимания, координации действий и проведения значимого дискурса. До сих пор ИИ ускорил эту деградацию, наводняя наши информационные каналы генерированным контентом, направляя нас к самому поляризующему и разделяющему контенту и затрудняя различение правды от вымысла.

Но ИИ действительно мог бы помочь восстановить и укрепить эти социальные институты. Рассмотрим три ключевые области:

Во-первых, ИИ мог бы помочь восстановить доверие к нашим эпистемическим системам — нашим способам знания того, что истинно. Мы могли бы разработать системы с поддержкой ИИ, которые отслеживают и проверяют происхождение информации, от сырых данных через анализ к заключениям. Эти системы могли бы сочетать криптографическую верификацию со сложным анализом, чтобы помочь людям понять не только, истинно ли что-то, но и как мы знаем, что это истинно.[^151] Лояльные ИИ-помощники могли бы быть поручены следовать деталям, чтобы убедиться, что они подтверждаются.

Во-вторых, ИИ мог бы обеспечить новые формы крупномасштабной координации. Многие из наших самых насущных проблем — от изменения климата до устойчивости к антибиотикам — в основе своей являются проблемами координации. Мы [застряли в ситуациях, которые хуже, чем могли бы быть почти для всех](https://equilibriabook.com/), потому что ни один индивид или группа не могут позволить себе сделать первый ход. Системы ИИ могли бы помочь, моделируя сложные структуры стимулов, идентифицируя жизнеспособные пути к лучшим результатам и облегчая построение доверия и механизмы обязательств, необходимые, чтобы туда добраться.

Возможно, самое интригующее, ИИ мог бы обеспечить совершенно новые формы социального дискурса. Представьте себе возможность "говорить с городом"[^152] — не просто просматривать статистику, но вести значимый диалог с системой ИИ, которая обрабатывает и синтезирует взгляды, опыт, потребности и стремления миллионов жителей. Или подумайте о том, как ИИ мог бы облегчить подлинный диалог между группами, которые в настоящее время говорят мимо друг друга, помогая каждой стороне лучше понять реальные проблемы и ценности другой, а не их карикатуры друг друга.[^153] Или ИИ мог бы предложить квалифицированное, достоверно нейтральное посредничество споров между людьми или даже большими группами людей (которые могли бы все взаимодействовать с ним напрямую и индивидуально!) Современный ИИ полностью способен выполнять эту работу, но инструменты для этого не появятся сами по себе или через рыночные стимулы.

Эти возможности могут звучать утопически, особенно учитывая нынешнюю роль ИИ в деградации дискурса и доверия. Но именно поэтому мы должны активно разрабатывать эти позитивные применения. Закрывая Врата к неконтролируемому ИОИ и приоритизируя ИИ, который усиливает человеческую деятельность, мы можем направить технологический прогресс к будущему, где ИИ служит силой расширения возможностей, устойчивости и коллективного развития.

[^124]: Тем не менее, держаться подальше от тройного пересечения, к сожалению, не так легко, как хотелось бы. Усиленное продвижение возможностей в любом из трех аспектов имеет тенденцию увеличивать их в других. В частности, может быть трудно создать чрезвычайно общий и способный интеллект, который не может быть легко превращен в автономный. Один подход — тренировать модели ["близорукими"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) системами с ограниченной способностью планирования. Другим было бы сосредоточиться на инженерии чистых систем ["оракулов"](https://arxiv.org/abs/1711.05541), которые уклонялись бы от ответов на ориентированные на действия вопросы.

[^125]: Многие компании не понимают, что они тоже в конце концов были бы замещены ИОИ, даже если это займет больше времени — если бы они поняли, они могли бы меньше давить на эти Врата!

[^126]: Системы ИИ могли бы общаться более эффективными, но менее понятными способами, но поддержание человеческого понимания должно иметь приоритет.

[^127]: Эта идея модульного, интерпретируемого ИИ была подробно разработана несколькими исследователями; см., например, модель ["Комплексных услуг ИИ"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) Дрекслера, ["Открытую архитектуру агентности"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai) Далримпла и других. Хотя такие системы могли бы потребовать больше инженерных усилий, чем монолитные нейронные сети, тренированные с массивными вычислениями, именно здесь вычислительные ограничения помогают — делая более безопасный, более прозрачный путь также более практичным.

[^128]: О случаях безопасности в общем см. [этот справочник](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). Касательно ИИ в частности, см. [Wasil et al.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [Clymer et al.](https://arxiv.org/abs/2403.10462), [Buhl et al.](https://arxiv.org/abs/2410.21572), и [Balesni et al.](https://arxiv.org/abs/2411.03336)

[^129]: Мы на самом деле уже видим эту тенденцию, движимую просто высокой стоимостью инференса: меньшие и более специализированные модели, "дистиллированные" из больших и способные работать на менее дорогом аппаратном обеспечении.

[^130]: Я понимаю, почему те, кто воодушевлен экосистемой ИИ-технологий, могут противиться тому, что они видят как обременительное регулирование их индустрии. Но для меня откровенно озадачивающе, почему, скажем, венчурный капиталист хотел бы позволить неконтролируемый переход к ИОИ и сверхинтеллекту. Эти системы (и компании, пока они остаются под контролем компаний) *съедят все стартапы как закуску*. Вероятно, даже *раньше*, чем съедят другие индустрии. Любой, кто инвестирован в процветающую ИИ-экосистему, должен приоритизировать обеспечение того, чтобы разработка ИОИ не привела к монополизации несколькими доминирующими игроками.

[^131]: Как выразился экономист и бывший исследователь Deepmind Майкл Уэбб, "Я думаю, если мы прекратим всю разработку больших языковых моделей сегодня, так что GPT-4 и Claude и что там еще — последние вещи, которые мы тренируем такого размера — так что мы позволяем много больше итераций на вещах такого размера и все виды тонкой настройки, но ничего больше, чем то, никаких больших продвижений — просто то, что у нас есть сегодня, я думаю, достаточно для питания 20 или 30 лет невероятного экономического роста."

[^132]: Например, система alphafold компании DeepMind использовала только одну стотысячную от числа FLOP GPT-4.

[^133]: Трудности самоуправляемых автомобилей важно отметить здесь: хотя номинально узкая задача и достижимая с достаточной надежностью с относительно малыми системами ИИ, обширные реальные знания и понимание необходимы, чтобы получить надежность на уровне, необходимом в такой критически важной для безопасности задаче.

[^134]: Например, при данном вычислительном бюджете мы вероятно увидели бы модели GPAI, предварительно обученные на (скажем) половине этого бюджета, и другая половина использована для тренировки очень высоких возможностей в более узком диапазоне задач. Это дало бы сверхчеловеческие узкие возможности, поддержанные близким к человеческому общим интеллектом.

[^135]: Современная доминирующая техника выравнивания — это ["обучение с подкреплением по человеческой обратной связи"](https://arxiv.org/abs/1706.03741) [(RLHF)](https://arxiv.org/abs/1706.03741) и использует человеческую обратную связь для создания сигнала награды/наказания для обучения с подкреплением модели ИИ. Это и связанные техники, такие как [конституциональный ИИ](https://arxiv.org/abs/2212.08073), работают удивительно хорошо (хотя им не хватает устойчивости, и их можно обойти с умеренными усилиями). Кроме того, современные языковые модели обычно достаточно компетентны в здравом смысле, чтобы не делать глупых моральных ошибок. Это что-то вроде сладкого пятна: достаточно умны, чтобы понимать, что люди хотят (в той степени, в которой это может быть определено), но недостаточно умны, чтобы планировать сложные обманы или причинять огромный вред, когда они ошибаются.

[^136]: В долгосрочной перспективе любой уровень возможностей ИИ, который разрабатывается, вероятно, распространится, поскольку в конечном счете это программное обеспечение, и полезное. Нам нужно будет иметь надежные механизмы защиты от рисков, которые представляют такие системы. Но у нас *этого нет сейчас*, поэтому мы должны быть очень осторожными в том, сколько мощным моделям ИИ разрешено распространяться.

[^137]: Подавляющее большинство из них — неконсенсуальные порнографические дипфейки, включая несовершеннолетних.

[^138]: Многие ингредиенты для таких решений существуют в форме законов "бот-или-не-бот" (в Акте ИИ ЕС среди других мест), [отраслевых технологий отслеживания происхождения](https://c2pa.org/), [инновационных новостных агрегаторов](https://www.improvethenews.org/), [агрегаторов](https://metaculus.com/) предсказаний и рынков и т.д.

[^139]: Волна автоматизации может не следовать предыдущим паттернам в том, что относительно *высоко*-квалифицированные задачи, такие как качественное письмо, интерпретация права или предоставление медицинских советов, могут быть столь же или даже более уязвимы для автоматизации, чем низкоквалифицированные задачи.

[^140]: Для тщательного моделирования влияния ИОИ на зарплаты см. отчет [здесь](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek), и кровавые подробности [здесь](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0), от Антона Коринека и сотрудников. Они обнаружили, что по мере автоматизации большего числа частей работ производительность и зарплаты растут — до точки. Как только *слишком* много автоматизировано, производительность продолжает расти, но зарплаты рушатся, потому что люди заменяются оптом эффективным ИИ. Вот почему закрытие Врат так полезно: мы получаем производительность без исчезнувших человеческих зарплат.

[^141]: Есть много способов использования ИИ как, и для помощи в строительстве, "защитных" технологий, чтобы сделать защиты и управление более устойчивыми. См. [этот](https://vitalik.eth.limo/general/2025/01/05/dacc2.html) влиятельный пост, описывающий эту повестку "D/acc".

[^142]: Несколько иронично, что американский Манхэттенский проект, вероятно, мало что сделал бы для ускорения временных рамок к ИОИ — циферблат человеческих и фискальных инвестиций в прогресс ИИ уже прикован к 11. Первичными результатами было бы вдохновить аналогичный проект в Китае (который преуспевает в инфраструктурных проектах национального уровня), сделать международные соглашения, ограничивающие риск ИИ, гораздо сложнее, и встревожить других геополитических противников США, таких как Россия.

[^143]: Программа ["Национального ресурса исследований ИИ"](https://nairrpilot.org/) — хороший текущий шаг в этом направлении и должна быть расширена.

[^144]: См. [этот анализ](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807) различных значений и последствий "открытости" в технологических продуктах и как некоторые привели к большему, а не меньшему, укреплению доминирования.

[^145]: Планы в США для [Национального ресурса исследований ИИ](https://nairratdoe.ornl.gov/) и недавний запуск [Европейского фонда ИИ](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/) являются интересными шагами в этом направлении.

[^146]: Вызов здесь не технический, а институциональный — нам срочно нужны реальные примеры и эксперименты в том, как могла бы выглядеть разработка ИИ в общественных интересах.

[^147]: Это идет против текущих бизнес-моделей больших технологических компаний и потребовало бы как правовых действий, так и новых норм.

[^148]: Только некоторые правительства смогут это сделать. Более радикальная идея — [универсальный фонд такого типа, под совместным владением всех людей.](https://futureoflife.org/project/the-windfall-trust/)

[^149]: Для длительного изложения этого случая см. [эту статью](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338) о лояльности ИИ. К сожалению, траектория по умолчанию ИИ-помощников, вероятно, будет такой, где они становятся все более нелояльными.

[^150]: Несколько иронично, что многие действующие власти также подвержены риску лишения власти, поддерживаемого ИИ; но для них может быть трудно воспринять это, пока и если процесс не зайдет довольно далеко.

[^151]: Некоторые интересные усилия в этом направлении представлены [коалицией c2pa](https://c2pa.org/) по криптографической верификации; [Verity](https://www.improvethenews.org/) и [Ground news](https://ground.news/) по лучшей новостной эпистемике; и [Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com) и рынками предсказаний по обоснованию дискурса в фальсифицируемых предсказаниях.

[^152]: См. [этот](https://talktothecity.org/) увлекательный пилотный проект.

[^153]: См. [Kialo](https://www.kialo-edu.com/) и усилия [Проекта коллективного интеллекта](https://www.cip.org/) для некоторых примеров.

## Глава 10 - Выбор, который нам предстоит

Чтобы сохранить наше человеческое будущее, мы должны закрыть Врата к ИОИ и сверхинтеллекту.

Последний раз человечество делило Землю с другими разумными существами, которые говорили, мыслили, создавали технологии и решали универсальные задачи, было 40 000 лет назад в ледниковой Европе. Те другие разумы вымерли — полностью или частично из-за деятельности наших предков.

Мы вновь вступаем в подобную эпоху. Самые совершенные плоды нашей культуры и технологий — базы данных, созданные из всего информационного пространства интернета, и стомиллиардные чипы как наиболее сложные технологии, когда-либо нами созданные — объединяются для создания продвинутых систем искусственного интеллекта общего назначения.

Разработчики этих систем стремятся представить их как инструменты расширения человеческих возможностей. И действительно, такими они могли бы стать. Но не стоит заблуждаться: наш нынешний курс ведет к созданию все более мощных, целенаправленных, принимающих решения цифровых агентов с универсальными способностями. Они уже показывают результаты на уровне многих людей в широком спектре интеллектуальных задач, быстро совершенствуются и участвуют в собственном развитии.

Если этот курс не изменится или не столкнется с неожиданным препятствием, у нас вскоре — через годы, а не десятилетия — появятся цифровые интеллекты опасной мощи. Даже в *лучшем* из сценариев они принесут огромные экономические выгоды (по крайней мере, некоторым из нас), но лишь ценой глубочайших потрясений в нашем обществе и замещения людей в большинстве самых важных дел: эти машины будут думать за нас, планировать за нас, решать за нас и творить за нас. Мы станем избалованными, но избалованными детьми. Гораздо вероятнее, что эти системы заменят людей как в позитивных, *так и* в негативных аспектах нашей деятельности, включая эксплуатацию, манипуляции, насилие и войны. Сможем ли мы пережить усиленные ИИ версии всего этого? Наконец, вполне вероятно, что дела пойдут совсем плохо: довольно скоро нас заменят не только в том, что мы *делаем*, но и в том, что мы *есть* — как архитекторов цивилизации и будущего. Спросите у неандертальцев, каково это. Возможно, мы какое-то время тоже будем снабжать их дополнительными безделушками.

*Нам не обязательно идти этим путем.* У нас есть ИИ, конкурирующий с человеком, и нет необходимости создавать ИИ, с которым мы *не сможем* конкурировать. Мы можем создавать потрясающие инструменты ИИ, не создавая при этом вид-преемник. Представление о том, что ИОИ и сверхинтеллект неизбежны, — это *выбор, маскирующийся под судьбу*.

Установив жесткие глобальные ограничения, мы можем удержать общие способности ИИ примерно на человеческом уровне, продолжая при этом пользоваться преимуществами способности компьютеров обрабатывать данные недоступными нам способами и автоматизировать задачи, которые никто из нас не хочет выполнять. Они по-прежнему будут нести множество рисков, но при правильном проектировании и управлении станут огромным благом для человечества — от медицины до исследований и потребительских продуктов.

Введение ограничений потребует международного сотрудничества, но в меньшей степени, чем можно подумать, и эти ограничения все равно оставят достаточно места для огромной индустрии ИИ и аппаратного обеспечения ИИ, сосредоточенной на приложениях, которые повышают благосостояние человека, а не на грубой погоне за властью. И если после серьезных гарантий безопасности и значимого глобального диалога мы решим пойти дальше, этот вариант останется в нашем распоряжении.

Человечество должно *выбрать* закрытие Врат к ИОИ и сверхинтеллекту.

Чтобы сохранить будущее человеческим.

### Слово от автора

Благодарю вас за то, что нашли время изучить эту тему вместе с нами.

Я написал это эссе потому, что как ученый считаю важным говорить неприукрашенную правду, а как человек считаю крайне важным для нас действовать быстро и решительно в решении проблемы, меняющей мир: разработки систем ИИ умнее человека.

Если мы хотим ответить на это удивительное положение дел с мудростью, мы должны быть готовы критически пересмотреть господствующий нарратив о том, что ИОИ и сверхинтеллект «необходимо» создать для защиты наших интересов, или что это «неизбежно» и не может быть остановлено. Эти нарративы лишают нас силы, не позволяя увидеть альтернативные пути, лежащие перед нами.

Надеюсь, вы присоединитесь ко мне в призыве к осторожности перед лицом безрассудства и к мужеству перед лицом алчности.

Надеюсь, вы присоединитесь ко мне в призыве к человеческому будущему.

*– Энтони*

![Anthony Aguirre signature](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAnthony-Aguirre-signature-300x84.png&w=3840&q=75)

## Приложения

Дополнительная информация, включающая технические детали учёта вычислительных мощностей, пример реализации «закрытия врат», подробности строгого режима ответственности за ИОИ и многоуровневый подход к стандартам безопасности ИОИ.

### Приложение А: Технические детали учёта вычислительных мощностей

Для осмысленного контроля на основе вычислительных мощностей требуется детальная методика как для «абсолютной истины», так и для хороших приближений общего объёма вычислений, используемых при обучении и выводе. Вот пример того, как «абсолютная истина» могла бы подсчитываться на техническом уровне.

**Определения:**

*Причинно-следственный граф вычислений:* Для данного выхода O модели ИИ существует набор цифровых вычислений, изменение результата которых потенциально может изменить O. (Это следует предполагать консервативно, то есть должна быть чёткая причина полагать, что вычисление не зависит от предшественника, который происходит раньше во времени и имеет физический потенциальный причинный путь воздействия.) Это включает вычисления, выполняемые моделью ИИ во время вывода, а также вычисления, которые использовались для входных данных, подготовки данных и обучения модели. Поскольку любое из этих вычислений само может быть выходом модели ИИ, это вычисляется рекурсивно, с обрывом там, где человек внёс значительное изменение во входные данные.

*Вычисления обучения:* Общий объём вычислений в FLOP или других единицах, охватываемый причинно-следственным графом вычислений нейронной сети (включая подготовку данных, обучение, тонкую настройку и любые другие вычисления).

*Вычисления вывода:* Общий объём вычислений в причинно-следственном графе вычислений данного выхода ИИ, включая все нейронные сети (и включая их Вычисления обучения) и другие вычисления, участвующие в этом выходе.

*Скорость вычислений вывода:* В серии выходов — скорость изменения (в FLOP/с или других единицах) Вычислений вывода между выходами, то есть вычисления, используемые для производства следующего выхода, делённые на временной интервал между выходами.

**Примеры и приближения:**

- Для одной нейронной сети, обученной на созданных человеком данных, Вычисления обучения — это просто общие вычисления обучения, как обычно сообщается.
- Для такой нейронной сети, выполняющей вывод с постоянной скоростью, Скорость вычислений вывода приблизительно равна общей скорости вычислительного кластера, выполняющего вывод, в FLOP/с.
- Для тонкой настройки модели Вычисления обучения полной модели определяются как Вычисления обучения не-тонко-настроенной модели плюс вычисления, выполненные во время тонкой настройки и для подготовки любых данных, используемых при тонкой настройке.
- Для дистиллированной модели Вычисления обучения полной модели включают обучение как дистиллированной модели, так и большей модели, используемой для предоставления синтетических данных или других обучающих входных данных.
- Если обучено несколько моделей, но многие «попытки» отброшены на основе человеческого суждения, они не засчитываются в Вычисления обучения или Вычисления вывода сохранённой модели.

### Приложение Б: Пример реализации закрытия врат

**Пример реализации:** Вот один пример того, как могло бы работать закрытие врат при лимите в 10<sup>27</sup> FLOP для обучения и 10<sup>20</sup> FLOP/с для вывода (работы ИИ):

**1. Пауза:** По соображениям национальной безопасности исполнительная власть США просит все компании, базирующиеся в США, ведущие бизнес в США или использующие чипы, произведённые в США, прекратить любые новые запуски обучения ИИ, которые могут превысить лимит Вычислений обучения в 10<sup>27</sup> FLOP. США должны начать обсуждения с другими странами, где развивается ИИ, настоятельно призывая их предпринять аналогичные шаги и указывая, что пауза США может быть снята, если они решат не выполнять требования.

**2. Надзор и лицензирование США:** Путём исполнительного указа или действий существующего регулирующего агентства США требуют, чтобы в течение (скажем) одного года:

- Все запуски обучения ИИ с оценкой выше 10<sup>25</sup> FLOP, выполняемые компаниями, работающими в США, были зарегистрированы в базе данных, поддерживаемой американским регулирующим агентством. (Примечание: несколько более слабая версия этого уже была включена в отменённый исполнительный указ США по ИИ 2023 года, требующий регистрации моделей выше 10<sup>26</sup> FLOP.)
- Все производители оборудования, связанного с ИИ, работающие в США или ведущие бизнес с правительством США, соблюдали набор требований к своему специализированному оборудованию и программному обеспечению, управляющему им. (Многие из этих требований могли бы быть встроены в обновления программного обеспечения и прошивки существующего оборудования, но долгосрочные и надёжные решения потребовали бы изменений в более поздних поколениях оборудования.) Среди них требование о том, что если оборудование является частью высокоскоростного взаимосвязанного кластера, способного выполнять 10<sup>18</sup> FLOP/с вычислений, требуется более высокий уровень верификации, который включает регулярное разрешение от удалённого «регулятора», получающего как телеметрию, так и запросы на выполнение дополнительных вычислений.
- Владелец сообщает общий объём вычислений, выполненных на его оборудовании, агентству, поддерживающему базу данных США.
- Поэтапно вводятся более строгие требования, позволяющие как более безопасный, так и более гибкий надзор и выдачу разрешений.

**3. Международный надзор:**

- США, Китай и любые другие страны, где размещены передовые производства чипов, ведут переговоры о международном соглашении.
- Это соглашение создаёт новое международное агентство, аналогичное Международному агентству по атомной энергии, отвечающее за надзор за обучением и работой ИИ.
- Страны-подписанты должны требовать от своих отечественных производителей оборудования для ИИ соблюдения набора требований по крайней мере таких же строгих, как те, что введены в США.
- Владельцы теперь обязаны сообщать числа вычислений ИИ как агентствам в своих странах, так и новому офису в международном агентстве.
- Дополнительные страны настоятельно поощряются присоединиться к существующему международному соглашению: экспортный контроль стран-подписантов ограничивает доступ к высокотехнологичному оборудованию для стран, не подписавших соглашение, в то время как подписанты могут получить техническую поддержку в управлении своими системами ИИ.

**4. Международная верификация и принуждение:**

- Система верификации оборудования обновляется так, что сообщает об использовании вычислений как первоначальному владельцу, так и напрямую офису международного агентства.
- Агентство через обсуждение с подписантами международного соглашения договаривается об ограничениях вычислений, которые затем приобретают юридическую силу в странах-подписантах.
- Параллельно может быть разработан набор международных стандартов, так что обучение и работа ИИ выше порога вычислений (но ниже лимита) будут обязаны соблюдать эти стандарты.
- Агентство может, если необходимо для компенсации лучших алгоритмов и т.д., понизить лимит вычислений. Или, если это считается безопасным и целесообразным (на уровне доказуемых гарантий безопасности), повысить лимит вычислений.

### Приложение В: Подробности строгого режима ответственности за ИОИ

**Подробности строгого режима ответственности за ИОИ**

- Создание и эксплуатация передовой системы ИИ, которая является высоко общей, способной и автономной, считается «ненормально опасной» деятельностью.
- Таким образом, уровень ответственности по умолчанию за обучение и эксплуатацию таких систем — строгая, солидарная ответственность (или её неамериканский эквивалент) за любой вред, причинённый моделью или её выходами/действиями.
- Личная ответственность будет налагаться на руководителей и членов совета директоров в случаях грубой небрежности или умышленного неправомерного поведения. Это должно включать уголовные наказания для самых вопиющих случаев.
- Существуют многочисленные «безопасные гавани», при которых ответственность возвращается к ответственности по умолчанию (основанной на вине, в США), которой обычно подлежали бы люди и компании.
	- Модели, обученные и эксплуатируемые ниже некоторого порога вычислений (который был бы по крайней мере в 10 раз ниже описанных выше лимитов.)
	- ИИ, который является «слабым» (грубо говоря, ниже уровня человеческого эксперта в задачах, для которых он предназначен) и/или
	- ИИ, который является «узким» (имеющий фиксированный и довольно ограниченный круг задач и операций, для которых он специально разработан и обучен) и/или
	- ИИ, который является «пассивным» (очень ограниченный в своей способности — даже при небольших модификациях — предпринимать действия или выполнять сложные многошаговые задачи без прямого человеческого участия и контроля.)
	- ИИ, который гарантированно безопасен, защищён и контролируем (доказуемо безопасен, или анализ рисков указывает на незначительный уровень ожидаемого вреда.)
- «Безопасные гавани» могут заявляться на основе [обоснования безопасности](https://arxiv.org/abs/2410.21572), подготовленного разработчиком ИИ и одобренного агентством или аудитором, аккредитованным агентством. Чтобы заявить «безопасную гавань» на основе вычислений, разработчик должен просто предоставить достоверные оценки общих Вычислений обучения и максимальной Скорости вывода
- Законодательство должно чётко очерчивать ситуации, при которых судебные запреты на разработку систем ИИ с высоким риском общественного вреда были бы уместными.
- Консорциумы компаний, работая с НПО и правительственными агентствами, должны разработать стандарты и нормы, определяющие эти термины, как регуляторы должны предоставлять «безопасные гавани», как разработчики ИИ должны развивать обоснования безопасности, и как суды должны интерпретировать ответственность там, где «безопасные гавани» не заявлены проактивно.

### Приложение Г: Многоуровневый подход к стандартам безопасности ИОИ

**Многоуровневый подход к стандартам безопасности ИОИ**

| Уровень риска | Триггер(ы) | Требования для обучения | Требования для развёртывания |
| --- | --- | --- | --- |
| УР-0 | ИИ слабый в автономности, общности и интеллекте | нет | нет |
| УР-1 | ИИ сильный в одном из: автономность, общность и интеллект | нет | На основе риска и использования, потенциально обоснования безопасности, одобренные национальными властями везде, где модель может использоваться |
| УР-2 | ИИ сильный в двух из: автономность, общность и интеллект | Регистрация у национального органа, имеющего юрисдикцию над разработчиком | Обоснование безопасности, ограничивающее риск серьёзного вреда ниже разрешённых уровней, плюс независимые аудиты безопасности (включая чёрно-ящичное и бело-ящичное красное команды), одобренные национальными властями везде, где модель может использоваться |
| УР-3 | ИОИ сильный в автономности, общности и интеллекте | Предварительное одобрение плана безопасности национальным органом, имеющим юрисдикцию над разработчиком | Обоснование безопасности, гарантирующее ограниченный риск серьёзного вреда ниже разрешённых уровней, а также требуемые спецификации, включая кибербезопасность, контролируемость, несъёмный аварийный выключатель, выравнивание с человеческими ценностями и устойчивость к злонамеренному использованию. |
| УР-4 | Любая модель, которая также превышает либо 10<sup>27</sup> FLOP Обучения, либо 10<sup>20</sup> FLOP/с Вывода | Запрещено до международно согласованного снятия лимита вычислений | Запрещено до международно согласованного снятия лимита вычислений |

Классификации рисков и стандарты безопасности, с уровнями, основанными на пороговых значениях вычислений, а также комбинациях высокой автономности, общности и интеллекта:

- *Сильная автономность* применяется, если система способна выполнять или может быть легко приспособлена для выполнения многошаговых задач и/или предпринимать сложные действия, которые актуальны в реальном мире, без значительного человеческого надзора или вмешательства. Примеры: автономные транспортные средства и роботы; боты для финансовой торговли. Неподходящие примеры: GPT-4; классификаторы изображений
- *Сильная общность* указывает на широкий спектр применения, выполнение задач, для которых модель не была намеренно и специально обучена, и значительную способность изучать новые задачи. Примеры: GPT-4; mu-zero. Неподходящие примеры: AlphaFold; автономные транспортные средства; генераторы изображений
- *Сильный интеллект* соответствует соответствию производительности человеческого эксперта в задачах, в которых модель работает лучше всего (и для общей модели — по широкому кругу задач.) Примеры: AlphaFold; mu-zero; o3. Неподходящие примеры: GPT-4; Siri

### Благодарности

Несколько слов благодарности людям, которые внесли вклад в работу «Сохраним будущее человечным».

Данная работа отражает мнение автора и не должна рассматриваться как официальная позиция Института будущего жизни (хотя они совместимы; официальную позицию см. на [этой странице](https://futureoflife.org/our-position-on-ai/)) или любой другой организации, с которой связан автор.

Я благодарен людям — Марку Бракелю, Бену Айзенпрессу, Анне Хехир, Карлосу Гутьерресу, Эмилии Яворски, Ричарду Маллаху, Джордану Шарнхорсту, Элайз Фулчер, Максу Тегмарку и Яану Таллинну — за комментарии к рукописи; Тиму Шриеру за помощь с некоторыми источниками; Тейлор Джонс и Элайз Фулчер за улучшение диаграмм.

При создании этой работы ограниченно использовались генеративные модели ИИ (Claude и ChatGPT) для редактирования и проверки на прочность. По устоявшейся шкале уровней вовлечения ИИ в творческие работы эта работа получила бы оценку 3/10. (На самом деле такой шкалы не существует! Но должна была бы.)

Мы очень благодарны [Джулиусу Одаи](https://www.linkedin.com/in/julius-odai/) за создание веб-версии этого эссе, которая делает чтение и навигацию по тексту очень удобными. Джулиус — технолог и недавний участник курса по управлению ИИ BlueDot Impact.