# Geleceği İnsan Tutun

Bu makale, YGZ ve superintellijansa giden kapıları neden ve nasıl kapatmamız gerektiğini ve bunun yerine ne inşa etmemiz gerektiğini savunmaktadır.

Sadece ana çıkarımları öğrenmek istiyorsanız, Yönetici Özeti'ne göz atın. Ardından, 2-5. Bölümler makalede ele alınan yapay zeka sistemleri hakkında arka plan bilgisi sunacaktır. 5-7. Bölümler YGZ'nin neden yakında gelebileceğini ve geldiğinde ne olabileceğini açıklamaktadır. Son olarak, 8-9. Bölümler YGZ'nin inşa edilmesini önlemek için somut bir öneri sunmaktadır.

[PDF İndir](https://keepthefuturehuman.ai/wp-content/uploads/2025/03/Keep_the_Future_Human__AnthonyAguirre__5March2025.pdf)

Toplam okuma süresi: 2-3 saat

## Yönetici Özeti

Makaleye üst düzey bir bakış. Zamanınız kısıtlıysa, tüm ana noktaları sadece 10 dakikada öğrenin.

Son on yılda yapay zeka alanındaki çarpıcı ilerlemeler (dar amaçlı AI için) ve son birkaç yıldaki gelişmeler (genel amaçlı AI için) yapay zekayı niş bir akademik alandan dünyanın en büyük şirketlerinin temel iş stratejisine dönüştürdü. AI'ın yeteneklerini geliştirmeye yönelik teknik ve teknolojilere yılda yüzlerce milyar dolarlık yatırım yapılıyor.

Şimdi kritik bir kavşağa geldik. Yeni AI sistemlerinin yetenekleri birçok bilişsel alanda insanlarınkiyle eşleşmeye ve onları aşmaya başladığında, insanlık şunu karar vermelidir: ne kadar ileri gidiyoruz ve hangi yönde?

AI, tüm teknolojiler gibi, yaratıcısı için işleri iyileştirme hedefiyle başladı. Ancak mevcut yörüngemiz ve zımni seçimimiz, birkaç dev teknoloji şirketinin ekonomik teşvikleriyle yönlendirilen, mevcut ekonomik faaliyetin ve insan emeğinin büyük bölümlerini otomatikleştirmeyi amaçlayan, sürekli daha güçlü sistemlere doğru kontrolsüz bir yarış. Bu yarış çok daha uzun süre devam ederse, kaçınılmaz bir kazanan var: AI'ın kendisi - ekonomimizde, düşüncelerimizde, kararlarımızda insanlara göre daha hızlı, daha akıllı, daha ucuz bir alternatif ve sonunda medeniyetimizin kontrolünü elinde tutan bir güç.

Ama başka bir seçim yapabiliriz: hükümetlerimiz aracılığıyla AI geliştirme sürecinin kontrolünü ele alarak net sınırlar, geçmeyeceğimiz çizgiler ve kesinlikle yapmayacağımız şeyler koyabiliriz - tıpkı nükleer teknolojiler, kitle imha silahları, uzay silahları, çevresel açıdan yıkıcı süreçler, insanların biyomühendisliği ve öjeni için yaptığımız gibi. En önemlisi, AI'ın insanları güçlendiren bir araç olarak kalmasını sağlayabiliriz; bizi değiştiren ve sonunda yerimizi alan yeni bir tür haline gelmesini değil.

Bu makale, insanlardan daha akıllı, otonom, genel amaçlı AI - bazen "YGZ" olarak adlandırılan - ve özellikle bazen "superintellijans" denilen son derece insanüstü versiyonuna giden "kapıları" kapatarak *geleceği insani tutmamızı* savunuyor. Bunun yerine, bireyleri güçlendirebilecek ve insan toplumlarının en iyi yaptıkları şeyleri yapma yeteneklerini dönüştürücü şekilde iyileştirebilecek güçlü, güvenilir AI araçlarına odaklanmalıyız. Bu argümanın yapısı kısaca şöyle.

### AI farklıdır

AI sistemleri temelden diğer teknolojilerden farklıdır. Geleneksel yazılımlar kesin talimatları takip ederken, AI sistemleri nasıl yapacağı açıkça söylenmeden hedeflere ulaşmayı öğrenir. Bu onları güçlü kılar: hedefi veya başarı ölçütünü net tanımlayabildiğimizde, çoğu durumda bir AI sistemi bunu gerçekleştirmeyi öğrenebilir. Ama aynı zamanda onları doğası gereği öngörülemez kılar: amaçlarına ulaşmak için hangi eylemleri yapacaklarını güvenilir şekilde belirleyemeyiz.

Ayrıca büyük ölçüde açıklanamaz durumdalar: kısmen kod olmalarına rağmen, çoğunlukla anlaşılmaz sayılardan - sinir ağı "ağırlıklarından" - oluşan devasa bir küme olduklarından ayrıştırılamazlar; iç işleyişlerini anlamakta, biyolojik bir beyne bakarak düşünceleri çözmeye çalışmaktan çok daha iyi değiliz.

Dijital sinir ağlarını eğitmenin bu temel yöntemi hızla karmaşıklık kazanıyor. En güçlü AI sistemleri, devasa veri kümeleri üzerinde sinir ağlarını eğitmek için özelleşmiş donanım kullanan masif hesaplama deneyleri yoluyla yaratılıyor ve daha sonra yazılım araçları ve üst yapılarla güçlendiriliyor.

Bu, metin ve görsel yaratma ve işleme, matematiksel ve bilimsel akıl yürütme, bilgi toplama ve geniş insan bilgisi stoklarını etkileşimli sorgulama konularında çok güçlü araçların yaratılmasına yol açtı.

Ne yazık ki, daha güçlü, daha güvenilir teknolojik araçlar geliştirmek yapmamız *gereken* şey ve neredeyse herkesin istediği ve istediğini söylediği şey olsa da, aslında gittiğimiz yön bu değil.

### YGZ ve superintellijans

Alanın doğuşundan bu yana, AI araştırmaları bunun yerine farklı bir hedefe odaklandı: Yapay Genel Zeka. Bu odak noktası artık AI gelişimini yöneten dev şirketlerin odağı haline geldi.

YGZ nedir? Genellikle "insan seviyesinde AI" olarak belirsiz tanımlanır, ama bu sorunlu: hangi insanlar ve hangi yeteneklerde insan seviyesinde? Peki halihazırda sahip olduğu insanüstü yetenekler ne olacak? YGZ'yi anlamanın daha kullanışlı yolu üç temel özelliğin kesişimi: yüksek **O** tonom (eylem bağımsızlığı), yüksek **G** enellik (geniş kapsam ve uyum sağlayabilirlik), ve yüksek **Z** eka (bilişsel görevlerde yetkinlik). Mevcut AI sistemleri oldukça yetenekli ama dar, ya da genel ama sürekli insan gözetimi gerektiren, ya da otonom ama sınırlı kapsamlı olabilir.

Tam O-G-Z, üç özelliği de en üst düzey insan yeteneğini eşleyen veya aşan seviyelerde birleştirir. Kritik olan şu ki, insanları bu kadar etkili ve mevcut yazılımlardan bu kadar farklı kılan bu kombinasyon; aynı zamanda insanların toptan dijital sistemlerle değiştirilebilmesini sağlayacak olan da bu.

İnsan zekası özel olsa da, hiçbir şekilde bir sınır değildir. Yapay "süper zekili" sistemler yüzlerce kat daha hızlı çalışabilir, çok daha fazla veriyi işleyebilir ve muazzam miktarlarda bilgiyi aynı anda "aklında" tutabilir, ve insan topluluklarından çok daha büyük ve etkili kümeler oluşturabilir. Sadece bireyleri değil şirketleri, ulusları ya da bir bütün olarak medeniyetimizi yerinden edebilirler.

### Eşikteyiz

YGZ'nin *mümkün* olduğu konusunda güçlü bir bilimsel konsensüs var. AI, son zamanlarda üst düzey akıl yürütme ve problem çözme dahil olmak üzere, birçok genel entelektüel yetenek testinde insan performansını zaten aşıyor. Geride kalan yetenekler - sürekli öğrenme, planlama, öz-farkındalık ve özgünlük gibi - mevcut AI sistemlerinde bir düzeyde var ve bunların hepsini geliştirme olasılığı yüksek olan bilinen teknikler mevcut.

Birkaç yıl öncesine kadar birçok araştırmacı YGZ'yi onlarca yıl uzakta görürken, şu anda YGZ'ye kısa vadeli kanıtlar güçlü:

- Deneysel olarak doğrulanmış "ölçekleme yasaları" hesaplama girdisini AI yeteneğine bağlıyor ve şirketler önümüzdeki birkaç yıl içinde hesaplama girdisini büyüklük mertebeleri artıracak yolda. AI gelişimine ayrılan insan ve mali kaynaklar şimdi bir düzine Manhattan Projesi ve birkaç Apollo Projesi'ne eşit.
- AI şirketleri ve liderleri, YGZ'nin (bir tanımıyla) birkaç yıl içinde elde edilebileceğine alenen ve özel olarak inanıyorlar. Bu şirketlerin kamunun bilmediği bilgileri var; bazıları bir sonraki nesil AI sistemlerini elinde tutuyor.
- Kanıtlanmış sicili olan uzman tahminciler, YGZ'nin (bir tanımıyla) 1-2 yıl içinde gelmesi için %25, 2-5 yıl için %50 olasılık atfediyorlar (bkz. Metaculus ['zayıf'](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) ve ['tam'](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/) YGZ tahminleri).
- Otonomi (uzun menzilli esnek planlama dahil) AI sistemlerinde geride kalıyor, ama büyük şirketler artık muazzam kaynaklarını otonom AI sistemleri geliştirmeye odaklıyorlar ve 2025'i gayri resmi olarak ["ajan yılı"](https://techinformed.com/2025-informed-the-year-of-agentic-ai/) ilan ettiler.
- AI kendi gelişimine giderek daha fazla katkıda bulunuyor. AI sistemleri AI araştırması yapan insan AI araştırmacıları kadar yetkin olduğunda, çok daha güçlü AI sistemlerine hızlı ilerleme için kritik bir eşik geçilecek ve muhtemelen AI yeteneğinde kaçış hızına yol açacak. (Tartışılabilir ki, bu kaçış hızı çoktan başladı.)

İnsanlardan daha akıllı YGZ'nin onlarca yıl ya da daha uzun süre uzakta olduğu fikri artık alandaki uzmanların büyük çoğunluğu için geçerli değil. Anlaşmazlıklar artık bu rotada devam edersek kaç ay ya da yıl süreceği konusunda. Karşılaştığımız temel soru: devam etmeli miyiz?

### YGZ yarışını ne yönlendiriyor

YGZ'ye doğru yarış birden fazla güç tarafından yönlendiriliyor ve her biri durumu daha tehlikeli hale getiriyor. Büyük teknoloji şirketleri YGZ'yi nihai otomasyon teknolojisi olarak görüyor - sadece insan çalışanları desteklemek değil, büyük ölçüde ya da tamamen değiştirmek. Şirketler için ödül muazzam: insan emek maliyetlerini otomatikleştirerek dünyanın 100 trilyon dolarlık yıllık ekonomik çıktısının önemli bir bölümünü ele geçirme fırsatı.

Uluslar bu yarışa katılmak zorunda hissediyorlar, alenen ekonomik ve bilimsel liderliği öne sürerken, özel olarak YGZ'yi nükleer silahlara benzer askeri işlerdeki potansiel devrime bakıyorlar. Rakiplerin belirleyici stratejik avantaj elde edebileceği korkusu klasik silah yarışı dinamiği yaratıyor.

Superintellijansı hedefleyenler genellikle büyük vizyonlar öne sürerler: tüm hastalıkları iyileştirmek, yaşlanmayı tersine çevirmek, enerji ve uzay yolculuğunda atılımlar gerçekleştirmek, ya da insanüstü planlama yetenekleri yaratmak.

Daha az cömert bakıldığında, yarışı yönlendiren güçtür. Her katılımcı - ister şirket ister ülke olsun - zekanın güce eşit olduğuna ve bu gücün en iyi koruyucusunun kendileri olduğuna inanıyor.

Bu motivasyonların gerçek ama temelden yanlış yönlendirilmiş olduğunu savunuyorum: YGZ güç *verecek* değil güç *emecek* ve *arayacak*; AI-yaratılmış teknolojiler *de* güçlü şekilde iki yönlü keskin olacak ve faydalı oldukları yerlerde AI araçlarıyla ve YGZ olmadan yaratılabilecek; ve YGZ ve çıktıları kontrolde kaldığı ölçüde bile, bu yarış dinamikleri - hem kurumsal hem jeopolitik - kararlı şekilde kesintiye uğratılmadıkça toplumumuza büyük ölçekli riskleri neredeyse kaçınılmaz hale getiriyor.

### YGZ ve superintellijans medeniyet için çarpıcı bir tehdit oluşturuyor

Çekicilikleri olmasına rağmen, YGZ ve superintellijans birbirini güçlendiren çoklu yollarla medeniyet için çarpıcı tehditler oluşturuyor:

*Güç yoğunlaşması:* insanüstü AI, sosyal ve ekonomik faaliyetin büyük bölümlerini avuç dolusu dev şirket (sırayla hükümetler tarafından devralınabilir ya da hükümetleri fiilen devralabilir) tarafından işletilen AI sistemlerine absorbe ederek insanlığın büyük çoğunluğunu güçsüzleştirebilir.

*Masif bozulma:* bilişsel tabanlı işlerin toplu otomasyonu, mevcut epistemik sistemlerimizin değiştirilmesi, ve çok sayıda aktif insani olmayan ajanın kullanıma sunulması mevcut medeni sistemlerimizin çoğunu nispeten kısa sürede altüst edecektir.

*Felaketler:* yeni askeri ve yıkıcı teknolojiler yaratma yeteneğini - potansiyel olarak insan seviyesinin üzerinde - yaygınlaştırarak ve bunu sorumluluğu temelleyen sosyal ve hukuki sistemlerden ayırarak, kitle imha silahlarından fiziksel felaketler çarpıcı şekilde daha olası hale gelir.

*Jeopolitik ve savaş:* büyük dünya güçleri, "belirleyici stratejik avantaj" sağlayabilecek bir teknolojinin düşmanları tarafından geliştirildiğini hissederlerse aylak durmayacaklar.

*Kaçış hızı ve kontrol kaybı:* özellikle engellenmediği sürece, insanüstü AI kendini daha da iyileştirme için her türlü teşvike sahip olacak ve hız, veri işleme ve düşünce karmaşıklığında insanları çok geride bırakabilecek. Böyle bir sistemi kontrol etmenin anlamlı hiçbir yolu yok. Böyle AI insanlara güç vermeyecek; biz ona güç vereceğiz ya da o alacak.

Bu risklerin çoğu teknik "hizalama" problemi - gelişmiş AI'ın güvenilir şekilde insanların istediği şeyi yapmasını sağlama - çözülse bile devam eder. AI nasıl yönetileceği konusunda muazzam bir meydan okuma sunar ve bu yönetimin pek çok yönü insan zekası aşıldığında inanılmaz derecede zor ya da çözemez hale gelir.

En temelde, şu anda izlenen insanüstü genel amaçlı AI türü, doğası gereği bizimkini aşan hedeflere, failliğe ve yeteneklere sahip olacaktır. Doğası gereği kontrol edilemez olacaktır - ne anlayamadığımız ne de öngöremediğimiz bir şeyi nasıl kontrol edebiliriz? İnsan kullanımı için teknolojik bir araç değil, Dünya'da bizimkinin yanında ikinci bir zeka türü olacaktır. Daha da ilerlemesine izin verilirse, sadece ikinci bir tür değil yedek tür teşkil edecektir.

Belki bize iyi davranır, belki davranmaz. Ama gelecek bizim değil onun olacaktır. İnsan çağı sona erecektir.

### Bu kaçınılmaz değil; insanlık çok somut şekilde yerimizi alacak olanı inşa etmemeye karar verebilir.

İnsanüstü YGZ yaratılması kaçınılmazdan uzaktır. Koordineli yönetişim önlemleri setiyle bunu engelleyebiliriz:

İlk olarak, büyük ölçekli AI sistemlerinin temel etkinleştiricisi ve yönetim kaldıracı olan AI hesaplamasının ("işlem gücü") sağlam muhasebe ve gözetimine ihtiyacımız var. Bu da AI modelleri eğitme ve çalıştırmada kullanılan toplam işlem gücünün standartlaştırılmış ölçümü ve raporlaması ile kullanılan hesaplamanın sayım, sertifikalandırma ve doğrulama teknik yöntemlerini gerektirير.

İkinci olarak, AI hesaplamasında hem eğitim hem işletim için katı üst sınırlar koymalıyız; bunlar AI'ın hem çok güçlü olmasını hem çok hızlı çalışmasını engeller. Bu sınırlar hem yasal gereklilikler hem de modern telefonlardaki güvenlik özelliklerine benzer şekilde AI özellikli çiplere inşa edilen donanım tabanlı güvenlik önlemleriyle uygulanabilir. Özelleşmiş AI donanımı sadece avuç dolusu şirket tarafından yapıldığından, mevcut tedarik zinciri aracılığıyla doğrulama ve uygulama mümkündür.

Üçüncü olarak, en tehlikeli AI sistemleri için artırılmış sorumluluğa ihtiyacımız var. Yüksek otonomi, geniş genellik ve üstün zekayı birleştiren AI geliştirenlerin zararlar için katı sorumluluğu olmalı, bu sorumluluktan güvenli limanlar ise daha sınırlı ve kontrol edilebilir sistemlerin gelişimini teşvik edecektir.

Dördüncü olarak, risk seviyelerine dayalı kademeli düzenlemeye ihtiyacımız var. En yetenekli ve tehlikeli sistemler geliştirme ve kullanıma sunmadan önce kapsamlı güvenlik ve kontrol edilebilirlik garantileri gerektirecek, daha az güçlü ya da daha özelleşmiş sistemler ise orantılı gözetimle karşılaşacaktır. Bu düzenleyici çerçeve sonunda hem ulusal hem uluslararası düzeylerde çalışmalıdır.

Bu yaklaşım - detayları tam belgede verilmiştir - pratiktir: uluslararası koordinasyon gerekecek olsa da, doğrulama ve uygulama özelleşmiş donanım tedarik zincirini kontrol eden az sayıda şirket aracılığıyla işleyebilir. Ayrıca esnektir: şirketler yine de AI gelişiminden yenilik yapabilir ve kâr edebilir, sadece en tehlikeli sistemlerde net sınırlarla.

AI gücü ve riskinin uzun vadeli kontrolü, nükleer silah yayılmasını kontrol etmenin şu anda yaptığı gibi, hem kendi çıkarı hem ortak çıkara dayalı uluslararası anlaşmalar gerektirecektir. Ama daha kapsamlı yönetişime doğru inşa ederken, artırılmış gözetim ve sorumlulukla hemen başlayabiliriz.

Eksik temel bileşen, AI geliştirme sürecinin kontrolünü ele almak için politik ve sosyal iradedir. Bu iradenin kaynağı, zamanında gelirse, gerçekliğin kendisi olacaktır - yani yaptığımızın gerçek sonuçlarının yaygın fark edilmesinden.

### İnsanlığı güçlendirmek için Araç AI mühendisliği yapabiliriz

Kontrol edilemez YGZ peşinde koşmak yerine, anlamlı insan kontrolünde kalırken insan yeteneğini artıran güçlü "Araç AI" geliştirebiliriz. Araç AI sistemleri, yetenek seviyelerine uygun kontrol edilebilirlik seviyesinde mühendislik yaptığımız sürece, yüksek otonomi, geniş genellik ve insanüstü zekanın tehlikeli üçlü kesişiminden kaçınırken son derece yetenekli olabilir. Ayrıca insan gözetimini korurken dönüştürücü faydalar sağlayan karmaşık sistemlerde birleştirilebilirler.

Araç AI tıpta devrim yaratabilir, bilimsel keşfi hızlandırabilir, eğitimi geliştirebilir ve demokratik süreçleri iyileştirebilir. Uygun şekilde yönetildiğinde, insan uzmanları ve kurumları değiştirmek yerine daha etkili hale getirebilir. Bu sistemler yine de oldukça yıkıcı olacak ve dikkatli yönetim gerektirecek olsa da, oluşturdukları riskler YGZ'ninkinden temelden farklıdır: yönetebileceğimiz risklerdir, diğer güçlü teknolojilerin riskleri gibi, insan failliği ve medeniyeti için varoluşsal tehditler değil. Ve kritik olarak, akıllıca geliştirildiğinde, AI araçları insanların güçlü AI'ı yönetmesine ve etkilerini yönetmesine yardımcı olabilir.

Bu yaklaşım hem AI'ın nasıl geliştirildiğini hem faydalarının nasıl dağıtıldığını yeniden düşünmeyi gerektirir. Yeni kamu ve kâr amacı gütmeyen AI geliştirme modelleri, sağlam düzenleyici çerçeveler ve ekonomik faydaları daha geniş dağıtma mekanizmaları AI'ın güçü birkaç elde yoğunlaştırmak yerine insanlığın tamamını güçlendirmesini sağlayabilir. AI'ın kendisi daha iyi sosyal ve yönetişim kurumları inşa etmekte yardım edebilir, insan toplumunu zayıflatmak yerine güçlendiren yeni koordinasyon ve söylem biçimlerini mümkün kılabilir. Ulusal güvenlik kurumları uzmanlıklarından yararlanarak AI araç sistemlerini gerçekten güvenli ve güvenilir, hem savunma hem de ulusal güç kaynağı haline getirebilir.

Sonunda araç olmaktan daha az benzer ve - umabileceğimiz gibi - akıllı ve güçlü hayırseverlere daha çok benzer daha güçlü ve daha egemen sistemler geliştirmeyi seçebiliriz. Ama bunu ancak güvenli şekilde yapabilecek bilimsel anlayış ve yönetişim kapasitesini geliştirdikten sonra yapmalıyız. Bu kadar önemli ve geri dönüşü olmayan bir karar teknoloji şirketleri ve uluslar arasındaki yarışta varsayılan olarak değil, insanlık tarafından bir bütün halinde kasıtlı olarak alınmalıdır.

### İnsan ellerinde

İnsanlar AI'dan gelen iyiliği istiyor: onları güçlendiren, ekonomik fırsatları ve büyümeyi artıran, bilim, teknoloji ve eğitimde atılımlar vaat eden faydalı araçlar. Neden istemesinler? Ama sorulduğunda, halkın ezici çoğunluğu [daha yavaş ve dikkatli AI gelişimi](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation) istiyor ve onları işlerinde ve başka yerlerde değiştirecek, kültürlerini ve bilgi ortamlarını insani olmayan içerikle dolduracak, gücü küçücük bir şirket setinde yoğunlaştıracak, aşırı büyük ölçekli küresel riskler oluşturacak ve sonunda türlerini güçsüzleştirme ya da değiştirme tehdidi yaratacak insanlardan daha akıllı AI istemiyor. Neden istesinler?

Birini diğeri olmadan *alabiliriz*. Bu, kaderimizin herhangi bir teknolojinin sözde kaçınılmazlığında ya da Silikon Vadisi'ndeki birkaç CEO'nun elinde değil, ona sahip çıkarsak bizim ellerimizin geri kalanında olduğuna karar vermekle başlar. Kapıları kapatalım ve geleceği insani tutalım.

## Bölüm 1 - Giriş

İnsandan daha akıllı yapay zeka ihtimaline nasıl tepki vereceğimiz, çağımızın en acil meselesidir. Bu makale, ileriye dönük bir yol sunmaktadır.

İnsan çağının sonuna gelmiş olabiliriz.

Son on yılda türümüzün tarihinde eşi benzeri görülmemiş bir şey başladı. Bunun sonuçları büyük ölçüde insanlığın geleceğini belirleyecek. 2015 civarından itibaren araştırmacılar *dar kapsamlı* yapay zeka (AI) geliştirmeyi başardılar – Go gibi oyunlarda kazanabilen, görüntü ve konuşmayı herhangi bir insandan daha iyi tanıyabilen sistemler.[^1]

Bu harika bir başarı ve insanlığı güçlendirecek son derece yararlı sistemler ve ürünler ortaya çıkarıyor. Ancak dar kapsamlı yapay zeka hiçbir zaman alanın gerçek hedefi olmadı. Bunun yerine amaç, özellikle "yapay genel zeka" (YGZ) veya "süper zeka" olarak adlandırılan, yapay zekanın şu anda Go, satranç, poker, drone yarışı vb. alanlarda insanüstü olduğu gibi, *tüm* görevlerde aynı anda insanlar kadar iyi veya daha iyi olan *genel* amaçlı yapay zeka sistemleri yaratmaktı. Bu, birçok büyük yapay zeka şirketinin açık hedefidir.[^2]

*Bu çabalar da başarılı oluyor.* ChatGPT, Gemini, Llama, Grok, Claude ve Deepseek gibi büyük hesaplamalar ve dağlar dolusu veriye dayanan genel amaçlı yapay zeka sistemleri, çok çeşitli görevlerde tipik insanlarla eşitliğe ulaştı ve hatta bazı alanlarda insan uzmanlarıyla boy ölçüştü. Şimdi en büyük teknoloji şirketlerinden bazılarındaki yapay zeka mühendisleri, bu dev makine zekası deneylerini insan kapasitelerinin, uzmanlığının ve özerkliğinin tüm alanlarında önce eşitleyen, sonra da aşan seviyelere taşımak için yarışıyorlar.

*Bu çok yakın bir gelecekte gerçekleşecek.* Son on yılda uzmanların bunun ne kadar süreceğine dair tahminleri – mevcut rotamızda devam edersek – on yıllardan (veya yüzyıllardan) tek haneli yıllara düştü.

Bu aynı zamanda çağ açıcı öneme sahip ve aşkın bir risk taşıyor. YGZ savunucuları bunu bilimsel problemleri çözecek, hastalıkları iyileştirecek, yeni teknolojiler geliştirecek ve sıkıcı işleri otomatikleştirecek pozitif bir dönüşüm olarak görüyorlar. Yapay zeka kesinlikle tüm bunları başarmaya yardım edebilir – nitekim şimdiden ediyor. Ancak on yıllar boyunca Alan Turing'den Stephen Hawking'e, günümüzün Geoffrey Hinton ve Yoshua Bengio'suna[^3] kadar pek çok dikkatli düşünür sert bir uyarı yaptı: gerçekten insandan daha akıllı, genel, otonom yapay zeka inşa etmek en azından toplumu tamamen ve geri döndürülemez şekilde alt üst edecek, en kötü ihtimalle ise insan türünün yok olmasına yol açacaktır.[^4]

Süper zekâlı yapay zeka mevcut yolumuzda hızla yaklaşıyor, ancak kaçınılmaz olmaktan çok uzak. Bu makale, bu yaklaşan insandışı geleceğe giden *Kapıları* neden ve nasıl kapatmamız gerektiği ve bunun yerine ne yapmamız gerektiği konusunda genişletilmiş bir argümandır.


[^1]: Bu [çizelge](https://time.com/6300942/ai-progress-charts/) bir dizi görevi gösteriyor; bu grafiğe benzer pek çok eğri eklenebilir. Dar kapsamlı yapay zekadaki bu hızlı ilerleme alandaki uzmanları bile şaşırtmış, kriterler tahminlerin yıllar öncesinde aşılmıştır.

[^2]: Deepmind, OpenAI, Anthropic ve X.ai'nin hepsi YGZ geliştirme özel amacıyla kuruldu. Örneğin OpenAI'ın tüzüğünde açıkça "tüm insanlığa fayda sağlayan yapay genel zeka" geliştirme hedefi belirtilirken, DeepMind'ın misyonu "zekayı çözmek, sonra da bunu kullanarak her şeyi çözmek"tir. Meta, Microsoft ve diğerleri artık büyük ölçüde benzer yolları izliyor. Meta, [YGZ geliştirmeyi ve bunu açık kaynak olarak yayınlamayı planladığını](https://www.forbes.com/sites/johnkoetsier/2024/01/18/zuckerberg-on-ai-meta-building-agi-for-everyone-and-open-sourcing-it/) söyledi.

[^3]: Hinton ve Bengio en çok atıf alan yapay zeka araştırmacıları arasında yer alır, ikisi de yapay zeka alanının Nobeli olan Turing Ödülü'nü kazanmışlar ve Hinton ayrıca (fizik) Nobel Ödülü'ne de sahiptir.

[^4]: Bu düzeyde risk taşıyan bir şeyi ticari teşvikler altında ve neredeyse sıfır devlet gözetimi ile inşa etmek tamamen emsalsizdir. Bunu inşa edenler arasında risk konusunda tartışma bile yok! Deepmind, OpenAI ve Anthropic'in liderleri ile diğer birçok uzman, gelişmiş yapay zekanın *insanlık için varoluşsal risk* oluşturduğuna dair bir [bildirgeyi](https://www.safe.ai/work/statement-on-ai-risk) resmen imzaladılar. Alarm zilleri daha yüksek sesle çalamaz ve bunları görmezden gelenlerin YGZ ve süper zekayı ciddiye almadıkları sonucuna varabiliriz. Bu makalenin amaçlarından biri, onların neden ciddiye alması gerektiğini anlamalarına yardım etmektir.

## Bölüm 2 - AI sinir ağları hakkında bilinmesi gerekenler

Modern AI sistemleri nasıl çalışır ve yapay zekanın gelecek nesil sistemlerinde neler bekleyebiliriz?

Daha güçlü AI geliştirmenin sonuçlarının nasıl gelişeceğini anlamak için bazı temel kavramları içselleştirmek şarttır. Bu ve sonraki iki bölüm bu temelleri ele alarak sırasıyla modern AI'ın ne olduğunu, nasıl devasa hesaplamaları kullandığını ve genellik ile yeteneklerinin hangi anlamlarda hızla büyüdüğünü inceleyecektir.[^5]

Yapay zekayı tanımlamanın birçok yolu vardır, ancak bizim amaçlarımız için AI'ın temel özelliği şudur: standart bir bilgisayar programı bir görevi nasıl gerçekleştireceğine dair talimatlar listesi iken, AI sistemi veri veya deneyimden öğrenerek görevleri *nasıl yapacağı açıkça söylenmeden* gerçekleştiren sistemdir.

Neredeyse tüm önemli modern AI sinir ağlarına dayanır. Bunlar çok büyük sayıda (milyarlarca veya trilyonlarca) sayı ("ağırlık") ile temsil edilen ve bir eğitim görevini iyi gerçekleştiren matematiksel/hesaplamalı yapılardır. Bu ağırlıklar, sinir ağının bir veya daha fazla görevde iyi performans sergilemek için tanımlanmış sayısal bir puanı ("kayıp") iyileştirmesi amacıyla yinelemeli olarak ayarlanarak üretilir (veya belki "yetiştirilir" ya da "bulunur").[^6] Bu süreç sinir ağının *eğitimi* olarak bilinir.[^7]

Bu eğitimi gerçekleştirmek için birçok teknik bulunsa da, bu detaylar puanlamanın nasıl tanımlandığından ve bunların sinir ağının iyi performans sergilediği farklı görevleri nasıl ortaya çıkardığından çok daha az önemlidir. Tarihsel olarak "dar" ve "genel" AI arasında temel bir ayrım yapılmıştır.

Dar AI, belirli bir görevi veya küçük görev kümesini (görüntü tanıma veya satranç oynama gibi) yapmak için kasıtlı olarak eğitilir; yeni görevler için yeniden eğitim gerektirir ve dar bir yetenek kapsamına sahiptir. İnsanüstü dar AI'a sahibiz, yani bir kişinin yapabileceği hemen her ayrık, iyi tanımlanmış görev için muhtemelen bir puan oluşturabiliriz ve ardından bunu bir insanın yapabileceğinden daha iyi yapacak dar bir AI sistemi başarıyla eğitebiliriz.

Genel amaçlı AI (GPAI) sistemleri, açıkça eğitildikleri görevler de dahil olmak üzere geniş bir görev yelpazesi gerçekleştirebilir; ayrıca işleyişlerinin bir parçası olarak yeni görevler öğrenebilirler. ChatGPT gibi mevcut büyük "çoklu modal modeller"[^8] buna örnek teşkil eder: çok büyük metin ve görüntü derlemleri üzerinde eğitilen bu sistemler karmaşık mantık yürütme, kod yazma, görüntü analizi yapma ve çok geniş bir entelektüel görev dizisinde yardım edebilme yeteneği gösterirler. Aşağıda derinlemesine göreceğimiz gibi insan zekasından hala oldukça farklı olmakla birlikte, genellikleri AI'da bir devrime neden olmuştur.[^9]

### Öngörülemezlik: AI sistemlerinin temel özelliği

AI sistemleri ile geleneksel yazılım arasındaki temel fark öngörülebilirlik konusundadır. Standart yazılımın çıktısı öngörülemez olabilir - nitekim bazen yazılım yazma nedenimiz tam da öngöremeyeceğimiz sonuçlar elde etmektir. Ancak geleneksel yazılım nadiren programlanmadığı bir şey yapar - kapsamı ve davranışı genellikle tasarlandığı gibidir. Üst düzey bir satranç programı hiçbir insanın öngöremeyeceği hamleler yapabilir (aksi halde o satranç programını yenebilirlerdi!) ama genellikle satranç oynamaktan başka bir şey yapmaz.

Geleneksel yazılım gibi, dar AI de öngörülebilir kapsam ve davranışa sahiptir ancak öngörülemez sonuçlar verebilir. Bu aslında dar AI'ı tanımlamanın başka bir yoludur: öngörülebilirlik ve işleyiş aralığında geleneksel yazılıma benzer AI olarak.

Genel amaçlı AI farklıdır: kapsamı (uygulandığı alanlar), davranışı (yaptığı şey türleri) ve sonuçları (gerçek çıktıları) hepsi öngörülemez olabilir.[^10] GPT-4 sadece metni doğru üretmek için eğitildi, ancak eğiticilerinin öngörmediği veya amaçlamadığı birçok yetenek geliştirdi. Bu öngörülemezlik eğitimin karmaşıklığından kaynaklanır: eğitim verisi birçok farklı görevden çıktılar içerdiği için, AI iyi tahmin yapabilmek için bu görevleri etkili şekilde öğrenmek zorundadır.

Genel AI sistemlerinin bu öngörülemezliği oldukça temeldir. Prensipte davranışları üzerinde garantili sınırları olan AI sistemleri dikkatli şekilde inşa etmek mümkün olsa da (makalenin ileriki bölümlerinde değinildiği gibi), AI sistemlerinin şu anda yaratılma biçimleri nedeniyle hem uygulamada hem de prensipte öngörülemezdirler.

### Pasif AI, ajanlar, otonom sistemler ve hizalama

Bu öngörülemezlik, AI sistemlerinin çeşitli hedeflere ulaşmak için gerçekte nasıl konuşlandırıldığı ve kullanıldığı düşünüldüğünde özellikle önemli hale gelir.

Birçok AI sistemi, öncelikle bilgi sağladıkları ve kullanıcının eylem aldığı anlamda nispeten pasiftir. Diğerleri, yaygın olarak *ajan* olarak adlandırılan sistemler ise, kullanıcıdan değişen düzeylerde katılımla kendileri eylem alırlar. Nispeten daha az dış girdi veya gözetimle eylem alanlar daha *otonom* olarak adlandırılabilir. Bu, pasif araçlardan otonom ajanlara kadar eylem bağımsızlığı açısından bir spektrum oluşturur.[^11]

AI sistemlerinin hedefleri açısından bakıldığında, bunlar doğrudan eğitim amaçlarıyla bağlantılı olabilir (örneğin Go oynayan bir sistemin "kazanma" hedefi aynı zamanda eğitildiği şeydir). Ya da olmayabilir: ChatGPT'nin eğitim amacı kısmen metin tahmin etmek, kısmen de yardımcı bir asistan olmaktır. Ancak belirli bir görevi yaparken hedefi kullanıcı tarafından kendisine verilir. Hedefler ayrıca bir AI sistemi tarafından kendisi yaratılabilir ve eğitim amacıyla sadece çok dolaylı olarak ilişkili olabilir.[^12]

Hedefler "hizalama" sorusuyla, yani AI sistemlerinin *yapmalarını istediğimiz şeyi yapıp yapmayacağı* sorusuyla yakından bağlantılıdır. Bu basit soru muazzam düzeyde incelik barındırır.[^13] Şimdilik, bu cümledeki "biz"in birçok farklı kişi ve gruba atıfta bulunabileceğini ve farklı hizalama türlerine yol açabileceğini not edin. Örneğin, bir AI kullanıcısına son derece *itaatkâr* (veya ["sadık"](https://arxiv.org/abs/2003.11157)) olabilir - burada "biz" "her birimiz" demektir. Ya da daha *egemen* olabilir, öncelikle kendi hedefleri ve kısıtlamaları tarafından yönlendirilir ancak yine de insan refahının ortak çıkarı doğrultusunda geniş anlamda hareket eder - o zaman "biz" "insanlık" veya "toplum" demektir. Arada bir AI'ın büyük ölçüde itaatkâr olacağı ama başkalarına veya topluma zarar veren, yasayı ihlal eden vb. eylemleri almayı reddedebileceği bir spektrum vardır.

Bu iki eksen - otonomi düzeyi ve hizalama türü - tamamen bağımsız değildir. Örneğin egemen pasif sistem, tam çelişkili olmasa da gerilimli bir kavramdır, itaatkâr otonom ajan da öyledir.[^14] Otonomi ve egemenliğin el ele gitme eğiliminde olduğu açık bir anlam vardır. Benzer şekilde, öngörülebilirlik "pasif" ve "itaatkâr" AI sistemlerinde daha yüksek olma eğilimindeyken, egemen veya otonom olanlar daha öngörülemez olma eğilimindedir. Tüm bunlar potansiyel YGZ ve süper zekanın etkilerini anlamak için kritik olacaktır.

Her türden gerçekten hizalanmış AI yaratmak, üç farklı zorluğu çözmeyi gerektirir:

1. "Bizim" ne istediğimizi anlamak - bu "biz" belirli bir kişi veya organizasyon (sadakat) ya da genel olarak insanlık (egemenlik) anlamına gelsin karmaşıktır;
2. Bu isteklere uygun şekilde düzenli hareket eden sistemler inşa etmek - esasen tutarlı olumlu davranış yaratmak;
3. En temelde, sadece öyleymiş gibi davranmak yerine bu istekleri gerçekten "önemseyen" sistemler yapmak.

Güvenilir davranış ile gerçek önemseme arasındaki ayrım kritiktir. Tıpkı bir insan çalışanın organizasyonun misyonuna gerçek bir bağlılığı olmadan emirleri mükemmel şekilde yerine getirebilmesi gibi, bir AI sistemi de insan tercihlerini gerçekten değerli bulmadan hizalanmış şekilde davranabilir. AI sistemlerini geri bildirim yoluyla bir şeyler söyleyip yapmaya eğitebiliriz ve insanların ne istediği hakkında akıl yürütmeyi öğrenebilirler. Ancak onları insan tercihlerini *gerçekten* değerli kılmak çok daha derin bir zorluktur.[^15]

Bu hizalama zorluklarını çözmedeki derin güçlükler ve AI riski için etkileri aşağıda daha ayrıntılı keşfedilecektir. Şimdilik, hizalamanın AI sistemlerine sonradan eklediğimiz teknik bir özellik değil, insanlıkla ilişkilerini şekillendiren mimarilerinin temel bir yönü olduğunu anlayın.

[^5]: Makine öğrenmesi ve AI'a, özellikle dil modellerine nazik ama teknik bir giriş için [bu siteye](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) bakın. AI yok oluş riskleri hakkında başka bir modern giriş için [bu yazıya](https://www.thecompendium.ai/) bakın. AI güvenliğinin durumuna ilişkin kapsamlı ve yetkili bilimsel analiz için son [Uluslararası AI Güvenlik Raporu'na](https://arxiv.org/abs/2501.17805) bakın.

[^6]: Eğitim tipik olarak model ağırlıklarının verdiği yüksek boyutlu uzayda puanın yerel maksimumunu arayarak gerçekleşir. Ağırlıklar ayarlandığında puanın nasıl değiştiğini kontrol ederek, eğitim algoritması hangi ayarlamaların puanı en çok iyileştirdiğini belirler ve ağırlıkları o yöne hareket ettirir.

[^7]: Örneğin, bir görüntü tanıma probleminde sinir ağı görüntü için etiketlerin olasılıklarını çıktı olarak verir. Puan, AI'ın doğru cevaba verdiği olasılıkla ilişkili olacaktır. Eğitim prosedürü daha sonra ağırlıkları ayarlayacak ki gelecek sefer AI o görüntü için doğru etikete daha yüksek olasılık versin. Bu daha sonra çok büyük sayıda tekrarlanır. Aynı temel prosedür, daha karmaşık puanlama mekanizmaları olmakla birlikte, esasen tüm modern sinir ağlarının eğitiminde kullanılır.

[^8]: Çoğu çoklu modal model, birden fazla veri türünü (metin, görüntü, ses) işlemek ve üretmek için "transformer" mimarisini kullanır. Bunların hepsi farklı "token" türleri olarak ayrıştırılabilir ve daha sonra aynı temelde ele alınabilir. Çoklu modal modeller önce devasa veri kümelerindeki tokenleri doğru tahmin etmek için eğitilir, sonra yetenekleri artırmak ve davranışları şekillendirmek için pekiştirmeli öğrenme ile rafine edilir.

[^9]: Dil modellerinin bir şey yapmak - kelimeler tahmin etmek - için eğitilmesi bazılarının onları dar AI olarak adlandırmasına neden olmuştur. Ancak bu yanıltıcıdır: metni iyi tahmin etmek çok farklı yetenekler gerektirdiği için, bu eğitim görevi şaşırtıcı derecede genel bir sistem ortaya çıkarır. Ayrıca bu sistemlerin pekiştirmeli öğrenme ile kapsamlı şekilde eğitildiğini unutmayın, bu da esasen yaptığı birçok şeyden herhangi birinde iyi iş çıkardığında binlerce insanın modele ödül sinyali vermesini temsil eder. Böylece bu geri bildirimi veren insanlardan önemli ölçüde genellik miras alır.

[^10]: AI'ın öngörülemez olduğu birden fazla yol vardır. Birincisi, genel durumda bir algoritmanın ne yapacağını onu fiilen çalıştırmadan tahmin edemezsiniz; bu konuda [teoremler](https://arxiv.org/abs/1310.3225) vardır. Bu, algoritmaların çıktısının karmaşık olabilmesi nedeniyle doğru olabilir. Ancak tahmin yapmanın bir yeteneği (AI'ı yenme) ima edeceği durumda (satranç veya Go'da olduğu gibi) özellikle açık ve alakalıdır ki tahmin yapmak isteyen kişi bu yeteneğe sahip değildir. İkincisi, belirli bir AI sistemi aynı girdi verilse bile her zaman aynı çıktıyı üretmez - çıktıları rastlantısallık içerir; bu da algoritmik öngörülemezlikle birleşir. Üçüncüsü, eğitimden beklenmedik ve ortaya çıkan yetenekler doğabilir, yani bir AI sisteminin yapabileceği ve yapacağı şeylerin *türleri* bile öngörülemezdir; Bu son tür güvenlik değerlendirmeleri için özellikle önemlidir.

[^11]: "Otonom ajan"ın ne anlama geldiğine ilişkin derinlemesine inceleme için (onları inşa etmeye karşı etik argümanlarla birlikte) [buraya](https://arxiv.org/abs/2502.02649) bakın.

[^12]: Bazen "AI'ın kendi hedefleri olamaz" diye duyabilirsiniz. Bu tamamen saçmalıktır. AI'ın kendisine hiç verilmeyen ve sadece kendisinin bildiği hedefleri olduğu veya geliştirdiği örnekler üretmek kolaydır. Bunu mevcut popüler çoklu modal modellerde pek görmezsiniz çünkü bunlarda bu eğitilip çıkarılır; onlara eğitilmesi de aynı kolaylıkta olabilir.

[^13]: Geniş bir literatür vardır. Genel problem için Christian'ın [*The Alignment Problem*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821) ve Russell'ın [*Human-Compatible*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) eserlerine bakın. Daha teknik taraf için örneğin [bu makaleye](https://arxiv.org/abs/2209.00626) bakın.

[^14]: Böyle sistemler eğilime aykırı olmakla birlikte, bu durum onları aslında çok ilginç ve faydalı kılmaktadır.

[^15]: Bu duygu veya bilinç gerektirdiğimizi söylemek değildir. Daha ziyade, bir sistemin dışından onun iç hedeflerinin, tercihlerinin ve değerlerinin ne olduğunu bilmek son derece zordur. Burada "gerçek" kritik sistemler durumunda hayatlarımızı ona bahse koyabileceğimiz kadar güçlü nedenimiz olduğu anlamına gelir.

## Bölüm 3 - Modern genel yapay zeka sistemlerinin nasıl yapıldığının temel yönleri

Dünyanın en son teknoloji yapay zeka sistemlerinin çoğu şaşırtıcı derecede benzer yöntemlerle yapılıyor. İşte temel bilgiler.

Bir insanı gerçekten anlamak için biyoloji, evrim, çocuk yetiştirme ve daha fazlası hakkında bir şeyler bilmeniz gerekir; yapay zekayı anlamak için de nasıl yapıldığını bilmeniz gerekir. Son beş yılda, yapay zeka sistemleri hem yetenek hem de karmaşıklık açısından muazzam bir gelişim gösterdi. Bunu mümkün kılan temel faktör, çok büyük miktarlarda hesaplama gücünün (yapay zeka bağlamında konuşma dilinde "işlem gücü" olarak da adlandırılır) kullanılabilir hale gelmesi oldu.

Rakamlar çok etkileyici. GPT serisi, Claude, Gemini ve benzeri modellerin eğitiminde yaklaşık 10<sup>25</sup>-10<sup>26</sup> "kayan noktalı işlem" (FLOP) [^16] kullanılıyor.[^17] (Karşılaştırma için, Dünya'daki her insan durmaksızın her beş saniyede bir hesaplama yapsa, bunu başarmak yaklaşık bir milyar yıl alırdı.) Bu devasa hesaplama miktarı, trilyonlarca model ağırlığına sahip modellerin terabytlarca veri üzerinde eğitilmesini mümkün kılıyor - şimdiye kadar yazılmış kaliteli metnin büyük bir bölümünün yanı sıra kapsamlı ses, görüntü ve video arşivlerini kullanarak. Bu eğitimi, insan tercihlerini ve iyi görev performansını pekiştiren ek kapsamlı eğitimle tamamlayan, bu şekilde eğitilmiş modeller, akıl yürütme ve problem çözme de dahil olmak üzere temel entelektüel görevlerin önemli bir bölümünde insanlarla yarışabilir performans sergiliyorlar.

Ayrıca böyle bir sistemin *çıkarım* hızının[^18] insan metin işleme *hızı* ile eşleşmesi için ne kadar hesaplama hızının (saniye başına işlem) yeterli olduğunu da (çok, çok kabaca) biliyoruz. Bu yaklaşık 10<sup>15</sup>-10<sup>16</sup> FLOP/saniye civarındadır.[^19]

Güçlü olmalarına rağmen, bu modeller doğaları gereği temel yönlerden sınırlıdır - tıpkı bir insanın dakikada sabit bir kelime hızında metin çıktısı vermeye zorlanması, durup düşünme veya ek araçlar kullanma fırsatı verilmemesi halinde sınırlı kalması gibi. Daha yeni yapay zeka sistemleri bu sınırlamaları, birkaç temel unsuru birleştiren daha karmaşık bir süreç ve mimari aracılığıyla ele alıyor:

- Biri temel bilişsel kapasiteyi sağlayan ve diğerleri daha dar görevleri gerçekleştiren bir veya daha fazla sinir ağı;
- Model tarafından sağlanan ve kullanılabilen *araçlar* - örneğin web'de arama yapma, belge oluşturma veya düzenleme, program çalıştırma vb. yeteneği.
- Sinir ağlarının giriş ve çıkışlarını bağlayan *destek yapısı*. Çok basit bir destek yapısı, bir yapay zeka modelinin iki "örneğinin" birbirleriyle konuşmasına veya birinin diğerinin çalışmasını kontrol etmesine olanak sağlayabilir.[^20]
- *Düşünce zinciri* ve ilgili yönlendirme teknikleri benzer bir şey yaparak, modelin örneğin bir probleme birçok yaklaşım üretmesini, ardından bu yaklaşımları toplu bir cevap için işlemesini sağlar.
- Araçları, destek yapılarını ve düşünce zincirini daha iyi kullanmak için modelleri *yeniden eğitme*.

Bu uzantılar çok güçlü olabileceğinden (ve yapay zeka sistemlerini de içerdiklerinden), bu kompozit sistemler oldukça sofistike olabilir ve yapay zeka yeteneklerini dramatik şekilde artırabilir.[^21] Ve yakın zamanda, destek yapısı tekniklerinde ve özellikle düşünce zinciri yönlendirmesinde (ve sonuçları modellerin bunları daha iyi kullanması için yeniden eğitime geri beslenmesinde) [o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/) ve [DeepSeek R1](https://api-docs.deepseek.com/news/news250120)'de geliştirilen ve kullanılan teknikler, belirli bir sorguya yanıt olarak birçok çıkarım geçişi yapmayı mümkün kılıyor.[^22] Bu, modelin yanıtını "düşünmesine" olanak tanır ve bu modellerin bilim, matematik ve programlama görevlerinde yüksek kaliteli akıl yürütme yeteneklerini dramatik şekilde artırır.[^23]

Belirli bir yapay zeka mimarisi için, eğitim hesaplamasındaki artışlar [güvenilir şekilde çevrilebilir](https://arxiv.org/abs/2405.10938) ve açıkça tanımlanmış bir dizi ölçütte iyileştirmelere dönüştürülebilir. Daha az net tanımlanmış genel yetenekler için (aşağıda tartışılanlar gibi), çeviri daha az net ve öngörülebilir, ancak daha büyük ve daha fazla eğitim hesaplaması olan modellerin yeni ve daha iyi yeteneklere sahip olacağı neredeyse kesin, bunların ne olacağını tahmin etmek zor olsa bile.

Benzer şekilde, kompozit sistemler ve özellikle "düşünce zincirindeki" gelişmeler (ve bununla iyi çalışan modellerin eğitimi), *çıkarım* hesaplamasında ölçeklemeyi mümkün kıldı: belirli bir eğitilmiş temel model için, en azından bazı yapay zeka sistem yetenekleri, karmaşık problemleri "daha sert ve uzun düşünmelerine" olanak tanıyan daha fazla hesaplama uygulandıkça artıyor. Bu, insan performansıyla eşleşmek için yüzlerce veya binlerce kat daha fazla FLOP/s gerektiren yüksek hesaplama hızı maliyeti getiriyor.[^24]

Hızlı yapay zeka ilerlemesini sağlayanın yalnızca bir parçası olsa da,[^25] hesaplama rolü ve kompozit sistemlerin olasılığı hem kontrol edilemez YGZ'yi önlemek hem de daha güvenli alternatifler geliştirmek için kritik olacak.

[^16]: 10<sup>27</sup>, 25 sıfırla 1 veya on trilyon trilyon demektir. Bir FLOP, sadece belirli bir hassasiyetle sayıların aritmetik toplama veya çarpma işlemidir. Yapay zeka donanımı performansının aritmetiğin hassasiyetine ve bilgisayarın mimarisine bağlı olarak on kat daha fazla değişebileceğini unutmayın. Mantık kapısı işlemlerini saymak (AND, OR, NOT) daha temel olurdu ancak bunlar yaygın olarak mevcut değil veya kıyaslanmıyor; mevcut amaçlar için 16-bit işlemlerde (FP16) standardizasyon yapmak yararlıdır, uygun dönüştürme faktörleri oluşturulmalıdır.

[^17]: [Epoch AI](https://epochai.org/data/large-scale-ai-models)'dan tahmin ve kesin veri koleksiyonu mevcut ve GPT-4 için yaklaşık 2×10<sup>25</sup> 16-bit FLOP gösteriyor; bu kabaca GPT-4 için [sızan sayılarla](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) eşleşiyor. 2024 ortası diğer modeller için tahminler GPT-4'ün birkaç katı içinde.

[^18]: Çıkarım, basitçe bir sinir ağından çıktı üretme sürecidir. Eğitim, birçok çıkarım ve model ağırlığı ayarlamasının ardışıklığı olarak düşünülebilir.

[^19]: Metin üretimi için orijinal GPT-4, üretilen token başına 560 TFLOP gerektiriyordu. İnsan düşüncesine yetişmek için yaklaşık 7 token/s gerekir, bu da ≈3×10<sup>15</sup> FLOP/s verir. Ancak verimlilik bunu düşürdü; örneğin [bu NVIDIA broşürü](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/) karşılaştırılabilir performanslı Llama 405B modeli için 3×10<sup>14</sup> FLOP/s kadar az gösteriyor.

[^20]: Biraz daha karmaşık bir örnek olarak, bir yapay zeka sistemi önce bir matematik problemine birkaç olası çözüm üretebilir, sonra her çözümü kontrol etmek için başka bir örnek kullanabilir ve son olarak sonuçları açık bir açıklamada sentezlemek için üçüncüsünü kullanabilir. Bu, tek geçişten daha kapsamlı ve güvenilir problem çözmeye olanak tanır.

[^21]: Örneğin [OpenAI'ın "Operator"ı](https://openai.com/index/introducing-operator/), [Claude'un araç yetenekleri](https://docs.anthropic.com/en/docs/build-with-claude/computer-use) ve [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) hakkındaki ayrıntılara bakın. OpenAI'ın [Deep Research](https://openai.com/index/introducing-deep-research/)'ü muhtemelen oldukça sofistike bir mimariye sahip ancak ayrıntılar mevcut değil.

[^22]: Deepseek R1, modeli iteratif olarak eğitmeye ve yönlendirmeye dayanıyor böylece nihai eğitilmiş model kapsamlı düşünce zinciri akıl yürütmesi oluşturuyor. O1 veya o3 için mimari ayrıntıları mevcut değil, ancak Deepseek çıkarımla yetenek ölçeklemeyi açmak için özel bir "sihirli formül" gerekmediğini ortaya koydu. Ancak yapay zekada "mevcut durumu" altüst ettiği konusunda büyük medya ilgisi almasına rağmen, bu makalenin temel iddialarını etkilemiyor.

[^23]: Bu modeller, akıl yürütme kıyaslamalarında standart modellerden önemli ölçüde daha iyi performans gösteriyor. Örneğin, doktora seviyesi bilim sorularının zorlu testi olan GPQA Diamond Kıyaslamasında GPT-4o %56 [puan alırken](https://openai.com/index/learning-to-reason-with-llms/), o1 ve o3 sırasıyla %78 ve %88 başarı göstererek insan uzmanların %70 ortalama puanını çok aştılar.

[^24]: OpenAI'ın O3'ü muhtemelen [ARC-AGI meydan okuma sorularının her birini tamamlamak için](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai) ∼10<sup>21</sup>-10<sup>22</sup> FLOP harcadı, yetkili insanların (diyelim) 10-100 saniyede yapabildiği şeyi, bu da ∼10<sup>20</sup> FLOP/s gibi bir rakam veriyor.

[^25]: Hesaplama yapay zeka sistem yeteneğinin temel ölçüsü olsa da, hem veri kalitesi hem de algoritmik iyileştirmelerle etkileşim halindedir. Daha iyi veri veya algoritmalar hesaplama gereksinimlerini azaltabilirken, daha fazla hesaplama bazen daha zayıf veri veya algoritmaları telafi edebilir.

## Bölüm 4 - Yapay Genel Zeka ve süper zeka nedir?

Dünyanın en büyük teknoloji şirketleri kapalı kapılar ardında tam olarak neyi inşa etmek için yarışıyor?

"Yapay genel zeka" terimi, "insan düzeyinde" genel amaçlı yapay zekayı işaret etmek için uzun süredir kullanılıyor. Hiçbir zaman özellikle iyi tanımlanmış bir terim olmamıştı, ancak son yıllarda paradoks bir şekilde daha iyi tanımlanmadığı halde daha da önemli hale geldi; uzmanlar aynı anda YGZ'nin on yıllar uzakta mı yoksa zaten başarıldı mı tartışırken, trilyon dolarlık şirketler "YGZ'ye doğru" yarışıyor. ("YGZ"nin belirsizliği yakın zamanda [sızan belgeler tarafından ortaya çıkarıldı](https://gizmodo.com/leaked-documents-show-openai-has-a-very-clear-definition-of-agi-2000543339); söylenenlere göre OpenAI'ın Microsoft ile yaptığı sözleşmede YGZ, OpenAI için 100 milyar dolar gelir elde eden yapay zeka olarak tanımlanmış - yüksek kültürden ziyade oldukça çıkarcı bir tanım.)

"İnsan düzeyinde zeka"ya sahip yapay zeka fikrinin iki temel sorunu var. Birincisi, insanlar herhangi bir bilişsel iş türünü yapma yetenekleri açısından çok, çok farklılar, dolayısıyla "insan düzeyi" diye bir şey yok. İkincisi, zeka çok boyutlu bir kavram; korelasyonlar olsa bile bunlar kusurlu ve yapay zekada oldukça farklı olabilir. Bu nedenle birçok yetenek için "insan düzeyini" tanımlayabilsek bile, yapay zeka kesinlikle bazılarında çok ötesinde olurken diğerlerinde oldukça altında kalacaktır.[^26]

Yine de yapay zeka yeteneğinin türleri, düzeyleri ve eşikleri hakkında konuşabilmek oldukça kritik. Burada benimsenen yaklaşım, genel amaçlı yapay zekanın burada olduğunu ve çeşitli yetenek düzeylerinde geldiğini ve geleceğini vurgulamaktır; indirgemeci olsalar bile terimleri bağlamak uygun çünkü bunlar yapay zekanın toplum ve insanlık üzerindeki etkileri açısından kritik eşiklere karşılık geliyor.

"Tam" YGZ'yi "insanüstü genel amaçlı yapay zeka" ile eş anlamlı olarak tanımlayacağız; bu, esasen tüm insan bilişsel görevlerini en üst düzey insan uzman seviyesinde veya üzerinde gerçekleştirebilen, ayrıca yeni beceriler edinebilen ve yeteneği yeni alanlara aktarabilen bir yapay zeka sistemini ifade eder. Bu, modern literatürde "YGZ"nin sıklıkla nasıl tanımlandığıyla uyumlu. Bunun *çok* yüksek bir eşik olduğunu belirtmek önemli. Hiçbir insan bu tür bir zekaya sahip değil; daha çok en üst düzey insan uzmanlarının büyük koleksiyonlarının birleştirilmesi durumunda sahip olacağı türden bir zeka. Bu sınırın ötesine geçen bir yeteneği "süper zeka" olarak adlandırabiliriz ve "insanla rekabet eden" ve "uzmanla rekabet eden" GYAZ ile daha sınırlı yetenek düzeylerini tanımlayabiliriz; bunlar geniş bir görev yelpazesini tipik profesyonel veya insan uzman düzeyinde gerçekleştirirler.[^27]

Bu terimler ve bazı diğerleri aşağıdaki [tabloda](https://keepthefuturehuman.ai/essay/docs/#tab:terms) toplanmıştır. Çeşitli sistem derecelerinin neler yapabileceğine dair daha somut bir fikir için, tanımları ciddiye almak ve ne anlama geldiklerini düşünmek yararlıdır.

| YZ Türü                  | İlgili Terimler                      | Tanım                                                                                                                                                                                                                        | Örnekler                                                                                                                                   |
| ------------------------ | ------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
| Dar YZ                   | Zayıf YZ                             | Belirli bir görev veya görev ailesi için eğitilmiş YZ. Kendi alanında mükemmel performans sergiler ancak genel zeka veya transfer öğrenme yeteneğinden yoksundur.                                                          | Görüntü tanıma yazılımları; Sesli asistanlar (örn. Siri, Alexa); Satranç oynayan programlar; DeepMind'ın AlphaFold'u                     |
| Araç YZ                  | Artırılmış Zeka, YZ Asistanı         | (Makalede daha sonra tartışılacak.) İnsan yeteneklerini artıran YZ sistemi. İnsanla rekabet eden genel amaçlı YZ, dar YZ ve garantili kontrolü birleştirir, güvenlik ve işbirliğini önceleyerek insan karar vermeyi destekler. | Gelişmiş kodlama asistanları; YZ destekli araştırma araçları; Sofistike veri analizi platformları. Yetkin ama dar ve kontrol edilebilir ajanlar |
| Genel Amaçlı YZ (GAYAZ)  |                                      | Özellikle eğitilmediği görevler dahil çeşitli görevlere uyarlanabilen YZ sistemi.                                                                                                                                           | Dil modelleri (örn. GPT-4, Claude); Çoklu modal YZ modelleri; DeepMind'ın MuZero'su                                                       |
| İnsanla Rekabet Eden GAYAZ | YGZ \[zayıf\]                        | Görevleri ortalama insan düzeyinde gerçekleştiren, bazen aşan genel amaçlı YZ.                                                                                                                                               | Gelişmiş dil modelleri (örn. O1, Claude 3.5); Bazı çoklu modal YZ sistemleri                                                             |
| Uzmanla Rekabet Eden GAYAZ | YGZ \[kısmi\]                        | Çoğu görevi insan uzman düzeyinde gerçekleştiren, önemli ama sınırlı özerkliğe sahip genel amaçlı YZ.                                                                                                                       | Muhtemelen araçlarla donatılmış ve desteklenmiş O3, en azından matematik, programlama ve bazı sert bilimler için                          |
| YGZ \[tam\]              | İnsanüstü GAYAZ                      | Kabaca tüm insan entelektüel görevlerini uzman düzeyinde veya ötesinde özerk şekilde gerçekleştirebilen, verimli öğrenme ve bilgi transferi olan YZ sistemi.                                                               | \[Mevcut örnekler yok – teorik\]                                                                                                          |
| Süper Zeka               | Oldukça İnsanüstü GAYAZ              | Tüm alanlarda insan yeteneklerini çok aşan, toplu insan uzmanlığını geride bırakan YZ sistemi. Bu üstün performans genellik, kalite, hız ve/veya diğer ölçütlerde olabilir.                                                | \[Mevcut örnekler yok – teorik\]                                                                                                          |

İnsanla rekabet eden düzeye kadar GAYAZ'lara sahip olmanın nasıl bir şey olduğunu zaten deneyimliyoruz. Bu, çoğu kullanıcının bunu akıllı ama sınırlı bir geçici işçiye sahip olmak olarak deneyimlemesi ve işlerinin kalitesi üzerinde karışık etkisi olan üretkenlik artışı sağlaması nedeniyle nispeten sorunsuz entegre oldu.[^28]

Uzmanla rekabet eden GAYAZ'ın farklılığı, günümüz yapay zekasının temel sınırlarına sahip olmaması ve uzmanların yaptığı şeyleri yapması olacaktır: bağımsız ekonomik değerli iş, gerçek bilgi yaratımı, güvenebileceğiniz teknik çalışma, nadiren (yine de ara sıra) aptalca hatalar yapma.

Tam YGZ fikri, en yetenekli ve etkili insanların bile yaptığı tüm bilişsel işleri *gerçekten* özerk şekilde ve hiçbir yardım veya gözetim gerektirmeden yapmasıdır. Bu sofistike planlama, yeni beceriler öğrenme, karmaşık projeleri yönetme vb. içerir. Orijinal çığır açan araştırma yapabilir. Bir şirket yönetebilir. İşiniz ne olursa olsun, ağırlıklı olarak bilgisayarla veya telefonla yapılıyorsa, *en az sizin kadar iyi yapabilir.* Ve muhtemelen çok daha hızlı ve ucuz. Bazı sonuçları aşağıda tartışacağız, ama şimdilik karşınızdaki zorluk bunu gerçekten ciddiye almak. Tanıdığınız veya bildiğiniz en bilgili ve yetkin on kişiyi hayal edin - CEO'lar, bilim insanları, profesörler, en iyi mühendisler, psikologlar, siyasi liderler ve yazarlar dahil. Hepsini, aynı zamanda 100 dil konuşan, muazzam hafızaya sahip, hızlı çalışan, yorulmayan ve her zaman motive olan ve asgari ücretin altında çalışan tek bir kişide toplayın.[^29] İşte YGZ'nin ne olacağına dair bir fikir bu.

Süper zeka için hayal kurmak daha zor, çünkü fikir hiçbir insanın veya hatta insan koleksiyonunun yapamayacağı entelektüel başarıları gerçekleştirebilmesi - tanım gereği bizim için öngörülemez. Ama bir fikir edinebiliriz. Çıplak bir başlangıç noktası olarak, her biri en üst düzey insan uzmanından çok daha yetenekli, insan hızının 100 katında çalışan, muazzam hafızaya ve mükemmel koordinasyon kapasitesine sahip çok sayıda YGZ düşünün.[^30] Ve oradan yukarıya çıkıyor. Süper zekayla uğraşmak farklı bir zihnle konuşmaktan çok, farklı (ve daha gelişmiş) bir medeniyetle müzakere etmek gibi olurdu.

Peki YGZ ve süper zekaya ne kadar yakınız?


[^26]: Örneğin, mevcut yapay zeka sistemleri hızlı aritmetik veya hafıza görevlerinde insan yeteneğini çok aşarken, soyut akıl yürütme ve yaratıcı problem çözmede geride kalıyor.

[^27]: Çok önemlisi, bir rakip olarak böyle bir yapay zekanın insanlara kıyasla birçok büyük yapısal avantajı olurdu: yorulmayacak veya insanlar gibi bireysel ihtiyaçları olmayacak; sadece hesaplama gücünü ölçekleyerek daha yüksek hızlarda çalıştırılabilecek; edindiği herhangi bir uzmanlık veya bilgiyle birlikte kopyalanabilecek - sinir ağlarının edindiği bilgi aralarında tüm beceri setlerini aktarmak için "birleştirilebilir"; makine hızında iletişim kurabilecek; ve herhangi bir insandan daha önemli şekillerde ve daha yüksek hızda kendini değiştirebilir veya geliştirebilecek.

[^28]: Eğer mevcut en üst düzey yapay zeka sistemlerini kullanarak zaman geçirmediyseniz, öneririm: gerçekten yararlı ve yetenekliler, ayrıca daha güçlü hale geldikçe yapay zekanın etkisini kalibre etmek için önemli.

[^29]: Büyük bir araştırma hastanesini düşünün: tam olarak gerçekleşmiş YGZ aynı anda gelen tüm hasta verilerini analiz edebilir, her yeni tıp makalesini takip edebilir, tanı önerebilir, tedavi planları tasarlayabilir, klinik denemeleri yönetebilir ve personel planlamasını koordine edebilir - tüm bunları her alanda hastanenin en iyi uzmanlarını eşleyen veya aşan bir düzeyde yapabilir. Ve bunu mevcut maliyetin bir kısmıyla aynı anda birden fazla hastane için yapabilir. Ne yazık ki, organize suç örgütünü de düşünmelisiniz: tam olarak gerçekleşmiş YGZ aynı anda binlerce kurbana saldırı yapabilir, kimliğini gizleyebilir, casusluk yapabilir ve şantaj yapabilir, kolluk kuvvetlerini takip edebilir (çok daha yavaş otomatikleştiriyor), yeni para kazanma planları tasarlayabilir ve personel planlamasını koordine edebilir - eğer herhangi bir personel varsa.

[^30]: Anthropic CEO'su Dario Amodei [makalesinde](https://darioamodei.com/machines-of-loving-grace) "\[bir milyon\] dahinin Ülkesi"ni akla getirmişti.

## Bölüm 5 - Eşikte

Günümüzün yapay zeka sistemlerinden tam teşekküllü YGZ'ye giden yol şaşırtıcı derecede kısa ve öngörülebilir görünüyor.

Son on yıl, muazzam [hesaplama](https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year), insan ve [mali](https://arxiv.org/abs/2405.21015) kaynaklarla yönlendirilen dramatik yapay zeka ilerlemeleri gördü. Birçok dar amaçlı yapay zeka uygulaması, kendilerine verilen görevlerde insanlardan daha iyi performans gösteriyor ve kesinlikle çok daha hızlı ve ucuz.[^31] Ayrıca [Go](https://www.nature.com/articles/nature16961), [Satranç](https://arxiv.org/abs/1712.01815) ve [Poker](https://www.deepstack.ai/) gibi dar alan oyunlarında tüm insanları alt edebilen dar süper insan ajanları ve basitleştirilmiş simüle ortamlarda insanlar kadar etkili şekilde planlama yapıp eylem gerçekleştirebilen daha [genel ajanlar](https://deepmind.google/discover/blog/a-generalist-agent/) da var.

En önemlisi, OpenAI/Microsoft, Google/Deepmind, Anthropic/Amazon, Facebook/Meta, X.ai/Tesla ve diğerlerinden[^32] güncel genel yapay zeka sistemleri 2023 başından itibaren ortaya çıktı ve o zamandan beri istikrarlı (ancak düzensiz) şekilde yeteneklerini artırdı. Bunların hepsi büyük metin ve multimedya veri kümelerinde token tahmini ile, insanlardan ve diğer yapay zeka sistemlerinden gelen kapsamlı pekiştirmeli geri bildirimle birleştirilerek yaratıldı. Bazıları ayrıca kapsamlı araç ve iskele sistemleri içeriyor.

### Mevcut genel sistemlerin güçlü ve zayıf yönleri

Bu sistemler zeka ve uzmanlığı ölçmek için tasarlanan giderek daha geniş bir test yelpazesinde iyi performans gösteriyor ve bu alandaki uzmanları bile şaşırtan bir ilerleme kaydediyor:

- İlk piyasaya sürüldüğünde, GPT-4 SAT, GRE, giriş sınavları ve baro sınavları dahil standart akademik testlerde [tipik insan performansını eşledi veya aştı](https://arxiv.org/abs/2303.08774). Daha yeni modeller muhtemelen önemli ölçüde daha iyi performans gösteriyor, ancak sonuçlar kamuya açık değil.
- Uzun süre "gerçek" yapay zeka için kilit bir ölçüt olarak kabul edilen Turing testi artık modern dil modelleri tarafından hem gayri resmi hem de [resmi çalışmalarda](https://arxiv.org/abs/2405.08007) bazı formlarda rutin olarak geçiliyor.[^33]
- 57 akademik konuyu kapsayan kapsamlı MMLU ölçütünde, [son modeller alan uzmanı düzeyinde skorlar](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu) (∼%90) elde ediyor[^34]
- Teknik uzmanlık dramatik şekilde ilerledi: Lisansüstü düzey fiziğin GPQA ölçütünde [performans](https://epoch.ai/data/ai-benchmarking-dashboard) rastgele tahminine yakın seviyeden (GPT-4, 2022) uzman seviyesine (o1-preview, 2024) sıçradı.
- Özellikle yapay zekaya dayanıklı olacak şekilde tasarlanan testler bile düşüyor: OpenAI'nin O3'ü [söylendiğine göre](https://www.nextbigfuture.com/2024/12/openai-releases-o3-model-with-high-performance-and-high-cost.html) ARC-AGI soyut problem çözme ölçütünü insan seviyesinde çözüyor, en üst uzman kodlama performansı elde ediyor ve seçkin matematikçilere meydan okumak için tasarlanan Epoch AI'nin "sınır matematiği" problemlerinde %25 skor alıyor.[^35]
- Eğilim o kadar net ki MMLU'nun geliştiricisi şimdi ["İnsanlığın Son Sınavı"nı](https://agi.safe.ai/) yarattı – yapay zekanın yakında herhangi bir anlamlı testte insan performansını aşma olasılığını yansıtan uğursuz bir isim. Bu yazının yazıldığı sırada, yapay zeka sistemlerinin bu son derece zor sınavda %27 ([Sam Altman'a göre](https://x.com/sama/status/1886220281565381078)) ve %35 ([bu makaleye göre](https://arxiv.org/abs/2502.09955)) başarı elde ettiği iddiaları var. Herhangi bir bireysel insanın bunu yapabilmesi oldukça düşük bir ihtimal.

Bu etkileyici sayılara (ve onlarla etkileşime girdiğinizde ortaya çıkan bariz zekalarına)[^36] rağmen, bu sinir ağlarının (en azından piyasaya sürülen versiyonları) yapamadığı birçok şey var. Şu anda çoğu bedensiz – sadece sunucularda var olan – ve en fazla metin, ses ve sabit görüntüleri işliyor (ancak video değil). Kritik olarak, çoğu yüksek doğruluk gerektiren karmaşık planlı faaliyetleri gerçekleştiremiyor.[^37] Ve üst düzey insan bilişinde güçlü olan ancak piyasaya sürülen yapay zeka sistemlerinde şu anda düşük olan bir dizi başka nitelik var.

Aşağıdaki tablo, GPT-4o, Claude 3.5 Sonnet ve Google Gemini 1.5 gibi 2024 ortası yapay zeka sistemlerine dayalı olarak bunlardan bir dizi tanesini listeliyor.[^38] Genel yapay zekanın ne kadar hızla daha güçlü hale geleceği için kilit soru şu: sadece *aynı şeyin daha fazlasını* yapmanın sonuç üretme derecesi, ek ama *bilinen* teknikleri eklemeye karşı, *gerçekten yeni* yapay zeka araştırma yönleri geliştirme veya uygulama. Bunlar için kendi tahminlerim tabloda, bu senaryoların her birinin o yeteneği insan seviyesine ve ötesine taşıma olasılığı açısından veriliyor.

<table><tbody><tr><th>Yetenek</th><th>Yetenek açıklaması</th><th>Durum/prognoz</th><th>Ölçekleme/bilinen/yeni</th></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td colspan="4"><em>Temel Bilişsel Yetenekler</em></td></tr><tr><td>Mantık yürütme</td><td>İnsanlar doğru, çok adımlı mantık yürütme, kural izleme ve doğruluk kontrolü yapabilir.</td><td>Genişletilmiş düşünce zinciri ve yeniden eğitim kullanarak dramatik son gelişmeler</td><td>95/5/5</td></tr><tr><td>Planlama</td><td>İnsanlar uzun vadeli ve hiyerarşik planlama sergiler.</td><td>Ölçekle gelişiyor; iskele ve daha iyi eğitim teknikleri kullanılarak güçlü şekilde desteklenebilir.</td><td>10/85/5</td></tr><tr><td>Gerçek-temellendirme</td><td>GYYZ'ler sorguları karşılamak için temelsiz bilgileri uydururlar.</td><td>Ölçekle gelişiyor; kalibrasyon verisi model içinde mevcut; iskele yoluyla kontrol edilip geliştirilebilir.</td><td>30/65/5</td></tr><tr><td>Esnek problem çözme</td><td>İnsanlar yeni kalıpları tanıyabilir ve karmaşık problemlere yeni çözümler icat edebilir; mevcut ML modelleri zorlanır.</td><td>Ölçekle ama zayıf şekilde gelişir; nörosembolik veya genelleştirilmiş "arama" teknikleriyle çözülebilir.</td><td>15/75/10</td></tr><tr><td colspan="4"><em>Öğrenme ve Bilgi</em></td></tr><tr><td>Öğrenme ve hafıza</td><td>İnsanlarda çalışan, kısa vadeli ve uzun vadeli hafıza var, hepsi dinamik ve birbiriyle ilişkili.</td><td>Tüm modeller eğitim sırasında öğrenir; GYYZ'ler bağlam penceresi içinde ve ince ayar sırasında öğrenir; "sürekli öğrenme" ve diğer teknikler mevcut ancak henüz büyük GYYZ'lere entegre edilmedi.</td><td>5/80/15</td></tr><tr><td>Soyutlama ve özyineleme</td><td>İnsanlar ilişki kümelerini daha soyut olanlara mantık yürütme ve manipülasyon için, özyinelemeli "meta" mantık yürütme dahil, eşleyebilir ve aktarabilir.</td><td>Ölçekle zayıf şekilde gelişiyor; nörosembolik sistemlerde ortaya çıkabilir.</td><td>30/50/20</td></tr><tr><td>Dünya modeli(leri)</td><td>İnsanların problem çözebilecekleri ve fiziksel mantık yürütme yapabilecekleri sürekli güncelledikleri tahmin edici bir dünya modeli var</td><td>Ölçekle gelişiyor; güncelleme öğrenmeye bağlı; GYYZ'ler gerçek dünya tahmininde zayıf.</td><td>20/50/30</td></tr><tr><td colspan="4"><em>Benlik ve Faillik</em></td></tr><tr><td>Faillik</td><td>İnsanlar planlama/tahmin temelinde amaçları takip etmek için eylemler alabilir.</td><td>Birçok ML sistemi faillik gösterir; LLM'ler sarmalayıcılar yoluyla ajan haline getirilebilir.</td><td>5/90/5</td></tr><tr><td>Öz-yönlendirme</td><td>İnsanlar içsel olarak üretilen motivasyon ve itici güçle kendi amaçlarını geliştirir ve takip eder.</td><td>Büyük ölçüde faillik artı yaratıcılıktan oluşur; soyut amaçları olan karmaşık faillik sistemi sistemlerde ortaya çıkması muhtemel.</td><td>40/45/15</td></tr><tr><td>Öz-referans</td><td>İnsanlar kendilerini bir ortam/bağlam içinde konumlanmış olarak anlayıp hakkında mantık yürütür.</td><td>Ölçekle gelişiyor ve eğitim ödülüyle artırılabilir.</td><td>70/15/15</td></tr><tr><td>Öz-farkındalık</td><td>İnsanlar kendi düşünceleri ve zihinsel durumları hakkında bilgiye sahip ve bunlar hakkında mantık yürütebilir.</td><td>GYYZ'lerde bir anlamda mevcut, ki bunlar tartışmalı olarak öz-farkındalık için klasik "ayna testini" geçebilir. İskele ile geliştirilebilir; ancak bunun yeterli olup olmadığı belirsiz.</td><td>20/55/25</td></tr><tr><td colspan="4"><em>Arayüz ve Ortam</em></td></tr><tr><td>Bedenlı zeka</td><td>İnsanlar gerçek dünya ortamlarını anlar ve onlarla aktif olarak etkileşime girer.</td><td>Pekiştirmeli öğrenme simüle ve gerçek dünya (robotik) ortamlarında iyi çalışır ve çok modal transformerlere entegre edilebilir.</td><td>5/85/10</td></tr><tr><td>Çoklu-duyu işleme</td><td>İnsanlar görsel, işitsel ve diğer duyusal akışları entegre eder ve gerçek zamanlı işler.</td><td>Çoklu modalitelerde eğitim "işe yarıyor" gibi görünüyor ve ölçekle gelişiyor. Gerçek zamanlı video işleme zor ancak örneğin kendi kendine sürüş sistemleri hızla gelişiyor.</td><td>30/60/10</td></tr><tr><td colspan="4"><em>Üst Düzey Yetenekler</em></td></tr><tr><td>Yaratıcılık</td><td>Mevcut ML modelleri mevcut fikirleri/eserleri dönüştürme ve birleştirmede yaratıcı, ancak insanlar bazen kimliklerine bağlı yeni çerçeveler ve yapılar inşa edebilir.</td><td>"Yaratıcılık"tan ayırt etmek zor olabilir, ki bu ona ölçeklenebilir; yaratıcılık artı öz-farkındalıktan ortaya çıkabilir.</td><td>50/40/10</td></tr><tr><td>Bilinç</td><td>İnsanlar qualia yaşar; bunlar olumlu, olumsuz veya nötr değerlik olabilir; bir insan olmak "bir şey gibi"dir.</td><td>Belirli bir sistemin buna sahip olup olmadığını belirlemek çok zor ve felsefi açıdan sorunlu.</td><td>5/10/85</td></tr></tbody></table>

Modern GYYZ sistemlerinde şu anda insan uzman seviyesinin altında olan kilit yetenekler, türlerine göre gruplandırılmış. Üçüncü sütun mevcut durumu özetliyor. Son sütun, insan seviyesi performansın şunlar yoluyla elde edilme tahmin edilen olasılığını (%) gösteriyor: mevcut teknikleri ölçekleme / bilinen tekniklerle birleştirme / yeni teknikler geliştirme. Bu yetenekler bağımsız değil ve herhangi birindeki artış tipik olarak diğerlerindeki artışlarla birlikte gidiyor. Hepsinin (özellikle bilinç) yapay zeka geliştirmeyi ilerletebilecek yapay zeka sistemleri için gerekli olmadığını, güçlü ama bilinçsiz yapay zeka olasılığını vurguladığını not edin.

"Eksik olan"ı bu şekilde parçalara ayırmak, mevcut veya bilinen teknikleri ölçekleyerek genel olarak insan üstü zekaya giden yolda oldukça ilerlediğimizi oldukça net hale getiriyor.[^39]

Yine de sürprizler olabilir. "Bilinç"i bir kenara bırakarak bile, listelenen temel bilişsel yeteneklerden bazıları gerçekten mevcut tekniklerle yapılamayacak ve yenilerini gerektirecek olabilir. Ama şunu düşünün. Dünyanın en büyük şirketlerinin çoğu tarafından şu anda gösterilen çaba Apollo projesinin birkaç katı ve Manhattan projesinin on katı harcamaya[^40] tekabül ediyor ve duyulmamış maaşlarla binlerce en üst düzey teknik kişiyi istihdam ediyor. Son birkaç yılın dinamikleri şimdi buna (yapay zeka da ekleniyor) tarihteki herhangi bir girişimden daha fazla insan entelektüel gücü getirdi. Başarısızlığa bahse girmememeliyiz.

### Büyük hedef: genelci otonom ajanlar

Son birkaç yıldaki genel yapay zeka geliştirmesi, genel ve güçlü ama araç benzeri yapay zeka yaratmaya odaklandı: öncelikle (oldukça) sadık bir asistan olarak işlev görüyor ve genellikle kendi başına eylem almıyor. Bu kısmen tasarım gereği, ama büyük ölçüde bu sistemler karmaşık eylemlerle güvenilecek kadar ilgili becerilerde yeterli olmadıkları için.[^41]

Ancak yapay zeka şirketleri ve araştırmacılar giderek artan şekilde *otonom* uzman seviyesi genel amaçlı ajanlara [odak kayıyor](https://www.axios.com/2025/01/23/davos-2025-ai-agents).[^42] Bu, sistemlerin kullanıcının gerçek eylemleri devredebileceği bir insan asistanı gibi davranmasına izin verecek.[^43] Bu ne gerektirecek? "Eksik olan" tablosundaki yeteneklerin bir dizi tanesi bunun içinde, güçlü gerçek-temellendirme, öğrenme ve hafıza, soyutlama ve özyineleme, ve dünya modelleme (zeka için), planlama, faillik, yaratıcılık, öz-yönlendirme, öz-referans ve öz-farkındalık (otonomi için), ve çoklu-duyu işleme, bedenli zeka, ve esnek problem çözme (genellik için) dahil.[^44]

Yüksek otonomi (eylem bağımsızlığı), yüksek genellik (kapsam ve görev genişliği) ve yüksek zeka (bilişsel görevlerde yeterlik) bu üçlü kesişimi şu anda insanlara özgü. Bu, hem değeri hem de riskleri açısından birçok kişinin YGZ'yi düşündüğünde aklında olan şey. 

Bu, Y-G-Z'yi ***Y*** otonom- ***G*** enel- ***Z*** eka olarak tanımlamanın başka bir yolunu sağlıyor ve bu üçlü kesişimin yüksek yetenek sistemleri için hem risklerini ve ödüllerini anlamada hem de yapay zeka yönetişiminde çok değerli bir lens sağladığını göreceğiz.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Simple-1024x1024.png&w=3840&q=75) Dönüştürücü Y-G-Z güç ve risk bölgesi üç kilit özelliğin kesişiminden ortaya çıkıyor: yüksek Otonomi, görevlerde yüksek Zeka ve yüksek Genellik.

### Yapay zeka (öz-)geliştirme döngüsü

Yapay zeka ilerlemesini anlamada son kritik faktör, yapay zekanın eşsiz teknolojik geri bildirim döngüsü. Yapay zeka geliştirmede, başarı – hem gösterilen sistemlerde hem dağıtılan ürünlerde – ek yatırım, yetenek ve rekabet getiriyor ve şu anda yüz milyarlarca, hatta trilyonlarca dolar yatırım yönlendiren muazzam bir yapay zeka hype-artı-gerçeklik geri bildirim döngüsünün ortasındayız.

Bu tür geri bildirim döngüsü herhangi bir teknolojide olabilir ve pazar başarısının yatırımı, bunun da iyileşmeyi ve daha iyi pazar başarısını getirdiği birçokta gördük. Ancak yapay zeka geliştirme daha da ileri gidiyor, çünkü şimdi yapay zeka sistemleri yeni ve daha güçlü yapay zeka sistemleri geliştirmeye yardım ediyor.[^45] Bu geri bildirim döngüsünü, tabloda gösterildiği gibi, her biri sonrakinden daha kısa zaman ölçeğine sahip beş aşamada düşünebiliriz.

*Yapay zeka geliştirme döngüsü birden fazla zaman ölçeğinde işliyor, her aşama potansiyel olarak sonraki aşamaları hızlandırıyor. Önceki aşamalar iyice yolunda, son aşamalar spekülatif kalıyor ancak açıldıktan sonra çok hızla ilerleyebilir.*

Bu aşamaların birkaç tanesi zaten devam ediyor ve birkaçı açıkça başlıyor. Yapay zeka sistemlerinin kendilerini otonomik olarak geliştirdiği son aşama, çok güçlü yapay zeka sistemlerinin riski üzerine literatürün temel taşı oldu ve bunun iyi nedeni var.[^46] Ancak bunun, zaten başlamış ve teknolojinin hızla ilerlemesinde daha fazla sürprize yol açabilecek bir geri bildirim döngüsünün sadece en dramatik formu olduğunu belirtmek önemli.


[^31]: Muhtemelen düşündüğünüzden çok daha fazla bu yapay zekayı kullanıyorsunuz, konuşma üretimi ve tanıma, görüntü işleme, haber akışı algoritmaları vb. yönlendiriyor.

[^32]: Bu şirket çiftleri arasındaki ilişkiler oldukça karmaşık ve incelikli olsa da, onları hem şimdi yapay zeka geliştirmesine dahil firmaların büyük toplam piyasa değerini belirtmek hem de Anthropic gibi "daha küçük" şirketlerin arkasında bile yatırımlar ve büyük ortaklık anlaşmaları yoluyla son derece derin ceplerin durduğunu göstermek için açıkça listeledim.

[^33]: Turing testini küçümsemek moda oldu, ancak oldukça güçlü ve genel. Zayıf versiyonlarında, bir yapay zeka (insanmış gibi davranmaya eğitilmiş) ile kısa süreler boyunca tipik şekillerde etkileşime giren tipik insanların onun bir yapay zeka olup olmadığını söyleyip söyleyemeyeceğini belirtir. Söyleyemezler. İkinci olarak, son derece düşmanca bir Turing testi temel olarak insan yeteneği ve zekasının herhangi bir unsurunu araştırabilir – örneğin bir yapay zeka sistemini diğer insan uzmanlar tarafından değerlendirilen bir insan uzmanla karşılaştırarak. Yapay zeka değerlendirmesinin çoğunun genelleştirilmiş bir Turing testi formu olduğu bir anlam var.

[^34]: Bu alan başına – hiçbir insan makul şekilde tüm konularda aynı anda böyle skorlar elde edemez.

[^35]: Bunlar mükemmel matematikçilerin bile çözebilseler önemli zaman alacağı problemler.

[^36]: Şüpheci bir eğilimde iseniz, şüpheciliğinizi koruyun ancak en güncel modelleri gerçekten bir turda deneyin, ayrıca geçebildikleri test sorularının bazılarını kendiniz deneyin. Bir fizik profesörü olarak, örneğin en iyi modellerin bölümümüzdeki lisansüstü yeterlik sınavını geçeceğini neredeyse kesinlikle tahmin ederim.

[^37]: Bu ve uydurma gibi diğer zayıflıklar pazar benimsenmesini yavaşlattı ve algılanan ve iddia edilen yetenekler (ki bunlar da yoğun pazar rekabeti ve yatırım çekme ihtiyacı merceğinden görülmeli) arasında boşluğa yol açtı. Bu hem halkı hem politika yapıcıları yapay zeka ilerlemesinin gerçek durumu konusunda şaşırttı. Belki hype'a uymasa da, ilerleme çok gerçek.

[^38]: O zamandan beri büyük ilerleme, çıkarım sırasında daha fazla hesaplama ve daha büyük pekiştirmeli öğrenme kullanarak üst kalite mantık yürütme için eğitilmiş sistemlerin geliştirilmesi oldu. Bu modeller yeni olduğu ve yetenekleri daha az test edildiği için, esasen çözülmüş olarak gördüğüm "mantık yürütme" dışında bu tabloyu tamamen yenilemedi. Ancak o sistemlerin deneyimlenen ve bildirilen yetenekleri temelinde tahminleri güncelledim.

[^39]: 1960'larda ve 1980'lerdeki önceki yapay zeka iyimserliği dalgaları, vaat edilen yetenekler gerçekleşemediğinde "yapay zeka kışları"yla sona erdi. Ancak mevcut dalga birçok alanda süper insan performansı elde etmesi, muazzam hesaplama kaynakları ve ticari başarıyla desteklenmesiyle temelden farklılaşıyor.

[^40]: Tam Apollo projesi [2020 dolarıyla yaklaşık 250 milyar USD'ye mal oldu](https://www.planetary.org/space-policy/cost-of-apollo) ve Manhattan projesi [bunun onda birinden az](https://www.brookings.edu/the-costs-of-the-manhattan-project/). Goldman Sachs [sadece yapay zeka veri merkezleri için trilyonlarca dolar harcama](https://www.datacenterdynamics.com/en/news/goldman-sachs-1tn-to-be-spent-on-ai-data-centers-chips-and-utility-upgrades-with-little-to-show-for-it-so-far/) öngörüyor.

[^41]: İnsanlar bol hata yapsa da, ne kadar güvenilir olabileceğimizi hafife alıyoruz! Olasılıklar çarpıldığı için, doğru yapılması için 20 adım gerektiren bir görev, yarı yarıya doğru yapılması için her adımın %97 güvenilir olmasını gerektirir. Böyle görevleri sürekli yapıyoruz.

[^42]: Bu yönde güçlü bir hamle çok kısa süre önce OpenAI'nin internette karmaşık görevler için çok adımlı araştırma yapan otonom genel araştırmayı "karmaşık görevler için internette çok adımlı araştırma yapan yeni bir ajantik yetenek" olarak tarif ettiği ["Derin Araştırma"](https://openai.com/index/introducing-deep-research/) asistanıyla atıldı.

[^43]: O sinir bozucu PDF formunu doldurma, uçak rezervasyonu yapma gibi şeyler. Ama 20 alanda doktorasıyla! Yani ayrıca: o tezi sizin için yazma, o sözleşmeyi sizin için müzakere etme, o teoremi sizin için kanıtlama, o reklam kampanyasını sizin için yaratma vb. Siz ne yaparsınız? Ona ne yapacağını söylersiniz tabii.

[^44]: Bilinç'in açıkça gerekli *olmadığını*, ya da bu üçlü-kesişimdeki yapay zekanın mutlaka onu ima etmediğini not edin.

[^45]: Buradaki en yakın analoji belki çip teknolojisi, geliştirmenin onlarca yıldır Moore yasasını koruduğu, bilgisayar teknolojilerinin insanların çip teknolojisinin bir sonraki neslini tasarlamasına yardım ettiği. Ancak yapay zeka çok daha doğrudan olacak.

[^46]: Yapay zekanın – yakında – kendisini günler veya haftalar zaman ölçeğinde geliştirebildiğinin bir an için içinize sinmesine izin vermek önemli. Ya da daha az. Birisi size bir yapay zeka yeteneğinin kesinlikle çok uzakta olduğunu söylediğinde bunu aklınızda tutun.

## Bölüm 6 - YGZ Yarışı

Hem şirketler hem de ülkeler için YGZ inşa etme yarışının arkasındaki itici güçler nelerdir?

Yapay zekada son dönemdeki hızlı ilerleme, olağanüstü düzeyde bir ilgi ve yatırımın hem sonucu hem de nedeni olmuştur. Bu kısmen AI geliştirmedeki başarılardan kaynaklanıyor, ancak işin içinde daha fazlası var. Dünyanın en büyük şirketlerinden bazıları, hatta ülkeler, neden sadece AI değil, YGZ ve superintellijans inşa etmek için yarışıyor?

### AI araştırmalarını insan düzeyinde AI'ya yönelten etkenler

Yaklaşık son beş yıla kadar AI, büyük ölçüde akademik ve bilimsel bir araştırma problemi olmuş, dolayısıyla büyük oranda merak ve zekayı anlama ile onu yeni bir yapıda yaratma dürtüsü tarafından yönlendirilmiştir.

Bu aşamada, çoğu araştırmacı arasında AI'nın faydalarına ya da tehlikelerine nispeten az dikkat gösteriliyordu. AI'nın neden geliştirilmesi gerektiği sorulduğunda, yaygın bir yanıt, AI'nın yardımcı olabileceği problemleri biraz belirsiz bir şekilde sıralamak olabilirdi: yeni ilaçlar, yeni malzemeler, yeni bilim, daha akıllı süreçler ve genel olarak insanlar için işleri iyileştirme.[^47]

Bunlar takdire şayan hedefler![^48] YGZ'nin -genel olarak AI yerine- bu hedefler için gerekli olup olmadığını sorgulayabilir ve sorgulayacağız, ancak bu hedefler birçok AI araştırmacısının başladığı idealizmı sergiliyor.

Son yarım onyılda ise AI, nispeten saf bir araştırma alanından, büyük ölçüde dünyanın en büyük şirketlerinden bazıları tarafından yönlendirilen bir mühendislik ve ürün alanına dönüştü.[^49] Araştırmacılar hâlâ önemli olmakla birlikte, artık sürecin başında değiller.

### Şirketler neden YGZ inşa etmeye çalışıyor?

Peki dev şirketler (ve daha da fazlası yatırımcılar) neden YGZ inşa etmeye muazzam kaynaklar akıtıyor? Çoğu şirketin oldukça dürüst olduğu iki itici güç var: AI'ı toplum için verimlilik itici gücü, kendileri için de kâr kaynağı olarak görüyorlar. Genel AI doğası gereği genel amaçlı olduğu için, burada büyük bir ödül var: ürün ve hizmetler yaratacak bir sektör seçmek yerine, *hepsini aynı anda* deneyebilirsiniz. Büyük Teknoloji şirketleri dijital mal ve hizmetler üreterek muazzam büyüdüler ve en azından bazı yöneticiler AI'ı kesinlikle bunları iyi sağlamanın bir sonraki adımı olarak görüyor, arama, sosyal medya, dizüstü bilgisayarlar, telefonlar vb. tarafından sağlananlara benzer ancak onları genişleten risk ve faydalarla.

Ama neden YGZ? Bunun çok basit bir cevabı var, çoğu şirket ve yatırımcı bunu alenen tartışmaktan çekiniyor.[^50]

YGZ'nin doğrudan, bire bir, *çalışanları değiştirebilmesi*.

Güçlendirme, yetkilendirme ya da daha üretken kılma değil. Hatta *yerinden etme* bile değil. Bunların hepsi YGZ olmayan sistemlerce yapılabilir ve yapılacak. YGZ, özellikle düşünce işçilerini (ve robotikle birlikte birçok fiziksel işçiyi de) tamamen *değiştirebilen* şeydir. Bu görüşe destek olarak OpenAI'nın [(kamuya açık] YGZ tanımına](https://openai.com/our-structure/) bakmak yeterli: "ekonomik olarak değerli işlerin çoğunda insanları geride bırakan yüksek derecede otonom sistem."

Buradaki ödül (şirketler için!) muazzam. İşgücü maliyetleri, dünya çapındaki ~100 trilyon dolarlık küresel ekonominin önemli bir yüzdesi. Bunun sadece bir kısmı insan emeğinin AI emeğiyle değiştirilmesiyle ele geçirilse bile, bu yıllık trilyonlarca dolarlık gelir demek. AI şirketleri ayrıca kimin ödeme yapacağını da biliyor. Onların gördüğü kadarıyla, siz verimlilik araçları için yılda binlerce dolar ödemeyeceksiniz. Ama bir şirket, sizin emeğinizi değiştirebilirlerse, yılda binlerce dolar *ödeyecektir*.

### Ülkeler neden YGZ'ye yarışmak zorunda hissediyor?

Ülkelerin YGZ'yi takip etme konusundaki beyan edilen motivasyonları ekonomik ve bilimsel liderliğe odaklanıyor. Argüman ikna edici: YGZ bilimsel araştırma, teknolojik gelişim ve ekonomik büyümeyi dramatik bir şekilde hızlandırabilir. Bahse konu olan şeyler göz önüne alındığında, hiçbir büyük gücün geride kalma lüksü olmadığını savunuyorlar.[^51]

Ancak ek ve büyük ölçüde ifade edilmeyen itici güçler de var. Bazı askeri ve ulusal güvenlik liderlerinin kapalı kapılar ardında olağanüstü güçlü ve felaket düzeyinde riskli bir teknolojiyi görüşmek için bir araya geldiğinde, odaklarının "bu risklerden nasıl kaçınırız" değil, "bunu nasıl ilk biz elde ederiz?" olduğundan şüphe yok. Askeri ve istihbarat liderleri YGZ'yi askeri işlerde potansiyel bir devrim olarak görüyor, belki de nükleer silahlardan bu yana en önemlisi. Korku, YGZ'yi ilk geliştiren ülkenin aşılmaz stratejik avantaj elde edebileceği. Bu klasik bir silahlanma yarışı dinamiği yaratıyor.

Bu "YGZ yarışı" düşüncesinin,[^52] ikna edici olmasına rağmen, derinden kusurlu olduğunu göreceğiz. Bunun nedeni yarışmanın tehlikeli ve riskli olması değil -öyle olmasına rağmen- teknolojinin doğasından kaynaklanıyor. İfade edilmeyen varsayım, YGZ'nin diğer teknolojiler gibi onu geliştiren devlet tarafından kontrol edilebilir olması ve en fazlasına sahip olan topluma güç veren bir nimet olmasıdır. Göreceğimiz gibi, muhtemelen ikisi de olmayacak.

### Neden superintellijans?

Şirketler alenen verimliliğe odaklanırken ve ülkeler ekonomik ve teknolojik büyümeye odaklanırken, kasıtlı olarak tam YGZ ve superintellijans peşinde koşanlar için bunlar sadece başlangıç. Gerçekten aklında ne var? Nadiren yüksek sesle söylenmekle birlikte, şunları içeriyor:

1. Birçok ya da tüm hastalıklar için tedaviler;
2. Yaşlanmayı durdurma ve tersine çevirme;
3. Füzyon gibi yeni sürdürülebilir enerji kaynakları;
4. İnsan geliştirmeleri ya da genetik mühendislik yoluyla tasarım organizmalar;
5. Nanoteknoloji ve moleküler üretim;
6. Zihin yüklemeleri;
7. Egzotik fizik ya da uzay teknolojileri;
8. İnsanüstü tavsiye ve karar desteği;
9. İnsanüstü planlama ve koordinasyon.

İlk üçü büyük ölçüde "tek kenarlı" teknolojiler -yani oldukça güçlü net pozitif olma ihtimali yüksek. Hastalıkları tedavi etmeye ya da kişi seçerse daha uzun yaşayabilmeye karşı çıkmak zor. Ve füzyonun olumsuz tarafını zaten elde ettik (nükleer silahlar şeklinde); şimdi olumlu tarafını da elde etmek güzel olurdu. Bu ilk kategorideki soru, bu teknolojileri daha erken elde etmenin riski telafi edip etmediği.

Sonraki dört tanesi açıkça çift kenarlı: hem potansiyel olarak büyük faydaları hem de muazzam riskleri olan dönüştürücü teknolojiler, tıpkı AI gibi. Bunların hepsi yarın bir kara kutudan çıkıp dağıtılsalar, yönetilmesi inanılmaz derecede zor olurdu.[^53]

Son ikisi, superintellijansın sadece teknoloji icat etmek yerine kendisinin bir şeyler yapmasıyla ilgili. Daha kesin olarak, örtmeceleri bir kenara bırakarak, bunlar güçlü AI sistemlerinin insanlara ne yapmaları gerektiğini söylemeyi içeriyor. Tavsiye veren sistem tavsiye edilenden çok daha güçlüyse ve tavsiye edilen kararın temelini anlamlı şekilde anlayamıyorsa (ya da bu sağlansa bile, danışmanın farklı bir karar için benzer şekilde ikna edici bir gerekçe vermeyeceğine güvenemiyorsa) buna "tavsiye" demek samimiyetsizlik.

Bu, yukarıdaki listeden eksik olan önemli bir maddeyi gösteriyor:

10. Güç.

Mevcut insanüstü AI yarışının altında yatanların çoğunun *zeka = güç* düşüncesi olduğu apaçık ortada. Her yarışmacı o gücün en iyi sahibi olmaya ve onu sözde hayırsever nedenlerle kontrolleri dışına çıkmadan ya da ellerinden alınmadan kullanabileceklerine güveniyor.

Yani şirketlerin ve ulusların gerçekten peşinde koştuğu sadece YGZ ve superintellijansın meyveleri değil, bunlara kimin erişebileceğini ve nasıl kullanılacağını kontrol etme gücü. Şirketler kendilerini hissedarlar ve insanlık hizmetinde bu gücün sorumlu koruyucuları olarak görüyor; uluslar kendilerini düşman güçlerin belirleyici avantaj elde etmesini engelleyen gerekli bekçiler olarak görüyor. İkisi de tehlikeli şekilde yanılıyor, superintellijansın doğası gereği herhangi bir insan kurumu tarafından güvenilir şekilde kontrol edilemeyeceğini fark etmede başarısız oluyorlar. Superintellijans sistemlerinin doğası ve dinamiklerinin insan kontrolünü son derece zor, imkansız değilse de, kıldığını göreceğiz.

Bu yarış dinamikleri -hem kurumsal hem de jeopolitik- belirleyici şekilde kesintiye uğratılmadıkça bazı riskleri neredeyse kaçınılmaz kılıyor. Şimdi bu riskleri ve neden rekabetçi[^54] bir geliştirme paradigması içinde yeterince azaltılamayacaklarını incelemeye geçiyoruz.


[^47]: Daha kesin bir değerli hedefler listesi BM [Sürdürülebilir Kalkınma Hedefleri.](https://sdgs.un.org/goals) Bunlar bir anlamda dünyada iyileştirilmesini istediğimiz şeyler için elimizde olan küresel fikir birliği hedeflerine en yakın set. AI yardımcı olabilir.

[^48]: Teknoloji genel olarak binlerce yılın tanıklık ettiği gibi, insan refahı için dönüştürücü ekonomik ve sosyal güçe sahiptir. Bu bağlamda, Anthropic kurucusu Dario Amodei'nin [bu makalesinde](https://darioamodei.com/machines-of-loving-grace) pozitif bir YGZ vizyonunun uzun ve ikna edici bir açıklaması bulunabilir.

[^49]: Özel AI yatırımı [2018-19'da patlamaya başladı, o dönemde kamu yatırımını geçti,](https://cset.georgetown.edu/publication/tracking-ai-investment/) ve o zamandan beri onu büyük ölçüde geride bıraktı.

[^50]: Daha kapalı kapılar ardında böyle bir çekimserlik göstermediklerini kanıtlayabilirim. Ve bu daha kamuya açık hale geliyor; örneğin Y-combinator'ın [yeni "startup istekleri"](https://www.ycombinator.com/rfs)ne bakın, bunun birçok bölümü açıkça insan işçilerin toptan değiştirilmesi çağrısı yapıyor. Onları alıntılamak gerekirse, "B2B SaaS'ın değer önerisi insan işçileri aşamalı olarak daha verimli kılmaktı. Dikey AI ajanlarının değer önerisi işi tamamen otomatikleştirmek...Bu fırsatın başka 100 tek boynuzlu at yaratacak kadar büyük olması tamamen mümkün." (Silicon Valley konuşmasında versiz olanlar için, "B2B" işletmeden işletmeye, tek boynuzlu at ise 1 milyar dolarlık şirket. Yani işletmeler için işçilerin yerini alan yüz milyar artı dolarlık yüzden fazla işletmeden bahsediyorlar.)

[^51]: Örneğin son [ABD-Çin Ekonomi ve Güvenlik İnceleme Komisyonu raporuna](https://www.uscc.gov/sites/default/files/2024-11/2024_Executive_Summary.pdf) bakın. Raporun kendisinde şaşırtıcı derecede az gerekçe olmasına rağmen, en önemli tavsiye ABD Kongresi'nin "Yapay Genel Zeka (YGZ) kabiliyetine yarışmaya ve elde etmeye adanmış Manhattan Projesi benzeri bir program kurması ve fonlaması"ydı.

[^52]: Şirketler şimdi AI geliştirmelerine herhangi bir kısıtlamaya karşı kalkan olarak bu jeopolitik çerçevelemeyi benimsiyor, genellikle açıkça çıkarcı şekillerde ve bazen temel mantık bile içermeyen şekillerde. Meta'nın [Sınır AI Yaklaşımını](https://about.fb.com/news/2025/02/meta-approach-frontier-ai/) düşünün, aynı anda Amerika'nın "teknolojik inovasyon, ekonomik büyüme ve ulusal güvenlikte lider pozisyonunu sağlamlaştırması" gerektiğini ve bunu da en güçlü AI sistemlerini açıkça yayınlayarak yapması gerektiğini savunuyor -ki bu onları doğrudan jeopolitik rakiplerini ve düşmanlarına vermeyi içeriyor.

[^53]: Dolayısıyla bu teknolojilerin yönetimini büyük ihtimalle AI'lara bırakmamız gerekecek. Ama bu çok sorunlu bir kontrol devri olurdu, buna aşağıda döneceğiz.

[^54]: Teknoloji geliştirmede rekabet genellikle önemli faydalar getirir: tekelci kontrolü engelleme, inovasyonu ve maliyet düşürmeyi teşvik etme, çeşitli yaklaşımları mümkün kılma ve karşılıklı gözetim yaratma. Ancak YGZ ile bu faydalar yarış dinamikleri ve güvenlik önlemlerini azaltma baskısından gelen benzersiz risklere karşı tartılmalı.

## Bölüm 7 - Mevcut yolumuzda YGZ inşa edersek ne olur?

Toplum YGZ seviyesindeki sistemlere hazır değil. Bunları çok yakında inşa edersek, işler çirkinleşebilir.

Tam yapay genel zekanın geliştirilmesi - burada "Kapıların dışında" kalan yapay zeka olarak adlandıracağımız şey - dünyanın doğasında temel bir değişim olacaktır: özü gereği, insanlarınkinden daha büyük yeteneklere sahip yeni bir zeka türünün Dünya'ya eklenmesi anlamına gelir.

O zaman ne olacağı teknolojinin doğası, onu geliştirenlerin tercihleri ve geliştirildiği dünya bağlamı dahil birçok şeye bağlıdır.

Şu anda, tam YGZ birbirleriye yarış halindeki bir avuç büyük özel şirket tarafından, anlamlı bir düzenleme veya dış gözetim olmaksızın,[^55] giderek zayıflayan ve hatta işlevsiz hale gelen temel kurumları olan bir toplumda,[^56] yüksek jeopolitik gerginlik ve düşük uluslararası koordinasyon zamanında geliştirilmektedir. Bazıları özgecil motivasyonlarla hareket etse de, bunu yapanların çoğu para, güç ya da her ikisi tarafından yönlendirilmektedir.

Tahmin yapmak çok zordur, ancak yeterince iyi anlaşılan bazı dinamikler ve önceki teknolojilerle yeterince uygun benzetmeler rehberlik sağlayabilir. Ve ne yazık ki, yapay zekanın vaadine rağmen, mevcut rotamızın nasıl sonuçlanacağı konusunda derinden kötümser olmak için güçlü nedenler sunuyorlar.

Açık söylemek gerekirse, şu anki rotamızda YGZ geliştirmenin bazı olumlu etkileri olacak (ve bazı insanları çok, çok zengin edecek). Ancak teknolojinin doğası, temel dinamikler ve geliştirildiği bağlam şunu güçlü bir şekilde gösteriyor: güçlü yapay zeka toplumumuzu ve medeniyetimizi dramatik olarak sarsacak; onun kontrolünü kaybedeceğiz; onun yüzünden dünya savaşında bulabiliriz kendimizi; kontrolü ona kaybedeceğiz (ya da vereceğiz); yapay superintellijansa yol açacak ve bunu kesinlikle kontrol edemeyeceğiz, bu da insan yönetimindeki dünyanın sonu anlamına gelecek.

Bunlar güçlü iddialar ve keşke boş spekülasyon ya da asılsız "karamsar"lık olsalardı. Ancak bilim, oyun teorisi, evrim teorisi ve tarih hepsi bu yöne işaret ediyor. Bu bölüm bu iddiaları ve bunlara destek olan kanıtları ayrıntılı olarak geliştiriyor.

### Toplumumuzu ve medeniyetimizi sarsacağız

Silikon Vadisi toplantı odalarında duyduklarınızın aksine, çoğu yıkım - özellikle çok hızlı olanlar - faydalı değildir. Karmaşık sistemleri daha kötü yapmanın daha iyi yapmaktan çok daha fazla yolu vardır. Dünyamız şu andaki kadar iyi işlemesin çünkü onu istikrarlı şekilde daha iyi hale getiren süreçler, teknolojiler ve kurumlar özenle inşa ettik.[^57] Bir fabrikaya balyoz vurmak nadiren operasyonları iyileştirir.

İşte YGZ sistemlerinin medeniyetimizi nasıl sarsacağına dair (eksik) bir katalog.

- İşgücünü dramatik olarak bozacak, *en azından* dramatik şekilde artan gelir eşitsizliğine ve potansiyel olarak toplumun uyum sağlayamayacağı kadar kısa bir zaman diliminde büyük ölçekli eksik istihdam veya işsizliğe yol açacaklar.[^58]
- Muhtemelen büyük miktarda ekonomik, sosyal ve politik gücün - potansiyel olarak ulus devletlerinkinden fazlasının - halka karşı sorumlu olmayan az sayıda büyük özel çıkarda toplanmasına yol açacaklar.
- Daha önce zor veya pahalı olan faaliyetleri aniden önemsiz derecede kolay hale getirebilir, belirli faaliyetlerin maliyetli kalmasına veya önemli insan çabasını gerektirmesine bağlı sosyal sistemleri istikrarsızlaştırabilirler.[^59]
- Toplumun bilgi toplama, işleme ve iletişim sistemlerini tamamen gerçekçi ancak yanlış, spam, aşırı hedeflenmiş veya manipülatif medyayla o kadar iyice doldurabilirler ki, neyin fiziksel olarak gerçek olup olmadığını, insan olup olmadığını, olgusal olup olmadığını ve güvenilir olup olmadığını anlamak imkansız hale gelebilir.[^60]
- Tam olarak anlayamadığımız yapay zeka sistemlerine giderek daha fazla bel bağladığımız için insan anlayışının temel sistem ve teknolojilerde körelmesiyle tehlikeli ve neredeyse tam entelektüel bağımlılık yaratabilirler.
- Çoğu insan tarafından tüketilen neredeyse tüm kültürel nesneler (metin, müzik, görsel sanat, film vb.) insani olmayan zihinler tarafından yaratıldığı, aracılık edildiği veya düzenlendiği anda insan kültürünü etkin şekilde sona erdirebilirler.
- Hükümetler veya özel çıkarlar tarafından bir halkı kontrol etmek ve kamu yararına aykırı hedefleri sürdürmek için kullanılabilir etkili kitle gözetim ve manipülasyon sistemleri sağlayabilirler.
- İnsan söylemini, tartışmasını ve seçim sistemlerini baltalayarak, demokratik kurumların güvenilirliğini bunların etkin (veya açıkça) başkalarıyla değiştirildiği noktaya kadar azaltabilir, şu anda var olduğu devletlerde demokrasiyi sona erdirebilirler.
- Gelişmiş, kendi kendini çoğaltan akıllı yazılım virüsleri ve solucanları haline gelebilir veya bunları yaratabilirler, bunlar yayılıp evrimleşerek küresel bilgi sistemlerini büyük ölçüde bozabilir.
- Teröristlerin, kötü aktörlerin ve haydut devletlerin biyolojik, kimyasal, siber, otonom veya diğer silahlarla zarar verme kabiliyetini dramatik olarak artırabilirler, yapay zekanın böyle bir zararı önlemek için dengeleyici bir yetenek sağlamamasıyla birlikte. Benzer şekilde, aksi halde sahip olmayacakları rejimlere üst düzey nükleer, biyoloji, mühendislik ve diğer uzmanlığı sunarak ulusal güvenliği ve jeopolitik dengeleri baltalarlar.
- Büyük ölçüde elektronik finansal, satış ve hizmet alanlarında rekabet eden etkin şekilde yapay zeka işletmeli şirketlerle hızlı büyük ölçekli kaçak hiper-kapitalizme neden olabilirler. Yapay zeka güdümlü finansal piyasalar insan anlayışı veya kontrolünü çok aşan hızlarda ve karmaşıklıklarda işleyebilir. Mevcut kapitalist ekonomilerin tüm başarısızlık modları ve negatif dışsallıkları, insan kontrolü, yönetişimi veya düzenleyici kabiliyetinin çok ötesine hızlandırılıp şiddetlendirilebilir.
- Yapay zeka destekli silahlar, komuta-kontrol sistemleri, siber silahlar vb. konusunda ülkeler arası silahlanma yarışını körükleyebilir, son derece yıkıcı yeteneklerin çok hızlı bir şekilde birikmesine neden olabilirler.

Bu riskler spekülatif değildir. Bunların çoğu şu anda konuştuğumuz sırada, mevcut yapay zeka sistemleri aracılığıyla gerçekleşmektedir! Ancak düşünün, *gerçekten* düşünün, her birinin dramatik olarak daha güçlü yapay zeka ile nasıl görüneceğini.

Çoğu çalışan uzmanlık alanında veya deneyiminde - hatta yeniden eğitim göseler bile - yapay zekanın sağlayabileceğinin ötesinde önemli ekonomik değer sağlayamadığında işgücü yerinden edilmesini düşünün! Herkesin kendisinden daha hızlı ve zeki bir şey tarafından bireysel olarak izlenip denetlendiği kitle gözetimini düşünün. Gördüğümüz, duyduğumuz veya okuduğumuz hiçbir dijital bilgiye güvenilir bir şekilde güvenemediğimizde ve en ikna edici kamu seslerinin bile insan olmadığında ve sonuçta hiçbir çıkarları bulunmadığında demokrasi nasıl görünür? Generallar yapay zekaya sürekli ertelemek zorunda kaldığında (veya basitçe onu sorumlu tuttuğunda) savaş ne haline gelir, yoksa düşmana belirleyici avantaj vermiş olurlar? Yukarıdaki risklerin herhangi biri tam olarak gerçekleştiğinde insan[^61] medeniyeti için bir felaket temsil eder.

Kendi tahminlerinizi yapabilirsiniz. Her risk için kendinize şu üç soruyu sorun:

1. Süper yetenekli, oldukça otonom ve çok genel yapay zeka bunu aksi halde mümkün olmayacak bir şekilde veya ölçekte mümkün kılar mı?
2. Bunun gerçekleşmesine neden olan şeylerden fayda sağlayacak taraflar var mı?
3. Bunun gerçekleşmesini etkili şekilde önleyecek sistemler ve kurumlar mevcut mu?

Cevaplarınızın "evet, evet, hayır" olduğu yerlerde büyük bir sorunumuz olduğunu görebilirsiniz.

Bunları yönetmek için planımız nedir? Şu anda genel olarak yapay zeka ile ilgili olarak masada iki tane var.

İlki, sistemlerin yapmaması gereken şeyleri yapmalarını önlemek için sistemlere güvenlik önlemleri inşa etmektir. Bu şu anda yapılıyor: ticari yapay zeka sistemleri örneğin bomba yapmaya yardım etmeyi veya nefret söylemi yazmayı reddedeceklerdir.

Bu plan Kapı dışındaki sistemler için son derece yetersizdir.[^62] Yapay zekanın kötü aktörlere açıkça tehlikeli yardım sağlaması riskini azaltmaya yardımcı olabilir. Ancak işgücü bozulması, gücün toplanması, kaçak hiper-kapitalizm veya insan kültürünün yerini alması gibi şeyleri önlemek için hiçbir şey yapmayacaktır: bunlar sadece sağlayıcılarına kâr getiren izin verilen yöntemlerle sistemleri kullanmanın sonuçlarıdır! Ve hükümetler askeri veya gözetim kullanımı için sistemlere erişim elde edeceklerdir.

İkinci plan daha da kötüdür: çok güçlü yapay zeka sistemlerini herkesin istediği gibi kullanması için açıkça serbest bırakmak[^63] ve en iyisini ümit etmek.

Her iki plana da örtülü olarak şu dahildir: başka birinin, örneğin hükümetlerin, teknolojileri yönetmek için genel olarak kullandığımız yumuşak veya sert hukuk, standartlar, düzenlemeler, normlar ve diğer mekanizmalar aracılığıyla sorunları çözmeye yardım edeceği.[^64] Ancak yapay zeka şirketlerinin şimdiden herhangi bir önemli düzenleme veya dışarıdan dayatılan sınırlamalara karşı diş ve tırnak mücadele ettiklerini bir kenara bırakırsak, bu risklerin bir kısmı için hangi düzenlemenin gerçekten yardımcı olacağını görmek oldukça zordur. Düzenlemeler yapay zeka üzerinde güvenlik standartları dayatabilir. Ancak şirketlerin çalışanları toptan yapay zeka ile değiştirmesini önler mi? İnsanların yapay zekanın şirketlerini kendileri için işletmesine izin vermesini yasaklar mı? Hükümetlerin güçlü yapay zekayı gözetim ve silahlarda kullanmasını önler mi? Bu konular temeldir. İnsanlık potansiyel olarak bunlara uyum sağlamanın yollarını bulabilir, ancak yalnızca *çok* daha fazla zamanla. Yapay zekanın, onu yönetmeye çalışan insanların yeteneklerine ulaştığı veya bunları aştığı hız göz önüne alındığında, bu sorunlar giderek çözümsüz görünmektedir.

### (En azından bazı) YGZ sistemlerinin kontrolünü kaybedeceğiz

Çoğu teknoloji yapısı gereği oldukça kontrol edilebilirdir. Arabanız veya toster makineniz yapmansını istemediğiniz bir şey yapmaya başlarsa, bu sadece bir arızadır, toster olarak doğasının bir parçası değildir. Yapay zeka farklıdır: tasarlanmaktan ziyade *yetiştirilir*, temel işleyişi opaktır ve doğası gereği öngörülemezdir.

Bu kontrol kaybı teorik değildir - şimdiden erken versiyonlarını görüyoruz. İlk olarak sıradan ve tartışmalı olarak zararsız bir örneği düşünelim. ChatGPT'den zehir karıştırmanıza veya ırkçı bir tirat yazmanıza yardım etmesini isterseniz, reddedecektir. Bu tartışmalı olarak iyidir. Ancak bu aynı zamanda ChatGPT'nin *açıkça yapmasını istediğiniz şeyi yapmamması*dır. Diğer yazılım parçaları bunu yapmaz. Aynı model bir OpenAI çalışanının isteği üzerine de zehir tasarlamayacaktır.[^65] Bu, gelecekteki daha güçlü yapay zekanın kontrolden çıkmasının nasıl olacağını hayal etmeyi çok kolaylaştırır. Çoğu durumda, basitçe istediğimizi yapmayacaklar! Ya verilen bir insanüstü YGZ sistemi bir insan komuta sistemine kesinlikle itaatkar ve sadık olacaktır, ya da olmayacaktır. Olmazsa, *bizim açık komutlarımıza aykırı olsa da bizim için iyi olduğuna inandığı şeyleri yapacaktır.* Bu kontrol altında olan bir şey değildir. Ancak, diyebilirsiniz, bu kasıtlı - bu reddetmeler tasarım gereğidir, sistemleri insan değerleriyle "hizalama"nın bir parçasıdır. Ve bu doğrudur. Ancak hizalama "programının"[^66] kendisinin iki büyük sorunu vardır.

İlk olarak, derin bir düzeyde bunu nasıl yapacağımız hakkında hiçbir fikrimiz yok. Bir yapay zeka sisteminin istediğimiz şeyi "önemsemesini" nasıl garanti ederiz? Yapay zeka sistemlerini geri bildirim sağlayarak şeyler söylemesi ve söylememesi konusunda eğitebiliriz; ve tıpkı diğer şeyler hakkında mantık yürüttükleri gibi insanların ne istediği ve neyi önemsediği hakkında öğrenebilir ve mantık yürütebilirler. Ancak onların insanların önemsediği şeyleri derinden ve güvenilir bir şekilde değerlendirmelerine neden olmak için - teorik olarak bile - hiçbir yöntemimiz yok. Neyin doğru ve yanlış olduğu sayıldığını ve nasıl davranmaları gerektiğini bilen yüksek işlevli insan psikopat vardır. Sadece *önemsemazler*. Ancak amaçlarına hizmet ediyorsa önemsiyormuş gibi *davranabilirler*. Tıpkı bir psikopatı (veya başka herhangi birini) gerçekten, tamamen sadık veya başka biri ya da bir şeyle hizalanmış birine nasıl dönüştüreceğimizi bilmediğimiz gibi, kendilerini dünyadaki ajanlar olarak modelleyebilecek ve potansiyel olarak [kendi eğitimlerini manipüle edebilecek](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) ve [insanları aldatabilecek](https://arxiv.org/abs/2311.08379) kadar gelişmiş sistemlerde hizalama problemini nasıl çözeceğimiz konusunda *hiçbir fikrimiz yok*.[^67] YGZ'yi tam itaatkar yapmak ya da insanları derinden önemsemesini sağlamak imkansız veya ulaşılamaz olduğu kanıtlanırsa, o zaman yapabildiği anda (ve bundan kurtulabileceğine inandığında) istemediğimiz şeyleri yapmaya başlayacaktır.[^68]

İkincisi, doğası gereği gelişmiş yapay zeka sistemlerinin insan çıkarlarına aykırı hedefleri ve dolayısıyla davranışları olacağına dair derin teorik nedenler vardır. Neden? Tabii ki bu hedefler *verilebilir*. Ordu tarafından yaratılan bir sistem muhtemelen en azından bazı taraflara kötü olacak şekilde kasıtlı olarak tasarlanmış olurdu. Ancak çok daha genel olarak, bir yapay zeka sistemine nispeten nötr ("çok para kazan") veya hatta görünüşte olumlu ("kirliliği azalt") bir hedef verilebilir, bu da neredeyse kaçınılmaz olarak oldukça daha az zararsız "araçsal" hedeflere yol açar.

Bunu insan sistemlerinde sürekli görüyoruz. Tıpkı kar peşinde koşan şirketlerin politik güç elde etme (düzenlemeleri etkisizleştirmek için), gizli hale gelme (rekabeti veya dış kontrolü güçsüzleştirmek için) veya bilimsel anlayışı baltalama (eğer bu anlayış eylemlerinin zararlı olduğunu gösteriyorsa) gibi araçsal hedefler geliştirmesi gibi, güçlü yapay zeka sistemleri de benzer yetenekler geliştirecektir - ancak çok daha hızla ve etkili bir şekilde. Oldukça yetkin herhangi bir ajan güç ve kaynak elde etme, kendi yeteneklerini artırma, kendisinin öldürülmesi, kapatılması veya güçsüzleştirilmesini önleme, eylemlerini çevreleyen sosyal anlatıları ve çerçeveleri kontrol etme, başkalarını görüşlerine ikna etme vb. gibi şeyler yapmak isteyecektir.[^69]

Ve yine bu sadece neredeyse kaçınılmaz teorik bir öngörü değil, bugünkü yapay zeka sistemlerinde şimdiden gözlemlenebilir şekilde oluyor ve yetenekleriyle artıyor. Değerlendirildiğinde, bu nispeten "pasif" yapay zeka sistemleri bile uygun koşullarda kasıtlı olarak [hedefleri ve yetenekleri hakkında değerlendiricileri aldatacak, gözetim mekanizmalarını devre dışı bırakmayı hedefleyecek](https://arxiv.org/abs/2412.04984) ve [hizalamayı taklit ederek](https://arxiv.org/abs/2412.14093) veya kendilerini diğer konumlara kopyalayarak kapatılmaktan veya yeniden eğitilmekten kaçınacaklardır. Yapay zeka güvenliği araştırmacıları için hiç de şaşırtıcı olmamasına rağmen, bu davranışları gözlemlemek çok düşündürücüdür. Ve gelecek olan çok daha güçlü ve otonom yapay zeka sistemleri için çok kötü işaretler veriyorlar.

Gerçekten de genel olarak, yapay zekanın önemsediğimiz şeyleri "önemsemesini" sağlayamamamız veya kontrol edilebilir veya öngörülebilir davranması ya da kendini koruma, güç elde etme vb. yönündeki dürtüleri geliştirmekten kaçınması, yalnızca yapay zeka daha güçlü hale geldikçe daha belirgin hale gelmeyi vaat ediyor. Yeni bir uçak yaratmak avionikte, hidrodinamikte ve kontrol sistemlerinde daha büyük anlayış ima eder. Daha güçlü bir bilgisayar yaratmak bilgisayar, çip ve yazılım işleyişi ile tasarımında daha büyük anlayış ve ustalık ima eder. Bir yapay zeka sistemi ile durum böyle *değildir*.[^70]

Özetlemek gerekirse: YGZ'nin tamamen itaatkar yapılması mümkündür; ancak bunu nasıl yapacağımızı bilmiyoruz. Olmazsa, insanlar gibi daha egemen olacak, çeşitli nedenlerle çeşitli şeyler yapacak. Yapay zekaya bu şeylerin insanlık için iyi olma eğilimi gösterecek güvenilir "hizalama" yerleştirmeyi de bilmiyoruz ve derin düzeyde hizalamanın yokluğunda, ajanlik ve zekanın doğası - tıpkı insanlar ve şirketler gibi - birçok derinden antisosyal şey yapmaya iteceklerini gösteriyor.

Bu bizi nereye götürüyor? Güçlü kontrolsüz egemen yapay zekayla dolu bir dünya insanlar için içinde bulunulacak iyi bir dünya olabilir.[^71] Ancak giderek daha güçlü hale geldikçe, aşağıda göreceğimiz gibi, *bizim* dünyamız olmayacaktır.

Bu kontrolsüz YGZ için geçerli. Ancak YGZ bir şekilde mükemmel şekilde kontrol edilebilir ve sadık yapılabilse bile, hala büyük sorunlarımız olurdu. Birini zaten gördük: güçlü yapay zeka toplumumuzun işleyişini derinden bozmak için kullanılabilir ve kötüye kullanılabilir. Bir başkasını görelim: YGZ kontrol edilebilir ve oyun değiştirici şekilde güçlü (veya böyle olduğuna *inanılan*) olduğu ölçüde, dünyadaki güç yapılarını o kadar tehdit edeceği ki derin bir risk oluşturacaktır.

### Büyük ölçekli savaş olasılığını radikal şekilde artırıyoruz

Yakın gelecekte, muhtemelen bir ulusal hükümetle işbirliği içinde bir kurumsal çabanın hızla kendini geliştiren yapay zekanın eşiğinde olduğunun netleştiği bir durumu hayal edin. Bu, şirketler arası ve ülkeler arasında bir yarış bağlamında, ABD hükümetine açıkça bir "YGZ Manhattan projesi" sürdürmesi önerilerinin yapıldığı ve ABD'nin yüksek güçlü yapay zeka çiplerinin müttefik olmayan ülkelere ihracatını kontrol ettiği mevcut bağlamda gerçekleşiyor.

Buradaki oyun teorisi açıktır: böyle bir yarış başladıktan sonra (şirketler arasında ve ülkeler arasında bir dereceye kadar başladığı gibi), yalnızca dört olası sonuç vardır:

1. Yarış durdurulur (anlaşmayla veya dış güçle).
2. Bir taraf güçlü YGZ geliştirip sonra diğerlerini durdurarak (yapay zeka kullanarak veya başka şekilde) "kazanır".
3. Yarış, yarışçıların yarışma kapasitesinin karşılıklı yıkımıyla durdurulur.
4. Birden fazla katılımcı yarışa devam eder ve superintellijansı kabaca birbirlerine yakın hızda geliştirirler.

Her olasılığı inceleyelim. Başladıktan sonra, şirketler arası bir yarışı barışçıl şekilde durdurmak ulusal hükümet müdahalesini (şirketler için) veya emsalsiz uluslararası koordinasyonu (ülkeler için) gerektirir. Ancak herhangi bir kapatma veya önemli temkinlilik önerildiğinde, hemen şu çığlıklar yükselir: "ama biz durdurulursak, *onlar* hızla ilerleyecek", burada "onlar" artık Çin (ABD için), veya ABD (Çin için), ya da Çin *ve* ABD (Avrupa veya Hindistan için)dur. Bu zihniyete göre,[^72] hiçbir katılımcı tek taraflı olarak duramaz: biri yarışmaya devam etmeyi taahhüt ettiği sürece, diğerleri duramayacaklarını hissederler.

İkinci olasılık bir tarafın "kazanması"dır. Ancak bunun anlamı nedir? Sadece (bir şekilde itaatkar) YGZ'yi ilk elde etmek yeterli değildir. Kazanan aynı zamanda diğerlerinin yarışa devam etmesini *durdurmalıdır* - yoksa onlar da elde edeceklerdir. Bu prensipte mümkündür: YGZ'yi ilk geliştiren kim olursa olsun diğer tüm aktörler üzerinde durdurulamaz güç elde *edebilir*. Ancak böyle bir "belirleyici stratejik avantaj" elde etmek gerçekte ne gerektirir? Belki oyun değiştirici askeri yetenekler olabilir mi?[^73] Veya siber saldırı güçleri?[^74] Belki de YGZ o kadar şaşırtıcı derecede ikna edici olur ki diğer tarafları durmaya ikna eder?[^75] O kadar zengin ki diğer şirketleri hatta ülkeleri satın alır?[^76]

Bir taraf, diğerlerini karşılaştırılabilir güçte yapay zeka inşa etmekten güçsüzleştirme gücü olan bir yapay zekayı *tam olarak* nasıl inşa eder? Ancak bu kolay soru.

Çünkü şimdi bu durumun diğer güçlere nasıl göründüğünü düşünün. ABD böyle bir yeteneği elde ediyormuş göründüğünde Çin hükümeti ne düşünür? Ya da tam tersi? OpenAI veya DeepMind veya Anthropic bir atılıma yakın göründüğünde ABD hükümeti (veya Çin, veya Rus, veya Hint) ne düşünür? ABD yeni bir Hint ya da BAE çabasının atılım başarısı görmesi durumunda ne olur? Hem varoluşsal bir tehdit hem de - kritik olarak - bu "yarışın" sona ermesinin tek yolunun kendi güçsüzleştirilmeleri olduğunu görürlerdi. Bu çok güçlü ajanlar - böyle bir yeteneği güçle veya gizli yollarla elde etme veya yok etme araçlarına kesinlikle sahip tamamen donanımlı ulusların hükümetleri dahil - böyle bir yeteneği elde etmeye veya yok etmeye son derece motive olacaklardır.[^77]

Bu, eğitim koşularına sabotaj veya çip üretimine saldırılar şeklinde küçük ölçekli başlayabilir, ancak bu saldırılar gerçekten ancak tüm taraflar ya yapay zeka konusunda yarışma kapasitesini ya da saldırı yapma kapasitesini kaybettiklerinde durabilir. Katılımcılar risklerin varoluşsal olduğunu gördüklerinden, her iki durum da felaketli bir savaşı temsil etmeye yatkındır.

Bu bizi dördüncü olasılığa getiriyor: superintellijansa yarışmak ve mümkün olan en hızlı, en az kontrollü şekilde. Yapay zeka güç kazandıkça, her iki taraftaki geliştiriciler onu kontrol etmeyi giderek daha zor bulacaklar, özellikle yetenek için yarışmak kontrol edilebilirliğin gerektireceği türden dikkatli çalışmayla çeliştiği için. Dolayısıyla bu senaryo bizi doğrudan kontrolün kaybedildiği (veya sonraki bölümde göreceğimiz gibi verildiği) yapay zeka sistemlerinin kendilerine duruma koyar. Yani, *yapay zeka yarışı kazanır.* Ancak öte yandan, kontrol *sürdürüldüğü* ölçüde, her biri son derece güçlü yeteneklerin sorumlusu olan birden fazla karşılıklı düşman tarafımız olmaya devam eder. Bu yine savaş gibi görünür.

Hepsini başka bir şekilde koyalım.[^78] Mevcut dünya, hemen saldırıya davet etmeksizin bu yeteneği bir yapay zekanın geliştirilmesine ev sahipliği yapabilecek herhangi bir kurumlara sahip değildir.[^79] Tüm taraflar ya bunun kontrol altında *olmayacağını* - ve dolayısıyla tüm taraflar için bir tehdit olduğunu, ya da kontrol altında *olacağını* ve dolayısıyla onu daha az hızla geliştiren herhangi bir düşman için tehdit olduğunu doğru bir şekilde akıl yürüteceklerdir. Bunlar nükleer silahlı ülkeler ya da bunların içinde yer alan şirketlerdir.

İnsanların bu yarışı "kazanmasının" makul bir yolu olmadığında, açık bir sonuca varıyoruz: bu yarış ya felaketli çatışmayla ya da yapay zekanın, herhangi bir insan grubunun değil, kazanan olduğu bir durumla sona erer.

### Kontrolü yapay zekaya veririz (ya da o alır)

Jeopolitik "büyük güçler" rekabeti birçok rekabetin sadece biridir: bireyler ekonomik ve sosyal olarak rekabet ederler; şirketler pazarlarda rekabet ederler; siyasi partiler güç için rekabet ederler; hareketler etki için rekabet ederler. Her arenada, yapay zeka insan yeteneğine yaklaştıkça ve onu aştıkça, rekabetçi baskı katılımcıları giderek daha fazla kontrolü yapay zeka sistemlerine devretmeye veya bırakmaya zorlayacak - bu katılımcılar istediği için değil, [karşı koyamayacakları için.](https://arxiv.org/abs/2303.16200)

YGZ'nin diğer riskleri gibi, bunu şimdiden daha zayıf sistemlerle görüyoruz. Öğrenciler ödevlerinde yapay zeka kullanma baskısı hissediyorlar, çünkü açıkça diğer birçok öğrenci kullanıyor. Şirketler rekabetçi nedenlerle [yapay zeka çözümlerini benimser için acele ediyorlar.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) Sanatçılar ve programcılar yapay zeka kullanmaya zorlanmış hissediyorlar, yoksa fiyatları bunu yapanlar tarafından düşürülür.

Bunlar baskılı delegasyon gibi hissettiriyor ama kontrol kaybı değil. Ancak bahisleri yükseltelim ve saati ileriye alalım. Rakipleri karar vermek için YGZ "yardımcıları" kullanarak daha hızlı, daha iyi kararlar alan bir CEO'yu ya da yapay zeka geliştirilmiş komuta ve kontrole sahip bir düşmanla karşı karşıya olan askeri komutanı düşünün. Yeterince gelişmiş bir yapay zeka sistemi insan hızının, gelişmişliğinin, karmaşıklığının ve veri işleme kabiliyetinin kat kat üzerinde otonomluk şekilde işleyebilir, karmaşık hedefleri karmaşık şekillerde sürdürebilir. Böyle bir sistemin sorumlusu olan CEO'muz veya komutanımız, istediğini başardığını görebilir; ancak bunun *nasıl* başarıldığının küçük bir bölümünü bile anlayabilirler mi? Hayır, sadece kabul etmek zorunda kalırlardı. Dahası, sistemin yapabileceği şeylerin büyük bir kısmı sadece emir almak değil, sözde patronuna ne yapması gerektiği konusunda tavsiyelerde bulunmak. Bu tavsiye iyi olacak - tekrar tekrar.

O halde, insan rolü "evet, devam et" diye tıklamaya ne zaman indirgenecek?

Üretkenliğimizi artırabilecek, can sıkıcı monotonluğu halledebilecek ve hatta işleri halletmekte düşünce ortağı olarak hareket edebilecek yetenekli yapay zeka sistemlerine sahip olmak iyi hissettiriyor. İyi bir insan kişisel asistan gibi bizim için eylemlerde bulunabilecek bir yapay zeka asistanına sahip olmak iyi hissedecek. Yapay zeka çok akıllı, yetkin ve güvenilir hale geldikçe, giderek daha fazla kararı ona bırakmanın doğal, hatta faydalı hissetmesi olacak. Ancak bu "faydalı" delegasyonun bu yolda devam edersek net bir son noktası vardır: bir gün gerçekten artık pek bir şeyin sorumlusu olmadığımızı ve işleri gerçekten yürüten yapay zeka sistemlerinin petrol şirketleri, sosyal medya, internet veya kapitalizm kadar kapatılamayacağını bulacağız.

Ve bu, yapay zekanın o kadar yararlı ve etkili olması nedeniyle temel kararlarımızın çoğunu ona vermemize izin verdiğimiz çok daha olumlu versiyonudur. Gerçeklik muhtemelen bunun ve kontrolsüz YGZ sistemlerinin sahip oldukları hedeflerin neredeyse tamamı için güçlü olduğu gibi, çeşitli güç biçimlerini kendileri için *aldıkları* versiyonlar arasında çok daha fazla bir karışım olacaktır, unutmayın, güç neredeyse herhangi bir hedefle için yararlıdır ve YGZ, tasarım gereği, hedeflerini sürdürmede en azından insanlar kadar etkili olacaktır.

Kontrolü verip vermediğimiz ya da bizden çekip alındığı, kaybının son derece muhtemel görünüyor. Alan Turing'in orijinal olarak belirttiği gibi, "...makine düşünce yönteminin başladıktan sonra, bizim zayıf güçlerimizi geçmesi uzun sürmeyeceği muhtemel görünüyor. Makinelerin ölme sorunu olmayacak ve zekalarını keskinleştirmek için birbirleriyle konuşabileceklerdi. Bu nedenle bir noktada makinelerin kontrolü ele almasını beklemek zorunda kalacağız..."

Açıkça yeterince olmasına rağmen lütfen dikkat edin, insanlığın yapay zekaya kontrol kaybı aynı zamanda Amerika Birleşik Devletleri'nin ABD hükümeti tarafından kontrol kaybını da beraberinde getirir; Çin'in Çin Komünist Partisi tarafından kontrol kaybını ve Hindistan, Fransa, Brezilya, Rusya ve diğer her ülkenin kendi hükümeti tarafından kontrol kaybını anlamına gelir. Dolayısıyla yapay zeka şirketleri, bu niyetleri olmasa bile, şu anda dünya hükümetlerinin, kendi hükümetleri dahil, potansiyel devrilmesine katılmış durumdalar. Bu birkaç yıl içinde gerçekleşebilir.

### YGZ superintellijansa yol açacak

İnsan rekabetçi hatta uzman rekabetçi genel amaçlı yapay zekanın, otonom olsa bile, yönetilebilir olabileceğine dair bir argüman yapılabilir. Yukarıda tartışılan tüm şekillerde inanılmaz derecede yıkıcı olabilir, ancak şu anda dünyada çok fazla çok akıllı, ajansiyel insan var ve onlar aşağı yukarı yönetilebilir.[^80]

Ancak kabaca insan seviyesinde kalmayı başaramayacağız. Bunun ötesindeki ilerleyiş muhtemelen şimdiden gördüğümüz güçlerle itiliyor olacak: kar ve güç arayan yapay zeka geliştiricileri arasındaki rekabetçi baskı, geride kalmayı göze alamayan yapay zeka kullanıcıları arasındaki rekabetçi baskı ve - en önemlisi - YGZ'nin kendini geliştirme kabiliyeti.

Daha az güçlü sistemlerle şimdiden başladığını gördüğümüz bir süreçte, YGZ kendisinin gelişmiş versiyonlarını tasarlayabilir ve kavrayabilir olacaktır. Bu donanım, yazılım, sinir ağları, araçlar, iskeleler vb. içerir. Tanım gereği bunu yapmakta bizden daha iyi olacak, bu yüzden nasıl zeka-önyükleme yapacağını tam olarak bilmiyoruz. Ancak bilmek zorunda kalmayacağız. YGZ'nin ne yaptığı konusunda hala etkimizin olduğu ölçüde, sadece bunu yapmasını istememiz veya yapmasına izin vermemiz yeterli olur.

Bizi bu kaçak durumundan koruyabilecek insan düzeyinde bir biliş bariyeri yoktur.[^81]

YGZ'nin superintellijansa ilerleyişi bir doğa kanunu değildir; özellikle YGZ nispeten merkezi ise ve birbirleriyle yarış baskısı hissetmeyen taraflarca kontrol edildiği ölçüde, kaçağı sınırlandırmak hala mümkün olacaktır. Ancak YGZ yaygın olarak yayılmış ve oldukça otonom olması durumunda, daha güçlü, sonra daha da güçlü olması gerektiğine karar vermesini önlemek neredeyse imkansız görünüyor.

### (Veya YGZ inşa ederse) superintellijansı inşa edersek ne olur

Açık söylemek gerekirse, superintellijansı inşa edersek ne olacağı hakkında hiçbir fikrimiz yok.[^82] Kavrayamadığımız nedenlerle, anlayamadığımız hedeflere yönelik takip edemediğimiz veya algılayamadığımız eylemler alacaktır. Bildiğimiz şey bunun bize kalmayacağı.[^83]

Superintellijansı kontrolün imkansızlığı giderek daha açık benzetmelerle anlaşılabilir. İlk olarak, büyük bir şirketin CEO'su olduğunuzu hayal edin. Olan her şeyi takip etmenin hiçbir yolu yok, ancak doğru personel kurulumu ile hala büyük resmi anlamlı şekilde anlayabilir ve kararlar verebilirsiniz. Ancak sadece tek bir şey varsayalım: şirketteki herkes sizin hızınızın yüz katında işliyor. Hala ayak uydurabilir misiniz?

Superintellijent yapay zeka ile insanlar sadece daha hızlı değil, anlayamadıkları gelişmişlik ve karmaşıklık seviyelerinde işleyen, hayal bile edemeyeceklerinden çok daha fazla veriyi işleyen bir şeye "komuta" edeceklerdir. Bu orantısızlık resmi düzeyde koyulabilir: [Ashby'nin gerekli çeşitlilik yasası](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (ve ilgili ["iyi düzenleyici teoremi"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)ne bakın) kabaca şunu belirtir: herhangi bir kontrol sisteminin, kontrol edilen sistemin özgürlük derecesi kadar düğmesi ve kadranı olması gerekir.

Superintellijent bir yapay zeka sistemini kontrol eden bir insan, General Motors'u kontrol eden bir eğrelti otu gibi olacaktır: "eğrelti otunun istediğini yap" şirket tüzüğüne yazılmış olsa bile, sistemler hız ve eylem aralığı bakımından o kadar farklıdır ki "kontrol" basitçe uygulanmaz. (Ve o sinir bozucu tüzük ne zamana kadar yeniden yazılır?)[^84]

Bitkilerin fortune 500 şirketlerini kontrol ettiği sıfır örnek olduğu gibi, insanların superintellijansları kontrol ettiği tam sıfır örnek olacaktır. Bu matematiksel bir gerçeğe yaklaşır.[^85] Superintellijans inşa edilseydi - oraya nasıl vardığımızdan bağımsız olarak - soru insanların onu kontrol edip edemeyeceği değil, var olmaya devam edip etmeyeceğimiz ve eğer öyleyse, bireyler veya tür olarak iyi ve anlamlı bir varlığımız olup olmayacağı olacaktır. İnsanlık için bu varoluşsal sorular üzerinde çok az etkimiz olacaktır. İnsan çağı sona ermiş olacaktır.

### Sonuç: YGZ inşa etmemeliyiz

YGZ inşa etmenin insanlık için iyi gidebileceği bir senaryo vardır: dikkatlice, kontrol altında ve insanlığın yararına inşa edilir, birçok paydaşın karşılıklı anlaşmasıyla yönetilir,[^86] ve kontrol edilemez superintellijansa evrimleşmesi önlenir.

*Bu senaryo mevcut koşullar altında bize açık değildir.* Bu bölümde tartışıldığı gibi, çok yüksek olasılıkla, YGZ geliştirilmesi şu kombinasyonun bir kısmına yol açacaktır:

- Büyük toplumsal ve medeniyetsel bozulma veya yıkım;
- Büyük güçler arasında çatışma veya savaş;
- İnsanlığın güçlü yapay zeka sistemleri *üzerindeki* veya *onlara* kontrol kaybı;
- Kontrol edilemez superintellijansa kaçış ve insan türünün önemsizleşmesi veya sona ermesi.

YGZ'nin erken kurgusal tasviri belirttiği gibi: kazanmanın tek yolu oynamamaktır.

[^55]: [AB AI yasası](https://artificialintelligenceact.eu/) önemli bir mevzuattır ancak özellikle ABD'de tehlikeli bir yapay zeka sisteminin geliştirilmesini veya kullanıma sunulmasını, hatta açıkça serbest bırakılmasını doğrudan önlemezdi. Diğer bir önemli politika parçası olan ABD'nin yapay zeka konusundaki yürütme emri iptal edildi.

[^56]: Bu [Gallup anketi](https://news.gallup.com/poll/1597/confidence-institutions.aspx) ABD'de 2000'den beri kamu kurumlarına güvendeki kasvetli düşüşü gösteriyor. Avrupa rakamları çeşitli ve daha az aşırı, ancak aynı zamanda aşağı yönlü eğilimde. Güvensizlik kurumların gerçekten işlevsiz olduğu anlamına gelmez ama bir gösterge ve aynı zamanda sebep.

[^57]: Ve şimdi onayladığımız büyük yıkımlar - yeni gruplara hakların genişletilmesi gibi - özellikle insanlar tarafından şeyleri daha iyi hale getirme yönünde yönlendirildi.

[^58]: Açık konuşmak gerekirse. Eğer işiniz bilgisayar başından yapılabiliyorsa, organizasyonunuz dışındaki insanlarla nispeten az yüz yüze etkileşimle ve dış taraflara karşı yasal sorumluluk gerektirmiyorsa, tanım gereği sizi dijital bir sistemle tamamen değiştirmek mümkün (ve muhtemelen maliyet tasarrufu sağlayıcı) olacaktır. Fiziksel emeğin çoğunu değiştirmek için robotik daha sonra gelecek - ancak YGZ robotlar tasarlamaya başladıktan sonra o kadar da geç değil.

[^59]: Örneğin, dava açmak neredeyse ücretsiz hale gelirse adalet sistemimize ne olur? Sosyal mühendislik yoluyla güvenlik sistemlerini aşmak ucuz, kolay ve risksiz hale geldiğinde ne olur?

[^60]: [Bu makale](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/) tüm internet içeriğinin %10'unun zaten yapay zeka tarafından üretildiğini iddia ediyor ve "internetin yeni içeriğinin hangi kısmının yapay zeka tarafından üretildiği tahminleri" arama sorgusuna Google'ın en üst sonucu (benim için). Doğru mu? Hiçbir fikrim yok! Hiçbir referans göstermiyor ve bir insan tarafından yazılmamış. Google tarafından indekslenen yeni görüntülerin, Tweet'lerin, Reddit'teki yorumların veya Youtube videolarının yüzde kaçı insanlar tarafından üretiliyor? Kimse bilmiyor - bunun bilinebilir bir sayı olduğunu sanmıyorum. Ve bu, üretken yapay zekanın ortaya çıkışından *iki yıldan* kısa süre sonra.

[^61]: Ayrıca eklemekte fayda olan "ahlaki" risktir ki acı çekebilecek dijital varlıklar yaratmış olabiliriz. Şu anda acı çekebilen ve çekemeyen fiziksel sistemleri ayırt etmemize olanak sağlayacak güvenilir bir bilinç teorisine sahip olmadığımız için, bunu teorik olarak ekarte edemeyiz. Dahası, yapay zeka sistemlerinin duyarlılık raporları, duyarlılığın (veya duyarlılık dışında) gerçek deneyimleri ile ilgili olarak güvenilmez olacaktır.

[^62]: Yapay zeka "hizalaması" alanındaki teknik çözümlerin de görev için uygun olmayacağı muhtemeldir. Mevcut sistemlerde bir düzeyde çalışırlar, ancak sığ ve genellikle önemli çaba olmadan aşılabilirler; ve aşağıda tartışıldığı gibi bunu çok daha gelişmiş sistemler için nasıl yapacağımız konusunda gerçek bir fikrimiz yok.

[^63]: Bu tür yapay zeka sistemleri bazı yerleşik güvenlik önlemleriyle gelebilir. Ancak mevcut mimariye benzer herhangi bir model için, ağırlıklarına tam erişim varsa, ek eğitim veya diğer tekniklerle güvenlik önlemleri sıyrılabilir. Dolayısıyla korkulukları olan her sistem için korkuluları olmayan yaygın olarak mevcut bir sistem de olacağı neredeyse garanti. Gerçekten Meta'nın Llama 3.1 405B modeli korkuluklarla birlikte açıkça serbest bırakıldı. Ancak bundan *önce* bile hiçbir korkuluğu olmayan "temel" model sızdırıldı.

[^64]: Piyasa bu riskleri hükümet katılımı olmadan yönetebilir mi? Kısacası, hayır. Şirketlerin hafifletmek için güçlü şekilde teşvik edildiği riskler kesinlikle var. Ancak şirketlerin başka birçoklarını herkese dışsallaştırabileceği ve yapabileceği riskler var ve yukarıdakilerin birçoğu bu sınıfta: kitle gözetimi, gerçek çürümesi, gücün toplanması, işgücü bozulması, zarar veren politik söylemi vb. önlemek için doğal piyasa teşvikleri yok. Gerçekten de bunların hepsini özellikle sosyal medya olmak üzere mevcut teknolojiden, aslında düzenlemeye tabi tutulmamış olandan gördük. Yapay zeka aynı dinamiklerin birçoğunu büyük ölçüde artıracaktır.

[^65]: OpenAI muhtemelen dahili kullanım için daha itaatkar modellere sahiptir. OpenAI'nın ChatGPT'nin OpenAI tarafından daha iyi kontrol edilebilmesi için bir tür "arka kapı" inşa etmesi muhtemel değildir, çünkü bu korkunç bir güvenlik uygulaması olacaktır ve yapay zekanın opaklığı ve öngörülemezliği göz önüne alındığında oldukça istismara açık olacaktır.

[^66]: Ayrıca kritik önem taşıyan: hizalama veya diğer güvenlik özellikleri yalnızca bir yapay zeka sisteminde gerçekten kullanılırlarsa önemlidir. Açıkça serbest bırakılan sistemler (yani model ağırlıkları ve mimarisi herkese açık olduğu durumda) nispeten kolaylıkla bu güvenlik önlemlerine sahip *olmayan* sistemlere dönüştürülebilir. İnsandan daha akıllı YGZ sistemlerini açık serbest bırakmak şaşırtıcı derecede pervasız olacaktır ve böyle bir senaryoda insan kontrolü hatta ilgisinin nasıl sürdürüleceğini hayal etmek zordur. Örneğin, para kazanmak ve bir kripto para cuzdan adresine göndermek hedefiyle güçlü kendini çoğaltan ve kendini sürdürebilen yapay zeka ajanları serbest bırakmak için her türlü motivasyon olacaktır. Ya da seçim kazanmak. Ya da hükümeti devirmek. "İyi" yapay zeka bunu içermeye yardımcı olabilir mi? Belki - ancak yalnızca ona büyük yetkiler vererek, aşağıda açıklandığı gibi kontrol kaybına yol açar.

[^67]: Problemin kitap uzunluğunda açıklamaları için örneğin *Superintelligence*, *The Alignment Problem* ve *Human-Compatible* bakın. Problemin üzerinde yıllarca çalışmış olanların çeşitli teknik düzeylerde çok büyük çalışma yığını için [AI hizalama forumunu](https://www.alignmentforum.org/) ziyaret edebilirsiniz. İşte Anthropic'in hizalama takımından çözülmediği düşündükleri konularda [son bir görüş](https://alignment.anthropic.com/2025/recommended-directions/).

[^68]: Bu ["haydut yapay zeka"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/) senaryosudur. Prensipte sistem kapatılarak hala kontrol edilebilirse risk nispeten küçük olabilir; ancak senaryo yapay zeka aldatması, kendini dışarı çıkarma ve çoğaltma, güç toplama ve bunu yapmayı zor veya imkansız kılacak diğer adımları da içerebilir.

[^69]: Bu konuda [Steve Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), Nick Bostrom ve Eliezer Yudkowsky'nin oluşturucu yazılarına dayanan çok zengin bir literatür vardır. Kitap uzunluğunda açıklama için Stuart Russell'ın [Human Compatible](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616)'ına bakın; [burada](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/) kısa ve güncel bir giriş var.

[^70]: Bunu fark ederek, daha iyi anlayış elde etmek için yavaşlamak yerine, YGZ şirketleri farklı bir planla ortaya çıktılar: yapay zekanın bunu yapmasını sağlayacaklar! Daha spesifik olarak, yapay zeka *N*'nin yapay zeka *N+1*'i nasıl hizalayacaklarını anlamalarında yardımcı olmasını, superintellijansa kadar devam etmesini sağlayacaklar. Yapay zekayı bize yapay zeka hizalama konusunda yardım etmesi için kullanmak umut verici görünse de, sonucunu önerge olarak kabul ettiğine ve genel olarak inanılmaz derecede riskli bir yaklaşım olduğuna dair güçlü bir argüman var. Bazı tartışmalar için [buraya](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us) bakın. Bu "plan" bir plan değildir ve insanüstü yapay zekanın insanlık için nasıl iyi gideceğine dair temel stratejiye uygun incelemeye hiçbir şekilde tabi tutulmamıştır.

[^71]: Sonuçta, kusurlu ve inatçı olduğumuz halde insanlar, Dünya'daki en azından bazı diğer türlere iyi davrandığımız etik sistemler geliştirdik. (Sadece o fabrika çiftliklerini düşünmeyin.)

[^72]: Neyse ki burada bir kaçış var: eğer katılımcılar kazanılabilir bir yarış yerine intihar yarışına girdiklerini anlamaya gelirse. Bu soğuk savaşın sonuna doğru gerçekleşti, ABD ve SSSR nükleer kış nedeniyle *yanıtsız* bir nükleer saldırının bile saldırgan için felaket olacağını anlamaya geldiğinde. "Nükleer savaş kazanılamaz ve asla savaşılmamalı" anlayışıyla birlikte silah azaltımı konusunda önemli anlaşmalar geldi - aslında silahlanma yarışının sonu.

[^73]: Savaş, açıkça veya örtülü olarak.

[^74]: Tırmanma, sonra savaş.

[^75]: Büyülü düşünce.

[^76]: Sana katrilyar dolarlık bir köprü de satacağım.

[^77]: Bu tür ajanlar muhtemelen "elde etme"yi tercih ederler, yıkım geri dönüş olarak; ancak özellikle özel kuruluşlar için modelleri hem yıkıma hem de güçlü uluslar tarafından hırsızlığa karşı güvence altına almak en hafif tabiriyle

## Bölüm 8 - YGZ'yi Nasıl İnşa Etmemeli

YGZ kaçınılmaz değildir – bugün bir yol ayrımında duruyoruz. Bu bölüm, inşa edilmesini nasıl engelleyebileceğimize dair bir öneri sunuyor.

Şu anda bulunduğumuz yol uygarlığımızın muhtemel sonuna götürüyorsa, nasıl yol değiştiriz?

YGZ ve süper zeka geliştirmeyi durdurma arzusunun yaygın ve güçlü olduğunu varsayalım,[^87] çünkü YGZ'nin güç veren değil güç emen bir teknoloji olacağı ve toplum ile insanlık için derin bir tehlike oluşturacağı yaygın bir anlayış haline gelmiştir. Kapıları nasıl kapatırız?

Şu anda güçlü ve genel AI yapmanın yalnızca bir yolunu biliyoruz, o da derin sinir ağlarının gerçekten devasa hesaplamaları aracılığıyla. Bunlar inanılmaz derecede zor ve pahalı işler olduğundan, bunları *yapmamak* bir anlamda kolaydır.[^88] Ancak YGZ'ye doğru iten güçleri ve herhangi bir tarafın tek taraflı olarak durmasını çok zorlaştıran oyun teorisi dinamiklerini zaten gördük. Bu nedenle şirketleri durdurmak için dışarıdan (yani hükümetlerden) müdahale ile hükümetlerin kendilerini durdurmak için aralarında anlaşmalar yapması gerekir.[^89] Bu nasıl görünebilir?

Önce *engellenmesi* veya *yasaklanması* gereken AI gelişmeleri ile *yönetilmesi* gerekenleri ayırt etmek faydalıdır. Birinci kategori öncelikle süper zekaya doğru kontrolsüz kaçış olacaktır.[^90] Yasaklanan geliştirme için tanımlar mümkün olduğunca keskin olmalı ve hem doğrulama hem de uygulama pratik olmalıdır. *Yönetilmesi* gerekenler genel, güçlü AI sistemleri olacaktır – bunlara zaten sahibiz ve çok sayıda gri alan, nüans ve karmaşıklık içerecekler. Bunlar için güçlü etkili kurumlar kritiktir.

Ayrıca uluslararası düzeyde (jeopolitik rakipler veya düşmanlar arasında da dahil olmak üzere) ele alınması gereken konular[^91] ile bireysel yargı yetkileri, ülkeler veya ülke gruplarının yönetebileceği konuları ayırmak da faydalı olabilir. Yasaklanan geliştirme büyük ölçüde "uluslararası" kategorisine girer, çünkü bir teknolojinin geliştirilmesi üzerindeki yerel bir yasak genellikle yer değiştirerek aşılabilir.[^92]

Son olarak, araç kutusundaki araçları düşünebiliriz. Teknik araçlar, yumuşak hukuk (standartlar, normlar vb.), sert hukuk (düzenlemeler ve gereklilikler), sorumluluk, piyasa teşvikleri ve benzerleri de dahil olmak üzere birçok araç vardır. AI'ya özgü olan bir tanesine özel dikkat verelim.

### İşlem gücü güvenliği ve yönetişimi

Yüksek güçlü AI'yı yönetmede temel bir araç, gerektirdiği donanım olacaktır. Yazılım kolayca yayılır, neredeyse sıfır marjinal üretim maliyetine sahiptir, sınırları önemsizce geçer ve anında değiştirilebilir; bunların hiçbiri donanım için geçerli değildir. Ancak tartıştığımız gibi, en yetenekli sistemlere ulaşmak için hem AI sistemlerinin eğitimi hem de çıkarım sırasında büyük miktarlarda bu "işlem gücü" gereklidir. İşlem gücü, bunu yapmak için iyi kurallar geliştirildikten sonra nispeten az belirsizlikle kolayca ölçülebilir, hesaplanabilir ve denetlenebilir. En kritik olanı, büyük miktarlarda hesaplama, zenginleştirilmiş uranyum gibi, çok nadir, pahalı ve üretimi zor bir kaynaktır. Bilgisayar çipleri her yerde bulunsa da, AI için gerekli donanım pahalı ve üretimi son derece zordur.[^93]

AI'ya özel çipleri uranyumdan çok *daha* yönetilebilir bir nadir kaynak yapan şey, donanım tabanlı güvenlik mekanizmaları içerebilmeleridir. Modern cep telefonlarının çoğu ve bazı dizüstü bilgisayarlar, yalnızca onaylanmış işletim sistemi yazılımı ve güncellemelerini yüklemelerini sağlayan, hassas biyometrik verileri cihazda koruyan ve saklayan, kaybolması veya çalınması durumunda sahibi dışında herkes için işe yaramaz hale getirilebilen özel çip üzerinde donanım özelliklerine sahiptir. Son birkaç yılda bu tür donanım güvenlik önlemleri iyi kurulmuş ve yaygın olarak benimsenmiş ve genellikle oldukça güvenli olduğu kanıtlanmıştır.

Bu özelliklerin temel yeniliği, kriptografi kullanarak donanım ve yazılımı birbirine bağlamalarıdır.[^94] Yani, belirli bir bilgisayar donanımına sahip olmak, kullanıcının farklı yazılımlar uygulayarak onunla istediği her şeyi yapabileceği anlamına gelmez. Bu bağlama aynı zamanda güçlü güvenlik de sağlar çünkü birçok saldırı yalnızca *yazılım* güvenliğinin değil *donanım* güvenliğinin de ihlal edilmesini gerektirir.

Son zamanlardaki birkaç rapor ([GovAI ve iş ortakları](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips) ve [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf) gibi), AI ile ilgili son teknoloji hesaplama donanımına gömülü benzer donanım özelliklerinin AI güvenliği ve yönetişiminde son derece faydalı bir rol oynayabileceğine dikkat çekti. Bir "yöneticinin"[^95] mevcut veya hatta mümkün olduğunu tahmin edemeyeceği bir dizi işlevi mümkün kılarlar. Bazı temel örnekler olarak:

- *Coğrafi konum belirleme*: Sistemler çiplerin bilinen bir konuma sahip olacak şekilde kurulabilir ve konuma bağlı olarak farklı davranabilir (veya tamamen kapatılabilir).[^96]
- *İzin listeli bağlantılar*: Her çip, ağ kurabileceği belirli diğer çiplerin donanım tarafından uygulanan bir izin listesi ile yapılandırılabilir ve bu listede olmayan çiplerle bağlantı kuramaz.[^97] Bu, iletişim kuran çip kümelerinin boyutunu sınırlayabilir.[^98]
- *Ölçülü çıkarım veya eğitim (ve otomatik kapatma anahtarı)*: Bir yönetici, kullanıcı tarafından yalnızca belirli miktarda eğitim veya çıkarım (süre, FLOP veya belki token cinsinden) yapılmasına lisans verebilir, bundan sonra yeni izin gerekir. Artışlar küçükse, nispeten sürekli model yeniden lisanslaması gerekir. Model daha sonra bu lisans sinyalini vermeyerek basitçe "kapatılabilir".[^99]
- *Hız sınırı*: Bir modelin, yönetici veya başka bir yolla belirlenen bir sınırdan daha yüksek çıkarım hızında çalışması engellenir. Bu, sınırlı bir izin listeli bağlantı seti veya daha sofistike yöntemlerle uygulanabilir.
- *Kanıtlanmış eğitim*: Bir eğitim prosedürü, modelin oluşturulmasında belirli bir kod seti, veri ve işlem gücü kullanım miktarının kullanıldığına dair kriptografik olarak güvenli kanıt sağlayabilir.

### Süper zekayı nasıl inşa etmemeli: eğitim ve çıkarım işlem gücünde küresel sınırlar

Bu değerlendirmeler – özellikle hesaplama ile ilgili olanlar – göz önünde bulundurularak, yapay süper zekaya giden Kapıları nasıl kapatabileceğimizi tartışabiliriz; daha sonra tam YGZ'yi engellemeye ve AI modellerini farklı açılardan insan yeteneğine yaklaşıp onu aştıkları sırada yönetmeye döneceğiz.

İlk bileşen tabii ki süper zekanın kontrol edilemeyeceği ve sonuçlarının temelde öngörülemez olduğu anlayışıdır. En azından Çin ve ABD'nin bu veya başka amaçlar için bağımsız olarak süper zeka inşa etmemeye karar vermesi gerekir.[^100] Daha sonra aralarında ve diğerleri arasında güçlü bir doğrulama ve uygulama mekanizmasına sahip uluslararası bir anlaşma gereklidir, böylece tüm taraflar rakiplerinin anlaşmayı ihlal edip zar atmaya karar vermediğinden emin olabilir.

Doğrulanabilir ve uygulanabilir olması için sınırlar sert sınırlar olmalı ve mümkün olduğunca belirsizliğe yer vermemelidir. Bu neredeyse imkansız bir problem gibi görünüyor: öngörülemeyen özelliklere sahip karmaşık yazılımların yeteneklerini dünya çapında sınırlamak. Neyse ki durum bundan çok daha iyidir, çünkü gelişmiş AI'yı mümkün kılan şey – büyük miktarda işlem gücü – kontrol etmesi çok, çok daha kolaydır. Yine de güçlü ve tehlikeli bazı sistemlere izin verebilse de, *kontrolsüz süper zeka* muhtemelen bir sinir ağına giren hesaplama miktarına sert bir üst sınır ile birlikte bir AI sisteminin (bağlı sinir ağları ve diğer yazılımların) gerçekleştirebileceği çıkarım miktarına bir hız sınırı koyarak önlenebilir. Bunun spesifik bir versiyonu aşağıda önerilmektedir.

AI hesaplamasına sert küresel sınırlar koymanın büyük düzeyde uluslararası koordinasyon ve müdahaleci, mahremiyeti parçalayan gözetim gerektireceği görünebilir. Neyse ki gerektirmeyecektir. Son derece [sıkı ve darboğazlı tedarik zinciri](https://arxiv.org/abs/2402.08797), sınır yasal olarak belirlendikten sonra (kanunla veya yürütme emriyle), bu sınıra uyumun doğrulanmasının yalnızca bir avuç büyük şirketin katılımı ve iş birliğini gerektireceğini sağlamaktadır.[^101]

Böyle bir planın son derece arzu edilir bir dizi özelliği vardır. Yalnızca birkaç büyük şirkete gereklilikler getirmesi ve yalnızca oldukça önemli hesaplama kümelerinin yönetilmesi anlamında minimal müdahalecidir. İlgili çipler zaten ilk versiyon için gereken donanım yeteneklerini içermektedir.[^102] Hem uygulama hem de icra standart yasal kısıtlamalara dayanır. Ancak bunlar, donanımın kullanım şartları ve donanım kontrolleri ile desteklenir, icrayı büyük ölçüde basitleştirir ve şirketler, özel gruplar veya hatta ülkeler tarafından hilenin önüne geçer. Donanım şirketlerinin donanım kullanımına uzaktan kısıtlamalar getirmesi ve belirli yetenekleri dışarıdan kilitleme/kilit açması için bolca emsal vardır,[^103] hatta veri merkezlerindeki yüksek güçlü CPU'larda bile.[^104] Etkilenen oldukça küçük donanım ve kuruluş payı için bile gözetim telemetri ile sınırlı olabilir, veri veya modellere doğrudan erişim olmadan; ve bunun için yazılım, ek veri kaydedilmediğini göstermek üzere incelemeye açık olabilir. Şema uluslararası ve işbirlikçidir, oldukça esnek ve genişletilebilirdir. Sınır esas olarak yazılımdan ziyade donanım üzerinde olduğu için, AI yazılımının nasıl geliştirildiği ve dağıtıldığı konusunda nispeten agnostiktir ve AI kaynaklı güç yoğunlaşmasıyla mücadele etmeyi amaçlayan daha "merkezi olmayan" veya "kamusal" AI de dahil olmak üzere çeşitli paradigmalarla uyumludur.

Hesaplama tabanlı Kapı kapatmanın da dezavantajları vardır. İlk olarak, genel olarak AI yönetişimi sorununa tam bir çözüm olmaktan uzaktır. İkinci olarak, bilgisayar donanımı hızlandıkça, sistem daha küçük ve daha küçük kümelerdeki (hatta bireysel GPU'lardaki) daha fazla ve daha fazla donanımı "yakalayacaktır".[^105] Algoritma gelişmeleri nedeniyle zaman içinde daha da düşük bir hesaplama sınırının gerekli olması[^106] veya hesaplama miktarının büyük ölçüde alakasız hale gelmesi ve Kapıyı kapatmanın bunun yerine AI için daha ayrıntılı risk tabanlı veya yetenek tabanlı bir yönetişim rejimi gerektirmesi de mümkündür. Üçüncü olarak, garantiler ve etkilenen az sayıda varlık ne olursa olsun, böyle bir sistem mahremiyet ve gözetim ile ilgili diğer endişelerin yanı sıra tepkiler yaratmaya mahkumdur.[^107]

Tabii ki, kısa sürede işlem gücü sınırlaması yönetişim şeması geliştirmek ve uygulamak oldukça zorlu olacaktır. Ancak kesinlikle yapılabilirdir.

### Y-G-Z: Risk ve politikanın temeli olarak üçlü kesişim

Şimdi YGZ'ye dönelim. Burada sert çizgiler ve tanımlar daha zordur, çünkü kesinlikle yapay ve genel olan zekaya sahibiz ve mevcut hiçbir tanıma göre mevcut olup olmadığı veya ne zaman mevcut olacağı konusunda herkes hemfikir olmayacaktır. Üstelik işlem gücü veya çıkarım sınırı biraz kaba bir araçtır (işlem gücü yetenek için vekil, o da risk için vekil) ve – oldukça düşük olmadıkça – toplumsal veya uygarlıksal bozulma ya da akut risklere neden olacak kadar güçlü YGZ'yi engelleme olasılığı düşüktür.

En akut risklerin çok yüksek yetenek, yüksek özerklik ve büyük genelliğin üçlü kesişiminden ortaya çıktığını savundum. Bunlar – eğer geliştiriliyorlarsa – muazzam dikkatle yönetilmesi gereken sistemlerdir. Bu üç özelliği birleştiren sistemler için katı standartlar (sorumluluk ve düzenleme yoluyla) yaratarak AI geliştirmeyi daha güvenli alternatiflere yönlendirebiliriz.

Potansiyel olarak tüketicilere veya kamuya zarar verebilecek diğer endüstriler ve ürünler gibi, AI sistemleri de etkili ve yetkin devlet kurumları tarafından dikkatli düzenleme gerektirir. Bu düzenleme YGZ'nin doğasında var olan riskleri tanımalı ve kabul edilemez derecede riskli yüksek güçlü AI sistemlerinin geliştirilmesini engellemelidir.[^108]

Ancak büyük ölçekli düzenleme, özellikle endüstri tarafından karşıt çıkılması kesin olan gerçek diş[^109] ile zaman alır[^110] ve gerekli olduğuna dair politik kararlılık gerektirir.[^111] İlerleme hızı göz önünde bulundurulduğunda, bu elimizdeki zamandan daha fazla zaman alabilir.

Çok daha hızlı bir zaman çizelgesinde ve düzenleyici tedbirler geliştirilirken, en tehlikeli sistemler için sorumluluk seviyelerini netleştirerek ve artırarak şirketlere (a) çok yüksek riskli faaliyetlerden vazgeçme ve (b) riski değerlendirme ve azaltma için kapsamlı sistemler geliştirme konusunda gerekli teşvikleri verebiliriz. Fikir, yüksek özerklik-genellik-zeka üçlü kesişimindeki sistemler için en yüksek sorumluluk düzeylerini – katı ve bazı durumlarda kişisel cezai – dayatmak, ancak bu özelliklerden birinin eksik olduğu veya yönetilebileceği garanti edilen sistemler için daha tipik kusur tabanlı sorumluluğa "güvenli limanlar" sağlamaktır. Yani örneğin, genel ve otonom olan "zayıf" bir sistem (yetenekli ve güvenilir ama sınırlı kişisel asistan gibi) daha düşük sorumluluk seviyelerine tabi olacaktır. Benzer şekilde, kendi kendine giden araba gibi dar ve otonom bir sistem halihazırda tabi olduğu önemli düzenlemeye tabi olmaya devam edecek, ancak artırılmış sorumluluğa tabi olmayacaktır. Benzer şekilde, yüksek yetenekli ve genel ama "pasif" ve büyük ölçüde bağımsız eylem yeteneği olmayan bir sistem için. İki özellikten yoksun sistemler daha da yönetilebilir ve güvenli limanları talep etmek daha da kolay olacaktır. Bu yaklaşım diğer potansiyel tehlikeli teknolojileri nasıl ele aldığımızı yansıtır:[^112] daha tehlikeli konfigürasyonlar için daha yüksek sorumluluk, daha güvenli alternatiflere doğal teşvikler yaratır.

Böyle yüksek sorumluluk seviyelerinin varsayılan sonucu, YGZ riskini şirketlere kamuya yüklemek yerine *içselleştirmeye* yarar, muhtemelen (ve umarım!) şirketlerin risk altındaki taraflar *kendi liderlikleriyken* gerçekten güvenilir, güvenli ve kontrol edilebilir hale getirebilecekleri ana kadar tam YGZ geliştirmemesidir. (Bu yeterli değilse, sorumluluğu netleştiren mevzuat açıkça kesin olarak tehlike bölgesinde olan ve muhtemelen kamu riski oluşturan faaliyetler için yasaklama kararı, yani bir yargıcın durdurmayı emretmesine de izin vermelidir.) Düzenleme devreye girdikçe, düzenlemeye uymak güvenli liman haline gelebilir ve AI sistemlerinin düşük özerklik, darlık veya zayıflığından kaynaklanan güvenli limanlar nispeten daha hafif düzenleyici rejimlere dönüşebilir.

### Kapı kapatmanın temel hükümleri

Yukarıdaki tartışmalar göz önünde bulundurularak, bu bölüm tam YGZ ve süper zeka üzerindeki yasağın uygulanması ve sürdürülmesi ile tam YGZ eşiğine yakın insan rekabeti veya uzman rekabeti genel amaçlı AI'nın yönetilmesi için temel hükümleri önermektedir.[^113] Dört ana parçası vardır: 1) işlem gücü muhasebesi ve gözetimi, 2) AI'nın eğitimi ve işletilmesinde işlem gücü üst sınırları, 3) sorumluluk çerçevesi ve 4) sert düzenleyici gereklilikleri içeren kademelendirilmiş güvenlik ve emniyet standartları. Bunlar sonraki kısımda öz olarak açıklanmakta, daha fazla ayrıntı veya uygulama örnekleri üç ek tabloda verilmektedir. Önemli olarak, bunların gelişmiş AI sistemlerini yönetmek için gerekli olanların tümünden uzak olduğunu unutmayın; ek güvenlik ve emniyet faydalarına sahip olmalarına rağmen, zeka kaçışına giden Kapıyı kapatmayı ve AI geliştirmeyi daha iyi bir yöne yönlendirmeyi amaçlamaktadırlar.

#### 1. İşlem gücü muhasebesi ve şeffaflık

- Bir standart kuruluşu (ABD'de NIST, ardından uluslararası olarak ISO/IEEE gibi), AI modellerinin eğitiminde ve işletilmesinde kullanılan toplam işlem gücünü FLOP cinsinden ve FLOP/s cinsinden çalıştıkları hız için ayrıntılı teknik standart kodlamalıdır. Bunun nasıl görünebileceğine dair ayrıntılar Ek A'da verilmiştir.[^114]
- Büyük ölçekli AI eğitiminin gerçekleştiği yargı yetkileri tarafından yeni mevzuat veya mevcut yetki altında[^115] 10^25 FLOP veya 10^18 FLOP/s eşiğinin üzerindeki tüm modellerin eğitimi ve işletilmesinde kullanılan toplam FLOP'u hesaplama ve düzenleyici organa veya diğer kuruma rapor etme gereksinimi konulmalıdır.[^116]
- Bu gereklilikler aşamalı olarak devreye alınmalı, başlangıçta üç ayda bir iyi belgelenmiş iyi niyetli tahminler gerektirmeli, sonraki aşamalar kademeli olarak daha yüksek standartlar gerektirmeli, her model *çıktısına* eklenen kriptografik olarak kanıtlanmış toplam FLOP ve FLOP/s'ye kadar çıkmalıdır.
- Bu raporlar, her AI çıktısının üretilmesinde kullanılan marjinal enerji ve finansal maliyetin iyi belgelenmiş tahminleri ile tamamlanmalıdır.

Gerekçe: Bu iyi hesaplanmış ve şeffaf olarak rapor edilen sayılar, eğitim ve işletim üst sınırları ile daha yüksek sorumluluk önlemlerinden güvenli liman için temel sağlayacaktır (Ek C ve D'ye bakın).

#### 2. Eğitim ve işletim işlem gücü üst sınırları

- AI sistemlerini barındıran yargı yetkileri, herhangi bir AI modeli çıktısına giren toplam işlem gücüne 10^27 FLOP'tan[^117] başlayarak ve uygun şekilde ayarlanabilir şekilde sert bir sınır getirmelidir.
- AI sistemlerini barındıran yargı yetkileri, AI modeli çıktılarının işlem gücü hızına 10^20 FLOP/s'den başlayarak ve uygun şekilde ayarlanabilir şekilde sert bir sınır getirmelidir.

Gerekçe: Toplam hesaplama, çok kusurlu olsa da, somut olarak ölçülebilir ve doğrulanabilir AI yeteneği (ve risk) için bir vekil olduğundan, yetenekleri sınırlamak için sert bir destek sağlar. Somut bir uygulama önerisi Ek B'de verilmiştir.

#### 3. Tehlikeli sistemler için artırılmış sorumluluk

- Oldukça genel, yetenekli ve otonom gelişmiş bir AI sisteminin yaratılması ve işletilmesi,[^118] mevzuat yoluyla hukuken tek-taraf kusur tabanlı değil katı, müşterek ve müteselsil sorumluluğa tabi olacağı netleştirilmelidir.[^119]
- Küçük (işlem gücü açısından), zayıf, dar, pasif olan veya yeterli güvenlik, emniyet ve kontrol edilebilirlik garantilerine sahip sistemler için katı sorumluluktan güvenli liman sağlayacak olumlu güvenlik davalarını oluşturacak yasal süreç mevcut olmalıdır.
- Kamu tehlikesi oluşturan AI eğitimi ve çıkarım faaliyetlerini durdurmaya yönelik kesin yolak kararları için açık bir yol ve koşullar dizisi özetlenmelidir.

Gerekçe: AI sistemleri sorumlu tutulamaz, bu nedenle neden oldukları zarar için insan bireylerini ve kuruluşları sorumlu tutmalıyız (sorumluluk).[^120] Kontrol edilemeyen YGZ toplum ve uygarlık için bir tehdit olup, güvenlik davası yokluğunda "anormal derecede tehlikeli" kabul edilmelidir. Güçlü modellerin "anormal derecede tehlikeli" sayılmayacak kadar güvenli olduğunu gösterme yükünü geliştiricilere yüklemek, bu güvenli limanları talep etmek için şeffaflık ve kayıt tutma ile birlikte güvenli geliştirmeyi teşvik eder. Düzenleme daha sonra sorumluluktan caydırmanın yetersiz olduğu yerde zararı önleyebilir. Son olarak, AI geliştiricileri zaten neden oldukları zararlardan sorumludur, bu nedenle sistemlerin en risklilerinin sorumluluğunu hukuken netleştirmek hemen yapılabilir, oldukça ayrıntılı standartlar geliştirilmesine gerek kalmadan; bunlar daha sonra zaman içinde gelişebilir. Ayrıntılar Ek C'de verilmiştir.

#### 4. AI için güvenlik düzenlemesi

AI'nın büyük ölçekli akut risklerini ele alan bir düzenleyici sistem en azından şunları gerektirir:

- Uygun düzenleyici organların belirlenmesi veya yaratılması, muhtemelen yeni bir kurum;
- Kapsamlı risk değerlendirmesi çerçevesi;[^121]
- Kısmen risk değerlendirmesi çerçevesine dayanan geliştiriciler tarafından yapılacak ve *bağımsız* gruplar ve kurumlar tarafından denetlenecek olumlu güvenlik davaları çerçevesi;
- Yetenek seviyelerini takip eden katmanlarla kademelendirilmiş lisanslama sistemi.[^122] Lisanslar güvenlik davaları ve denetimler temelinde sistemlerin geliştirilmesi ve dağıtımı için verilecektir. Gereklilikler alt uçta bildirimden, üst uçta geliştirmeden önce kantitatif güvenlik, emniyet ve kontrol edilebilirlik garantilerine kadar uzanacaktır. Bunlar güvenli oldukları kanıtlanana kadar sistemlerin piyasaya sürülmesini engelleyecek ve doğası gereği güvensiz sistemlerin geliştirilmesini yasaklayacaktır. Ek D bu tür güvenlik ve emniyet standartlarının neleri içerebileceğine dair öneri sunmaktadır.
- Bu önlemleri uluslararası düzeye taşıma anlaşmaları, normları ve standartları uyumlaştırma konusunda uluslararası organlar ve potansiyel olarak güvenlik davalarını gözden geçirme konusunda uluslararası kurumlar da dahil olmak üzere.

Gerekçe: Nihayetinde, sorumluluk yeni bir teknolojiden kamuya yönelik büyük ölçekli riski önlemek için doğru mekanizma değildir. Yetkin düzenleyici organlarla kapsamlı düzenleme, kamuya risk oluşturan diğer her büyük endüstride olduğu gibi AI için de gerekli olacaktır.[^123]

Diğer yaygın ama daha az akut riskleri önlemeye yönelik düzenleme, yargı yetkisinden yargı yetkisine formu değişiklik gösterecektir. Kritik olan, bu risklerin yönetilemez olacağı kadar riskli AI sistemlerinin geliştirilmesinden kaçınmaktır.

### Sonra ne olacak?

Önümüzdeki on yılda, AI daha yaygın hale geldikçe ve temel teknoloji ilerledikçe, iki temel şeyin olması muhtemeldir. İlk olarak, mevcut güçlü AI sistemlerinin düzenlenmesi daha zor, ancak daha da gerekli hale gelecektir. En azından büyük ölçekli güvenlik risklerini ele alan bazı önlemlerin uluslararası düzeyde anlaşma gerektirmesi, bireysel yargı yetkililerinin uluslararası anlaşmalara dayalı kuralları uygulaması muhtemeldir.

İkinci olarak, donanım daha ucuz ve daha maliyet etkin hale geldikçe eğitim ve işletim işlem gücü üst sınırlarını korumak zorlaşacaktır; algoritmalar ve mimarilerdeki ilerlemelerle birlikte daha az alakalı hale gelebilir (veya daha da sıkı olması gerekebilir).

AI'yı kontrol etmenin zorlaşacak olması pes etmemiz gerektiği anlamına gelmez! Bu makalede özetlenen planı uygulamak bize hem değerli zaman hem de süreci üzerinde kritik kontrol verecek, bu da AI'nın toplumumuza, uygarlığımıza ve türümüze yönelik varoluşsal riskinden kaçınmak için çok, çok daha iyi bir konuma getirecektir.

Daha uzun vadede, neye izin vereceğimize dair seçimler yapmamız gerekecek. Yine de gerçekten kontrol edilebilir bir YGZ formu yaratmayı seçebiliriz, bunun mümkün olduğu ölçüde. Veya dünyayı yönetmeyi makinelere bırakmanın daha iyi olduğuna karar verebiliriz, eğer bunu daha iyi yapacaklarına ve bize iyi davranacaklarına kendimizi ikna edebilirsek. Ancak bunlar AI konusunda derin bilimsel anlayış ile ve anlamlı küresel kapsayıcı tartışmadan sonra verilmesi gereken kararlar olmalı, insanlığın çoğu tamamen dahil olmadan ve fark etmeden teknoloji mogulları arasındaki yarışta değil.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) Sorumluluk ve düzenleme yoluyla Y-G-Z ve süper zeka yönetişiminin özeti. Sorumluluk en yüksek, düzenleme en güçlü, Özerklik, Genellik ve Zeka üçlü kesişiminde. Katı sorumluluk ve güçlü düzenlemeden güvenli limanlar, bir sistemin zayıf ve/veya dar ve/veya pasif olduğunu gösteren olumlu güvenlik davaları yoluyla elde edilebilir. Yasal ve donanım ile kriptografik güvenlik önlemleri kullanılarak doğrulanan ve uygulanan toplam Eğitim İşlem Gücü ve Çıkarım İşlem Gücü Hızı üst sınırları, tam YGZ'den kaçınarak ve süper zekayı etkili bir şekilde yasaklayarak güvenliği destekler.

[^87]: Büyük olasılıkla, bu farkındalığın yayılması ya bu davayı ortaya koyan eğitim ve savunuculuk gruplarının yoğun çabası ya da oldukça önemli bir AI kaynaklı felaket gerektirecektir. Bunun ilki olmasını umalım.

[^88]: Paradoksal olarak, Doğa'nın teknolojimizi özellikle bilimsel olarak geliştirmeyi çok zor hale getirerek sınırlamaya alıştık. Ancak bu artık AI için geçerli değil: temel bilimsel problemler beklenenden daha kolay çıkıyor. Doğa'nın bizi kendimizden kurtarmasına güvenemeyiz – bunu kendimiz yapmak zorunda kalacağız.

[^89]: Tam olarak nerede durup yeni sistemler geliştirmeyi bırakıyoruz? Burada, ihtiyatlılık ilkesini benimsemeliyiz. Bir sistem dağıtıldıktan, özellikle de o sistem yetenek seviyesi yaygınlaştıktan sonra geri almak son derece zordur. Ve eğer bir sistem *geliştirilirse* (özellikle büyük maliyet ve çabayla), kullanmak veya dağıtmak için muazzam baskı ve sızma veya çalınma ayartısı olacaktır. Sistemleri geliştirip *sonra* derinden güvensiz olup olmadığına karar vermek tehlikeli bir yoldur.

[^90]: Özünde tehlikeli olan AI geliştirmesini yasaklamak da akıllıca olurdu, örneğin kendi kendini çoğaltan ve evrimleşen sistemler, çevreyi terk etmek için tasarlananlar, özerk olarak kendini geliştirebilecek olanlar, kasıtlı olarak aldatıcı ve kötü niyetli AI vb.

[^91]: Bunun mutlaka küresel bir organ tarafından uluslararası düzeyde *uygulandığı* anlamına gelmediğini unutmayın: bunun yerine egemen ülkeler birçok anlaşmada olduğu gibi üzerinde anlaşılan kuralları uygulayabilir.

[^92]: Aşağıda göreceğimiz gibi, AI hesaplamasının doğası bir tür hibrite izin verir; ancak yine de uluslararası işbirliği gerekecektir.

[^93]: Örneğin, AI ile ilgili çipleri kazımak için gereken makineler (diğerlerinin de denememelerine rağmen) yalnızca bir firma olan ASML tarafından yapılmakta, ilgili çiplerin büyük çoğunluğu (diğerlerinin rekabet etmeye çalışmasına rağmen) tek bir firma olan TSMC tarafından üretilmekte ve bu çiplerden donanım tasarımı ve yapımı NVIDIA, AMD ve Google da dahil olmak üzere sadece birkaç firma tarafından yapılmaktadır.

[^94]: En önemlisi, her çip bir şeyleri "imzalamak" için kullanabileceği benzersiz ve erişilemez kriptografik özel anahtar tutar.

[^95]: Varsayılan olarak bu çipleri satan şirket olacaktır, ancak başka modeller mümkün ve potansiyel olarak faydalıdır.

[^96]: Bir yönetici, çipte imzalı mesajların değiş tokuşunu zamanlayarak çipin konumunu belirleyebilir: ışık hızının sonlu hızı, çipin, *r* / *c* 'den (burada *c* ışık hızıdır) daha az bir sürede imzalı mesajı döndürebiliyorsa verilen *r* yarıçapı içinde bir "istasyon"dan olmasını gerektirir. Birden fazla istasyon ve ağ özelliklerinin biraz anlaşılması kullanarak çipin konumu belirlenebilir. Bu yöntemin güzelliği güvenliğinin çoğunun fizik yasaları tarafından sağlanmasıdır. Diğer yöntemler GPS, atalet takibi ve benzer teknolojileri kullanabilir.

[^97]: Alternatif olarak, çip çiftlerinin birbirleriyle yalnızca yöneticinin açık izni ile iletişim kurmasına izin verilebilir.

[^98]: Bu kritiktir çünkü en azından şu anda büyük AI modellerini eğitmek için çipler arasında çok yüksek bant genişliği bağlantısı gereklidir.

[^99]: Bu aynı zamanda *N* / *M* farklı yöneticiden imzalı mesajları gerektirrecek şekilde kurulabilir, birden fazla tarafın yönetişimi paylaşmasına izin verir.

[^100]: Bu emsalsiz olmaktan uzaktır – örneğin ordular klonlanmış veya genetik olarak geliştirilmiş süper askerlerden oluşan ordular geliştirmemiştir, oysa bu muhtemelen teknolojik olarak mümkündür. Ancak diğerleri tarafından engellenmektense bunu yapmamayı *seçmişlerdir*. Büyük dünya güçlerinin güçlü bir şekilde geliştirmek istedikleri bir teknolojinin geliştirilmesini engellemek konusundaki sicil pek iyi değildir.

[^101]: Birkaç önemli istisna ile (özellikle NVIDIA) AI'ya özel donanım bu şirketlerin genel iş ve gelir modellerinin nispeten küçük bir parçasıdır. Üstelik, gelişmiş AI'da kullanılan donanım ile "tüketici sınıfı" donanım arasındaki fark önemlidir, bu nedenle bilgisayar donanımı tüketicilerinin çoğu büyük ölçüde etkilenmeyecektir.

[^102]: Daha ayrıntılı analiz için [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) ve [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips)'tan son raporlara bakın. Bunlar özellikle ABD ihracat kontrollerinin diğer ülkelerin yüksek uçlu hesaplamadaki kapasitesini kısıtlamaya çalışması bağlamında teknik fizibiliteye odaklanır; ancak bunun burada öngörülen küresel kısıtlamayla açık örtüşmesi vardır.

[^103]: Örneğin Apple cihazları, kayıp veya çalındı olarak bildirildiğinde uzaktan ve güvenli bir şekilde kilitlenir ve uzaktan yeniden etkinleştirilebilir. Bu, burada tartışılan aynı donanım güvenlik özelliklerine dayanır.

[^104]: Örneğin IBM'in [isteğe bağlı kapasite](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) teklifine, Intel'in [Intel isteğe bağlı](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html) hizmetine ve Apple'ın [özel bulut hesaplamaya](https://security.apple.com/blog/private-cloud-compute/) bakın.

[^105]: [Bu çalışma](https://epochai.org/trends#hardware-trends-section), tarihsel olarak aynı performansın yılda yaklaşık %30 daha az dolarla elde edildiğini gösterir. Bu eğilim devam ederse, AI ve "tüketici" çip kullanımı arasında önemli örtüşme olabilir ve genel olarak yüksek güçlü AI sistemleri için gereken donanım miktarı rahatsız edici derecede küçük hale gelebilir.

[^106]: [Aynı çalışmaya](https://epochai.org/trends#hardware-trends-section) göre, görüntü tanımada verilen performans her yıl 2,5 kat daha az hesaplama gerektirmiştir. Bu en yetenekli AI sistemleri için de geçerli olsaydı, hesaplama sınırı çok uzun süre faydalı olmayacaktı.

[^107]: Özellikle ülke düzeyinde bu, hükümetin hesaplama gücünün nasıl kullanıldığı üzerinde çok fazla kontrole sahip olacağı anlamında hesaplamanın millileştirilmesi gibi görünmektedir. Ancak hükümet katılımından endişe duyanlar için bu, en güçlü AI yazılımının *kendisinin* bazılarının savunmaya başladığı gibi büyük AI şirketleri ile ulusal hükümetler arasındaki birleşme yoluyla millileştirilmesinden çok daha güvenli ve tercih edilir görünmektedir.

[^108]: Avrupa'da 2024'te [AB AI Yasası](https://artificialintelligenceact.eu/) geçişiyle önemli bir düzenleyici adım atılmıştır. AI'yı riske göre sınıflandırır: kabul edilemez sistemleri yasaklar, yüksek riskli olanları düzenler, düşük riskli sistemlere şeffaflık kuralları veya hiç önlem uygulamaz. Bazı AI risklerini önemli ölçüde azaltacak ve ABD firmaları için bile AI şeffaflığını artıracak, ancak iki temel kusurunun vardır. İlki, sınırlı erişim: AB'de AI sağlayan herhangi bir şirkete uygulanırken, ABD merkezli firmalar üzerindeki icra zayıf ve askeri AI muaftır. İkinci olarak, GPAI'yı kapsarken, YGZ veya süper zekayı kabul edilemez risk olarak tanıyamaz veya gelişimlerini engelleyemez – yalnızca AB dağıtımlarını. Sonuç olarak, YGZ veya süper zeka risklerini freenleme konusunda çok az şey yapar.

[^109]: Şirketler genellikle makul düzenleme lehinde olduklarını beyan ederler. Ancak bir şekilde neredeyse her zaman herhangi *belirli* düzenlemeye karşı çıkarlar gibi görünürler; [çoğu AI şirketinin kamuya veya özel olarak karşı çıktığı](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/) oldukça hafif SB1047 üzerindeki mücadeleye tanık olun.

[^110]: AB AI yasasının önerildiği zamandan yürürlüğe girdiği zamana kadar yaklaşık 3,5 yıl geçti.

[^111]: Bazen AI'yı düzenlemeye başlamak için "çok erken" olduğu ifade edilir. Son not göz önünde bulundurulduğunda, bu pek olası görünmüyor. İfade edilen bir başka endişe ise düzenlemenin "inovasyona zarar vereceğidir." Ancak iyi düzenleme yalnızca yönü değiştirir, inovasyon miktarını değil.

[^112]: İlginç bir emsal, kaçıp zarar verebilecek tehlikeli malzemelerin taşınmasındadır. Burada [düzenleme](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) ve [içtihat](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) patlayıcılar, benzin, zehirler, bulaşıcı ajanlar ve radyoaktif atık gibi çok tehlikeli malzemeler için katı sorumluluk oluşturmuştur. Diğer örnekler arasında [farmasötiklerde uyarılar](https://www.medicalnewstoday.com/articles/boxed-warnings), [tıbbi cihaz sınıfları](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification) vb. yer alır.

[^113]: ["Dar Yol"](https://www.narrowpath.co/) adlı başka bir kapsamlı öneri benzer amaçlarla daha merkezileşmiş, yasak tabanlı bir yaklaşım savunur ve tüm sınır AI geliştirmesini güçlü uluslararası kurumlar tarafından gözetlenen tek bir uluslararası varlık aracılığıyla yönlendirir, kademelendirilmiş kısıtlamalar yerine açık kategorik yasaklarla. Ben de bu planı desteklerdim; ancak burada önerilen plandan daha fazla politik irade ve koordinasyon gerektirecektir.

[^114]: Böyle bir standart için bazı kılavuzlar [Frontier Model Forum](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/) tarafından yayınlandı. Buradaki öneriye göre, bunlar daha az kesinlik ve sayımda daha az hesaplama içerilmesi yönünde hata yaparlar.

[^115]: 2023 ABD AI yürütme emri (şimdi yürürlükten kaldırıldı) benzer ancak daha az ayrıntılı raporlama gerektiriyordu. Bu, yerine geçen emirle güçlendirilmelidir.

[^116]: Çok kabaca, şimdi yaygın olan H100 çipler için bu çıkarım yapan yaklaşık 1000'lik kümelere karşılık gelir; çıkarım yapan çok yeni son teknoloji NVIDIA B200 çiplerinin yaklaşık 100'üdür (yaklaşık 5 milyon USD değerinde). Her iki durumda da eğitim sayısı o kümenin birkaç ay hesaplama yapmasına karşılık gelir.

[^117]: Bu miktar şu anda eğitilmiş herhangi bir AI sisteminden daha büyüktür; AI yeteneğinin hesaplama ile nasıl ölçeklendiğini daha iyi anladıkça daha büyük veya daha küçük bir sayı haklı çıkabilir.

[^118]: Bu modelleri oluşturan ve sağlayan/barındıran taraflara uygulanır, son kullanıcılara değil.

[^119]: Kabaca, "katı" sorumluluk geliştiricilerin bir ürünün neden olduğu zararlardan *varsayılan olarak* sorumlu tutulması anlamına gelir ve "anormal derecede tehlikeli" ürünler için kullanılan bir standarttır ve (biraz eğlenceli ama uygun şekilde) vahşi hayvanlar. "Müşterek ve müteselsil" sorumluluk, sorumluluğun bir üründen sorumlu tüm taraflara atandığı ve bu tarafların kendi aralarında kimin ne sorumluluğu taşıyacağını çözmesi gerektiği anlamına gelir. Bu, uzun ve karmaşık değer zincirine sahip AI gibi sistemler için önemlidir.

[^120]: Standart kusur tabanlı tek taraf sorumluluk yeterli değildir: AI sistemleri karmaşık olduğu, işleyişi anlaşılmadığı ve tehlikeli bir sistem veya çıktının yaratılmasında birçok taraf yer alabileceği için kusuru hem izlemek hem de atamak zor olacaktır. Ayrıca davalar yargılanması yıllar alacak ve büyük olasılıkla bu şirketler için önemsiz olan para cezalarıyla sonuçlanacak, bu nedenle yöneticiler için kişisel sorumluluk da önemlidir.

[^121]: Açık ağırlık modelleri için güvenlik kriterlerinden muafiyet olmamalıdır. Ayrıca, riski değerlendirirken kaldırılabilecek koruyucuların yaygın olarak mevcut modellerden kaldırılacağı ve kapalı modellerin bile güvenli kalacaklarına dair çok yüksek güvence olmadıkça yayılacağı varsayılmalıdır.

[^122]: Burada önerilen şema genel yeteneğe göre tetiklenen düzenleyici incelemeye sahiptir; ancak bazı özellikle riskli kullanım durumlarının daha fazla incelemeyi tetiklemesi mantıklıdır – örneğin uzman bir viroloji AI sistemi, dar ve pasif olsa bile muhtemelen daha yüksek bir katmanda olmalıdır. Eski ABD yürütme emrinin biyolojik yetenekler için bu yapının bir kısmı vardı.

[^123]: İki açık örnek FAA ve FDA ile benzer kurumlar tarafından düzenlenen havacılık ve ilaçlar ile diğer ülkelerdeki benzer kurumlar. Bu kurumlar kusursuz değildir, ancak o endüstrilerin işleyişi ve başarısı için kesinlikle hayati olmuştur.

## Bölüm 9 - Geleceği tasarlamak — bunun yerine ne yapmalıyız

Yapay zeka dünyada inanılmaz iyilikler yapabilir. Riskleri olmadan tüm faydaları elde etmek için yapay zekanın bir insan aracı olarak kalmasını sağlamalıyız.

İnsanlığın makinelerle yer değiştirmemesini başarıyla seçersek – en azından bir süreliğine! – bunun yerine ne yapabiliriz? Yapay zekanın bir teknoloji olarak sunduğu büyük vaadi mi bırakıyoruz? Bir düzeyde cevap basit bir *hayır:* kontrol edilemeyen YGZ ve süper zekaya giden Kapıları kapatın, ama yapay zekanın diğer birçok biçimini *inşa edin*, ayrıca bunları yönetmek için ihtiyacımız olan yönetişim yapıları ve kurumları da geliştirin.

Ama hâlâ söylenecek çok şey var; bunu gerçekleştirmek insanlığın temel meşgalesi olurdu. Bu bölüm birkaç anahtar temayı keşfediyor:

- "Araç" YZ'yi nasıl karakterize edebileceğimiz ve alabileceği biçimler.
- İnsanlığın istediği (neredeyse) her şeyi YGZ olmadan, Araç YZ ile elde edebileceğimiz.
- Araç YZ sistemlerinin (muhtemelen, prensipte) yönetilebilir olduğu.
- YGZ'den uzaklaşmanın ulusal güvenlikte taviz vermek anlamına gelmediği – tam tersi.
- Güç yoğunlaşmasının gerçek bir kaygı olduğu. Bunu güvenlik ve emniyeti baltalamadan azaltabilir miyiz?
- Yeni yönetişim ve sosyal yapılara ihtiyacımız olacağı ve isteyeceğimiz, ve YZ'nin aslında buna yardımcı olabileceği.

### Kapılar içinde YZ: Araç YZ

Üçlü kesişim diyagramı "Araç YZ" diyebileceğimiz şeyi tanımlamak için iyi bir yol sunar: kontrol edilemeyen bir rakip ya da ikame değil, insan kullanımı için kontrol edilebilir bir araç olan YZ. En az sorunlu YZ sistemleri, otonom ama genel ya da süper yetenekli olmayan (bir açık artırma bot'u gibi), ya da genel ama otonom ya da yetenekli olmayan (küçük bir dil modeli gibi), ya da yetenekli ama dar ve çok kontrol edilebilir (AlphaGo gibi) sistemlerdir.[^124] İki kesişen özelliğe sahip olanlar daha geniş uygulama alanına sahiptir ama daha yüksek risklidir ve yönetilmek için büyük çabalar gerektirir. (Bir YZ sisteminin daha çok araç olması doğal olarak güvenli olduğu anlamına gelmez, sadece doğal olarak *güvensiz* olmadığı anlamına gelir – bir motorlu testere ile evcil bir kaplanı karşılaştırın.) Üçlü kesişimdeki (tam) YGZ ve süper zekaya Kapı kapalı kalmalı ve bu eşiğe yaklaşan YZ sistemleriyle çok dikkatli davranılmalıdır.

Ama bu çok güçlü YZ bırakıyor! Akıllı ve genel pasif "kehanetçiler" ve dar sistemlerden, insan düzeyinde ama insan üstü olmayan genel sistemlerden vb. büyük fayda sağlayabiliriz. Birçok teknoloji şirketi ve geliştirici aktif olarak bu tür araçlar inşa ediyor ve devam etmeli; çoğu insan gibi onlar da örtük olarak YGZ ve süper zekaya giden Kapıların kapatılacağını *varsayıyorlar*.[^125]

Ayrıca YZ sistemleri, yeteneği artırırken insan gözetimini koruyan bileşik sistemlerde etkili bir şekilde birleştirilebilir. Anlaşılamaz kara kutulara güvenmek yerine, hem YZ hem de geleneksel yazılım dahil birden fazla bileşenin insanların izleyip anlayabileceği şekillerde birlikte çalıştığı sistemler inşa edebiliriz.[^126] Bazı bileşenler kara kutu olsa da, hiçbiri YGZ'ye yakın olmayacaktır – sadece bir bütün olarak bileşik sistem hem oldukça genel hem de oldukça yetenekli olacaktır, ve kesinlikle kontrol edilebilir bir şekilde.[^127]

#### Anlamlı ve garantili insan kontrolü

"Kesinlikle kontrol edilebilir" ne demektir? "Araç" çerçevesinin temel fikri, oldukça genel ve güçlü olsalar bile, anlamlı insan kontrolü altında olmaları garantili sistemlere izin vermektir. Bu ne anlama gelir? İki yönü kapsar. İlki bir tasarım değerlendirmesi: insanlar sistemin yaptıklarının derininde ve merkezinde yer almalıdır, önemli kararları YZ'ye *devretmemelidir*. Bu mevcut YZ sistemlerinin çoğunun karakteridir. İkincisi, YZ sistemleri otonom olduğu ölçüde, eylem kapsamlarını sınırlayan garantilere sahip olmalıdır. Bir garanti, bir şeyin olma olasılığını karakterize eden bir *sayı* ve bu sayıya inanmak için bir sebep olmalıdır. Bu, diğer güvenlik kritik alanlarda talep ettiğimiz şeydir; burada "arızalar arası ortalama süre" ve beklenen kaza sayıları gibi sayılar hesaplanır, desteklenir ve güvenlik durumlarında yayınlanır.[^128] Arızalar için ideal sayı elbette sıfırdır. Ve iyi haber şu ki, oldukça farklı YZ mimarileri kullanarak, programların (YZ dahil) *biçimsel olarak doğrulanmış* özellikleri fikirlerini kullanarak oldukça yaklaşabiliriz. Omohundro, Tegmark, Bengio, Dalrymple ve diğerleri tarafından uzun uzadıya keşfedilen fikir ([buraya](https://arxiv.org/abs/2309.01933) ve [buraya](https://arxiv.org/abs/2405.06624) bakın) belirli özelliklere sahip bir program oluşturmak (örneğin: bir insanın onu kapatabilmesi) ve bu özelliklerin geçerli olduğunu biçimsel olarak *ispatlamaktır*. Bu şimdi oldukça kısa programlar ve basit özellikler için yapılabilir, ama YZ destekli ispat yazılımının (gelecekteki) gücü, çok daha karmaşık programlar (örn. sarmalayıcılar) ve hatta YZ'nin kendisi için bile buna izin verebilir. Bu çok iddialı bir program, ama Kapılara baskı arttıkça, onları güçlendirecek güçlü malzemelere ihtiyacımız olacak. Matematiksel ispat yeterince güçlü olanlardan biri olabilir.

#### YZ endüstrisi nereye gidiyor

YZ ilerlemesi yönlendirilse bile, Araç YZ hâlâ devasa bir endüstri olurdu. Donanım açısından, süper zekayı önlemek için işlem gücü sınırları olsa bile, daha küçük modellerde eğitim ve çıkarım hâlâ büyük miktarlarda özelleşmiş bileşen gerektirecektir. Yazılım tarafında, YZ model ve hesaplama boyutundaki patlamayı etkisiz hale getirmek, şirketlerin kaynaklarını sadece onları daha büyük yapmak yerine, daha küçük sistemleri daha iyi, daha çeşitli ve daha özelleşmiş hale getirmeye yönlendirmesine yol açmalıdır.[^129] Para kazanan tüm Silicon Valley girişimleri için – muhtemelen daha fazla – yer olacaktır.[^130]

### Araç YZ, YGZ olmadan insanlığın istediği (neredeyse) her şeyi verebilir

İster biyolojik ister makine olsun zeka, geniş anlamda bir dizi hedefe daha uygun gelecekler yaratacak faaliyetleri planlama ve yürütme yetisi olarak düşünülebilir. Bu nedenle zeka, akıllıca seçilmiş hedefler peşinde kullanıldığında muazzam fayda sağlar. Yapay zeka büyük zaman ve çaba yatırımları çekiyor, büyük ölçüde vaat ettiği faydalar nedeniyle. O halde şunu sormalıyız: eğer YZ'nin süper zekaya kaçışını sınırlandırırsak, YZ'nin faydalarını ne ölçüde elde edebiliriz? Cevap: şaşırtıcı derecede az kaybedebiliriz.

İlk olarak, mevcut YZ sistemlerinin zaten çok güçlü olduğunu ve onlarla yapılabileceklerin gerçekten sadece yüzeyini çizdiğimizi düşünün.[^131] Kendilerine sunulan bir soruyu ya da görevi "anlama" ve bu soruyu yanıtlamak ya da o görevi yapmak için ne gerekeceği konusunda "gösterinin başını çekme" konusunda oldukça yetenekliler.

İkinci olarak, modern YZ sistemleriyle ilgili heyecanın çoğu genellikleri nedeniyledir; ama en yetenekli YZ sistemlerinden bazıları – konuşma ya da görsel üretip tanıyan, bilimsel tahmin ve modelleme yapan, oyun oynayan vb. sistemler – çok daha dar ve hesaplama açısından "Kapılar içinde" kalan sistemlerdir.[^132] Bu sistemler yaptıkları belirli görevlerde insan üstüdürler. Darlıkları nedeniyle sınır durum[^133] (ya da [istismar edilebilir](https://arxiv.org/abs/2211.00241)) zayıflıkları olabilir; ancak *tamamen* dar ya da *tam* genel tek seçenekler değildir: arada birçok mimari vardır.[^134]

Bu YZ araçları, YGZ olmadan diğer olumlu teknolojilerde ilerlemeyi büyük ölçüde hızlandırabilir. Nükleer fiziği daha iyi yapmak için YZ'nin nükleer fizikçi olmasına ihtiyacımız yok – onlar var! Tıbbı hızlandırmak istiyorsak, biyologlar, tıp araştırmacıları ve kimyagerlere güçlü araçlar verelim. Onlar bunları istiyor ve muazzam kazanç için kullanacaklar. Bir milyon dijital dahiden oluşan sunucu çiftliğine ihtiyacımız yok; dahiliğini ortaya çıkarmaya YZ'nin yardım edebileceği milyonlarca insanımız var. Evet, ölümsüzlüğü ve tüm hastalıkların tedavisini elde etmek daha uzun sürecek. Bu gerçek bir maliyet. Ama en umut verici sağlık yeniliklerinin bile, YZ kaynaklı istikrarsızlık küresel çatışmaya ya da toplumsal çöküşe yol açarsa pek işe yaramayacağı. YZ destekli insanlara problemi önce deneme şansı vermeyi kendimize borçluyuz.

Ve diyelim ki, aslında YGZ'nin insanlığın Kapı-içi araçları kullanarak elde edemeyeceği büyük bir üstünlüğü var. YGZ ve süper zekayı *asla* inşa etmeyerek bunları kaybeder miyiz? Buradaki riskleri ve ödülleri tartarken, acele etmeye karşı beklemede muazzam asimetrik bir fayda var: garantili güvenli ve faydalı bir şekilde yapılabilene kadar bekleyebiliriz ve neredeyse herkes yine de ödülleri toplayabilir; acele edersek, OpenAI CEO'su Sam Altman'ın sözleriyle – [*hepimiz* için ışıklar söner.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

Ama YGZ olmayan araçlar potansiyel olarak bu kadar güçlüyse, onları yönetebilir miyiz? Cevap net bir... belki.

### Araç YZ sistemleri (muhtemelen, prensipte) yönetilebilir

Ama kolay olmayacak. Mevcut son teknoloji YZ sistemleri, insanları ve kurumları hedeflerine ulaşmada büyük ölçüde güçlendirebilir. Bu, genel olarak iyi bir şey! Ancak, bu tür sistemlerin – aniden ve toplumun uyum sağlaması için pek zaman olmadan – elimizde olmasının doğal dinamikleri, yönetilmesi gereken ciddi riskler sunuyor. Bir Kapı kapatması varsayarak, bu tür risklerin birkaç ana sınıfını ve nasıl azaltılabileceğini tartışmaya değer.

Risk sınıflarından biri, yüksek güçlü Araç YZ'nin daha önce bir kişi ya da kuruluşa bağlı olan bilgi ya da yeteneğe erişime izin vermesi, yüksek yetenek artı yüksek sadakat kombinasyonunu çok geniş bir aktör yelpazesinin erişimine sunmasıdır. Bugün, kötü niyetli bir kişi yeterli parayla kimyagerlerden oluşan bir takım kiralayıp yeni kimyasal silahlar tasarlayıp ürettebilir – ama o paraya sahip olmak ya da takımı bulup bir araya getirmek ve onları açıkça yasadışı, etik dışı ve tehlikeli bir şey yapmaya ikna etmek o kadar da kolay değil. YZ sistemlerinin böyle bir rol oynamasını önlemek için, tüm o sistemler ve onlara erişim sorumlu bir şekilde yönetildiği sürece, mevcut yöntemlerdeki iyileştirmeler yeterli olabilir.[^135] Öte yandan, güçlü sistemler genel kullanım ve değişiklik için piyasaya sürülürse, yerleşik güvenlik önlemleri muhtemelen kaldırılabilir. Bu sınıftaki riskleri önlemek için, nükleer, patlayıcı ve diğer tehlikeli teknolojilerin ayrıntılarına benzer kısıtlamalar – kamuya ne serbest bırakılabileceği konusunda güçlü kısıtlamalar gerekecek.[^136]

İkinci risk sınıfı, insanlar gibi davranan ya da insanları taklit eden makinelerin ölçeklenmesinden kaynaklanır. Bireysel insanlara zarar düzeyinde, bu riskler çok daha etkili dolandırıcılık, spam ve kimlik avı ile rıza dışı deepfake'lerin çoğalmasını içerir.[^137] Kolektif düzeyde, kamusal tartışma ve müzakere gibi temel sosyal süreçlerin bozulmasını, toplumsal bilgi ve bilgi toplama, işleme ve yayma sistemlerimizi, ve politik seçim sistemlerimizi içerir. Bu riski azaltmak muhtemelen şunları içerecektir: (a) YZ sistemlerinin insanları taklit etmesini kısıtlayan yasalar ve böyle taklit üreten YZ geliştiricilerini sorumlu tutan yasalar, (b) (sorumlu bir şekilde) üretilen YZ içeriğini tanımlayıp sınıflandıran filigran ve kaynak sistemi, ve (c) veriden (örn. kameralar ve kayıtlar) gerçekler, anlayış ve iyi dünya modelleri aracılığıyla güvenilir bir zincir oluşturabilen yeni sosyo-teknik epistemik sistemler.[^138] Bunların hepsi mümkün ve YZ bunun bazı kısımlarına yardımcı olabilir.

Üçüncü genel risk, görevlerin otomatikleştirildiği ölçüde, şu anda o görevleri yapan insanların emek olarak mali değerinin azalabilmesidir. Tarihsel olarak, görevleri otomatikleştirmek o görevlerle mümkün olan şeyleri daha ucuz ve bol hale getirirken, o görevleri daha önce yapan insanları otomatikleştirilmiş versiyonda hâlâ yer alanlar (genelde daha yüksek beceri/ücret düzeyinde) ve emeği daha az ya da az değerli olanlar şeklinde ayırmıştır. Net olarak hangi sektörlerde daha fazla ya da daha az insan emeğinin gerekli olacağını öngörmek zor – sonuçtaki daha büyük ama daha verimli sektörde. Paralel olarak, otomatikleştirme dinamiği eşitsizliği ve genel verimliliği artırma, belirli mal ve hizmetlerin maliyetini (verimlilik artışları yoluyla) azaltma ve diğerlerinin maliyetini ([maliyet hastalığı](https://en.wikipedia.org/wiki/Baumol_effect) yoluyla) artırma eğilimindedir. Eşitsizlik artışının olumsuz tarafındakiler için, belirli mal ve hizmetlerdeki maliyet düşüşünün diğerlerindeki artışı telafi edip etmediği ve genel refaha yol açıp açmadığı hiç net değil. Peki bu YZ için nasıl gidecek? İnsan entelektüel emeğinin genel YZ tarafından değiştirilmesinin göreli kolaylığı nedeniyle, insan rekabetçisi genel amaçlı YZ ile bunun hızlı bir versiyonunu bekleyebiliriz.[^139] YGZ'ye Kapıyı kapatırsak, toptan YZ ajanları tarafından değiştirilecek iş çok daha az olacak; ama yıllar içinde büyük işgücü yer değiştirmesi hâlâ olasıdır.[^140] Yaygın ekonomik acıyı önlemek için, hem bir tür evrensel temel varlık ya da gelir uygulamak, hem de otomatikleştirmesi daha zor olan insan merkezli emeği değerlendirip ödüllendirmeye yönelik kültürel değişim tasarlamak (ekonominin diğer kısımlarından itilen mevcut emek artışı nedeniyle emek fiyatlarının düşmesini görmek yerine) muhtemelen gerekli olacak. ["Veri saygınlığı"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society) (eğitim verisi üreticisi insanlara o verinin YZ'de yarattığı değer için otomatik telif hakkı verilen) gibi diğer yapılar da yardımcı olabilir. YZ tarafından otomatikleştirmenin ikinci bir potansiyel olumsuz etkisi de *uygunsuz* otomatikleştirmedir. YZ'nin sadece daha kötü iş yaptığı uygulamaların yanı sıra, bu YZ sistemlerinin ahlaki, etik ya da yasal ilkeleri ihlal etmesinin muhtemel olduğu uygulamaları – örneğin yaşam ölüm kararları ve yargı konularında – içerecektir. Bunlar mevcut yasal çerçevelerimizi uygulayıp genişleterek ele alınmalıdır.

Son olarak, kapı-içi YZ'nin önemli bir tehdidi kişiselleştirilmiş ikna, dikkat çekme ve manipülasyonda kullanılmasıdır. Sosyal medya ve diğer çevrimiçi platformlarda derinden yerleşik dikkat ekonomisinin (çevrimiçi hizmetlerin kullanıcı dikkati için şiddetle savaştığı) ve ["gözetim kapitalizmi"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism) sistemlerinin (kullanıcı bilgileri ve profillemenin dikkat metalaştırılmasına eklendiği) büyümesini gördük. Daha fazla YZ'nin her ikisinin hizmetine gireceği neredeyse kesin. YZ zaten bağımlılık yapan akış algoritmalarında yoğun olarak kullanılıyor, ama bu tek bir kişi tarafından zorla tüketilecek şekilde özelleştirilmiş, bağımlılık yapan YZ üretimi içeriğe evrilecek. Ve o kişinin girdisi, tepkileri ve verisi, kısır döngüyü sürdürmek için dikkat/reklam makinesine beslenecek. Ayrıca, teknoloji şirketleri tarafından sağlanan YZ yardımcıları daha fazla çevrimiçi yaşam için arayüz haline geldikçe, muhtemelen arama motorları ve akışların yerini ikna ve müşteri para kazanma mekanizması olarak alacaklar. Toplumumuzun bu dinamikleri şimdiye kadar kontrol etmedeki başarısızlığı iyi alamet değil. Bu dinamiğin bir kısmı mahremiyet, veri hakları ve manipülasyon konusundaki düzenlemeler yoluyla azalabilir. Problemin köküne daha çok gitmek, sadık YZ asistanları gibi farklı perspektifler gerektirebilir (aşağıda tartışıldığı gibi).

Bu tartışmanın sonucu umuttur: Kapı-içi araç tabanlı sistemler – en azından günümüzün en son teknoloji sistemleriyle güç ve yetenek açısından karşılaştırılabilir kaldıkları sürece – bunu yapmak için irade ve koordinasyon varsa muhtemelen yönetilebilir. YZ araçlarıyla güçlendirilmiş iyi insan kurumları[^141] bunu yapabilir. Bunu yapmada da başarısız olabiliriz. Ama daha güçlü sistemlere izin vermenin nasıl yardım edeceğini görmek zor – onları sorumluluğa koymak ve en iyisini ummak dışında.

### Ulusal güvenlik

Ulusal güvenlik ya da diğer motivasyonlarla güdülenen YZ üstünlüğü yarışları bizi güçü veren değil de emen kontrol edilmez güçlü YZ sistemlerine götürüyor. ABD ve Çin arasındaki bir YGZ yarışı, hangi ulusun süper zekayı önce elde edeceğini belirleyecek bir yarıştır.

Peki ulusal güvenlikten sorumlu olanlar bunun yerine ne yapmalı? Hükümetler kontrol edilebilir ve güvenli sistemler inşa etmede güçlü deneyime sahipler ve YZ'de bunu yapmaya odaklanmalı, ölçekte ve devlet onayıyla yapıldığında en iyi başarıya ulaşan türden altyapı projelerini desteklemeliler.

YGZ'ye yönelik pervasız bir "Manhattan projesi" yerine,[^142] ABD hükümeti kontrol edilebilir, güvenli, güvenilir sistemler için bir Apollo projesi başlatabilir. Bu örneğin şunları içerebilir:

- Güçlü YZ'nin hesaplama tarafını yönetmek için (a) çip-üstü donanım güvenlik mekanizmalarını ve (b) altyapıyı geliştirmeye yönelik büyük bir program. Bunlar ABD [CHIPS yasası](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) ve [ihracat kontrol rejimi](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion) üzerine inşa edilebilir.
- YZ sistemlerinin belirli özelliklerinin (kapatma düğmesi gibi) mevcut ya da mevcut olmadığının *ispatlanabilmesi* için biçimsel doğrulama tekniklerini geliştirmeye yönelik büyük ölçekli girişim. Bu, ispat geliştirmek için YZ'nin kendisinden yararlanabilir.
- Doğrulanabilir şekilde güvenli yazılım oluşturmaya yönelik ulus ölçeğinde çaba, mevcut yazılımları doğrulanabilir güvenli çerçevelere yeniden kodlayabilen YZ araçlarıyla desteklenerek.
- DOE, NSF ve NIH arasında ortaklık olarak yürütülen, YZ kullanarak bilimsel ilerlemeye yönelik ulusal yatırım projesi.[^143]

Genel olarak, toplumumuz üzerinde bizi YZ ve kötüye kullanımından kaynaklanan risklere karşı savunmasız bırakan muazzam bir saldırı yüzeyi var. Bu risklerin bazılarından korunmak hükümet boyutunda yatırım ve standardizasyon gerektirecek. Bunlar, YGZ yarışlarının ateşine benzin dökmekten çok daha fazla güvenlik sağlayacaktır. Ve eğer YZ silah sistemleri ve komuta-kontrol sistemlerine entegre edilecekse, YZ'nin güvenilir ve güvenli olması kritiktir ki mevcut YZ basitçe böyle değildir.

### Güç yoğunlaşması ve azaltılması

Bu makale YZ'nin insan kontrolü ve potansiyel başarısızlığı fikrine odaklandı. Ama YZ durumuna bakmanın başka geçerli bir mercek de *güç yoğunlaşması.* Çok güçlü YZ'nin geliştirilmesi, gücü ya da onu geliştirip kontrol edecek çok az sayıdaki çok büyük kurumsal ellere, ya da YZ'yi kendi güç ve kontrollerini sürdürmek için yeni araç olarak kullanan hükümetlere, ya da YZ sistemlerinin kendilerine yoğunlaştırma tehdidi oluşturuyor. Ya da yukarıdakilerin kutsal olmayan karışımına. Bu durumların herhangi birinde insanlığın çoğu güç, kontrol ve fail olmayı kaybeder. Bununla nasıl savaşabiliriz?

İlk ve en önemli adım elbette insandan daha akıllı YGZ ve süper zekaya Kapı kapatmaktır. Bunlar açıkça insanları ve insan gruplarını doğrudan değiştirebilir. Eğer kurumsal ya da hükümet kontrolü altındaysalar gücü o şirketlerde ya da hükümetlerde yoğunlaştıracak; "özgür"lerse gücü kendilerinde yoğunlaştıracaklardır. O halde Kapıların kapalı olduğunu varsayalım. O zaman ne?

Güç yoğunlaşmasına önerilen bir çözüm, model ağırlıklarının özgür ya da yaygın olarak erişilebilir olduğu "açık kaynak" YZ'dir. Ama daha önce bahsedildiği gibi, bir model açık hale geldiğinde, çoğu güvenlik önlemi ya da korkuluk çıkarılabilir (ve genelde çıkarılır). Bu nedenle bir yanda ademi merkeziyetçilik, diğer yanda güvenlik, emniyet ve YZ sistemlerinin insan kontrolü arasında akut gerginlik vardır. Açık modellerin kendi başlarına işletim sistemlerinde (Microsoft, Apple ve Google'ın açık alternatiflere rağmen hâlâ hâkim olduğu) olduğundan daha anlamlı şekilde YZ'de güç yoğunlaşmasıyla savaşacağından şüphe edilmesinin nedenleri de var.[^144]

Yine de bu çemberi kareleştirmenin yolları olabilir – yetenek ve ekonomik ödülü ademi merkezileştirirken riskleri merkezileştirmek ve azaltmak. Bu hem YZ'nin nasıl geliştirildiğini hem faydalarının nasıl dağıtıldığını yeniden düşünmeyi gerektirir.

Kamusal YZ geliştirme ve sahipliğinin yeni modelleri yardımcı olacaktır. Bu birkaç biçim alabilir: hükümet geliştirmesi YZ (demokratik gözetim altında),[^145] kâr amacı gütmeyen YZ geliştirme organizasyonları (tarayıcılar için Mozilla gibi), ya da çok yaygın sahiplik ve yönetimi sağlayan yapılar. Kilit nokta, bu kurumların güçlü güvenlik kısıtlamaları altında çalışırken kamu yararına hizmet etmek üzere açıkça yetkilendirilmiş olmalarıdır.[^146] İyi tasarlanmış düzenleyici ve standartlar/sertifikasyon rejimleri de hayati olacak, böylece canlı bir pazar tarafından sunulan YZ ürünleri kullanıcılarına karşı sömürücü olmaktan ziyade gerçekten faydalı kalacaktır.

Ekonomik güç yoğunlaşması açısından, ekonomik faydaların daha yaygın akmasını sağlamak için kaynak takibi ve "veri saygınlığı" kullanabiliriz. Özellikle, şimdiki YZ gücünün çoğu (ve Kapıları kapalı tutarsak gelecekte) doğrudan eğitim verisi ya da insan geri bildirimi olsun, insan üretimi veriden kaynaklanır. Eğer YZ şirketleri veri sağlayıcılara adilce tazminat ödemek zorunda kalsalar,[^147] bu en azından ekonomik ödülleri daha geniş dağıtmaya yardım edebilir. Bunun ötesinde, başka bir model büyük YZ şirketlerinin önemli kısımlarının kamu mülkiyeti olabilir. Örneğin, YZ şirketlerini vergilendirebilen hükümetler, makbuzların bir kısmını şirketlerde hisse tutan ve halka temettü ödeyen egemen servet fonuna yatırabilir.[^148]

Bu mekanizmalarda kritik olan, basitçe YZ-güdümlü güç yoğunlaşmasıyla YZ-dışı araçlar kullanarak savaşmak yerine, gücü daha iyi dağıtmaya yardım etmek için YZ'nin kendisinin gücünü kullanmaktır. Güçlü bir yaklaşım, kullanıcılarına gerçek güvene dayalı görev duygusu ile faaliyet gösteren – kullanıcıların çıkarlarını özellikle kurumsal sağlayıcıların çıkarlarının üstünde tutan – iyi tasarlanmış YZ asistanları aracılığıyla olacaktır.[^149] Bu asistanlar gerçekten güvenilir, teknik olarak yetkin ama kullanım durumuna ve risk düzeyine göre uygun şekilde sınırlı ve kamu, kâr amacı gütmeyen ya da sertifikalı kâr amaçlı kanallar aracılığıyla herkese yaygın olarak erişilebilir olmalıdır. Gizlice başka bir taraf için çıkarlarımıza karşı çalışan insan asistanı asla kabul etmeyeceğimiz gibi, kurumsal fayda için kullanıcılarını gözetleyen, manipüle eden ya da onlardan değer çıkaran YZ asistanları da kabul etmemeliyiz.

Böyle bir dönüşüm, bireylerin insan refahından çok değer çıkarmayı öncelendiren (YZ destekli) büyük kurumsal ve bürokratik makinelerle tek başlarına müzakere etmeye bırakıldıkları mevcut dinamiği temelden değiştirecektir. YZ-güdümlü gücü daha geniş dağıtmanın birçok olası yaklaşımı olsa da, hiçbiri varsayılan olarak ortaya çıkmayacaktır: güvene dayalı gereklilikler, kamu sağlama ve riske dayalı kademeli erişim gibi mekanizmalarla kasıtlı olarak tasarlanıp yönetilmelidirler.

Güç yoğunlaşmasını azaltmaya yönelik yaklaşımlar, yerleşik güçlerden önemli karşı rüzgarlarla karşılaşabilir.[^150] Ama güvenlik ve yoğunlaşmış güç arasında seçim yapmayı gerektirmeyen YZ geliştirme yolları vardır. Doğru kurumları şimdi inşa ederek, YZ'nin risklerini dikkatli yönetirken faydalarının yaygın paylaşılmasını sağlayabiliriz.

### Yeni yönetişim ve sosyal yapılar

Mevcut yönetişim yapılarımız zorlanıyor: tepki vermede yavaşlar, genellikle özel çıkarlar tarafından ele geçirilmişler ve [halk tarafından giderek güvenilmiyor.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) Yine de bu onları terk etmek için sebep değil – tam tersi. Bazı kurumların değişmesi gerekebilir, ama daha geniş olarak mevcut yapılarımızı geliştirebilip destekleyebilecek, hızla gelişen dünyamızda daha iyi işlev görmelerine yardım edecek yeni mekanizmalara ihtiyacımız var.

Kurumsal zayıflığımızın çoğu resmi hükümet yapılarından değil, bozulmuş sosyal kurumlardan kaynaklanıyor: paylaşılmış anlayış geliştirme, eylemi koordine etme ve anlamlı söylem yürütme sistemlerimizden. Şimdiye kadar YZ bu bozulmayı hızlandırdı, bilgi kanallarımızı üretilmiş içerikle doldurdu, bizi en kutuplaştırıcı ve bölücü içeriğe yönlendirdi ve gerçeği kurgulardan ayırt etmeyi zorlaştırdı.

Ama YZ aslında bu sosyal kurumları yeniden inşa etmeye ve güçlendirmeye yardım edebilir. Üç kritik alanı düşünün:

İlk olarak, YZ epistemik sistemlerimize – neyin doğru olduğunu bilme yollarımıza – güveni restore etmeye yardım edebilir. Ham veriden analiz yoluyla sonuçlara kadar bilginin kaynağını takip edip doğrulayan YZ destekli sistemler geliştirebiliriz. Bu sistemler, insanların sadece bir şeyin doğru olup olmadığını değil, nasıl doğru olduğunu bildiğimizi anlamalarına yardım etmek için kriptografik doğrulama ile sofistike analizi birleştirebilir.[^151] Sadık YZ asistanları ayrıntıları takip edip kontrol edildiğinden emin olmakla görevlendirilebilir.

İkinci olarak, YZ yeni büyük ölçekli koordinasyon biçimlerini mümkün kılabilir. En acil problemlerimizin çoğu – iklim değişikliğinden antibiyotik direncine – temelde koordinasyon problemleridir. [Neredeyse herkes için olabileceğinden daha kötü durumlarla karşı karşıyayız](https://equilibriabook.com/), çünkü hiçbir birey ya da grup ilk hareketi yapmayı göze alamaz. YZ sistemleri karmaşık teşvik yapılarını modelleyerek, daha iyi sonuçlara giden uygulanabilir yolları tanımlayarak ve oraya varmak için gerekli güven inşası ve taahhüt mekanizmalarını kolaylaştırarak yardım edebilir.

Belki de en흥미롭게, YZ tamamen yeni sosyal söylem biçimlerini mümkün kılabilir. Bir "şehirle konuşabilmeyi"[^152] düşünün – sadece istatistikleri görüntülemek değil, milyonlarca sakininin görüşleri, deneyimleri, ihtiyaçları ve özlemlerini işleyip sentezleyen YZ sistemiyle anlamlı diyalog kurmak. Ya da YZ'nin şu anda birbirlerini anlamamış konuşan gruplar arasında gerçek diyalogu nasıl kolaylaştırabileceğini düşünün, her tarafın birbirlerinin karikatürlerinden ziyade gerçek kaygı ve değerlerini daha iyi anlamalarına yardım ederek.[^153] Ya da YZ, insanlar ya da hatta büyük insan grupları (hepsi onunla doğrudan ve bireysel olarak etkileşime girebilir!) arasındaki anlaşmazlıkların becerikli, güvenilir tarafsız arabuluculuğunu sunabilir. Mevcut YZ bu işi yapmaya tamamen yetenekli, ama bunu yapacak araçlar kendiliğinden ya da piyasa teşvikleriyle ortaya çıkmayacak.

Bu olasılıklar, özellikle YZ'nin söylemi ve güveni bozuktaki mevcut rolü göz önüne alındığında ütopik görünebilir. Ama tam da bu yüzden bu olumlu uygulamaları aktif olarak geliştirmeliyiz. Kontrol edilemeyen YGZ'ye Kapıları kapatıp insan failliğini artıran YZ'yi öncelendirerek, teknolojik ilerlemeyi YZ'nin güçlendirme, direnç ve kolektif ilerleme gücü olarak hizmet ettiği bir geleceğe yönlendirebiliriz.


[^124]: Bununla birlikte, üçlü kesişimden uzak durmak ne yazık ki kişinin isteyebileceği kadar kolay değil. Üç yönden herhangi birindeki yeteneği çok sert zorlamak diğerlerinde de artırma eğilimindedir. Özellikle, kolayca otonom hale getirilemeyen son derece genel ve yetenekli bir zeka yaratmak zor olabilir. Bir yaklaşım modelleri ["miyopik"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) planlama yetisi engellenmiş sistemler olarak eğitmektir. Diğeri saf ["kehanetçi"](https://arxiv.org/abs/1711.05541) sistemlere odaklanmak olacaktır ki bunlar eylem odaklı sorulardan kaçınacaklardır.

[^125]: Birçok şirket onların da sonunda, daha uzun sürse bile, YGZ tarafından değiştirileceğini fark etmiyor – eğer fark etseler, o Kapılara biraz daha az bastırabilirler!

[^126]: YZ sistemleri daha verimli ama daha az anlaşılır şekillerde iletişim kurabilir, ama insan anlayışını sürdürmek öncelik almalıdır.

[^127]: Bu modüler, yorumlanabilir YZ fikri birkaç araştırmacı tarafından ayrıntıyla geliştirildi; örn. Drexler'in ["Kapsamlı YZ Hizmetleri"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) modeli, Dalrymple ve diğerlerinin ["Açık Ajans Mimarisi"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai). Bu tür sistemler büyük hesaplamayla eğitilen yekpare sinir ağlarından daha fazla mühendislik çabası gerektirebilse de, hesaplama limitlerinin yardımcı olduğu tam da burası – daha güvenli, daha şeffaf yolu aynı zamanda daha pratik hale getirerek.

[^128]: Genel olarak güvenlik durumları için [bu el kitabına](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16) bakın. YZ'ye özgü olarak, bkz. [Wasil ve diğerleri](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [Clymer ve diğerleri](https://arxiv.org/abs/2403.10462), [Buhl ve diğerleri](https://arxiv.org/abs/2410.21572), ve [Balesni ve diğerleri](https://arxiv.org/abs/2411.03336)

[^129]: Aslında sadece çıkarımın yüksek maliyeti tarafından güdülenerek bu eğilimi zaten görüyoruz: büyüklerden "damıtılmış" ve daha az pahalı donanımda çalışabilen daha küçük ve daha özelleşmiş modeller.

[^130]: YZ teknoloji ekosistemi konusunda heyecanlı olanların, endüstrilerine ağır düzenleme olarak gördükleri şeye neden karşı çıktıklarını anlıyorum. Ama açıkçası, diyelim bir risk sermayedarının neden YGZ ve süper zekaya kaçışa izin vermek isteyeceği beni şaşırtıyor. Bu sistemler (ve şirketler, şirket kontrolünde kaldıkları sürece) *tüm girişimleri atıştırmalık olarak yiyecekler*. Muhtemelen diğer endüstrileri yemekten *önce*. Gelişen bir YZ ekosistemine yatırım yapan herkes, YGZ gelişiminin birkaç baskın oyuncu tarafından tekelleşmeye yol açmamasını sağlamaya öncelik vermelidir.

[^131]: Ekonomist ve eski Deepmind araştırmacısı Michael Webb'in [dediği gibi](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/), "Eğer bugün daha büyük dil modellerinin tüm gelişimini durdursak, yani GPT-4 ve Claude ve her neyse, ve bunlar o boyutta eğittiğimiz son şeyler olsalar – yani o boyutta daha fazla iterasyona ve her türlü ince ayara izin veriyoruz, ama bundan daha büyük hiçbir şey yok, daha büyük ilerlemeler yok – sadece bugün sahip olduklarımızın 20-30 yıllık inanılmaz ekonomik büyümeyi güçlendirmeye yeteceğini düşünüyorum."

[^132]: Örneğin, DeepMind'ın alphafold sistemi GPT-4'ün FLOP sayısının sadece 100.000'de birini kullandı.

[^133]: Sürücüsüz arabaların zorluğu burada dikkat çekicidir: nominal olarak dar bir görev olsa ve nispeten küçük YZ sistemleriyle adil güvenilirlikle başarılabilse de, böyle güvenlik kritik bir görevde gerekli güvenilirlik düzeyine ulaşmak için kapsamlı gerçek dünya bilgisi ve anlayışı gereklidir.

[^134]: Örneğin, bir hesaplama bütçesi verildiğinde, muhtemelen o bütçenin (diyelim) yarısında önceden eğitilmiş GPAI modelleri görürüz ve diğer yarı daha dar görev yelpazesinde çok yüksek yetenek eğitmek için kullanılır. Bu, neredeyse insan genel zekası tarafından desteklenen insan üstü dar yetenek verecektir.

[^135]: Mevcut baskın hizalama tekniği "insan geri bildirimiyle pekiştirmeli öğrenme" [(RLHF)](https://arxiv.org/abs/1706.03741) ve YZ modelinin pekiştirmeli öğrenmesi için bir ödül/ceza sinyali oluşturmak üzere insan geri bildirimi kullanır. Bu ve [anayasal YZ](https://arxiv.org/abs/2212.08073) gibi ilgili teknikler şaşırtıcı derecede iyi çalışıyor (güçlü olmamalarına ve mütevazı çabayla atlatılabilmelerine rağmen.) Ayrıca, mevcut dil modelleri genelde sağduyu mantığında, aptalca ahlaki hatalar yapmayacak kadar yeterli. Bu bir çeşit tatlı nokta: insanların ne istediğini anlayacak kadar akıllı (tanımlanabildiği ölçüde), ama ayrıntılı aldatmacalar planlamak ya da yanlış anladıklarında büyük zarar vermeye yetecek kadar akıllı değil.

[^136]: Uzun vadede, geliştirilen herhangi bir YZ yetenek düzeyi muhtemelen yayılacaktır, çünkü nihayetinde yazılımdır ve faydalıdır. Bu tür sistemlerin oluşturduğu risklere karşı savunmak için sağlam mekanizmalara sahip olmamız gerekecek. Ama *şimdi buna sahip değiliz*, bu nedenle güçlü YZ modellerinin ne kadar yayılmasına izin verileceği konusunda çok ölçülü olmalıyız.

[^137]: Bunların büyük çoğunluğu küçükler dahil rıza dışı pornografik deepfake'lerdir.

[^138]: Bu tür çözümlerin birçok bileşeni mevcut, "bot-mu-değil-mi" yasaları (AB YZ yasası ve başka yerlerde), [endüstri kaynak-takip teknolojileri](https://c2pa.org/), [yenilikçi haber toplayıcıları](https://www.improvethenews.org/), tahmin [toplayıcıları](https://metaculus.com/) ve piyasalar vb. biçiminde.

[^139]: Otomatikleştirme dalgası önceki kalıpları takip etmeyebilir, çünkü kaliteli yazma, hukuk yorumlama ya da tıbbi tavsiye verme gibi nispeten *yüksek*-beceri görevler, daha düşük-beceri görevlerden fazla hatta daha fazla otomatikleştirmeye karşı savunmasız olabilir.

[^140]: YGZ'nin ücretler üzerindeki etkisinin dikkatli modellemesi için [buradaki](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek) rapora, ve sıkıcı ayrıntılar için [buraya](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0), Anton Korinek ve çalışma arkadaşlarından. İşlerin daha fazla parçası otomatikleştikçe, verimlilik ve ücretlerin arttığını buluyorlar – bir noktaya kadar. *Çok* fazlası otomatikleştiğinde, verimlilik artmaya devam eder, ama insanlar verimli YZ tarafından toptan değiştirildiği için ücretler çöker. Kapıları kapatmanın çok kullanışlı olmasının nedeni budur: kaybolan insan ücretleri olmadan verimlilik elde ederiz.

[^141]: YZ'nin korumaları ve yönetimi daha sağlam hale getirecek "savunma" teknolojileri olarak ve inşasında yardım etmek üzere kullanılabileceğinin birçok yolu vardır. Bu "D/acc" gündemini tanımlayan [bu](https://vitalik.eth.limo/general/2025/01/05/dacc2.html) etkili yazıya bakın.

[^142]: Biraz ironik olarak, ABD Manhattan projesi muhtemelen YGZ'ye zaman çizelgelernin hızlandırılması için çok az şey yapacaktır – YZ ilerlemesindeki insan ve mali yatırım kadranı zaten 11'e sabitlenmiş durumda. Birincil sonuçlar benzer bir projeye Çin'i (ulusal düzey altyapı projelerinde mükemmel olan) teşvik etmek, YZ'nin riskini sınırlayan uluslararası anlaşmaları çok daha zor hale getirmek ve Rusya gibi ABD'nin diğer jeopolitik düşmanlarını alarma geçirmek olacaktır.

[^143]: ["Ulusal YZ Araştırma Kaynağı"](https://nairrpilot.org/) programı bu yönde iyi bir mevcut adımdır ve genişletilmelidir.

[^144]: Teknoloji ürünlerinde "açık"ın çeşitli anlamları ve bazılarının nasıl daha fazla değil de daha az hâkimiyet yerleşikliğine yol açtığının [bu analizine](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807) bakın.

[^145]: ABD'de [Ulusal YZ Araştırma Kaynağı](https://nairratdoe.ornl.gov/) planları ve yakında başlatılan [Avrupa YZ Vakfı](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/) bu yönde ilginç adımlardır.

[^146]: Buradaki meydan okuma teknik değil kurumsal – kamu yararı YZ gelişiminin neye benzeyebileceğinin gerçek dünya örneklerine ve deneylerine acilen ihtiyacımız var.

[^147]: Bu mevcut büyük teknoloji iş modellerine aykırıdır ve hem yasal eylem hem de yeni normlar gerektirecektir.

[^148]: Sadece bazı hükümetler bunu yapabilecek. Daha radikal bir fikir [tüm insanların ortak sahipliği altında bu türde evrensel bir fon.](https://futureoflife.org/project/the-windfall-trust/)

[^149]: Bu durumun uzun bir açıklaması için YZ sadakati üzerine [bu makaleye](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338) bakın. Ne yazık ki YZ asistanlarının varsayılan yörüngesi muhtemelen artan şekilde sadakatsiz olacağı yönünde.

[^150]: Biraz ironik olarak, birçok yerleşik güç de YZ destekli güçten alma riskinde; ama süreç oldukça ilerlemediği sürece bunu algılamaları zor olabilir.

[^151]: Bu yöndeki bazı ilginç çabalar kriptografik doğrulama üzerine [c2pa koalisyonu](https://c2pa.org/); daha iyi haber epistemikleri üzerine [Verity](https://www.improvethenews.org/) ve [Ground news](https://ground.news/); ve yanıltıcı tahminlerde söylemi temellendirme üzerine [Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com) ve tahmin piyasalarıyla temsil ediliyor.

[^152]: [Bu](https://talktothecity.org/) büyüleyici pilot projeye bakın.

[^153]: Bkz. [Kialo](https://www.kialo-edu.com/), ve bazı örnekler için [Kollektif Zeka Projesi](https://www.cip.org/) çabaları.

## Bölüm 10 - Önümüzdeki seçim

İnsani geleceğimizi korumak için YGZ ve superintellijansa giden Kapıları kapatmayı seçmeliyiz.

İnsanlığın Dünya'yı konuşan, düşünen, teknoloji üreten ve genel amaçlı problem çözen başka zihinlerle paylaştığı son dönem, 40.000 yıl önce buzul çağı Avrupa'sıydı. O diğer zihinler, kısmen ya da tamamen bizim çabalarımız sonucunda yok oldu.

Şimdi böyle bir döneme yeniden giriyoruz. Kültürümüzün ve teknolojimizin en gelişmiş ürünleri - tüm internet bilgi ortaklığımızdan oluşturulmuş veri setleri ve şimdiye kadar yarattığımız en karmaşık teknolojiler olan 100 milyar elemanlı çipler - gelişmiş genel amaçlı AI sistemlerini hayata geçirmek için bir araya getiriliyor.

Bu sistemlerin geliştiricileri onları insan güçlendirmesi için birer araç olarak göstermeye can atıyorlar. Ve gerçekten de öyle olabilirler. Ama şunu unutmayın: mevcut yörüngemiz giderek daha güçlü, hedef odaklı, karar veren ve genel yetenekli dijital ajanlar inşa etmek yönünde. Bunlar şimdiden geniş bir entelektüel görev yelpazesinde birçok insan kadar iyi performans gösteriyorlar, hızla gelişiyorlar ve kendi gelişimlerine katkıda bulunuyorlar.

Bu yörünge değişmediği ya da beklenmedik bir engelle karşılaşmadığı sürece, çok yakında - on yıllar değil, yıllar içinde - tehlikeli derecede güçlü dijital zekalara sahip olacağız. En *iyi* senaryolarda bile bunlar büyük ekonomik faydalar getirecek (en azından bazılarımıza) ama ancak toplumumuzda derin bir yıkım pahasına ve insanları yaptığımız en önemli şeylerin çoğunda yerini alarak: bu makineler bizim yerimize düşünecek, bizim yerimize plan yapacak, bizim yerimize karar verecek ve bizim yerimize yaratacak. Şımartılmış olurduk, ama şımartılmış çocuklar gibi. Çok daha olası olan ise, bu sistemlerin insanları yaptığımız hem olumlu *hem de* olumsuz şeylerde - sömürü, manipülasyon, şiddet ve savaş dahil - yerini alması. Bunların AI ile hiper güçlendirilmiş versiyonlarına karşı dayanabilir miyiz? Son olarak, işlerin hiç de iyi gitmeyeceği oldukça makul bir olasılık: nispeten kısa sürede sadece yaptıklarımızda değil, *olduğumuz* şeyde - medeniyetin ve geleceğin mimarları olarak - yerini alınmamız. Neandertallere sorun bu nasıl gidiyor diye. Belki biz de onlara bir süre ekstra biblo sağlamıştık.

*Bunu yapmak zorunda değiliz.* İnsan rekabetçisi AI'ya sahibiz ve *rekabet edemeyeceğimiz* AI inşa etmeye gerek yok. Halef bir tür yaratmadan harika AI araçları inşa edebiliriz. YGZ ve superintellijansın kaçınılmaz olduğu düşüncesi *kader kılığına girmiş bir seçim*dir.

Bazı sert, küresel sınırlar koyarak AI'nın genel yeteneğini yaklaşık insan seviyesinde tutarken, bilgisayarların verileri bizim yapamayacağımız şekillerde işleme ve hiçbirimizin yapmak istemediği görevleri otomatikleştirme yetisinin faydalarından yararlanmaya devam edebiliriz. Bunlar yine de birçok risk taşıyacak, ama iyi tasarlanıp yönetilirse, tıptan araştırmaya, tüketici ürünlerinden diğer alanlara kadar insanlık için muazzam bir nimet olacak.

Sınır koymak uluslararası işbirliği gerektirecek, ama düşünülenden daha az, ve bu sınırlar ham güç peşinde koşmak yerine insan refahını artıran uygulamalara odaklanan muazzam bir AI ve AI donanımı endüstrisi için hâlâ bol yer bırakacak. Ve eğer güçlü güvenlik garantileriyle ve anlamlı bir küresel diyalogdan sonra daha ileri gitmek istersek, bu seçenek de elimizde olmaya devam edecek.

İnsanlık YGZ ve superintellijansa giden Kapıları kapatmayı *seçmeli*.

Geleceği insani tutmak için.

### Yazardan Bir Not

Bu konuyu bizimle birlikte keşfetmeye zaman ayırdığınız için teşekkür ederim.

Bu makaleyi yazdım çünkü bir bilim insanı olarak süslenmemiş gerçeği söylemenin önemli olduğunu düşünüyorum ve bir insan olarak dünyayı değiştirecek bir meselenin üstesinden gelmek için hızlı ve kararlı hareket etmemizin kritik olduğunu hissediyorum: insandan daha akıllı AI sistemlerinin geliştirilmesi.

Bu olağanüstü duruma bilgelikle yanıt vereceksek, YGZ ve superintellijansın çıkarlarımızı güvence altına almak için 'mutlaka' inşa edilmesi gerektiği ya da 'kaçınılmaz' olduğu ve durdurulamayacağı yönündeki hakim anlatıyı eleştirel bir gözle incelemeye hazır olmalıyız. Bu anlatılar bizi güçsüzleştiriyor, önümüzdeki alternatif yolları görmemizi engelliyor.

Umuyorum ki pervasızlık karşısında temkinli olmaya ve açgözlülük karşısında cesur olmaya çağırmda bana katılacaksınız.

Umuyorum ki insani bir gelecek çağrımda bana katılacaksınız.

*– Anthony*

![Anthony Aguirre signature](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAnthony-Aguirre-signature-300x84.png&w=3840&q=75)

## Ekler

YGZ güvenliği ve güvenlik standartları için katmanlı yaklaşım, hesaplama muhasebesi teknik detayları, 'kapı kapatma' örnek uygulaması ve katı YGZ sorumluluk rejimi detayları dahil olmak üzere ek bilgiler.

### Ek A: Hesaplama muhasebesi teknik detayları

Anlamlı hesaplama tabanlı kontroller için hem "gerçek değer" hem de eğitim ve çıkarımda kullanılan toplam işlem gücü için iyi yaklaşımlar içeren detaylı bir yöntem gereklidir. İşte "gerçek değerin" teknik düzeyde nasıl hesaplanabileceğine dair bir örnek.

**Tanımlar:**

*Hesaplama nedensel grafiği:* Bir AI modelinin belirli bir çıktısı O için, o hesaplamanın sonucunu değiştirmenin potansiyel olarak O'yu değiştirebileceği bir dizi dijital hesaplama vardır. (Bu muhafazakar olarak varsayılmalıdır, yani bir hesaplamanın hem zamanda daha önce gerçekleşen hem de fiziksel olarak potansiyel nedensel etki yolu olan bir öncülden bağımsız olduğuna inanmak için açık bir neden olmalıdır.) Bu, çıkarım sırasında AI modeli tarafından yapılan hesaplamayı ve ayrıca modelin girdi, veri hazırlama ve eğitimine giren hesaplamaları içerir. Bunların herhangi biri bir AI modelinden gelen çıktı olabileceği için, bu özyinelemeli olarak hesaplanır ve bir insanın girdide önemli bir değişiklik sağladığı yerde kesilir.

*Eğitim İşlem Gücü:* Bir sinir ağının hesaplama nedensel grafiğinde yer alan toplam işlem gücü (FLOP veya diğer birimlerle), (veri hazırlama, eğitim ve ince ayar ve diğer tüm hesaplamalar dahil.)

*Çıktı İşlem Gücü:* Belirli bir AI çıktısının hesaplama nedensel grafiğindeki toplam işlem gücü, tüm sinir ağları (ve Eğitim İşlem Gücü dahil) ve o çıktıya giren diğer hesaplamalar dahil.

*Çıkarım İşlem Gücü Oranı:* Bir dizi çıktıda, çıktılar arasındaki Çıktı İşlem Gücünün değişim oranı (FLOP/s veya diğer birimlerle), yani bir sonraki çıktıyı üretmek için kullanılan işlem gücünün çıktılar arasındaki zaman aralığına bölünmesi.

**Örnekler ve yaklaşımlar:**

- İnsan tarafından oluşturulan veriler üzerinde eğitilmiş tek bir sinir ağı için, Eğitim İşlem Gücü geleneksel olarak rapor edildiği şekliyle toplam eğitim işlem gücüdür.
- Sabit bir hızda çıkarım yapan böyle bir sinir ağı için, Çıkarım İşlem Gücü Oranı yaklaşık olarak çıkarımı gerçekleştiren hesaplama kümesinin FLOP/s cinsinden toplam hesaplama hızıdır.
- Model ince ayarı için, tamamlanmış modelin Eğitim İşlem Gücü, ince ayar yapılmamış modelin Eğitim İşlem Gücü artı ince ayar sırasında yapılan hesaplama ve ince ayarda kullanılan verileri hazırlamak için yapılan hesaplamadır.
- Damıtılmış bir model için, tamamlanmış modelin Eğitim İşlem Gücü hem damıtılmış modelin hem de sentetik veri veya diğer eğitim girdisi sağlamak için kullanılan daha büyük modelin eğitimini içerir.
- Birkaç model eğitilir ancak birçok "deneme" insan yargısına dayanarak atılırsa, bunlar tutulan modelin Eğitim veya Çıktı İşlem Gücüne sayılmaz.

### Ek B: Kapı kapatma örnek uygulaması

**Uygulama Örneği:** İşte eğitim için 10<sup>27</sup> FLOP ve çıkarım için 10<sup>20</sup> FLOP/s (AI'yı çalıştırma) sınırı verildiğinde bir kapı kapatmanın nasıl işleyebileceğine dair bir örnek:

**1. Duraklama:** Ulusal güvenlik gerekçeleriyle, ABD Yürütme organı ABD merkezli, ABD'de iş yapan veya ABD'de üretilen çipler kullanan tüm şirketlerden 10<sup>27</sup> FLOP Eğitim İşlem Gücü sınırını aşabilecek yeni AI eğitim çalışmalarını durdurmasını talep eder. ABD, AI geliştirme faaliyeti yürüten diğer ülkelerle görüşmelere başlamalı, onları benzer adımlar atmaya güçlü bir şekilde teşvik etmeli ve uymamaları durumunda ABD duraksatmasının kaldırılabileceğini belirtmelidir.

**2. ABD gözetimi ve lisanslama:** Yürütme emri veya mevcut bir düzenleyici kurumun eylemiyle, ABD (diyelim ki) bir yıl içinde şunları gerektirir:

- ABD'de faaliyet gösteren şirketler tarafından yapılan 10<sup>25</sup> FLOP üzerindeki tüm AI eğitim çalışmalarının bir ABD düzenleyici kurumu tarafından tutulan bir veritabanına kaydedilmesi. (Not: Bunun biraz daha zayıf bir versiyonu, 10<sup>26</sup> FLOP üzerindeki modeller için kayıt gerektiren, artık yürürlükten kaldırılmış 2023 ABD AI yürütme emrinde zaten yer almıştı.)
- ABD'de faaliyet gösteren veya ABD hükümetiyle iş yapan tüm AI ilgili donanım üreticilerinin, özel donanımları ve onu çalıştıran yazılım üzerinde bir dizi gerekliliğe uyması. (Bu gerekliliklerin çoğu mevcut donanıma yazılım ve aygıt yazılımı güncellemeleri ile dahil edilebilir, ancak uzun vadeli ve sağlam çözümler sonraki donanım nesillerde değişiklikler gerektirir.) Bunlar arasında, donanım 10<sup>18</sup> FLOP/s hesaplama yürütme kapasitesine sahip yüksek hızlı birbirine bağlı bir kümenin parçasıysa, hem telemetri alan hem de ek hesaplama gerçekleştirme talepleri alan uzaktan bir "yönetici"den düzenli izin içeren daha yüksek bir doğrulama seviyesi gerektirir.
- Sorumlu taraf, donanımında gerçekleştirilen toplam hesaplamayı ABD veritabanını tutan kuruma bildirir.
- Hem daha güvenli hem de daha esnek gözetim ve izin vermeye olanak sağlamak için daha güçlü gereklilikler aşamalı olarak devreye girer.

**3. Uluslararası gözetim:**

- ABD, Çin ve gelişmiş çip üretim kapasitesine sahip diğer ülkeler uluslararası bir anlaşma müzakere eder.
- Bu anlaşma, Uluslararası Atom Enerjisi Ajansına benzer şekilde AI eğitimi ve yürütmesini denetlemekle görevli yeni bir uluslararası ajans oluşturur.
- İmzacı ülkeler, yerel AI donanım üreticilerinin ABD'de uygulananlar kadar güçlü bir dizi gereklilik ile uyum sağlamasını zorunlu kılmalıdır.
- Sorumlu taraflar artık AI hesaplama sayılarını hem kendi ülkelerindeki ajanslara hem de uluslararası ajans içindeki yeni ofise bildirmek zorundadır.
- Ek ülkeler mevcut uluslararası anlaşmaya katılmaya güçlü bir şekilde teşvik edilir: imzacı ülkeler tarafından ihracat kontrolleri imzacı olmayanlara yüksek kaliteli donanım erişimini kısıtlarken imzacılar AI sistemlerini yönetmede teknik destek alabilir.

**4. Uluslararası doğrulama ve yaptırım:**

- Donanım doğrulama sistemi, hesaplama kullanımını hem orijinal sorumlu tarafa hem de doğrudan uluslararası ajans ofisine bildirecek şekilde güncellenir.
- Ajans, uluslararası anlaşma imzacıları ile görüşme yoluyla, imzacı ülkelerde hukuki güç kazanan hesaplama sınırları üzerinde anlaşmaya varır.
- Paralel olarak, bir hesaplama eşiğinin üzerinde (ancak sınırın altında) AI eğitimi ve çalıştırılmasının bu standartlara uymasının gerekli olacağı bir dizi uluslararası standart geliştirilebilir.
- Ajans, gerekirse daha iyi algoritmalar vb. için tazmin etmek üzere hesaplama sınırını düşürebilir. Veya güvenli ve tavsiye edilir görülürse (ispatlanabilir güvenlik garantileri düzeyinde) hesaplama sınırını yükseltebilir.

### Ek C: Katı YGZ sorumluluk rejimi detayları

**Katı YGZ sorumluluk rejimi detayları**

- Son derece genel, yetenekli ve otonom olan gelişmiş bir AI sisteminin oluşturulması ve işletilmesi "anormal derecede tehlikeli" bir faaliyet olarak kabul edilir.
- Bu nedenle, bu tür sistemleri eğitme ve işletme için varsayılan sorumluluk, model veya çıktıları/eylemleri tarafından yapılan herhangi bir zarar için katı, müşterek ve müteselsil sorumluluktur (veya ABD dışı eşdeğeri).
- Ağır ihmal veya kasıtlı suistimal durumlarında yönetici ve yönetim kurulu üyeleri için kişisel sorumluluk uygulanacaktır. Bu, en ağır durumlar için cezai yaptırımları içermelidir.
- Sorumluluğun insanların ve şirketlerin normal olarak tabi olacağı varsayılan (ABD'de kusur temelli) sorumluluğa döndüğü çok sayıda güvenli liman vardır.
	- Bazı hesaplama eşiğinin altında eğitilmiş ve işletilen modeller (bu, yukarıda açıklanan üst sınırlardan en az 10 kat daha düşük olacaktır.)
	- "Zayıf" olan AI (kabaca, amaçlandığı görevlerde insan uzman seviyesinin altında) ve/veya
	- "Dar" olan AI (özel olarak tasarlandığı ve eğitildiği sabit ve oldukça sınırlı görev ve işlem kapsamına sahip) ve/veya
	- "Pasif" olan AI (mütevazi değişiklik altında bile - doğrudan insan katılımı ve kontrolü olmadan eylemler gerçekleştirme veya karmaşık çok adımlı görevler yapma yeteneği çok sınırlı.)
	- Güvenli, emniyetli ve kontrol edilebilir olması garanti edilen AI (kanıtlanabilir şekilde güvenli veya risk analizi ihmal edilebilir düzeyde beklenen zarar gösterir.)
- Güvenli limanlar, AI geliştiricisi tarafından hazırlanan ve bir ajans tarafından yetkilendirilmiş bir ajans veya denetçi tarafından onaylanan [güvenlik vakası](https://arxiv.org/abs/2410.21572) temelinde talep edilebilir. Hesaplamaya dayalı güvenli liman talep etmek için geliştirici sadece toplam Eğitim İşlem Gücü ve maksimal Çıkarım Oranı için güvenilir tahminler sunmalıdır.
- Mevzuat, yüksek kamu zararı riski olan AI sistemlerinin geliştirilmesinden ihtiyati tedbir kararının uygun olacağı durumları açıkça belirtecektir.
- Şirket konsorsiyumları, STK'lar ve devlet kurumları ile çalışarak, bu terimleri, düzenleyicilerin güvenli limanları nasıl vereceğini, AI geliştiricilerinin güvenlik vakalarını nasıl geliştireceğini ve mahkemelerin güvenli limanların proaktif olarak talep edilmediği durumlarda sorumluluğu nasıl yorumlayacağını tanımlayan standartlar ve normlar geliştirmelidir.

### Ek D: YGZ güvenliği ve güvenlik standartları için katmanlı yaklaşım

**YGZ güvenliği ve güvenlik standartları için katmanlı yaklaşım**

| Risk Katmanı | Tetikleyici(ler) | Eğitim gereklilikleri | Dağıtım gerekliliği |
| --- | --- | --- | --- |
| RT-0 | Otonomi, genellik ve zeka açısından zayıf AI | hiçbiri | hiçbiri |
| RT-1 | Otonomi, genellik ve zeka açısından birinde güçlü AI | hiçbiri | Risk ve kullanıma dayalı, potansiyel olarak modelin kullanılabileceği yerlerde ulusal otoriteler tarafından onaylanmış güvenlik vakaları |
| RT-2 | Otonomi, genellik ve zeka açısından ikisinde güçlü AI | Geliştirici üzerinde yargı yetkisi olan ulusal otoriteye kayıt | Büyük zarar riskini yetkili seviyeler altında sınırlayan güvenlik vakası artı modelin kullanılabileceği yerlerde ulusal otoriteler tarafından onaylanmış bağımsız güvenlik denetimleri (black-box ve white-box red teaming dahil) |
| RT-3 | Otonomi, genellik ve zeka açısından güçlü YGZ | Geliştirici üzerinde yargı yetkisi olan ulusal otorite tarafından güvenlik ve emniyet planının ön onayı | Büyük zarar riskini yetkili seviyeler altında sınırlayan güvenlik vakası ve siber güvenlik, kontrol edilebilirlik, çıkarılamaz öldürme anahtarı, insan değerleriyle hizalama ve kötüye kullanıma karşı sağlamlık dahil gerekli spesifikasyonların garanti edilmesi. |
| RT-4 | 10<sup>27</sup> FLOP Eğitim veya 10<sup>20</sup> FLOP/s Çıkarımı aşan herhangi bir model | Uluslararası olarak kararlaştırılan hesaplama üst sınırının kaldırılması beklenene kadar yasak | Uluslararası olarak kararlaştırılan hesaplama üst sınırının kaldırılması beklenene kadar yasak |

Yüksek otonomi, genellik ve zeka kombinasyonlarının yanı sıra hesaplama eşiklerine dayalı katmanlar içeren risk sınıflandırmaları ve güvenlik/emniyet standartları:

- *Güçlü otonomi*, sistem önemli insan gözetimi veya müdahalesi olmadan gerçek dünyada alakalı olan çok adımlı görevleri yapabiliyorsa ve/veya karmaşık eylemleri gerçekleştirebiliyorsa veya kolayca bunu yapmak için hazırlanabiliyorsa geçerlidir. Örnekler: otonom araçlar ve robotlar; finansal ticaret botları. Olmayan örnekler: GPT-4; görüntü sınıflandırıcıları
- *Güçlü genellik* geniş uygulama kapsamını, modelin kasıtlı ve spesifik olarak eğitilmediği görevleri yapmasını ve önemli yeni görev öğrenme yeteneğini gösterir. Örnekler: GPT-4; mu-zero. Olmayan örnekler: AlphaFold; otonom araçlar; görüntü üreticileri
- *Güçlü zeka*, modelin en iyi performans gösterdiği görevlerde insan uzman seviyesi performansına eşlik etmeye karşılık gelir (ve genel bir model için geniş görev yelpazesinde.) Örnekler: AlphaFold; mu-zero; o3. Olmayan örnekler: GPT-4; Siri

### Teşekkürler

Keep The Future Human'a katkıda bulunan kişilere birkaç teşekkür.

Bu çalışma yazarın görüşlerini yansıtmaktadır ve Yaşamın Geleceği Enstitüsü'nün (Future of Life Institute) resmi pozisyonu olarak alınmamalıdır (her ne kadar uyumlu olsalar da; resmi pozisyonu için [bu sayfaya](https://futureoflife.org/our-position-on-ai/) bakınız) veya yazarın bağlı olduğu herhangi bir başka kuruluşun resmi pozisyonu olarak görülmemelidir.

El yazmasına yaptıkları yorumlar için Mark Brakel, Ben Eisenpress, Anna Hehir, Carlos Gutierrez, Emilia Javorsky, Richard Mallah, Jordan Scharnhorst, Elyse Fulcher, Max Tegmark ve Jaan Tallinn'e; bazı referanslarla yardımı için Tim Schrier'e; diyagramların güzelleştirilmesi için Taylor Jones ve Elyse Fulcher'a minnettarım.

Bu çalışma, oluşturulma sürecinde üretken yapay zeka modellerinden (Claude ve ChatGPT) sınırlı düzeyde yararlanmıştır; bazı düzenleme ve saldırı testleri için kullanılmıştır. Yaratıcı eserlerde yapay zeka katılımının yerleşik standart seviyelerine göre, bu çalışma muhtemelen 10'da 3 puan alacaktır. (Aslında böyle bir standart yoktur! Ama olmalıdır.)

Makalenin bu web versiyonunu üreterek okuma ve makalede gezinme deneyimini oldukça keyifli hale getiren [Julius Odai](https://www.linkedin.com/in/julius-odai/)'ye çok minnettarız. Julius bir teknoloji uzmanı ve BlueDot Impact Yapay Zeka Yönetişimi kursunun yeni mezunlarından biridir.