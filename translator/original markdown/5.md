# Chapter 5 - At the threshold

The path from today's AI systems to fully-fledged AGI seems shockingly short and predictable.

The past ten years have seen dramatic advances in AI driven by huge [computational](https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year), human, and [fiscal](https://arxiv.org/abs/2405.21015) resources. Many narrow AI applications are better than humans at their assigned tasks, and are certainly far faster and cheaper.[^1] And there are also narrow super-human agents that can trounce all people at narrow-domain games such as [Go](https://www.nature.com/articles/nature16961), [Chess](https://arxiv.org/abs/1712.01815), and [Poker](https://www.deepstack.ai/), as well as more [general agents](https://deepmind.google/discover/blog/a-generalist-agent/) that can plan and execute actions in simplified simulated environments as effectively as humans can.

Most prominently, current general AI systems from OpenAI/Microsoft, Google/Deepmind, Anthropic/Amazon, Facebook/Meta, X.ai/Tesla and others [^2] have emerged since early 2023 and steadily (though unevenly) increased their capabilities since then. All of these have been created via token-prediction on huge text and multimedia datasets, combined with extensive reinforcement feedback from humans and other AI systems. Some of them also include extensive tool and scaffold systems.

## Strengths and weaknesses of current general systems

These systems perform well across an increasingly broad range of tests designed to measure intelligence and expertise, with progress that has surprised even experts in the field:

- When first released, GPT-4 [matched or exceeded typical human performance](https://arxiv.org/abs/2303.08774) on standard academic tests including SATs, GRE, entrance exams, and bar exams. More recent models likely perform significantly better, though results are not publicly available.
- The Turing test – long considered a key benchmark for "true" AI – is now routinely passed in some forms by modern language models, both informally and in [formal studies](https://arxiv.org/abs/2405.08007).[^3]
- On the comprehensive MMLU benchmark spanning 57 academic subjects, [recent models achieve domain-expert level scores](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu) (∼90%) [^4]
- Technical expertise has advanced dramatically: The GPQA benchmark of graduate-level physics saw [performance jump](https://epoch.ai/data/ai-benchmarking-dashboard) from near-random guessing (GPT-4, 2022) to expert level (o1-preview, 2024).
- Even tests specifically designed to be AI-resistant are falling: OpenAI's O3 [reportedly](https://www.nextbigfuture.com/2024/12/openai-releases-o3-model-with-high-performance-and-high-cost.html) solves the ARC-AGI abstract problem-solving benchmark at human level, achieves top-expert coding performance, and scores 25% on Epoch AI's "frontier math" problems designed to challenge elite mathematicians.[^5]
- The trend is so clear that MMLU's developer has now created ["Humanity's Last Exam"](https://agi.safe.ai/) – an ominous name reflecting the possibility that AI will soon surpass human performance on any meaningful test. As of writing this, there are claims of AI systems achieving 27% (according to [Sam Altman](https://x.com/sama/status/1886220281565381078)) and 35% (according to [this paper](https://arxiv.org/abs/2502.09955)) on this extremely difficult exam. It is quite unlikely that any individual human could do so.

Despite these impressive numbers (and their obvious intelligence when one interacts with them) [^6] there are many things (at least the released versions of) these neural networks *cannot* do. Currently most are disembodied – existing only on servers – and process at most text, sound and still images (but not video.) Crucially, most cannot carry out complex planned activities requiring high accuracy.[^7] And there are a number of other qualities strong in high-level human cognition currently low in released AI systems.

The following table lists a number of these, based on mid-2024 AI systems such as GPT-4o, Claude 3.5 Sonnet, and Google Gemini 1.5.[^8] The key question for how rapidly general AI will become more powerful is: to what degree will just doing *more of the same* produce results, versus adding additional but *known* techniques, versus developing or implementing *really new* AI research directions. My own predictions for this are given in the table, in terms of how likely each of these scenarios is to get that capability to and beyond human level.

<table><tbody><tr><th>Capability</th><th>Description of capability</th><th>Status/prognosis</th><th>Scaling/known/new</th></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td colspan="4"><em>Core Cognitive Capabilities</em></td></tr><tr><td>Reasoning</td><td>People can do accurate, multistep reasoning, following rules and checking accuracy.</td><td>Dramatic recent progress using extended chain-of-thought and retraining</td><td>95/5/5</td></tr><tr><td>Planning</td><td>People exhibit long-term and hierarchical planning.</td><td>Improving with scale; can be strongly aided using scaffolding and better training techniques.</td><td>10/85/5</td></tr><tr><td>Truth-grounding</td><td>GPAIs confabulate ungrounded information to satisfy queries.</td><td>Improving with scale; calibration data available within model; can be checked/improved via scaffolding.</td><td>30/65/5</td></tr><tr><td>Flexible problem-solving</td><td>Humans can recognize new patterns and invent new solutions to complex problems; current ML models struggle.</td><td>Improves with scale but weakly; may be solvable with neurosymbolic or generalized “search” techniques.</td><td>15/75/10</td></tr><tr><td colspan="4"><em>Learning and Knowledge</em></td></tr><tr><td>Learning &amp; memory</td><td>People have working, short-term, and long-term memory, all of which are dynamic and inter-related.</td><td>All models learn during training; GPAIs learn within context window and during fine-tuning; “continual learning” and other techniques exist but not yet integrated into large GPAIs.</td><td>5/80/15</td></tr><tr><td>Abstraction &amp; recursion</td><td>People can map and transfer relation sets into more abstract ones for reasoning and manipulation, including recursive “meta” reasoning.</td><td>Weakly improving with scale; could emerge in neurosymbolic systems.</td><td>30/50/20</td></tr><tr><td>World model(s)</td><td>People have and continually update a predictive world model within which they can solve problems and do physical reasoning</td><td>Improving with scale; updating tied to learning; GPAIS weak in real-world prediction.</td><td>20/50/30</td></tr><tr><td colspan="4"><em>Self and Agency</em></td></tr><tr><td>Agency</td><td>People can take actions in order to pursue goals, based on planning/prediction.</td><td>Many ML systems are agentic; LLMs can be made agents via wrappers.</td><td>5/90/5</td></tr><tr><td>Self-direction</td><td>People develop and pursue their own goals, with internally-generated motivation and drive.</td><td>Largely composed of agency plus originality; likely to emerge in complex agential systems with abstract goals.</td><td>40/45/15</td></tr><tr><td>Self-reference</td><td>People understand and reason about themselves as situated within an environment/context.</td><td>Improving with scale and could be augmented with training reward.</td><td>70/15/15</td></tr><tr><td>Self-awareness</td><td>People have knowledge of and can reason regarding their own thoughts and mental states.</td><td>Exists in some sense in GPAIs, which can arguably pass the classic “mirror test” for self-awareness. Can be improved with scaffolding; but unclear if this is enough.</td><td>20/55/25</td></tr><tr><td colspan="4"><em>Interface and Environment</em></td></tr><tr><td>Embodied intelligence</td><td>People understand and actively interact with their real-world environment.</td><td>Reinforcement learning works well in simulated and real-world (robotic) environments and can be integrated into multimodal transformers.</td><td>5/85/10</td></tr><tr><td>Multi-sense processing</td><td>People integrate and real-time process visual, audio, and other sensory streams.</td><td>Training in multiple modalities appears to “just work,” and improve with scale. Realtime video processing is difficult but e.g. self-driving systems are rapidly improving.</td><td>30/60/10</td></tr><tr><td colspan="4"><em>Higher-order Capabilities</em></td></tr><tr><td>Originality</td><td>Current ML models are creative in transforming and combining existing ideas/works, but people can build new frameworks and structures, sometimes tied to their identity.</td><td>Can be hard to discern from “creativity,” which may scale into it; may emerge from creativity plus self-awareness.</td><td>50/40/10</td></tr><tr><td>Sentience</td><td>People experience qualia; these can be positive, negative or neutral valence; it is “like something” to be a person.</td><td>Very difficult and philosophically fraught to determine whether a given system has this.</td><td>5/10/85</td></tr></tbody></table>

Key capabilities currently below human expert level in modern GPAI systems, grouped by type. The third column summarizes current status. Final column shows predicted likelihood (%) that human-level performance will be achieved through: scaling current techniques / combining with known techniques / developing new techniques. These capabilities are not independent, and increase in any one typically goes along with increases in others. Note that not all (particularly sentience) are necessary for AI systems capable of advancing AI development, highlighting the possibility of powerful but non-sentient AI.

Breaking down what is "missing" in this way makes it fairly clear that we are quite on-track for broadly above-human intelligence by scaling existing or known techniques.[^9]

There could still be surprises. Even putting aside "sentience," there could be some of the listed core cognitive capabilities that really can't be done with current techniques and require new ones. But consider this. The present effort being put forth by many of the world's largest companies amounts to multiple times the Apollo project's and tens of times the Manhattan project's spend,[^10] and is employing thousands of the very top technical people at unheard of salaries. The dynamics of the past few years have now brought to bear more human intellectual firepower (with AI now being added) to this than any endeavor in history. We should not bet on failure.

## The big target: generalist autonomous agents

The development of general AI over the past several years has focused on creating general and powerful but tool-like AI: it functions primarily as a (fairly) loyal assistant, and generally does not take actions on its own. This is partly by design, but largely because these systems have simply not been competent enough at the relevant skills to be entrusted with complex actions.[^11]

AI companies and researchers are, however, increasing [shifting focus](https://www.axios.com/2025/01/23/davos-2025-ai-agents) toward *autonomous* expert-level general-purpose agents.[^12] This would allow the systems to act more like a human assistant to which the user can delegate real actions.[^13] What will that take? A number of the capabilities in the "what's missing" table are implicated, including strong truth-grounding, learning and memory, abstraction and recursion, and world-modeling (for intelligence), planning, agency, originality, self-direction, self-reference, and self-awareness (for autonomy), and multi-sense-processing, embodied intelligence, and flexible problem-solving (for generality).[^14]

This triple-intersection of high autonomy (independence of action), high generality (scope and task breadth) and high intelligence (competence at cognitive tasks) is currently unique to humans. It is implicitly what many probably have in mind when they think of AGI – both in terms of its value as well as its risks.

This provides another way to define A-G-I as ***A*** utonomous- ***G*** eneral- ***I*** ntelligence, and we'll see that this triple intersection provides a very valuable lens for high-capability systems both in understanding their risks and rewards, and in governance of AI.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Simple-1024x1024.png&w=3840&q=75) The transformative A-G-I power and risk zone emerges from the intersection of three key properties: high Autonomy, high Intelligence at tasks, and high Generality.

## The AI (self-)improvement cycle

A final crucial factor in understanding AI progress is AI's unique technological feedback loop. In developing AI, success – in both demonstrated systems and deployed products – brings additional investment, talent, and competition, and we are currently in the midst of an enormous AI hype-plus-reality feedback loop that is driving hundreds of billions, or even trillions, of dollars in investment.

This type of feedback cycle could happen with any technology, and we've seen it in many, where market success begets investment, which begets improvement and better market success. But AI development goes further, in that now AI systems are helping to develop new and more powerful AI systems.[^15] We can think of this feedback loop in five stages, each with a shorter timescale than the last, as shown in the table.

*The AI improvement cycle operates across multiple timescales, with each stage potentially accelerating subsequent stages. Earlier stages are well underway, while later stages remain speculative but could proceed very rapidly once unlocked.*

Several of these stages are already underway, and a couple clearly getting started. The last stage, in which AI systems autonomously improve themselves, has been a staple of the literature on the risk of very powerful AI systems, and for good reason.[^16] But it is important to note that it is just the most drastic form of a feedback cycle that has already started and could lead to more surprises in the rapid advancement of the technology.


[^1]: You use a lot more of this AI than you probably think, driving speech generation and recognition, image processing, newsfeed algorithms, etc.

[^2]: While relationships between these pairs of companies are quite complex and nuanced, I have explicitly listed them to indicate both the vast overall market capitalization of firms now enjoined in AI development, and also that behind even "smaller" companies like Anthropic sit enormously deep pockets via investments and major partnership deals.

[^3]: It has become fashionable to disparage the Turing test, but it is quite powerful and general. In weak versions it indicates whether typical people interacting with an AI (which is trained to act human) in typical ways for brief periods can tell whether it is an AI. They cannot. Second, a highly adversarial Turing test can probe essentially any element of human capability and intelligence – by e.g. comparing an AI system to a human expert, evaluated by other human experts. There is a sense in which much of AI evaluation is a generalized form of Turing test.

[^4]: This is per domain – no human could plausibly achieve such scores across all subjects simultaneously.

[^5]: These are problems that would take even excellent mathematicians substantial time to solve, if they could solve them at all.

[^6]: If you are of a skeptical bent, retain your skepticism but really take the most current models for a spin, as well as try for yourself some of the test questions they can pass. As a physics professor, I would predict with near certainty that, for example, the top models would pass the graduate qualifying exam in our department.

[^7]: This and other weaknesses like confabulation have slowed market adoption and led to a gap between perceived and claimed capabilities (which must also be viewed through the lens of intense market competition and the need to attract investment.) This has confused both the public and policymakers about the actual state of AI progress. While perhaps not matching the hype, the progress is very real.

[^8]: The major advance since then has been development of systems trained for top-quality reasoning, leveraging more computation during inference and greater reinforcement learning. Because these models are new and their capabilities less tested, I've not wholly revamped this table except for "reasoning", which I regard as essentially solved. But I have updated predictions based on experienced and reported capabilities of those systems.

[^9]: Previous waves of AI optimism in the 1960s and 1980s ended in "AI winters" when promised capabilities failed to materialize. However, the current wave differs fundamentally in having achieved superhuman performance in many domains, backed by massive computational resources and commercial success.

[^10]: The full Apollo project [cost about $250bn USD in 2020 dollars](https://www.planetary.org/space-policy/cost-of-apollo), and the Manhattan project [less than a tenth that](https://www.brookings.edu/the-costs-of-the-manhattan-project/). Goldman Sachs [projects a trillion dollars of spend just on AI data centers](https://www.datacenterdynamics.com/en/news/goldman-sachs-1tn-to-be-spent-on-ai-data-centers-chips-and-utility-upgrades-with-little-to-show-for-it-so-far/) over the next few years.

[^11]: Although humans make plenty of mistakes, we underestimate just how reliable we can be! Because probabilities multiply, a task requiring 20 steps to do correctly requires each step to be 97% reliable just to get it done right half the time. We do such tasks all the time.

[^12]: A strong move in this direction has very recently been taken with OpenAI's ["Deep Research"](https://openai.com/index/introducing-deep-research/) assistant that autonomously performs general research, described as "a new agentic capability that conducts multi-step research on the internet for complex tasks."

[^13]: Things like fill in that pesky PDF form, book flights, etc. But with a PhD in 20 fields! So also: write that thesis for you, negotiate that contract for you, prove that theorem for you, create that ad campaign for you, etc. What do *you* do? You tell it what to do, of course.

[^14]: Note that sentience is *not* clearly required, nor does AI in this triple-intersection necessarily imply it.

[^15]: The closest analogy here is perhaps chip technology, where development has maintained Moore's law for decades, as computer technologies help people design the next generation of chip technology. But AI will be far more direct.

[^16]: It's important to let it sink in for a moment that AI could – soon – be improving itself on a timescale of days or weeks. Or less. Keep this in mind when someone tells you an AI capability is definitely far away.
