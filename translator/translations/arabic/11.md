# الملاحق

المعلومات التكميلية، بما في ذلك - التفاصيل التقنية حول محاسبة القوة الحاسوبية، مثال لتطبيق "إغلاق البوابة"، تفاصيل نظام المسؤولية المطلقة للذكاء الاصطناعي العام، ونهج متدرج لمعايير سلامة وأمن الذكاء الاصطناعي العام.

## الملحق أ: التفاصيل التقنية لمحاسبة القوة الحاسوبية

يتطلب تطبيق ضوابط فعالة قائمة على القوة الحاسوبية وضع منهجية مفصلة لحساب "الحقيقة المطلقة" بالإضافة إلى تقديرات دقيقة للقوة الحاسوبية الإجمالية المستخدمة في التدريب والاستنتاج. فيما يلي مثال على كيفية حساب "الحقيقة المطلقة" على المستوى التقني.

**التعريفات:**

*الرسم البياني السببي للحوسبة:* لمخرج معين O من نموذج ذكاء اصطناعي، هناك مجموعة من العمليات الحاسوبية الرقمية التي قد يؤدي تغيير نتيجتها إلى تغيير O. (يجب افتراض هذا بحذر، أي يجب وجود سبب واضح للاعتقاد بأن عملية حاسوبية مستقلة عن سابق يحدث قبلها زمنياً وله مسار سببي فيزيائي محتمل للتأثير.) يشمل ذلك الحوسبة التي يقوم بها نموذج الذكاء الاصطناعي أثناء الاستنتاج، بالإضافة إلى العمليات الحاسوبية التي دخلت في إعداد المدخلات والبيانات وتدريب النموذج. نظراً لأن أياً من هذه قد تكون بحد ذاتها مخرجات من نموذج ذكاء اصطناعي، يتم حسابها بشكل تكراري، مع التوقف عند نقطة قدم فيها الإنسان تغييراً كبيراً للمدخل.

*القوة الحاسوبية للتدريب:* إجمالي القوة الحاسوبية، بوحدة FLOP أو وحدات أخرى، التي ينطوي عليها الرسم البياني السببي للحوسبة لشبكة عصبية (شاملاً إعداد البيانات والتدريب والضبط الدقيق وأي عمليات حاسوبية أخرى.)

*القوة الحاسوبية للمخرجات:* إجمالي القوة الحاسوبية في الرسم البياني السببي للحوسبة لمخرج معين من الذكاء الاصطناعي، شاملاً جميع الشبكات العصبية (بما في ذلك قوتها الحاسوبية للتدريب) والعمليات الحاسوبية الأخرى التي تدخل في ذلك المخرج.

*معدل القوة الحاسوبية للاستنتاج:* في سلسلة من المخرجات، معدل التغير (بوحدة FLOP/s أو وحدات أخرى) في القوة الحاسوبية للمخرجات بين المخرجات، أي القوة الحاسوبية المستخدمة لإنتاج المخرج التالي، مقسومة على الفترة الزمنية بين المخرجات.

**أمثلة وتقديرات:**

- بالنسبة لشبكة عصبية واحدة مدربة على بيانات أنشأها الإنسان، القوة الحاسوبية للتدريب هي ببساطة إجمالي القوة الحاسوبية للتدريب كما يتم الإبلاغ عنها عادة.
- بالنسبة لمثل هذه الشبكة العصبية التي تقوم بالاستنتاج بمعدل ثابت، فإن معدل القوة الحاسوبية للاستنتاج يعادل تقريباً إجمالي سرعة مجموعة الحوسبة التي تؤدي الاستنتاج بوحدة FLOP/s.
- بالنسبة للضبط الدقيق للنموذج، تُحسب القوة الحاسوبية للتدريب للنموذج الكامل بجمع القوة الحاسوبية للتدريب للنموذج غير المضبوط دقيقاً مع الحوسبة المنجزة أثناء الضبط الدقيق ولإعداد أي بيانات مستخدمة في الضبط الدقيق.
- بالنسبة للنموذج المقطر، تشمل القوة الحاسوبية للتدريب للنموذج الكامل تدريب كل من النموذج المقطر والنموذج الأكبر المستخدم لتوفير البيانات الاصطناعية أو مدخلات التدريب الأخرى.
- إذا تم تدريب عدة نماذج، لكن تم استبعاد العديد من "المحاولات" بناءً على الحكم البشري، فإن هذه لا تحسب ضمن القوة الحاسوبية للتدريب أو المخرجات للنموذج المحتفظ به.

## الملحق ب: مثال لتطبيق إغلاق البوابة

**مثال للتطبيق:** فيما يلي مثال واحد لكيفية عمل إغلاق البوابة، بحد أقصى 10<sup>27</sup> FLOP للتدريب و10<sup>20</sup> FLOP/s للاستنتاج (تشغيل الذكاء الاصطناعي):

**1. التوقف:** لأسباب الأمن القومي، تطلب السلطة التنفيذية الأمريكية من جميع الشركات المتمركزة في الولايات المتحدة، أو التي تمارس أعمالها في الولايات المتحدة، أو التي تستخدم رقائق مصنعة في الولايات المتحدة، التوقف عن أي عمليات تدريب جديدة للذكاء الاصطناعي قد تتجاوز حد القوة الحاسوبية للتدريب البالغ 10<sup>27</sup> FLOP. يجب على الولايات المتحدة بدء المناقشات مع الدول الأخرى التي تستضيف تطوير الذكاء الاصطناعي، مع تشجيعها بقوة على اتخاذ خطوات مماثلة والإشارة إلى أن التوقف الأمريكي قد يُرفع في حال اختاروا عدم الامتثال.

**2. الإشراف والترخيص الأمريكي:** من خلال أمر تنفيذي أو إجراء من وكالة تنظيمية قائمة، تطالب الولايات المتحدة بأنه خلال (مثلاً) سنة واحدة:

- يجب تسجيل جميع عمليات تدريب الذكاء الاصطناعي المقدرة بأكثر من 10<sup>25</sup> FLOP التي تقوم بها الشركات العاملة في الولايات المتحدة في قاعدة بيانات تحتفظ بها وكالة تنظيمية أمريكية. (ملاحظة: نسخة أضعف قليلاً من هذا كانت مدرجة بالفعل في الأمر التنفيذي الأمريكي لعام 2023 حول الذكاء الاصطناعي الذي تم إلغاؤه لاحقاً، والذي يتطلب التسجيل للنماذج التي تتجاوز 10<sup>26</sup> FLOP.)
- يجب على جميع مصنعي الأجهزة ذات الصلة بالذكاء الاصطناعي العاملين في الولايات المتحدة أو الذين يمارسون أعمالهم مع الحكومة الأمريكية الالتزام بمجموعة من المتطلبات على أجهزتهم المتخصصة والبرمجيات التي تشغلها. (يمكن تضمين العديد من هذه المتطلبات في تحديثات البرمجيات والبرامج الثابتة للأجهزة الموجودة، لكن الحلول طويلة المدى والقوية ستتطلب تغييرات في الأجيال اللاحقة من الأجهزة.) من بين هذه المتطلبات ضرورة أنه إذا كان الجهاز جزءاً من مجموعة مترابطة عالية السرعة قادرة على تنفيذ 10<sup>18</sup> FLOP/s من الحوسبة، فإن مستوى أعلى من التحقق مطلوب، والذي يشمل إذناً منتظماً من "حاكم" بعيد يستقبل كلاً من القياسات عن بُعد وطلبات تنفيذ حوسبة إضافية.
- يبلغ الحارس عن إجمالي الحوسبة المنجزة على أجهزته إلى الوكالة التي تحتفظ بقاعدة البيانات الأمريكية.
- يتم تطبيق متطلبات أقوى تدريجياً للسماح بإشراف وترخيص أكثر أماناً ومرونة.

**3. الإشراف الدولي:**

- تتفاوض الولايات المتحدة والصين وأي دول أخرى تستضيف قدرات تصنيع الرقائق المتقدمة على اتفاقية دولية.
- تنشئ هذه الاتفاقية وكالة دولية جديدة، مماثلة للوكالة الدولية للطاقة الذرية، مكلفة بالإشراف على تدريب وتنفيذ الذكاء الاصطناعي.
- يجب على الدول الموقعة أن تطالب مصنعي أجهزة الذكاء الاصطناعي المحليين بالامتثال لمجموعة من المتطلبات قوية على الأقل مثل تلك المفروضة في الولايات المتحدة.
- الحراس مطالبون الآن بالإبلاغ عن أرقام حوسبة الذكاء الاصطناعي إلى كل من الوكالات في دولهم الأم وكذلك مكتب جديد داخل الوكالة الدولية.
- يتم تشجيع الدول الإضافية بقوة للانضمام إلى الاتفاقية الدولية القائمة: تقيد ضوابط التصدير من قبل الدول الموقعة الوصول إلى الأجهزة المتقدمة من قبل غير الموقعين بينما يمكن للموقعين الحصول على دعم فني في إدارة أنظمة الذكاء الاصطناعي الخاصة بهم.

**4. التحقق والإنفاذ الدولي:**

- يتم تحديث نظام التحقق من الأجهزة بحيث يبلغ عن استخدام الحوسبة إلى كل من الحارس الأصلي وأيضاً مباشرة إلى مكتب الوكالة الدولية.
- تتفق الوكالة، عبر المناقشة مع الموقعين على الاتفاقية الدولية، على قيود حاسوبية تكتسب بعد ذلك قوة قانونية في الدول الموقعة.
- بالتوازي، قد يتم تطوير مجموعة من المعايير الدولية بحيث يُطلب من تدريب وتشغيل أنظمة الذكاء الاصطناعي فوق عتبة من الحوسبة (لكن تحت الحد الأقصى) الالتزام بتلك المعايير.
- يمكن للوكالة، إذا لزم الأمر للتعويض عن خوارزميات أفضل وغيرها، خفض حد الحوسبة. أو، إذا اعتُبر آمناً ومستحسناً (على مستوى ضمانات السلامة القابلة للإثبات مثلاً)، رفع حد الحوسبة.

## الملحق ج: تفاصيل نظام المسؤولية المطلقة للذكاء الاصطناعي العام

**تفاصيل نظام المسؤولية المطلقة للذكاء الاصطناعي العام**

- يُعتبر إنشاء وتشغيل نظام ذكاء اصطناعي متقدم عالي العمومية والقدرة والاستقلالية نشاطاً "خطيراً بشكل غير طبيعي".
- وبذلك، فإن مستوى المسؤولية الافتراضي لتدريب وتشغيل مثل هذه الأنظمة هو المسؤولية المطلقة والمشتركة والتضامنية (أو ما يعادلها خارج الولايات المتحدة) عن أي أضرار يسببها النموذج أو مخرجاته/أفعاله.
- ستُفرض المسؤولية الشخصية على المديرين التنفيذيين وأعضاء مجلس الإدارة في حالات الإهمال الجسيم أو سوء السلوك المتعمد. يجب أن يشمل هذا عقوبات جنائية للحالات الأكثر فداحة.
- هناك العديد من الملاذات الآمنة التي تعود في ظلها المسؤولية إلى المسؤولية الافتراضية (القائمة على الخطأ، في الولايات المتحدة) التي يخضع لها الناس والشركات عادة.
	- النماذج المدربة والمشغلة تحت عتبة حاسوبية معينة (والتي ستكون أقل بـ 10 مرات على الأقل من الحدود الموصوفة أعلاه.)
	- الذكاء الاصطناعي "الضعيف" (تقريباً، تحت مستوى الخبير البشري في المهام المخصص لها) و/أو
	- الذكاء الاصطناعي "الضيق" (له نطاق ثابت ومحدود جداً من المهام والعمليات المصمم والمدرب عليها تحديداً) و/أو
	- الذكاء الاصطناعي "السلبي" (محدود جداً في قدرته – حتى مع التعديل المتواضع – على اتخاذ إجراءات أو أداء مهام متعددة الخطوات معقدة دون تدخل ورقابة بشرية مباشرة.)
	- ذكاء اصطناعي مضمون السلامة والأمان والقابلية للتحكم (آمن بشكل قابل للإثبات، أو تشير دراسة المخاطر إلى مستوى ضئيل من الضرر المتوقع.)
- يمكن المطالبة بالملاذات الآمنة على أساس [حالة سلامة](https://arxiv.org/abs/2410.21572) تُعد من قبل مطور الذكاء الاصطناعي وتُعتمد من قبل وكالة أو مدقق معتمد من قبل وكالة. للمطالبة بملاذ آمن قائم على الحوسبة، على المطور فقط تقديم تقديرات موثوقة لإجمالي القوة الحاسوبية للتدريب والحد الأقصى لمعدل الاستنتاج
- ستحدد التشريعات صراحة المواقف التي يكون فيها الانتصاف القضائي من تطوير أنظمة الذكاء الاصطناعي ذات المخاطر العالية للضرر العام مناسباً.
- يجب على اتحادات الشركات، بالعمل مع المنظمات غير الحكومية والوكالات الحكومية، تطوير معايير وقواعد تحدد هذه المصطلحات، وكيف يجب على المنظمين منح الملاذات الآمنة، وكيف يجب على مطوري الذكاء الاصطناعي تطوير حالات السلامة، وكيف يجب على المحاكم تفسير المسؤولية حيث لا تُطالب بالملاذات الآمنة بشكل استباقي.

## الملحق د: نهج متدرج لمعايير سلامة وأمن الذكاء الاصطناعي العام

**نهج متدرج لمعايير سلامة وأمن الذكاء الاصطناعي العام**

| مستوى المخاطر | المحفز(ات) | متطلبات التدريب | متطلبات النشر |
| --- | --- | --- | --- |
| م م-0 | ذكاء اصطناعي ضعيف في الاستقلالية والعمومية والذكاء | لا شيء | لا شيء |
| م م-1 | ذكاء اصطناعي قوي في واحد من الاستقلالية أو العمومية أو الذكاء | لا شيء | بناءً على المخاطر والاستخدام، يحتمل حالات سلامة معتمدة من السلطات الوطنية أينما يمكن استخدام النموذج |
| م م-2 | ذكاء اصطناعي قوي في اثنين من الاستقلالية أو العمومية أو الذكاء | التسجيل لدى السلطة الوطنية التي لها ولاية قضائية على المطور | حالة سلامة تحدد مخاطر الضرر الكبير تحت المستويات المصرح بها بالإضافة إلى عمليات تدقيق سلامة مستقلة (تشمل الاختبار الأحمر بالصندوق الأسود والأبيض) معتمدة من السلطات الوطنية أينما يمكن استخدام النموذج |
| م م-3 | ذكاء اصطناعي عام قوي في الاستقلالية والعمومية والذكاء | الموافقة المسبقة على خطة السلامة والأمن من السلطة الوطنية التي لها ولاية قضائية على المطور | حالة سلامة تضمن مخاطر محدودة للضرر الكبير تحت المستويات المصرح بها بالإضافة إلى مواصفات مطلوبة، تشمل الأمن السيبراني والقابلية للتحكم ومفتاح إيقاف غير قابل للإزالة والمواءمة مع القيم البشرية والقوة ضد سوء الاستخدام الخبيث. |
| م م-4 | أي نموذج يتجاوز أيضاً إما 10<sup>27</sup> FLOP للتدريب أو 10<sup>20</sup> FLOP/s للاستنتاج | محظور في انتظار رفع الحد الأقصى للحوسبة المتفق عليه دولياً | محظور في انتظار رفع الحد الأقصى للحوسبة المتفق عليه دولياً |

تصنيفات المخاطر ومعايير السلامة/الأمن، مع مستويات قائمة على عتبات الحوسبة وكذلك مجموعات من الاستقلالية العالية والعمومية والذكاء:

- *الاستقلالية القوية* تنطبق إذا كان النظام قادراً على أداء، أو يمكن بسهولة جعله يؤدي، مهام متعددة الخطوات و/أو اتخاذ إجراءات معقدة ذات صلة بالعالم الحقيقي، دون إشراف أو تدخل بشري كبير. أمثلة: المركبات والروبوتات المستقلة؛ روبوتات التداول المالي. أمثلة مضادة: GPT-4؛ مصنفات الصور
- *العمومية القوية* تشير إلى نطاق واسع من التطبيق، وأداء مهام لم يُدرب النموذج عليها عمداً وتحديداً، وقدرة كبيرة على تعلم مهام جديدة. أمثلة: GPT-4؛ mu-zero. أمثلة مضادة: AlphaFold؛ المركبات المستقلة؛ مولدات الصور
- *الذكاء القوي* يقابل مطابقة أداء مستوى الخبير البشري في المهام التي يؤديها النموذج بشكل أفضل (وبالنسبة للنموذج العام، عبر نطاق واسع من المهام.) أمثلة: AlphaFold؛ mu-zero؛ o3. أمثلة مضادة: GPT-4؛ Siri