# الفصل الثاني - معلومات أساسية حول الشبكات العصبية للذكاء الاصطناعي

كيف تعمل أنظمة الذكاء الاصطناعي الحديثة، وما الذي قد نشهده في الجيل القادم من هذه الأنظمة؟

لفهم كيفية تطور عواقب تطوير ذكاء اصطناعي أكثر قوة، من الضروري استيعاب بعض الأساسيات. هذا الفصل والفصلان التاليان يتناولان هذه الأساسيات، حيث نغطي على التوالي ماهية الذكاء الاصطناعي الحديث، وكيف يستفيد من الحوسبة الهائلة، والطرق التي ينمو بها بسرعة من ناحية العمومية والقدرة.[^1]

هناك طرق عديدة لتعريف الذكاء الاصطناعي، لكن الخاصية الرئيسية للذكاء الاصطناعي بالنسبة لأغراضنا هي أنه بينما يمثل البرنامج الحاسوبي التقليدي قائمة من التعليمات لكيفية أداء مهمة معينة، فإن نظام الذكاء الاصطناعي هو النظام الذي يتعلم من البيانات أو الخبرة لأداء المهام *دون أن يُخبر صراحة عن كيفية القيام بذلك.*

يعتمد تقريباً كل الذكاء الاصطناعي الحديث المهم على الشبكات العصبية. هذه هياكل رياضية/حاسوبية، تُمثل بمجموعة كبيرة جداً (مليارات أو تريليونات) من الأرقام ("الأوزان")، والتي تؤدي مهمة التدريب بشكل جيد. يتم صياغة هذه الأوزان (أو ربما "إنماؤها" أو "إيجادها") من خلال تعديلها بشكل تكراري بحيث تحسن الشبكة العصبية من نتيجة رقمية (تُعرف أيضاً باسم "الخسارة") مُعرَّفة لتحقيق الأداء الجيد في مهمة واحدة أو أكثر.[^2] تُعرف هذه العملية باسم *تدريب* الشبكة العصبية.[^3]

هناك تقنيات عديدة للقيام بهذا التدريب، لكن هذه التفاصيل أقل أهمية بكثير من الطرق التي يُعرَّف بها التسجيل، وكيف تؤدي تلك الطرق إلى مهام مختلفة تؤديها الشبكة العصبية بشكل جيد. تاريخياً، كان هناك تمييز رئيسي بين الذكاء الاصطناعي "الضيق" و"العام".

الذكاء الاصطناعي الضيق يُدرَّب عمداً للقيام بمهمة معينة أو مجموعة صغيرة من المهام (مثل التعرف على الصور أو لعب الشطرنج)؛ ويتطلب إعادة تدريب للمهام الجديدة، وله نطاق ضيق من القدرات. لدينا ذكاء اصطناعي ضيق فائق على البشر، مما يعني أنه لأي مهمة منفصلة ومُعرَّفة بوضوح يمكن للشخص القيام بها، يمكننا على الأرجح بناء نظام تسجيل ثم تدريب نظام ذكاء اصطناعي ضيق بنجاح للقيام بها بشكل أفضل مما يمكن للإنسان.

أنظمة الذكاء الاصطناعي متعددة الأغراض يمكنها أداء مجموعة واسعة من المهام، بما في ذلك العديد من المهام التي لم تُدرَّب عليها صراحة؛ ويمكنها أيضاً تعلم مهام جديدة كجزء من عملها. النماذج "متعددة الوسائط" [^4] الكبيرة الحالية مثل ChatGPT تمثل مثالاً على ذلك: مُدرَّبة على مجموعة كبيرة جداً من النصوص والصور، يمكنها الانخراط في التفكير المعقد، وكتابة الرموز البرمجية، وتحليل الصور، والمساعدة في مجموعة واسعة من المهام الفكرية. رغم أنها لا تزال مختلفة تماماً عن الذكاء البشري بطرق سنراها بالتفصيل أدناه، إلا أن عموميتها أحدثت ثورة في الذكاء الاصطناعي.[^5]

## عدم القابلية للتنبؤ: خاصية رئيسية لأنظمة الذكاء الاصطناعي

الفرق الرئيسي بين أنظمة الذكاء الاصطناعي والبرمجيات التقليدية يكمن في القابلية للتنبؤ. مخرجات البرمجيات القياسية يمكن أن تكون غير قابلة للتنبؤ - في الواقع أحياناً هذا هو سبب كتابة البرمجيات، لتعطينا نتائج لم نكن نستطيع التنبؤ بها. لكن البرمجيات التقليدية نادراً ما تفعل شيئاً لم تُبرمج للقيام به - نطاقها وسلوكها عموماً كما هو مصمم. برنامج الشطرنج من الدرجة الأولى قد يقوم بحركات لا يستطيع أي إنسان التنبؤ بها (وإلا لاستطاعوا هزيمة ذلك البرنامج!) لكنه عموماً لن يفعل أي شيء سوى لعب الشطرنج.

مثل البرمجيات التقليدية، الذكاء الاصطناعي الضيق له نطاق وسلوك قابلان للتنبؤ لكن يمكن أن تكون له نتائج غير قابلة للتنبؤ. هذه في الواقع طريقة أخرى لتعريف الذكاء الاصطناعي الضيق: كذكاء اصطناعي يشبه البرمجيات التقليدية في قابليته للتنبؤ ونطاق عمله.

الذكاء الاصطناعي متعدد الأغراض مختلف: نطاقه (المجالات التي يطبق عليها)، وسلوكه (أنواع الأشياء التي يفعلها)، ونتائجه (مخرجاته الفعلية) يمكن أن تكون جميعها غير قابلة للتنبؤ.[^6] GPT-4 دُرِّب فقط لتوليد النصوص بدقة، لكنه طوَّر قدرات عديدة لم يتنبأ بها مدربوه أو يقصدوها. عدم القابلية للتنبؤ هذا ينبع من تعقيد التدريب: لأن بيانات التدريب تحتوي على مخرجات من مهام مختلفة عديدة، يجب على الذكاء الاصطناعي فعلياً تعلم أداء هذه المهام للتنبؤ بشكل جيد.

عدم القابلية للتنبؤ هذا في أنظمة الذكاء الاصطناعي العام أساسي تماماً. رغم أنه من الممكن من حيث المبدأ بناء أنظمة ذكاء اصطناعي بعناية لها حدود مضمونة على سلوكها (كما هو مذكور لاحقاً في المقال)، إلا أن الطريقة التي تُنشأ بها أنظمة الذكاء الاصطناعي الآن تجعلها غير قابلة للتنبؤ في الممارسة وحتى من حيث المبدأ.

## الذكاء الاصطناعي السلبي، والوكلاء، والأنظمة المستقلة، والمواءمة

عدم القابلية للتنبؤ هذا يصبح مهماً بشكل خاص عندما نأخذ بالاعتبار كيفية نشر أنظمة الذكاء الاصطناعي واستخدامها فعلياً لتحقيق أهداف مختلفة.

العديد من أنظمة الذكاء الاصطناعي سلبية نسبياً بمعنى أنها تقدم المعلومات بشكل أساسي، والمستخدم يتخذ الإجراءات. أخرى، تُسمى عادة *الوكلاء*، تتخذ إجراءات بنفسها، بمستويات متفاوتة من مشاركة المستخدم. تلك التي تتخذ إجراءات بمدخلات أو إشراف خارجي أقل نسبياً قد تُوصف بأنها أكثر *استقلالية*. هذا يشكل طيفاً من ناحية استقلالية العمل، من الأدوات السلبية إلى الوكلاء المستقلين.[^7]

بالنسبة لأهداف أنظمة الذكاء الاصطناعي، قد تكون مرتبطة مباشرة بهدف تدريبها (مثلاً هدف "الفوز" لنظام لعب الغو هو أيضاً ما دُرِّب للقيام به صراحة). أو قد لا تكون كذلك: هدف تدريب ChatGPT جزئياً هو التنبؤ بالنص، وجزئياً أن يكون مساعداً مفيداً. لكن عند القيام بمهمة معينة، يزوده المستخدم بالهدف. قد تُنشأ الأهداف أيضاً بواسطة نظام الذكاء الاصطناعي نفسه، مرتبطة فقط بشكل غير مباشر جداً بهدف تدريبه.[^8]

الأهداف مرتبطة بقوة بمسألة "المواءمة"، أي مسألة ما إذا كانت أنظمة الذكاء الاصطناعي ستفعل *ما نريدها أن تفعله*. هذا السؤال البسيط يخفي مستوى هائلاً من التعقيد.[^9] في الوقت الحالي، لاحظ أن "نحن" في هذه الجملة قد يشير إلى أشخاص ومجموعات مختلفة، مما يؤدي إلى أنواع مختلفة من المواءمة. مثلاً، قد يكون الذكاء الاصطناعي *مطيعاً* بدرجة عالية (أو ["مخلصاً"](https://arxiv.org/abs/2003.11157)) لمستخدمه - هنا "نحن" تعني "كل واحد منا." أو قد يكون أكثر *استقلالاً*، مدفوعاً أساساً بأهدافه وقيوده الخاصة، لكن لا يزال يتصرف بشكل عام لصالح رفاهية الإنسان - "نحن" تعني حينها "الإنسانية" أو "المجتمع." في المنتصف يوجد طيف حيث سيكون الذكاء الاصطناعي مطيعاً إلى حد كبير، لكنه قد يرفض اتخاذ إجراءات تضر بالآخرين أو المجتمع، أو تنتهك القانون، إلخ.

هذان المحوران - مستوى الاستقلالية ونوع المواءمة - ليسا مستقلين تماماً. مثلاً، نظام سلبي مستقل، رغم أنه ليس متناقضاً ذاتياً تماماً، هو مفهوم متوتر، كما هو الحال مع الوكيل المستقل المطيع.[^10] هناك معنى واضح لكون الاستقلالية والسيادة تميلان للارتباط. بروح مشابهة، تميل القابلية للتنبؤ لأن تكون أعلى في أنظمة الذكاء الاصطناعي "السلبية" و"المطيعة"، بينما الأنظمة المستقلة أو المستقلة ستميل لأن تكون أكثر عدم قابلية للتنبؤ. كل هذا سيكون أساسياً لفهم تداعيات الذكاء الاصطناعي العام والذكاء الفائق المحتملين.

إنشاء ذكاء اصطناعي متوائم حقاً، من أي نوع، يتطلب حل ثلاثة تحديات منفصلة:

1. فهم ما "نريده" - وهو أمر معقد سواء كان "نحن" يعني شخصاً أو منظمة محددة (الإخلاص) أو الإنسانية بشكل عام (الاستقلالية)؛
2. بناء أنظمة تتصرف بانتظام وفقاً لتلك الرغبات - وهو في الأساس إنشاء سلوك إيجابي متسق؛
3. الأهم من ذلك، جعل الأنظمة "تهتم" حقاً بتلك الرغبات بدلاً من مجرد التصرف كما لو كانت تفعل.

التمييز بين السلوك الموثوق والاهتمام الحقيقي أساسي. تماماً كما قد يتبع الموظف البشري الأوامر بشكل مثالي مع افتقاره لأي التزام حقيقي برسالة المنظمة، قد يتصرف نظام الذكاء الاصطناعي بشكل متوائم دون أن يُقدر حقاً تفضيلات البشر. يمكننا تدريب أنظمة الذكاء الاصطناعي لتقول وتفعل أشياء من خلال التغذية الراجعة، ويمكنها تعلم التفكير في ما يريده البشر. لكن جعلها *تُقدر حقاً* تفضيلات البشر تحدٍ أعمق بكثير.[^11]

الصعوبات العميقة في حل تحديات المواءمة هذه، وآثارها على مخاطر الذكاء الاصطناعي، ستُستكشف أكثر أدناه. في الوقت الحالي، افهم أن المواءمة ليست مجرد خاصية تقنية نضعها على أنظمة الذكاء الاصطناعي، بل جانب أساسي من هيكلها الذي يشكل علاقتها مع الإنسانية.


[^1]: للحصول على مقدمة لطيفة لكن تقنية للتعلم الآلي والذكاء الاصطناعي، خاصة نماذج اللغة، انظر [هذا الموقع.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) لمقدمة حديثة أخرى حول مخاطر انقراض الذكاء الاصطناعي، انظر [هذا المقال.](https://www.thecompendium.ai/) للحصول على تحليل علمي شامل وموثوق لحالة سلامة الذكاء الاصطناعي، انظر [تقرير السلامة الدولي للذكاء الاصطناعي](https://arxiv.org/abs/2501.17805) الحديث.

[^2]: التدريب يحدث عادة بالبحث عن أقصى محلي للنتيجة في فضاء عالي الأبعاد محدد بأوزان النموذج. من خلال فحص كيفية تغير النتيجة عند تعديل الأوزان، تحدد خوارزمية التدريب أي التعديلات تحسن النتيجة أكثر، وتحرك الأوزان في ذلك الاتجاه.

[^3]: مثلاً، في مشكلة التعرف على الصور، ستخرج الشبكة العصبية احتماليات لتسميات الصورة. ستكون النتيجة مرتبطة بالاحتمالية التي يعطيها الذكاء الاصطناعي للجواب الصحيح. إجراء التدريب سيعدل الأوزان حينها بحيث في المرة القادمة، سيخرج الذكاء الاصطناعي احتمالية أعلى للتسمية الصحيحة لتلك الصورة. هذا يتكرر عدداً هائلاً من المرات. نفس الإجراء الأساسي يُستخدم في تدريب كل الشبكات العصبية الحديثة تأساساً، وإن كان بآلية تسجيل أكثر تعقيداً.

[^4]: معظم النماذج متعددة الوسائط تستخدم هندسة "المحول" لمعالجة وتوليد أنواع متعددة من البيانات (نص، صور، صوت). يمكن تفكيك كل هذه إلى، ثم التعامل معها على قدم المساواة، كأنواع مختلفة من "الرموز المميزة." النماذج متعددة الوسائط تُدرَّب أولاً للتنبؤ بدقة بالرموز المميزة ضمن مجموعات بيانات ضخمة، ثم تُحسَّن من خلال التعلم المعزز لتعزيز القدرات وتشكيل السلوكيات.

[^5]: أن نماذج اللغة مدربة للقيام بشيء واحد - التنبؤ بالكلمات - جعل البعض يسميها ذكاءً اصطناعياً ضيقاً. لكن هذا مضلل: لأن التنبؤ بالنص بشكل جيد يتطلب قدرات مختلفة كثيرة، مهمة التدريب هذه تؤدي إلى نظام عام بشكل مفاجئ. لاحظ أيضاً أن هذه الأنظمة مدربة بشكل واسع بالتعلم المعزز، مما يمثل فعلياً آلاف الأشخاص الذين يعطون النموذج إشارة مكافأة عندما يقوم بعمل جيد في أي من الأشياء الكثيرة التي يفعلها. ثم يرث عمومية كبيرة من الأشخاص الذين يعطون هذه التغذية الراجعة.

[^6]: هناك طرق متعددة يكون بها الذكاء الاصطناعي غير قابل للتنبؤ. إحداها أنه في الحالة العامة لا يمكن للمرء التنبؤ بما ستفعله خوارزمية دون تشغيلها فعلياً؛ هناك [نظريات](https://arxiv.org/abs/1310.3225) بهذا الصدد. هذا يمكن أن يكون صحيحاً فقط لأن مخرجات الخوارزميات يمكن أن تكون معقدة. لكنه واضح وذو صلة بشكل خاص في الحالة (كما في الشطرنج أو الغو) حيث التنبؤ سيعني قدرة (هزيمة الذكاء الاصطناعي) لا يملكها المتنبئ المحتمل. ثانياً، نظام ذكاء اصطناعي معين لن ينتج دائماً نفس المخرجات حتى لو أعطي نفس المدخلات - مخرجاته تحتوي على عشوائية؛ هذا يقترن أيضاً بعدم القابلية للتنبؤ الخوارزمي. ثالثاً، قدرات غير متوقعة وناشئة يمكن أن تنشأ من التدريب، مما يعني حتى *أنواع* الأشياء التي يمكن لنظام الذكاء الاصطناعي فعلها وسيفعلها غير قابلة للتنبؤ؛ هذا النوع الأخير مهم بشكل خاص لاعتبارات السلامة.

[^7]: انظر [هنا](https://arxiv.org/abs/2502.02649) لمراجعة متعمقة لما يُقصد بـ "الوكيل المستقل" (مع حجج أخلاقية ضد بنائها).

[^8]: قد تسمع أحياناً "الذكاء الاصطناعي لا يمكن أن تكون له أهدافه الخاصة." هذا هراء مطلق. من السهل توليد أمثلة حيث الذكاء الاصطناعي له أو يطور أهدافاً لم تُعط له أبداً ولا يعرفها إلا هو. لا ترى هذا كثيراً في النماذج متعددة الوسائط الشائعة الحالية لأنه يُدرَّب خارجاً منها؛ يمكن بسهولة تدريبه داخلها.

[^9]: هناك أدبيات كبيرة. حول المشكلة العامة انظر كتاب كريستيان [*مشكلة المواءمة*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821)، وكتاب راسل [*متوافق مع الإنسان*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616). على الجانب التقني أكثر انظر مثلاً [هذا المقال](https://arxiv.org/abs/2209.00626).

[^10]: سنرى لاحقاً أنه رغم أن هذه الأنظمة تخالف الاتجاه، إلا أن ذلك يجعلها في الواقع مثيرة جداً للاهتمام ومفيدة.

[^11]: هذا لا يعني أننا نتطلب عواطف أو إحساساً. بل أنه من الصعب للغاية من خارج النظام معرفة ما هي أهدافه وتفضيلاته وقيمه الداخلية. "الحقيقي" هنا يعني أن لدينا سبباً قوياً كافياً للاعتماد عليه بحيث في حالة الأنظمة الحرجة يمكننا المراهنة بحياتنا عليه.