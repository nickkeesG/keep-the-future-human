# الفصل 3 - الجوانب الأساسية لكيفية صناعة أنظمة الذكاء الاصطناعي العامة الحديثة

معظم أنظمة الذكاء الاصطناعي الأكثر تطوراً في العالم تُصنع باستخدام طرق متشابهة بشكل مدهش. إليكم الأساسيات.

لفهم الإنسان حقاً، تحتاج إلى معرفة شيء عن علم الأحياء والتطور وتربية الأطفال والمزيد؛ ولفهم الذكاء الاصطناعي، تحتاج أيضاً إلى معرفة كيفية صناعته. خلال السنوات الخمس الماضية، تطورت أنظمة الذكاء الاصطناعي بشكل هائل في كلٍ من القدرة والتعقيد. وكان العامل الرئيسي المُمكِّن لذلك هو توفر كميات كبيرة جداً من الحوسبة (أو "القوة الحاسوبية" كما تُستخدم في مجال الذكاء الاصطناعي).

الأرقام مذهلة. حوالي 10<sup>25</sup> -10<sup>26</sup> "عملية فاصلة عائمة" (FLOP)[^1] تُستخدم في تدريب نماذج مثل سلسلة GPT وClaude وGemini وغيرها.[^2] (للمقارنة، إذا عمل كل إنسان على الأرض دون توقف بمعدل حساب واحد كل خمس ثوان، فسيحتاج الأمر إلى حوالي مليار سنة لإنجاز هذا.) هذه الكمية الهائلة من الحوسبة تمكّن من تدريب نماذج تحتوي على تريليونات من المعاملات على تيرابايتات من البيانات - جزء كبير من كل النصوص عالية الجودة التي كُتبت على الإطلاق إلى جانب مكتبات ضخمة من الأصوات والصور والفيديوهات. وبتكملة هذا التدريب مع تدريب إضافي واسع يعزز التفضيلات البشرية والأداء الجيد في المهام، تُظهر النماذج المدربة بهذه الطريقة أداءً منافساً للبشر عبر نطاق واسع من المهام الفكرية الأساسية، بما في ذلك التفكير وحل المشكلات.

نعرف أيضاً (بشكل تقريبي جداً) مقدار سرعة الحوسبة، بالعمليات في الثانية الواحدة، الكافية لكي تضاهي سرعة *الاستنتاج*[^3] لمثل هذا النظام *سرعة* المعالجة النصية البشرية. إنها حوالي 10<sup>15</sup> -10<sup>16</sup> FLOP في الثانية.[^4]

رغم قوتها، فإن هذه النماذج محدودة بطبيعتها بطرق أساسية، تماماً كما سيكون الإنسان الفرد محدوداً لو أُجبر على مجرد إخراج نص بمعدل ثابت من الكلمات في الدقيقة، دون توقف للتفكير أو استخدام أي أدوات إضافية. الأنظمة الأحدث للذكاء الاصطناعي تتعامل مع هذه القيود من خلال عملية وبنية معمارية أكثر تعقيداً تجمع بين عدة عناصر أساسية:

- شبكة عصبية واحدة أو أكثر، مع نموذج واحد يوفر القدرة المعرفية الأساسية، وحتى عدة أخرى تؤدي مهام أخرى أكثر تخصصاً؛
- *أدوات* مُوفرة للنموذج وقابلة للاستخدام من قبله - على سبيل المثال القدرة على البحث في الويب، أو إنشاء أو تحرير الوثائق، أو تنفيذ البرامج، إلخ.
- *سقالة* تربط بين مدخلات ومخرجات الشبكات العصبية. قد تسمح سقالة بسيطة جداً لـ"نسختين" من نموذج ذكاء اصطناعي بالتحدث مع بعضهما البعض، أو لواحدة بفحص عمل الأخرى.[^5]
- *سلسلة التفكير* وتقنيات التوجيه ذات الصلة تقوم بشيء مشابه، حيث تجعل النموذج يولد على سبيل المثال عدة مناهج لمشكلة، ثم يعالج تلك المناهج للوصول لإجابة مجمعة.
- *إعادة تدريب* النماذج لتحسين استخدام الأدوات والسقالة وسلسلة التفكير.

نظراً لأن هذه الإضافات يمكن أن تكون قوية جداً (وتشمل أنظمة ذكاء اصطناعي بحد ذاتها)، فإن هذه الأنظمة المركبة يمكن أن تكون متطورة جداً وتعزز قدرات الذكاء الاصطناعي بشكل كبير.[^6] ومؤخراً، طُورت تقنيات في السقالة وخاصة توجيه سلسلة التفكير (وإدماج النتائج في إعادة تدريب النماذج لاستخدام هذه بشكل أفضل) واستُخدمت في [o1](https://openai.com/o1/) و[o3](https://openai.com/index/openai-o3-mini/) و[DeepSeek R1](https://api-docs.deepseek.com/news/news250120) لإجراء مرات عديدة من الاستنتاج استجابةً لاستعلام معين.[^7] هذا يسمح فعلياً للنموذج بـ"التفكير في" استجابته ويعزز بشكل كبير قدرة هذه النماذج على القيام بتفكير عالي المستوى في مهام العلوم والرياضيات والبرمجة.[^8]

بالنسبة لبنية معمارية معينة للذكاء الاصطناعي، يمكن [ترجمة الزيادات في حوسبة التدريب بشكل موثوق](https://arxiv.org/abs/2405.10938) إلى تحسينات في مجموعة من المقاييس المحددة بوضوح. بالنسبة للقدرات العامة المحددة بشكل أقل وضوحاً (مثل تلك المناقشة أدناه)، الترجمة أقل وضوحاً وتنبؤية، لكن من المؤكد تماماً أن النماذج الأكبر مع المزيد من حوسبة التدريب ستحصل على قدرات جديدة وأفضل، حتى لو كان من الصعب التنبؤ بما ستكون عليه تلك القدرات.

وبالمثل، الأنظمة المركبة وخاصة التطورات في "سلسلة التفكير" (وتدريب النماذج التي تعمل جيداً معها) فتحت المجال للتوسع في حوسبة *الاستنتاج*: بالنسبة لنموذج أساسي مدرب معين، تزيد قدرات نظام الذكاء الاصطناعي -على الأقل بعضها- كلما طُبقت حوسبة أكثر تسمح لها بـ"التفكير بجدية وطويلاً" في المشاكل المعقدة. هذا يأتي بتكلفة باهظة في سرعة الحوسبة، حيث يتطلب مئات أو آلاف أكثر من FLOP/s لمضاهاة الأداء البشري.[^9]

رغم كونها جزءاً فقط مما يقود إلى التقدم السريع في الذكاء الاصطناعي،[^10] سيثبت دور الحوسبة وإمكانية الأنظمة المركبة أنه حاسم لكل من منع الذكاء الاصطناعي العام غير القابل للسيطرة وتطوير بدائل أكثر أماناً.


[^1]: 10<sup>27</sup> يعني 1 متبوعاً بـ 25 صفراً، أو عشرة تريليون تريليون. FLOP هي مجرد عملية جمع أو ضرب حسابية لأرقام بدقة معينة. لاحظ أن أداء عتاد الذكاء الاصطناعي يمكن أن يتفاوت بعامل عشرة أو أكثر اعتماداً على دقة العمليات الحسابية وبنية الحاسوب المعمارية. عد عمليات البوابات المنطقية (ANDs، ORs، AND NOTs) سيكون أساسياً لكن هذه غير متاحة أو مقيسة عادة؛ لأغراض الوقت الحالي من المفيد التوحيد على عمليات 16-بت (FP16)، رغم أنه يجب إنشاء عوامل تحويل مناسبة.

[^2]: مجموعة من التقديرات والبيانات المؤكدة متاحة من [Epoch AI](https://epochai.org/data/large-scale-ai-models) وتشير إلى حوالي 2×10<sup>25</sup> 16-بت FLOP لـ GPT-4؛ هذا يطابق تقريباً [الأرقام التي تسربت](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) لـ GPT-4. التقديرات لنماذج أخرى من منتصف 2024 كلها ضمن عامل قليل من GPT-4.

[^3]: الاستنتاج هو ببساطة عملية توليد مخرج من شبكة عصبية. يمكن اعتبار التدريب تعاقباً لعمليات استنتاج كثيرة وتعديلات لمعاملات النموذج.

[^4]: بالنسبة لإنتاج النص، تطلب GPT-4 الأصلي 560 TFLOP لكل رمز مولد. حوالي 7 رموز/ث مطلوبة لمواكبة التفكير البشري، لذا هذا يعطي ≈3×10<sup>15</sup> FLOP/s. لكن التحسينات خفضت هذا؛ [هذا الكتيب من NVIDIA](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/) على سبيل المثال يشير إلى أقل من 3×10<sup>14</sup> FLOP/s لنموذج Llama 405B بأداء مماثل.

[^5]: كمثال أكثر تعقيداً قليلاً، قد يولد نظام ذكاء اصطناعي أولاً عدة حلول محتملة لمسألة رياضية، ثم يستخدم نسخة أخرى لفحص كل حل، وأخيراً يستخدم ثالثة لتجميع النتائج في شرح واضح. هذا يسمح بحل مشاكل أكثر شمولية وموثوقية من مرة واحدة.

[^6]: انظر على سبيل المثال تفاصيل ["Operator" من OpenAI](https://openai.com/index/introducing-operator/)، [قدرات أدوات Claude](https://docs.anthropic.com/en/docs/build-with-claude/computer-use)، و[AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). [Deep Research](https://openai.com/index/introducing-deep-research/) من OpenAI ربما لديه بنية معمارية متطورة جداً لكن التفاصيل غير متاحة.

[^7]: Deepseek R1 يعتمد على تدريب وتوجيه النموذج بشكل تكراري بحيث ينشئ النموذج المدرب النهائي تفكير سلسلة مكثف. التفاصيل المعمارية غير متاحة لـ o1 أو o3، لكن Deepseek كشف أنه لا توجد "صلصة سرية" خاصة مطلوبة لفتح توسع القدرة مع الاستنتاج. لكن رغم تلقيه الكثير من التغطية الصحفية كمقلب لـ"الوضع الراهن" في الذكاء الاصطناعي، فإنه لا يؤثر على الادعاءات الأساسية لهذا المقال.

[^8]: هذه النماذج تتفوق بشكل كبير على النماذج القياسية في مقاييس التفكير. على سبيل المثال، في GPQA Diamond Benchmark—اختبار صارم لأسئلة علمية بمستوى الدكتوراه—[سجل](https://openai.com/index/learning-to-reason-with-llms/) GPT-4o 56%، بينما حقق o1 و o3 78% و 88% على التوالي، متجاوزين بكثير متوسط نتيجة 70% للخبراء البشر.

[^9]: O3 من OpenAI استهلك ربما ∼10<sup>21</sup> -10<sup>22</sup> FLOP [لإكمال كل سؤال من أسئلة تحدي ARC-AGI](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai)، والتي يمكن للبشر المختصين فعلها في (لنقل) 10-100 ثانية، مما يعطي رقماً أشبه بـ ∼10<sup>20</sup> FLOP/s.

[^10]: بينما الحوسبة مقياس أساسي لقدرة نظام الذكاء الاصطناعي، فإنها تتفاعل مع كل من جودة البيانات والتحسينات الخوارزمية. البيانات أو الخوارزميات الأفضل يمكن أن تقلل المتطلبات الحاسوبية، بينما المزيد من الحوسبة يمكن أحياناً أن تعوض عن بيانات أو خوارزميات أضعف.