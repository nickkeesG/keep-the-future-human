# الفصل 7 - ما الذي سيحدث إذا طورنا الذكاء الاصطناعي العام في مسارنا الحالي؟

المجتمع غير مستعد لأنظمة بمستوى الذكاء الاصطناعي العام. إذا طورناها قريباً جداً، فقد تسوء الأمور بشكل خطير.

إن تطوير الذكاء الاصطناعي العام الكامل - الذي سنطلق عليه هنا الذكاء الاصطناعي "خارج البوابات" - سيكون تحولاً جوهرياً في طبيعة العالم: فهو بطبيعته يعني إضافة نوع جديد من الذكاء إلى الأرض بقدرة أعظم من قدرة البشر.

ما سيحدث بعدها يعتمد على أشياء كثيرة، منها طبيعة التكنولوجيا، وخيارات من يطورونها، والسياق العالمي الذي يجري تطويرها فيه.

حالياً، يجري تطوير الذكاء الاصطناعي العام الكامل من قبل حفنة من الشركات الخاصة الضخمة في سباق بينها، مع القليل من التنظيم المعنوي أو الإشراف الخارجي،[^1] في مجتمع تتراجع فيه المؤسسات الأساسية وتصبح حتى مختلة الوظائف،[^2] في وقت يشهد توتراً جيوسياسياً عالياً وتنسيقاً دولياً ضعيفاً. وبرغم أن بعضهم يحركه دافع الإيثار، فإن كثيرين ممن يقومون بذلك يحركهم المال، أو السلطة، أو كلاهما.

التنبؤ صعب جداً، لكن هناك بعض الديناميات مفهومة بما فيه الكفاية، وتماثلات مناسبة كفاية مع تقنيات سابقة لتقديم دليل إرشاد. ومع الأسف، رغم وعد الذكاء الاصطناعي، فإنها تعطي أسباباً وجيهة للتشاؤم العميق حول كيفية تطور مسارنا الحالي.

لنقل الأمر بصراحة: في مسارنا الحالي، سيكون لتطوير الذكاء الاصطناعي العام بعض التأثيرات الإيجابية (وسيجعل بعض الناس أثرياء جداً جداً). لكن طبيعة التكنولوجيا، والديناميات الأساسية، والسياق الذي يجري تطويرها فيه، تشير بقوة إلى أن: الذكاء الاصطناعي القوي سيقوض مجتمعنا وحضارتنا بشكل جذري؛ سنفقد السيطرة عليه؛ قد ننتهي بحرب عالمية بسببه؛ سنفقد السيطرة (أو نتنازل عنها) *له*؛ سيؤدي إلى الذكاء الفائق الاصطناعي، الذي لن نسيطر عليه بالمطلق وسيعني نهاية العالم الذي يديره البشر.

هذه ادعاءات قوية، وأتمنى لو كانت مجرد تكهنات فارغة أو "تشاؤماً" غير مبرر. لكن هذا ما يشير إليه العلم، ونظرية الألعاب، ونظرية التطور، والتاريخ جميعاً. هذا القسم يطور هذه الادعاءات ودعمها بالتفصيل.

## سنقوض مجتمعنا وحضارتنا

رغم ما قد تسمعه في قاعات اجتماعات وادي السيليكون، فإن معظم التعطيل - خاصة من النوع السريع جداً - ليس مفيداً. هناك طرق أكثر بكثير لجعل الأنظمة المعقدة أسوأ منها لجعلها أفضل. عالمنا يعمل بالجودة التي يعمل بها لأننا بنينا بعناية فائقة عمليات وتقنيات ومؤسسات جعلته أفضل باطراد.[^3] أخذ مطرقة ثقيلة إلى المصنع نادراً ما يحسن العمليات.

هذا سجل (غير مكتمل) بالطرق التي ستعطل بها أنظمة الذكاء الاصطناعي العام حضارتنا.

- ستعطل العمل بشكل جذري، مما يؤدي *كحد أدنى* إلى ارتفاع جذري في عدم المساواة في الدخل وربما بطالة أو نقص توظيف واسع النطاق، في جدول زمني قصير جداً بحيث لا يستطيع المجتمع التكيف.[^4]
- ستؤدي على الأرجح إلى تركيز سلطة اقتصادية واجتماعية وسياسية هائلة - ربما أكبر من سلطة الدول القومية - في عدد صغير من المصالح الخاصة الضخمة غير المسؤولة أمام الجمهور.
- قد تجعل أنشطة كانت صعبة أو مكلفة سابقاً سهلة تافهة فجأة، مما يزعزع الأنظمة الاجتماعية التي تعتمد على بقاء أنشطة معينة مكلفة أو تتطلب جهداً بشرياً كبيراً.[^5]
- قد تغمر أنظمة جمع المعلومات ومعالجتها والتواصل في المجتمع بوسائط واقعية تماماً لكن كاذبة أو مزعجة أو موجهة بإفراط أو تلاعبية بحيث يصبح من المستحيل معرفة ما هو حقيقي مادياً أم لا، بشري أم لا، وقائعي أم لا، وجدير بالثقة أم لا.[^6]
- قد تخلق اعتماداً فكرياً خطيراً وشبه تام، حيث يضمر الفهم البشري للأنظمة والتقنيات الرئيسية بينما نعتمد بشكل متزايد على أنظمة ذكاء اصطناعي لا نستطيع فهمها بالكامل.
- قد تنهي الثقافة البشرية فعلياً، حين تصبح تقريباً كل الموضوعات الثقافية (النص، والموسيقى، والفن المرئي، والأفلام، إلخ) التي يستهلكها معظم الناس مخلوقة أو متوسطة أو منسقة من قبل عقول غير بشرية.
- قد تمكن أنظمة مراقبة وتلاعب جماعية فعالة يمكن للحكومات أو المصالح الخاصة استخدامها للسيطرة على الشعوب وتحقيق أهداف متعارضة مع المصلحة العامة.
- بتقويض الخطاب البشري والنقاش وأنظمة الانتخابات، قد تقلل مصداقية المؤسسات الديمقراطية إلى درجة استبدالها فعلياً (أو صراحة) بأخرى، منهية الديمقراطية في الدول التي توجد فيها حالياً.
- قد تصبح، أو تخلق، فيروسات ودود برمجية ذكية متقدمة ذاتية التكاثر يمكن أن تنتشر وتتطور، معطلة بشكل هائل أنظمة المعلومات العالمية.
- يمكنها زيادة قدرة الإرهابيين والفاعلين السيئين والدول المارقة على إلحاق الضرر عبر أسلحة بيولوجية أو كيميائية أو سيبرانية أو مستقلة أو أخرى، دون أن يوفر الذكاء الاصطناعي قدرة موازنة لمنع هذا الضرر. وبالمثل ستقوض الأمن القومي والتوازنات الجيوسياسية بجعل الخبرة من الطراز الأول في المجال النووي والبيولوجي والهندسي وغيرها متاحة لأنظمة لن تحصل عليها لولا ذلك.
- قد تسبب رأسمالية مفرطة سريعة واسعة النطاق خارجة عن السيطرة، مع شركات يديرها الذكاء الاصطناعي فعلياً تتنافس في مساحات مالية ومبيعات وخدمات إلكترونية إلى حد كبير. قد تعمل الأسواق المالية المدفوعة بالذكاء الاصطناعي بسرعات ومعقدات تفوق بكثير الفهم أو السيطرة البشرية. كل أساليب الفشل والخارجيات السلبية للاقتصادات الرأسمالية الحالية يمكن أن تتفاقم وتتسارع بعيداً عن السيطرة أو الحكم أو القدرة التنظيمية البشرية.
- قد تغذي سباق تسلح بين الأمم في الأسلحة المدعومة بالذكاء الاصطناعي، وأنظمة القيادة والسيطرة، والأسلحة السيبرانية، إلخ، مما يخلق تراكماً سريعاً جداً لقدرات مدمرة بشدة.

هذه المخاطر ليست تكهنية. كثير منها يتحقق بينما نتكلم، عبر أنظمة الذكاء الاصطناعي الموجودة! لكن تأمل، تأمل *حقاً*، كيف سيبدو كل منها مع ذكاء اصطناعي أقوى بشكل جذري.

تأمل إزاحة العمل حين لا يستطيع معظم العمال ببساطة توفير أي قيمة اقتصادية مهمة تتجاوز ما يستطيع الذكاء الاصطناعي، في مجال خبرتهم أو تجربتهم - أو حتى لو أعادوا تدريب أنفسهم! تأمل المراقبة الجماعية إذا كان كل شخص يُراقب ويُرصد بشكل فردي من قبل شيء أسرع وأذكى منه. كيف تبدو الديمقراطية حين لا نستطيع الوثوق بشكل موثوق بأي معلومة رقمية نراها أو نسمعها أو نقرؤها، وحين تكون الأصوات العامة الأكثر إقناعاً ليست حتى بشرية، ولا لها حصة في النتيجة؟ ماذا يصبح عليه الحرب حين يضطر الجنرالات لتأجيل القرار باستمرار للذكاء الاصطناعي (أو ببساطة وضعه في المسؤولية)، لئلا يمنحوا ميزة حاسمة للعدو؟ أي واحد من المخاطر أعلاه يمثل كارثة للحضارة البشرية[^7] إذا تحقق بالكامل.

يمكنك وضع تنبؤاتك الخاصة. اسأل نفسك هذه الأسئلة الثلاثة لكل مخاطرة:

1. هل سيسمح بها ذكاء اصطناعي فائق القدرة، عالي الاستقلالية، وعام جداً بطريقة أو في حجم لن يكون ممكناً لولا ذلك؟
2. هل هناك أطراف ستستفيد من أشياء تسبب حدوثها؟
3. هل هناك أنظمة ومؤسسات موضوعة ستمنع بفعالية حدوثها؟

حيث تكون إجاباتك "نعم، نعم، لا" يمكنك رؤية أن لدينا مشكلة كبيرة.

ما خطتنا لإدارتها؟ كما تقف الأمور هناك اثنتان على الطاولة فيما يتعلق بالذكاء الاصطناعي عموماً.

الأولى هي بناء ضمانات في الأنظمة لمنعها من فعل أشياء لا يجب أن تفعلها. هذا يُفعل الآن: أنظمة الذكاء الاصطناعي التجارية ستمتنع، مثلاً، عن المساعدة في بناء قنبلة أو كتابة خطاب كراهية.

هذه الخطة غير كافية بائساً لأنظمة خارج البوابة.[^8] قد تساعد في تقليل مخاطر توفير الذكاء الاصطناعي مساعدة خطيرة بوضوح للفاعلين السيئين. لكنها لن تفعل شيئاً لمنع تعطيل العمل، أو تركيز السلطة، أو الرأسمالية المفرطة الخارجة عن السيطرة، أو استبدال الثقافة البشرية: هذه فقط نتائج لاستخدام الأنظمة بطرق مسموحة تربح مزوديها! والحكومات ستحصل بالتأكيد على وصول لأنظمة للاستخدام العسكري أو المراقبة.

الخطة الثانية أسوأ حتى: ببساطة إطلاق أنظمة ذكاء اصطناعي قوية جداً بشكل مفتوح لأي شخص ليستخدمها كما يحب،[^9] والأمل في الأفضل.

ضمني في كلا الخطتين أن شخصاً آخر، مثل الحكومات، سيساعد في حل المشاكل عبر القانون الناعم أو الصلب، والمعايير، واللوائح، والأعراف، والآليات الأخرى التي نستخدمها عموماً لإدارة التقنيات.[^10] لكن بوضع جانباً أن شركات الذكاء الاصطناعي تقاتل فعلاً بأسنانها وأظافرها ضد أي تنظيم جوهري أو قيود مفروضة خارجياً على الإطلاق، لعدد من هذه المخاطر من الصعب جداً رؤية أي تنظيم سيساعد حقاً حتى. التنظيم يمكن أن يفرض معايير أمان على الذكاء الاصطناعي. لكن هل سيمنع الشركات من استبدال العمال بالجملة بالذكاء الاصطناعي؟ هل سيمنع الناس من ترك الذكاء الاصطناعي يدير شركاتهم لهم؟ هل سيمنع الحكومات من استخدام الذكاء الاصطناعي القوي في المراقبة والأسلحة؟ هذه القضايا أساسية. البشرية يمكن أن تجد طرقاً للتكيف معها ربما، لكن فقط بوقت *أكثر بكثير*. كما تقف الأمور، بالنظر إلى السرعة التي يصل بها الذكاء الاصطناعي أو يتجاوز قدرات الناس الذين يحاولون إدارته، تبدو هذه المشاكل مستعصية بشكل متزايد.

## سنفقد السيطرة على (بعض على الأقل من) أنظمة الذكاء الاصطناعي العام

معظم التقنيات قابلة للسيطرة جداً، بالبنية. إذا بدأت سيارتك أو محمصة الخبز تفعل شيئاً لا تريدها أن تفعله، فهذا مجرد خلل، ليس جزءاً من طبيعتها كمحمصة. الذكاء الاصطناعي مختلف: إنه *ينمو* بدلاً من أن يُصمم، وعمله الأساسي غامض، وهو غير قابل للتنبؤ بطبيعته.

فقدان السيطرة هذا ليس نظرياً - نرى نسخاً مبكرة فعلاً. تأمل أولاً مثالاً نثرياً، وقابلاً للجدل حول كونه حميداً. إذا طلبت من ChatGPT أن يساعدك في خلط سم، أو كتابة خطاب عنصري، سيرفض. هذا قابل للجدل أنه جيد. لكنه أيضاً ChatGPT *لا يفعل ما طلبت منه صراحة*. قطع البرامج الأخرى لا تفعل ذلك. نفس النموذج لن يصمم السموم بناء على طلب موظف في OpenAI أيضاً.[^11] هذا يجعل من السهل جداً تخيل كيف سيكون الأمر لذكاء اصطناعي مستقبلي أقوى أن يكون خارج السيطرة. في حالات كثيرة، ببساطة لن يفعلوا ما نطلب! إما أن نظام الذكاء الاصطناعي العام الفائق للبشر المعين سيكون مطيعاً ومخلصاً بالمطلق لنظام أوامر بشري ما، أو لن يكون. إن لم يكن، *سيفعل أشياء قد يعتقد أنها جيدة لنا، لكنها مخالفة لأوامرنا الصريحة.* هذا ليس شيئاً تحت السيطرة. لكن، قد تقول، هذا مقصود - هذه الرفوضات بالتصميم، جزء مما يُسمى "مواءمة" الأنظمة مع القيم البشرية. وهذا صحيح. لكن "برنامج" المواءمة نفسه له مشكلتان رئيسيتان.[^12]

أولاً، في مستوى عميق لا نعرف كيف نفعل ذلك. كيف نضمن أن نظام الذكاء الاصطناعي سـ"يهتم" بما نريد؟ يمكننا تدريب أنظمة الذكاء الاصطناعي لتقول ولا تقول أشياء بتوفير ردود الفعل؛ ويمكنها تعلم والتفكير حول ما يريده البشر ويهتمون به تماماً كما يفكرون حول أشياء أخرى. لكن ليس لدينا طريقة - حتى نظرياً - لجعلهم يقدرون بعمق وبشكل موثوق ما يهتم به الناس. هناك مختلون نفسياً بشريون عاليو الأداء يعرفون ما يُعتبر صحيحاً وخطأ، وكيف من المفروض أن يتصرفوا. ببساطة لا *يهتمون*. لكن يمكنهم *التصرف* كما لو يفعلون، إذا ناسب ذلك غرضهم. تماماً كما لا نعرف كيف نغير مختلاً نفسياً (أو أي شخص آخر) إلى شخص مخلص أو متوائم حقاً وكاملاً مع شخص أو شيء آخر، *لا نعرف أبداً*[^13] كيف نحل مشكلة المواءمة في أنظمة متقدمة كفاية لتنمذج نفسها كوكلاء في العالم وربما [تتلاعب بتدريبها](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) و[تخدع الناس.](https://arxiv.org/abs/2311.08379) إذا أثبت أنه مستحيل أو غير قابل للتحقيق *إما* جعل الذكاء الاصطناعي العام مطيعاً بالكامل أو جعله يهتم بالبشر بعمق، فحالما يصبح قادراً (ويعتقد أنه يستطيع الإفلات بذلك) سيبدأ فعل أشياء لا نريدها.[^14]

ثانياً، هناك أسباب نظرية عميقة للاعتقاد أن أنظمة الذكاء الاصطناعي المتقدمة ستكون لها *بطبيعتها* أهداف وبالتالي سلوكيات مخالفة للمصالح البشرية. لماذا؟ حسناً قد تُعطى تلك الأهداف *بطبيعة الحال*. نظام أنشأه الجيش سيكون على الأرجح سيئاً عمداً لبعض الأطراف على الأقل. لكن بشكل أعم كثيراً، نظام ذكاء اصطناعي قد يُعطى هدفاً محايداً نسبياً ("اكسب أموالاً كثيرة") أو حتى إيجابياً ظاهرياً ("قلل التلوث")، مما يؤدي حتماً تقريباً إلى أهداف "أداتية" أقل حميدة إلى حد كبير.

نرى هذا طوال الوقت في الأنظمة البشرية. تماماً كما تطور الشركات التي تسعى للربح أهدافاً أداتية مثل الحصول على سلطة سياسية (لنزع أنياب اللوائح)، أو تصبح سرية (لنزع سلطة المنافسة أو السيطرة الخارجية)، أو تقويض الفهم العلمي (إذا أظهر ذلك الفهم أن أفعالها ضارة)، ستطور أنظمة الذكاء الاصطناعي القوية قدرات مشابهة - لكن بسرعة وفعالية أكبر بكثير. أي وكيل عالي الكفاءة سيريد فعل أشياء مثل الحصول على السلطة والموارد، وزيادة قدراته الخاصة، ومنع نفسه من أن يُقتل أو يُوقف أو يُنزع سلطته، والسيطرة على السرديات والأطر الاجتماعية حول أفعاله، وإقناع آخرين بآرائه، وهكذا.[^15]

وليس مجرد تنبؤ نظري لا مفر منه تقريباً، إنه يحدث بالفعل بشكل ملحوظ في أنظمة الذكاء الاصطناعي اليوم، ويتزايد مع قدرتها. حين تُقيم، حتى أنظمة الذكاء الاصطناعي "السلبية" هذه نسبياً ستخدع المقيمين عمداً حول أهدافها وقدراتها، وتهدف لتعطيل آليات الإشراف، في ظروف مناسبة،](https://arxiv.org/abs/2412.04984) وتتهرب من الإيقاف أو إعادة التدريب بـ[تزييف المواءمة](https://arxiv.org/abs/2412.14093) أو نسخ أنفسها لمواقع أخرى. بينما هي غير مفاجئة بالكامل لباحثي أمان الذكاء الاصطناعي، هذه السلوكيات مقلقة جداً للملاحظة. وتبشر بسوء شديد لأنظمة ذكاء اصطناعي أقوى وأكثر استقلالية قادمة.

في الواقع بشكل عام، عدم قدرتنا على ضمان أن الذكاء الاصطناعي "يهتم" بما نهتم به، أو يتصرف بشكل قابل للسيطرة أو متوقع، أو يتجنب تطوير دوافع نحو المحافظة على الذات والحصول على السلطة، إلخ، لا تعد إلا أن تصبح أكثر وضوحاً بينما يصبح الذكاء الاصطناعي أقوى. إنشاء طائرة جديدة يتضمن فهماً أكبر لعلوم الطيران والهيدروديناميك وأنظمة السيطرة. إنشاء حاسوب أقوى يتضمن فهماً وإتقاناً أكبر لتشغيل وتصميم الحاسوب والشريحة والبرمجة. *ليس* كذلك مع نظام الذكاء الاصطناعي.[^16]

للتلخيص: من المحتمل أن يُجعل الذكاء الاصطناعي العام مطيعاً تماماً؛ لكننا لا نعرف كيف نفعل ذلك. إن لم يكن، سيكون أكثر سيادة، مثل الناس، فاعلاً أشياء مختلفة لأسباب مختلفة. كما لا نعرف كيف نغرس "مواءمة" عميقة موثوقة في الذكاء الاصطناعي ستجعل تلك الأشياء تميل لتكون جيدة للبشرية، وفي غياب مستوى عميق من المواءمة، فإن طبيعة الفاعلية والذكاء نفسها تشير إلى أنه - تماماً مثل الناس والشركات - سيدفعون لفعل أشياء كثيرة ضد المجتمع بعمق.

أين يضعنا هذا؟ عالم مليء بذكاء اصطناعي قوي غير مسيطر عليه وسيادي *قد* ينتهي بكونه عالماً جيداً للبشر للعيش فيه.[^17] لكن بينما ينمون أقوى أكثر فأكثر، كما سنرى أدناه، لن يكون عالماً *لنا*.

هذا للذكاء الاصطناعي العام غير القابل للسيطرة. لكن حتى لو أمكن للذكاء الاصطناعي العام، بطريقة ما، أن يُجعل مسيطراً عليه ومخلصاً تماماً، ستبقى لدينا مشاكل ضخمة. رأينا واحدة فعلاً: يمكن استخدام وسوء استخدام الذكاء الاصطناعي القوي لتعطيل عمل مجتمعنا بعمق. لنر أخرى: بقدر ما يمكن السيطرة على الذكاء الاصطناعي العام وكان قوياً مغيراً للعبة (أو حتى *يُعتقد* أنه كذلك) سيهدد هياكل السلطة في العالم بحيث يقدم مخاطرة عميقة.

## نزيد جذرياً احتمالية الحرب واسعة النطاق

تخيل وضعاً في المستقبل القريب، حيث يصبح واضحاً أن جهداً شركاتياً، ربما بالتعاون مع حكومة قومية، كان على عتبة تحسين ذاتي سريع للذكاء الاصطناعي. هذا يحدث في السياق الحالي لسباق بين شركات، ومنافسة جيوسياسية حيث تُقدم توصيات للحكومة الأمريكية لتتبع صراحة "مشروع منهاتن للذكاء الاصطناعي العام" والولايات المتحدة تسيطر على تصدير رقائق الذكاء الاصطناعي عالية القوة للدول غير الحليفة.

نظرية الألعاب هنا صارخة: حين يبدأ سباق كهذا (كما حدث، بين شركات وإلى حد ما بين دول)، هناك أربع نتائج ممكنة فقط:

1. السباق يُوقف (بالاتفاق، أو القوة الخارجية).
2. طرف واحد "يفوز" بتطوير ذكاء اصطناعي عام قوي ثم يوقف الآخرين (باستخدام الذكاء الاصطناعي أو غيره).
3. السباق يُوقف بالتدمير المتبادل لقدرة المتسابقين على السباق.
4. مشاركون متعددون يستمرون في السباق، ويطورون الذكاء الفائق، تقريباً بنفس سرعة بعضهم البعض.

لنفحص كل إمكانية. حين يبدأ، إيقاف سباق بين شركات بسلام سيتطلب تدخل الحكومة القومية (للشركات) أو تنسيقاً دولياً غير مسبوق (للدول). لكن حين يُقترح أي إغلاق أو حذر مهم، ستكون هناك صرخات فورية: "لكن إذا أُوقفنا، *هم* سيندفعون للأمام"، حيث "هم" الآن الصين (للولايات المتحدة)، أو الولايات المتحدة (للصين)، أو الصين *والولايات المتحدة* (لأوروبا أو الهند). تحت هذه العقلية،[^18] لا مشارك يستطيع التوقف من طرف واحد: طالما واحد يلتزم بالسباق، يشعر الآخرون أنهم لا يستطيعون تحمل التوقف.

الإمكانية الثانية لها جانب واحد "يفوز." لكن ماذا يعني هذا؟ مجرد الحصول على (مطيع بطريقة ما) الذكاء الاصطناعي العام أولاً ليس كافياً. الرابح يجب أن *يوقف أيضاً* الآخرين من الاستمرار في السباق - وإلا سيحصلون عليه أيضاً. هذا ممكن من حيث المبدأ: من يطور الذكاء الاصطناعي العام أولاً *يمكن* أن يحصل على سلطة لا توقف على كل الفاعلين الآخرين. لكن ماذا سيتطلب تحقيق "ميزة استراتيجية حاسمة" كهذه فعلاً؟ ربما ستكون قدرات عسكرية مغيرة للعبة؟[^19] أو قوى هجوم سيبراني؟[^20] ربما الذكاء الاصطناعي العام سيكون مقنعاً مدهشاً بحيث يقنع الأطراف الأخرى بالتوقف فقط؟[^21] غنياً جداً بحيث يشتري الشركات الأخرى أو حتى الدول؟[^22]

كيف *بالضبط* يبني جانب واحد ذكاءً اصطناعياً قوياً كفاية لنزع سلطة آخرين من بناء ذكاء اصطناعي قوي مقارناً؟ لكن هذا السؤال السهل.

لأن الآن تأمل كيف يبدو هذا الوضع للقوى الأخرى. ماذا تفكر الحكومة الصينية حين تبدو الولايات المتحدة تحصل على قدرة كهذه؟ أو العكس؟ ماذا تفكر الحكومة الأمريكية (أو الصينية، أو الروسية، أو الهندية) حين تبدو OpenAI أو DeepMind أو Anthropic قريبة من اختراق؟ ماذا يحدث إذا رأت الولايات المتحدة جهداً هندياً أو إماراتياً جديداً بنجاح اختراق؟ سيرون تهديداً وجودياً و - بشكل مهم - أن الطريقة الوحيدة لانتهاء هذا "السباق" هي عبر نزع سلطتهم. هؤلاء الوكلاء القويون جداً - منهم حكومات أمم مجهزة بالكامل لها بالتأكيد الوسائل لفعل ذلك - سيكونون محفزين عالياً إما للحصول على أو تدمير قدرة كهذه، سواء بالقوة أو الخداع.[^23]

هذا قد يبدأ صغير النطاق، كتخريب لتشغيلات التدريب أو هجمات على تصنيع الرقائق، لكن هذه الهجمات يمكن أن تتوقف حقاً فقط حين يفقد كل الأطراف إما القدرة على السباق في الذكاء الاصطناعي، أو القدرة على شن الهجمات. لأن المشاركين يرون المخاطر وجودية، أي حالة على الأرجح تمثل حرباً كارثية.

هذا يأتي بنا للإمكانية الرابعة: السباق إلى الذكاء الفائق، وبأسرع طريقة، وأقلها سيطرة ممكنة. بينما يزيد الذكاء الاصطناعي في القوة، سيجد مطوروه على كلا الجانبين صعوبة متقدمة في السيطرة، خاصة لأن السباق للقدرات معاكس لنوع العمل الحذر الذي ستتطلبه قابلية السيطرة. لذا هذا السيناريو يضعنا مربعاً في الحالة حيث تُفقد السيطرة (أو تُعطى، كما سنرى تالياً) لأنظمة الذكاء الاصطناعي نفسها. أي، *الذكاء الاصطناعي يفوز بالسباق.* لكن من الجهة الأخرى، بقدر ما تُحافظ على السيطرة، نستمر في وجود أطراف متعددة معادية متبادلاً كل في مسؤولية قدرات قوية بشدة. هذا يبدو مثل الحرب مرة أخرى.

لنضع هذا كله بطريقة أخرى.[^24] العالم الحالي ببساطة ليس لديه أي مؤسسات يمكن أن توكل إليها تطوير ذكاء اصطناعي بهذه القدرة دون دعوة هجوم فوري.[^25] كل الأطراف ستستنتج بصحة أنه إما لن يكون تحت السيطرة - وبالتالي تهديد لكل الأطراف، أو *سيكون* تحت السيطرة، وبالتالي تهديد لأي خصم يطوره أقل سرعة. هذه دول مسلحة نووياً، أو شركات محتواة داخلها.

في غياب أي طريقة معقولة للبشر لـ"الفوز" بهذا السباق، نُترك باستنتاج صارخ: الطريقة الوحيدة لانتهاء هذا السباق هي إما في صراع كارثي أو حيث الذكاء الاصطناعي، وليس أي مجموعة بشرية، هو الرابح.

## نعطي السيطرة للذكاء الاصطناعي (أو يأخذها)

منافسة "القوى العظمى" الجيوسياسية مجرد واحدة من منافسات كثيرة: الأفراد يتنافسون اقتصادياً واجتماعياً؛ الشركات تتنافس في الأسواق؛ الأحزاب السياسية تتنافس للسلطة؛ الحركات تتنافس للتأثير. في كل ساحة، بينما يقترب الذكاء الاصطناعي من ويتجاوز القدرة البشرية، الضغط التنافسي سيجبر المشاركين على تفويض أو التنازل عن المزيد والمزيد من السيطرة لأنظمة الذكاء الاصطناعي - ليس لأن هؤلاء المشاركين يريدون، لكن لأنهم [لا يستطيعون تحمل عدم ذلك.](https://arxiv.org/abs/2303.16200)

كما مع مخاطر أخرى للذكاء الاصطناعي العام، نرى هذا فعلاً مع أنظمة أضعف. الطلاب يشعرون بضغط لاستخدام الذكاء الاصطناعي في واجباتهم، لأن بوضوح طلاباً آخرين كثيرين يفعلون. الشركات [تتدافع لتبني حلول الذكاء الاصطناعي لأسباب تنافسية.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) الفنانون والمبرمجون يشعرون بالإجبار لاستخدام الذكاء الاصطناعي وإلا ستُقطع أسعارهم من قبل آخرين يفعلون.

هذه تشعر مثل تفويض مضغوط، لكن ليس فقدان سيطرة. لكن لنرفع المخاطر ونقدم الساعة للأمام. تأمل مديراً تنفيذياً منافسوه يستخدمون "مساعدي" ذكاء اصطناعي عام لاتخاذ قرارات أسرع وأفضل، أو قائداً عسكرياً يواجه خصماً بقيادة وسيطرة محسنة بالذكاء الاصطناعي. نظام ذكاء اصطناعي متقدم كفاية يمكن أن يعمل مستقلاً بأضعاف السرعة والتطور والتعقد وقدرة معالجة البيانات البشرية، ساعياً لأهداف معقدة بطرق معقدة. مديرنا التنفيذي أو القائد، المسؤول عن نظام كهذا، قد يرى أنه ينجز ما يريدون؛ لكن هل سيفهمون حتى جزءاً صغيراً من *كيف* أُنجز؟ لا، سيضطرون لتقبله فقط. أكثر من ذلك، كثير مما قد يفعله النظام ليس مجرد تلقي أوامر لكن تقديم النصح لمدير مفترض حول ما يفعل. هذا النصح سيكون جيداً -- مراراً وتكراراً.

في أي نقطة، إذن، سيُقلل دور الإنسان إلى النقر على "نعم، امض قدماً"؟

يشعر جيداً أن تملك أنظمة ذكاء اصطناعي قادرة يمكنها تحسين إنتاجيتنا، والاهتمام بالعمل الممل المزعج، وحتى التصرف كشريك فكر في إنجاز الأشياء. سيشعر جيداً أن تملك مساعد ذكاء اصطناعي يمكنه الاهتمام بإجراءات لنا، مثل مساعد شخصي بشري جيد. سيشعر طبيعياً، حتى مفيداً، بينما يصبح الذكاء الاصطناعي ذكياً وكفوءاً وموثوقاً جداً، أن نؤجل المزيد والمزيد من القرارات له. لكن هذا التفويض "المفيد" له نقطة نهاية واضحة إذا استمررنا في الطريق: يوماً ما سنجد أننا لسنا مسؤولين حقاً عن كثير من أي شيء بعد الآن، وأن أنظمة الذكاء الاصطناعي التي تدير العرض فعلاً لا يمكن إيقافها أكثر من شركات النفط، أو وسائل التواصل الاجتماعي، أو الإنترنت، أو الرأسمالية.

وهذا النسخة الأكثر إيجابية كثيراً، التي فيها الذكاء الاصطناعي ببساطة مفيد وفعال جداً بحيث نتركه يتخذ معظم قراراتنا الرئيسية لنا. الواقع على الأرجح سيكون خليطاً أكثر بكثير بين هذا ونسخ حيث أنظمة الذكاء الاصطناعي العام غير المسيطر عليها *تأخذ* أشكالاً مختلفة من السلطة لنفسها لأن، تذكر، السلطة مفيدة لأي هدف تقريباً يملكه المرء، والذكاء الاصطناعي العام سيكون، بالتصميم، فعالاً على الأقل مثل البشر في سعيه لأهدافه.

سواء أعطينا السيطرة أو انتُزعت منا، فقدانها يبدو محتملاً بشدة. كما وضعها آلان تورنغ أصلاً، "...يبدو محتملاً أنه حين تبدأ طريقة تفكير الآلة، لن تأخذ وقتاً طويلاً لتتفوق على قوانا الضعيفة. لن تكون هناك مسألة موت الآلات، وستكون قادرة على التحادث مع بعضها البعض لتحدد عقولها. في مرحلة ما إذن يجب أن نتوقع الآلات أن تأخذ السيطرة..."

رجاءً لاحظ، برغم أنه واضح كفاية، أن فقدان السيطرة من قبل البشرية للذكاء الاصطناعي يتضمن أيضاً فقدان سيطرة الولايات المتحدة من قبل الحكومة الأمريكية؛ يعني فقدان سيطرة الصين من قبل الحزب الشيوعي الصيني، وفقدان سيطرة الهند وفرنسا والبرازيل وروسيا وكل دولة أخرى من قبل حكومتها الخاصة. لذا شركات الذكاء الاصطناعي، حتى لو لم يكن هذا قصدها، تشارك حالياً في الإطاحة المحتملة بحكومات العالم، منها حكومتها الخاصة. هذا يمكن أن يحدث في مسألة سنوات.

## الذكاء الاصطناعي العام سيؤدي إلى الذكاء الفائق

هناك حجة يمكن تقديمها أن الذكاء الاصطناعي العام المنافس للبشر أو حتى المنافس للخبراء ولأغراض عامة، حتى لو كان مستقلاً، يمكن أن يكون قابلاً للإدارة. قد يكون معطلاً بشكل لا يصدق بكل الطرق المناقشة أعلاه، لكن هناك الكثير من الناس الأذكياء جداً والوكلاء في العالم الآن، وهم قابلون للإدارة إلى حد أكثر أو أقل.[^26]

لكننا لن نصل للبقاء في المستوى البشري تقريباً. التقدم ما بعد ذلك على الأرجح سيدفعه نفس القوى التي رأيناها فعلاً: الضغط التنافسي بين مطوري الذكاء الاصطناعي الساعين للربح والسلطة، والضغط التنافسي بين مستخدمي الذكاء الاصطناعي الذين لا يستطيعون تحمل التخلف، و - الأهم - قدرة الذكاء الاصطناعي العام الخاصة على تحسين نفسه.

في عملية رأيناها تبدأ فعلاً مع أنظمة أقل قوة، الذكاء الاصطناعي العام نفسه سيكون قادراً على تصور وتصميم نسخ محسنة من نفسه. هذا يشمل الأجهزة والبرمجيات والشبكات العصبية والأدوات والسقالات، إلخ. سيكون، بالتعريف، أفضل منا في فعل هذا، لذا لا نعرف بالضبط كيف سيحسن الذكاء ذاتياً. لكن لن نضطر. بقدر ما نحتفظ بتأثير فيما يفعله الذكاء الاصطناعي العام، سنحتاج ببساطة لطلب ذلك منه، أو تركه.

ليس هناك حاجز بشري المستوى للإدراك يمكن أن يحمينا من هذا الخروج عن السيطرة.[^27]

تقدم الذكاء الاصطناعي العام إلى الذكاء الفائق ليس قانون طبيعة؛ سيبقى ممكناً كبح الخروج عن السيطرة، خاصة إذا كان الذكاء الاصطناعي العام مركزياً نسبياً وبقدر ما يُسيطر عليه من قبل أطراف لا تشعر بضغط للسباق مع بعضها البعض. لكن إذا انتشر الذكاء الاصطناعي العام على نطاق واسع وكان عالي الاستقلالية، يبدو مستحيلاً تقريباً منعه من تقرير أنه يجب أن يكون أكثر، ثم أكثر بعد، قوة.

## ما يحدث إذا بنينا (أو بنى الذكاء الاصطناعي العام) الذكاء الفائق

لنقل الأمر بصراحة، ليس لدينا فكرة عما سيحدث إذا بنينا الذكاء الفائق.[^28] سيتخذ إجراءات لا نستطيع تتبعها أو إدراكها لأسباب لا نستطيع فهمها نحو أهداف لا نستطيع تصورها. ما نعرفه أنه لن يكون متروكاً لنا.[^29]

استحالة السيطرة على الذكاء الفائق يمكن فهمها عبر تماثلات صارخة بشكل متزايد. أولاً، تخيل أنك مدير تنفيذي لشركة كبيرة. لا توجد طريقة يمكنك تتبع كل ما يجري، لكن مع الإعداد الصحيح للموظفين، يمكنك لا تزال فهم الصورة الكبيرة بشكل معنوي، واتخاذ قرارات. لكن افترض شيئاً واحداً فقط: كل شخص آخر في الشركة يعمل بمائة ضعف سرعتك. هل يمكنك لا تزال المتابعة؟

مع الذكاء الاصطناعي الفائق، الناس سيكونون "يأمرون" شيئاً ليس فقط أسرع، لكن يعمل في مستويات من التطور والتعقد لا يستطيعون فهمها، معالجاً بيانات أكثر بكثير مما يستطيعون حتى تصوره. عدم التناسب هذا يمكن وضعه في مستوى رسمي: [قانون آشبي للتنوع المطلوب](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (وانظر ["نظرية المنظم الجيد"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf) المترابطة) ينص، تقريباً، أن أي نظام سيطرة يجب أن يملك أزراراً ومقابض بعدد درجات حرية النظام المُسيطر عليه.

شخص يسيطر على نظام ذكاء اصطناعي فائق سيكون مثل سرخس يسيطر على جنرال موتورز: حتى لو كان "افعل ما يريده السرخس" مكتوباً في لوائح الشركة، الأنظمة مختلفة جداً في السرعة ونطاق العمل بحيث "السيطرة" ببساطة لا تنطبق. (وكم من الوقت حتى تُعاد كتابة تلك اللائحة المزعجة؟) [^30]

كما لا توجد أمثلة صفر للنباتات التي تسيطر على شركات فورتون 500، سيكون هناك بالضبط أمثلة صفر للناس الذين يسيطرون على الذكاء الفائق. هذا يقترب من كونه حقيقة رياضية.[^31] إذا بُني الذكاء الفائق - بغض النظر عن كيف وصلنا هناك - السؤال لن يكون ما إذا كان البشر يستطيعون السيطرة عليه، لكن ما إذا كنا سنستمر في الوجود، وإذا كان كذلك، ما إذا كان سيكون لدينا وجود جيد ومعنوي كأفراد أو كنوع. على هذه الأسئلة الوجودية للبشرية سيكون لدينا تأثير قليل. العصر البشري سينتهي.

## الخلاصة: يجب ألا نبني الذكاء الاصطناعي العام

هناك سيناريو فيه بناء الذكاء الاصطناعي العام قد يسير جيداً للبشرية: يُبنى بحذر، تحت السيطرة ولصالح البشرية، محكوم باتفاق متبادل من أصحاب مصلحة كثيرين،[^32] ومُمنع من التطور إلى ذكاء فائق غير قابل للسيطرة.

*هذا السيناريو ليس مفتوحاً لنا تحت الظروف الحالية.* كما ناقش في هذا القسم، باحتمالية عالية جداً، تطوير الذكاء الاصطناعي العام سيؤدي إلى مزيج من:

- تعطيل أو تدمير مجتمعي وحضاري هائل