# الفصل 8 - كيفية عدم بناء الذكاء الاصطناعي العام

الذكاء الاصطناعي العام ليس حتمياً - نحن اليوم نقف عند مفترق طرق. يقدم هذا الفصل اقتراحاً لكيفية منع بنائه.

إذا كان الطريق الذي نسلكه حالياً يؤدي إلى النهاية المحتملة لحضارتنا، فكيف نغير المسار؟

لنفترض أن الرغبة في وقف تطوير الذكاء الاصطناعي العام والذكاء الفائق كانت واسعة الانتشار وقوية،[^1] لأنه أصبح من المفهوم الشائع أن الذكاء الاصطناعي العام سيكون ماصاً للقوة وليس منحها للبشر، وخطراً جسيماً على المجتمع والإنسانية. كيف نغلق البوابات؟

في الوقت الحالي لا نعرف سوى طريقة واحدة *لصنع* ذكاء اصطناعي قوي وعام، وهي عبر الحوسبة الضخمة حقاً للشبكات العصبية العميقة. ولأن هذه أشياء صعبة ومكلفة بشكل لا يصدق، فهناك معنى يجعل *عدم* فعلها أمراً سهلاً.[^2] لكننا رأينا بالفعل القوى التي تدفع نحو الذكاء الاصطناعي العام، والديناميكيات الاستراتيجية التي تجعل من الصعب جداً على أي طرف التوقف من جانب واحد. لذا ستحتاج إلى مزيج من التدخل من الخارج (أي الحكومات) لإيقاف الشركات، واتفاقيات بين الحكومات لإيقاف أنفسها.[^3] كيف يمكن أن يبدو هذا؟

من المفيد أولاً التمييز بين تطورات الذكاء الاصطناعي التي يجب *منعها* أو *حظرها*، وتلك التي يجب *إدارتها*. الأولى ستكون في المقام الأول الانطلاق نحو الذكاء الفائق.[^4] بالنسبة للتطوير المحظور، يجب أن تكون التعريفات واضحة قدر الإمكان، وكل من التحقق والإنفاذ يجب أن يكونا عمليين. ما يجب *إداراته* سيكون أنظمة الذكاء الاصطناعي العامة والقوية - التي لدينا بالفعل، والتي ستكون لها العديد من المناطق الرمادية والتعقيدات والتفاصيل الدقيقة. بالنسبة لهذه، تُعتبر المؤسسات القوية والفعالة أمراً بالغ الأهمية.

قد نجد أيضاً فائدة في تحديد القضايا التي يجب معالجتها على المستوى الدولي (بما في ذلك بين المنافسين أو الخصوم الجيوسياسيين)[^5] من تلك التي يمكن للولايات القضائية الفردية أو البلدان أو مجموعات البلدان إدارتها. يقع التطوير المحظور إلى حد كبير في فئة "الدولي"، لأن الحظر المحلي على تطوير تقنية معينة يمكن عموماً تجاوزه بتغيير الموقع.[^6]

أخيراً، يمكننا النظر في الأدوات المتاحة في صندوق الأدوات. هناك العديد منها، بما في ذلك الأدوات التقنية، والقانون الناعم (المعايير والقواعد وما إلى ذلك)، والقانون الصارم (الأنظمة والمتطلبات)، والمسؤولية القانونية، وحوافز السوق، وما إلى ذلك. دعونا نولي اهتماماً خاصاً لإحداها المميزة للذكاء الاصطناعي.

## أمن وحكومة القوة الحاسوبية

ستكون الأجهزة التي يتطلبها الذكاء الاصطناعي عالي القوة أداة أساسية في حكومته. البرمجيات تنتشر بسهولة، ولها تكلفة إنتاج حدية تقارب الصفر، وتعبر الحدود بسهولة، ويمكن تعديلها فوراً؛ لا شيء من هذا ينطبق على الأجهزة. ومع ذلك، كما ناقشنا، هناك حاجة إلى كميات ضخمة من "القوة الحاسوبية" هذه أثناء كل من تدريب أنظمة الذكاء الاصطناعي وأثناء الاستنتاج لتحقيق أكثر الأنظمة قدرة. يمكن قياس القوة الحاسوبية وحسابها ومراجعتها بسهولة، مع غموض قليل نسبياً بمجرد وضع قواعد جيدة للقيام بذلك. والأهم من ذلك، أن كميات كبيرة من الحوسبة هي، مثل اليورانيوم المخصب، مورد نادر جداً ومكلف وصعب الإنتاج. وعلى الرغم من أن الرقائق الحاسوبية موجودة في كل مكان، إلا أن الأجهزة المطلوبة للذكاء الاصطناعي مكلفة وصعبة الصنع بشكل هائل.[^7]

ما يجعل الرقائق المتخصصة في الذكاء الاصطناعي أكثر قابلية للإدارة كمورد نادر من اليورانيوم هو أنها يمكن أن تتضمن آليات أمان قائمة على الأجهزة. معظم الهواتف المحمولة الحديثة، وبعض أجهزة الكمبيوتر المحمولة، لديها ميزات أجهزة متخصصة على الرقاقة تسمح لها بضمان تثبيت برامج وتحديثات نظام التشغيل المعتمدة فقط، وأن تحتفظ بالبيانات البيومترية الحساسة وتحميها على الجهاز، وأن تصبح عديمة الفائدة لأي شخص غير مالكها إذا فُقدت أو سُرقت. على مدى السنوات العديدة الماضية أصبحت مثل هذه التدابير الأمنية للأجهزة راسخة ومعتمدة على نطاق واسع، وثبت أنها آمنة بشكل عام.

الابتكار الرئيسي لهذه الميزات هو أنها تربط الأجهزة والبرمجيات معاً باستخدام التشفير.[^8] أي أن مجرد امتلاك قطعة معينة من أجهزة الكمبيوتر لا يعني أن المستخدم يمكنه فعل أي شيء يريده بها عن طريق تطبيق برمجيات مختلفة. وهذا الربط يوفر أيضاً أماناً قوياً لأن العديد من الهجمات ستتطلب اختراق أمن *الأجهزة* وليس فقط أمن *البرمجيات*.

أشارت عدة تقارير حديثة (على سبيل المثال من [GovAI وزملاؤهم](https://www.governance.ai/post/computing-power-and-the-governance-of-ai)، و[CNAS](https://www.cnas.org/publications/reports/secure-governable-chips)، و[RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) إلى أن ميزات أجهزة مماثلة مدمجة في أجهزة الحوسبة الحديثة ذات الصلة بالذكاء الاصطناعي يمكن أن تلعب دوراً مفيداً للغاية في أمن وحكومة الذكاء الاصطناعي. إنها تمكن عدداً من الوظائف المتاحة "للحاكم"[^9] التي قد لا يخمن المرء أنها متاحة أو حتى ممكنة. كأمثلة رئيسية:

- *تحديد الموقع الجغرافي*: يمكن إعداد الأنظمة بحيث تكون للرقائق مواقع معروفة، ويمكنها التصرف بشكل مختلف (أو إيقافها تماماً) بناءً على الموقع.[^10]
- *الاتصالات المسموحة*: يمكن تكوين كل رقاقة بقائمة مسموح بها مفروضة بالأجهزة لرقائق معينة أخرى يمكنها الاتصال بها، وتكون غير قادرة على الاتصال بأي رقائق ليست في هذه القائمة.[^11] هذا يمكن أن يحد من حجم مجموعات الرقائق التواصلية.[^12]
- *قياس الاستنتاج أو التدريب (وإيقاف التشغيل التلقائي)*: يمكن للحاكم ترخيص كمية معينة فقط من التدريب أو الاستنتاج (في الوقت، أو FLOP، أو ربما الرموز المميزة) ليتم تنفيذها من قبل مستخدم، وبعد ذلك تكون هناك حاجة لإذن جديد. إذا كانت الزيادات صغيرة، فإن إعادة ترخيص مستمرة نسبياً للنموذج مطلوبة. يمكن حينها "إيقاف تشغيل" النموذج ببساطة عن طريق حجب إشارة الترخيص هذه.[^13]
- *حد السرعة*: يُمنع النموذج من العمل بسرعة استنتاج أعلى من حد معين يحدده حاكم أو بطريقة أخرى. يمكن تنفيذ هذا عبر مجموعة محدودة من الاتصالات المسموح بها، أو بوسائل أكثر تطوراً.
- *التدريب المُشهد عليه*: يمكن لإجراء التدريب أن ينتج إثباتاً آمناً تشفيرياً أن مجموعة معينة من الأكواد والبيانات وكمية استخدام القوة الحاسوبية تم استخدامها في توليد النموذج.

## كيفية عدم بناء الذكاء الفائق: حدود عالمية على قوة الحوسبة للتدريب والاستنتاج

مع وضع هذه الاعتبارات - خاصة فيما يتعلق بالحوسبة - في الاعتبار، يمكننا مناقشة كيفية إغلاق البوابات أمام الذكاء الفائق الاصطناعي؛ ثم ننتقل إلى منع الذكاء الاصطناعي العام الكامل، وإدارة نماذج الذكاء الاصطناعي حيث تقترب وتتجاوز القدرة البشرية في جوانب مختلفة.

العنصر الأول هو، بطبيعة الحال، فهم أن الذكاء الفائق لن يكون قابلاً للسيطرة، وأن عواقبه لا يمكن التنبؤ بها أساساً. يجب على الأقل أن تقرر الصين والولايات المتحدة، بشكل مستقل، لهذا الغرض أو لأغراض أخرى، عدم بناء الذكاء الفائق.[^14] ثم هناك حاجة لاتفاقية دولية بينهما وبين آخرين، مع آلية تحقق وإنفاذ قوية، لضمان جميع الأطراف أن منافسيهم لا ينحرفون ويقررون المجازفة.

لتكون قابلة للتحقق والإنفاذ يجب أن تكون الحدود حدوداً صارمة، وواضحة قدر الإمكان. يبدو هذا كمشكلة مستحيلة تقريباً: تحديد قدرات البرمجيات المعقدة ذات الخصائص غير المتوقعة، على مستوى العالم. لحسن الحظ أن الوضع أفضل بكثير من ذلك، لأن الشيء ذاته الذي جعل الذكاء الاصطناعي المتقدم ممكناً - كمية ضخمة من القوة الحاسوبية - أسهل بكثير، بكثير في السيطرة عليه. وعلى الرغم من أنه قد يظل يسمح ببعض الأنظمة القوية والخطيرة، إلا أن *انطلاق الذكاء الفائق* يمكن منعه على الأرجح بوضع حد أقصى صارم على كمية الحوسبة التي تدخل في الشبكة العصبية، إلى جانب حد معدل على كمية الاستنتاج التي يمكن لنظام الذكاء الاصطناعي (المكون من شبكات عصبية متصلة وبرمجيات أخرى) أن يؤديها. نسخة محددة من هذا مقترحة أدناه.

قد يبدو أن وضع حدود عالمية صارمة على حوسبة الذكاء الاصطناعي سيتطلب مستويات ضخمة من التنسيق الدولي والمراقبة التطفلية المدمرة للخصوصية. لحسن الحظ، لن يتطلب ذلك. [سلسلة التوريد المحدودة للغاية وذات العنق الضيق](https://arxiv.org/abs/2402.08797) تنص على أنه بمجرد تحديد الحد قانونياً (سواء بالقانون أو الأمر التنفيذي)، فإن التحقق من الامتثال لذلك الحد سيتطلب فقط مشاركة وتعاون حفنة من الشركات الكبيرة.[^15]

خطة كهذه لها عدد من الميزات المرغوبة للغاية. إنها تدخلية بأقل قدر ممكن بمعنى أن عدداً قليلاً فقط من الشركات الكبرى لديها متطلبات موضوعة عليها، وفقط مجموعات كبيرة إلى حد ما من الحوسبة ستكون محكومة. الرقائق ذات الصلة تحتوي بالفعل على القدرات الأجهزة اللازمة للنسخة الأولى.[^16] كل من التنفيذ والإنفاذ يعتمدان على القيود القانونية المعيارية. لكن هذه مدعومة بشروط استخدام الأجهزة وبضوابط الأجهزة، مما يبسط الإنفاذ بشكل هائل ويمنع الغش من قبل الشركات أو المجموعات الخاصة أو حتى البلدان. هناك سابقة واسعة لشركات الأجهزة التي تضع قيوداً عن بُعد على استخدام أجهزتها، وقفل/فتح قدرات معينة خارجياً،[^17] بما في ذلك حتى في وحدات المعالجة المركزية عالية القوة في مراكز البيانات.[^18] حتى بالنسبة للجزء الصغير نسبياً من الأجهزة والمؤسسات المتأثرة، يمكن أن تقتصر الرقابة على القياس عن بُعد، دون وصول مباشر للبيانات أو النماذج نفسها؛ ويمكن أن تكون البرمجيات لهذا مفتوحة للفحص لإظهار أنه لا يتم تسجيل بيانات إضافية. المخطط دولي وتعاوني، ومرن وقابل للتوسيع إلى حد كبير. ولأن الحد يقع بشكل أساسي على الأجهزة وليس البرمجيات، فهو لا يتدخل نسبياً في كيفية حدوث تطوير ونشر برمجيات الذكاء الاصطناعي، وهو متوافق مع مجموعة متنوعة من النماذج بما في ذلك الذكاء الاصطناعي الأكثر "لامركزية" أو "عاماً" الهادف لمكافحة تركز القوة المدفوع بالذكاء الاصطناعي.

إغلاق البوابة المبني على الحوسبة له عيوب أيضاً. أولاً، هو بعيد عن كونه حلاً كاملاً لمشكلة حكومة الذكاء الاصطناعي بشكل عام. ثانياً، مع تطور أجهزة الكمبيوتر وتسارعها، سيتم "التقاط" المزيد والمزيد من الأجهزة في مجموعات أصغر وأصغر (أو حتى وحدات معالجة الرسوم الفردية).[^19] من الممكن أيضاً أنه بسبب التحسينات الخوارزمية قد يصبح حد حوسبي أقل ضرورياً،[^20] أو أن كمية الحوسبة تصبح غير ذات صلة إلى حد كبير وأن إغلاق البوابة سيستلزم بدلاً من ذلك نظام حكومة أكثر تفصيلاً قائماً على المخاطر أو القدرات للذكاء الاصطناعي. ثالثاً، بغض النظر عن الضمانات والعدد القليل من الكيانات المتأثرة، مثل هذا النظام مُلزم بخلق مقاومة فيما يتعلق بالخصوصية والمراقبة، من بين مخاوف أخرى.[^21]

بطبيعة الحال، تطوير وتنفيذ مخطط حكومة محدد للحوسبة في فترة زمنية قصيرة سيكون تحدياً كبيراً. لكنه بالتأكيد ممكن.

## ذ-ا-ع: التقاطع الثلاثي كأساس للمخاطر، والسياسة

دعونا الآن ننتقل إلى الذكاء الاصطناعي العام. الخطوط والتعريفات الصارمة هنا أكثر صعوبة، لأننا بالتأكيد لدينا ذكاء اصطناعي وعام، وبأي تعريف موجود لن يتفق الجميع إذا أو متى يوجد. علاوة على ذلك، حد القوة الحاسوبية أو الاستنتاج هو أداة بليدة إلى حد ما (كون الحوسبة بديلاً للقدرة، والتي هي بدورها بديل للمخاطر) - ما لم يكن منخفضاً تماماً - من غير المرجح أن يمنع الذكاء الاصطناعي العام القوي بما يكفي لإحداث تعطيل اجتماعي أو حضاري أو مخاطر حادة.

لقد احتججت بأن أشد المخاطر حدة تنشأ من التقاطع الثلاثي للقدرة العالية جداً، والاستقلالية العالية، والعمومية الكبيرة. هذه هي الأنظمة التي - إذا تم تطويرها على الإطلاق - يجب إدارتها بعناية هائلة. من خلال إنشاء معايير صارمة (من خلال المسؤولية والأنظمة) للأنظمة التي تجمع بين الخصائص الثلاث جميعاً، يمكننا توجيه تطوير الذكاء الاصطناعي نحو بدائل أكثر أماناً.

كما هو الحال مع الصناعات والمنتجات الأخرى التي يمكن أن تضر بالمستهلكين أو الجمهور، تتطلب أنظمة الذكاء الاصطناعي تنظيماً دقيقاً من قبل وكالات حكومية فعالة ومخولة. يجب أن يدرك هذا التنظيم المخاطر المتأصلة في الذكاء الاصطناعي العام، ويمنع تطوير أنظمة الذكاء الاصطناعي عالي القوة غير المقبولة المخاطر.[^22]

ومع ذلك، التنظيم واسع النطاق، خاصة مع أسنان حقيقية مؤكدة أن تعارضها الصناعة،[^23] يحتاج وقتاً[^24] بالإضافة إلى إقتناع سياسي بأنه ضروري.[^25] بالنظر إلى وتيرة التقدم، قد يستغرق هذا وقتاً أكثر مما لدينا.

على نطاق زمني أسرع بكثير وكما يتم تطوير التدابير التنظيمية، يمكننا منح الشركات الحوافز الضرورية (أ) للامتناع عن الأنشطة عالية المخاطر جداً و(ب) تطوير أنظمة شاملة لتقييم وتخفيف المخاطر، عن طريق توضيح وزيادة مستويات المسؤولية للأنظمة الأكثر خطورة. الفكرة ستكون فرض أعلى مستويات المسؤولية - مطلقة وفي بعض الحالات جنائية شخصية - للأنظمة في التقاطع الثلاثي للاستقلالية-العمومية-الذكاء العاليين، لكن توفير "ملاذات آمنة" للمسؤولية المعتادة المبنية على الخطأ للأنظمة التي تفتقر إلى إحدى تلك الخصائص أو مضمونة أن تكون قابلة للإدارة. أي، على سبيل المثال، نظام "ضعيف" عام ومستقل (مثل مساعد شخصي قادر وموثوق لكن محدود) سيخضع لمستويات مسؤولية أقل. وبالمثل نظام ضيق ومستقل مثل السيارة ذاتية القيادة سيظل يخضع للتنظيم الكبير الذي يخضع له بالفعل، لكن ليس مسؤولية محسنة. وكذلك بالنسبة لنظام عالي القدرة وعام ولكنه "سلبي" وعاجز إلى حد كبير عن العمل المستقل. الأنظمة التي تفتقر لاثنتين من الخصائص الثلاث أكثر قابلية للإدارة والملاذات الآمنة ستكون أسهل للمطالبة بها. هذا النهج يعكس كيف نتعامل مع التقنيات الأخرى المحتملة الخطورة:[^26] مسؤولية أعلى للتكوينات الأكثر خطورة تخلق حوافز طبيعية لبدائل أكثر أماناً.

النتيجة الافتراضية لمثل هذه المستويات العالية من المسؤولية، التي تعمل على *تدخيل* مخاطر الذكاء الاصطناعي العام للشركات بدلاً من تفريغها على الجمهور، من المحتمل (ونأمل!) أن تجعل الشركات ببساطة لا تطور الذكاء الاصطناعي العام الكامل حتى وما لم يتمكنوا حقاً من جعله جديراً بالثقة وآمناً وقابلاً للسيطرة نظراً لأن *قيادتهم ذاتها* هي الأطراف المعرضة للخطر. (في حالة عدم كفاية هذا، يجب أن يسمح التشريع الذي يوضح المسؤولية صراحة أيضاً بالإنصاف الزجري، أي أن يأمر قاضٍ بوقف، للأنشطة التي تكون بوضوح في منطقة الخطر وتشكل مخاطر عامة على الأرجح.) مع وضع الأنظمة، يمكن أن يصبح الالتزام بالأنظمة الملاذ الآمن، ويمكن أن تتحول الملاذات الآمنة من الاستقلالية المنخفضة أو الضيق أو ضعف أنظمة الذكاء الاصطناعي إلى أنظمة تنظيمية أخف نسبياً.

## الأحكام الرئيسية لإغلاق البوابة

مع المناقشة أعلاه في الاعتبار، يقدم هذا القسم اقتراحات للأحكام الرئيسية التي ستنفذ وتحافظ على حظر الذكاء الاصطناعي العام الكامل والذكاء الفائق، وإدارة الذكاء الاصطناعي عام الغرض المنافس للبشر أو المتفوق على الخبراء بالقرب من عتبة الذكاء الاصطناعي العام الكامل.[^27] لديها أربع قطع رئيسية: 1) محاسبة ورقابة القوة الحاسوبية، 2) حدود القوة الحاسوبية في التدريب والتشغيل للذكاء الاصطناعي، 3) إطار مسؤولية، و4) معايير أمان وأمن متدرجة محددة تتضمن متطلبات تنظيمية صارمة. هذه موصوفة بإيجاز تالياً، مع تفاصيل إضافية أو أمثلة تنفيذ معطاة في ثلاثة جداول مرافقة. من المهم، لاحظ أن هذه بعيدة عن كل ما سيكون ضرورياً لحكم أنظمة الذكاء الاصطناعي المتقدمة؛ بينما ستكون لها فوائد أمان وأمن إضافية، فهي تهدف إلى إغلاق البوابة أمام انطلاق الذكاء، وإعادة توجيه تطوير الذكاء الاصطناعي في اتجاه أفضل.

### 1. محاسبة القوة الحاسوبية، والشفافية

- يجب على منظمة معايير (مثل NIST في الولايات المتحدة متبوعة بـ ISO/IEEE دولياً) أن تُقنن معياراً تقنياً مفصلاً للقوة الحاسوبية الإجمالية المستخدمة في تدريب وتشغيل نماذج الذكاء الاصطناعي، بـ FLOP، والسرعة بـ FLOP/s التي تعمل بها. تفاصيل لكيف يمكن أن يبدو هذا معطاة في الملحق أ.[^28]
- يجب فرض متطلب - إما بتشريع جديد أو تحت سلطة موجودة[^29] - من قبل الولايات القضائية التي يحدث فيها تدريب الذكاء الاصطناعي واسع النطاق لحساب وإبلاغ هيئة تنظيمية أو وكالة أخرى بإجمالي FLOP المستخدم في تدريب وتشغيل جميع النماذج فوق عتبة 10<sup>25</sup> FLOP أو 10<sup>18</sup> FLOP/s.[^30]
- يجب أن تُدخل هذه المتطلبات على مراحل، تتطلب في البداية تقديرات حسنة النية موثقة جيداً على أساس فصلي، مع مراحل لاحقة تتطلب معايير أعلى تدريجياً، وصولاً إلى إجمالي FLOP و FLOP/s مُشهد عليه تشفيرياً مرفق بكل *مخرج* نموذج.
- يجب أن تُكمل هذه التقارير بتقديرات موثقة جيداً لتكلفة الطاقة والمالية الحدية المستخدمة في توليد كل مخرج ذكاء اصطناعي.

المنطق: هذه الأرقام المحسوبة جيداً والمبلغ عنها بشفافية ستوفر الأساس لحدود التدريب والتشغيل، بالإضافة إلى ملاذ آمن من تدابير المسؤولية الأعلى (انظر الملاحق ج ود).

### 2. حدود قوة حوسبة التدريب والتشغيل

- يجب على الولايات القضائية التي تستضيف أنظمة الذكاء الاصطناعي فرض حد أقصى صارم على إجمالي الحوسبة الداخلة في أي مخرج نموذج ذكاء اصطناعي، بدءاً من 10<sup>27</sup> FLOP[^31] وقابل للتعديل حسب الاقتضاء.
- يجب على الولايات القضائية التي تستضيف أنظمة الذكاء الاصطناعي فرض حد أقصى صارم على معدل حوسبة مخرجات نماذج الذكاء الاصطناعي، بدءاً من 10<sup>20</sup> FLOP/s وقابل للتعديل حسب الاقتضاء.

المنطق: إجمالي الحوسبة، وإن كان غير مثالي جداً، هو بديل لقدرة الذكاء الاصطناعي (والمخاطر) قابل للقياس والتحقق بشكل ملموس، لذا يوفر نقطة إيقاف صارمة لتحديد القدرات. اقتراح تنفيذ ملموس معطى في الملحق ب.

### 3. مسؤولية محسنة للأنظمة الخطيرة

- يجب توضيح إنشاء وتشغيل[^32] نظام ذكاء اصطناعي متقدم عام وقادر ومستقل بدرجة عالية، عبر التشريع ليخضع للمسؤولية المطلقة والمشتركة والتضامنية، وليس المسؤولية القائمة على خطأ طرف واحد.[^33]
- يجب أن تكون عملية قانونية متاحة لعمل قضايا أمان إيجابية، والتي ستمنح ملاذاً آمناً من المسؤولية المطلقة للأنظمة الصغيرة (من ناحية الحوسبة)، أو الضعيفة، أو الضيقة، أو السلبية، أو التي لديها ضمانات أمان وأمن وقابلية سيطرة كافية.
- يجب تحديد مسار صريح ومجموعة شروط للإنصاف الزجري لوقف أنشطة تدريب واستنتاج الذكاء الاصطناعي التي تشكل خطراً عاماً.

المنطق: أنظمة الذكاء الاصطناعي لا يمكن أن تُحمل المسؤولية، لذا يجب أن نحمل الأفراد والمؤسسات البشرية المسؤولية عن الضرر الذي يسببونه (المسؤولية).[^34] الذكاء الاصطناعي العام غير القابل للسيطرة تهديد للمجتمع والحضارة وفي غياب قضية أمان يجب اعتباره خطيراً بشكل غير طبيعي. وضع عبء المسؤولية على المطورين لإظهار أن النماذج القوية آمنة بما يكفي لعدم اعتبارها "خطيرة بشكل غير طبيعي" يحفز التطوير الآمن، إلى جانب الشفافية وحفظ السجلات للمطالبة بتلك الملاذات الآمنة. يمكن للتنظيم حينها منع الضرر حيث الردع من المسؤولية غير كافٍ. أخيراً، مطورو الذكاء الاصطناعي مسؤولون بالفعل عن الأضرار التي يسببونها، لذا التوضيح القانوني للمسؤولية عن أكثر الأنظمة خطورة يمكن أن يُعمل فوراً، دون تطوير معايير مفصلة جداً؛ هذه يمكن أن تتطور حينها مع الوقت. التفاصيل معطاة في الملحق ج.

### 4. تنظيم الأمان للذكاء الاصطناعي

نظام تنظيمي يعالج المخاطر الحادة واسعة النطاق للذكاء الاصطناعي سيتطلب كحد أدنى:

- تحديد أو إنشاء مجموعة مناسبة من الهيئات التنظيمية، ربما وكالة جديدة؛
- إطار تقييم مخاطر شامل؛[^35]
- إطار لقضايا أمان إيجابية، مبني جزئياً على إطار تقييم المخاطر، ليتم عمله من قبل المطورين، وللمراجعة من قبل مجموعات ووكالات *مستقلة*؛
- نظام ترخيص متدرج، مع طبقات تتبع مستويات القدرة.[^36] ستُمنح التراخيص على أساس قضايا الأمان والمراجعات، لتطوير ونشر الأنظمة. ستتراوح المتطلبات من الإخطار في النهاية المنخفضة، إلى ضمانات أمان وأمن وقابلية سيطرة كمية قبل التطوير، في النهاية العليا. هذه ستمنع إطلاق الأنظمة حتى تُظهر أنها آمنة، وتحظر تطوير الأنظمة غير الآمنة جوهرياً. الملحق د يقدم اقتراحاً لما يمكن أن تستلزمه معايير الأمان والأمن هذه.
- اتفاقيات لجلب مثل هذه التدابير إلى المستوى الدولي، بما في ذلك هيئات دولية لتنسيق القواعد والمعايير، وربما وكالات دولية لمراجعة قضايا الأمان.

المنطق: في النهاية، المسؤولية ليست الآلية الصحيحة لمنع المخاطر واسعة النطاق للجمهور من تقنية جديدة. التنظيم الشامل، مع هيئات تنظيمية مخولة، سيكون مطلوباً للذكاء الاصطناعي كما هو الحال لكل صناعة رئيسية أخرى تشكل مخاطر على الجمهور.[^37]

التنظيم نحو منع المخاطر المنتشرة الأخرى ولكن الأقل حدة من المحتمل أن يختلف في شكله من ولاية قضائية إلى أخرى. الشيء المهم هو تجنب تطوير أنظمة الذكاء الاصطناعي الخطيرة جداً بحيث تصبح هذه المخاطر غير قابلة للإدارة.

## ماذا بعد ذلك؟

على مدى العقد القادم، مع انتشار الذكاء الاصطناعي أكثر وتقدم التقنية الأساسية، من المحتمل أن يحدث شيئان رئيسيان. أولاً، تنظيم أنظمة الذكاء الاصطناعي القوية الموجودة سيصبح أكثر صعوبة، ومع ذلك أكثر ضرورة. من المحتمل أن بعض التدابير على الأقل التي تعالج مخاطر الأمان واسعة النطاق ستتطلب اتفاقاً على المستوى الدولي، مع ولايات قضائية فردية تنفذ قواعد مبنية على اتفاقيات دولية.

ثانياً، حدود قوة حوسبة التدريب والتشغيل ستصبح أصعب للحفاظ عليها مع أن الأجهزة تصبح أرخص وأكثر كفاءة في التكلفة؛ قد تصبح أيضاً أقل صلة (أو تحتاج لأن تكون أكثر إحكاماً حتى) مع التقدم في الخوارزميات والهياكل.

أن السيطرة على الذكاء الاصطناعي ستصبح أصعب لا يعني أننا يجب أن نستسلم! تنفيذ الخطة المحددة في هذا المقال سيعطينا كلاً من الوقت القيم والسيطرة المهمة على العملية التي ستضعنا في موقف أفضل بكثير، بكثير لتجنب المخاطر الوجودية للذكاء الاصطناعي على مجتمعنا وحضارتنا ونوعنا.

في المدى الأطول بعد، ستكون هناك خيارات لنتخذها بخصوص ما نسمح به. قد نختار مع ذلك إنشاء شكل من الذكاء الاصطناعي العام القابل للسيطرة حقاً، إلى الدرجة التي يثبت أن هذا ممكن. أو قد نقرر أن تشغيل العالم أفضل أن يُترك للآلات، إذا استطعنا إقناع أنفسنا أنها ستقوم بعمل أفضل فيه، وتعاملنا جيداً. لكن هذه يجب أن تكون قرارات مُتخذة مع فهم علمي عميق للذكاء الاصطناعي في اليد، وبعد نقاش عالمي شامل ومعنوي، وليس في سباق بين أباطرة التقنية مع معظم الإنسانية غير مشاركة تماماً وغير مدركة.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) ملخص حكومة الذكاء الاصطناعي العام والذكاء الفائق عبر المسؤولية والتنظيم. المسؤولية هي الأعلى، والتنظيم هو الأقوى، في التقاطع الثلاثي للاستقلالية والعمومية والذكاء. يمكن الحصول على ملاذات آمنة من المسؤولية المطلقة والتنظيم القوي عبر قضايا أمان إيجابية تُظهر أن النظام ضعيف و/أو ضيق و/أو سلبي. حدود إجمالي قوة الحوسبة للتدريب ومعدل قوة حوسبة الاستنتاج، مُتحقق منها ومُنفذة قانونياً وباستخدام تدابير أمان الأجهزة والتشفير، تدعم الأمان بتجنب الذكاء الاصطناعي العام الكامل ومنع الذكاء الفائق فعلياً.

[^1]: على الأرجح، انتشار هذا الإدراك سيحتاج إما جهداً مكثفاً من مجموعات التعليم والدعوة التي تقدم هذه الحجة، أو كارثة ذكاء اصطناعي كبيرة إلى حد ما. يمكننا أن نأمل أن يكون الأول.

[^2]: من المفارقات، اعتدنا على الطبيعة تحد من تقنياتنا بجعل تطويرها صعباً جداً، خاصة علمياً. لكن هذا لم يعد هو الحال بالنسبة للذكاء الاصطناعي: المشاكل العلمية الرئيسية تتبين أنها أسهل من المتوقع. لا يمكننا الاعتماد على الطبيعة لإنقاذنا من أنفسنا هنا - سيتوجب علينا أن نفعل ذلك.

[^3]: أين، بالضبط، نتوقف في تطوير أنظمة جديدة؟ هنا، يجب أن نتبنى مبدأ الحذر. بمجرد نشر نظام، وخاصة بمجرد انتشار ذلك المستوى من قدرة النظام، من الصعب للغاية التراجع. وإذا *تم تطوير* نظام (خاصة بتكلفة وجهد كبيرين)، ستكون هناك ضغوط هائلة لاستخدامه أو نشره، وإغراء لتسريبه أو سرقته. تطوير الأنظمة *ثم* تقرير ما إذا كانت غير آمنة بعمق طريق خطير.

[^4]: سيكون من الحكمة أيضاً منع تطوير الذكاء الاصطناعي الخطير جوهرياً، مثل الأنظمة ذاتية التكاثر والتطور، وتلك المصممة للهروب من الحاوية، وتلك التي يمكنها التحسن الذاتي المستقل، والذكاء الاصطناعي الخادع والخبيث عمداً، إلخ.

[^5]: لاحظ أن هذا لا يعني بالضرورة *مُنفذاً* على المستوى الدولي من قبل نوع من الهيئة العالمية: بدلاً من ذلك يمكن للأمم ذات السيادة إنفاذ قواعد متفق عليها، كما في العديد من المعاهدات.

[^6]: كما سنرى أدناه، طبيعة حوسبة الذكاء الاصطناعي ستسمح بشيء من التهجين؛ لكن التعاون الدولي سيظل مطلوباً.

[^7]: على سبيل المثال، الآلات المطلوبة لحفر الرقائق ذات الصلة بالذكاء الاصطناعي مصنوعة من قبل شركة واحدة فقط، ASML (رغم محاولات كثيرة أخرى للقيام بذلك)، الغالبية العظمى من الرقائق ذات الصلة مُصنعة من قبل شركة واحدة، TSMC (رغم محاولات أخرى للمنافسة)، وتصميم وبناء الأجهزة من تلك الرقائق يتم من قبل قلة فقط بما في ذلك NVIDIA وAMD وGoogle.

[^8]: الأهم، كل رقاقة تحمل مفتاحاً تشفيرياً خاصاً فريداً وغير قابل للوصول يمكنها استخدامه "لتوقيع" الأشياء.

[^9]: افتراضياً سيكون هذا الشركة التي تبيع الرقائق، لكن نماذج أخرى ممكنة ومفيدة محتملاً.

[^10]: يمكن للحاكم تحديد موقع الرقاقة عن طريق توقيت تبادل الرسائل الموقعة معها: سرعة الضوء المحدودة تتطلب أن تكون الرقاقة ضمن نصف قطر معين *r* من "محطة" إذا استطاعت إرجاع رسالة موقعة في وقت أقل من *r* / *c*، حيث *c* هي سرعة الضوء. باستخدام محطات متعددة، وبعض الفهم لخصائص الشبكة، يمكن تحديد موقع الرقاقة. جمال هذه الطريقة هو أن معظم أمانها مزود بقوانين الفيزياء. طرق أخرى يمكن أن تستخدم GPS، وتتبع القصور الذاتي، وتقنيات مماثلة.

[^11]: بدلاً من ذلك، يمكن السماح لأزواج الرقائق بالتواصل مع بعضها البعض فقط عبر إذن صريح من حاكم.

[^12]: هذا بالغ الأهمية لأنه على الأقل حالياً، اتصال عرض النطاق العالي جداً بين الرقائق مطلوب لتدريب نماذج ذكاء اصطناعي كبيرة عليها.

[^13]: يمكن أيضاً إعداد هذا لتطلب رسائل موقعة من *N* من *M* حكام مختلفين، مما يسمح لأطراف متعددة بمشاركة الحكومة.

[^14]: هذا بعيد عن كونه مسبوق - على سبيل المثال الجيوش لم تطور جيوشاً من الجنود الخارقين المستنسخين أو المعدلين وراثياً، رغم أن هذا ممكن تقنياً على الأرجح. لكنهم *اختاروا* عدم فعل هذا، بدلاً من منعهم من قبل آخرين. السجل التاريخي ليس رائعاً للقوى العالمية الرئيسية التي يُمنع من تطوير تقنية يرغبون بقوة في تطويرها.

[^15]: مع استثناءين ملحوظين (خاصة NVIDIA) الأجهزة المتخصصة في الذكاء الاصطناعي جزء صغير نسبياً من نموذج العمل والإيرادات الإجمالي لهذه الشركات. علاوة على ذلك، الفجوة بين الأجهزة المستخدمة في الذكاء الاصطناعي المتقدم والأجهزة "درجة المستهلك" كبيرة، لذا معظم مستهلكي أجهزة الكمبيوتر سيكونون غير متأثرين إلى حد كبير.

[^16]: للتحليل الأكثر تفصيلاً، انظر التقارير الحديثة من [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) و[CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). هذه تركز على الجدوى التقنية، خاصة في سياق ضوابط التصدير الأمريكية الساعية لتقييد قدرة البلدان الأخرى في الحوسبة عالية الجودة؛ لكن هذا له تداخل واضح مع القيد العالمي المُتصور هنا.

[^17]: أجهزة Apple، على سبيل المثال، مقفلة عن بُعد وبأمان عندما يُبلغ عن فقدانها أو سرقتها، ويمكن إعادة تفعيلها عن بُعد. هذا يعتمد على نفس ميزات أمان الأجهزة المناقشة هنا.

[^18]: انظر مثلاً عرض IBM's [capacity on demand](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand)، Intel's [Intel on demand.](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html)، وApple's [private cloud compute](https://security.apple.com/blog/private-cloud-compute/).

[^19]: [هذه الدراسة](https://epochai.org/trends#hardware-trends-section) تظهر أن تاريخياً تم تحقيق نفس الأداء باستخدام حوالي 30% أقل من الدولارات كل عام. إذا استمر هذا الاتجاه، قد يكون هناك تداخل كبير بين استخدام رقائق الذكاء الاصطناعي و"المستهلك"، وبشكل عام كمية الأجهزة المطلوبة لأنظمة الذكاء الاصطناعي عالي القوة يمكن أن تصبح صغيرة بشكل مقلق.

[^20]: وفقاً [لنفس الدراسة](https://epochai.org/trends#hardware-trends-section)، أداء معطى في التعرف على الصور تطلب 2.5x أقل حوسبة كل عام. إذا كان هذا سيصدق أيضاً على أكثر أنظمة الذكاء الاصطناعي قدرة كذلك، حد حوسبي لن يكون مفيداً لفترة طويلة جداً.

[^21]: خاصة، على مستوى البلد هذا يبدو كثيراً مثل تأميم الحوسبة، في أن الحكومة ستكون لها سيطرة كبيرة على كيفية استخدام القوة الحاسوبية. ومع ذلك، لأولئك القلقين بشأن مشاركة الحكومة، هذا يبدو أكثر أماناً ومفضلاً عن برمجيات الذكاء الاصطناعي الأكثر قوة *نفسها* التي يتم تأميمها عبر نوع من الاندماج بين شركات الذكاء الاصطناعي الرئيسية والحكومات الوطنية، كما يبدأ البعض في الدعوة إليه.

[^22]: خطوة تنظيمية رئيسية في أوروبا اتُخذت مع إقرار 2024 لـ[قانون الذكاء الاصطناعي الأوروبي.](https://artificialintelligenceact.eu/) إنه يصنف الذكاء الاصطناعي بالمخاطر: يحظر الأنظمة غير المقبولة، وينظم عالية المخاطر، ويفرض قواعد شفافية، أو لا تدابير على الإطلاق، على أنظمة من