# الفصل التاسع - هندسة المستقبل — ما يجب أن نفعله بدلاً من ذلك

يمكن للذكاء الاصطناعي أن يحقق خيراً لا يُصدق في العالم. وللحصول على جميع الفوائد دون المخاطر، يجب أن نضمن بقاء الذكاء الاصطناعي أداة بشرية.

إذا نجحنا في اختيار عدم استبدال البشرية بالآلات - على الأقل لفترة من الوقت! - فماذا يمكننا أن نفعل بدلاً من ذلك؟ هل نتخلى عن الإمكانات الهائلة للذكاء الاصطناعي كتقنية؟ على مستوى معين، الإجابة بسيطة: *لا* - أغلقوا البوابات أمام الذكاء الاصطناعي العام والذكاء الفائق غير القابلين للسيطرة، لكن *اصنعوا* أشكالاً أخرى كثيرة من الذكاء الاصطناعي، إضافة إلى هياكل الحكم والمؤسسات التي سنحتاجها لإدارتها.

لكن لا يزال هناك الكثير لنقوله؛ فتحقيق هذا سيكون مهمة مركزية للبشرية. يستكشف هذا القسم عدة محاور رئيسية:

- كيف يمكننا توصيف الذكاء الاصطناعي "الأداتي" والأشكال التي يمكن أن يتخذها.
- أنه يمكننا الحصول على (تقريباً) كل ما تريده البشرية دون الذكاء الاصطناعي العام، باستخدام الذكاء الاصطناعي الأداتي.
- أن أنظمة الذكاء الاصطناعي الأداتي قابلة للإدارة (على الأرجح، من حيث المبدأ).
- أن التحول بعيداً عن الذكاء الاصطناعي العام لا يعني التنازل عن الأمن القومي - بل العكس تماماً.
- أن تركز السلطة مصدر قلق حقيقي. هل يمكننا التخفيف من ذلك دون تقويض السلامة والأمن؟
- أننا سنريد - ونحتاج - هياكل حكم واجتماعية جديدة، ويمكن للذكاء الاصطناعي أن يساعد فعلاً في ذلك.

## الذكاء الاصطناعي داخل البوابات: الذكاء الاصطناعي الأداتي

يوفر مخطط التقاطع الثلاثي طريقة جيدة لتحديد ما يمكن أن نسميه "الذكاء الاصطناعي الأداتي": الذكاء الاصطناعي الذي يبقى أداة قابلة للسيطرة للاستخدام البشري، وليس منافساً أو بديلاً غير قابل للسيطرة. أقل أنظمة الذكاء الاصطناعي إشكالاً هي تلك المستقلة لكن ليست عامة أو فائقة القدرة (مثل روبوت المزايدة في المزادات)، أو العامة لكن ليست مستقلة أو قادرة (مثل نموذج لغوي صغير)، أو القادرة لكن محدودة وقابلة للسيطرة جداً (مثل AlphaGo).[^1] تلك التي تحمل خاصيتين متقاطعتين لها تطبيق أوسع لكن مخاطر أعلى وستتطلب جهوداً كبيرة لإدارتها. (مجرد كون نظام الذكاء الاصطناعي أكثر شبهاً بأداة لا يعني أنه آمن بطبيعته، وإنما أنه ليس *غير آمن* بطبيعته - فكروا في المنشار مقابل نمر أليف.) يجب أن تبقى البوابة مغلقة أمام الذكاء الاصطناعي العام (الكامل) والذكاء الفائق عند التقاطع الثلاثي، ويجب توخي الحذر الشديد مع أنظمة الذكاء الاصطناعي التي تقترب من تلك العتبة.

لكن هذا يترك الكثير من الذكاء الاصطناعي القوي! يمكننا الحصول على فائدة هائلة من "قوارير المشورة" الذكية والعامة والسلبية والأنظمة المحدودة، والأنظمة العامة على المستوى البشري وليس فوق البشري، وهكذا. العديد من شركات التكنولوجيا والمطورين يبنون فعلياً هذه الأنواع من الأدوات ويجب أن يستمروا؛ مثل معظم الناس، فهم يفترضون ضمنياً أن البوابات أمام الذكاء الاصطناعي العام والذكاء الفائق *ستُغلق*.[^2]

كما يمكن دمج أنظمة الذكاء الاصطناعي فعلياً في أنظمة مركبة تحافظ على الإشراف البشري مع تعزيز القدرة. بدلاً من الاعتماد على صناديق سوداء غامضة، يمكننا بناء أنظمة تعمل فيها مكونات متعددة - بما في ذلك الذكاء الاصطناعي والبرمجيات التقليدية - معاً بطرق يمكن للبشر مراقبتها وفهمها.[^3] بينما قد تكون بعض المكونات صناديق سوداء، لن يكون أي منها قريباً من الذكاء الاصطناعي العام - فقط النظام المركب ككل سيكون عاماً وقادراً جداً، وذلك بطريقة قابلة للسيطرة بدقة.[^4]

### السيطرة البشرية المعنوية والمضمونة

ماذا تعني "قابلة للسيطرة بدقة"؟ الفكرة الرئيسية لإطار "الأداة" هي السماح للأنظمة - حتى لو كانت عامة وقوية جداً - أن تكون مضمونة السيطرة البشرية المعنوية. ماذا يعني هذا؟ ينطوي على جانبين. الأول اعتبار تصميمي: يجب أن يكون البشر مشاركين بعمق ومحوريتهم فيما يفعله النظام، *دون* تفويض القرارات الرئيسية المهمة للذكاء الاصطناعي. هذه هي طبيعة معظم أنظمة الذكاء الاصطناعي الحالية. ثانياً، بقدر ما تكون أنظمة الذكاء الاصطناعي مستقلة، يجب أن تملك ضمانات تحد من نطاق عملها. الضمان يجب أن يكون *رقماً* يوصف احتمال حدوث شيء ما، وسبباً للإيمان بذلك الرقم. هذا ما نطلبه في مجالات أخرى حرجة للسلامة، حيث تُحسب وتُدعم وتُنشر أرقام مثل "متوسط الوقت بين الأعطال" والأعداد المتوقعة للحوادث في حالات السلامة.[^5] الرقم المثالي للأعطال هو صفر، بالطبع. والأخبار الجيدة أننا قد نقترب كثيراً، وإن باستخدام معمارات ذكاء اصطناعي مختلفة تماماً، باستخدام أفكار الخصائص *المثبتة رسمياً* للبرامج (بما في ذلك الذكاء الاصطناعي). الفكرة، التي استكشفها بإسهاب أوموهوندرو وتيجمارك وبينجيو ودالريمبل وآخرون (انظروا [هنا](https://arxiv.org/abs/2309.01933) و[هنا](https://arxiv.org/abs/2405.06624)) هي إنشاء برنامج بخصائص معينة (مثلاً: أن بإمكان الإنسان إغلاقه) و*إثبات* تلك الخصائص رسمياً. يمكن القيام بهذا الآن لبرامج قصيرة جداً وخصائص بسيطة، لكن القوة (القادمة) لبرمجيات الإثبات المدعومة بالذكاء الاصطناعي قد تسمح بذلك لبرامج أكثر تعقيداً بكثير (مثل الأغلفة) وحتى للذكاء الاصطناعي نفسه. هذا برنامج طموح جداً، لكن مع تزايد الضغط على البوابات، سنحتاج لبعض المواد القوية لتعزيزها. الإثبات الرياضي قد يكون أحد القليل القوي بما يكفي.

### أين تتجه صناعة الذكاء الاصطناعي

مع إعادة توجيه تقدم الذكاء الاصطناعي، سيبقى الذكاء الاصطناعي الأداتي صناعة ضخمة. من ناحية الأجهزة، حتى مع حدود القوة الحاسوبية لمنع الذكاء الفائق، سيتطلب تدريب واستنتاج النماذج الأصغر كميات ضخمة من المكونات المتخصصة. من ناحية البرمجيات، إبطال انفجار حجم نماذج الذكاء الاصطناعي والحوسبة يجب أن يقود ببساطة الشركات لإعادة توجيه الموارد نحو تحسين الأنظمة الأصغر وجعلها أكثر تنوعاً وتخصصاً، بدلاً من مجرد تكبيرها.[^6] سيكون هناك مساحة كافية - وربما أكثر - لكل تلك الشركات الناشئة الربحية في وادي السيليكون.[^7]

## الذكاء الاصطناعي الأداتي يمكن أن يحقق (تقريباً) كل ما تريده البشرية، دون الذكاء الاصطناعي العام

الذكاء، سواء كان بيولوجياً أو آلياً، يمكن اعتباره على نطاق واسع القدرة على التخطيط وتنفيذ أنشطة تحقق مستقبلات أكثر انسجاماً مع مجموعة من الأهداف. وبالتالي، فالذكاء مفيد بشكل هائل عند استخدامه في سعي لأهداف مختارة بحكمة. الذكاء الاصطناعي يجذب استثمارات ضخمة من الوقت والجهد إلى حد كبير بسبب فوائده الموعودة. لذا يجب أن نسأل: إلى أي مدى سنحصد ثمار فوائد الذكاء الاصطناعي إذا احتوينا هروبه نحو الذكاء الفائق؟ الإجابة: قد نفقد القليل بشكل مفاجئ.

تأملوا أولاً أن أنظمة الذكاء الاصطناعي الحالية قوية جداً بالفعل، ولم نخدش سوى السطح لما يمكن القيام به بها.[^8] هي قادرة بشكل معقول على "إدارة العرض" من ناحية "فهم" سؤال أو مهمة مقدمة إليها، وما سيتطلبه الإجابة على هذا السؤال أو القيام بتلك المهمة.

ثانياً، الكثير من الحماس حول أنظمة الذكاء الاصطناعي الحديثة يرجع لعموميتها؛ لكن بعض أقدر أنظمة الذكاء الاصطناعي - مثل تلك التي تولد أو تتعرف على الكلام أو الصور، وتقوم بالتنبؤ والنمذجة العلمية، وتلعب الألعاب، إلخ - أكثر محدودية وتقع "داخل البوابات" من ناحية الحوسبة.[^9] هذه الأنظمة فوق بشرية في المهام المحددة التي تقوم بها. قد تملك نقاط ضعف في الحالات الحدية[^10] (أو [قابلة للاستغلال](https://arxiv.org/abs/2211.00241)) بسبب محدوديتها؛ مع ذلك فالمحدود *كلياً* أو العام *تماماً* ليسا الخيارين الوحيدين المتاحين: هناك معمارات كثيرة بينهما.[^11]

هذه الأدوات من الذكاء الاصطناعي يمكن أن تسرع التقدم كثيراً في تقنيات إيجابية أخرى، دون الذكاء الاصطناعي العام. لتحسين فيزياء نووية، لا نحتاج للذكاء الاصطناعي أن يكون عالم فيزياء نووية - لدينا هؤلاء! إذا أردنا تسريع الطب، أعطوا علماء الأحياء والباحثين الطبيين والكيميائيين أدوات قوية. هم يريدونها وسيستخدمونها لربح هائل. لا نحتاج لمزرعة خوادم مليئة بمليون عبقري رقمي؛ لدينا ملايين البشر الذي يمكن للذكاء الاصطناعي أن يساعد في إخراج عبقريتهم. نعم، سيستغرق وقتاً أطول للحصول على الخلود وعلاج جميع الأمراض. هذه تكلفة حقيقية. لكن حتى أكثر الابتكارات الصحية الواعدة ستكون قليلة الفائدة إذا قاد عدم الاستقرار المدفوع بالذكاء الاصطناعي إلى صراع عالمي أو انهيار اجتماعي. نحن مدينون لأنفسنا أن نمنح البشر المدعومين بالذكاء الاصطناعي فرصة لحل المشكلة أولاً.

ولنفترض أن هناك، في الواقع، منفعة هائلة للذكاء الاصطناعي العام لا يمكن الحصول عليها من البشرية باستخدام أدوات داخل-البوابات. هل نفقد تلك بعدم بناء الذكاء الاصطناعي العام والذكاء الفائق *أبداً*؟ في موازنة المخاطر والمكافآت هنا، هناك منفعة غير متناسقة هائلة في الانتظار مقابل التسرع: يمكننا الانتظار حتى يمكن القيام بذلك بطريقة مضمونة الأمان والفائدة، وتقريباً الجميع سيحصلون على الثمار؛ إذا تسرعنا، قد يكون ذلك - بكلمات سام ألتمان الرئيس التنفيذي لشركة OpenAI - [انطفاء الأنوار على *جميعنا*.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

لكن إذا كانت الأدوات غير العامة قوية جداً فعلياً، هل يمكننا إدارتها؟ الإجابة واضحة... ربما.

## أنظمة الذكاء الاصطناعي الأداتي قابلة للإدارة (على الأرجح، من حيث المبدأ)

لكن لن يكون سهلاً. أنظمة الذكاء الاصطناعي الحديثة المتطورة يمكن أن تمكن الأشخاص والمؤسسات كثيراً في تحقيق أهدافهم. هذا، بشكل عام، أمر جيد! مع ذلك، هناك ديناميات طبيعية لوجود مثل هذه الأنظمة تحت تصرفنا - فجأة ودون وقت كافٍ للمجتمع للتكيف - تحمل مخاطر جدية تحتاج للإدارة. يستحق مناقشة بعض الفئات الرئيسية لهذه المخاطر، وكيف يمكن تقليلها، بافتراض إغلاق البوابة.

فئة واحدة من المخاطر هي السماح للذكاء الاصطناعي الأداتي عالي القوة بالوصول لمعرفة أو قدرة كانت مرتبطة سابقاً بشخص أو منظمة، مما يجعل مزيج القدرة العالية مع الولاء العالي متاحاً لمجموعة واسعة جداً من الفاعلين. اليوم، بما يكفي من المال، يمكن لشخص سيء النية أن يوظف فريقاً من الكيميائيين لتصميم وإنتاج أسلحة كيماوية جديدة - لكن ليس سهلاً جداً أن يملك ذلك المال أو أن يجد/يجمع الفريق ويقنعهم بفعل شيء واضح أنه غير قانوني وأخلاقي وخطير. لمنع أنظمة الذكاء الاصطناعي من لعب هذا الدور، قد تكفي تحسينات على الطرق الحالية،[^12] طالما أن جميع تلك الأنظمة والوصول إليها يُدار بمسؤولية. من ناحية أخرى، إذا أُطلقت أنظمة قوية للاستخدام والتعديل العامين، فمن المرجح إزالة أي تدابير أمان مدمجة. لذا لتجنب المخاطر في هذه الفئة، ستُطلب قيود قوية على ما يمكن إطلاقه علنياً - مشابهة للقيود على تفاصيل التقنيات النووية والمتفجرة وغيرها من التقنيات الخطرة.[^13]

فئة ثانية من المخاطر تنشأ من توسيع الآلات التي تتصرف مثل أو تنتحل الأشخاص. على مستوى الضرر للأفراد، تشمل هذه المخاطر حيلاً أكثر فعالية، ورسائل مزعجة، وانتحال هوية، وانتشار التزييف العميق غير المرضي.[^14] على مستوى جماعي، تشمل تعطيل العمليات الاجتماعية الأساسية مثل النقاش والجدل العامين، وأنظمة المعلومات والمعرفة المجتمعية وجمعها ومعالجتها ونشرها، وأنظمة الاختيار السياسي. التخفيف من هذا الخطر سيشمل على الأرجح (أ) قوانين تقيد انتحال أنظمة الذكاء الاصطناعي للأشخاص، وتحميل مطوري الذكاء الاصطناعي المسؤولية القانونية لإنشاء أنظمة تولد مثل هذه الانتحالات، و(ب) أنظمة العلامات المائية والمصدر التي تحدد وتصنف (بمسؤولية) المحتوى المولد بالذكاء الاصطناعي، و(ج) أنظمة معرفية اجتماعية-تقنية جديدة يمكنها إنشاء سلسلة موثوقة من البيانات (مثل الكاميرات والتسجيلات) عبر الوقائع والفهم ونماذج العالم الجيدة.[^15] كل هذا ممكن، ويمكن للذكاء الاصطناعي أن يساعد في بعض أجزائه.

خطر عام ثالث هو أنه بقدر أتمتة بعض المهام، يمكن للبشر الذين يقومون حالياً بتلك المهام أن تقل قيمتهم المالية كعمالة. تاريخياً، أتمتة المهام جعلت الأشياء الممكنة بتلك المهام أرخص وأكثر وفرة، بينما رتبت الأشخاص الذين كانوا يقومون بتلك المهام سابقاً إلى من لا يزالون مشاركين في النسخة المؤتمتة (عموماً بمهارة/أجر أعلى)، ومن تصبح عمالتهم أقل قيمة أو قليلة القيمة. صافياً، من الصعب التنبؤ بأي القطاعات ستحتاج عمالة بشرية أكثر مقابل أقل في القطاع الأكبر الناتج لكن الأكثر كفاءة. بالتوازي، تميل ديناميكية الأتمتة لزيادة اللامساواة والإنتاجية العامة، وتقليل تكلفة سلع وخدمات معينة (عبر زيادات الكفاءة)، وزيادة تكلفة أخرى (عبر [مرض التكاليف](https://en.wikipedia.org/wiki/Baumol_effect)). لأولئك في الجانب المحروم من زيادة اللامساواة، من غير الواضح تماماً ما إذا كان نقصان التكلفة في تلك السلع والخدمات المعينة يتفوق على الزيادة في الأخرى، ويقود لرفاهية أعظم إجمالياً. إذن كيف سيسير هذا مع الذكاء الاصطناعي؟ بسبب السهولة النسبية التي يمكن بها استبدال العمل الفكري البشري بالذكاء الاصطناعي العام، يمكننا توقع نسخة سريعة من هذا مع الذكاء الاصطناعي عام الغرض بمستوى بشري.[^16] إذا أغلقنا البوابة أمام الذكاء الاصطناعي العام، سيُستبدل وظائف أقل كثيراً جملة بوكلاء الذكاء الاصطناعي؛ لكن نزوح عمالي ضخم محتمل عبر فترة سنوات.[^17] لتجنب معاناة اقتصادية واسعة، سيكون ضرورياً على الأرجح تطبيق شكل من أشكال الأصول أو الدخل الأساسي الشامل، وأيضاً هندسة تحول ثقافي نحو تقدير ومكافأة العمالة محورية البشر الأصعب للأتمتة (بدلاً من رؤية أسعار العمالة تنخفض بسبب الارتفاع في العمالة المتاحة المدفوعة من أجزاء أخرى من الاقتصاد.) بنى أخرى، مثل ["كرامة البيانات"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society) (التي يُمنح فيها منتجو البيانات البشريين تلقائياً إتاوات للقيمة المخلوقة بتلك البيانات في الذكاء الاصطناعي) قد تساعد. الأتمتة بالذكاء الاصطناعي لها أيضاً تأثير سلبي ثان محتمل، وهو الأتمتة *غير المناسبة*. إضافة للتطبيقات التي يقوم فيها الذكاء الاصطناعي بعمل أسوأ، ستشمل هذه تلك التي من المحتمل أن تنتهك أنظمة الذكاء الاصطناعي فيها المبادئ الأخلاقية والأخلاقية أو القانونية - مثلاً في قرارات الحياة والموت، وفي المسائل القضائية. يجب معاملة هذه بتطبيق وتوسيع أطرنا القانونية الحالية.

أخيراً، تهديد مهم للذكاء الاصطناعي داخل-البوابات هو استخدامه في الإقناع الشخصي وجذب الانتباه والتلاعب. رأينا في وسائل التواصل الاجتماعي ومنصات أخرى نمو اقتصاد انتباه راسخ بعمق (حيث تحارب الخدمات الرقمية بضراوة لانتباه المستخدم) وأنظمة ["رأسمالية المراقبة"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism) (التي تُضاف فيها معلومات المستخدم والتنميط لسلعنة الانتباه.) مؤكد تقريباً أن ذكاء اصطناعي أكثر سيُوضع في خدمة كلاهما. الذكاء الاصطناعي يُستخدم بالفعل كثيراً في خوارزميات التغذية الإدمانية، لكن هذا سيتطور إلى محتوى مولد بالذكاء الاصطناعي إدماني، مخصص ليُستهلك قهرياً من شخص واحد. ومدخلات ذلك الشخص واستجاباته وبياناته، ستُغذى في آلة الانتباه/الإعلان لاستمرار الدورة الشريرة. كما أنه، مع تطور مساعدي الذكاء الاصطناعي المقدمين من شركات التقنية لتصبح الواجهة لحياة رقمية أكثر، سيستبدلون على الأرجح محركات البحث والتغذيات كآلية يحدث بها الإقناع وتحقيق الربح من العملاء. فشل مجتمعنا في السيطرة على هذه الديناميات حتى الآن لا يبشر بخير. قد يُقلل من بعض هذه الديناميكية عبر لوائح حول الخصوصية وحقوق البيانات والتلاعب. الوصول أكثر لجذر المشكلة قد يتطلب منظورات مختلفة، مثل مساعدي الذكاء الاصطناعي الموالين (المناقشين أدناه.)

خلاصة هذه المناقشة هي الأمل: الأنظمة الأداتية داخل-البوابات - على الأقل طالما بقيت مقاربة في القوة والقدرة لأكثر الأنظمة تطوراً اليوم - قابلة للإدارة على الأرجح إذا كانت هناك إرادة وتنسيق للقيام بذلك. المؤسسات البشرية اللائقة، الممكنة بأدوات الذكاء الاصطناعي،[^18] يمكنها فعل ذلك. يمكننا أيضاً الفشل في فعله. لكن من الصعب رؤية كيف سيساعد السماح بأنظمة أقوى - عدا وضعها المسؤولة والأمل للأفضل.

## الأمن القومي

سباقات هيمنة الذكاء الاصطناعي - مدفوعة بالأمن القومي أو دوافع أخرى - تدفعنا نحو أنظمة ذكاء اصطناعي قوية غير محكومة تميل لامتصاص، بدلاً من منح، السلطة. سباق للذكاء الاصطناعي العام بين أمريكا والصين هو سباق لتحديد أي دولة تحصل على الذكاء الفائق أولاً.

فماذا يجب على المسؤولين عن الأمن القومي فعله بدلاً من ذلك؟ الحكومات لها خبرة قوية في بناء أنظمة قابلة للسيطرة وآمنة، ويجب أن تضاعف الجهد في القيام بذلك في الذكاء الاصطناعي، ودعم نوع مشاريع البنية التحتية التي تنجح أفضل عند القيام بها على نطاق وبختم حكومي.

بدلاً من "مشروع مانهاتن" متهور نحو الذكاء الاصطناعي العام،[^19] يمكن للحكومة الأمريكية إطلاق مشروع أبولو لأنظمة قابلة للسيطرة وآمنة وجديرة بالثقة. يمكن أن يشمل هذا مثلاً:

- برنامج كبير لـ(أ) تطوير آليات الأمان للأجهزة على الرقاقة و(ب) البنية التحتية، لإدارة الجانب الحاسوبي للذكاء الاصطناعي القوي. يمكن لهذه أن تبني على [قانون CHIPS](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) الأمريكي و[نظام ضوابط التصدير](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion).
- مبادرة واسعة النطاق لتطوير تقنيات التحقق الرسمي حتى يمكن *إثبات* وجود أو غياب خصائص معينة لأنظمة الذكاء الاصطناعي (مثل مفتاح الإيقاف). يمكن لهذا الاستفادة من الذكاء الاصطناعي نفسه لتطوير براهين الخصائص.
- جهد على نطاق قومي لخلق برمجيات آمنة بشكل قابل للتحقق، مدعومة بأدوات ذكاء اصطناعي يمكنها إعادة ترميز البرمجيات الموجودة في أطر آمنة قابلة للتحقق.
- مشروع استثمار قومي في التقدم العلمي باستخدام الذكاء الاصطناعي،[^20] يدار كشراكة بين وزارة الطاقة ومؤسسة العلوم القومية والمعاهد الوطنية للصحة.

بشكل عام، هناك سطح هجوم هائل على مجتمعنا يجعلنا عرضة لمخاطر من الذكاء الاصطناعي وسوء استخدامه. الحماية من بعض هذه المخاطر ستتطلب استثماراً وتوحيداً بحجم حكومي. هذه ستوفر أماناً أكثر بشكل هائل من سكب البنزين على نار سباقات نحو الذكاء الاصطناعي العام. وإذا كان الذكاء الاصطناعي سيُبنى في الأسلحة وأنظمة القيادة والسيطرة، فمن الحاسم أن يكون الذكاء الاصطناعي جديراً بالثقة وآمناً، وهو ما ليس عليه الذكاء الاصطناعي الحالي ببساطة.

## تركز السلطة وتخفيفاتها

ركزت هذه المقالة على فكرة السيطرة البشرية على الذكاء الاصطناعي وفشلها المحتمل. لكن عدسة أخرى صحيحة لرؤية وضع الذكاء الاصطناعي هي عبر *تركز السلطة.* تطوير ذكاء اصطناعي قوي جداً يهدد بتركيز السلطة إما في الأيدي القليلة والشركية الكبيرة جداً التي طورته وستتحكم به، أو في الحكومات التي تستخدم الذكاء الاصطناعي كوسيلة جديدة للحفاظ على سلطتها وسيطرتها، أو في أنظمة الذكاء الاصطناعي نفسها. أو خليط غير مقدس مما أعلاه. في أي من هذه الحالات، تفقد معظم البشرية السلطة والسيطرة والوكالة. كيف يمكننا محاربة هذا؟

الخطوة الأولى والأهم، بالطبع، هي إغلاق البوابة أمام الذكاء الاصطناعي العام والذكاء الفائق أذكى من البشر. هذه يمكنها صراحة استبدال البشر ومجموعات البشر مباشرة. إذا كانت تحت سيطرة شركية أو حكومية ستركز السلطة في تلك الشركات أو الحكومات؛ إذا كانت "حرة" ستركز السلطة في أنفسها. فلنفترض إغلاق البوابات. ثم ماذا؟

حل مقترح واحد لتركز السلطة هو ذكاء اصطناعي "مفتوح المصدر"، حيث أوزان النموذج متاحة بحرية أو على نطاق واسع. لكن كما ذُكر سابقاً، مرة إطلاق نموذج، يمكن (وعموماً يُجرى) تجريد معظم تدابير السلامة أو الحواجز الوقائية. لذا هناك توتر حاد بين من ناحية اللامركزية، ومن الناحية الأخرى السلامة والأمان والسيطرة البشرية على أنظمة الذكاء الاصطناعي. هناك أيضاً أسباب للشك في أن النماذج المفتوحة ستحارب بمفردها تركز السلطة في الذكاء الاصطناعي بشكل معنوي أكثر مما فعلته في أنظمة التشغيل (لا تزال مهيمناً عليها مايكروسوفت وآبل وجوجل رغم البدائل المفتوحة).[^21]

مع ذلك قد تكون هناك طرق لحل هذا التناقض - لمركزة وتخفيف المخاطر بينما لامركزة القدرة والمكافأة الاقتصادية. هذا يتطلب إعادة تفكير في كيفية تطوير الذكاء الاصطناعي وكيفية توزيع فوائده.

نماذج جديدة لتطوير وملكية الذكاء الاصطناعي العامة ستساعد. يمكن أن تأخذ هذه أشكالاً عدة: ذكاء اصطناعي مطور حكومياً (خاضع للإشراف الديمقراطي)،[^22] منظمات تطوير ذكاء اصطناعي غير ربحية (مثل موزيلا للمتصفحات)، أو هياكل تمكن الملكية والحكم على نطاق واسع جداً. المفتاح أن هذه المؤسسات ستُوكل صراحة لخدمة المصلحة العامة بينما تعمل تحت قيود سلامة قوية.[^23] أنظمة تنظيمية ومعايير/شهادات مصممة جيداً ستكون حيوية أيضاً، حتى تبقى منتجات الذكاء الاصطناعي المقدمة من سوق حيوي مفيدة فعلاً بدلاً من استغلالية تجاه مستخدميها.

من ناحية تركز السلطة الاقتصادية، يمكننا استخدام تتبع المصدر و"كرامة البيانات" لضمان تدفق الفوائد الاقتصادية على نطاق أوسع. خاصة، معظم قوة الذكاء الاصطناعي الآن (وفي المستقبل إذا أبقينا البوابات مغلقة) تنشأ من بيانات مولدة بشرياً، سواء بيانات تدريب مباشرة أو تعليقات بشرية. إذا طُلب من شركات الذكاء الاصطناعي تعويض مقدمي البيانات بإنصاف،[^24] فهذا يمكن على الأقل أن يساعد في توزيع المكافآت الاقتصادية بشكل أوسع. خلاف هذا، نموذج آخر يمكن أن يكون الملكية العامة لكسور مهمة من شركات الذكاء الاصطناعي الكبيرة. مثلاً، الحكومات القادرة على فرض ضرائب على شركات الذكاء الاصطناعي يمكن أن تستثمر كسراً من المقبوضات في صندوق ثروة سيادي يحمل أسهماً في الشركات، ويدفع أرباحاً للسكان.[^25]

الحاسم في هذه الآليات استخدام قوة الذكاء الاصطناعي نفسه لمساعدة توزيع السلطة بشكل أفضل، بدلاً من مجرد قتال تركز السلطة المدفوع بالذكاء الاصطناعي باستخدام وسائل غير ذكاء اصطناعي. نهج قوي واحد سيكون عبر مساعدي ذكاء اصطناعي مصممين جيداً يعملون بواجب ائتماني حقيقي لمستخدميهم - واضعين مصالح المستخدمين أولاً، خاصة فوق مصالح المقدمين الشركيين.[^26] هؤلاء المساعدون يجب أن يكونوا جديرين بالثقة حقاً، وأكفاء تقنياً لكن محدودين بشكل مناسب حسب حالة الاستخدام ومستوى الخطر، ومتاحين على نطاق واسع للجميع عبر قنوات عامة أو غير ربحية أو ربحية معتمدة. كما لن نقبل أبداً مساعداً بشرياً يعمل سراً ضد مصالحنا لطرف آخر، يجب ألا نقبل مساعدي ذكاء اصطناعي يراقبون أو يتلاعبون أو يستخرجون قيمة من مستخدميهم لصالح شركي.

مثل هذا التحول سيغير الديناميكية الحالية جذرياً حيث يُترك الأفراد للتفاوض وحدهم مع آلات شركية وبيروقراطية هائلة (مدعومة بالذكاء الاصطناعي) تعطي أولوية لاستخراج القيمة على رفاهية البشر. بينما هناك نهج كثيرة ممكنة لإعادة توزيع السلطة المدفوعة بالذكاء الاصطناعي بشكل أوسع، لن يظهر أي منها بالافتراضية: يجب هندستها وحكمها عمداً بآليات مثل متطلبات الائتمان والتقديم العام والوصول المتدرج حسب الخطر.

نهج تخفيف تركز السلطة يمكن أن تواجه رياحاً معاكسة مهمة من السلطات الحاكمة.[^27] لكن هناك مسارات نحو تطوير ذكاء اصطناعي لا تتطلب الاختيار بين الأمان والسلطة المركزة. ببناء المؤسسات الصحيحة الآن، يمكننا ضمان أن فوائد الذكاء الاصطناعي تُشارك على نطاق واسع بينما مخاطره تُدار بعناية.

## هياكل الحكم والاجتماعية الجديدة

هياكل الحكم الحالية تكافح: هي بطيئة الاستجابة، وغالباً مأسورة من مصالح خاصة، و[تقل ثقة الجمهور بها بشكل متزايد.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) مع ذلك، هذا ليس سبباً لهجرها - بل العكس تماماً. بعض المؤسسات قد تحتاج لاستبدال، لكن بشكل أوسع نحتاج آليات جديدة يمكنها تعزيز ومكملة هياكلنا الموجودة، مساعدتها على العمل بشكل أفضل في عالمنا سريع التطور.

الكثير من ضعف مؤسساتنا ينشأ ليس من هياكل حكومية رسمية، بل من مؤسسات اجتماعية متدهورة: أنظمتنا لتطوير الفهم المشترك وتنسيق العمل وإجراء خطاب معنوي. حتى الآن، سرع الذكاء الاصطناعي هذا التدهور، بإغراق قنوات معلوماتنا بمحتوى مولد، وتوجيهنا للمحتوى الأكثر استقطاباً وتفريقاً، وجعل تمييز الحق من الباطل أصعب.

لكن الذكاء الاصطناعي يمكن أن يساعد فعلاً في إعادة بناء وتقوية هذه المؤسسات الاجتماعية. تأملوا ثلاث مجالات حاسمة:

أولاً، يمكن للذكاء الاصطناعي أن يساعد في استعادة الثقة في أنظمتنا المعرفية - طرقنا في معرفة ما هو صحيح. يمكننا تطوير أنظمة مدعومة بالذكاء الاصطناعي تتتبع وتتحقق من مصدر المعلومات، من البيانات الخام عبر التحليل للاستنتاجات. هذه الأنظمة يمكن أن تدمج التحقق التشفيري مع تحليل متطور لمساعدة الأشخاص على فهم ليس فقط ما إذا كان شيء صحيحاً، لكن كيف نعرف أنه صحيح.[^28] مساعدو الذكاء الاصطناعي الموالون يمكن أن يُكلفوا بتتبع التفاصيل للتأكد من صحتها.

ثانياً، يمكن للذكاء الاصطناعي أن يمكن أشكالاً جديدة من التنسيق واسع النطاق. الكثير من أكثر مشاكلنا إلحاحاً - من تغير المناخ لمقاومة المضادات الحيوية - هي مشاكل تنسيق جوهرياً. نحن [عالقون في مواقف أسوأ مما يمكن أن تكون تقريباً لكل شخص](https://equilibriabook.com/)، لأنه لا يستطيع أي فرد أو مجموعة تحمل الخطوة الأولى. أنظمة الذكاء الاصطناعي يمكن أن تساعد بنمذجة هياكل حوافز معقدة، وتحديد مسارات قابلة للتطبيق لنتائج أفضل، وتسهيل بناء الثقة وآليات الالتزام المطلوبة للوصول هناك.

ربما الأكثر إثارة للاهتمام، يمكن للذكاء الاصطناعي أن يمكن أشكالاً جديدة كلياً من الخطاب الاجتماعي. تخيلوا القدرة على "التحدث لمدينة"[^29] - ليس مجرد عرض إحصائيات، لكن حوار معنوي مع نظام ذكاء اصطناعي يعالج ويركب آراء وخبرات وحاجات وتطلعات ملايين السكان. أو تأملوا كيف يمكن للذكاء الاصطناعي تسهيل حوار حقيقي بين مجموعات تتحدث حالياً بجانب بعضها، بمساعدة كل جانب على فهم مخاوف وقيم الجانب الآخر الفعلية أفضل بدلاً من كاريكاتيراتهم عن بعضهم.[^30] أو يمكن للذكاء الاصطناعي أن يقدم وساطة ماهرة وموثوقة للخلافات بين أشخاص أو حتى مجموعات كبيرة من الأشخاص (الذين يمكن كلهم التفاعل معه مباشرة وفردياً!) الذكاء الاصطناعي الحالي قادر كلياً على القيام بهذا العمل، لكن الأدوات للقيام بذلك لن تظهر بنفسها، أو عبر حوافز السوق.

هذه الإمكانات قد تبدو طوباوية، خاصة نظراً لدور الذكاء الاصطناعي الحالي في تدهور الخطاب والثقة. لكن ذلك تحديداً لماذا يجب أن نطور بفعالية هذه التطبيقات الإيجابية. بإغلاق البوابات أمام الذكاء الاصطناعي العام غير القابل للسيطرة وإعطاء أولوية للذكاء الاصطناعي الذي يعزز الوكالة البشرية، يمكننا توجيه التقدم التقني نحو مستقبل يخدم فيه الذكاء الاصطناعي كقوة للتمكين والمرونة والتقدم الجماعي.

[^1]: مع ذلك، البقاء بعيداً عن التقاطع الثلاثي ليس سهلاً كما قد يحب المرء لسوء الحظ. دفع القدرة بشدة جداً في أي من الجوانب الثلاثة يميل لزيادتها في الآخرين. خاصة، قد يكون صعباً خلق ذكاء عام وقادر جداً لا يمكن تحويله لمستقل بسهولة. نهج واحد تدريب نماذج ["قصيرة النظر"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) بقدرة تخطيط معطلة. آخر سيكون التركيز على هندسة أنظمة ["قوارير مشورة"](https://arxiv.org/abs/1711.05541) خالصة تتجنب إجابة أسئلة موجهة للعمل.

[^2]: شركات كثيرة تفشل في إدراك أنها أيضاً ستُستبدل في النهاية بالذكاء الاصطناعي العام، حتى لو استغرق وقتاً أطول - لو فعلوا، قد يدفعون على تلك البوابات أقل قليلاً!

[^3]: أنظمة الذكاء الاصطناعي يمكن أن تتواصل بطرق أكثر كفاءة لكن أقل فهماً، لكن الحفاظ على الفهم البشري يجب أن يأخذ أولوية.

[^4]: هذه الفكرة للذكاء الاصطناعي المعياري القابل للتفسير طُورت بتفصيل من عدة باحثين؛ انظروا مثلاً نموذج ["خدمات الذكاء الاصطناعي الشاملة"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) لدركسلر، ["معمارية الوكالة المفتوحة"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai) لدالريمبل وآخرين. بينما قد تتطلب مثل هذه الأنظمة جهد هندسي أكثر من الشبكات العصبية الأحادية المدربة بحوسبة ضخمة، ذلك تحديداً حيث تساعد حدود الحوسبة - بجعل المسار الآمن الأكثر شفافية أيضاً الأكثر عملية.

[^5]: حول حالات السلامة بشكل عام انظروا [هذا الدليل](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). بالنسبة للذكاء الاصطناعي خاصة، انظروا [واسيل وآخرين](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274)، [كليمر وآخرين](https://arxiv.org/abs/2403.10462)، [بوهل وآخرين](https://arxiv.org/abs/2410.21572)، و[بالسني وآخرين](https://arxiv.org/abs/2411.03336)

[^6]: نحن نرى فعلاً هذا الاتجاه مدفوعاً فقط من التكلفة العالية للاستنتاج: نماذج أصغر وأكثر تخصصاً "مقطرة" من الأكبر وقادرة على العمل على أجهزة أرخص.

[^7]: أفهم لماذا المتحمسون لنظام التقنية للذكاء الاصطناعي قد يعارضون ما يرونه تنظيماً مرهقاً على صناعتهم. لكن من المحير صراحة لماذا، مثلاً، رأسمالي مخاطر يريد السماح بهروب للذكاء الاصطناعي العام والذكاء الفائق. تلك الأنظمة (والشركات، بينما تبقى تحت سيطرة الشركة) *ستأكل كل الشركات الناشئة كوجبة خفيفة*. ربما حتى *أسرع* من أكل الصناعات الأخرى. أي شخص مستثمر في نظام ذكاء اصطناعي مزدهر يجب أن يعطي أولوية لضمان ألا يقود تطوير الذكاء الاصطناعي العام لاحتكار من قبل بعض اللاعبين