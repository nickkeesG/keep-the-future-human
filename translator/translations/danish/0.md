# Executive summary

En overordnet gennemgang af essayet. Hvis du har travlt, kan du få alle hovedpointerne på bare 10 minutter.

De dramatiske fremskridt inden for kunstig intelligens i løbet af det seneste årti (for snævert specialiserede AI-systemer) og de seneste år (for generelle AI-systemer) har forvandlet AI fra et nichepræget akademisk felt til kerneforretningsstrategien for mange af verdens største virksomheder, med hundredvis af milliarder dollars i årlige investeringer i teknikker og teknologier til at udvikle AI's kapaciteter.

Nu står vi ved et kritisk vendepunkt. Efterhånden som nye AI-systemers kapaciteter begynder at matche og overgå menneskers på mange kognitive områder, må menneskeheden beslutte: hvor langt skal vi gå, og i hvilken retning?

AI startede, som alle andre teknologier, med målet om at forbedre tingene for sin skaber. Men vores nuværende bane og implicitte valg er et ukontrolleret kapløb mod stadig mere kraftfulde systemer, drevet af økonomiske incitamenter fra få gigantiske teknologivirksomheder, der søger at automatisere store dele af den nuværende økonomiske aktivitet og menneskelige arbejdskraft. Hvis dette kapløb fortsætter meget længere, er der en uundgåelig vinder: AI'en selv – et hurtigere, smartere, billigere alternativ til mennesker i vores økonomi, vores tænkning, vores beslutninger og til sidst i kontrol med vores civilisation.

Men vi kan træffe et andet valg: gennem vores regeringer kan vi tage kontrol over AI-udviklingsprocessen for at pålægge klare grænser, linjer vi ikke vil krydse, og ting vi simpelthen ikke vil gøre – som vi har gjort for nuklear teknologi, masseødelæggelsesvåben, rumvåben, miljøskadelige processer, bioengineering af mennesker og eugenik. Vigtigst af alt kan vi sikre, at AI forbliver et værktøj til at styrke mennesker, frem for en ny art der erstatter og til sidst fortrænger os.

Dette essay argumenterer for, at vi bør *holde fremtiden menneskelig* ved at lukke "Portene" til smartere-end-menneske, autonome, generelle AI-systemer – sommetider kaldet "AGI" – og især til den stærkt overmenneskelige version, der sommetider kaldes "superintelligens." I stedet bør vi fokusere på kraftfulde, troværdige AI-værktøjer, der kan styrke individer og transformativt forbedre menneskelige samfunds evner til at gøre det, de gør bedst. Strukturen i denne argumentation følger kort herunder.

## AI er anderledes

AI-systemer er fundamentalt forskellige fra andre teknologier. Mens traditionel software følger præcise instruktioner, lærer AI-systemer hvordan de opnår mål uden eksplicit at blive fortalt hvordan. Dette gør dem kraftfulde: hvis vi kan definere målet klart eller en metrik for succes, kan et AI-system i de fleste tilfælde lære at opnå det. Men det gør dem også inherent uforudsigelige: vi kan ikke pålideligt fastslå, hvilke handlinger de vil foretage for at opnå deres mål.

De er også stort set uforklarlige: selvom de delvist består af kode, består de for det meste af et enormt sæt ugennemskuelige tal – neurale netværks "vægte" – der ikke kan fortolkes; vi er ikke meget bedre til at forstå deres indre funktioner end til at udlede tanker ved at kigge ind i en biologisk hjerne.

Denne grundlæggende måde at træne digitale neurale netværk på bliver hurtigt mere og mere kompleks. De mest kraftfulde AI-systemer skabes gennem massive beregningseksperimenter, hvor man bruger specialiseret hardware til at træne neurale netværk på enorme datasæt, som derefter udvides med softwareværktøjer og overstruktur.

Dette har ført til skabelsen af meget kraftfulde værktøjer til at skabe og behandle tekst og billeder, udføre matematisk og videnskabelig ræsonnering, aggregere information og interaktivt søge i et enormt lager af menneskelig viden.

Desværre, selvom udvikling af mere kraftfulde, mere troværdige teknologiske værktøjer er det, vi *burde* gøre, og det næsten alle ønsker og siger de ønsker, er det ikke den bane, vi faktisk befinder os på.

## AGI og superintelligens

Siden feltets begyndelse har AI-forskning i stedet fokuseret på et andet mål: Artificial General Intelligence. Dette fokus er nu blevet fokus for de titaniske virksomheder, der leder AI-udviklingen.

Hvad er AGI? Det defineres ofte vagt som "AI på menneskeniveau," men dette er problematisk: hvilke mennesker, og på hvilke kapaciteter er det på menneskeniveau? Og hvad med de overmenneskelige kapaciteter, det allerede har? En mere brugbar måde at forstå AGI på er gennem skæringspunktet mellem tre nøgleegenskaber: høj **A**utonomi (handlingsindependence), høj **G**eneralitet (bred rækkevidde og tilpasningsevne) og høj **I**ntelligens (kompetence til kognitive opgaver). Nuværende AI-systemer kan være meget kapable men snævre, eller generelle men kræve konstant menneskelig overvågning, eller autonome men begrænsede i rækkevidde.

Fuld A-G-I ville kombinere alle tre egenskaber på niveauer, der matcher eller overgår top menneskelige kapaciteter. Kritisk set er det denne kombination, der gør mennesker så effektive og så forskellige fra nuværende software; det er også det, der ville gøre det muligt at erstatte mennesker fuldstændigt med digitale systemer.

Selvom menneskelig intelligens er speciel, er den på ingen måde en grænse. Kunstige "superintelligente" systemer kunne operere hundredvis af gange hurtigere, behandle enormt meget mere data og holde enorme mængder "i tankerne" på én gang, og danne aggregater, der er meget større og mere effektive end samlinger af mennesker. De kunne fortrænge ikke blot individer, men virksomheder, nationer eller vores civilisation som helhed.

## Vi står på tærsklen

Der er stærk videnskabelig konsensus om, at AGI er *mulig.* AI overgår allerede menneskelig præstation i mange generelle test af intellektuel kapacitet, herunder for nylig højt niveau ræsonnering og problemløsning. Haltende kapaciteter – såsom kontinuerlig læring, planlægning, selvbevidsthed og originalitet – eksisterer alle på et vist niveau i nuværende AI-systemer, og kendte teknikker eksisterer, som sandsynligvis vil forbedre dem alle.

Mens mange forskere indtil for få år siden så AGI som årtier væk, er der i øjeblikket stærk evidens for korte tidslinjer til AGI:

- Empirisk verificerede "skaleringslove" forbinder beregningsinput med AI-kapacitet, og virksomheder er på sporet til at skalere beregningsinput med størrelsesordener over de kommende år. De menneskelige og finansielle ressourcer dedikeret til AI-udvikling svarer nu til dem fra et dusin Manhattan-projekter og flere Apollo-projekter.
- AI-virksomheder og deres ledere tror offentligt og privat, at AGI (efter en eller anden definition) er opnåelig inden for få år. Disse virksomheder har information, som offentligheden ikke har, herunder nogle der har næste generation af AI-systemer i hænde.
- Ekspertforudsigere med dokumenterede track records tildeler 25% sandsynlighed for at AGI (efter en eller anden definition) ankommer inden for 1-2 år, og 50% for 2-5 år (se Metaculus forudsigelser for ['svag'](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) og ['fuld'](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/) AGI).
- Autonomi (herunder langtrækkende fleksibel planlægning) halter i AI-systemer, men større virksomheder fokuserer nu deres store ressourcer på at udvikle autonome AI-systemer og har uformelt døbt 2025 ["agenternes år."](https://techinformed.com/2025-informed-the-year-of-agentic-ai/)
- AI bidrager mere og mere til sin egen forbedring. Når AI-systemer bliver lige så kompetente som menneskelige AI-forskere til at lave AI-forskning, vil en kritisk tærskel for hurtig fremskridt til meget mere kraftfulde AI-systemer blive ramt og sandsynligvis føre til en ukontrolleret udvikling i AI-kapacitet. (Kunne man argumentere for, at den ukontrollerede udvikling allerede er begyndt.)

Ideen om, at smartere-end-menneske AGI er årtier væk eller mere, er simpelthen ikke længere holdbar for langt de fleste eksperter på området. Uenigheder nu handler om, hvor mange måneder eller år det vil tage, hvis vi holder fast i dette forløb. Det centrale spørgsmål, vi står over for, er: skal vi?

## Hvad der driver kapløbet mod AGI

Kapløbet mod AGI drives af flere kræfter, der hver gør situationen mere farlig. Store teknologivirksomheder ser AGI som den ultimative automatiseringsteknologi – ikke bare supplere menneskelige arbejdere, men erstatte dem stort set eller helt. For virksomheder er præmien enorm: muligheden for at indfange en betydelig del af verdens 100 billioner dollars årlige økonomiske produktion ved at automatisere menneskelige arbejdsomkostninger væk.

Nationer føler sig tvunget til at deltage i dette kapløb, og henviser offentligt til økonomisk og videnskabeligt lederskab, men betragter privat AGI som en potentiel revolution i militære anliggender sammenlignelig med atomvåben. Frygten for, at rivaler måtte opnå en afgørende strategisk fordel, skaber en klassisk våbenkapløbsdynamik.

De, der forfølger superintelligens, henviser ofte til store visioner: kurere alle sygdomme, vende aldring om, opnå gennembrud inden for energi og rumfart eller skabe overmenneskelige planlægningskapaciteter.

Mindre velvilligt er det, der driver kapløbet, magt. Hver deltager – hvad enten det er virksomhed eller land – tror, at intelligens er lig med magt, og at de vil være den bedste forvalter af den magt.

Jeg argumenterer for, at disse motiver er reelle, men fundamentalt fejlvejledte: AGI vil *absorbere* og *søge* magt frem for at give den; AI-skabte teknologier vil *også* være stærkt tveæggede, og hvor de er gavnlige, kan de skabes med AI-værktøjer og uden AGI; og selv i det omfang AGI og dens resultater forbliver under kontrol, gør disse kapløbsdynamikker – både virksomhedsmæssige og geopolitiske – store risici for vores samfund næsten uundgåelige, medmindre de afgørende afbrydes.

## AGI og superintelligens udgør en dramatisk trussel mod civilisationen

På trods af deres tiltrækning udgør AGI og superintelligens dramatiske trusler mod civilisationen gennem flere forstærkende veje:

*Magtkoncentration:* overmenneskelig AI kunne fratage langt størstedelen af menneskeheden magt ved at absorbere enorme dele af social og økonomisk aktivitet ind i AI-systemer drevet af en håndfuld gigantiske virksomheder (som til gengæld enten kan blive overtaget af, eller effektivt overtage, regeringer.)

*Massiv disruption:* bulkautomatisering af de fleste kognitivt baserede job, erstatning af vores nuværende epistemiske systemer og udrulning af enorme mængder aktive ikke-menneskelige agenter ville vælte de fleste af vores nuværende civilisatoriske systemer på relativt kort tid.

*Katastrofer:* ved at sprede evnen – potentielt over menneskeniveau – til at skabe nye militære og destruktive teknologier og afkoble den fra de sociale og juridiske systemer, der forankrer ansvar, bliver fysiske katastrofer fra masseødelæggelsesvåben dramatisk mere sandsynlige.

*Geopolitik og krig:* store verdensmagter vil ikke sidde stille, hvis de føler, at en teknologi, der kunne levere en "afgørende strategisk fordel," bliver udviklet af deres modstandere.

*Ukontrolleret udvikling og tab af kontrol:* Medmindre det specifikt forhindres, vil overmenneskelig AI have alle incitamenter til yderligere at forbedre sig selv og kunne langt overgå mennesker i hastighed, databehandling og sofistikering af tænkning. Der er ingen meningsfuld måde, hvorpå vi kan have kontrol over et sådant system. Sådan AI vil ikke give magt til mennesker; vi vil give magt til den, eller den vil tage den.

Mange af disse risici forbliver, selv hvis det tekniske "alignment"-problem – at sikre at avanceret AI pålideligt gør det, mennesker ønsker, den skal gøre – løses. AI præsenterer en enorm udfordring i, hvordan den vil blive forvaltet, og meget mange aspekter af denne forvaltning bliver utroligt vanskelige eller uløselige, når menneskelig intelligens brydes.

Mest fundamentalt ville den type overmenneskelig generel AI, der i øjeblikket forfølges, i sagens natur have mål, aktørskab og kapaciteter, der overstiger vores egne. Den ville være inherent ukontrollerbar – hvordan kan vi kontrollere noget, vi hverken kan forstå eller forudsige? Den ville ikke være et teknologisk værktøj til menneskelig brug, men en anden intelligensart på Jorden ved siden af vores. Hvis den fik lov at udvikle sig yderligere, ville den udgøre ikke blot en anden art, men en erstatningsart.

Måske ville den behandle os godt, måske ikke. Men fremtiden ville tilhøre den, ikke os. Den menneskelige æra ville være forbi.

## Dette er ikke uundgåeligt; menneskeheden kan meget konkret beslutte ikke at bygge sin erstatning.

Skabelsen af overmenneskelig AGI er langt fra uundgåelig. Vi kan forhindre det gennem et koordineret sæt styringsforanstaltninger:

For det første har vi brug for robust regnskabsføring og overvågning af AI-beregning ("compute"), som er en grundlæggende mulighedsskaber for, og løftestang til at styre, storskala AI-systemer. Dette kræver til gengæld standardiseret måling og rapportering af den samlede compute, der bruges til at træne AI-modeller og køre dem, og tekniske metoder til at opgøre, certificere og verificere anvendt beregning.

For det andet bør vi implementere hårde begrænsninger på AI-beregning, både til træning og til drift; disse forhindrer AI i både at være for kraftfuld og operere for hurtigt. Disse begrænsninger kan implementeres gennem både juridiske krav og hardwarebaserede sikkerhedsforanstaltninger bygget ind i AI-specialiserede chips, analogt med sikkerhedsfeatures i moderne telefoner. Fordi specialiseret AI-hardware kun laves af en håndfuld virksomheder, er verificering og håndhævelse mulig gennem den eksisterende forsyningskæde.

For det tredje har vi brug for øget ansvar for de mest farlige AI-systemer. De, der udvikler AI, som kombinerer høj autonomi, bred generalitet og overlegen intelligens, bør møde strikt erstatningsansvar for skader, mens sikre havne fra dette ansvar ville tilskynde til udvikling af mere begrænsede og kontrollerbare systemer.

For det fjerde har vi brug for trindelt regulering baseret på risikoniveauer. De mest kapable og farlige systemer ville kræve omfattende sikkerheds- og kontrollerbarhedsgarantier før udvikling og implementering, mens mindre kraftfulde eller mere specialiserede systemer ville møde proportional overvågning. Denne reguleringsramme bør til sidst operere på både nationale og internationale niveauer.

Denne tilgang – med detaljeret specifikation givet i det fulde dokument – er praktisk: selvom international koordination vil være nødvendig, kan verificering og håndhævelse virke gennem det lille antal virksomheder, der kontrollerer den specialiserede hardware-forsyningskæde. Den er også fleksibel: virksomheder kan stadig innovere og profitere fra AI-udvikling, bare med klare grænser på de mest farlige systemer.

Langsigtet inddæmning af AI-magt og -risiko ville kræve internationale aftaler baseret på både selv- og fælles interesse, ligesom kontrol af atomvåbenspredning gør nu. Men vi kan starte med det samme med øget overvågning og ansvar, mens vi bygger mod mere omfattende styring.

Den vigtige manglende ingrediens er politisk og social vilje til at tage kontrol over AI-udviklingsprocessen. Kilden til den vilje, hvis den kommer i tide, vil være virkeligheden selv – det vil sige fra udbredt erkendelse af de reelle implikationer af det, vi gør.

## Vi kan designe Værktøjs-AI til at styrke menneskeheden

I stedet for at forfølge ukontrollerbar AGI kan vi udvikle kraftfulde "Værktøjs-AI"-systemer, der forbedrer menneskelige kapaciteter, mens de forbliver under meningsfuld menneskelig kontrol. Værktøjs-AI-systemer kan være yderst kapable, mens de undgår det farlige tredobbelte skæringspunkt mellem høj autonomi, bred generalitet og overmenneskelig intelligens, så længe vi designer dem til at være kontrollerbare på et niveau, der står mål med deres kapacitet. De kan også kombineres til sofistikerede systemer, der opretholder menneskelig overvågning, mens de leverer transformative fordele.

Værktøjs-AI kan revolutionere medicin, accelerere videnskabelig opdagelse, forbedre uddannelse og forbedre demokratiske processer. Når det styres korrekt, kan det gøre menneskelige eksperter og institutioner mere effektive frem for at erstatte dem. Selvom sådanne systemer stadig vil være meget disruptive og kræve omhyggelig forvaltning, er de risici, de udgør, fundamentalt forskellige fra AGI: det er risici, vi kan styre, som dem fra andre kraftfulde teknologier, ikke eksistentielle trusler mod menneskelig aktørskab og civilisation. Og kritisk set, når det udvikles fornuftigt, kan AI-værktøjer hjælpe mennesker med at styre kraftfuld AI og forvalte dens effekter.

Denne tilgang kræver, at man gentænker både hvordan AI udvikles, og hvordan dens fordele fordeles. Nye modeller for offentlig og non-profit AI-udvikling, robuste reguleringsrammer og mekanismer til at fordele økonomiske fordele mere bredt kan hjælpe med at sikre, at AI styrker menneskeheden som helhed frem for at koncentrere magt i få hænder. AI selv kan hjælpe med at bygge bedre sociale og styringsinsitutioner, muliggøre nye former for koordination og diskurs, der styrker frem for underminerer menneskelige samfund. Nationale sikkerhedsapparater kan udnytte deres ekspertise til at gøre AI-værktøjssystemer virkelig sikre og troværdige og en sand kilde til forsvar såvel som national magt.

Vi kan til sidst vælge at udvikle endnu mere kraftfulde og mere suveræne systemer, der er mindre som værktøjer og – kan vi håbe – mere som vise og kraftfulde velgørere. Men vi bør kun gøre det, efter at vi har udviklet den videnskabelige forståelse og styringskapacitet til at gøre det sikkert. En så betydningsfuld og irreversibel beslutning bør træffes bevidst af menneskeheden som helhed, ikke som standard i et kapløb mellem tech-virksomheder og nationer.

## I menneskelige hænder

Folk ønsker det gode, der kommer fra AI: nyttige værktøjer, der styrker dem, oplader økonomiske muligheder og vækst og lover gennembrud inden for videnskab, teknologi og uddannelse. Hvorfor skulle de ikke? Men når de bliver spurgt, ønsker overvældende flertal af den brede offentlighed [langsommere og mere omhyggelig AI-udvikling](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation) og ønsker ikke smartere-end-menneske AI, der vil erstatte dem i deres job og andre steder, fylde deres kultur og informationsfællesskaber med ikke-menneskelig indhold, koncentrere magt i et lille sæt virksomheder, udgøre ekstreme storskala globale risici og til sidst true med at fratage magt eller erstatte deres art. Hvorfor skulle de?

Vi *kan* have det ene uden det andet. Det starter med at beslutte, at vores skæbne ikke ligger i den formodede uundgåelighed af en eller anden teknologi eller i hænderne på nogle få CEO'er i Silicon Valley, men i resten af vores hænder, hvis vi griber fat i det. Lad os lukke Portene og holde fremtiden menneskelig.