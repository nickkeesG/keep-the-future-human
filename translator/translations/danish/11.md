# Bilag

Supplerende information, herunder - Tekniske detaljer omkring compute-regnskab, et eksempel på implementering af en 'portlukning', detaljer for et strengt AGI-ansvarsregime, og en trinvis tilgang til AGI-sikkerheds- og sikkerhedsstandarder.

## Bilag A: Tekniske detaljer for compute-regnskab

En detaljeret metode til både "absolut sandhed" samt gode tilnærmelser for den samlede compute brugt i træning og inferens er nødvendig for meningsfuld compute-baseret kontrol. Her er et eksempel på, hvordan den "absolutte sandhed" kunne opgøres på teknisk niveau.

**Definitioner:**

*Compute kausal graf:* For et givet output O fra en AI-model, er der et sæt digitale beregninger, hvor ændring af resultatet af den beregning potentielt kunne ændre O. (Dette bør antages konservativt, dvs. der bør være en klar grund til at tro, at en beregning er uafhængig af en forløber, der både forekommer tidligere i tid og har en fysisk potentiel kausal virkningssti.) Dette inkluderer beregning udført af AI-modellen under inferens, samt beregninger der indgik i input, dataforberedelse og træning af modellen. Da enhver af disse selv kan være output fra en AI-model, beregnes dette rekursivt, afbrudt hvor et menneske har givet en væsentlig ændring til inputtet.

*Trænings-compute:* Den samlede compute, i FLOP eller andre enheder, der indgås af compute-kausal-grafen for et neuralt netværk (inkluderende dataforberedelse, træning og finjustering, og andre beregninger.)

*Output-compute:* Den samlede compute i compute-kausal-grafen for et givet AI-output, inkluderende alle neurale netværk (og inkluderende deres Trænings-compute) og andre beregninger, der indgår i det output.

*Inferens-compute-rate:* I en serie af outputs, ændringshastigheden (i FLOP/s eller andre enheder) af Output-compute mellem outputs, dvs. den compute der bruges til at producere det næste output, divideret med tidsintervallet mellem outputs.

**Eksempler og tilnærmelser:**

- For et enkelt neuralt netværk trænet på menneske-skabte data er Trænings-compute blot den samlede trænings-compute som sædvanligt rapporteret.
- For et sådant neuralt netværk, der udfører inferens med en konstant hastighed, er Inferens-compute-raten cirka den samlede hastighed af beregningsklyngen, der udfører inferensen i FLOP/s.
- For model-finjustering gives Trænings-compute for den komplette model ved Trænings-compute for den ikke-finjusterede model plus beregningen udført under finjustering og til forberedelse af data brugt i finjustering.
- For en destilleret model inkluderer Trænings-compute for den komplette model træning af både den destillerede model og den større model brugt til at levere syntetiske data eller andet træningsinput.
- Hvis flere modeller trænes, men mange "forsøg" kasseres på baggrund af menneskelig vurdering, tæller disse ikke med i Trænings- eller Output-compute for den beholdte model.

## Bilag B: Eksempel på implementering af en portlukning

**Implementeringseksempel:** Her er et eksempel på, hvordan en portlukning kunne fungere, givet en grænse på 10<sup>27</sup> FLOP for træning og 10<sup>20</sup> FLOP/s for inferens (kørsel af AI'en):

**1. Pause:** Af nationale sikkerhedshensyn beder den amerikanske regering alle virksomheder baseret i USA, der driver forretning i USA, eller som bruger chips fremstillet i USA, om at ophøre med nye AI-træningskørsler, der kunne overskride 10<sup>27</sup> FLOP Trænings-compute-grænsen. USA bør indlede diskussioner med andre lande, der huser AI-udvikling, og kraftigt opfordre dem til at tage lignende skridt og indikere, at den amerikanske pause kan ophæves, hvis de vælger ikke at efterkomme.

**2. Amerikansk tilsyn og licensering:** Ved præsidentiel bekendtgørelse eller handling fra et eksisterende reguleringsagentur kræver USA, at inden (f.eks.) et år:

- Alle AI-træningskørsler estimeret over 10<sup>25</sup> FLOP udført af virksomheder, der opererer i USA, registreres i en database vedligeholdt af et amerikansk reguleringsagentur. (Bemærk: En lidt svagere version af dette var allerede inkluderet i den nu tilbagekaldte amerikanske præsidentielle bekendtgørelse om AI fra 2023, der krævede registrering for modeller over 10<sup>26</sup> FLOP.)
- Alle AI-relevante hardwareproducenter, der opererer i USA eller handler med den amerikanske regering, overholder et sæt krav til deres specialiserede hardware og den software, der driver den. (Mange af disse krav kunne indbygges i software- og firmware-opdateringer til eksisterende hardware, men langsigtede og robuste løsninger ville kræve ændringer til senere generationer af hardware.) Blandt disse er et krav om, at hvis hardwaren er del af en højhastigheds-sammenkoblet klynge, der kan udføre 10<sup>18</sup> FLOP/s beregning, kræves et højere niveau af verifikation, som inkluderer regelmæssig tilladelse fra en fjern "styrer", der modtager både telemetri og anmodninger om at udføre yderligere beregning.
- Forvalteren rapporterer den samlede beregning udført på sin hardware til agenturet, der vedligeholder den amerikanske database.
- Stærkere krav indføres gradvist for at tillade både mere sikker og mere fleksibel overvågning og tilladelse.

**3. Internationalt tilsyn:**

- USA, Kina og andre lande, der huser avanceret chip-fremstillingskapacitet, forhandler en international aftale.
- Denne aftale skaber et nyt internationalt agentur, analogt med Det Internationale Atomenergiagentur, der har til opgave at overvåge AI-træning og -udførelse.
- Underskriverlande skal kræve, at deres indenlandske AI-hardwareproducenter overholder et sæt krav, der er mindst lige så stærke som dem pålagt i USA.
- Forvaltere skal nu rapportere AI-beregningstal til både agenturer i deres hjemlande samt et nyt kontor inden for det internationale agentur.
- Yderligere lande opfordres kraftigt til at tilslutte sig den eksisterende internationale aftale: eksportkontrol fra underskriverlande begrænser adgang til high-end hardware for ikke-underskrivere, mens underskrivere kan modtage teknisk støtte til at administrere deres AI-systemer.

**4. International verifikation og håndhævelse:**

- Hardware-verifikationssystemet opdateres, så det rapporterer beregningsbrug til både den oprindelige forvalter og også direkte til det internationale agenturs kontor.
- Agenturet aftaler via diskussion med underskriverne af den internationale aftale compute-begrænsninger, som derefter får juridisk kraft i underskriverlandene.
- Parallelt kan et sæt internationale standarder udvikles, så træning og kørsel af AI'er over en beregnings-tærskel (men under grænsen) skal overholde disse standarder.
- Agenturet kan, hvis nødvendigt for at kompensere for bedre algoritmer osv., sænke compute-grænsen. Eller, hvis det anses for sikkert og tilrådeligt (på niveau med beviseligt sikkerhedsgarantier), hæve compute-grænsen.

## Bilag C: Detaljer for et strengt AGI-ansvarsregime

**Detaljer for et strengt AGI-ansvarsregime**

- Skabelse og drift af et avanceret AI-system, der er meget generelt, kapabelt og autonomt, betragtes som en "unormalt farlig" aktivitet.
- Som sådan er standardansvar for træning og drift af sådanne systemer strengt, solidarisk ansvar (eller dets ikke-amerikanske ækvivalent) for enhver skade forårsaget af modellen eller dens outputs/handlinger.
- Personligt ansvar pålægges direktører og bestyrelsesmedlemmer i tilfælde af grov forsømmelighed eller bevidst uredelighed. Dette bør inkludere kriminelle straffe for de mest alvorlige tilfælde.
- Der er talrige sikre havne, under hvilke ansvar vender tilbage til standardansvar (fejlbaseret i USA), som personer og virksomheder normalt ville være underlagt.
	- Modeller trænet og drevet under en vis compute-tærskel (som ville være mindst 10 gange lavere end grænserne beskrevet ovenfor.)
	- AI der er "svag" (groft, under menneskelig ekspertniveau på de opgaver, den er beregnet til) og/eller
	- AI der er "snæver" (har et fast og ret begrænset omfang af opgaver og operationer, den er specifikt designet og trænet til) og/eller
	- AI der er "passiv" (meget begrænset i sin evne – selv under beskeden modifikation – til at udføre handlinger eller komplekse flertrinssopgaver uden direkte menneskelig involvering og kontrol.)
	- En AI der garanteres at være sikker, sikret og kontrollerbar (beviseligt sikker, eller en risikoanalyse indikerer et ubetydelig niveau af forventet skade.)
- Sikre havne kan påberåbes på baggrund af et [sikkerheds-case](https://arxiv.org/abs/2410.21572) udarbejdet af AI-udvikleren og godkendt af et agentur eller revisor akkrediteret af et agentur. For at påberåbe sig en sikker havn baseret på compute skal udvikleren blot levere troværdige estimater af samlet Trænings-compute og maksimal Inferens-rate
- Lovgivning ville eksplicit skitsere situationer, under hvilke påbudsmæssig lindring fra udvikling af AI-systemer med høj risiko for offentlig skade ville være passende.
- Virksomhedskonsorrier, der arbejder med NGO'er og regeringsagenturer, bør udvikle standarder og normer, der definerer disse termer, hvordan regulatorer skal tildele sikre havne, hvordan AI-udviklere skal udvikle sikkerheds-cases, og hvordan domstole skal fortolke ansvar, hvor sikre havne ikke proaktivt påberåbes.

## Bilag D: En trinvis tilgang til AGI-sikkerheds- og sikkerhedsstandarder

**En trinvis tilgang til AGI-sikkerheds- og sikkerhedsstandarder**

| Risiko-niveau | Udløser | Krav til træning | Krav til udrulning |
| --- | --- | --- | --- |
| RT-0 | AI svag i autonomi, generalitet og intelligens | ingen | ingen |
| RT-1 | AI stærk i en af autonomi, generalitet og intelligens | ingen | Baseret på risiko og brug, potentielt sikkerheds-cases godkendt af nationale myndigheder, hvor modellen kan bruges |
| RT-2 | AI stærk i to af autonomi, generalitet og intelligens | Registrering hos national myndighed med jurisdiktion over udvikleren | Sikkerheds-case der afgrænser risiko for større skade under autoriserede niveauer plus uafhængige sikkerhedsaudits (inkluderende black-box og white-box redteaming) godkendt af nationale myndigheder, hvor modellen kan bruges |
| RT-3 | AGI stærk i autonomi, generalitet og intelligens | Forhåndsgodkendelse af sikkerheds- og sikkerhedsplan af national myndighed med jurisdiktion over udvikleren | Sikkerheds-case der garanterer afgrænset risiko for større skade under autoriserede niveauer samt påkrævede specifikationer, inkluderende cybersikkerhed, kontrollerbarhed, en ikke-fjernbar dræberknap, tilpasning til menneskelige værdier og robusthed over for ondsindet brug. |
| RT-4 | Enhver model der også overstiger enten 10<sup>27</sup> FLOP Træning eller 10<sup>20</sup> FLOP/s Inferens | Forbudt indtil international aftalt ophævelse af compute-grænse | Forbudt indtil international aftalt ophævelse af compute-grænse |

Risikokategorisering og sikkerheds-/sikkerhedsstandarder, med niveauer baseret på compute-tærskler samt kombinationer af høj autonomi, generalitet og intelligens:

- *Stærk autonomi* gælder, hvis systemet er i stand til at udføre, eller nemt kan gøres til at udføre, flertrinssopgaver og/eller tage komplekse handlinger, der er virkelige verdens-relevante, uden betydelig menneskelig overvågning eller intervention. Eksempler: autonome køretøjer og robotter; finansielle handelsbots. Ikke-eksempler: GPT-4; billede-klassifikatorer
- *Stærk generalitet* indikerer et bredt anvendelsesområde, udførelse af opgaver, som modellen ikke var bevidst og specifikt trænet til, og betydelig evne til at lære nye opgaver. Eksempler: GPT-4; mu-zero. Ikke-eksempler: AlphaFold; autonome køretøjer; billedgeneratorer
- *Stærk intelligens* svarer til at matche menneskelig ekspert-niveau præstation på de opgaver, som modellen præsterer bedst på (og for en generel model, på tværs af et bredt spektrum af opgaver.) Eksempler: AlphaFold; mu-zero; o3. Ikke-eksempler: GPT-4; Siri