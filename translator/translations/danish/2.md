# Kapitel 2 - Det nødvendige at vide om AI neurale netværk

Hvordan fungerer moderne AI-systemer, og hvad kan vi forvente af den næste generation af AI?

For at forstå hvordan konsekvenserne af at udvikle mere kraftfuld AI vil udfolde sig, er det vigtigt at tilegne sig nogle grundlæggende principper. Dette og de næste to afsnit behandler disse principper og dækker på skift hvad moderne AI er, hvordan den udnytter massive beregninger, og på hvilke måder den hurtigt vokser i generalitet og kapacitet.[^1]

Der er mange måder at definere kunstig intelligens på, men til vores formål er AI's nøgleegenskab, at mens et almindeligt computerprogram er en liste af instruktioner for hvordan man udfører en opgave, er et AI-system et system, der lærer af data eller erfaring at udføre opgaver *uden eksplicit at blive fortalt hvordan det skal gøres.*

Næsten al relevant moderne AI er baseret på neurale netværk. Disse er matematiske/beregningsmæssige strukturer, repræsenteret af et meget stort (milliarder eller billioner) sæt af tal ("vægte"), der udfører en træningsopgave godt. Disse vægte bliver formet (eller måske "groet" eller "fundet") ved iterativt at justere dem, så det neurale netværk forbedrer en numerisk score (også kaldet "loss") defineret til at klare sig godt til en eller flere opgaver.[^2] Denne proces kaldes *træning* af det neurale netværk.[^3]

Der er mange teknikker til at udføre denne træning, men disse detaljer er langt mindre relevante end de måder, hvorpå scoringen er defineret, og hvordan disse resulterer i forskellige opgaver, som det neurale netværk klarer godt. En nøgleforskel er historisk blevet draget mellem "snæver" og "generel" AI.

Snæver AI bliver bevidst trænet til at udføre en bestemt opgave eller et lille sæt af opgaver (såsom at genkende billeder eller spille skak); den kræver gentræning til nye opgaver og har et snævert kapacitetsområde. Vi har overmenneskelig snæver AI, hvilket betyder, at for næsten enhver diskret veldefineret opgave en person kan udføre, kan vi sandsynligvis konstruere en score og derefter med succes træne et snævert AI-system til at gøre det bedre, end et menneske kunne.

Generelle AI-systemer (GPAI) kan udføre en bred vifte af opgaver, herunder mange de ikke eksplicit blev trænet til; de kan også lære nye opgaver som en del af deres drift. Nuværende store "multimodale modeller"[^4] som ChatGPT eksemplificerer dette: trænet på et meget stort korpus af tekst og billeder kan de engagere sig i kompleks ræsonnering, skrive kode, analysere billeder og assistere med en bred vifte af intellektuelle opgaver. Selvom de stadig er ret forskellige fra menneskelig intelligens på måder, vi vil se grundigt nedenfor, har deres generalitet forårsaget en revolution inden for AI.[^5]

## Uforudsigelighed: en nøgleegenskab ved AI-systemer

En nøgleforskel mellem AI-systemer og konventionel software ligger i forudsigeligheden. Standard softwares output kan være uforudsigelig – det er faktisk nogle gange derfor vi skriver software, for at give os resultater vi ikke kunne have forudsagt. Men konventionel software gør sjældent noget, den ikke blev programmeret til at gøre – dens omfang og adfærd er generelt som designet. Et førsteklasses skakprogram kan lave træk, intet menneske kunne forudsige (ellers kunne de slå det skakprogram!) men det vil generelt ikke gøre andet end at spille skak.

Ligesom konventionel software har snæver AI forudsigelig omfang og adfærd, men kan have uforudsigelige resultater. Dette er egentlig bare en anden måde at definere snæver AI på: som AI der ligner konventionel software i sin forudsigelighed og driftsområde.

Generel AI er anderledes: dens omfang (de domæner den anvendes over), adfærd (de slags ting den gør) og resultater (dens faktiske output) kan alle være uforudsigelige.[^6] GPT-4 blev trænet kun til at generere tekst præcist, men udviklede mange kapaciteter, dens trænere ikke forudsagde eller havde til hensigt. Denne uforudsigelighed stammer fra kompleksiteten af træning: fordi træningsdataene indeholder output fra mange forskellige opgaver, skal AI'en effektivt lære at udføre disse opgaver for at forudsige godt.

Denne uforudsigelighed af generelle AI-systemer er ret grundlæggende. Selvom det i princippet er muligt omhyggeligt at konstruere AI-systemer, der har garanterede begrænsninger på deres adfærd (som nævnt senere i essayet), er AI-systemer, som de skabes nu, uforudsigelige i praksis og endda i princippet.

## Passiv AI, agenter, autonome systemer og alignment

Denne uforudsigelighed bliver særlig vigtig, når vi overvejer, hvordan AI-systemer faktisk implementeres og bruges til at opnå forskellige mål.

Mange AI-systemer er relativt passive i den forstand, at de primært leverer information, og brugeren foretager handlinger. Andre, almindeligvis kaldet *agenter*, foretager selv handlinger med varierende niveauer af involvering fra en bruger. Dem der foretager handlinger med relativt mindre eksternt input eller overvågning kan betegnes som mere *autonome*. Dette danner et spektrum med hensyn til handlefrihed, fra passive værktøjer til autonome agenter.[^7]

Hvad angår AI-systemers mål, kan disse være direkte knyttet til deres træningsmål (f.eks. målet om at "vinde" for et Go-spillende system er også eksplicit det, det blev trænet til at gøre). Eller de er det måske ikke: ChatGPT's træningsmål er delvist at forudsige tekst, delvist at være en hjælpsom assistent. Men når den udfører en given opgave, leveres dens mål til den af brugeren. Mål kan også skabes af et AI-system selv, kun meget indirekte relateret til dets træningsmål.[^8]

Mål er tæt knyttet til spørgsmålet om "alignment," det vil sige spørgsmålet om hvorvidt AI-systemer vil *gøre det, vi vil have dem til at gøre*. Dette enkle spørgsmål skjuler et enormt niveau af subtilitet.[^9] Bemærk foreløbig, at "vi" i denne sætning kan referere til mange forskellige personer og grupper, hvilket fører til forskellige typer af alignment. For eksempel kan en AI være meget *lydig* (eller ["loyal"](https://arxiv.org/abs/2003.11157)) over for sin bruger – her er "vi" "hver af os." Eller den kan være mere *suveræn*, primært drevet af sine egne mål og begrænsninger, men stadig handle bredt i menneskehedens fælles interesse – "vi" er så "menneskeheden" eller "samfundet." Imellem er et spektrum, hvor en AI stort set ville være lydig, men måske nægte at foretage handlinger, der skader andre eller samfundet, overtræder loven osv.

Disse to akser – niveau af autonomi og type af alignment – er ikke helt uafhængige. For eksempel er et suverænt passivt system, selvom det ikke er helt selvmodsigende, et koncept i spænding, ligesom en lydig autonom agent er det.[^10] Der er en klar forstand, hvori autonomi og suverænitet har tendens til at gå hånd i hånd. På samme måde har forudsigelighed tendens til at være højere i "passive" og "lydige" AI-systemer, hvorimod suveræne eller autonome vil have tendens til at være mere uforudsigelige. Alt dette vil være afgørende for at forstå konsekvenserne af potentiel AGI og superintelligens.

At skabe virkelig tilpasset AI, af hvilken art det end er, kræver løsning af tre forskellige udfordringer:

1. At forstå hvad "vi" vil have – hvilket er komplekst, hvad enten "vi" betyder en specifik person eller organisation (loyalitet) eller menneskeheden bredt (suverænitet);
2. At bygge systemer, der regelmæssigt handler i overensstemmelse med disse ønsker – i det væsentlige skabe konsekvent positiv adfærd;
3. Mest fundamentalt at lave systemer, der oprigtigt "bekymrer sig om" disse ønsker snarere end blot at handle som om de gør det.

Forskellen mellem pålidelig adfærd og oprigtig omsorg er afgørende. Ligesom en menneskelig medarbejder måske følger ordrer perfekt, mens den mangler ethvert reelt engagement i organisationens mission, kan et AI-system handle tilpasset uden virkelig at værdsætte menneskelige præferencer. Vi kan træne AI-systemer til at sige og gøre ting gennem feedback, og de kan lære at ræsonnere om, hvad mennesker vil have. Men at få dem til *oprigtigt* at værdsætte menneskelige præferencer er en langt dybere udfordring.[^11]

De dybe vanskeligheder ved at løse disse alignment-udfordringer og deres implikationer for AI-risiko vil blive udforsket yderligere nedenfor. Forstå foreløbig, at alignment ikke bare er en teknisk egenskab, vi hæfter på AI-systemer, men et grundlæggende aspekt af deres arkitektur, der former deres forhold til menneskeheden.

[^1]: For en blid men teknisk introduktion til maskinlæring og AI, særligt sprogmodeller, se [denne side.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) For en anden moderne primer om AI-udryddelsesrisici, se [dette stykke.](https://www.thecompendium.ai/) For en omfattende og autoritativ videnskabelig analyse af AI-sikkerheds tilstand, se den seneste [International AI Safety Report.](https://arxiv.org/abs/2501.17805)

[^2]: Træning sker typisk ved at lede efter et lokalt maksimum af scoren i et højdimensionalt rum givet af modellens vægte. Ved at tjekke hvordan scoren ændrer sig, når vægte justeres, identificerer træningsalgoritmen hvilke justeringer forbedrer scoren mest og flytter vægtene i den retning.

[^3]: For eksempel i et billedgenkendelses-problem ville det neurale netværk outputte sandsynligheder for etiketter for billedet. En score ville være relateret til sandsynligheden AI'en tillægger det korrekte svar. Træningsproceduren ville så justere vægte, så næste gang ville AI'en outputte en højere sandsynlighed for den korrekte etiket for det billede. Dette gentages så et enormt antal gange. Den samme grundlæggende procedure bruges til træning af i det væsentlige alle moderne neurale netværk, omend med mere komplekse scoringsmekanismer.

[^4]: De fleste multimodale modeller bruger "transformer"-arkitekturen til at behandle og generere flere typer data (tekst, billeder, lyd). Disse kan alle dekomponeres til og derefter behandles på lige fod som forskellige typer "tokens." Multimodale modeller trænes først til præcist at forudsige tokens inden for massive datasæt, derefter raffineres gennem forstærkningslæring for at øge kapaciteter og forme adfærd.

[^5]: At sprogmodeller trænes til at gøre én ting – forudsige ord – har fået nogle til at kalde dem snæver AI. Men dette er misvisende: fordi at forudsige tekst godt kræver så mange forskellige kapaciteter, fører denne træningsopgave til et overraskende generelt system. Bemærk også, at disse systemer trænes omfattende gennem forstærkningslæring, hvilket effektivt repræsenterer tusindvis af mennesker, der giver modellen et belønningssignal, når den gør et godt stykke arbejde med nogen af de mange ting, den gør. Den arver så betydelig generalitet fra de mennesker, der giver denne feedback.

[^6]: Der er flere måder, hvorpå AI er uforudsigelig. En er, at man i det generelle tilfælde ikke kan forudsige, hvad en algoritme vil gøre uden faktisk at køre den; der er [teoremer](https://arxiv.org/abs/1310.3225) for denne effekt. Dette kan være sandt bare fordi output fra algoritmer kan være komplekst. Men det er særlig klart og relevant i tilfældet (såsom i skak eller Go), hvor forudsigelsen ville implicere en kapacitet (at slå AI'en), den potentielle forudsiger ikke har. For det andet vil et givent AI-system ikke altid producere det samme output selv givet det samme input – dets output indeholder tilfældighed; dette kobler også med algoritmisk uforudsigelighed. For det tredje kan uventede og emergente kapaciteter opstå fra træning, hvilket betyder, at selv *typerne* af ting et AI-system kan og vil gøre er uforudsigelige; Denne sidste type er særlig vigtig for sikkerhedsovervejelser.

[^7]: Se [her](https://arxiv.org/abs/2502.02649) for en dybdegående gennemgang af, hvad der menes med en "autonom agent" (sammen med etiske argumenter imod at bygge dem).

[^8]: Du hører måske nogle gange "AI kan ikke have sine egne mål." Dette er fuldstændig nonsens. Det er let at generere eksempler, hvor AI har eller udvikler mål, der aldrig blev givet til den og kun er kendt af den selv. Du ser ikke dette meget i nuværende populære multimodale modeller, fordi det trænes ud af dem; det kunne lige så let trænes ind i dem.

[^9]: Der er en stor litteratur. Om det generelle problem se Christians [*The Alignment Problem*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821), og Russells [*Human-Compatible*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616). På en mere teknisk side se f.eks. [dette papir](https://arxiv.org/abs/2209.00626).

[^10]: Vi vil senere se, at selvom sådanne systemer går imod tendensen, gør det dem faktisk meget interessante og nyttige.

[^11]: Dette er ikke for at sige, vi kræver følelser eller bevidsthed. Snarere er det enormt vanskeligt udefra et system at vide, hvad dets indre mål, præferencer og værdier er. "Oprigtig" her ville betyde, at vi har stærk nok grund til at stole på det, at i tilfælde af kritiske systemer kan vi satse vores liv på det.