# Kapitel 3 - Nøgleaspekter af hvordan moderne generelle AI-systemer fremstilles

Verdens mest avancerede AI-systemer fremstilles ved hjælp af overraskende lignende metoder. Her er det grundlæggende.

For virkelig at forstå et menneske er man nødt til at vide noget om biologi, evolution, børneopdragelse og mere; for at forstå AI er man også nødt til at vide noget om, hvordan det fremstilles. I løbet af de seneste fem år har AI-systemer udviklet sig enormt både i kapacitet og kompleksitet. En central fremmende faktor har været tilgængeligheden af meget store mængder beregning (eller i daglig tale "compute" når det anvendes på AI).

Tallene er svimlende. Omkring 10<sup>25</sup>-10<sup>26</sup> "floating-point operations" (FLOP) [^1] anvendes til træning af modeller som GPT-serien, Claude, Gemini osv.[^2] (Til sammenligning: hvis alle mennesker på Jorden arbejdede uafbrudt med at udføre én beregning hvert femte sekund, ville det tage omkring en milliard år at opnå dette.) Denne enorme mængde beregning muliggør træning af modeller med op til billioner af modelvægte på terabytes af data – en stor brøkdel af al kvalitetstekst, der nogensinde er blevet skrevet, sammen med store biblioteker af lyd, billeder og video. Ved at supplere denne træning med yderligere omfattende træning, der forstærker menneskelige præferencer og god opgaveudførelse, udviser modeller trænet på denne måde menneskekonkurrencedygtige præstationer på tværs af et betydeligt spektrum af grundlæggende intellektuelle opgaver, herunder ræsonnement og problemløsning.

Vi ved også (meget, meget groft) hvor meget beregningshastighed i operationer per sekund, der er tilstrækkelig til at *inferens*-hastigheden [^3] af et sådant system matcher *hastigheden* af menneskelig tekstbehandling. Det er omkring 10<sup>15</sup>-10<sup>16</sup> FLOP per sekund.[^4]

Selvom de er kraftfulde, er disse modeller af deres natur begrænsede på nøgleområder, ret analogt med hvordan et individuelt menneske ville være begrænset, hvis det blev tvunget til blot at producere tekst med en fast hastighed af ord per minut uden at stoppe op for at tænke eller bruge yderligere værktøjer. Nyere AI-systemer adresserer disse begrænsninger gennem en mere kompleks proces og arkitektur, der kombinerer flere nøgleelementer:

- Et eller flere neurale netværk, hvor én model leverer den centrale kognitive kapacitet, og op til flere andre udfører andre mere snævre opgaver;
- *Værktøjer* leveret til og anvendelige af modellen – for eksempel evnen til at søge på nettet, oprette eller redigere dokumenter, udføre programmer osv.
- *Stilladsering* der forbinder input og output af neurale netværk. Et meget simpelt stillads kunne blot tillade to "instanser" af en AI-model at konversere med hinanden, eller at den ene kontrollerer den andens arbejde.[^5]
- *Tankegang* og relaterede prompting-teknikker gør noget lignende og får en model til for eksempel at generere mange tilgange til et problem og derefter behandle disse tilgange for et samlet svar.
- *Omtræning* af modeller til at gøre bedre brug af værktøjer, stilladsering og tankegang.

Eftersom disse udvidelser kan være meget kraftfulde (og inkludere AI-systemer selv), kan disse sammensatte systemer være ret sofistikerede og dramatisk forbedre AI-kapaciteter.[^6] Og for nylig er teknikker inden for stilladsering og især tankegangs-prompting (og indarbejdelse af resultater tilbage i omtræning af modeller til at bruge disse bedre) blevet udviklet og anvendt i [o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/) og [DeepSeek R1](https://api-docs.deepseek.com/news/news250120) til at foretage mange inferens-gennemgange som respons på en given forespørgsel.[^7] Dette gør det i praksis muligt for modellen at "tænke over" sit svar og øger dramatisk disse modellers evne til at udføre højkvalitets-ræsonnement inden for videnskab, matematik og programmeringsopgaver.[^8]

For en given AI-arkitektur kan stigninger i træningsberegning [pålideligt oversættes](https://arxiv.org/abs/2405.10938) til forbedringer i et sæt klart definerede målinger. For mindre præcist definerede generelle kapaciteter (såsom dem diskuteret nedenfor) er oversættelsen mindre klar og forudsigelig, men det er næsten sikkert, at større modeller med mere træningsberegning vil have nye og bedre kapaciteter, selvom det er svært at forudsige, hvad disse vil være.

Tilsvarende har sammensatte systemer og især fremskridt inden for "tankegang" (og træning af modeller, der fungerer godt med det) låst op for skalering i *inferens*-beregning: for en given trænet kernemodel stiger i det mindste nogle AI-systemkapaciteter, efterhånden som mere beregning anvendes, der gør det muligt for dem at "tænke hårdere og længere" over komplekse problemer. Dette kommer til en høj pris i beregningshastighed og kræver hundredvis eller tusinder flere FLOP/s for at matche menneskelig præstation.[^9]

Selvom det kun er en del af det, der fører til hurtige AI-fremskridt,[^10] vil rollen af beregning og muligheden for sammensatte systemer vise sig at være afgørende for både at forhindre ukontrollerbar AGI og udvikle sikrere alternativer.

[^1]: 10<sup>27</sup> betyder 1 efterfulgt af 25 nuller, eller ti billioner billioner. En FLOP er blot en aritmetisk addition eller multiplikation af tal med en vis præcision. Bemærk at AI-hardware-ydeevne kan variere med en faktor på ti mere afhængigt af den aritmetiske præcision og computerens arkitektur. At tælle logiske gate-operationer (ANDS, ORS, AND NOTS) ville være fundamentalt, men disse er ikke almindeligt tilgængelige eller benchmark'et; til nuværende formål er det nyttigt at standardisere på 16-bit operationer (FP16), selvom passende konverteringsfaktorer bør etableres.

[^2]: En samling af estimater og hårde data er tilgængelig fra [Epoch AI](https://epochai.org/data/large-scale-ai-models) og indikerer omkring 2×10<sup>25</sup> 16-bit FLOP for GPT-4; dette matcher nogenlunde [tal der blev lækket](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) for GPT-4. Estimater for andre mid-2024 modeller er alle inden for en faktor på få af GPT-4.

[^3]: Inferens er simpelthen processen med at generere et output fra et neuralt netværk. Træning kan betragtes som en rækkefølge af mange inferenser og modelvægt-justeringer.

[^4]: For tekstproduktion krævede den oprindelige GPT-4 560 TFLOP per token genereret. Omkring 7 tokens/s er nødvendigt for at følge med menneskelig tankegang, så dette giver ≈3×10<sup>15</sup> FLOP/s. Men effektiviseringer har drevet dette ned; [denne NVIDIA-brochure](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/) indikerer for eksempel så lidt som 3×10<sup>14</sup> FLOP/s for en sammenlignelig præsterende Llama 405B model.

[^5]: Som et lidt mere komplekst eksempel kunne et AI-system først generere flere mulige løsninger på et matematikproblem, derefter bruge en anden instans til at kontrollere hver løsning og til sidst bruge en tredje til at syntetisere resultaterne til en klar forklaring. Dette muliggør mere grundig og pålidelig problemløsning end et enkelt gennemløb.

[^6]: Se for eksempel detaljer om [OpenAI's "Operator"](https://openai.com/index/introducing-operator/), [Claude's værktøjskapaciteter](https://docs.anthropic.com/en/docs/build-with-claude/computer-use) og [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). OpenAI's [Deep Research](https://openai.com/index/introducing-deep-research/) har formodentlig en ret sofistikeret arkitektur, men detaljer er ikke tilgængelige.

[^7]: Deepseek R1 bygger på iterativ træning og prompting af modellen, så den endelige trænede model skaber omfattende tankegangs-ræsonnement. Arkitektoniske detaljer er ikke tilgængelige for o1 eller o3, men Deepseek har afsløret, at der ikke kræves nogen særlig "hemmelig ingrediens" for at låse op for kapacitetsskalering med inferens. Men på trods af at have modtaget meget presse som omvæltende af "status quo" inden for AI, påvirker det ikke denne artikels kerneargumenter.

[^8]: Disse modeller overgår betydeligt standardmodeller på ræsonnement-benchmarks. For eksempel på GPQA Diamond Benchmark - en stringent test af spørgsmål på PhD-niveau inden for videnskab - [scorede](https://openai.com/index/learning-to-reason-with-llms/) GPT-4o 56%, mens o1 og o3 opnåede henholdsvis 78% og 88%, hvilket langt oversteg de 70% gennemsnitsscore fra menneskelige eksperter.

[^9]: OpenAI's O3 brugte formodentlig ∼10<sup>21</sup>-10<sup>22</sup> FLOP [til at gennemføre hver af ARC-AGI udfordrings-spørgsmålene](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai), som kompetente mennesker kan gøre på (lad os sige) 10-100 sekunder, hvilket giver et tal mere som ∼10<sup>20</sup> FLOP/s.

[^10]: Selvom beregning er et nøglemål for AI-systemkapacitet, interagerer det med både datakvalitet og algoritmiske forbedringer. Bedre data eller algoritmer kan reducere beregningskrav, mens mere beregning nogle gange kan kompensere for svagere data eller algoritmer.