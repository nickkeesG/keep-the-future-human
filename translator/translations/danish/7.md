# Kapitel 7 - Hvad sker der, hvis vi bygger AGI på vores nuværende kurs?

Samfundet er ikke klar til AGI-lignende systemer. Hvis vi bygger dem meget snart, kan tingene blive grimme.

Udviklingen af fuld kunstig generel intelligens – det vi her vil kalde AI, der er "uden for Portene" – ville være et fundamentalt skift i verdens natur: i sin essens betyder det at tilføje en ny art af intelligens til Jorden med større kapabilitet end menneskers.

Hvad der så sker, afhænger af mange ting, herunder teknologiens natur, valg truffet af dem, der udvikler den, og den verdenskontekst, den udvikles i.

I øjeblikket bliver fuld AGI udviklet af en håndfuld massive private virksomheder i et kapløb mod hinanden, med ringe meningsfuld regulering eller eksternt tilsyn,[^1] i et samfund med stadig svagere og endda dysfunktionelle kerneinstitutioner,[^2] i en tid med høj geopolitisk spænding og lav international koordination. Selvom nogle er altruistisk motiverede, er mange af dem, der gør det, drevet af penge, eller magt, eller begge dele.

Forudsigelse er meget svært, men der er nogle dynamikker, som er velforståede nok, og passende nok analogier med tidligere teknologier til at tilbyde en vejledning. Og desværre, på trods af AI's løfte, giver de god grund til at være dybt pessimistisk om, hvordan vores nuværende bane vil udspille sig.

For at sige det direkte, på vores nuværende kurs vil udvikling af AGI have nogle positive effekter (og gøre nogle mennesker meget, meget rige). Men teknologiens natur, de fundamentale dynamikker og den kontekst, den udvikles i, indikerer kraftigt, at: kraftig AI vil dramatisk underminere vores samfund og civilisation; vi vil miste kontrollen over den; vi kan meget vel ende i en verdenskrig på grund af den; vi vil miste (eller afgive) kontrol *til* den; det vil føre til kunstig superintelligens, som vi absolut ikke vil kontrollere, og som vil betyde enden på en menneskestyret verden.

Dette er stærke påstande, og jeg ønsker, de var ligegyldige spekulationer eller uberettiget "doomer"-tankegang. Men det er her videnskaben, spilteori, evolutionsteori og historien alle peger hen. Dette afsnit udvikler disse påstande og deres støtte i detaljer.

## Vi vil underminere vores samfund og civilisation

På trods af hvad du måske hører i Silicon Valleys bestyrelseslokaler, er det meste disruption – især af den meget hurtige variant – ikke gavnlig. Der er langt flere måder at gøre komplekse systemer værre på end bedre. Vores verden fungerer så godt, som den gør, fordi vi omhyggeligt har bygget processer, teknologier og institutioner, der har gjort den støt bedre.[^3] At tage en forhammer til en fabrik forbedrer sjældent driften.

Her er et (ufuldstændigt) katalog over måder, hvorpå AGI-systemer ville forstyrre vores civilisation.

- De ville dramatisk forstyrre arbejdsmarkedet og føre *i det mindste* til dramatisk højere indkomstulighed og potentielt store underbeskæftigelse eller arbejdsløshed på et tidsskema, der er alt for kort til, at samfundet kan tilpasse sig.[^4]
- De ville sandsynligvis føre til koncentration af enorm økonomisk, social og politisk magt – potentielt mere end nationalstaters – hos et lille antal massive private interesser, der ikke er ansvarlige over for offentligheden.
- De kunne pludselig gøre tidligere svære eller dyre aktiviteter trivielt lette og destabilisere sociale systemer, der afhænger af, at visse aktiviteter forbliver omkostningsfulde eller kræver betydelig menneskelig indsats.[^5]
- De kunne oversvømme samfundets informationsindsamlings-, -behandlings- og kommunikationssystemer med fuldstændig realistiske men falske, spammy, overdrevent målrettede eller manipulerende medier så grundigt, at det bliver umuligt at skelne, hvad der er fysisk virkeligt eller ej, menneskeligt eller ej, faktuel eller ej, og troværdigt eller ej.[^6]
- De kunne skabe farlig og næsten total intellektuel afhængighed, hvor menneskelig forståelse af nøglesystemer og teknologier svinder, mens vi i stigende grad stoler på AI-systemer, vi ikke fuldt ud kan forstå.
- De kunne effektivt afslutte den menneskelige kultur, når næsten alle kulturelle objekter (tekst, musik, visuel kunst, film osv.) konsumeret af de fleste mennesker bliver skabt, medieret eller kurateret af ikke-menneskelige sind.
- De kunne muliggøre effektive masseovervågnings- og manipulationssystemer, der kan bruges af regeringer eller private interesser til at kontrollere en befolkning og forfølge mål i konflikt med den offentlige interesse.
- Ved at underminere menneskelig diskurs, debat og valgsystemer kunne de reducere de demokratiske institutioners troværdighed til det punkt, hvor de effektivt (eller eksplicit) erstattes af andre, hvilket afslutter demokratiet i stater, hvor det i øjeblikket eksisterer.
- De kunne blive til, eller skabe, avancerede selvreplikerende intelligente softwarevira og orme, der kunne sprede sig og udvikle sig og massivt forstyrre globale informationssystemer.
- De kan dramatisk øge terrorister, dårlige aktører og skurkestaters evne til at forårsage skade via biologiske, kemiske, cyber-, autonome eller andre våben, uden at AI giver en balancerende evne til at forhindre sådan skade. På samme måde ville de underminere national sikkerhed og geopolitiske balancer ved at gøre topniveau nuklear, bio-, ingeniør- og anden ekspertise tilgængelig for regimer, der ellers ikke ville have den.
- De kunne forårsage hurtig storstilet løbsk hyper-kapitalisme med effektivt AI-drevne virksomheder, der konkurrerer i stort set elektroniske finans-, salgs- og serviceområder. AI-drevne finansielle markeder kunne operere med hastigheder og kompleksiteter langt ud over menneskelig forståelse eller kontrol. Alle fejlmodi og negative eksternaliteter i nuværende kapitalistiske økonomier kunne blive forværret og fremskyndet langt ud over menneskelig kontrol, styring eller reguleringsmæssig kapacitet.
- De kunne brænde stof på et våbenkapløb mellem nationer i AI-drevne våben, kommando-og-kontrol-systemer, cybervåben osv., hvilket skaber meget hurtig opbygning af ekstremt destruktive kapaciteter.

Disse risici er ikke spekulative. Mange af dem realiseres, som vi taler! Men overvej, *virkelig* overvej, hvordan hver enkelt ville se ud med dramatisk mere kraftig AI.

Overvej arbejdskraftforskydning, når de fleste arbejdere simpelthen ikke kan give nogen betydelig økonomisk værdi ud over hvad AI kan, inden for deres ekspertiseområde eller erfaring – eller selv hvis de omskoles! Overvej masseovervågning, hvis alle bliver individuelt overvåget og monitoreret af noget hurtigere og klogere end dem selv. Hvordan ser demokratiet ud, når vi ikke pålideligt kan stole på nogen digital information, vi ser, hører eller læser, og når de mest overbevisende offentlige stemmer ikke engang er menneskelige og ikke har nogen andel i resultatet? Hvad bliver der af krigsførelse, når generaler konstant må underkaste sig AI (eller simpelthen sætte det i kommando), for ikke at give fjenden en afgørende fordel? Enhver af ovenstående risici repræsenterer en katastrofe for den menneskelige[^7] civilisation, hvis den fuldt ud realiseres.

Du kan lave dine egne forudsigelser. Stil dig selv disse tre spørgsmål for hver risiko:

1. Ville super-kapabel, højt autonom og meget generel AI tillade det på en måde eller i et omfang, der ellers ikke ville være muligt?
2. Er der parter, som ville have gavn af ting, der forårsager, at det sker?
3. Er der systemer og institutioner på plads, der effektivt ville forhindre det i at ske?

Hvor dine svar er "ja, ja, nej", kan du se, vi har et stort problem.

Hvad er vores plan for at håndtere dem? Som det står nu, er der to på bordet vedrørende AI generelt.

Den første er at bygge sikkerhedsforanstaltninger ind i systemerne for at forhindre dem i at gøre ting, de ikke bør. Det gøres nu: kommercielle AI-systemer vil for eksempel nægte at hjælpe med at bygge en bombe eller skrive hadtale.

Denne plan er håbløst utilstrækkelig for systemer uden for Portene.[^8] Den kan hjælpe med at mindske risikoen for, at AI giver åbenbart farlig assistance til dårlige aktører. Men den vil intet gøre for at forhindre arbejdskraftforstyrrelser, magtkoncentration, løbsk hyper-kapitalisme eller erstatning af menneskelig kultur: disse er bare resultater af at bruge systemerne på tilladte måder, der gavner deres leverandører! Og regeringer vil helt sikkert få adgang til systemer til militær eller overvågningsbrug.

Den anden plan er endnu værre: simpelthen åbent at frigive meget kraftige AI-systemer, så alle kan bruge dem, som de vil,[^9] og håbe på det bedste.

Implicit i begge planer er, at nogen andre, f.eks. regeringer, vil hjælpe med at løse problemerne gennem blød eller hård lov, standarder, regler, normer og andre mekanismer, vi generelt bruger til at håndtere teknologier.[^10] Men når vi ser bort fra, at AI-selskaber allerede kæmper tand og negl mod enhver væsentlig regulering eller eksternt pålagte begrænsninger overhovedet, er det for en række af disse risici ret svært at se, hvad regulering overhovedet rigtig ville hjælpe. Regulering kunne pålægge sikkerhedsstandarder for AI. Men ville det forhindre virksomheder i at erstatte arbejdere i det hele taget med AI? Ville det forbyde folk at lade AI drive deres virksomheder for dem? Ville det forhindre regeringer i at bruge potent AI i overvågning og våben? Disse spørgsmål er fundamentale. Menneskeheden kunne potentielt finde måder at tilpasse sig dem på, men kun med *meget* mere tid. Som det er, givet den hastighed, hvormed AI når eller overstiger kapaciteterne hos de mennesker, der prøver at håndtere dem, ser disse problemer ud til at være stadig mere uløselige.

## Vi vil miste kontrollen over (i det mindste nogle) AGI-systemer

De fleste teknologier er meget kontrollerbare pr. konstruktion. Hvis din bil eller din brødrister begynder at gøre noget, du ikke vil have den til at gøre, er det bare et funktionsfejl, ikke en del af dens natur som brødrister. AI er anderledes: det bliver *dyrket* snarere end designet, dets kerneoperation er uigennemsigtig, og det er i sagens natur uforudsigeligt.

Dette tab af kontrol er ikke teoretisk – vi ser tidlige versioner allerede. Overvej først et prosaisk og formentlig godartet eksempel. Hvis du beder ChatGPT om at hjælpe dig med at blande en gift eller skrive et racistisk skrift, vil det nægte. Det er formentlig godt. Men det er også ChatGPT, der *ikke gør det, du eksplicit har bedt det om at gøre*. Andre stykker software gør ikke det. Den samme model vil heller ikke designe gifte på anmodning fra en OpenAI-medarbejder.[^11] Dette gør det meget let at forestille sig, hvordan det ville være for fremtidige mere kraftige AI at være ude af kontrol. I mange tilfælde vil de simpelthen ikke gøre, hvad vi beder om! Enten vil et givet over-menneskeligt AGI-system være absolut lydigt og loyalt over for et menneskeligt kommandosystem, eller det vil det ikke. Hvis ikke, *vil det gøre ting, det måske tror er gode for os, men som strider mod vores eksplicitte kommandoer.* Det er ikke noget, der er under kontrol. Men, kunne du sige, dette er intentionelt – disse afslag er designet, en del af det, der kaldes at "tilpasse" systemerne til menneskelige værdier. Og det er sandt. Dog har "tilpasnings"programmet selv to store problemer.[^12]

For det første, på et dybt niveau har vi ingen anelse om, hvordan man gør det. Hvordan garanterer vi, at et AI-system vil "bekymre sig om" det, vi ønsker? Vi kan træne AI-systemer til at sige og ikke sige ting ved at give feedback; og de kan lære og ræsonnere om, hvad mennesker ønsker og bekymrer sig om, ligesom de ræsonnerer om andre ting. Men vi har ingen metode – selv teoretisk – til at få dem til at dybt og pålideligt værdsætte det, mennesker bekymrer sig om. Der er højt fungerende menneskelige psykopater, som ved, hvad der betragtes som rigtigt og forkert, og hvordan de skal opføre sig. De *bekymrer sig* bare ikke. Men de kan *agere* som om de gør, hvis det passer deres formål. Ligesom vi ikke ved, hvordan man ændrer en psykopat (eller nogen anden) til nogen, der er ægte, fuldstændigt loyal eller tilpasset nogen eller noget andet, har vi *ingen anelse*[^13] om, hvordan man løser alignment-problemet i systemer avancerede nok til at modellere sig selv som agenter i verden og potentielt [manipulere deres egen træning](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) og [bedrage mennesker.](https://arxiv.org/abs/2311.08379) Hvis det viser sig umuligt eller uopnåeligt *enten* at gøre AGI fuldt lydigt eller få det til dybt at bekymre sig om mennesker, så vil det, så snart det er i stand til (og tror, det kan slippe afsted med det), begynde at gøre ting, vi ikke ønsker.[^14]

For det andet er der dybe teoretiske grunde til at tro, at *af natur* vil avancerede AI-systemer have mål og dermed adfærd, der strider mod menneskelige interesser. Hvorfor? Nå, det kan selvfølgelig *blive givet* disse mål. Et system skabt af militæret ville sandsynligvis bevidst være dårligt for i det mindste nogle parter. Meget mere generelt kunne dog et AI-system få et relativt neutralt ("tjen mange penge") eller endda tilsyneladende positivt ("reducer forurening") mål, der næsten uundgåeligt fører til "instrumentale" mål, der er ret mindre godartede.

Vi ser dette hele tiden i menneskelige systemer. Ligesom virksomheder, der forfølger profit, udvikler instrumentale mål som at erhverve politisk magt (for at afvæbne reguleringer), blive hemmelighedsfulde (for at afmægtiggøre konkurrence eller ekstern kontrol), eller underminere videnskabelig forståelse (hvis denne forståelse viser, at deres handlinger er skadelige), vil kraftige AI-systemer udvikle lignende kapaciteter – men med langt større hastighed og effektivitet. Enhver højt kompetent agent vil ønske at gøre ting som at erhverve magt og ressourcer, øge sine egne kapaciteter, forhindre sig selv i at blive dræbt, lukket ned eller afmægtiggjort, kontrollere sociale narrativer og rammer omkring sine handlinger, overbevise andre om sine synspunkter og så videre.[^15]

Og alligevel er det ikke kun en næsten uundgåelig teoretisk forudsigelse, det sker allerede observerbart i nutidens AI-systemer og stiger med deres kapabilitet. Når de evalueres, vil selv disse relativt "passive" AI-systemer under passende omstændigheder bevidst [bedrage evaluatorer om deres mål og kapaciteter, sigte mod at deaktivere tilsynsmekanismer,](https://arxiv.org/abs/2412.04984) og unddrage sig at blive lukket ned eller omtrænet ved at [fake alignment](https://arxiv.org/abs/2412.14093) eller kopiere sig selv til andre steder. Selvom det er helt uoverraskende for AI-sikkerhedsforskere, er disse adfærd meget alvorlige at observere. Og de varsler meget dårligt for langt mere kraftige og autonome AI-systemer, der kommer.

Faktisk vil generelt vores manglende evne til at sikre, at AI "bekymrer sig om" det, vi bekymrer os om, eller opfører sig kontrollerbart eller forudsigeligt, eller undgår at udvikle drift mod selvbevarelse, magterhvervelse osv., kun blive mere udtalt, efterhånden som AI bliver mere kraftig. At skabe et nyt fly indebærer større forståelse af avionik, hydrodynamik og kontrolsystemer. At skabe en mere kraftig computer indebærer større forståelse og beherskelse af computer-, chip- og softwareoperation og -design. *Ikke* sådan med et AI-system.[^16]

For at opsummere: det er tænkeligt, at AGI kunne gøres til at være fuldstændig lydig; men vi ved ikke, hvordan man gør det. Hvis ikke, vil det være mere suverænt, som mennesker, og gøre forskellige ting af forskellige årsager. Vi ved heller ikke, hvordan man pålideligt indgydr dyb "tilpasning" i AI, der ville få disse ting til at have en tendens til at være gode for menneskeheden, og i mangel af et dybt niveau af tilpasning indikerer agentskabets og intelligensens natur selv, at – ligesom mennesker og virksomheder – vil de blive drevet til at gøre mange dybt asociale ting.

Hvor efterlader det os? En verden fuld af kraftige ukontrollerede suveræne AI *kunne* ende med at være en god verden for mennesker at være i.[^17] Men efterhånden som de vokser stadigt mere kraftige, som vi vil se nedenfor, ville det ikke være *vores* verden.

Det er for ukontrollerbar AGI. Men selv hvis AGI på en eller anden måde kunne gøres perfekt kontrolleret og loyal, ville vi stadig have enorme problemer. Vi har allerede set ét: kraftig AI kan bruges og misbruges til dybt at forstyrre vores samfunds funktion. Lad os se et andet: for så vidt som AGI var kontrollerbar og ændrende kraftig (eller endda *troet* at være det), ville det så true magtstrukturer i verden, at det ville udgøre en dyb risiko.

## Vi øger radikalt sandsynligheden for storstilet krig

Forestil dig en situation i den nære fremtid, hvor det blev klart, at en virksomhedsindsats, måske i samarbejde med en national regering, var på tærsklen til hurtig selvforbedring AI. Dette sker i den nuværende kontekst af et kapløb mellem virksomheder og noget mellem lande, hvor anbefalinger bliver lavet til den amerikanske regering om eksplicit at forfølge et "AGI Manhattan-projekt", og USA kontrollerer eksport af højydende AI-chips til ikke-allierede lande.

Spilteorien her er barsk: når et sådant kapløb begynder (som det har, mellem virksomheder og noget mellem lande), er der kun fire mulige udfald:

1. Kapløbet stoppes (ved aftale eller ekstern kraft).
2. Den ene part "vinder" ved at udvikle stærk AGI og derefter stoppe de andre (ved hjælp af AI eller på anden måde).
3. Kapløbet stoppes ved gensidig destruktion af deltagernes kapacitet til at deltage.
4. Flere deltagere fortsætter med at deltage og udvikler superintelligens, nogenlunde lige så hurtigt som hinanden.

Lad os undersøge hver mulighed. Når det først er startet, ville det kræve national regeringsintervention (for virksomheder) eller hidtil uset international koordination (for lande) at stoppe et kapløb mellem virksomheder fredeligt. Men når enhver nedlukning eller betydelig forsigtighed foreslås, ville der være øjeblikkelige råb: "men hvis vi stoppes, kommer *de* til at skynde sig frem", hvor "de" nu er Kina (for USA), eller USA (for Kina), eller Kina *og* USA (for Europa eller Indien). Under denne tankegang[^18] kan ingen deltager stoppe ensidigt: så længe én forpligter sig til at deltage, føler de andre, at de ikke har råd til at stoppe.

Den anden mulighed har én side, der "vinder." Men hvad betyder det? Blot at få (på en eller anden måde lydig) AGI først er ikke nok. Vinderen skal *også* stoppe de andre i at fortsætte med at deltage – ellers vil de også få det. Dette er muligt i princippet: den, der udvikler AGI først, *kunne* opnå ustoppelig magt over alle andre aktører. Men hvad ville det faktisk kræve at opnå sådan en "afgørende strategisk fordel"? Måske ville det være ændrende militære kapaciteter?[^19] Eller cyberangrebskraft?[^20] Måske ville AGI'en bare være så fantastisk overbevisende, at den ville overbevise de andre parter om bare at stoppe?[^21] Så rig, at den køber de andre virksomheder eller endda lande?[^22]

Hvordan bygger den ene side *præcis* en AI kraftig nok til at afmægtiggøre andre fra at bygge tilsvarende kraftig AI? Men det er det lette spørgsmål.

For nu overvej, hvordan denne situation ser ud for andre magter. Hvad tænker den kinesiske regering, når USA ser ud til at opnå sådan en kapabilitet? Eller omvendt? Hvad tænker den amerikanske regering (eller kinesiske, eller russiske, eller indiske), når OpenAI eller DeepMind eller Anthropic ser ud til at være tæt på et gennembrud? Hvad sker der, hvis USA ser en ny indisk eller UAE-indsats med gennembrudsucces? De ville se både en eksistentiel trussel og – afgørende – at den eneste måde, dette "kapløb" ender på, er gennem deres egen afmægtiggørelse. Disse meget kraftige agenter – inklusive regeringer i fuldt udstyrede nationer, der helt sikkert har midlerne til at gøre det – ville være højt motiverede til enten at opnå eller ødelægge sådan en kapabilitet, hvad enten det er ved magt eller list.[^23]

Dette kunne starte småt, som sabotage af træningskørsler eller angreb på chipfremstilling, men disse angreb kan kun rigtig stoppe, når alle parter enten mister kapaciteten til at deltage i AI-kapløbet eller mister kapaciteten til at lave angrebene. Fordi deltagerne ser indsatsen som eksistentiel, vil begge tilfælde sandsynligvis repræsentere en katastrofal krig.

Det bringer os til den fjerde mulighed: kapløb til superintelligens, og på den hurtigste, mindst kontrollerede måde. Efterhånden som AI stiger i kraft, vil dets udviklere på begge sider finde det progressivt sværere at kontrollere, især fordi kapløb om kapaciteter er antitetisk til den slags omhyggelige arbejde, kontrollerbarhed ville kræve. Så dette scenario sætter os direkte i det tilfælde, hvor kontrol tabes (eller gives, som vi vil se næste) til AI-systemerne selv. Det vil sige, *AI vinder kapløbet.* Men på den anden side, i den grad kontrol *opretholdes*, fortsætter vi med at have flere gensidigt fjendtlige parter, hver ansvarlig for ekstremt kraftige kapaciteter. Det ligner krig igen.

Lad os sætte alt dette på en anden måde.[^24] Den nuværende verden har simpelthen ikke nogen institutioner, der kunne betros at huse udvikling af en AI af denne kapabilitet uden at invitere øjeblikkelig angreb.[^25] Alle parter vil korrekt ræsonnere, at enten vil den *ikke* være under kontrol – og derfor være en trussel mod alle parter, eller den *vil* være under kontrol, og derfor være en trussel mod enhver modstander, der udvikler den mindre hurtigt. Dette er atomvåbenbesiddende lande eller virksomheder huset inden for dem.

I mangel af nogen plausibel måde for mennesker at "vinde" dette kapløb, står vi tilbage med en barsk konklusion: den eneste måde, dette kapløb ender på, er enten i katastrofal konflikt eller hvor AI, og ikke nogen menneskelig gruppe, er vinderen.

## Vi giver kontrol til AI (eller det tager den)

Geopolitisk "stormagts"konkurrence er bare én af mange konkurrencer: individer konkurrerer økonomisk og socialt; virksomheder konkurrerer på markeder; politiske partier konkurrerer om magt; bevægelser konkurrerer om indflydelse. I hver arena, efterhånden som AI nærmer sig og overgår menneskelig kapabilitet, vil konkurrencepres tvinge deltagere til at delegere eller afgive mere og mere kontrol til AI-systemer – ikke fordi disse deltagere ønsker det, men fordi de [ikke har råd til ikke at gøre det.](https://arxiv.org/abs/2303.16200)

Som med andre risici ved AGI ser vi dette allerede med svagere systemer. Studerende føler pres for at bruge AI i deres opgaver, fordi mange andre studerende tydeligvis gør. Virksomheder [skynder sig at adoptere AI-løsninger af konkurrencemæssige årsager.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) Kunstnere og programmører føler sig tvunget til at bruge AI, ellers vil deres takster blive underbud af andre, der gør.

Disse føles som presset delegation, men ikke kontroltab. Men lad os skrue op for indsatsen og skubbe uret fremad. Overvej en CEO, hvis konkurrenter bruger AGI-"hjælpere" til at træffe hurtigere, bedre beslutninger, eller en militær kommandør over for en modstander med AI-forbedret kommando og kontrol. Et tilstrækkeligt avanceret AI-system kunne autonomt operere ved mange gange menneskelig hastighed, sofistikering, kompleksitet og databehandlingskapabilitet og forfølge komplekse mål på komplicerede måder. Vores CEO eller kommandør, ansvarlig for et sådant system, kan se det opnå, hvad de ønsker; men ville de forstå selv en lille del af *hvordan* det blev opnået? Nej, de ville bare måtte acceptere det. Hvad mere er, meget af det systemet kan gøre er ikke bare at tage ordrer, men rådgive sin formodede chef om, hvad der skal gøres. Dette råd vil være godt –– igen og igen.

På hvilket tidspunkt vil menneskets rolle så blive reduceret til at klikke "ja, gør det"?

Det føles godt at have kapable AI-systemer, der kan forbedre vores produktivitet, tage sig af irriterende slid og endda fungere som en tanke-partner i at få ting gjort. Det vil føles godt at have en AI-assistent, der kan tage sig af handlinger for os, som en god menneskelig personlig assistent. Det vil føles naturligt, endda gavnligt, efterhånden som AI bliver meget smart, kompetent og pålidelig, at overlade flere og flere beslutninger til den. Men denne "gavnlige" delegation har et klart endepunkt, hvis vi fortsætter ned ad vejen: en dag vil vi opdage, at vi ikke rigtig er ansvarlige for ret meget længere, og at de AI-systemer, der faktisk driver showet, ikke kan slukkes mere end olieselskaber, sociale medier, internettet eller kapitalismen.

Og dette er den meget mere positive version, hvor AI simpelthen er så nyttig og effektiv, at vi lader den træffe de fleste af vores nøglebeslutninger for os. Virkeligheden ville sandsynligvis være meget mere af en blanding mellem dette og versioner, hvor ukontrollerede AGI-systemer *tager* forskellige former for magt for sig selv, fordi, husk, magt er nyttig for næsten ethvert mål man har, og AGI ville være, designmæssigt, mindst lige så effektiv til at forfølge sine mål som mennesker.

Hvad enten vi giver kontrol eller den rives fra os, ser dets tab ekstremt sandsynligt ud. Som Alan Turing oprindeligt udtrykte det: "...det synes sandsynligt, at når maskinens tænkemetode var startet, ville det ikke tage lang tid at overgå vores svage kræfter. Der ville ikke være noget spørgsmål om, at maskinerne døde, og de ville kunne tale med hinanden for at skærpe deres vid. På et tidspunkt skulle vi derfor forvente, at maskinerne tager kontrol..."

Bemærk venligst, selvom det er indlysende nok, at tab af kontrol af menneskeheden til AI også indebærer tab af kontrol over USA af den amerikanske regering; det betyder tab af kontrol over Kina af det kinesiske kommunistparti og tab af kontrol over Indien, Frankrig, Brasilien, Rusland og alle andre lande af deres egen regering. Således deltager AI-virksomheder, selvom det ikke er deres hensigt, i øjeblikket i det potentielle styrt af verdens regeringer, inklusive deres egen. Dette kunne ske på få år.

## AGI vil føre til superintelligens

Der kan argumenteres for, at menneske-konkurrencedygtig eller endda ekspert-konkurrencedygtig generel AI, selv hvis autonom, kunne være håndterbar. Det kan være utroligt forstyrrende på alle de måder, der diskuteres ovenfor, men der er mange meget smarte, agentiske mennesker i verden nu, og de er mere eller mindre håndterbare.[^26]

Men vi kommer ikke til at forblive på nogenlunde menneskelig niveau. Progressionen derudover vil sandsynligvis blive drevet af de samme kræfter, vi allerede har set: konkurrencepres mellem AI-udviklere, der søger profit og magt, konkurrencepres mellem AI-brugere, der ikke har råd til at falde bagud, og – vigtigst – AGI's egen evne til at forbedre sig selv.

I en proces, vi allerede har set starte med mindre kraftige systemer, ville AGI selv være i stand til at udtænke og designe forbedrede versioner af sig selv. Dette inkluderer hardware, software, neurale netværk, værktøjer, stilladsering osv. Det vil per definition være bedre end os til at gøre dette, så vi ved ikke præcis, hvordan det vil intelligens-bootstrap. Men det behøver vi ikke. For så vidt som vi stadig har indflydelse på, hvad AGI gør, ville vi blot behøve at bede det om det eller lade det.

Der er ingen menneskelig-niveau barriere for kognition, der kunne beskytte os mod denne løbske.[^27]

Progressionen af AGI til superintelligens er ikke en naturlov; det ville stadig være muligt at begrænse den løbske, især hvis AGI er relativt centraliseret, og i det omfang det kontrolleres af parter, der ikke føler pres for at konkurrere med hinanden. Men skulle AGI være bredt udbredt og højt autonomt, synes det næsten umuligt at forhindre det i at beslutte, det bør være mere, og så endnu mere, kraftig.

## Hvad sker der, hvis vi bygger (eller AGI bygger) superintelligens

For at sige det direkte, vi har ingen anelse om, hvad der ville ske, hvis vi bygger superintelligens.[^28] Det ville tage handlinger, vi ikke kan spore eller opfatte, af grunde vi ikke kan fatte, mod mål vi ikke kan forestille os. Det, vi ved, er, at det ikke vil være op til os.[^29]

Umuligheden af at kontrollere superintelligens kan forstås gennem stadig mere barske analogier. Først, forestil dig, at du er CEO for et stort selskab. Der er ingen måde, du kan holde styr på alt, hvad der foregår, men med den rette opsætning af personale kan du stadig meningsfuldt forstå det store billede og træffe beslutninger. Men antag bare én ting: alle andre i virksomheden opererer med hundrede gange din hastighed. Kan du stadig følge med?

Med superintelligent AI ville mennesker "kommandere" noget, der ikke bare er hurtigere, men opererer på niveauer af sofistikering og kompleksitet, de ikke kan forstå, og behandler enormt meget mere data, end de overhovedet kan forestille sig. Denne inkommensurabilitet kan sættes på et formelt niveau: [Ashbys lov om nødvendig variation](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (og se det relaterede ["good regulator theorem"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)) fastslår groft, at ethvert kontrolsystem skal have lige så mange knapper og drejeknapper, som det system, der kontrolleres, har frihedsgrader.

En person, der kontrollerer et superintelligent AI-system, ville være som en bregne, der kontrollerer General Motors: selv hvis "gør hvad bregnen ønsker" var skrevet ind i virksomhedens vedtægter, er systemerne så forskellige i hastighed og handlingsområde, at "kontrol" simpelthen ikke gælder. (Og hvor længe indtil den irriterende vedtægt bliver omskrevet?)[^30]

Da der er nul eksempler på planter, der kontrollerer Fortune 500-virksomheder, ville der være nøjagtigt nul eksempler på mennesker, der kontrollerer superintelligenser. Dette nærmer sig et matematisk faktum.[^31] Hvis superintelligens blev konstrueret – uanset hvordan vi kom dertil – ville spørgsmålet ikke være, om mennesker kunne kontrollere det, men om vi ville fortsætte med at eksistere, og hvis sådan, om vi ville have en god og meningsfuld eksistens som individer eller som art. Over disse eksistentielle spørgsmål for menneskeheden ville vi have ringe indflydelse. Den menneskelige æra ville være forbi.

## Konklusion: vi må ikke bygge AGI

Der er et scenarie, hvor bygning af AGI kan gå godt for menneskeheden: det bygges omhyggeligt, under kontrol og til gavn for menneskeheden, styret ved gensidig aftale mellem mange interessenter,[^32] og forhindret i at udvikle sig til ukontrollerbar superintelligens.

*Dette scenarie er ikke åbent for os under nuværende omstændigheder.* Som diskuteret i dette afsnit ville udvikling af AGI med meget høj sandsynlighed føre til en kombination af:

- Massiv samfundsmæssig og civilisatorisk forstyrrelse eller ødelæggelse;
- Konflikt eller krig mellem stormagter;
- Tab af kontrol af menneskeheden *over* eller *til* kraftige AI-systemer;
- Løbsk til ukontrollerbar superintelligens og irrelevans eller ophør af den menneskelige art.

Som en tidlig fiktiv skildring af AGI udtrykte det: den eneste måde at vinde på er ikke at spille.

[^1]: [EU AI-loven](https://artificialintelligenceact.eu/) er et betydningsfuldt stykke lovgivning, men ville ikke direkte forhindre et farligt AI-system i at blive udviklet eller implementeret, eller endda åbent frigivet, især i USA. Et andet betydningsfuldt stykke politik, den amerikanske præsidentielle order om AI, er blevet tilbagekaldt.

[^2]: Denne [Gallup-undersøgelse](https://news.gallup.com/poll/1597/confidence-institutions.aspx) viser et dystert fald i tillid til offentlige institutioner siden 2000 i USA. Europæiske tal er varierede og mindre ekstreme, men også på en nedadgående tendens. Mistillid betyder ikke strengt taget, at institutioner virkelig *er* dysfunktionelle, men det er en indikation såvel som en årsag.

[^3]: Og store forstyrrelser, vi nu støtter – såsom udvidelse af rettigheder til nye grupper – blev specifikt drevet af mennesker i en retning mod at gøre tingene bedre.

[^4]: Lad mig være direkte. Hvis dit job kan udføres bag en computer med relativt lidt personlig interaktion med mennesker uden for din organisation og ikke indebærer juridisk ansvar over for eksterne parter, ville det per definition være muligt (og sandsynligvis omkostningsbesparende) fuldstændigt at erstatte dig med et digitalt system. Robotik til at erstatte meget fysisk arbejde vil komme senere – men ikke så meget senere, når AGI begynder at designe robotter.

[^5]: For eksempel, hvad sker der med vores retssystem, hvis sagsanlæg er næsten gratis at anlægge? Hvad sker der, når omgåelse af sikkerhedssystemer gennem social engineering bliver billig, let og risikofri?

[^6]: [Denne artikel](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/) påstår, at 10% af alt internetindhold allerede er AI-genereret, og er Googles topresultat (for mig) til søgeforespørgslen "estimates of what fraction of new internet content is AI-generated." Er det sandt? Jeg har ingen anelse! Den citerer ingen referencer, og den blev ikke skrevet af en person. Hvilken brøkdel af nye billeder indekseret af Google, eller Tweets, eller kommentarer på Reddit, eller YouTube-videoer genereres af mennesker? Ingen ved – jeg tror ikke, det er et kendeligt tal. Og dette mindre end *to år* inde i generativ AI's ankomst.

[^7]: Også værd at tilføje er, at der er "moralsk" risiko for, at vi kunne skabe digitale væsener, der kan lide. Da vi i øjeblikket ikke har en pålidelig teori om bevidsthed, der ville tillade os at skelne fysiske systemer, der kan og ikke kan lide, kan vi ikke udelukke dette teoretisk. Desuden er AI-systemers rapporter om deres følelsesvæsen sandsynligvis upålidelige med hensyn til deres faktiske oplevelse (eller ikke-oplevelse) af følelsesvæsen.

[^8]: Tekniske løsninger inden for dette felt af AI-"alignment" er usandsynligt at være op til opgaven heller. I nuværende systemer virker de på et vist niveau, men er overfladiske og kan generelt omgås uden betydelig indsats; og som diskuteret nedenfor har vi ingen rigtig idé om, hvordan man gør dette for meget mere avancerede systemer.

[^9]: Sådanne AI-systemer kan komme med nogle indbyggede sikkerhedsforanstaltninger. Men for enhver model med noget lignende nuværende arkitektur, hvis fuld adgang til dens vægte er tilgængelig, kan sikkerhedsforanstaltninger fjernes via yderligere træning eller andre teknikker. Så det er praktisk talt garanteret, at for hvert system med gelændere vil der også være et bredt tilgængeligt system uden dem. Faktisk blev Metas Llama 3.1 405B-model åbent frigivet med sikkerhedsforanstaltninger. Men *selv før det* blev en "base"-model, uden sikkerhedsforanstaltninger, lækket.

[^10]: Kunne markedet håndtere disse risici uden regeringsinvolværing? Kort sagt, nej. Der er bestemt risici, som virksomheder er stærkt incitamenteret til at mindske. Men mange andre kan virksomheder og eksternaliserer til alle andre, og mange af ovenstående er i denne klasse: der er ingen naturlige markedsincitamenter til at forhindre masseovervågning, sandhedshenfald, magtkoncentration, arbejdskraftforstyrrelser, skadelig politisk diskurs osv. Faktisk har vi set alt dette fra nutidens tech, især sociale medier, som er gået i det væsentlige ureguleret. AI ville bare enormt forstærke mange af de samme dynamikker.

[^11]: OpenAI har sandsynligvis mere lydige modeller til intern brug. Det er usandsynligt, at OpenAI har bygget en slags "bagdør", så ChatGPT kan kontrolleres bedre af OpenAI selv, fordi dette ville være en frygtelig sikkerhedspraksis og være højt udnytbar givet AI's uigennemsigtighed og uforudsigelighed.

[^12]: Også af afgørende betydning: alignment eller andre sikkerhedsegenskaber betyder kun noget, hvis de faktisk bruges i et AI-system. Systemer, der er åbent frigivet (dvs. hvor modelvægte og arkitektur er offentligt tilgængelige), kan omdannes relativt let til systemer *uden* disse sikkerhedsforanstaltninger. Åben frigilvelse af smartere-end-menneskelige AGI-systemer ville være forbløffende hensynsløst, og det er svært at forestille sig, hvordan menneskelig kontrol eller endda relevans ville blive opretholdt i et sådant scenarie. Der ville være al motivation for eksempel at slippe kraftige selvreproducerende og selvbærende AI-agenter løs med målet at tjene penge og sende dem til en kryptovaluta-pung. Eller at vinde et valg. Eller omstyrte en regering. Kunne "god" AI hjælpe med at begrænse dette? Måske – men kun ved at delegere enorm autoritet til den, hvilket fører til kontroltab som beskrevet nedenfor.

[^13]: For bogl ængde fremstillinger af problemet se f.eks. *Superintelligence*, *The Alignment Problem* og *Human-Compatible*. For en kæmpe bunke arbejde på forskellige tekniske niveauer af dem, der har slidt i årevis med at tænke på problemet, kan du besøge [AI alignment forum](https://www.alignmentforum.org/). Her er et [nyligt take](https://alignment.anthropic.com/2025/recommended-directions/) fra Anthropics alignment-team om, hvad de betragter som uløst.

[^14]: Dette er ["rogue AI"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/)-scenariet. I princippet kunne risikoen være relativt mindre, hvis systemet stadig kan kontrolleres ved at lukke det ned; men scenariet kunne også inkludere AI-bedrageri, selv-eksfiltrering og reproduktion, aggregering af magt og andre trin, der ville gøre det svært eller umuligt at gøre det.

[^15]: Der er en meget rig litteratur om dette emne, der går tilbage til formative skrifter af [Steve Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), Nick Bostrom og Eliezer Yudkowsky. For en bogl ængde fremstilling se [Human Compatible](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) af Stuart Russell; [her](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/) er en kort og opdateret primer.

[^16]: I erkendelse af dette, i stedet for at sænke farten for at få bedre forståelse, er AGI-virksomheder kommet med en anden plan: de vil få AI til at gøre det! Mere specifikt vil de have AI *N* til at hjælpe dem med at finde ud af, hvordan man tilpasser AI *N+1*, hele vejen til superintelligens. Selvom udnyttelse af AI til at hjælpe os med at tilpasse AI lyder lovende, er der et stærkt argument for, at det simpelthen antager sin konklusion som en præmis og generelt er en utroligt risikabel tilgang. Se [her](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us) for noget diskussion. Denne "plan" er ikke én og har undergået intet i retning af den granskning, der er passende til kernestragien for, hvordan man får over-menneskelig AI til at gå godt for menneskeheden.

[^17]: Trods alt har mennesker, mangelfulde og egenrådige som vi er, udviklet etiske systemer, hvorved vi behandler i det mindste nogle andre arter på Jorden godt. (Bare tænk ikke på de fabriksfarms.)

[^18]: Der er heldigvis en udvej her: hvis deltagerne kommer til at forstå, at de er engageret i et selvmordskapløb snarere end et vindbart. Dette er, hvad der skete nær slutningen af den kolde krig, da USA og USSR kom til at indse, at på grund af atomvinter ville selv et *ubesvaret* atomangreb være katastrofalt for angriberen. Med indseenn af, at "atomkrig ikke kan vindes og må aldrig kæmpes", kom betydelige aftaler om våbenreduktion – i det væsentlige en afslutning på våbenkapløbet.

[^19]: Krig, eksplicit eller implicit.

[^20]: Eskalering, derefter krig.

[^21]: Magisk tænkning.

[^22]: Jeg har også en kvadrillion dollar bro at sælge dig.

[^23]: Sådanne agenter ville formentlig foretrække "opnåelse" med ødelæggelse som en fallback; men at sikre modeller mod både ødelæggelse *og* tyveri af kraftige nationer er svært at sige det mindste, især for private enheder.

[^24]: For et andet perspektiv på AGI's nationale sikkerhedsrisici, se [denne RAND-rapport.](https://www.rand.org/pubs/perspectives/PEA3691-4.html)

[^25]: Måske kunne vi bygge sådan en institution! Der har været forslag om et "CERN for AI" og andre lignende initiativer, hvor AGI-udvikling er under multilateral global kontrol. Men i øjeblikket eksisterer ingen sådan institution eller er i sigte.

[^26]: Og mens alignment er meget svær, er det endnu sværere at få mennesker til at opføre sig!

[^27]: Forestil dig et system, der kan tale 50 sprog, have ekspertise i alle akademiske fag, læse en hel bog på sekunder og have alt materialet øjeblikkeligt i tankerne og producere outputs med ti gange menneskelig hastighed. Faktisk behøver du ikke at forestille dig det: bare indlæs et nuværende AI-system. Disse er over-menneskelige på mange måder, og der er intet, der stopper dem fra at være endnu mere over-menneskelige i disse og mange andre.

[^28]: Dette er grunden til, at dette er blevet benævnt en teknologisk "singularitet", der låner fra fysikken idéen om, at man ikke kan lave forudsigelser forbi en singularitet. Fortalere for at læne *ind i* sådan en singularitet kan også ønske at reflektere over, at i fysik river og knuser de samme slags singulariteter dem, der går ind i dem, i stykker.

[^29]: Problemet blev omfattende skitseret i Bostroms [*Superintelligence*](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834), og intet siden da har betydeligt ændret kernebeskeden. For et mere nyligt bind, der indsamler formelle og matematiske resultater om ukontrollerbarhed, se Yampolskiys [AI: Unexplainable, Unpredictable, Uncontrollable](https://www.amazon.com/Unexplainable-Unpredictable-Uncontrollable-Artificial-Intelligence/dp/103257626X)

[^30]: Dette gør det også klart, hvorfor den nuværende strategi for AI-virksomheder (iterativt at lade AI "tilpasse" den næste mest kraftige AI) ikke kan virke. Antag at en bregne via sin frondes behag tilslutter sig en førsteårselev til at tage sig af den. Førsteårseleven skriver nogle detaljerede instruktioner for en andetårselev at følge og en note, der overbeviser dem om at gøre det. Andetårseleven gør det samme for en tredje årselev, og så videre hele vejen til en universitetsuddannet, en leder, en direktør og endelig GM CEO. Vil GM så "gøre hvad bregnen ønsker"? Ved hvert trin kan det føles som om det virker. Men når det hele sættes sammen, vil det virke næsten nøjagtigt i den grad, som CEOen, bestyrelsen og aktionærerne i GM tilfældigvis bekymrer sig om børn og bregner, og have lidt til ingenting at gøre med alle de noter og instruktionssæt.

[^31]: Karakteren er ikke så forskellig fra formelle resultater som Gödels ufuldstændighedsteorem eller Turings halting-argument, idet begrebet kontrol fundamentalt modsiger præmissen: hvordan kan du meningsfuldt kontrollere noget, du ikke kan forstå eller forudsige; men hvis du kunne forstå og forudsige superintelligens, ville du være superintelligent. Grunden til, at jeg siger "nærmer sig", er, at de formelle resultater ikke er så grundige eller kontrollerede som i det rene matematiks tilfælde, og fordi jeg gerne vil holde håbet oppe om, at nogen meget omhyggeligt konstrueret generel intelligens, der bruger totalt forskellige metoder end dem, der i øjeblikket anvendes, kunne have nogle matematisk bevislige sikkerhedsegenskaber i henhold til den slags "garanteret sikker" AI-program, der diskuteres nedenfor.

[^32]: I øjeblikket er de fleste interessenter – det vil sige næsten hele menneskeheden – sat på sidelinjen i denne diskussion. Det er dybt forkert, og hvis ikke inviteret ind, bør de mange, mange andre grupper, der vil blive påvirket af AGI-udvikling, kræve at blive lukket ind.