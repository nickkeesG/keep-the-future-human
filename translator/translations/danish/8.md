# Kapitel 8 - Sådan undgår man at bygge AGI

AGI er ikke uundgåelig – i dag står vi ved en skillevej. Dette kapitel præsenterer et forslag til, hvordan vi kunne forhindre, at det bliver bygget.

Hvis den vej, vi i øjeblikket befinder os på, fører til vores civilisations sandsynlige undergang, hvordan skifter vi så vej?

Antag at ønsket om at stoppe udviklingen af AGI og superintelligens var udbredt og stærkt,[^1] fordi det bliver almindelig forståelse, at AGI ville være magtabsorberende snarere end magtgivende, og en dyb fare for samfundet og menneskeheden. Hvordan ville vi lukke Portene?

I øjeblikket kender vi kun én måde at *skabe* kraftfuld og generel AI på, hvilket er via virkelig massive beregninger af dybe neurale netværk. Fordi disse er utrolig vanskelige og dyre ting at gøre, er der en forstand, hvori det *ikke* at gøre dem er let.[^2] Men vi har allerede set de kræfter, der driver mod AGI, og den spilteoretiske dynamik, der gør det meget vanskeligt for nogen part ensidigt at stoppe. Så det ville kræve en kombination af indgriben udefra (dvs. regeringer) for at stoppe virksomheder, og aftaler mellem regeringer for at stoppe sig selv.[^3] Hvordan kunne dette se ud?

Det er nyttigt først at skelne mellem AI-udviklinger, der må *forhindres* eller *forbydes*, og dem, der må *håndteres.* Det første ville primært være løbsk udvikling til superintelligens.[^4] For forbudt udvikling bør definitioner være så skarpe som muligt, og både verifikation og håndhævelse bør være praktiske. Det, der må *håndteres*, ville være generelle, kraftfulde AI-systemer – som vi allerede har, og som vil have mange gråzoner, nuancer og kompleksitet. For disse er stærke effektive institutioner afgørende.

Vi kan også nyttigt afgrænse spørgsmål, der må behandles på internationalt niveau (inklusive mellem geopolitiske rivaler eller modstandere)[^5] fra dem, som individuelle jurisdiktioner, lande eller samlinger af lande kan håndtere. Forbudt udvikling falder i vid udstrækning ind under kategorien "international", fordi et lokalt forbud mod udvikling af en teknologi generelt kan omgås ved at skifte lokation.[^6]

Endelig kan vi overveje værktøjer i værktøjskassen. Der er mange, inklusive tekniske værktøjer, bløde love (standarder, normer osv.), hårde love (reguleringer og krav), ansvar, markedsincitamenter og så videre. Lad os lægge særlig opmærksomhed på ét, der er særligt for AI.

## Compute-sikkerhed og -styring

Et kerneredskab i styringen af højtydende AI vil være den hardware, den kræver. Software spreder sig let, har næsten nul marginal produktionsomkostning, krydser grænser trivielt og kan øjeblikkeligt modificeres; intet af dette gælder for hardware. Men som vi har diskuteret, er enorme mængder af denne "compute" nødvendige både under træning af AI-systemer og under inferens for at opnå de mest kapable systemer. Compute kan let kvantificeres, regnskabsføres og auditeres, med relativt lidt tvetydighed, når først gode regler for at gøre dette er udviklet. Mest afgørende er store mængder beregning, ligesom beriget uran, en meget knaphed, dyr og svær-at-producere ressource. Selvom computerchips er allestedsnærværende, er den hardware, der kræves til AI, dyr og enormt vanskelig at fremstille.[^7]

Det, der gør AI-specialiserede chips langt *mere* håndterbare som en knaphedsressource end uran, er, at de kan inkludere hardware-baserede sikkerhedsmekanismer. De fleste moderne mobiltelefoner, og nogle bærbare computere, har specialiserede on-chip hardware-funktioner, der giver dem mulighed for at sikre, at de kun installerer godkendt operativsystem-software og opdateringer, at de bevarer og beskytter følsomme biometriske data på enheden, og at de kan gøres ubrugelige for alle andre end deres ejer, hvis de mistes eller stjæles. I løbet af de seneste år er sådanne hardware-sikkerhedsforanstaltninger blevet veletablerede og bredt adopteret, og generelt vist sig ret sikre.

Nøglenyskabelsen ved disse funktioner er, at de binder hardware og software sammen ved hjælp af kryptografi.[^8] Det vil sige, bare at have et bestemt stykke computerhardware betyder ikke, at en bruger kan gøre alt, hvad de vil med det ved at anvende forskellig software. Og denne binding giver også kraftig sikkerhed, fordi mange angreb ville kræve et brud på *hardware*-sikkerhed snarere end bare *software*-sikkerhed.

Flere nylige rapporter (f.eks. fra [GovAI og samarbejdspartnere](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips), og [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) har påpeget, at lignende hardware-funktioner indlejret i banebrydende AI-relevant computerhardware kunne spille en ekstremt nyttig rolle i AI-sikkerhed og -styring. De muliggør en række funktioner tilgængelige for en "guvernør"[^9], som man måske ikke ville gætte var tilgængelige eller endda mulige. Som nogle nøgleeksempler:

- *Geolokalisering*: Systemer kan sættes op, så chips har en kendt lokation, og kan handle forskelligt (eller slukkes helt) baseret på lokation.[^10]
- *Tilladte forbindelser*: hvert chip kan konfigureres med en hardware-håndhævet tilladelsesliste over bestemte andre chips, som det kan netværke med, og være ude af stand til at forbinde med chips, der ikke er på denne liste.[^11] Dette kan begrænse størrelsen af kommunikerende klynger af chips.[^12]
- *Målt inferens eller træning (og auto-afbryder)*: En guvernør kan kun licensere en vis mængde træning eller inferens (i tid, eller FLOP, eller muligvis tokens) til at blive udført af en bruger, hvorefter ny tilladelse kræves. Hvis incrementerne er små, kræves så relativt kontinuerlig re-licensering af en model. Modellen kan så "slukkes" simpelthen ved at tilbageholde dette licenssignal.[^13]
- *Hastighedsbegrænsning*: En model forhindres i at køre ved højere inferenshastighed end en vis grænse, der bestemmes af en guvernør eller på anden vis. Dette kunne implementeres via et begrænset sæt af tilladte forbindelser eller ved mere sofistikerede midler.
- *Attesteret træning*: En træningsprocedure kan give kryptografisk sikkert bevis for, at et bestemt sæt koder, data og mængde compute-brug blev anvendt i generering af modellen.

## Sådan undgår man at bygge superintelligens: globale grænser for trænings- og inferens-compute

Med disse overvejelser – især vedrørende beregning – på plads, kan vi diskutere, hvordan man lukker Portene til kunstig superintelligens; vi vil så vende os til at forhindre fuld AGI og håndtere AI-modeller, når de nærmer sig og overgår menneskelig kapacitet i forskellige aspekter.

Den første ingrediens er selvfølgelig forståelsen af, at superintelligens ikke ville være kontrollerbar, og at dens konsekvenser er fundamentalt uforudsigelige. I det mindste Kina og USA må uafhængigt beslutte, af denne eller andre årsager, ikke at bygge superintelligens.[^14] Så er der brug for en international aftale mellem dem og andre, med en stærk verifikations- og håndhævelsesmekanisme, til at forsikre alle parter om, at deres rivaler ikke defekterer og beslutter at kaste terningerne.

For at være verificerbare og håndhævelige bør grænserne være hårde grænser og så entydige som muligt. Dette virker som et praktisk talt umuligt problem: at begrænse kapaciteterne af kompleks software med uforudsigelige egenskaber, på verdensplan. Heldigvis er situationen meget bedre end dette, fordi selve det, der har gjort avanceret AI mulig – en enorm mængde compute – er meget, meget lettere at kontrollere. Selvom det måske stadig ville tillade nogle kraftfulde og farlige systemer, kan *løbsk superintelligens* sandsynligvis forhindres af et hårdt loft over mængden af beregning, der går ind i et neuralt netværk, sammen med en hastighedsgrænse på mængden af inferens, som et AI-system (af forbundne neurale netværk og anden software) kan udføre. En specifik version af dette foreslås nedenfor.

Det kan synes, som om at placere hårde globale grænser på AI-beregning ville kræve enorme niveauer af international koordination og indtrængende, privatlivs-knusende overvågning. Heldigvis ville det ikke. Den ekstremt [stramme og flaskehalspræget forsyningskæde](https://arxiv.org/abs/2402.08797) sørger for, at når først en grænse er sat lovligt (hvad enten det er ved lov eller eksekutivordre), ville verifikation af overholdelse af den grænse kun kræve involvering og samarbejde fra en håndfuld store virksomheder.[^15]

En plan som denne har en række højst ønskværdige funktioner. Den er minimalt invasiv i den forstand, at kun få store virksomheder får krav pålagt dem, og kun ret betydelige klynger af beregning ville blive styret. De relevante chips indeholder allerede de hardware-kapaciteter, der er nødvendige for en første version.[^16] Både implementering og håndhævelse er afhængige af standard juridiske restriktioner. Men disse er bakket op af brugsbetingelser for hardwaren og af hardware-kontroller, hvilket drastisk forenkler håndhævelse og forhindrer snyd fra virksomheder, private grupper eller endda lande. Der er rigeligt præcedens for, at hardware-virksomheder placerer fjernrestriktioner på deres hardware-brug og låser/oplåser bestemte kapaciteter eksternt,[^17] inklusive endda i højtydende CPU'er i datacentre.[^18] Selv for den ret lille andel af hardware og organisationer, der berøres, kunne overvågningen begrænses til telemetri, uden direkte adgang til data eller modeller selv; og softwaren til dette kunne være åben for inspektion for at vise, at ingen yderligere data bliver registreret. Skemaet er internationalt og kooperativt og ret fleksibelt og udvidbart. Fordi grænsen hovedsageligt er på hardware snarere end software, er den relativt agnostisk med hensyn til, hvordan AI-software-udvikling og -deployment forekommer, og er kompatibel med en række paradigmer inklusive mere "decentraliserede" eller "offentlige" AI-tilgange, der sigter mod at bekæmpe AI-drevet magtkoncentration.

En beregningsbaseret Port-lukning har dog også ulemper. For det første er den langt fra en fuldstændig løsning på problemet med AI-styring generelt. For det andet, i takt med at computerhardware bliver hurtigere, ville systemet "fange" mere og mere hardware i mindre og mindre klynger (eller endda individuelle GPU'er).[^19] Det er også muligt, at på grund af algoritmiske forbedringer ville en endnu lavere beregningsgrænse med tiden være nødvendig,[^20] eller at beregnings-mængde bliver stort set irrelevant, og at lukke Porten i stedet ville nødvendiggøre et mere detaljeret risiko-baseret eller kapacitets-baseret styringsregime for AI. For det tredje, uanset garantierne og det lille antal berørte enheder, er et sådant system nødt til at skabe modreaktion vedrørende privatliv og overvågning blandt andre bekymringer.[^21]

Selvfølgelig vil udvikling og implementering af et compute-begrænsende styringsregime på kort tid være ret udfordrende. Men det er absolut gennemførligt.

## A-G-I: Det triple-skæringspunkt som basis for risiko og politik

Lad os nu vende os til AGI. Hårde linjer og definitioner her er vanskeligere, fordi vi bestemt har intelligens, der er kunstig og generel, og efter ingen eksisterende definition vil alle være enige om, hvorvidt eller hvornår det eksisterer. Desuden er en compute- eller inferensgrænse et noget sløvt værktøj (compute er en proxy for kapacitet, som så er en proxy for risiko), der – medmindre den er ret lav – næppe vil forhindre AGI, der er kraftfuld nok til at forårsage social eller civilisatorisk forstyrrelse eller akutte risici.

Jeg har argumenteret for, at de mest akutte risici opstår fra det triple-skæringspunkt af meget høj kapacitet, høj autonomi og stor generalitet. Dette er de systemer, der – hvis de overhovedet udvikles – må håndteres med enorm forsigtighed. Ved at skabe strenge standarder (gennem ansvar og regulering) for systemer, der kombinerer alle tre egenskaber, kan vi kanalisere AI-udvikling mod sikrere alternativer.

Som med andre industrier og produkter, der potentielt kunne skade forbrugere eller offentligheden, kræver AI-systemer omhyggelig regulering af effektive og bemyndigede regeringsagenturer. Denne regulering bør anerkende de iboende risici ved AGI og forhindre, at uacceptabelt risikable højtydende AI-systemer udvikles.[^22]

Dog tager storstilet regulering, især med rigtige tænder, der med sikkerhed vil blive modsat af industrien,[^23] tid[^24] såvel som politisk overbevisning om, at det er nødvendigt.[^25] Givet tempoet i fremskridt kan dette tage mere tid, end vi har til rådighed.

På en meget hurtigere tidsskala og mens reguleringsforanstaltninger udvikles, kan vi give virksomheder de nødvendige incitamenter til (a) at afstå fra meget højrisiko-aktiviteter og (b) at udvikle omfattende systemer til at vurdere og afbøde risiko, ved at afklare og øge ansvarsniveauer for de farligste systemer. Ideen ville være at pålægge de allerhøjeste niveauer af ansvar – streng og i nogle tilfælde personlig kriminel – for systemer i det triple-skæringspunkt af høj autonomi-generalitet-intelligens, men at give "sikre havne" til mere typisk fejl-baseret ansvar for systemer, hvor en af disse egenskaber mangler eller er garanteret at være håndterbar. Det vil sige, for eksempel, et "svagt" system, der er generelt og autonomt (som en kapabel og troværdig men begrænset personlig assistent) ville være underlagt lavere ansvarsniveauer. Ligeledes et snævert og autonomt system som en selvkørende bil ville stadig være underlagt den betydelige regulering, det allerede er, men ikke forhøjet ansvar. På samme måde for et højt kapabelt og generelt system, der er "passivt" og stort set ude af stand til uafhængig handling. Systemer, der mangler *to* af de tre egenskaber, er endnu mere håndterbare, og sikre havne ville være endnu lettere at påberåbe sig. Denne tilgang afspejler, hvordan vi håndterer andre potentielt farlige teknologier:[^26] højere ansvar for farligere konfigurationer skaber naturlige incitamenter for sikrere alternativer.

Det standard-udfald af sådanne høje niveauer af ansvar, som virker for at *internalisere* AGI-risiko til virksomheder snarere end at aflade det til offentligheden, er sandsynligt (og forhåbentlig!) for virksomheder simpelthen ikke at udvikle fuld AGI, indtil og medmindre de genuint kan gøre den troværdig, sikker og kontrollerbar givet, at *deres egen ledelse* er de parter i risiko. (I tilfælde af at dette ikke er tilstrækkeligt, bør lovgivningen, der afklarer ansvar, også eksplicit tillade injunktiv lettelse, dvs. en dommer, der beordrer et stop, for aktiviteter, der klart er i farezonen og diskutabelt udgør en offentlig risiko.) Når regulering kommer på plads, kan efterlevelse af regulering blive den sikre havn, og de sikre havne fra lav autonomi, snæverhed eller svaghed i AI-systemer kan konvertere til relativt lettere reguleringsregimer.

## Nøglebestemmelser for en Port-lukning

Med ovenstående diskussion i tankerne giver dette afsnit forslag til nøglebestemmelser, der ville implementere og vedligeholde forbud mod fuld AGI og superintelligens, og håndtering af menneske-konkurrencedygtig eller ekspert-konkurrencedygtig generel-formål AI nær den fulde AGI-tærskel.[^27] Det har fire nøgleelementer: 1) compute-regnskab og overvågning, 2) compute-lofter i træning og drift af AI, 3) et ansvarsrammeværk, og 4) trindelte sikkerheds- og sikkerhedsstandarder defineret, der inkluderer hårde reguleringsgrænser. Disse beskrives kortfattet næste gang, med yderligere detaljer eller implementeringseksempler givet i tre ledsagende tabeller. Vigtigt, bemærk at disse er langt fra alt, hvad der vil være nødvendigt for at styre avancerede AI-systemer; mens de vil have yderligere sikkerheds- og sikkerhedsfordele, sigter de mod at lukke Porten til intelligens-løbsk og omdirigere AI-udvikling i en bedre retning.

### 1\. Compute-regnskab og gennemsigtighed

- En standardorganisation (f.eks. NIST i USA efterfulgt af ISO/IEEE internationalt) bør kodificere en detaljeret teknisk standard for den totale compute, der bruges i træning og drift af AI-modeller, i FLOP, og hastigheden i FLOP/s, hvormed de opererer. Detaljer for, hvordan dette kunne se ud, gives i Appendiks A.[^28]
- Et krav – enten ved ny lovgivning eller under eksisterende autoritet[^29] – bør pålægges af jurisdiktioner, hvor storstilet AI-træning finder sted, for at beregne og rapportere til en reguleringsmyndighed eller andet agentur den totale FLOP, der bruges i træning og drift af alle modeller over en tærskel på 10<sup>25</sup> FLOP eller 10<sup>18</sup> FLOP/s.[^30]
- Disse krav bør indfases, hvor der indledningsvis kræves veldokumenterede god-tro estimater på kvartalsbasis, med senere faser, der kræver progressivt højere standarder, op til kryptografisk attesteret total FLOP og FLOP/s knyttet til hvert model-*output*.
- Disse rapporter bør suppleres med veldokumenterede estimater af marginal energi og finansielle omkostninger brugt på at generere hvert AI-output.

Begrundelse: Disse velberegnede og gennemsigtigt rapporterede tal ville give basis for trænings- og driftslofter, såvel som en sikker havn fra højere ansvarsforanstaltninger (se Appendiks C og D).

### 2\. Trænings- og drifts-compute-lofter

- Jurisdiktioner, der hoster AI-systemer, bør pålægge en hård grænse på den totale compute, der går ind i ethvert AI-model-output, startende ved 10<sup>27</sup> FLOP[^31] og justerbar som passende.
- Jurisdiktioner, der hoster AI-systemer, bør pålægge en hård grænse på compute-raten for AI-model-outputs, startende ved 10<sup>20</sup> FLOP/s og justerbar som passende.

Begrundelse: Total beregning, mens meget ufuldkommen, er en proxy for AI-kapacitet (og risiko), der er konkret målbar og verificerbar, så den giver en hård bagstopper til at begrænse kapaciteter. Et konkret implementeringsforslag gives i Appendiks B.

### 3\. Forhøjet ansvar for farlige systemer

- Skabelse og drift[^32] af et avanceret AI-system, der er højt generelt, kapabelt og autonomt, bør juridisk afklares via lovgivning til at være underlagt streng, fælles-og-solidarisk, snarere end enkelt-parts fejl-baseret, ansvar.[^33]
- En juridisk proces bør være tilgængelig for at gøre bekræftende sikkerhedssager, som ville give sikker havn fra strengt ansvar for systemer, der er små (i form af compute), svage, snævre, passive eller har tilstrækkelige sikkerheds-, sikkerheds- og kontrollerbarhedsgarantier.
- En eksplicit vej og et sæt betingelser for injunktiv lettelse til at stoppe AI-trænings- og inferens-aktiviteter, der udgør en offentlig fare, bør skitseres.

Begrundelse: AI-systemer kan ikke holdes ansvarlige, så vi må holde menneskelige individer og organisationer ansvarlige for skade, de forårsager (ansvar).[^34] Ukontrollerbar AGI er en trussel mod samfundet og civilisationen og bør i mangel af en sikkerhedssag betragtes som unormalt farlig. At lægge ansvarsbyren på udviklere for at vise, at kraftfulde modeller er sikre nok til ikke at blive betragtet som "unormalt farlige", giver incitament til sikker udvikling, sammen med gennemsigtighed og registrering for at påberåbe sig disse sikre havne. Regulering kan så forhindre skade, hvor afskrækkelse fra ansvar er utilstrækkelig. Endelig er AI-udviklere allerede ansvarlige for skader, de forårsager, så juridisk afklaring af ansvar for de mest risikable systemer kan gøres øjeblikkeligt, uden at højt detaljerede standarder udvikles; disse kan så udvikle sig over tid. Detaljer gives i Appendiks C.

### 4\. Sikkerhedsregulering for AI

Et reguleringsregime, der adresserer storskalede akutte risici ved AI, vil kræve som minimum:

- Identifikation eller oprettelse af et passende sæt af reguleringsmyndigheder, sandsynligvis et nyt agentur;
- Et omfattende risikovurderingsrammeværk;[^35]
- Et rammeværk for bekræftende sikkerhedssager, delvist baseret på risikovurderingsrammeværket, der skal laves af udviklere, og til auditering af *uafhængige* grupper og agenturer;
- Et trindt licenssystem, med trin, der sporer niveauer af kapacitet.[^36] Licenser ville blive givet på basis af sikkerhedssager og audits, til udvikling og deployment af systemer. Krav ville spænde fra notifikation i den lave ende til kvantitative sikkerheds-, sikkerheds- og kontrollerbarhedsgarantier før udvikling i den høje ende. Disse ville forhindre frigivelse af systemer, indtil de demonstreres sikre, og forbyde udviklingen af iboende usikre systemer. Appendiks D giver et forslag til, hvad sådanne sikkerheds- og sikkerhedsstandarder kunne indebære.
- Aftaler om at bringe sådanne foranstaltninger til det internationale niveau, inklusive internationale organer til at harmonisere normer og standarder, og potentielt internationale agenturer til at gennemgå sikkerhedssager.

Begrundelse: Ultimativt er ansvar ikke den rigtige mekanisme til at forhindre storstilet risiko for offentligheden fra en ny teknologi. Omfattende regulering med bemyndigede reguleringsmyndigheder vil være nødvendig for AI ligesom for alle andre større industrier, der udgør en risiko for offentligheden.[^37]

Regulering mod at forhindre andre gennemgribende men mindre akutte risici vil sandsynligvis variere i sin form fra jurisdiktion til jurisdiktion. Det afgørende er at undgå at udvikle de AI-systemer, der er så risikable, at disse risici er uhåndterbare.

## Hvad så?

I løbet af det næste årti, i takt med at AI bliver mere gennemgribende og kerneteknologien avancerer, vil to nøgleting sandsynligvis ske. For det første vil regulering af eksisterende kraftfulde AI-systemer blive vanskeligere, men endnu mere nødvendig. Det er sandsynligt, at i det mindste nogle foranstaltninger, der adresserer storskalede sikkerhedsrisici, vil kræve aftale på internationalt niveau, med individuelle jurisdiktioner, der håndhæver regler baseret på internationale aftaler.

For det andet vil trænings- og drifts-compute-lofter blive sværere at vedligeholde, i takt med at hardware bliver billigere og mere omkostningseffektiv; de kan også blive mindre relevante (eller skal være endnu strammere) med fremskridt i algoritmer og arkitekturer.

At kontrollere AI vil blive sværere betyder ikke, at vi skal give op! Implementering af planen skitseret i dette essay ville give os både værdifuld tid og afgørende kontrol over processen, der ville sætte os i en langt, langt bedre position til at undgå den eksistentielle risiko ved AI for vores samfund, civilisation og art.

På endnu længere sigt vil der være valg at træffe med hensyn til, hvad vi tillader. Vi kan stadig vælge at skabe en eller anden form for genuint kontrollerbar AGI, i det omfang dette viser sig muligt. Eller vi kan beslutte, at at køre verden er bedre overladt til maskinerne, hvis vi kan overbevise os selv om, at de vil gøre et bedre stykke arbejde med det og behandle os godt. Men disse bør være beslutninger truffet med dyb videnskabelig forståelse af AI i hånden, og efter meningsfuld global inkluderende diskussion, ikke i et kapløb mellem tech-moguler med det meste af menneskeheden fuldstændigt uinvolveret og uvidende.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) Sammenfatning af A-G-I og superintelligens-styring via ansvar og regulering. Ansvar er højest, og regulering stærkest, ved det triple-skæringspunkt af Autonomi, Generalitet og Intelligens. Sikre havne fra strengt ansvar og stærk regulering kan opnås via bekræftende sikkerhedssager, der demonstrerer, at et system er svagt og/eller snævert og/eller passivt. Lofter over total Trænings-Compute og Inferens-Compute-hastighed, verificeret og håndhævet juridisk og ved hjælp af hardware- og kryptografiske sikkerhedsforanstaltninger, understøtter sikkerhed ved at undgå fuld AGI og effektivt forbyde superintelligens.

[^1]: Mest sandsynligt vil spredningen af denne erkendelse tage enten intensiv indsats fra uddannelses- og fortalergrupper, der gør dette argument, eller en ret betydelig AI-forårsaget katastrofe. Vi kan håbe, det vil være det første.

[^2]: Paradoksalt nok er vi vant til, at Naturen begrænser vores teknologi ved at gøre den meget svær at udvikle, især videnskabeligt. Men det er ikke længere tilfældet for AI: de vigtigste videnskabelige problemer viser sig at være lettere end forventet. Vi kan ikke regne med, at Naturen redder os fra os selv her – det må vi gøre selv.

[^3]: Hvor præcist stopper vi med at udvikle nye systemer? Her bør vi adoptere et forsigtighedsprincip. Når først et system er deployeret, og især når det niveau af systemkapacitet prolifererer, er det ekstremt vanskeligt at rulle tilbage. Og hvis et system er *udviklet* (især til store omkostninger og indsats), vil der være enormt pres for at bruge eller deployere det, og fristelse til, at det lækker eller stjæles. At udvikle systemer og *så* beslutte om de er dybt usikre er en farlig vej.

[^4]: Det ville også være klogt at forbyde AI-udvikling, der er iboende farlig, såsom selv-replikerende og evolverende systemer, dem designet til at undslippe indeslutning, dem der kan autonomt selv-forbedre, bevidst vildledende og ondsindede AI, osv.

[^5]: Bemærk dette betyder ikke nødvendigvis *håndhævet* på internationalt niveau af en slags global organisation: i stedet kunne suveræne nationer håndhæve aftalte regler, som i mange traktater.

[^6]: Som vi vil se nedenfor, ville naturen af AI-beregning tillade noget af en hybrid; men internationalt samarbejde vil stadig være nødvendigt.

[^7]: For eksempel de maskiner, der kræves for at ætse AI-relevante chips, laves kun af ét firma, ASML (på trods af mange andre forsøg på at gøre det), langt størstedelen af relevante chips fremstilles af ét firma, TSMC (på trods af andre forsøg på at konkurrere), og design og konstruktion af hardware fra disse chips gøres af kun få inklusive NVIDIA, AMD og Google.

[^8]: Vigtigst holder hvert chip en unik og utilgængelig kryptografisk privat nøgle, det kan bruge til at "underskrive" ting.

[^9]: Som standard ville dette være virksomheden, der sælger chips, men andre modeller er mulige og potentielt nyttige.

[^10]: En guvernør kan fastslå et chips lokation ved at time udveksling af underskrevne beskeder med det: lysets endelige hastighed kræver, at chippet er inden for en given radius *r* af en "station", hvis det kan returnere en underskrevet besked på en tid mindre end *r* / *c*, hvor *c* er lysets hastighed. Ved at bruge flere stationer og noget forståelse af netværkskarakteristika kan chippets lokation bestemmes. Skønheden ved denne metode er, at det meste af dens sikkerhed leveres af fysikkens love. Andre metoder kunne bruge GPS, inertiafølgning og lignende teknologier.

[^11]: Alternativt kunne par af chips kun tillades at kommunikere med hinanden via eksplicit tilladelse fra en guvernør.

[^12]: Dette er afgørende, fordi i det mindste i øjeblikket er meget høj båndbreddeforbindelse mellem chips nødvendig for at træne store AI-modeller på dem.

[^13]: Dette kunne også sættes op til at kræve underskrevne beskeder fra *N* af *M* forskellige guvernører, hvilket tillader flere parter at dele styring.

[^14]: Dette er langt fra uden fortilfælde – for eksempel har militærer ikke udviklet hære af klonede eller genetisk konstruerede supersoldater, selvom dette sandsynligvis er teknologisk muligt. Men de har *valgt* ikke at gøre dette, snarere end at blive forhindret af andre. Track record er ikke god for store verdensmagter, der forhindres i at udvikle en teknologi, de stærkt ønsker at udvikle.

[^15]: Med et par bemærkelsesværdige undtagelser (især NVIDIA) er den AI-specialiserede hardware en relativt lille del af disse virksomheders overordnede forretning og indtægtsmodel. Desuden er gabet mellem hardware brugt i avanceret AI og "forbrugerkvalitets"-hardware betydelig, så de fleste forbrugere af computerhardware ville være stort set upåvirkede.

[^16]: For mere detaljeret analyse, se de nylige rapporter fra [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) og [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). Disse fokuserer på teknisk gennemførlighed, især i konteksten af amerikanske eksportkontroller, der søger at begrænse andre landes kapacitet i high-end beregning; men dette har åbenlyst overlap med den globale begrænsning, der forestilles her.

[^17]: Apple-enheder, for eksempel, fjernlåses sikkert, når de rapporteres tabt eller stjålet, og kan genaktiveres fjernt. Dette er afhængigt af de samme hardware-sikkerhedsfunktioner diskuteret her.

[^18]: Se f.eks. IBM's [capacity on demand](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) tilbud, Intel's [Intel on demand.](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html), og Apple's [private cloud compute](https://security.apple.com/blog/private-cloud-compute/).

[^19]: [Dette studie](https://epochai.org/trends#hardware-trends-section) viser, at historisk er den samme ydeevne opnået ved at bruge omkring 30% færre dollars per år. Hvis denne trend fortsætter, kan der være betydelig overlap mellem AI og "forbruger"-chipbrug, og generelt kunne mængden af nødvendig hardware til højtydende AI-systemer blive ubehageligt lille.

[^20]: Per det [samme studie](https://epochai.org/trends#hardware-trends-section) har given ydeevne på billedgenkendelse krævet 2,5x mindre beregning hvert år. Hvis dette også skulle gælde for de mest kapable AI-systemer også, ville en beregningsgrænse ikke være en nyttig en ret længe.

[^21]: Især på landeniveau ligner dette meget en nationalisering af beregning, idet regeringen ville have meget kontrol over, hvordan beregningskraft bliver brugt. Dog for dem, der bekymrer sig om regeringsinvolvering, synes dette langt sikrere end og at foretrække for den mest kraftfulde AI-software *selv* at blive nationaliseret via en fusion mellem store AI-virksomheder og nationale regeringer, som nogle begynder at advokere for.

[^22]: Et stort reguleringstrin i Europa blev taget med 2024-vedtagelsen af [EU AI Act.](https://artificialintelligenceact.eu/) Den klassificerer AI efter risiko: forbyder uacceptable systemer, regulerer højrisikonerne og pålægger gennemsigtighedsregler eller slet ingen foranstaltninger på lavrisikosystemer. Den vil betydeligt reducere nogle AI-risici og booste AI-gennemsigtighed selv for amerikanske firmaer, men har to nøglefejl. For det første begrænset rækkevidde: selvom den gælder for enhver virksomhed, der leverer AI i EU, er håndhævelse over amerikanske-baserede firmaer svag, og militær-AI er undtaget. For det andet, selvom den dækker GPAI, undlader den at anerkende AGI eller superintelligens som uacceptable risici eller forhindre deres udvikling—kun deres EU-deployment. Som resultat gør den lidt for at dæmme op for risiciene ved AGI eller superintelligens.

[^23]: Virksomheder repræsenterer ofte, at de er for fornuftig regulering. Men på en eller anden måde synes de næsten altid at modsætte sig enhver *bestemt* regulering; se kampen om den ret lav-berørende SB1047, som [de fleste AI-virksomheder offentligt eller privat modsatte sig.](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/)

[^24]: Det var omkring 3½ år fra det tidspunkt, EU AI-loven blev foreslået, til den trådte i kraft.

[^25]: Det udtrykkes sommetider, at det er "for tidligt" at begynde at regulere AI. Givet den sidste note synes det næppe sandsynligt. En anden udtrykt bekymring er, at regulering ville "skade innovation." Men god regulering ændrer bare retningen, ikke mængden, af innovation.

[^26]: Et interessant præcedens er i transporten af farlige materialer, som kunne undslippe og forårsage skade. Her har [regulering](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) og [retspraksis](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) etableret strengt ansvar for meget farlige materialer som sprængstoffer, benzin, gifte, infektiøse agenter og radioaktivt affald. Andre eksempler inkluderer [advarsler på lægemidler](https://www.medicalnewstoday.com/articles/boxed-warnings), [klasser af medicinske enheder,](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification) osv.

[^27]: Et andet omfattende forslag med lignende mål fremsat i ["A Narrow Path"](https://www.narrowpath.co/) advokerer for en mere centraliseret, forbud-baseret tilgang, der kanaliserer al frontier AI-udvikling gennem en enkelt international enhed, overvåget af stærke internationale institutioner, med klare kategoriske forbud snarere end graduerede restriktioner. Jeg ville også støtte den plan; dog vil den tage endnu mere politisk vilje og koordination end den foreslået her.

[^28]: Nogle retningslinjer for en sådan standard blev [udgivet](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/) af Frontier Model Forum. Relativt til forslaget her fejler de på siden af mindre præcision og mindre compute inkluderet i optællingen.

[^29]: Den 2023 amerikanske AI-eksekutivordre (nu ophævet) krævede lignende men mindre finmasket rapportering. Dette bør styrkes af en erstattende ordre.

[^30]: Meget groft, for nu-almindelige H100-chips svarer dette til klynger på omkring 1000, der laver inferens; det er omkring 100 (omkring 5 millioner USD værd) af de allernyeste top-of-the-line NVIDIA B200-chips, der laver inferens. I begge tilfælde svarer træningsnummeret til den klynge, der beregner i flere måneder.

[^31]: Denne mængde er større end ethvert i øjeblikket trænet AI-system; et større eller mindre tal kunne være begrundet, da vi bedre forstår, hvordan AI-kapacitet skalerer med compute.

[^32]: Dette gælder dem, der skaber og leverer/hoster modellerne, ikke slutbrugere.

[^33]: Nogenlunde betyder "streng" ansvar, at udviklere holdes ansvarlige for skader forårsaget af et produkt *som standard* og er en standard brugt for "unormalt farlige" produkter, og (noget morsomt men passende) vilde dyr. "Fælles og solidarisk" ansvar betyder, at ansvar tildeles alle parter ansvarlige for et produkt, og disse parter må sortere ud mellem sig, hvem der bærer hvilket ansvar. Dette er vigtigt for systemer som AI med en lang og kompleks værdikæde.

[^34]: Standard fejl-baseret enkelt-parts ansvar er ikke nok: fejl vil både være vanskelig at spore og tildele, fordi AI-systemer er komplekse, deres operation ikke forstås, og mange parter kan være involveret i skabelsen af et farligt system eller output. Derudover vil retssager tage år at afgøre og sandsynligvis resultere blot i bøder, der er ubetydelige for disse virksomheder, så personligt ansvar for ledere er vigtigt også.

[^35]: Der bør ikke være undtagelse fra sikkerhedskriterier for åben-vægt-modeller. Desuden bør det i vurderingen af risiko antages, at guardrails, der kan fjernes, vil blive fjernet fra bredt tilgængelige modeller, og at selv lukkede modeller vil proliferere, medmindre der er en meget høj sikkerhed for, at de vil forblive sikre.

[^36]: Det foreslåede skema her har reguleringsundersøgelse udløst på generel kapacitet; dog giver det mening for nogle særligt risikable brugssager at udløse mere undersøgelse – for eksempel et ekspert-virologi AI-system, selv hvis snævert og passivt, bør sandsynligvis gå i et højere trin. Den tidligere amerikanske eksekutivordre havde noget af denne struktur for biologiske kapaciteter.

[^37]: To klare eksempler er luftfart og medicin, reguleret af FAA og FDA, og lignende agenturer i andre lande. Disse agenturer er ufuldkomne, men har været absolut vitale for funktionen og succesen af disse industrier.