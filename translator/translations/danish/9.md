# Kapitel 9 - At designe fremtiden — hvad vi bør gøre i stedet

AI kan gøre utroligt meget godt i verden. For at få alle fordelene uden risiciene skal vi sikre, at AI forbliver et menneskeligt værktøj.

Hvis vi med succes vælger ikke at erstatte menneskeheden med maskiner – i hvert fald i et stykke tid! – hvad kan vi så gøre i stedet? Giver vi afkald på AI's enorme potentiale som teknologi? På et vist niveau er svaret et simpelt *nej:* luk Portene for ukontrollerbar AGI og superintelligens, men byg *gerne* mange andre former for AI samt de styringssystemer og institutioner, vi får brug for til at håndtere dem.

Men der er stadig meget at sige; at få dette til at virke ville være en central opgave for menneskeheden. Dette afsnit undersøger flere nøgletemaer:

- Hvordan vi kan karakterisere "Værktøjs"-AI og de former, den kan antage.
- At vi kan få (næsten) alt, hvad menneskeheden ønsker, uden AGI, med Værktøjs-AI.
- At Værktøjs-AI-systemer er (sandsynligvis, i princippet) håndterbare.
- At det at vende ryggen til AGI ikke betyder at gå på kompromis med national sikkerhed – tværtimod.
- At magtkoncentration er en reel bekymring. Kan vi begrænse den uden at underminere sikkerhed?
- At vi vil ønske – og få brug for – nye styrings- og sociale strukturer, og AI kan faktisk hjælpe.

## AI inden for Portene: Værktøjs-AI

Trippelkrydsets diagram giver en god måde at afgrænse det, vi kan kalde "Værktøjs-AI": AI der er et kontrollerbart værktøj til menneskelig brug, snarere end en ukontrollerbar rival eller erstatning. De mindst problematiske AI-systemer er dem, der er autonome, men ikke generelle eller særligt kapable (som en auktionsbot), eller generelle, men ikke autonome eller kapable (som en lille sprogmodel), eller kapable, men snævre og meget kontrollerbare (som AlphaGo).[^1] Dem med to overlappende egenskaber har bredere anvendelse, men højere risiko og vil kræve store indsatser at håndtere. (At et AI-system mere er et værktøj betyder ikke, at det er iboende sikkert, blot at det ikke er iboende *usikkert* – tænk på en kædesav versus en tamkat som kæledyr.) Porten skal forblive lukket for (fuld) AGI og superintelligens ved trippelkrydset, og der skal udvises enorm forsigtighed med AI-systemer, der nærmer sig denne tærskel.

Men det efterlader en masse kraftfuld AI! Vi kan få enormt udbytte af smarte og generelle passive "orakler" og snævre systemer, generelle systemer på menneskeligt, men ikke overmenskeligt niveau, og så videre. Mange tech-virksomheder og udviklere bygger aktivt sådanne værktøjer og bør fortsætte; ligesom de fleste mennesker antager de implicit, at Portene til AGI og superintelligens vil blive lukket.[^2]

Desuden kan AI-systemer effektivt kombineres i sammensatte systemer, der bevarer menneskeligt tilsyn mens de øger kapaciteten. I stedet for at stole på uigennemskuelige sorte kasser kan vi bygge systemer, hvor flere komponenter – både AI og traditionel software – arbejder sammen på måder, som mennesker kan overvåge og forstå.[^3] Mens nogle komponenter måske er sorte kasser, ville ingen være tæt på AGI – kun det sammensatte system som helhed ville være både meget generelt og meget kapabelt, og det på en strengt kontrollerbar måde.[^4]

### Meningsfuld og garanteret menneskelig kontrol

Hvad betyder "strengt kontrollerbar"? En nøgleidé i "Værktøjs"-frameworket er at tillade systemer – selv om de er ret generelle og kraftfulde – der er garanteret at være under meningsfuld menneskelig kontrol. Hvad betyder det? Det indebærer to aspekter. For det første er der en designovervejelse: mennesker bør være dybt og centralt involveret i, hvad systemet gør, *uden* at uddelegere vigtige beslutninger til AI'en. Det er karakteristisk for de fleste nuværende AI-systemer. For det andet skal AI-systemer, i det omfang de er autonome, have garantier, der begrænser deres handlingsområde. En garanti bør være et *tal*, der karakteriserer sandsynligheden for, at noget sker, og en grund til at tro på det tal. Det er, hvad vi kræver på andre sikkerhedskritiske områder, hvor tal som "gennemsnitlig tid mellem fejl" og forventet antal ulykker beregnes, understøttes og offentliggøres i sikkerhedsargumenter.[^5] Det ideelle tal for fejl er selvfølgelig nul. Og den gode nyhed er, at vi måske kan komme ret tæt på det, omend med helt andre AI-arkitekturer, ved at bruge idéer om *formelt verificerede* egenskaber ved programmer (herunder AI). Idéen, udfoldet i længden af Omohundro, Tegmark, Bengio, Dalrymple og andre (se [her](https://arxiv.org/abs/2309.01933) og [her](https://arxiv.org/abs/2405.06624)) er at konstruere et program med bestemte egenskaber (for eksempel: at et menneske kan slukke det) og formelt *bevise*, at disse egenskaber gælder. Det kan gøres nu for ret korte programmer og simple egenskaber, men den (kommende) kraft af AI-drevet bevissoftware kunne tillade det for meget mere komplekse programmer (f.eks. wrappere) og endda AI selv. Dette er et meget ambitiøst program, men i takt med at presset på Portene vokser, får vi brug for nogle kraftfulde materialer til at forstærke dem. Matematisk bevis kan være et af de få, der er stærkt nok.

### Hvorhen AI-industrien

Med AI-fremskridt omdirigeret ville Værktøjs-AI stadig være en enorm industri. Med hensyn til hardware vil træning og inferens i mindre modeller, selv med compute-begrænsninger for at forhindre superintelligens, stadig kræve enorme mængder specialiserede komponenter. På softwaresiden bør defusering af eksplosionen i AI-model- og beregningsstørrelse blot føre til, at virksomheder omdirigerer ressourcer mod at gøre de mindre systemer bedre, mere forskelligartede og mere specialiserede, snarere end blot at gøre dem større.[^6] Der ville være masser af plads – mere sandsynligt – til alle de pengeskabende Silicon Valley-startups.[^7]

## Værktøjs-AI kan give (næsten) alt, hvad menneskeheden ønsker, uden AGI

Intelligens, hvad enten den er biologisk eller maskinel, kan bredt betragtes som evnen til at planlægge og udføre aktiviteter, der skaber fremtider mere i overensstemmelse med et sæt mål. Som sådan er intelligens af enorm fordel, når den bruges i forfølgelsen af klogt valgte mål. Kunstig intelligens tiltrækker enorme investeringer af tid og indsats, hovedsageligt på grund af dens lovende fordele. Så vi bør spørge: i hvilket omfang ville vi stadig høste fordelene ved AI, hvis vi begrænser dens løbske udvikling til superintelligens? Svaret: vi mister måske overraskende lidt.

Overvej først, at nuværende AI-systemer allerede er meget kraftfulde, og vi har kun ridset i overfladen af, hvad der kan gøres med dem.[^8] De er rimeligt kapable til at "styre showet" med hensyn til at "forstå" et spørgsmål eller en opgave præsenteret for dem, og hvad det ville kræve at besvare dette spørgsmål eller udføre den opgave.

Dernæst skyldes meget af begejstringen for moderne AI-systemer deres generalitet; men nogle af de mest kapable AI-systemer – som dem der genererer eller genkender tale eller billeder, laver videnskabelig forudsigelse og modellering, spiller spil osv. – er meget snævrere og godt "inden for Portene" med hensyn til beregning.[^9] Disse systemer er overmenneskelige til de specifikke opgaver, de udfører. De kan have kanttilfælde[^10] (eller [udnyttelige](https://arxiv.org/abs/2211.00241)) svagheder på grund af deres snæverhed; men *totalt* snævre eller *fuldt* generelle er ikke de eneste tilgængelige muligheder: der er mange arkitekturer imellem.[^11]

Disse AI-værktøjer kan betydeligt fremskynde fremskridt inden for andre positive teknologier uden AGI. For at blive bedre til kernefysik behøver vi ikke AI til at være kernefysiker – dem har vi! Hvis vi vil accelerere medicin, så giv biologer, medicinske forskere og kemikere kraftfulde værktøjer. De ønsker dem og vil bruge dem til enorm gevinst. Vi behøver ikke en serverfarm fuld af en million digitale genier; vi har millioner af mennesker, hvis geni AI kan hjælpe med at bringe frem. Ja, det vil tage længere tid at opnå udødelighed og kuren mod alle sygdomme. Det er en reel omkostning. Men selv de mest lovende sundhedsinnovationer ville være til ringe nytte, hvis AI-dreven ustabilitet fører til global konflikt eller samfundsmæssigt sammenbrud. Vi skylder os selv at give AI-styrkede mennesker en chance for at tackle problemet først.

Og antag, at der faktisk er en eller anden enorm fordel ved AGI, som ikke kan opnås af menneskeheden ved brug af værktøjer inden for Portene. Mister vi dem ved *aldrig* at bygge AGI og superintelligens? Når vi vejer risici og belønninger her, er der en enorm asymmetrisk fordel ved at vente kontra skynde sig: vi kan vente, indtil det kan gøres på en garanteret sikker og gavnlig måde, og næsten alle vil stadig få glæde af belønningerne; hvis vi skynder os, kunne det være – med OpenAI-direktør Sam Altmans ord – [sluk for lyset for *os alle*.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

Men hvis ikke-AGI-værktøjer er potentielt så kraftfulde, kan vi så håndtere dem? Svaret er et klart... måske.

## Værktøjs-AI-systemer er (sandsynligvis, i princippet) håndterbare

Men det bliver ikke let. Nuværende avancerede AI-systemer kan i høj grad styrke mennesker og institutioner i at opnå deres mål. Det er generelt en god ting! Der er dog naturlige dynamikker ved at have sådanne systemer til rådighed – pludselig og uden megen tid for samfundet til at tilpasse sig – som byder på alvorlige risici, der skal håndteres. Det er værd at diskutere nogle få store klasser af sådanne risici og hvordan de kan begrænses, forudsat en lukning af Portene.

En klasse af risici er, at kraftfuld Værktøjs-AI tillader adgang til viden eller kapacitet, som tidligere var bundet til en person eller organisation, hvilket gør en kombination af høj kapacitet plus høj loyalitet tilgængelig for et meget bredt spektrum af aktører. I dag kunne en person med onde hensigter med nok penge hyre et hold kemikere til at designe og producere nye kemiske våben – men det er ikke så let at have de penge eller at finde/samle holdet og overbevise dem om at gøre noget, der er ret klart ulovligt, uetisk og farligt. For at forhindre AI-systemer i at spille en sådan rolle kan forbedringer af nuværende metoder meget vel være tilstrækkelige,[^12] så længe alle disse systemer og adgangen til dem håndteres ansvarligt. På den anden side, hvis kraftfulde systemer frigives til generel brug og modifikation, kan eventuelle indbyggede sikkerhedsforanstaltninger sandsynligvis fjernes. Så for at undgå risici i denne klasse vil der være behov for stærke restriktioner på, hvad der kan frigives offentligt – analog med restriktioner på detaljer om nukleare, eksplosive og andre farlige teknologier.[^13]

En anden klasse af risici stammer fra opskalering af maskiner, der opfører sig som eller efterligner mennesker. På niveau med skade på individuelle personer omfatter disse risici meget mere effektive svindelnumre, spam og phishing samt udbredelse af ikke-samtykkebaserede deepfakes.[^14] På kollektivt niveau omfatter de forstyrrelse af centrale sociale processer som offentlig diskussion og debat, vores samfundsmæssige informations- og videnindsamlings-, behandlings- og formidlingssystemer og vores politiske valgssystemer. Begrænsning af denne risiko vil sandsynligvis involvere (a) love, der begrænser efterligning af mennesker af AI-systemer og gør AI-udviklere ansvarlige for at skabe systemer, der genererer sådanne efterligninger, (b) vandmærke- og herkomst-systemer, der identificerer og klassificerer (ansvarligt) genereret AI-indhold, og (c) nye socio-tekniske epistemiske systemer, der kan skabe en betroet kæde fra data (f.eks. kameraer og optagelser) gennem fakta, forståelse og gode verdensmodeller.[^15] Alt dette er muligt, og AI kan hjælpe med nogle dele af det.

En tredje generel risiko er, at i det omfang nogle opgaver automatiseres, kan de mennesker, der i øjeblikket udfører disse opgaver, have mindre økonomisk værdi som arbejdskraft. Historisk set har automatisering af opgaver gjort ting muliggjort af disse opgaver billigere og mere righoldige, mens det sorterer de mennesker, der tidligere udførte disse opgaver, i dem, der stadig er involveret i den automatiserede version (generelt ved højere færdigheds-/lønniveau), og dem, hvis arbejdskraft er mindre værd eller intet værd. Samlet set er det vanskeligt at forudsige, i hvilke sektorer der vil være behov for mere versus mindre menneskelig arbejdskraft i den resulterende større, men mere effektive sektor. Parallelt hermed har automatiseringsdynamikken tendens til at øge ulighed og generel produktivitet, mindske omkostningerne til visse varer og tjenester (via effektivitetsstigninger) og øge omkostningerne til andre (via [omkostningssygdom](https://en.wikipedia.org/wiki/Baumol_effect)). For dem på den disfavoriserede side af ulighedsstigningen er det dybt uklart, om omkostningsfaldet for visse varer og tjenester opvejer stigningen i andre og fører til generelt større velbefindende. Så hvordan vil det gå med AI? På grund af den relative lethed, hvormed menneskelig intellektuel arbejdskraft kan erstattes af generel AI, kan vi forvente en hurtig version af dette med menneske-konkurrencedygtig generel AI.[^16] Hvis vi lukker Porten til AGI, vil mange færre job blive totalt erstattet af AI-agenter; men enorm fortrængning af arbejdskraft er stadig sandsynlig over en periode på år.[^17] For at undgå udbredt økonomisk lidelse vil det sandsynligvis være nødvendigt at implementere både en form for universelle grundaktiver eller -indkomst og også tilrettelægge et kulturelt skift mod at værdsætte og belønne menneskecentreret arbejde, der er sværere at automatisere (snarere end at se arbejdspriser falde på grund af stigningen i tilgængelig arbejdskraft skubbet ud af andre dele af økonomien). Andre konstruktioner, som ["datadignitet"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society) (hvor de menneskelige producenter af træningsdata automatisk tildeles royalties for den værdi, data skaber i AI), kan hjælpe. Automatisering af AI har også en anden potentielt negativ effekt, som er *uhensigtsmæssig* automatisering. Sammen med anvendelser, hvor AI simpelthen gør et dårligere arbejde, ville dette omfatte dem, hvor AI-systemer sandsynligvis vil krænke moralske, etiske eller juridiske principper – for eksempel i livs- og dødsbeslutninger og i juridiske anliggender. Disse skal behandles ved at anvende og udvide vores nuværende juridiske rammer.

Endelig er en væsentlig trussel fra AI inden for portene dens brug i personaliseret overtalelse, opmærksomhedsfangst og manipulation. Vi har set væksten af en dybt forankret opmærksomhedsøkonomi på sociale medier og andre online platforme (hvor online-tjenester kæmper hårdt om brugerens opmærksomhed) og ["overvågningskapitalisme"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism)-systemer (hvor brugerinformation og profilering tilføjes til kommodificeringen af opmærksomhed). Det er så godt som sikkert, at mere AI vil blive sat i tjeneste for begge dele. AI bruges allerede i høj grad i vanedannende feed-algoritmer, men dette vil udvikle sig til vanedannende AI-genereret indhold, skræddersyet til at blive kompulsivt konsumeret af en enkelt person. Og den persons input, reaktioner og data vil blive fodret ind i opmærksomheds-/reklamemaskinen for at fortsætte den onde cirkel. Desuden, i takt med at AI-hjælpere leveret af tech-virksomheder bliver grænsefladen for mere online-liv, vil de sandsynligvis erstatte søgemaskiner og feeds som mekanismen, hvorigennem overtalelse og monetarisering af kunder sker. Vores samfunds hidtidige fiasko i at kontrollere denne dynamik lover ikke godt. Noget af denne dynamik kan begrænses via regulering vedrørende privatliv, datarettigheder og manipulation. At komme mere til problemets rod kan kræve andre perspektiver, såsom loyale AI-assistenter (diskuteret nedenfor).

Konklusionen på denne diskussion er håb: værktøjsbaserede systemer inden for Portene – i hvert fald så længe de forbliver sammenlignelige i kraft og kapacitet med nutidens mest avancerede systemer – er sandsynligvis håndterbare, hvis der er vilje og koordination til at gøre det. Anstændige menneskelige institutioner, styrket af AI-værktøjer,[^18] kan gøre det. Vi kunne også fejle i at gøre det. Men det er svært at se, hvordan det at tillade mere kraftfulde systemer ville hjælpe – bortset fra at sætte dem til at styre og håbe på det bedste.

## National sikkerhed

Kapløb om AI-overlegenhed – drevet af national sikkerhed eller andre motivationer – driver os mod ukontrollerede kraftfulde AI-systemer, som har tendens til at absorbere snarere end at give magt. Et AGI-kapløb mellem USA og Kina er et kapløb om at afgøre, hvilken nation der får superintelligens først.

Så hvad bør dem, der har ansvaret for national sikkerhed, gøre i stedet? Regeringer har stærk erfaring med at bygge kontrollerbare og sikre systemer, og de bør fordoble indsatsen med at gøre det inden for AI, støtte den slags infrastrukturprojekter, der lykkes bedst, når de gøres i stor målestok og med regeringens godkendelse.

I stedet for et hensynsløst "Manhattan-projekt" mod AGI[^19] kunne den amerikanske regering lancere et Apollo-projekt for kontrollerbare, sikre, troværdige systemer. Dette kunne omfatte for eksempel:

- Et større program til (a) at udvikle on-chip hardware-sikkerhedsmekanismer og (b) infrastrukturen til at håndtere compute-siden af kraftfuld AI. Disse kunne bygge på det amerikanske [CHIPS-lovgivning](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) og [eksportkontrolsystem](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion).
- Et storstillet initiativ til at udvikle formel verificeringsteknikker, så bestemte egenskaber ved AI-systemer (som en afbryder) kan *bevises* at være til stede eller fraværende. Dette kan udnytte AI selv til at udvikle beviser for egenskaber.
- En national indsats for at skabe software, der kan verificeres som sikker, drevet af AI-værktøjer, der kan omkode eksisterende software til verificerbart sikre rammer.
- Et nationalt investeringsprojekt i videnskabelig fremgang ved brug af AI,[^20] der kører som et partnerskab mellem DOE, NSF og NIH.

Generelt er der en enorm angrebsflade på vores samfund, der gør os sårbare over for risici fra AI og dets misbrug. Beskyttelse mod nogle af disse risici vil kræve regeringsstørrelse investering og standardisering. Disse ville give langt mere sikkerhed end at hælde benzin på ilden af kapløb mod AGI. Og hvis AI skal bygges ind i våbensystemer og kommando-og-kontrol-systemer, er det afgørende, at AI'en er troværdig og sikker, hvilket nuværende AI simpelthen ikke er.

## Magtkoncentration og dens begrænsninger

Dette essay har fokuseret på idéen om menneskelig kontrol af AI og dens potentielle fiasko. Men en anden gyldig linse, hvorigennem AI-situationen kan betragtes, er gennem *magtkoncentration*. Udviklingen af meget kraftfuld AI truer med at koncentrere magt enten i de meget få og meget store virksomhedshænder, der har udviklet og vil kontrollere den, eller i regeringer, der bruger AI som et nyt middel til at opretholde deres egen magt og kontrol, eller i AI-systemerne selv. Eller en eller anden ugudelig blanding af ovenstående. I alle disse tilfælde mister størstedelen af menneskeheden magt, kontrol og handlekraft. Hvordan kan vi bekæmpe dette?

Det allerførste og vigtigste skridt er selvfølgelig en lukning af Portene til klogere-end-menneske AGI og superintelligens. Disse kan eksplicit direkte erstatte mennesker og grupper af mennesker. Hvis de er under virksomheds- eller regeringskontrol, vil de koncentrere magt i disse virksomheder eller regeringer; hvis de er "frie", vil de koncentrere magt i sig selv. Så lad os antage, at Portene er lukkede. Hvad så?

En foreslået løsning på magtkoncentration er "open source" AI, hvor modelvægte er frit eller bredt tilgængelige. Men som nævnt tidligere kan de fleste sikkerhedsforanstaltninger eller sikkerhedsmekanismer (og gøres generelt) fjernes, når en model er åben. Så der er en akut spænding mellem på den ene side decentralisering og på den anden side sikkerhed og menneskelig kontrol af AI-systemer. Der er også grunde til at være skeptiske over for, at åbne modeller i sig selv vil bekæmpe magtkoncentration i AI mere meningsfuldt, end de har gjort i operativsystemer (stadig domineret af Microsoft, Apple og Google trods åbne alternativer).[^21]

Der kan dog være måder at kvadrere denne cirkel på – at centralisere og begrænse risici samtidig med at decentralisere kapacitet og økonomisk belønning. Dette kræver at gentænke både hvordan AI udvikles, og hvordan dens fordele fordeles.

Nye modeller for offentlig AI-udvikling og -ejerskab ville hjælpe. Dette kunne antage flere former: regeringsudviklet AI (underlagt demokratisk tilsyn),[^22] nonprofitorganisationer til AI-udvikling (som Mozilla for browsere) eller strukturer, der muliggør meget udbredt ejerskab og styring. Det er centralt, at disse institutioner eksplicit ville have til opgave at tjene den offentlige interesse, mens de opererer under stærke sikkerhedsbegrænsninger.[^23] Veldesignede regulerings- og standarder/certificeringsregimer vil også være vitale, så AI-produkter tilbudt af et levende marked forbliver genuint nyttige snarere end udnyttende over for deres brugere.

Med hensyn til økonomisk magtkoncentration kan vi bruge herkomstsporing og "datadignitet" til at sikre, at økonomiske fordele strømmer bredere. Især stammer det meste AI-magt nu (og i fremtiden, hvis vi holder Portene lukkede) fra menneske-genererede data, hvad enten det er direkte træningsdata eller menneskelig feedback. Hvis AI-virksomheder blev påkrævet at kompensere dataleverandører retfærdigt,[^24] kunne dette i det mindste hjælpe med at fordele de økonomiske belønninger bredere. Udover dette kunne en anden model være offentligt ejerskab af betydelige dele af store AI-virksomheder. For eksempel kunne regeringer, der er i stand til at beskatte AI-virksomheder, investere en del af indtægterne i en statslig formuefond, der besidder aktier i virksomhederne og udbetaler udbytte til befolkningen.[^25]

Afgørende i disse mekanismer er at bruge AI's egen kraft til at hjælpe med at fordele magt bedre, snarere end blot at bekæmpe AI-drevet magtkoncentration ved brug af ikke-AI-midler. En kraftfuld tilgang ville være gennem veldesignede AI-assistenter, der opererer med ægte tillidsforpligtelse over for deres brugere – der sætter brugernes interesser først, især over virksomhedsleverandørernes.[^26] Disse assistenter skal være virkelig troværdige, teknisk kompetente, men passende begrænsede baseret på use case og risikoniveau, og bredt tilgængelige for alle gennem offentlige, nonprofit- eller certificerede for-profit-kanaler. Ligesom vi aldrig ville acceptere en menneskelig assistent, der i hemmelighed arbejder imod vores interesser for en anden part, bør vi ikke acceptere AI-assistenter, der overvåger, manipulerer eller udvinder værdi fra deres brugere til virksomhedsfordel.

En sådan transformation ville fundamentalt ændre den nuværende dynamik, hvor individer står alene og skal forhandle med enorme (AI-drevne) virksomheds- og bureaukratiske maskiner, der prioriterer værdiudvinding frem for menneskelig velfærd. Mens der er mange mulige tilgange til at omfordele AI-dreven magt bredere, vil ingen opstå af sig selv: de skal bevidst konstrueres og styres med mekanismer som tillidsforpligtelser, offentlig levering og lagdelt adgang baseret på risiko.

Tilgange til at begrænse magtkoncentration kan møde betydelige modvind fra etablerede magter.[^27] Men der er veje til AI-udvikling, der ikke kræver at vælge mellem sikkerhed og koncentreret magt. Ved at bygge de rigtige institutioner nu kunne vi sikre, at AI's fordele deles bredt, mens dens risici håndteres omhyggeligt.

## Nye styrings- og sociale strukturer

Vores nuværende styringsstrukturer kæmper: de er langsomme til at reagere, ofte fanget af særlige interesser og [i stigende grad ikke betroede af offentligheden.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) Dette er dog ikke en grund til at opgive dem – tværtimod. Nogle institutioner kan have behov for udskiftning, men mere bredt har vi brug for nye mekanismer, der kan styrke og supplere vores eksisterende strukturer og hjælpe dem med at fungere bedre i vores hurtigt udviklende verden.

Meget af vores institutionelle svaghed stammer ikke fra formelle regeringsstrukturer, men fra forringede sociale institutioner: vores systemer til at udvikle fælles forståelse, koordinere handling og føre meningsfuld diskurs. Indtil videre har AI accelereret denne forringelse, oversvømmet vores informationskanaler med genereret indhold, peget os mod det mest polariserende og splittende indhold og gjort det sværere at skelne sandhed fra fiktion.

Men AI kunne faktisk hjælpe med at genopbygge og styrke disse sociale institutioner. Overvej tre afgørende områder:

For det første kunne AI hjælpe med at genoprette tillid til vores epistemiske systemer – vores måder at vide, hvad der er sandt på. Vi kunne udvikle AI-drevne systemer, der sporer og verificerer informations herkomst, fra rå data gennem analyse til konklusioner. Disse systemer kunne kombinere kryptografisk verifikation med sofistikeret analyse for at hjælpe mennesker med at forstå ikke blot, om noget er sandt, men hvordan vi ved, at det er sandt.[^28] Loyale AI-assistenter kunne få til opgave at følge detaljerne for at sikre, at de stemmer.

For det andet kunne AI muliggøre nye former for storskala koordination. Mange af vores mest presserende problemer – fra klimaforandringer til antibiotikaresistens – er fundamentalt koordinationsproblemer. Vi [sidder fast i situationer, der er værre, end de kunne være for næsten alle](https://equilibriabook.com/), fordi ingen enkelt eller gruppe har råd til at tage det første skridt. AI-systemer kunne hjælpe ved at modellere komplekse incitamentsstrukturer, identificere levedygtige veje til bedre resultater og facilitere de tillidsopbygnings- og forpligtelsesmekanismer, der er nødvendige for at komme derhen.

Måske mest fascinerende kunne AI muliggøre helt nye former for social diskurs. Forestil dig at kunne "tale med en by"[^29] – ikke blot se statistikker, men have en meningsfuld dialog med et AI-system, der behandler og syntetiserer millioner af beboeres synspunkter, oplevelser, behov og aspirationer. Eller overvej, hvordan AI kunne facilitere ægte dialog mellem grupper, der i øjeblikket taler forbi hinanden, ved at hjælpe hver side med bedre at forstå den andens faktiske bekymringer og værdier snarere end deres karikaturer af hinanden.[^30] Eller AI kunne tilbyde dygtig, troværdigt neutral mægling af tvister mellem mennesker eller endda store grupper mennesker (som alle kunne interagere med det direkte og individuelt!) Nuværende AI er totalt i stand til at gøre dette arbejde, men værktøjerne til at gøre det vil ikke opstå af sig selv eller via markedsincitamenter.

Disse muligheder kan lyde utopiske, især i betragtning af AI's nuværende rolle i at forringe diskurs og tillid. Men det er præcis derfor, vi aktivt skal udvikle disse positive anvendelser. Ved at lukke Portene til ukontrollerbar AGI og prioritere AI, der styrker menneskelig handlekraft, kan vi styre teknologisk fremskridt mod en fremtid, hvor AI tjener som en kraft for styrkelse, modstandsdygtighed og kollektive fremskridt.


[^1]: Når det er sagt, er det desværre ikke så let, som man kunne ønske, at holde sig væk fra trippelkrydset. At presse kapaciteten meget hårdt i et af de tre aspekter har tendens til at øge den i de andre. Især kan det være svært at skabe en ekstremt generel og kapabel intelligens, der ikke let kan gøres autonom. En tilgang er at træne modeller som ["nærsynede"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) systemer med begrænsede planlægningsevner. En anden ville være at fokusere på at konstruere rene ["orakel"](https://arxiv.org/abs/1711.05541)-systemer, der ville sky væk fra at svare på handlingsorienterede spørgsmål.

[^2]: Mange virksomheder formår ikke at indse, at de også til sidst ville blive fortrængt af AGI, selv om det tager længere tid – hvis de gjorde, ville de måske skubbe lidt mindre på disse Porte!

[^3]: AI-systemer kunne kommunikere på mere effektive, men mindre forståelige måder, men at opretholde menneskelig forståelse bør have prioritet.

[^4]: Denne idé om modulær, fortolkelig AI er blevet udviklet i detaljer af flere forskere; se f.eks. ["Comprehensive AI Services"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf)-modellen af Drexler, ["Open Agency Architecture"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai) af Dalrymple og andre. Mens sådanne systemer måske kræver mere ingeniørarbejde end monolitiske neurale netværk trænet med massiv beregning, er det præcis der, compute-begrænsninger hjælper – ved at gøre den sikrere, mere transparente vej også den mere praktiske.

[^5]: Om sikkerhedsargumenter generelt se [denne håndbog](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). Vedrørende AI specifikt, se [Wasil et al.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [Clymer et al.](https://arxiv.org/abs/2403.10462), [Buhl et al.](https://arxiv.org/abs/2410.21572), og [Balesni et al.](https://arxiv.org/abs/2411.03336)

[^6]: Vi ser faktisk allerede denne tendens drevet blot af den høje omkostning ved inferens: mindre og mere specialiserede modeller "destilleret" fra større og i stand til at køre på mindre dyr hardware.

[^7]: Jeg forstår, hvorfor dem, der er begejstrede for AI-tech-økosystemet, kan modsætte sig det, de ser som byrdefulde regulering af deres industri. Men det er ærligt talt forvirrende for mig, hvorfor for eksempel en venturekapitalist ville ønske at tillade løbsk udvikling til AGI og superintelligens. Disse systemer (og virksomheder, så længe de forbliver under virksomhedskontrol) vil *spise alle startups som et mellemmåltid*. Sandsynligvis endda *hurtigere* end at spise andre industrier. Alle, der er investeret i et blomstrende AI-økosystem, bør prioritere at sikre, at AGI-udvikling ikke fører til monopolisering af få dominerende aktører.

[^8]: Som økonomen og tidligere Deepmind-forsker Michael Webb [udtrykte det](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/), "Jeg tror, at hvis vi stoppede al udvikling af større sprogmodeller i dag, så GPT-4 og Claude og hvad som helst er de sidste ting, vi træner af den størrelse – så vi tillader meget mere iteration på ting af den størrelse og alle slags fine-tuning, men intet større end det, ingen større fremskridt – bare det, vi har i dag, tror jeg er nok til at drive 20 eller 30 års utrolig økonomisk vækst."

[^9]: For eksempel brugte DeepMinds alphafold-system kun 1/100.000 af GPT-4's FLOP-tal.

[^10]: Vanskeligheden ved selvkørende biler er vigtig at bemærke her: mens det nominelt er en snæver opgave og opnåelig med fair pålidelighed med relativt små AI-systemer, er omfattende virkelig viden og forståelse nødvendig for at få pålidelighed til det niveau, der er nødvendigt i en sådan sikkerhedskritisk opgave.

[^11]: For eksempel, givet et beregningsbudget, ville vi sandsynligvis se GPAI-modeller fortrænet til (lad os sige) halvdelen af det budget, og den anden halvdel brugt til at træne op meget høj kapacitet i et mere snævert opgaveområde. Dette ville give overmenneskelig snæver kapacitet understøttet af næsten-menneskelig generel intelligens.

[^12]: Den nuværende dominerende alignment-teknik er "forstærkende læring ved menneskelig feedback" [(RLHF)](https://arxiv.org/abs/1706.03741) og bruger menneskelig feedback til at skabe et belønnings-/strafsignal for forstærkende læring af AI-modellen. Dette og relaterede teknikker som [konstitutionel AI](https://arxiv.org/abs/2212.08073) fungerer overraskende godt (selvom de mangler robusthed og kan omgås med beskedent besvær). Derudover er nuværende sprogmodeller generelt kompetente nok til sund fornuft-ræsonnement til, at de ikke vil lave tåbelige moralske fejl. Dette er noget af et sweet spot: smarte nok til at forstå, hvad folk ønsker (i det omfang det kan defineres), men ikke smarte nok til at planlægge omfattende bedrageri eller forårsage enorm skade, når de gør fejl.

[^13]: I det lange løb vil ethvert niveau af AI-kapacitet, der udvikles, sandsynligvis sprede sig, da det i sidste ende er software og nyttigt. Vi skal have robuste mekanismer til at forsvare os mod de risici, sådanne systemer udgør. Men vi *har ikke det nu*, så vi skal være meget afmålte i, hvor meget kraftfulde AI-modeller får lov til at sprede sig.

[^14]: Langt størstedelen af disse er ikke-samtykkebaserede pornografiske deepfakes, herunder af mindreårige.

[^15]: Mange ingredienser til sådanne løsninger eksisterer i form af "bot-eller-ej"-love (i EU AI-loven blandt andre steder), [industriens herkomstssporingsteknologier](https://c2pa.org/), [innovative nyhedsaggregatorer](https://www.improvethenews.org/), forudsigelse[saggregatorer](https://metaculus.com/) og markeder osv.

[^16]: Automatiseringsbølgen følger måske ikke tidligere mønstre, idet relativt *høj*-færdighedsopgaver som kvalitetsskrivning, fortolkning af lovgivning eller medicinsk rådgivning kan være lige så meget eller endnu mere sårbare over for automatisering end opgaver med lavere færdigheder.

[^17]: For omhyggelig modellering af AGI's effekt på lønninger, se rapporten [her](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek), og blodige detaljer [her](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0), fra Anton Korinek og samarbejdspartnere. De finder, at i takt med at flere dele af job automatiseres, stiger produktivitet og lønninger – til et punkt. Når *for* meget bliver automatiseret, fortsætter produktiviteten med at stige, men lønningerne styrtdykker, fordi mennesker erstattes totalt af effektiv AI. Dette er grunden til, at lukning af Portene er så nyttig: vi får produktiviteten uden de forsvundne menneskelige lønninger.

[^18]: Der er mange måder, AI kan bruges som og til at hjælpe med at bygge "defensive" teknologier for at gøre beskyttelse og håndtering mere robust. Se [dette](https://vitalik.eth.limo/general/2025/01/05/dacc2.html) indflydelsesrige indlæg, der beskriver denne "D/acc"-dagsorden.

[^19]: Noget ironisk ville et amerikansk Manhattan-projekt sandsynligvis gøre lidt for at fremskynde timelines mod AGI – skiven for menneskelig og finansiel investering i AI-fremskridt er allerede fastgjort på 11. De primære resultater ville være at inspirere et lignende projekt i Kina (som udmærker sig i nationale infrastrukturprojekter), at gøre internationale aftaler, der begrænser AI's risiko, meget sværere, og at alarmere andre geopolitiske modstandere af USA såsom Rusland.

[^20]: ["National AI Research Resource"](https://nairrpilot.org/)-programmet er et godt nuværende skridt i denne retning og bør udvides.

[^21]: Se [denne analyse](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807) af de forskellige betydninger og implikationer af "åben" i tech-produkter, og hvordan nogle har ført til mere snarere end mindre forankring af dominans.

[^22]: Planer i USA for en [National AI Research Resource](https://nairratdoe.ornl.gov/) og den nylige lancering af en [European AI Foundation](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/) er interessante skridt i denne retning.

[^23]: Udfordringen her er ikke teknisk, men institutionel – vi har akut brug for eksempler og eksperimenter fra den virkelige verden på, hvordan AI-udvikling i offentlighedens interesse kunne se ud.

[^24]: Dette går imod nuværende big tech-forretningsmodeller og ville kræve både juridisk handling og nye normer.

[^25]: Kun nogle regeringer vil være i stand til at gøre det. En mere radikal idé er [en universel fond af denne type under fælles ejerskab af alle mennesker.](https://futureoflife.org/project/the-windfall-trust/)

[^26]: For en længere fremstilling af dette argument se [dette papir](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338) om AI-loyalitet. Desværre er standardbanen for AI-assistenter sandsynligvis en, hvor de bliver i stigende grad illoyale.

[^27]: Noget ironisk er mange etablerede magter også i risiko for AI-støttet magtberøvelse; men det kan være vanskeligt for dem at opfatte dette, medmindre processen kommer ret langt.

[^28]: Nogle interessante indsatser i denne retning repræsenteres af [c2pa-koalitionen](https://c2pa.org/) om kryptografisk verifikation; [Verity](https://www.improvethenews.org/) og [Ground news](https://ground.news/) om bedre nyhedsepistemer; og [Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com) og forudsigelsesmarkeder om at forankre diskurs i falsificerbare forudsigelser.

[^29]: Se [dette](https://talktothecity.org/) fascinerende pilotprojekt.

[^30]: Se [Kialo](https://www.kialo-edu.com/), og indsatser fra [Collective Intelligence Project](https://www.cip.org/) for nogle eksempler.