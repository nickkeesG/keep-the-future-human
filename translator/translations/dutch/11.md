# Bijlagen

Aanvullende informatie, inclusief - Technische details betreffende rekenkrachtboekhouding, een voorbeeldimplementatie van een 'poortensluiting', details voor een strikt AGI-aansprakelijkheidsregime, en een gelaagde benadering van AGI-veiligheids- en beveiligingsstandaarden.

## Bijlage A: Technische details rekenkrachtboekhouding

Een gedetailleerde methode voor zowel "ground truth" als goede benaderingen van de totale rekenkracht die wordt gebruikt bij training en inferentie is vereist voor zinvolle rekenkracht-gebaseerde controles. Hier is een voorbeeld van hoe de "ground truth" op technisch niveau zou kunnen worden bijgehouden.

**Definities:**

*Compute causale grafiek:* Voor een gegeven output O van een AI-model is er een set digitale berekeningen waarvoor het wijzigen van het resultaat van die berekening mogelijk O zou kunnen veranderen. (Dit moet conservatief worden aangenomen, d.w.z. er moet een duidelijke reden zijn om te geloven dat een berekening onafhankelijk is van een voorloper die zowel eerder in de tijd plaatsvindt als een fysiek potentieel causaal effect-pad heeft.) Dit omvat berekeningen die door het AI-model worden uitgevoerd tijdens inferentie, evenals berekeningen die zijn gebruikt voor input, datavoorbereiding, en training van het model. Omdat elk van deze zelf output kan zijn van een AI-model, wordt dit recursief berekend, afgekapt waar een mens een significante wijziging aan de input heeft aangebracht.

*Training Rekenkracht:* De totale rekenkracht, in FLOP of andere eenheden, die wordt vereist door de compute causale grafiek van een neuraal netwerk (inclusief datavoorbereiding, training, en fine-tuning, en alle andere berekeningen.)

*Output Rekenkracht:* De totale rekenkracht in de compute causale grafiek van een gegeven AI-output, inclusief alle neurale netwerken (en inclusief hun Training Rekenkracht) en andere berekeningen die in die output gaan.

*Inferentie Rekenkrachtsnelheid:* In een reeks outputs, de veranderingssnelheid (in FLOP/s of andere eenheden) van Output Rekenkracht tussen outputs, d.w.z. de rekenkracht die wordt gebruikt om de volgende output te produceren, gedeeld door het tijdsinterval tussen de outputs.

**Voorbeelden en benaderingen:**

- Voor een enkel neuraal netwerk getraind op door mensen gecreëerde data, is de Training Rekenkracht simpelweg de totale training rekenkracht zoals gebruikelijk gerapporteerd.
- Voor zo'n neuraal netwerk dat inferentie doet met een stabiele snelheid, is de Inferentie Rekenkrachtsnelheid ongeveer de totale snelheid van het rekencluster dat de inferentie uitvoert in FLOP/s.
- Voor model fine-tuning wordt de Training Rekenkracht van het complete model gegeven door de Training Rekenkracht van het niet-fine-getunte model plus de berekening die wordt gedaan tijdens fine-tuning en om data voor te bereiden die wordt gebruikt in fine-tuning.
- Voor een gedistilleerd model omvat de Training Rekenkracht van het complete model training van zowel het gedistilleerde model als het grotere model dat wordt gebruikt om synthetische data of andere training input te verstrekken.
- Als verschillende modellen worden getraind, maar veel "pogingen" worden weggegooid op basis van menselijke beoordeling, tellen deze niet mee voor de Training of Output Rekenkracht van het behouden model.

## Bijlage B: Voorbeeldimplementatie van een poortensluiting

**Implementatievoorbeeld:** Hier is een voorbeeld van hoe een poortensluiting zou kunnen werken, gegeven een limiet van 10<sup>27</sup> FLOP voor training en 10<sup>20</sup> FLOP/s voor inferentie (het draaien van de AI):

**1. Pauze:** Om redenen van nationale veiligheid vraagt de Amerikaanse uitvoerende macht alle bedrijven die gevestigd zijn in de VS, zaken doen in de VS, of chips gebruiken die zijn geproduceerd in de VS, om te stoppen met nieuwe AI-training runs die de 10<sup>27</sup> FLOP Training Rekenkracht limiet zouden kunnen overschrijden. De VS zou discussies moeten beginnen met andere landen die AI-ontwikkeling hosten, hen sterk aanmoedigen om vergelijkbare stappen te nemen en aangeven dat de Amerikaanse pauze kan worden opgeheven als zij ervoor kiezen niet mee te werken.

**2. Amerikaanse toezicht en licentieverlening:** Door presidentieel decreet of actie van een bestaande regulatoire instantie, vereist de VS dat binnen (bijvoorbeeld) één jaar:

- Alle AI-training runs geschat boven 10<sup>25</sup> FLOP uitgevoerd door bedrijven die opereren in de VS worden geregistreerd in een database onderhouden door een Amerikaanse regulatoire instantie. (Opmerking: Een iets zwakkere versie hiervan was al opgenomen in het nu ingetrokken Amerikaanse presidentiële decreet van 2023 betreffende AI, dat registratie vereiste voor modellen boven 10<sup>26</sup> FLOP.)
- Alle AI-relevante hardwarefabrikanten die opereren in de VS of zaken doen met de Amerikaanse regering zich houden aan een set vereisten voor hun gespecialiseerde hardware en de software die deze aanstuurt. (Veel van deze vereisten zouden kunnen worden ingebouwd in software- en firmware-updates voor bestaande hardware, maar langetermijn en robuuste oplossingen zouden wijzigingen vereisen aan latere generaties hardware.) Onder deze is een vereiste dat als de hardware onderdeel is van een hoogsnelheid-verbonden cluster die 10<sup>18</sup> FLOP/s berekening kan uitvoeren, een hoger niveau van verificatie vereist is, wat regelmatige toestemming omvat door een externe "gouverneur" die zowel telemetrie ontvangt als verzoeken om aanvullende berekening uit te voeren.
- De bewaarder rapporteert de totale berekening uitgevoerd op zijn hardware aan de instantie die de Amerikaanse database onderhoudt.
- Sterkere vereisten worden gefaseerd ingevoerd om zowel veiliger als flexibeler toezicht en toestemmingsverlening mogelijk te maken.

**3. Internationaal toezicht:**

- De VS, China, en andere landen die geavanceerde chipproductiecapaciteit hosten onderhandelen over een internationale overeenkomst.
- Deze overeenkomst creëert een nieuwe internationale instantie, analoog aan het Internationaal Atoomenergieagentschap, belast met het toezicht op AI-training en -uitvoering.
- Ondertekenende landen moeten hun binnenlandse AI-hardwarefabrikanten verplichten zich te houden aan een set vereisten die ten minste zo streng zijn als die opgelegd in de VS.
- Bewaarders zijn nu verplicht AI-berekeningscijfers te rapporteren aan zowel instanties in hun thuislanden als een nieuw kantoor binnen de internationale instantie.
- Aanvullende landen worden sterk aangemoedigd zich aan te sluiten bij de bestaande internationale overeenkomst: exportcontroles door ondertekenende landen beperken toegang tot high-end hardware door niet-ondertekenende landen terwijl ondertekenende landen technische ondersteuning kunnen ontvangen bij het beheren van hun AI-systemen.

**4. Internationale verificatie en handhaving:**

- Het hardwareverificatiesysteem wordt bijgewerkt zodat het rekenkrachtgebruik rapporteert aan zowel de oorspronkelijke bewaarder als ook direct aan het internationale instantiekantoor.
- De instantie, via discussie met de ondertekenende partijen van de internationale overeenkomst, komt overeen over rekenkrachtbeperkingen die vervolgens juridische kracht krijgen in de ondertekenende landen.
- Parallel kan een set internationale standaarden worden ontwikkeld zodat training en het draaien van AI's boven een drempel van rekenkracht (maar onder de limiet) vereist zijn om zich aan die standaarden te houden.
- De instantie kan, indien nodig om te compenseren voor betere algoritmes etc., de rekenkrachtlimiet verlagen. Of, als het veilig en raadzaam wordt geacht (op bijvoorbeeld het niveau van bewijsbare veiligheidsgaranties), de rekenkrachtlimiet verhogen.

## Bijlage C: Details voor een strikt AGI-aansprakelijkheidsregime

**Details voor een strikt AGI-aansprakelijkheidsregime**

- Het creëren en opereren van een geavanceerd AI-systeem dat zeer algemeen, capabel en autonoom is, wordt beschouwd als een "abnormaal gevaarlijke" activiteit.
- Als zodanig is de standaardaansprakelijkheid voor training en het opereren van dergelijke systemen strikte, gezamenlijke en hoofdelijke aansprakelijkheid (of het niet-Amerikaanse equivalent) voor alle schade aangericht door het model of zijn outputs/acties.
- Persoonlijke aansprakelijkheid wordt opgelegd voor bestuurders en bestuursleden in gevallen van grove nalatigheid of opzettelijk wangedrag. Dit zou strafrechtelijke sancties moeten omvatten voor de meest ernstige gevallen.
- Er zijn talrijke veilige havens waaronder aansprakelijkheid terugkeert naar de standaard (op schuld gebaseerde, in de VS) aansprakelijkheid waaraan mensen en bedrijven normaal gesproken onderworpen zouden zijn.
	- Modellen getraind en geopereerd onder een bepaalde rekenkrachtdrempel (die ten minste 10x lager zou zijn dan de limieten hierboven beschreven.)
	- AI die "zwak" is (ruwweg, onder menselijk expertniveau bij de taken waarvoor het is bedoeld) en/of
	- AI die "smal" is (met een vaste en behoorlijk beperkte reikwijdte van taken en operaties waarvoor het specifiek is ontworpen en getraind) en/of
	- AI die "passief" is (zeer beperkt in zijn vermogen - zelfs onder bescheiden modificatie - om acties te ondernemen of complexe meerstapstaken uit te voeren zonder directe menselijke betrokkenheid en controle.)
	- Een AI die gegarandeerd veilig, beveiligd en controleerbaar is (bewijsbaar veilig, of een risicoanalyse geeft een verwaarloosbaar niveau van verwachte schade aan.)
- Veilige havens kunnen worden geclaimd op basis van een [veiligheidscase](https://arxiv.org/abs/2410.21572) voorbereid door de AI-ontwikkelaar en goedgekeurd door een instantie of auditor die gecertificeerd is door een instantie. Om een veilige haven te claimen gebaseerd op rekenkracht, moet de ontwikkelaar alleen geloofwaardige schattingen leveren van totale Training Rekenkracht en maximale Inferentiesnelheid
- Wetgeving zou expliciet situaties schetsen waarin gerechtelijk bevel tot staking van de ontwikkeling van AI-systemen met een hoog risico op publieke schade gepast zou zijn.
- Bedrijfsconsortia, in samenwerking met NGO's en overheidsinstanties, zouden standaarden en normen moeten ontwikkelen die deze termen definiëren, hoe regelgevers veilige havens zouden moeten verlenen, hoe AI-ontwikkelaars veiligheidscases zouden moeten ontwikkelen, en hoe rechtbanken aansprakelijkheid zouden moeten interpreteren waar veilige havens niet proactief worden geclaimd.

## Bijlage D: Een gelaagde benadering van AGI-veiligheids- en beveiligingsstandaarden

**Een gelaagde benadering van AGI-veiligheids- en beveiligingsstandaarden**

| Risicolaag | Trigger(s) | Vereisten voor training | Vereiste voor implementatie |
| --- | --- | --- | --- |
| RT-0 | AI zwak in autonomie, algemeenheid en intelligentie | geen | geen |
| RT-1 | AI sterk in één van autonomie, algemeenheid en intelligentie | geen | Gebaseerd op risico en gebruik, mogelijk veiligheidscases goedgekeurd door nationale autoriteiten waar het model kan worden gebruikt |
| RT-2 | AI sterk in twee van autonomie, algemeenheid en intelligentie | Registratie bij nationale autoriteit met jurisdictie over de ontwikkelaar | Veiligheidscase die risico van grote schade begrenst onder geautoriseerde niveaus plus onafhankelijke veiligheidsaudits (inclusief black-box en white-box redteaming) goedgekeurd door nationale autoriteiten waar het model kan worden gebruikt |
| RT-3 | AGI sterk in autonomie, algemeenheid en intelligentie | Voorafgaande goedkeuring van veiligheids- en beveiligingsplan door nationale autoriteit met jurisdictie over de ontwikkelaar | Veiligheidscase die begrensde risico van grote schade garandeert onder geautoriseerde niveaus evenals vereiste specificaties, inclusief cyberbeveiliging, controleerbaarheid, een niet-verwijderbare noodstop, alignment met menselijke waarden, en robuustheid tegen kwaadaardige gebruik. |
| RT-4 | Elk model dat ook 10<sup>27</sup> FLOP Training of 10<sup>20</sup> FLOP/s Inferentie overschrijdt | Verboden in afwachting van internationaal overeengekomen opheffing van rekenkrachtlimiet | Verboden in afwachting van internationaal overeengekomen opheffing van rekenkrachtlimiet |

Risicoclassificaties en veiligheids-/beveiligingsstandaarden, met lagen gebaseerd op rekenkrachtdrempels evenals combinaties van hoge autonomie, algemeenheid en intelligentie:

- *Sterke autonomie* is van toepassing als het systeem in staat is om meerstapstaken uit te voeren, of gemakkelijk kan worden gemaakt om die uit te voeren, en/of complexe acties kan ondernemen die relevant zijn voor de echte wereld, zonder significante menselijke toezicht of interventie. Voorbeelden: autonome voertuigen en robots; financiële handelsbots. Tegenvoorbeelden: GPT-4; beeldclassificeerders
- *Sterke algemeenheid* geeft een brede toepassingsreikwijdte aan, prestatie van taken waarvoor het model niet opzettelijk en specifiek werd getraind, en significant vermogen om nieuwe taken te leren. Voorbeelden: GPT-4; mu-zero. Tegenvoorbeelden: AlphaFold; autonome voertuigen; beeldgeneratoren
- *Sterke intelligentie* komt overeen met het evenaren van menselijke expertniveau-prestatie bij de taken waarop het model het beste presteert (en voor een algemeen model, over een breed scala van taken.) Voorbeelden: AlphaFold; mu-zero; o3. Tegenvoorbeelden: GPT-4; Siri