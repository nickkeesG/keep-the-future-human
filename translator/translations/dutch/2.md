# Hoofdstuk 2 - Basiskennis over AI-neurale netwerken

Hoe werken moderne AI-systemen, en wat kunnen we verwachten van de volgende generatie AI?

Om te begrijpen hoe de gevolgen van het ontwikkelen van krachtigere AI zich zullen ontvouwen, is het essentieel om enkele basisbeginselen te internaliseren. Dit hoofdstuk en de volgende twee secties behandelen deze aspecten: wat moderne AI is, hoe deze gebruikmaakt van massale berekeningen, en op welke manieren deze snel groeit in algemene toepasbaarheid en capaciteit.[^1]

Er zijn vele manieren om kunstmatige intelligentie te definiëren, maar voor onze doeleinden is de kerneigenschap van AI dat een AI-systeem, in tegenstelling tot een standaard computerprogramma dat een lijst instructies is voor het uitvoeren van een taak, leert van data of ervaring om taken uit te voeren *zonder expliciet te worden verteld hoe dit te doen.*

Vrijwel alle relevante moderne AI is gebaseerd op neurale netwerken. Dit zijn wiskundige/computationele structuren, weergegeven door een zeer grote (miljarden of biljoenen) verzameling getallen ("gewichten"), die een trainingstaak goed uitvoeren. Deze gewichten worden gecreëerd (of misschien "gekweekt" of "gevonden") door ze iteratief aan te passen zodat het neurale netwerk een numerieke score (ook wel "verlies" genoemd) verbetert die is gedefinieerd om goed te presteren bij een of meer taken.[^2] Dit proces staat bekend als het *trainen* van het neurale netwerk.[^3]

Er zijn veel technieken om deze training uit te voeren, maar die details zijn veel minder relevant dan de manieren waarop de scoring wordt gedefinieerd, en hoe deze resulteren in verschillende taken waarin het neurale netwerk goed presteert. Een belangrijk onderscheid wordt historisch gemaakt tussen "smalle" en "algemene" AI.

Smalle AI wordt bewust getraind om een bepaalde taak of kleine set taken uit te voeren (zoals het herkennen van afbeeldingen of het spelen van schaak); het vereist hertraining voor nieuwe taken en heeft een beperkte reikwijdte van capaciteiten. We hebben bovenmenselijke smalle AI, wat betekent dat we voor vrijwel elke discrete, goed gedefinieerde taak die een persoon kan uitvoeren, waarschijnlijk een score kunnen construeren en vervolgens succesvol een smal AI-systeem kunnen trainen om dit beter te doen dan een mens zou kunnen.

Algemene AI-systemen (GPAI) kunnen een breed scala aan taken uitvoeren, inclusief vele waarvoor ze niet expliciet zijn getraind; ze kunnen ook nieuwe taken leren als onderdeel van hun werking. Huidige grote "multimodale modellen"[^4] zoals ChatGPT exemplificeren dit: getraind op een zeer grote verzameling tekst en afbeeldingen, kunnen ze complexe redeneringen voeren, code schrijven, afbeeldingen analyseren en assisteren bij een enorme reeks intellectuele taken. Hoewel nog steeds behoorlijk verschillend van menselijke intelligentie op manieren die we hieronder diepgaand zullen zien, heeft hun algemeenheid een revolutie in AI veroorzaakt.[^5]

## Onvoorspelbaarheid: een kerneigenschap van AI-systemen

Een belangrijk verschil tussen AI-systemen en conventionele software ligt in voorspelbaarheid. De uitvoer van standaardsoftware kan onvoorspelbaar zijn – sterker nog, soms is dat waarom we software schrijven, om ons resultaten te geven die we niet hadden kunnen voorspellen. Maar conventionele software doet zelden iets waar het niet voor geprogrammeerd was – zijn reikwijdte en gedrag zijn over het algemeen zoals ontworpen. Een topklasse schaakprogramma kan zetten doen die geen mens zou kunnen voorspellen (anders zouden ze dat schaakprogramma kunnen verslaan!), maar het zal over het algemeen niets anders doen dan schaak spelen.

Net als conventionele software heeft smalle AI voorspelbare reikwijdte en gedrag, maar kan onvoorspelbare resultaten hebben. Dit is eigenlijk gewoon een andere manier om smalle AI te definiëren: als AI die vergelijkbaar is met conventionele software in zijn voorspelbaarheid en werkingsgebied.

Algemene AI is anders: zijn reikwijdte (de domeinen waarop het van toepassing is), gedrag (het soort dingen dat het doet) en resultaten (zijn werkelijke uitvoer) kunnen allemaal onvoorspelbaar zijn.[^6] GPT-4 werd alleen getraind om tekst accuraat te genereren, maar ontwikkelde vele capaciteiten die zijn trainers niet voorspelden of bedoelden. Deze onvoorspelbaarheid komt voort uit de complexiteit van training: omdat de trainingsdata uitvoer van veel verschillende taken bevat, moet de AI effectief leren om deze taken uit te voeren om goed te kunnen voorspellen.

Deze onvoorspelbaarheid van algemene AI-systemen is vrij fundamenteel. Hoewel het in principe mogelijk is om zorgvuldig AI-systemen te construeren die gegarandeerde limieten op hun gedrag hebben (zoals later in het essay wordt genoemd), zijn de AI-systemen zoals ze nu worden gecreëerd onvoorspelbaar in de praktijk en zelfs in principe.

## Passieve AI, agenten, autonome systemen en alignment

Deze onvoorspelbaarheid wordt bijzonder belangrijk wanneer we overwegen hoe AI-systemen daadwerkelijk worden ingezet en gebruikt om verschillende doelen te bereiken.

Veel AI-systemen zijn relatief passief in de zin dat ze voornamelijk informatie verstrekken, en de gebruiker onderneemt acties. Andere, gewoonlijk *agenten* genoemd, ondernemen zelf acties, met verschillende niveaus van betrokkenheid van een gebruiker. Degenen die acties ondernemen met relatief minder externe input of toezicht kunnen meer *autonoom* worden genoemd. Dit vormt een spectrum in termen van onafhankelijkheid van actie, van passieve tools tot autonome agenten.[^7]

Wat betreft doelen van AI-systemen, deze kunnen direct gekoppeld zijn aan hun trainingsdoelstelling (bijv. het doel van "winnen" voor een Go-spelend systeem is ook expliciet waarvoor het werd getraind). Of ze kunnen dat niet zijn: ChatGPT's trainingsdoelstelling is deels om tekst te voorspellen, deels om een behulpzame assistent te zijn. Maar bij het uitvoeren van een bepaalde taak wordt zijn doel door de gebruiker aan hem geleverd. Doelen kunnen ook door een AI-systeem zelf worden gecreëerd, alleen zeer indirect gerelateerd aan zijn trainingsdoelstelling.[^8]

Doelen zijn nauw verbonden met de kwestie van "alignment," dat wil zeggen de vraag of AI-systemen *zullen doen wat we willen dat ze doen*. Deze simpele vraag verbergt een enorm niveau van subtiliteit.[^9] Merk voor nu op dat "we" in deze zin kan verwijzen naar veel verschillende mensen en groepen, wat leidt tot verschillende typen alignment. Bijvoorbeeld, een AI kan zeer *gehoorzaam* zijn (of ["loyaal"](https://arxiv.org/abs/2003.11157)) aan zijn gebruiker – hier is "we" "elk van ons." Of het kan meer *soeverein* zijn, voornamelijk gedreven door zijn eigen doelen en beperkingen, maar nog steeds handelend in het brede gemeenschappelijke belang van menselijk welzijn – "we" is dan "de mensheid" of "de maatschappij." Daar tussenin ligt een spectrum waar een AI grotendeels gehoorzaam zou zijn, maar zou kunnen weigeren acties te ondernemen die anderen of de samenleving schaden, de wet overtreden, enz.

Deze twee assen – niveau van autonomie en type alignment – zijn niet volledig onafhankelijk. Bijvoorbeeld, een soeverein passief systeem is, hoewel niet geheel tegenstrijdig, een concept in spanning, evenals een gehoorzame autonome agent.[^10] Er is een duidelijke zin waarin autonomie en soevereiniteit de neiging hebben hand in hand te gaan. In een vergelijkbare geest heeft voorspelbaarheid de neiging hoger te zijn in "passieve" en "gehoorzame" AI-systemen, terwijl soevereine of autonome systemen meer onvoorspelbaar zullen zijn. Dit alles zal cruciaal zijn voor het begrijpen van de gevolgen van potentiële AGI en superintelligentie.

Het creëren van echt gealigneerde AI, van welke vorm dan ook, vereist het oplossen van drie onderscheiden uitdagingen:

1. Begrijpen wat "we" willen – wat complex is of "we" nu een specifiek persoon of organisatie (loyaliteit) of de mensheid in het algemeen (soevereiniteit) betekent;
2. Systemen bouwen die regelmatig handelen in overeenstemming met die wensen – in wezen het creëren van consistent positief gedrag;
3. Meest fundamenteel, systemen maken die oprecht "geven" om die wensen in plaats van alleen maar doen alsof ze dat doen.

Het onderscheid tussen betrouwbaar gedrag en oprechte zorg is cruciaal. Net zoals een menselijke werknemer orders perfect kan opvolgen terwijl hij geen echte toewijding heeft aan de missie van de organisatie, kan een AI-systeem gealigneerd handelen zonder werkelijk menselijke voorkeuren te waarderen. We kunnen AI-systemen trainen om dingen te zeggen en te doen door feedback, en ze kunnen leren redeneren over wat mensen willen. Maar hen *werkelijk* menselijke voorkeuren laten waarderen is een veel diepere uitdaging.[^11]

De diepgaande moeilijkheden bij het oplossen van deze alignment-uitdagingen, en hun implicaties voor AI-risico, zullen hieronder verder worden onderzocht. Begrijp voor nu dat alignment niet alleen een technische eigenschap is die we aan AI-systemen vastmaken, maar een fundamenteel aspect van hun architectuur dat hun relatie met de mensheid vormt.

[^1]: Voor een zachte maar technische inleiding tot machine learning en AI, met name taalmodellen, zie [deze site.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) Voor een andere moderne primer over AI-uitstervingsrisico's, zie [dit stuk.](https://www.thecompendium.ai/) Voor een uitgebreide en gezaghebbende wetenschappelijke analyse van de staat van AI-veiligheid, zie het recente [International AI Safety Report.](https://arxiv.org/abs/2501.17805)

[^2]: Training vindt typisch plaats door te zoeken naar een lokaal maximum van de score in een hoogdimensionale ruimte gegeven door de modelgewichten. Door te controleren hoe de score verandert naarmate gewichten worden aangepast, identificeert het trainingsalgoritme welke aanpassingen de score het meest verbeteren, en beweegt de gewichten in die richting.

[^3]: Bijvoorbeeld, in een beeldherkenningsprobleem zou het neurale netwerk waarschijnlijkheden uitvoeren voor labels voor de afbeelding. Een score zou gerelateerd zijn aan de waarschijnlijkheid die de AI toekent aan het juiste antwoord. De trainingsprocedure zou dan gewichten aanpassen zodat de volgende keer de AI een hogere waarschijnlijkheid zou uitvoeren voor het juiste label voor die afbeelding. Dit wordt dan een enorm aantal keren herhaald. Dezelfde basisprocedure wordt gebruikt bij het trainen van in wezen alle moderne neurale netwerken, zij het met complexere scoringsmechanismen.

[^4]: De meeste multimodale modellen gebruiken de "transformer"-architectuur om meerdere typen data (tekst, afbeeldingen, geluid) te verwerken en genereren. Deze kunnen allemaal worden ontleed in, en vervolgens behandeld op gelijke voet, als verschillende typen "tokens." Multimodale modellen worden eerst getraind om tokens binnen massale datasets accuraat te voorspellen, vervolgens verfijnd door reinforcement learning om capaciteiten te verbeteren en gedragingen te vormen.

[^5]: Dat taalmodellen getraind worden om één ding te doen – woorden voorspellen – heeft sommigen ertoe gebracht ze smalle AI te noemen. Maar dit is misleidend: omdat het goed voorspellen van tekst zo veel verschillende capaciteiten vereist, leidt deze trainingstaak tot een verrassend algemeen systeem. Merk ook op dat deze systemen uitgebreid getraind worden door reinforcement learning, wat effectief duizenden mensen vertegenwoordigt die het model een beloningssignaal geven wanneer het goed presteert bij een van de vele dingen die het doet. Het erft dan significante algemeenheid van de mensen die deze feedback geven.

[^6]: Er zijn meerdere manieren waarop AI onvoorspelbaar is. Een is dat men in het algemene geval niet kan voorspellen wat een algoritme zal doen zonder het daadwerkelijk uit te voeren; er zijn [stellingen](https://arxiv.org/abs/1310.3225) hieromtrent. Dit kan waar zijn alleen omdat de uitvoer van algoritmen complex kan zijn. Maar het is bijzonder duidelijk en relevant in het geval (zoals bij schaak of Go) waar de voorspelling een capaciteit zou impliceren (het verslaan van de AI) die de aspirant-voorspeller niet heeft. Ten tweede zal een gegeven AI-systeem niet altijd dezelfde uitvoer produceren zelfs bij dezelfde invoer – zijn uitvoer bevat willekeur; dit koppelt ook met algoritmische onvoorspelbaarheid. Ten derde kunnen onverwachte en emergente capaciteiten ontstaan uit training, wat betekent dat zelfs de *typen* dingen die een AI-systeem kan en zal doen onvoorspelbaar zijn; Dit laatste type is bijzonder belangrijk voor veiligheidsoverwegingen.

[^7]: Zie [hier](https://arxiv.org/abs/2502.02649) voor een diepgaande review van wat bedoeld wordt met een "autonome agent" (samen met ethische argumenten tegen het bouwen ervan).

[^8]: Je hoort soms wel eens "AI kan geen eigen doelen hebben." Dit is absolute onzin. Het is gemakkelijk om voorbeelden te genereren waar AI doelen heeft of ontwikkelt die nooit aan haar zijn gegeven en alleen aan haarzelf bekend zijn. Je ziet dit niet veel in huidige populaire multimodale modellen omdat het eruit getraind wordt; het zou net zo gemakkelijk erin getraind kunnen worden.

[^9]: Er is een grote literatuur. Over het algemene probleem zie Christian's [*The Alignment Problem*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821), en Russell's [*Human-Compatible*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616). Op een meer technische kant zie bijvoorbeeld [dit paper](https://arxiv.org/abs/2209.00626).

[^10]: We zullen later zien dat hoewel zulke systemen tegen de trend ingaan, dat hen eigenlijk zeer interessant en nuttig maakt.

[^11]: Dit betekent niet dat we emoties of bewustzijn vereisen. Eerder is het buitengewoon moeilijk van buitenaf een systeem te weten wat zijn innerlijke doelen, voorkeuren en waarden zijn. "Oprecht" zou hier betekenen dat we sterke genoeg reden hebben om erop te vertrouwen dat we in het geval van kritieke systemen ons leven erop kunnen zetten.
