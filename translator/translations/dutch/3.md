# Hoofdstuk 3 - Belangrijke aspecten van hoe moderne algemene AI-systemen worden gemaakt

De meeste van 's werelds meest geavanceerde AI-systemen worden gemaakt met verrassend vergelijkbare methoden. Hier zijn de basisprincipes.

Om een mens echt te begrijpen moet je iets weten van biologie, evolutie, kinderopvoeding en meer; om AI te begrijpen moet je ook weten hoe het wordt gemaakt. De afgelopen vijf jaar zijn AI-systemen enorm geëvolueerd, zowel in capaciteit als complexiteit. Een belangrijke enabler hiervoor is de beschikbaarheid van zeer grote hoeveelheden rekenkracht geweest (of informeel "compute" wanneer toegepast op AI).

De cijfers zijn verbluffend. Ongeveer 10 <sup>25</sup> -10 <sup>26</sup> "floating-point operaties" (FLOP) [^1] worden gebruikt bij de training van modellen zoals de GPT-serie, Claude, Gemini, etc.[^2] (Ter vergelijking: als elke mens op aarde non-stop zou werken en elke vijf seconden één berekening zou doen, zou het ongeveer een miljard jaar duren om dit te voltooien.) Deze enorme hoeveelheid berekeningen maakt het mogelijk om modellen te trainen met tot triljoenen modelgewichten op terabytes aan data – een groot deel van alle kwaliteitstekst die ooit is geschreven, samen met uitgebreide bibliotheken van geluiden, afbeeldingen en video. Door deze training aan te vullen met uitgebreide additionele training die menselijke voorkeuren en goede taakprestaties versterkt, vertonen op deze manier getrainde modellen prestaties die concurreren met mensen over een aanzienlijk spectrum van intellectuele basistaken, inclusief redeneren en probleemoplossing.

We weten ook (heel, heel ruwweg) hoeveel rekensnelheid, in operaties per seconde, voldoende is om de *inferentie*snelheid [^3] van zo'n systeem gelijk te laten zijn aan de *snelheid* van menselijke tekstverwerking. Het is ongeveer 10 <sup>15</sup> -10 <sup>16</sup> FLOP per seconde.[^4]

Hoewel krachtig, zijn deze modellen van nature beperkt op belangrijke manieren, vrij vergelijkbaar met hoe een individuele mens beperkt zou zijn als hij gedwongen zou worden om simpelweg tekst uit te voeren met een vast tempo van woorden per minuut, zonder te stoppen om na te denken of aanvullende tools te gebruiken. Meer recente AI-systemen pakken deze beperkingen aan door middel van een complexer proces en architectuur die verschillende sleutelelementen combineert:

- Een of meer neurale netwerken, waarbij één model de cognitieve kerncapaciteit levert, en maximaal verschillende andere meer specifieke taken uitvoeren;
- *Tooling* die beschikbaar wordt gesteld aan en bruikbaar is door het model – bijvoorbeeld de mogelijkheid om op het web te zoeken, documenten te creëren of bewerken, programma's uit te voeren, etc.
- *Scaffolding* die input en output van neurale netwerken verbindt. Een zeer eenvoudige scaffold zou bijvoorbeeld gewoon twee "instanties" van een AI-model met elkaar kunnen laten converseren, of de ene het werk van de andere kunnen laten controleren.[^5]
- *Chain-of-thought* en gerelateerde prompt-technieken doen iets soortgelijks en zorgen ervoor dat een model bijvoorbeeld vele benaderingen voor een probleem genereert en die benaderingen vervolgens verwerkt voor een samengevoegd antwoord.
- *Hertraining* van modellen om beter gebruik te maken van tools, scaffolding en chain-of-thought.

Omdat deze uitbreidingen zeer krachtig kunnen zijn (en AI-systemen zelf omvatten), kunnen deze samengestelde systemen vrij geraffineerd zijn en AI-capaciteiten dramatisch versterken.[^6] En recent zijn technieken in scaffolding en vooral chain-of-thought prompting (en het terugvoeren van resultaten in hertraining van modellen om deze beter te gebruiken) ontwikkeld en toegepast in [o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/) en [DeepSeek R1](https://api-docs.deepseek.com/news/news250120) om vele inferentie-rondes uit te voeren als reactie op een gegeven vraag.[^7] Dit stelt het model in feite in staat om "na te denken over" zijn reactie en verhoogt de capaciteit van deze modellen om hoogwaardig redeneren in wetenschap, wiskunde en programmeren dramatisch.[^8]

Voor een gegeven AI-architectuur kunnen toenames in trainingsrekenkracht [betrouwbaar worden vertaald](https://arxiv.org/abs/2405.10938) naar verbeteringen in een reeks duidelijk gedefinieerde meetwaarden. Voor minder scherp gedefinieerde algemene capaciteiten (zoals die hieronder besproken), is de vertaling minder helder en voorspellend, maar het is vrijwel zeker dat grotere modellen met meer trainingsrekenkracht nieuwe en betere capaciteiten zullen hebben, ook al is het moeilijk te voorspellen wat die zullen zijn.

Evenzo hebben samengestelde systemen en vooral vooruitgang in "chain of thought" (en training van modellen die er goed mee werken) schaalbaarheid in *inferentie*rekenkracht ontgrendeld: voor een gegeven getraind kernmodel nemen ten minste enkele AI-systeemcapaciteiten toe naarmate meer rekenkracht wordt toegepast die hen in staat stelt "harder en langer na te denken" over complexe problemen. Dit gaat gepaard met steile kosten aan rekensnelheid en vereist honderden of duizenden meer FLOP/s om menselijke prestaties te evenaren.[^9]

Hoewel slechts een deel van wat leidt tot snelle AI-vooruitgang,[^10] zal de rol van rekenkracht en de mogelijkheid van samengestelde systemen cruciaal blijken voor zowel het voorkomen van oncontroleerbare AGI als het ontwikkelen van veiligere alternatieven.

[^1]: 10 <sup>25</sup> betekent 1 gevolgd door 25 nullen, oftewel tien biljard biljoen. Een FLOP is gewoon een rekenkundige optelling of vermenigvuldiging van getallen met enige precisie. Let op dat AI-hardwareprestaties kunnen variëren met een factor tien meer afhankelijk van de precisie van de rekenkunde en de architectuur van de computer. Het tellen van logische-poort-operaties (ANDS, ORS, AND NOTS) zou fundamenteel zijn maar deze zijn niet algemeen beschikbaar of gebenchmarkt; voor huidige doeleinden is het nuttig om te standaardiseren op 16-bit operaties (FP16), hoewel passende conversiefactoren zouden moeten worden vastgesteld.

[^2]: Een verzameling schattingen en harde data is beschikbaar van [Epoch AI](https://epochai.org/data/large-scale-ai-models) en geeft ongeveer 2×10 <sup>25</sup> 16-bit FLOP voor GPT-4 aan; dit komt ongeveer overeen met [cijfers die gelekt zijn](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) voor GPT-4. Schattingen voor andere mid-2024 modellen liggen allemaal binnen een factor van enkele malen GPT-4.

[^3]: Inferentie is simpelweg het proces van het genereren van output uit een neuraal netwerk. Training kan worden beschouwd als een opeenvolging van vele inferenties en aanpassingen van modelgewichten.

[^4]: Voor tekstproductie vereiste de originele GPT-4 560 TFLOP per gegenereerd token. Ongeveer 7 tokens/s is nodig om menselijk denken bij te houden, dus dit geeft ≈3×10 <sup>15</sup> FLOP/s. Maar efficiëntieverbeteringen hebben dit naar beneden gebracht; [deze NVIDIA-brochure](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/) geeft bijvoorbeeld slechts 3×10 <sup>14</sup> FLOP/s aan voor een vergelijkbaar presterend Llama 405B-model.

[^5]: Als iets complexer voorbeeld zou een AI-systeem eerst verschillende mogelijke oplossingen voor een wiskundeprobleem kunnen genereren, dan een andere instantie gebruiken om elke oplossing te controleren, en ten slotte een derde gebruiken om de resultaten te synthetiseren tot een heldere uitleg. Dit maakt grondiger en betrouwbaarder probleemoplossen mogelijk dan een enkele doorgang.

[^6]: Zie bijvoorbeeld details over [OpenAI's "Operator"](https://openai.com/index/introducing-operator/), [Claude's tool-capaciteiten](https://docs.anthropic.com/en/docs/build-with-claude/computer-use), en [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). OpenAI's [Deep Research](https://openai.com/index/introducing-deep-research/) heeft waarschijnlijk een vrij geraffineerde architectuur maar details zijn niet beschikbaar.

[^7]: Deepseek R1 baseert zich op iteratieve training en prompting van het model zodat het uiteindelijke getrainde model uitgebreide chain-of-thought redenering creëert. Architecturale details zijn niet beschikbaar voor o1 of o3, echter Deepseek heeft onthuld dat er geen bijzondere "geheime saus" vereist is om capaciteitsschaling met inferentie te ontgrendelen. Maar ondanks dat het veel media-aandacht kreeg als het omverwerpen van de "status quo" in AI, beïnvloedt het de kernstellingen van dit essay niet.

[^8]: Deze modellen presteren aanzienlijk beter dan standaardmodellen op redeneerbenchmarks. Bijvoorbeeld in de GPQA Diamond Benchmark – een rigoureuze test van PhD-niveau wetenschapsvragen – [scoorde](https://openai.com/index/learning-to-reason-with-llms/) GPT-4o 56%, terwijl o1 en o3 respectievelijk 78% en 88% behaalden, ver boven de 70% gemiddelde score van menselijke experts.

[^9]: OpenAI's O3 heeft waarschijnlijk ∼10 <sup>21</sup> -10 <sup>22</sup> FLOP [gebruikt om elk van de ARC-AGI challenge-vragen te voltooien](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai), wat competente mensen kunnen doen in (pakweg) 10-100 seconden, wat een cijfer geeft van meer zoals ∼10 <sup>20</sup> FLOP/s.

[^10]: Hoewel rekenkracht een belangrijke maatstaf is voor AI-systeemcapaciteit, interacteert het met zowel datakwaliteit als algoritmische verbeteringen. Betere data of algoritmen kunnen rekenvereisten verminderen, terwijl meer rekenkracht soms kan compenseren voor zwakkere data of algoritmen.
