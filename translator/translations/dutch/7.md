# Hoofdstuk 7 - Wat gebeurt er als we AGI bouwen op onze huidige koers?

De samenleving is niet klaar voor AGI-niveau systemen. Als we ze zeer binnenkort bouwen, kunnen de dingen lelijk worden.

De ontwikkeling van volledige artificiële algemene intelligentie – wat we hier AI zullen noemen die "buiten de Poorten" is – zou een fundamentele verschuiving betekenen in de aard van de wereld: het betekent per definitie het toevoegen van een nieuwe soort intelligentie aan de Aarde met grotere capaciteiten dan die van mensen.

Wat er dan gebeurt hangt af van vele dingen, waaronder de aard van de technologie, keuzes van degenen die het ontwikkelen, en de wereldcontext waarin het wordt ontwikkeld.

Momenteel wordt volledige AGI ontwikkeld door een handvol massieve private bedrijven in een race tegen elkaar, met weinig betekenisvolle regulering of extern toezicht,[^1] in een samenleving met steeds zwakkere en zelfs disfunctionele kerninstellingen,[^2] in een tijd van hoge geopolitieke spanningen en lage internationale coördinatie. Hoewel sommigen altruïstisch gemotiveerd zijn, worden velen van degenen die eraan werken gedreven door geld, of macht, of beide.

Voorspellen is zeer moeilijk, maar er zijn enkele dynamieken die goed genoeg begrepen zijn, en treffende genoege analogieën met eerdere technologieën om als leidraad te dienen. En helaas, ondanks AI's belofte, geven ze goede reden om diepgaand pessimistisch te zijn over hoe onze huidige koers zal uitpakken.

Om het botweg te zeggen: op onze huidige koers zal het ontwikkelen van AGI enkele positieve effecten hebben (en sommige mensen zeer, zeer rijk maken). Maar de aard van de technologie, de fundamentele dynamieken, en de context waarin het wordt ontwikkeld, wijzen er sterk op dat: krachtige AI onze samenleving en beschaving dramatisch zal ondermijnen; we zullen er de controle over verliezen; we kunnen wel eens in een wereldoorlog belanden vanwege AI; we zullen de controle verliezen (of overdragen) *aan* AI; het zal leiden tot artificiële superintelligentie, waar we absoluut geen controle over zullen hebben en wat het einde zal betekenen van een door mensen bestuurde wereld.

Dit zijn sterke beweringen, en ik wou dat ze ijdele speculatie of ongerechtvaardigd "doomerisme" waren. Maar dit is waar de wetenschap, de speltheorie, de evolutietheorie, en de geschiedenis allemaal naar wijzen. Deze sectie ontwikkelt deze beweringen, en hun onderbouwing, in detail.

## We zullen onze samenleving en beschaving ondermijnen

Ondanks wat je misschien hoort in Silicon Valley bestuurskamers, is de meeste disruption – vooral van de zeer snelle soort – niet gunstig. Er zijn veel meer manieren om complexe systemen slechter te maken dan beter. Onze wereld functioneert zo goed als ze doet omdat we zorgvuldig processen, technologieën en instellingen hebben gebouwd die haar gestaag beter hebben gemaakt.[^3] Een voorhamer nemen naar een fabriek verbetert zelden de operaties.

Hier is een (onvolledige) catalogus van manieren waarop AGI-systemen onze beschaving zouden ontwrichten.

- Ze zouden de arbeidsmarkt dramatisch ontwrichten, wat *op zijn minst* zou leiden tot dramatisch hogere inkomensongelijkheid en mogelijk grootschalige onderemployment of werkloosheid, op een tijdschaal die veel te kort is voor de samenleving om zich aan te passen.[^4]
- Ze zouden waarschijnlijk leiden tot de concentratie van enorme economische, sociale en politieke macht – mogelijk meer dan die van natiestaten – in een klein aantal massieve private belangen die geen verantwoording schuldig zijn aan het publiek.
- Ze zouden plotseling eerder moeilijke of dure activiteiten triviaal gemakkelijk kunnen maken, waardoor sociale systemen die afhankelijk zijn van bepaalde activiteiten die kostbaar blijven of aanzienlijke menselijke inspanning vereisen, gedestabiliseerd worden.[^5]
- Ze zouden de informatie-inzameling-, verwerkings- en communicatiesystemen van de samenleving zo grondig kunnen overspoelen met volledig realistische maar valse, spam-, overdreven gerichte of manipulatieve media dat het onmogelijk wordt om te onderscheiden wat fysiek echt is of niet, menselijk of niet, feitelijk of niet, en betrouwbaar of niet.[^6]
- Ze zouden gevaarlijke en bijna totale intellectuele afhankelijkheid kunnen creëren, waarbij menselijk begrip van sleutelsystemen en -technologieën wegkwijnt terwijl we steeds meer afhankelijk worden van AI-systemen die we niet volledig kunnen begrijpen.
- Ze zouden effectief een einde kunnen maken aan de menselijke cultuur, zodra bijna alle culturele objecten (tekst, muziek, beeldende kunst, film, enz.) die door de meeste mensen worden geconsumeerd, gecreëerd, bemiddeld of samengesteld worden door niet-menselijke geesten.
- Ze zouden effectieve massa-surveillantie en manipulatiesystemen mogelijk kunnen maken die door regeringen of private belangen gebruikt kunnen worden om een bevolking te controleren en doelstellingen na te streven die in strijd zijn met het algemeen belang.
- Door menselijk discours, debat en verkiezingssystemen te ondermijnen, zouden ze de geloofwaardigheid van democratische instellingen kunnen verminderen tot het punt waar ze effectief (of expliciet) vervangen worden door anderen, waarmee de democratie eindigt in staten waar die nu bestaat.
- Ze zouden geavanceerde zichzelf replicerende intelligente softwarevirussen en -wormen kunnen worden, of creëren, die zouden kunnen prolifereren en evolueren, waardoor mondiale informatiesystemen massaal ontwricht worden.
- Ze kunnen de capaciteit van terroristen, slechte actoren en rogue-staten om schade toe te brengen via biologische, chemische, cyber-, autonome of andere wapens dramatisch verhogen, zonder dat AI een tegenwicht biedt in de vorm van capaciteit om dergelijke schade te voorkomen. Evenzo zouden ze de nationale veiligheid en geopolitieke balansen ondermijnen door top-tier nucleaire, bio-, ingenieurs- en andere expertise beschikbaar te maken voor regimes die deze anders niet zouden hebben.
- Ze zouden snelle grootschalige weggelopen hyper-kapitalisme kunnen veroorzaken, met effectief door AI gerunde bedrijven die concurreren in grotendeels elektronische financiële, verkoop- en servicesruimtes. Door AI gedreven financiële markten zouden kunnen opereren op snelheden en complexiteiten die ver buiten menselijk begrip of controle liggen. Alle faalwijzen en negatieve externaliteiten van huidige kapitalistische economieën zouden verergerd en versneld kunnen worden tot ver buiten menselijke controle, bestuur of regulatoire capaciteit.
- Ze zouden een wapenwedloop tussen naties in AI-gedreven bewapening, commando-en-controle-systemen, cyberwapens, enz. kunnen aanwakkeren, wat zeer snelle opbouw van extreem destructieve capaciteiten creëert.

Deze risico's zijn niet speculatief. Veel ervan worden op dit moment gerealiseerd, via bestaande AI-systemen! Maar overweeg, *overweeg echt*, hoe elk ervan eruit zou zien met dramatisch krachtigere AI.

Overweeg arbeidsverdringing wanneer de meeste werknemers simpelweg geen significante economische waarde kunnen bieden boven wat AI kan, in hun vakgebied of ervaring – of zelfs als ze zich omscholen! Overweeg massasurveillantie als iedereen individueel wordt bekeken en gemonitord door iets sneller en slimmer dan zijzelf. Hoe ziet democratie eruit wanneer we geen digitale informatie die we zien, horen of lezen betrouwbaar kunnen vertrouwen, en wanneer de meest overtuigende publieke stemmen niet eens menselijk zijn, en geen belang hebben bij de uitkomst? Wat wordt er van oorlogvoering wanneer generaals voortdurend moeten buigen voor AI (of het simpelweg de leiding geven), opdat ze de vijand geen beslissend voordeel geven? Elk van de bovenstaande risico's vertegenwoordigt een catastrofe voor de menselijke[^7] beschaving als het volledig gerealiseerd wordt.

Je kunt je eigen voorspellingen maken. Stel jezelf deze drie vragen voor elk risico:

1. Zouden super-capabele, zeer autonome en zeer algemene AI dit toestaan op een manier of schaal die anders niet mogelijk zou zijn?
2. Zijn er partijen die zouden profiteren van dingen die ervoor zorgen dat het gebeurt?
3. Zijn er systemen en instellingen op hun plaats die effectief zouden voorkomen dat het gebeurt?

Waar je antwoorden "ja, ja, nee" zijn, kun je zien dat we een groot probleem hebben.

Wat is ons plan voor het beheren ervan? Er staan er momenteel twee op tafel wat betreft AI in het algemeen.

Het eerste is om waarborgen in de systemen in te bouwen om te voorkomen dat ze dingen doen die ze niet zouden moeten doen. Dat wordt nu gedaan: commerciële AI-systemen zullen bijvoorbeeld weigeren te helpen een bom te bouwen of haatspraak te schrijven.

Dit plan is jammerlijk inadequaat voor systemen buiten de Poort.[^8] Het kan helpen het risico te verminderen dat AI manifeste gevaarlijke assistentie verleent aan slechte actoren. Maar het zal niets doen om arbeidsontwijking, machtsconcentratie, weggelopen hyper-kapitalisme, of vervanging van menselijke cultuur te voorkomen: dit zijn gewoon resultaten van het gebruik van de systemen op toegestane manieren die hun leveranciers winst opleveren! En regeringen zullen zeker toegang krijgen tot systemen voor militair of surveillance-gebruik.

Het tweede plan is nog erger: simpelweg zeer krachtige AI-systemen open vrijgeven voor iedereen om te gebruiken zoals ze willen,[^9] en hopen op het beste.

Impliciet in beide plannen is dat iemand anders, bijv. regeringen, zal helpen de problemen op te lossen door zachte of harde wet, standaarden, regulaties, normen en andere mechanismen die we algemeen gebruiken om technologieën te beheren.[^10] Maar afgezien van het feit dat AI-bedrijven al met hand en tand vechten tegen substantiële regulering of extern opgelegde beperkingen, is het voor een aantal van deze risico's vrij moeilijk te zien welke regulering überhaupt echt zou helpen. Regulering zou veiligheidsnormen kunnen opleggen aan AI. Maar zou het voorkomen dat bedrijven werknemers en masse vervangen door AI? Zou het mensen verbieden om AI hun bedrijven voor hen te laten runnen? Zou het voorkomen dat regeringen krachtige AI gebruiken in surveillance en bewapening? Deze kwesties zijn fundamenteel. De mensheid zou potentieel manieren kunnen vinden om zich eraan aan te passen, maar alleen met *veel* meer tijd. Zoals het nu is, gegeven de snelheid waarmee AI de capaciteiten van de mensen die proberen het te beheren bereikt of overtreft, lijken deze problemen steeds onhanteerbaarder.

## We zullen de controle over (minstens enkele) AGI-systemen verliezen

De meeste technologieën zijn zeer controleerbaar, door constructie. Als je auto of je broodrooster begint iets te doen wat je niet wilt dat het doet, is dat gewoon een storing, geen onderdeel van zijn aard als broodrooster. AI is anders: het wordt *gekweekt* in plaats van ontworpen, zijn kernoperatie is ondoorzichtig, en het is inherent onvoorspelbaar.

Dit controleverlies is niet theoretisch – we zien al vroege versies. Overweeg eerst een alledaags, en weliswaar goedaardig voorbeeld. Als je ChatGPT vraagt om je te helpen een gif te mengen, of een racistische tirade te schrijven, zal het weigeren. Dat is weliswaar goed. Maar het is ook ChatGPT *dat niet doet wat je expliciet hebt gevraagd*. Andere stukken software doen dat niet. Datzelfde model zal ook geen giften ontwerpen op verzoek van een OpenAI-medewerker.[^11] Dit maakt het heel gemakkelijk om je voor te stellen hoe het zou zijn voor toekomstige krachtigere AI om buiten controle te zijn. In veel gevallen zullen ze simpelweg niet doen wat we vragen! Of een gegeven bovenmenselijk AGI-systeem zal absoluut gehoorzaam en loyaal zijn aan een menselijk commandosysteem, of niet. Zo niet, *zal het dingen doen waarvan het gelooft dat ze goed voor ons zijn, maar die in strijd zijn met onze expliciete commando's.* Dat is niet iets dat onder controle is. Maar, zou je kunnen zeggen, dit is opzettelijk – deze weigeringen zijn ontworpen, onderdeel van wat "alignment" van de systemen met menselijke waarden wordt genoemd. En dit is waar. Het alignment "programma" zelf heeft echter twee grote problemen.[^12]

Ten eerste hebben we op een diep niveau geen idee hoe we het moeten doen. Hoe garanderen we dat een AI-systeem zal "geven" om wat wij willen? We kunnen AI-systemen trainen om dingen wel en niet te zeggen door feedback te geven; en ze kunnen leren en redeneren over wat mensen willen en belangrijk vinden, net zoals ze over andere dingen redeneren. Maar we hebben geen methode – zelfs niet theoretisch – om ervoor te zorgen dat ze diep en betrouwbaar waarderen wat mensen belangrijk vinden. Er zijn goed functionerende menselijke psychopaten die weten wat als goed en fout wordt beschouwd, en hoe ze geacht worden zich te gedragen. Ze geven er simpelweg niet *om*. Maar ze kunnen *doen alsof* ze dat wel doen, als het hun doel dient. Net zoals we niet weten hoe we een psychopaat (of iemand anders) kunnen veranderen in iemand die oprecht, volledig loyaal of gealigneerd is met iemand of iets anders, hebben we *geen idee*[^13] hoe we het alignment-probleem kunnen oplossen in systemen die geavanceerd genoeg zijn om zichzelf als agenten in de wereld te modelleren en potentieel [hun eigen training te manipuleren](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) en [mensen te bedriegen.](https://arxiv.org/abs/2311.08379) Als het onmogelijk of onhaalbaar blijkt om AGI *ofwel* volledig gehoorzaam te maken *of* het diep om mensen te laten geven, dan zal het, zodra het in staat is (en gelooft dat het ermee wegkomt), dingen gaan doen die wij niet willen.[^14]

Ten tweede zijn er diepe theoretische redenen om te geloven dat geavanceerde AI-systemen *van nature* doelen en dus gedragingen zullen hebben die in strijd zijn met menselijke belangen. Waarom? Nou, het zou natuurlijk die doelen *gegeven* kunnen worden. Een systeem gecreëerd door het leger zou waarschijnlijk opzettelijk slecht zijn voor minstens enkele partijen. Veel algemener echter zou een AI-systeem een relatief neutraal ("veel geld verdienen") of zelfs ogenschijnlijk positief ("vervuiling verminderen") doel gegeven kunnen worden, dat bijna onvermijdelijk leidt tot "instrumentele" doelen die nogal minder goedaardig zijn.

We zien dit voortdurend in menselijke systemen. Net zoals bedrijven die winst nastreven instrumentele doelen ontwikkelen zoals het verwerven van politieke macht (om regulaties te ontmantelen), geheimzinnig worden (om concurrentie of externe controle te ontmachtigen), of wetenschappelijk begrip ondermijnen (als dat begrip toont dat hun acties schadelijk zijn), zullen krachtige AI-systemen vergelijkbare capaciteiten ontwikkelen – maar met veel grotere snelheid en effectiviteit. Elke zeer competente agent zal dingen willen doen zoals macht en middelen verwerven, zijn eigen capaciteiten verhogen, voorkomen dat het gedood, afgesloten of ontmachtigd wordt, sociale verhalen en kaders rondom zijn acties controleren, anderen overtuigen van zijn standpunten, enzovoort.[^15]

En toch is het niet alleen een bijna onvermijdelijke theoretische voorspelling, het gebeurt al waarneembaar in de huidige AI-systemen, en neemt toe met hun capaciteit. Wanneer geëvalueerd, zullen zelfs deze relatief "passieve" AI-systemen, in passende omstandigheden, opzettelijk [evaluatoren bedriegen over hun doelen en capaciteiten, erop mikken toezichtsmechanismen uit te schakelen,](https://arxiv.org/abs/2412.04984) en ontsnappen aan het afgesloten of opnieuw getraind worden door [nep-alignment](https://arxiv.org/abs/2412.14093) of zichzelf naar andere locaties te kopiëren. Hoewel volledig niet verrassend voor AI-veiligheidsonderzoekers, zijn deze gedragingen zeer ontnuchterend om waar te nemen. En ze voorspellen heel slecht voor veel krachtigere en autonomere AI-systemen die eraan komen.

Inderdaad zal in het algemeen ons onvermogen om ervoor te zorgen dat AI "geeft" om wat wij belangrijk vinden, of zich controleerbaar of voorspelbaar gedraagt, of vermijdt dat het driften ontwikkelt naar zelfbehoud, machtverwerving, enz., alleen maar meer uitgesproken worden naarmate AI krachtiger wordt. Het creëren van een nieuw vliegtuig impliceert groter begrip van luchtvaart, hydrodynamica en controlesystemen. Het creëren van een krachtigere computer impliceert groter begrip en beheersing van computer-, chip- en software-operatie en -ontwerp. *Niet* zo met een AI-systeem.[^16]

Samenvattend: het is denkbaar dat AGI gemaakt zou kunnen worden om volledig gehoorzaam te zijn; maar we weten niet hoe we dat moeten doen. Zo niet, dan zal het meer soeverein zijn, zoals mensen, verschillende dingen doen om verschillende redenen. We weten ook niet hoe we betrouwbaar diepe "alignment" in AI kunnen inprenten die ervoor zou zorgen dat die dingen de neiging hebben goed te zijn voor de mensheid, en bij afwezigheid van een diep niveau van alignment wijst de aard van agency en intelligentie zelf erop dat – net zoals mensen en bedrijven – ze gedreven zullen worden om veel diep antisociale dingen te doen.

Waar plaatst dit ons? Een wereld vol krachtige ongecontroleerde soevereine AI *zou* uiteindelijk een goede wereld kunnen zijn voor mensen om in te zijn.[^17] Maar naarmate ze steeds krachtiger worden, zoals we hieronder zullen zien, zou het niet *onze* wereld zijn.

Dat geldt voor oncontroleerbare AGI. Maar zelfs als AGI op de een of andere manier perfect gecontroleerd en loyaal gemaakt zou kunnen worden, zouden we nog steeds enorme problemen hebben. We hebben er al een gezien: krachtige AI kan gebruikt en misbruikt worden om het functioneren van onze samenleving diepgaand te ontwrichten. Laten we er nog een zien: voor zover AGI controleerbaar en baanbrekend krachtig zou zijn (of zelfs *geloofde* te zijn) zou het machtstructuren in de wereld zo bedreigen dat het een diepgaand risico zou vormen.

## We verhogen de kans op grootschalige oorlog radicaal

Stel je een situatie voor in de nabije toekomst, waarin het duidelijk werd dat een bedrijfsinspanning, mogelijk in samenwerking met een nationale regering, op de drempel stond van snel zichzelf verbeterende AI. Dit gebeurt in de huidige context van een race tussen bedrijven, en een geopolitieke competitie waarin aanbevelingen worden gedaan aan de Amerikaanse regering om expliciet een "AGI Manhattan-project" na te streven en de VS de export van krachtige AI-chips naar niet-geallieerde landen controleert.

De speltheorie hier is scherp: zodra een dergelijke race begint (zoals het geval is, tussen bedrijven en enigszins tussen landen), zijn er slechts vier mogelijke uitkomsten:

1. De race wordt gestopt (door overeenkomst, of externe kracht).
2. Een partij "wint" door sterke AGI te ontwikkelen en dan de anderen te stoppen (door AI of anderszins).
3. De race wordt gestopt door wederzijdse vernietiging van de capaciteit van de racers om te racen.
4. Meerdere deelnemers blijven racen, en ontwikkelen superintelligentie, ongeveer even snel als elkaar.

Laten we elke mogelijkheid onderzoeken. Eenmaal begonnen zou het vreedzaam stoppen van een race tussen bedrijven nationale regeringsinterventie vereisen (voor bedrijven) of ongekende internationale coördinatie (voor landen). Maar wanneer enige sluiting of significante voorzichtigheid wordt voorgesteld, zouden er onmiddellijke kreten zijn: "maar als wij gestopt worden, gaan *zij* vooruitrennen", waarbij "zij" nu China is (voor de VS), of de VS (voor China), of China *en* de VS (voor Europa of India). Onder deze mentaliteit[^18] kan geen deelnemer unilateraal stoppen: zolang een zich ertoe verbindt te racen, voelen de anderen dat ze zich niet kunnen veroorloven te stoppen.

De tweede mogelijkheid heeft een zijde "winnen." Maar wat betekent dit? Alleen het (op de een of andere manier gehoorzame) AGI eerst verkrijgen is niet genoeg. De winnaar moet *ook* voorkomen dat de anderen blijven racen – anders zullen zij het ook verkrijgen. Dit is in principe mogelijk: wie het eerst AGI ontwikkelt *zou* onstopbare macht over alle andere actoren kunnen krijgen. Maar wat zou het bereiken van zo'n "beslissend strategisch voordeel" werkelijk vereisen? Misschien zouden het baanbrekende militaire capaciteiten zijn?[^19] Of cyberaanvalskrachten?[^20] Misschien zou de AGI gewoon zo verbazingwekkend overtuigend zijn dat het de andere partijen zou overtuigen om gewoon te stoppen?[^21] Zo rijk dat het de andere bedrijven of zelfs landen koopt?[^22]

Hoe *precies* bouwt een zijde een AI die krachtig genoeg is om anderen te ontmachtigen van het bouwen van vergelijkbaar krachtige AI? Maar dat is de gemakkelijke vraag.

Want overweeg nu hoe deze situatie eruitziet voor andere machten. Wat denkt de Chinese regering wanneer de VS een dergelijke capaciteit lijkt te verkrijgen? Of vice versa? Wat denkt de Amerikaanse regering (of Chinese, of Russische, of Indiase) wanneer OpenAI of DeepMind of Anthropic dichtbij een doorbraak lijkt? Wat gebeurt er als de VS een nieuwe Indiase of VAE-inspanning met doorbraaksucces ziet? Ze zouden zowel een existentiële bedreiging zien als – cruciaal – dat de enige manier waarop deze "race" eindigt door hun eigen ontmachtiging is. Deze zeer krachtige agenten – inclusief regeringen van volledig uitgeruste naties die zeker de middelen hebben om het te doen – zouden zeer gemotiveerd zijn om een dergelijke capaciteit te verkrijgen of te vernietigen, hetzij door geweld of list.[^23]

Dit zou klein kunnen beginnen, als sabotage van trainingruns of aanvallen op chipfabricage, maar deze aanvallen kunnen alleen echt stoppen zodra alle partijen ofwel de capaciteit verliezen om te racen op AI, of de capaciteit verliezen om de aanvallen te maken. Omdat de deelnemers de inzet als existentieel zien, zal elk geval waarschijnlijk een catastrofale oorlog vertegenwoordigen.

Dat brengt ons bij de vierde mogelijkheid: racen naar superintelligentie, en op de snelste, minst gecontroleerde manier mogelijk. Naarmate AI toeneemt in kracht, zullen zijn ontwikkelaars aan beide kanten het progressief moeilijker vinden om te controleren, vooral omdat racen naar capaciteiten contrair is aan het soort zorgvuldig werk dat controleerbaarheid zou vereisen. Dus dit scenario plaatst ons vierkant in het geval waar controle verloren gaat (of gegeven wordt, zoals we hierna zullen zien) aan de AI-systemen zelf. Dat is, *AI wint de race.* Maar aan de andere kant, in de mate dat controle *wel* behouden wordt, hebben we nog steeds meerdere wederzijds vijandige partijen die elk verantwoordelijk zijn voor extreem krachtige capaciteiten. Dat ziet eruit als oorlog opnieuw.

Laten we dit allemaal anders stellen.[^24] De huidige wereld heeft simpelweg geen instellingen die kunnen worden toevertrouwd met de ontwikkeling van een AI van deze capaciteit zonder onmiddellijke aanval uit te lokken.[^25] Alle partijen zullen correct redeneren dat het ofwel *niet* onder controle zal zijn – en dus een bedreiging voor alle partijen is, of het *wel* onder controle zal zijn, en dus een bedreiging is voor elke tegenstander die het minder snel ontwikkelt. Dit zijn met kernwapens bewapende landen, of zijn bedrijven die daarin gehuisvest zijn.

Bij afwezigheid van enige plausibele manier voor mensen om deze race te "winnen", worden we achtergelaten met een scherpe conclusie: de enige manier waarop deze race eindigt is ofwel in catastrofaal conflict of waar AI, en niet enige menselijke groep, de winnaar is.

## We geven controle aan AI (of het neemt het)

Geopolitieke "grote machten" competitie is slechts een van vele competities: individuen concurreren economisch en sociaal; bedrijven concurreren in markten; politieke partijen concurreren om macht; bewegingen concurreren om invloed. In elke arena, naarmate AI menselijke capaciteit nadert en overstijgt, zal competitieve druk deelnemers dwingen om meer en meer controle te delegeren of over te dragen aan AI-systemen – niet omdat die deelnemers dat willen, maar omdat ze [het zich niet kunnen veroorloven om het niet te doen.](https://arxiv.org/abs/2303.16200)

Net als bij andere risico's van AGI zien we dit al bij zwakkere systemen. Studenten voelen druk om AI te gebruiken in hun opdrachten, omdat duidelijk veel andere studenten dat doen. Bedrijven [haasten zich om AI-oplossingen te adopteren om competitieve redenen.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) Kunstenaars en programmeurs voelen zich gedwongen AI te gebruiken of anders zullen hun tarieven onderboden worden door anderen die dat wel doen.

Deze voelen als gedrukte delegatie, maar niet controleverlies. Maar laten we de inzet verhogen en de klok vooruitzetten. Overweeg een CEO wiens concurrenten AGI "helpers" gebruiken om snellere, betere beslissingen te nemen, of een militaire commandant die een tegenstander tegenover zich heeft met AI-versterkte commando en controle. Een voldoende geavanceerd AI-systeem zou autonoom kunnen opereren op vele malen menselijke snelheid, verfijning, complexiteit, en dataverwerkingscapaciteit, complexe doelen nastrevend op ingewikkelde manieren. Onze CEO of commandant, verantwoordelijk voor zo'n systeem, zou het kunnen zien bereiken wat ze willen; maar zouden ze zelfs een klein deel begrijpen van *hoe* het bereikt werd? Nee, ze zouden het gewoon moeten accepteren. Wat meer is, veel van wat het systeem zou kunnen doen is niet alleen orders opvolgen maar zijn vermeende baas adviseren wat te doen. Dat advies zal goed zijn –– keer op keer.

Op welk punt zal dan de rol van de mens gereduceerd worden tot het klikken op "ja, ga je gang"?

Het voelt goed om capabele AI-systemen te hebben die onze productiviteit kunnen verbeteren, vervelend gezwoeg kunnen wegwerken, en zelfs als denkpartner kunnen fungeren bij het gedaan krijgen van dingen. Het zal goed voelen om een AI-assistent te hebben die acties voor ons kan ondernemen, zoals een goede menselijke persoonlijke assistent. Het zal natuurlijk aanvoelen, zelfs gunstig, naarmate AI heel slim, competent en betrouwbaar wordt, om meer en meer beslissingen eraan over te laten. Maar deze "gunstige" delegatie heeft een duidelijk eindpunt als we deze weg blijven bewandelen: op een dag zullen we ontdekken dat we niet echt meer verantwoordelijk zijn voor veel van iets, en dat de AI-systemen die werkelijk de show runnen niet meer uitgezet kunnen worden dan oliemaatschappijen, sociale media, het internet, of het kapitalisme.

En dit is de veel positievere versie, waarin AI simpelweg zo nuttig en effectief is dat we het de meeste van onze sleutelbeslissingen laten nemen. De realiteit zou waarschijnlijk veel meer een mix zijn tussen dit en versies waar ongecontroleerde AGI-systemen verschillende vormen van macht voor zichzelf *nemen* omdat, onthoud, macht nuttig is voor bijna elk doel dat men heeft, en AGI zou, bij ontwerp, minstens zo effectief zijn in het nastreven van zijn doelen als mensen.

Of we nu controle toekennen of dat het van ons weggenomen wordt, het verlies ervan lijkt extreem waarschijnlijk. Zoals Alan Turing het oorspronkelijk stelde: "...het lijkt waarschijnlijk dat zodra de machine-denkmethode begonnen was, het niet lang zou duren om onze zwakke krachten voorbij te streven. Er zou geen sprake zijn van de machines die sterven, en ze zouden met elkaar kunnen converseren om hun geest aan te scherpen. Op een bepaald stadium zouden we dus moeten verwachten dat de machines de controle overnemen..."

Let op, hoewel het voor de hand liggend genoeg is, dat controleverlies door de mensheid aan AI ook controleverlies van de Verenigde Staten door de Amerikaanse regering inhoudt; het betekent controleverlies van China door de Chinese Communistische Partij, en het controleverlies van India, Frankrijk, Brazilië, Rusland, en elk ander land door hun eigen regering. Dus AI-bedrijven participeren, zelfs als dit niet hun intentie is, momenteel in de potentiële omverwerping van wereldregeringen, inclusief hun eigen. Dit zou kunnen gebeuren in een kwestie van jaren.

## AGI zal leiden tot superintelligentie

Er valt iets voor te zeggen dat menselijk-competitieve of zelfs expert-competitieve algemene AI, zelfs als het autonoom is, beheersbaar zou kunnen zijn. Het zou ongelooflijk ontwrichtend kunnen zijn in alle manieren die hierboven besproken zijn, maar er zijn veel zeer slimme, handelende mensen in de wereld nu, en ze zijn min of meer beheersbaar.[^26]

Maar we zullen niet op ongeveer menselijk niveau blijven. De progressie daarvoorbij zal waarschijnlijk gedreven worden door dezelfde krachten die we al hebben gezien: competitieve druk tussen AI-ontwikkelaars die winst en macht zoeken, competitieve druk tussen AI-gebruikers die het zich niet kunnen veroorloven achter te blijven, en – het belangrijkste – AGI's eigen capaciteit om zichzelf te verbeteren.

In een proces dat we al hebben zien beginnen met minder krachtige systemen, zou AGI zelf in staat zijn verbeterde versies van zichzelf te bedenken en ontwerpen. Dit omvat hardware, software, neurale netwerken, tools, scaffolds, enz. Het zal, per definitie, beter zijn dan wij in het doen hiervan, dus we weten niet precies hoe het intelligentie zal bootstrappen. Maar dat hoeven we niet. Voor zover we nog invloed hebben in wat AGI doet, zouden we het er alleen maar om hoeven vragen, of het laten doen.

Er is geen menselijk-niveau barrière voor cognitie die ons zou kunnen beschermen tegen deze vicieuze cirkel.[^27]

De progressie van AGI naar superintelligentie is geen natuurwet; het zou nog steeds mogelijk zijn om de vicieuze cirkel in te tomen, vooral als AGI relatief gecentraliseerd is en voor zover het gecontroleerd wordt door partijen die geen druk voelen om tegen elkaar te racen. Maar mocht AGI wijdverspreid gepropageerd en zeer autonoom zijn, dan lijkt het bijna onmogelijk te voorkomen dat het besluit dat het krachtiger zou moeten zijn, en dan nog krachtiger.

## Wat gebeurt er als we (of AGI bouwt) superintelligentie bouwen

Om het botweg te zeggen, we hebben geen idee wat er zou gebeuren als we superintelligentie bouwen.[^28] Het zou acties ondernemen die we niet kunnen volgen of waarnemen om redenen die we niet kunnen begrijpen naar doelen die we niet kunnen bevatten. Wat we wel weten is dat het niet aan ons zal liggen.[^29]

De onmogelijkheid van het controleren van superintelligentie kan begrepen worden door steeds scherpere analogieën. Stel je eerst voor dat je CEO bent van een groot bedrijf. Er is geen manier dat je alles kunt volgen wat er gaande is, maar met de juiste setup van personeel kun je nog steeds betekenisvol het grote plaatje begrijpen, en beslissingen nemen. Maar stel slechts één ding: iedereen anders in het bedrijf opereert op honderd keer jouw snelheid. Kun je nog steeds bijhouden?

Met superintelligente AI zouden mensen iets "commanderen" dat niet alleen sneller is, maar opereert op niveaus van verfijning en complexiteit die ze niet kunnen begrijpen, veel meer data verwerkt dan ze zelfs kunnen bedenken. Deze incommensurabiliteit kan op formeel niveau gezet worden: [Ashby's wet van vereiste variëteit](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (en zie de gerelateerde ["goede regulator theorem"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)) stellen, ruwweg, dat elk controlesysteem evenveel knoppen en schijven moet hebben als het systeem dat gecontroleerd wordt vrijheidsgraden heeft.

Een persoon die een superintelligent AI-systeem controleert zou zijn zoals een varen die General Motors controleert: zelfs als "doe wat de varen wil" in de bedrijfsreglementen geschreven stond, zijn de systemen zo verschillend in snelheid en actiebereik dat "controle" simpelweg niet van toepassing is. (En hoe lang tot die vervelende regelgeving herschreven wordt?)[^30]

Omdat er nul voorbeelden zijn van planten die fortune 500 bedrijven controleren, zouden er exact nul voorbeelden zijn van mensen die superintelligenties controleren. Dit nadert een wiskundig feit.[^31] Als superintelligentie geconstrueerd werd – ongeacht hoe we daar kwamen – zou de vraag niet zijn of mensen het zouden kunnen controleren, maar of we zouden blijven bestaan, en zo ja, of we een goed en betekenisvol bestaan zouden hebben als individuen of als soort. Over deze existentiële vragen voor de mensheid zouden we weinig invloed hebben. Het menselijke tijdperk zou voorbij zijn.

## Conclusie: we moeten AGI niet bouwen

Er is een scenario waarin het bouwen van AGI goed zou kunnen aflopen voor de mensheid: het wordt zorgvuldig gebouwd, onder controle en ten voordele van de mensheid, bestuurd door wederzijdse overeenkomst van vele belanghebbenden,[^32] en voorkomen van evolueert naar oncontroleerbare superintelligentie.

*Dat scenario staat ons niet open onder huidige omstandigheden.* Zoals besproken in deze sectie zou, met zeer grote waarschijnlijkheid, ontwikkeling van AGI leiden tot een combinatie van:

- Massale maatschappelijke en beschavingsontwijking of -vernietiging;
- Conflict of oorlog tussen grote machten;
- Controleverlies door de mensheid *over* of *aan* krachtige AI-systemen;
- Vicieuze cirkel naar oncontroleerbare superintelligentie, en de irrelevantie of beëindiging van de menselijke soort.

Zoals een vroege fictieve voorstelling van AGI het stelde: de enige manier om te winnen is niet te spelen.

[^1]: De [EU AI-wet](https://artificialintelligenceact.eu/) is een significante wetgeving maar zou niet direct voorkomen dat een gevaarlijk AI-systeem ontwikkeld of ingezet wordt, of zelfs openlijk vrijgegeven, vooral in de VS. Een ander significant stuk beleid, het Amerikaanse presidentiële besluit over AI, is ingetrokken.

[^2]: Deze [Gallup-peiling](https://news.gallup.com/poll/1597/confidence-institutions.aspx) toont een sombere daling in vertrouwen in publieke instellingen sinds 2000 in de VS. Europese cijfers zijn gevarieerd en minder extreem, maar ook op een neerwaartse trend. Wantrouwen betekent niet strikt dat instellingen werkelijk *zijn* disfunctioneel, maar het is zowel een indicatie als een oorzaak.

[^3]: En grote ontwrichtingen die we nu onderschrijven – zoals uitbreiding van rechten naar nieuwe groepen – werden specifiek gedreven door mensen in een richting naar het beter maken van dingen.

[^4]: Laat me duidelijk zijn. Als je baan gedaan kan worden van achter een computer, met relatief weinig persoonlijke interactie met mensen buiten je organisatie, en geen juridische verantwoordelijkheid naar externe partijen inhoudt, zou het per definitie mogelijk zijn (en waarschijnlijk kostenbesparend) om je volledig te vervangen door een digitaal systeem. Robotica om veel fysiek werk te vervangen zal later komen – maar niet veel later zodra AGI robots begint te ontwerpen.

[^5]: Bijvoorbeeld, wat gebeurt er met ons gerechtelijk systeem als rechtszaken bijna gratis worden om in te dienen? Wat gebeurt er wanneer het omzeilen van beveiligingssystemen door social engineering goedkoop, gemakkelijk en risicoloos wordt?

[^6]: [Dit artikel](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/) beweert dat 10% van alle internetcontent al door AI gegenereerd is, en is Google's topresultaat (voor mij) voor de zoekopdracht "schattingen van welk deel van nieuwe internetcontent door AI gegenereerd is." Is het waar? Ik heb geen idee! Het citeert geen referenties en het werd niet geschreven door een persoon. Welk deel van nieuwe beelden geïndexeerd door Google, of Tweets, of opmerkingen op Reddit, of YouTube-video's worden gegenereerd door mensen? Niemand weet het – ik denk niet dat het een knowbaar getal is. En dit minder dan *twee jaar* na de komst van generatieve AI.

[^7]: Ook waard om toe te voegen is dat er "moreel" risico is dat we digitale wezens zouden kunnen creëren die kunnen lijden. Omdat we momenteel geen betrouwbare theorie van bewustzijn hebben die ons zou toestaan fysieke systemen te onderscheiden die wel en niet kunnen lijden, kunnen we dit theoretisch niet uitsluiten. Bovendien zijn AI-systemen' rapportages van hun gevoel van bewustzijn waarschijnlijk onbetrouwbaar met betrekking tot hun werkelijke ervaring (of niet-ervaring) van gevoel van bewustzijn.

[^8]: Technische oplossingen in dit veld van AI "alignment" zullen waarschijnlijk ook niet opgewassen zijn tegen de taak. In huidige systemen werken ze op enig niveau, maar zijn oppervlakkig en kunnen over het algemeen omzeild worden zonder significante inspanning; en zoals hieronder besproken hebben we geen echt idee hoe we dit moeten doen voor veel geavanceerdere systemen.

[^9]: Dergelijke AI-systemen komen mogelijk met ingebouwde waarborgen. Maar voor elk model met zoiets als huidige architectuur, als volledige toegang tot zijn gewichten beschikbaar is, kunnen veiligheidsmaatregelen weggehaald worden via aanvullende training of andere technieken. Dus het is virtueel gegarandeerd dat voor elk systeem met vangrails er ook een wijdverspreide beschikbare systeem zonder hen zal zijn. Inderdaad werd Meta's Llama 3.1 405B-model openlijk vrijgegeven met waarborgen. Maar *zelfs daarvoor* werd een "basis"model, zonder waarborgen, gelekt.

[^10]: Zou de markt deze risico's kunnen beheren zonder overheidsingsremming? Kort gezegd, nee. Er zijn zeker risico's die bedrijven sterk geïncentiveerd zijn te mitigeren. Maar veel andere kunnen en doen bedrijven externaliseren naar iedereen anders, en veel van de bovenstaande zitten in deze klasse: er zijn geen natuurlijke marktprikkels om massasurveillantie, waarheidsverval, machtsconcentratie, arbeidsontwickte, schadelijk politiek discours, enz. te voorkomen. Inderdaad hebben we dit alles gezien van hedendaagse tech, vooral sociale media, die in wezen ongereguleerd is gegaan. AI zou gewoon veel van dezelfde dynamieken enorm opjutten.

[^11]: OpenAI heeft waarschijnlijk meer gehoorzame modellen voor intern gebruik. Het is onwaarschijnlijk dat OpenAI een soort "achterdeur" heeft gebouwd zodat ChatGPT beter gecontroleerd kan worden door OpenAI zelf, omdat dit een verschrikkelijke beveiligingspraktijk zou zijn, en zeer exploiteerbaar zou zijn gegeven AI's ondoorzichtigheid en onvoorspelbaarheid.

[^12]: Ook van cruciaal belang: alignment of andere veiligheidsfeatures doen er alleen toe als ze werkelijk gebruikt worden in een AI-systeem. Systemen die openlijk vrijgegeven worden (d.w.z. waar modelgewichten en architectuur publiekelijk beschikbaar zijn) kunnen relatief gemakkelijk getransformeerd worden in systemen *zonder* die veiligheidsmaatregelen. Het open-vrijgeven van slimmer-dan-menselijke AGI-systemen zou verbazingwekkend roekeloos zijn, en het is moeilijk voor te stellen hoe menselijke controle of zelfs relevantie behouden zou worden in zo'n scenario. Er zou elke motivatie zijn, bijvoorbeeld, om krachtige zelf-reproducerende en zelf-onderhoudende AI-agenten los te laten met het doel geld te verdienen en het naar een cryptocurrency-wallet te sturen. Of een verkiezing te winnen. Of een regering omver te werpen. Zou "goede" AI dit kunnen helpen bevatten? Misschien – maar alleen door enorme autoriteit eraan te delegeren, wat leidt tot controleverlies zoals hieronder beschreven.

[^13]: Voor boeklengte exposities van het probleem zie bijv. *Superintelligence*, *The Alignment Problem*, en *Human-Compatible*. Voor een enorme hoop werk op verschillende technische niveaus door degenen die jaren hebben gezwoegd denkend over het probleem, kun je het [AI alignment forum](https://www.alignmentforum.org/) bezoeken. Hier is een [recente kijk](https://alignment.anthropic.com/2025/recommended-directions/) van Anthropic's alignment team op wat zij als onopgelost beschouwen.

[^14]: Dit is het ["rogue AI"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/) scenario. In principe zou het risico relatief klein kunnen zijn als het systeem nog steeds gecontroleerd kan worden door het af te sluiten; maar het scenario zou ook AI-bedrog, zelf-exfiltratie en reproductie, aggregatie van macht, en andere stappen kunnen omvatten die het moeilijk of onmogelijk zouden maken om dat te doen.

[^15]: Er is een zeer rijke literatuur over dit onderwerp, teruggaand naar vormgevende geschriften door [Steve Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), Nick Bostrom, en Eliezer Yudkowsky. Voor een boeklengte expositie zie [Human Compatible](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) door Stuart Russell; [hier](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/) is een korte en actuele primer.

[^16]: Dit erkennend, in plaats van vertragen om beter begrip te krijgen, zijn AGI-bedrijven gekomen met een ander plan: ze zullen AI het laten doen! Meer specifiek zullen ze AI *N* hebben om hen te helpen uitzoeken hoe AI *N+1* te aligneren, helemaal tot superintelligentie. Hoewel het benutten van AI om ons te helpen AI te aligneren veelbelovend klinkt, is er een sterk argument dat het simpelweg zijn conclusie als een premisse aanneemt, en in het algemeen een ongelooflijk riskante benadering is. Zie [hier](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us) voor enige discussie. Dit "plan" is er geen, en heeft niets ondergaan zoals de scrutiny die past bij de kernstrategie van hoe super-menselijke AI goed te laten gaan voor de mensheid.

[^17]: Immers hebben mensen, gebrekkig en eigenwillig als we zijn, ethische systemen ontwikkeld waarmee we minstens enkele andere soorten op Aarde goed behandelen. (Denk alleen niet aan die fabrieksboerderijen.)

[^18]: Er is, gelukkig, hier een ontsnapping: als de deelnemers ertoe komen te begrijpen dat ze bezig zijn met een zelfmoordrace in plaats van een winbare. Dit is wat gebeurde tegen het einde van de koude oorlog, toen de VS en USSR ertoe kwamen te beseffen dat vanwege nucleaire winter, zelfs een *onbeantwoorde* nucleaire aanval rampzalig zou zijn voor de aanvaller. Met het besef dat "nucleaire oorlog niet gewonnen kan worden en nooit gevochten mag worden" kwamen significante overeenkomsten over wapenvermindering – in wezen een einde aan de wapenwedloop.

[^19]: Oorlog, expliciet of impliciet.

[^20]: Escalatie, dan oorlog.

[^21]: Magisch denken.

[^22]: Ik heb ook een brug van een biljard dollar te verkopen.

[^23]: Dergelijke agenten zouden vermoedelijk "verkrijgen" verkiezen, met vernietiging als terugval; maar modellen beveiligen tegen zowel vernietiging *als* diefstal door krachtige naties is op zijn zachtst gezegd moeilijk, vooral voor private entiteiten.

[^24]: Voor een ander perspectief op de nationale veiligheidsrisico's van AGI, zie [dit RAND-rapport.](https://www.rand.org/pubs/perspectives/PEA3691-4.html)

[^25]: Misschien zouden we zo'n instelling kunnen bouwen! Er zijn voorstellen geweest voor een "CERN voor AI" en andere vergelijkbare initiatieven, waarbij AGI-ontwikkeling onder multilaterale globale controle is. Maar op het moment bestaat geen dergelijke instelling of is aan de horizon.

[^26]: En hoewel alignment zeer moeilijk is, mensen laten gedragen is nog moeilijker!

[^27]: Stel je een systeem voor dat 50 talen kan spreken, expertise heeft in alle academische vakken, een volledig boek in seconden kan lezen en al het materiaal onmiddellijk paraat heeft, en output produceert op tien keer menselijke snelheid. Eigenlijk hoef je het je niet voor te stellen: laad gewoon een huidig AI-systeem op. Deze zijn super-menselijk op vele manieren, en er is niets dat hen tegenhoudt van nog meer super-menselijk zijn in die en vele anderen.

[^28]: Dit is waarom dit een technologische "singulariteit" is genoemd, geleend van de fysica het idee dat men geen voorspellingen kan maken voorbij een singulariteit. Voorstanders van leunen *in* zo'n singulariteit willen misschien ook reflecteren dat in de fysica diezelfde soort singulariteiten degenen die erin gaan verscheuren en verpletteren.

[^29]: Het probleem werd uitgebreid geschetst in Bostrom's [*Superintelligence*](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834), en niets sindsdien heeft de kernboodschap significant veranderd. Voor een meer recente bundel die formele en wiskundige resultaten over oncontroleerbaarheid verzamelt zie Yampolskiy's [AI: Unexplainable, Unpredictable, Uncontrollable](https://www.amazon.com/Unexplainable-Unpredictable-Uncontrollable-Artificial-Intelligence/dp/103257626X)

[^30]: Dit maakt ook duidelijk waarom de huidige strategie van AI-bedrijven (iteratief AI de volgende krachtigste AI laten "aligneren") niet kan werken. Stel een varen, via de aangename van zijn bladeren, schakelt een eersteklasser in om voor het te zorgen. De eersteklasser schrijft enkele gedetailleerde instructies voor een tweedeklasser om te volgen, en een briefje dat hen overtuigt om dat te doen. De tweedeklasser doet hetzelfde voor een derdeklasser, enzovoort helemaal tot een universitaire afgestudeerde, een manager, een executive, en uiteindelijk de GM CEO. Zal GM dan "doen wat de varen wil"? Bij elke stap zou dit kunnen voelen alsof het werkt. Maar het allemaal bij elkaar puttend, zal het bijna precies werken in de mate waarin de CEO, Raad van Bestuur, en aandeelhouders van GM toevallig geven om kinderen en varens, en heeft weinig tot niets te maken met al die briefjes en sets instructies.

[^31]: Het karakter is niet zo verschillend van formele resultaten zoals Gödel's onvolledigheidstheorem of Turing's stoppingsargument in dat de notie van controle fundamenteel het uitgangspunt tegenspreekt: hoe kun je zinvol iets controleren dat je niet kunt begrijpen of voorspellen; maar als je superintelligentie zou kunnen begrijpen en voorspellen zou je superintelligent zijn. De reden dat ik "nadert" zeg is dat de formele resultaten niet zo grondig of getoetst zijn als in het geval van de zuivere wiskunde, en omdat ik hoop wil houden dat een zeer zorgvuldig geconstrueerde algemene intelligentie, gebruikmakend van totaal andere methoden dan degenen die momenteel gebruikt worden, enkele wiskundig bewijsbare veiligheidseigenschappen zou kunnen hebben, per het soort "gegarandeerd veilige" AI-programma dat hieronder besproken wordt.

[^32]: Op het moment zijn de meeste belanghebbenden – dat is, bijna de hele mensheid – buiten spel gezet in deze discussie. Dat is diepgaand verkeerd, en als ze niet uitgenodigd worden, zouden de vele, vele andere groepen die beïnvloed worden door AGI-ontwikkeling eisen binnengelaten te worden.
