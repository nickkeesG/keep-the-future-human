# Hoofdstuk 8 - Hoe je geen AGI bouwt

AGI is niet onvermijdelijk – vandaag staan we bij een kruispunt. Dit hoofdstuk presenteert een voorstel voor hoe we kunnen voorkomen dat het wordt gebouwd.

Als de weg die we momenteel bewandelen leidt tot het waarschijnlijke einde van onze beschaving, hoe veranderen we dan van koers?

Stel dat de wens om te stoppen met het ontwikkelen van AGI en superintelligentie wijdverspreid en krachtig zou zijn,[^1] omdat het algemeen begrip wordt dat AGI machtsabsorberend zou zijn in plaats van machtverlenend, en een diepgaand gevaar voor de samenleving en de mensheid. Hoe zouden we de Poorten sluiten?

Op dit moment kennen we slechts één manier om krachtige en algemene AI te *maken*, namelijk via werkelijk massale berekeningen van diepe neurale netwerken. Omdat dit ongelooflijk moeilijke en dure zaken zijn om te doen, is er een zekere zin waarin het *niet* doen ervan gemakkelijk is.[^2] Maar we hebben al de krachten gezien die richting AGI drijven, en de speltheoretische dynamiek die het voor elke partij zeer moeilijk maakt om eenzijdig te stoppen. Dus zou het een combinatie van interventie van buitenaf (dus regeringen) vereisen om bedrijven te stoppen, en akkoorden tussen regeringen om zichzelf te stoppen.[^3] Hoe zou dit eruit kunnen zien?

Het is nuttig om eerst onderscheid te maken tussen AI-ontwikkelingen die *voorkomen* of *verboden* moeten worden, en die welke *beheerd* moeten worden. Het eerste betreft primair ontsporing naar superintelligentie.[^4] Voor verboden ontwikkeling moeten definities zo scherp mogelijk zijn, en zowel verificatie als handhaving moeten praktisch zijn. Wat *beheerd* moet worden zijn algemene, krachtige AI-systemen – die we al hebben, en die veel grijze gebieden, nuances en complexiteit zullen hebben. Voor deze zijn sterke effectieve instellingen cruciaal.

We kunnen ook nuttig een onderscheid maken tussen kwesties die op internationaal niveau aangepakt moeten worden (inclusief tussen geopolitieke rivalen of tegenstanders)[^5] en die welke individuele jurisdicties, landen, of groepen van landen kunnen beheren. Verboden ontwikkeling valt grotendeels in de "internationale" categorie, omdat een lokaal verbod op de ontwikkeling van een technologie over het algemeen kan worden omzeild door van locatie te veranderen.[^6]

Ten slotte kunnen we hulpmiddelen in de gereedschapskist overwegen. Er zijn er veel, inclusief technische hulpmiddelen, soft law (standaarden, normen, enz.), hard law (regelgeving en vereisten), aansprakelijkheid, marktprikkels, enzovoort. Laten we speciale aandacht besteden aan een die specifiek is voor AI.

## Rekenkracht-beveiliging en -governance

Een kernhulpmiddel bij het besturen van krachtige AI zal de hardware zijn die het vereist. Software verspreidt zich gemakkelijk, heeft bijna nul marginale productiekosten, kruist grenzen triviaal, en kan instant worden aangepast; geen van deze eigenschappen gelden voor hardware. Maar zoals we hebben besproken, zijn enorme hoeveelheden van deze "rekenkracht" noodzakelijk tijdens zowel training van AI-systemen als tijdens inferentie om de meest capabele systemen te bereiken. Rekenkracht kan gemakkelijk worden gekwantificeerd, verantwoord en geauditeerd, met relatief weinig dubbelzinnigheid zodra goede regels hiervoor zijn ontwikkeld. Het meest cruciaal is dat grote hoeveelheden berekening, net als verrijkt uranium, een zeer schaarse, dure en moeilijk te produceren hulpbron zijn. Hoewel computerchips alomtegenwoordig zijn, is de hardware die vereist is voor AI duur en enorm moeilijk te vervaardigen.[^7]

Wat AI-gespecialiseerde chips veel *beter* beheersbaar maakt als schaarse hulpbron dan uranium is dat ze hardware-gebaseerde beveiligingsmechanismen kunnen bevatten. De meeste moderne telefoons, en sommige laptops, hebben gespecialiseerde on-chip hardwarefeatures die hen in staat stellen om ervoor te zorgen dat ze alleen goedgekeurde besturingssysteemsoftware en updates installeren, dat ze gevoelige biometrische gegevens op het apparaat behouden en beschermen, en dat ze nutteloos kunnen worden gemaakt voor iedereen behalve hun eigenaar als ze verloren of gestolen worden. Gedurende de afgelopen jaren zijn dergelijke hardwarebeveiligingsmaatregelen goed gevestigd en wijdverspreid aangenomen geworden, en over het algemeen vrij veilig bewezen.

Het sleutelelement van deze features is dat ze hardware en software samenbinden met behulp van cryptografie.[^8] Dat wil zeggen, alleen het hebben van een bepaald stuk computerhardware betekent niet dat een gebruiker er alles mee kan doen wat ze willen door verschillende software toe te passen. En deze binding biedt ook krachtige beveiliging omdat veel aanvallen een inbreuk op *hardware* in plaats van alleen *software*-beveiliging zouden vereisen.

Verschillende recente rapporten (bijvoorbeeld van [GovAI en medewerkers](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips), en [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) hebben erop gewezen dat vergelijkbare hardwarefeatures ingebed in geavanceerde AI-relevante computerhardware een extreem nuttige rol zouden kunnen spelen in AI-beveiliging en -governance. Ze maken een aantal functies mogelijk die beschikbaar zijn voor een "governeur"[^9] die men misschien niet zou raden dat beschikbaar of zelfs mogelijk waren. Als enkele sleutelvoorbeelden:

- *Geolocatie*: Systemen kunnen zo worden opgezet dat chips een bekende locatie hebben, en anders kunnen handelen (of volledig kunnen worden uitgeschakeld) gebaseerd op locatie.[^10]
- *Toegestane verbindingen*: elke chip kan worden geconfigureerd met een hardware-afgedwongen lijst van toegestane andere specifieke chips waarmee het kan netwerken, en kan niet verbinden met chips die niet op deze lijst staan.[^11] Dit kan de grootte van communicatieve clusters van chips beperken.[^12]
- *Gemeten inferentie of training (en automatische uitschakelaar)*: Een governeur kan alleen een bepaalde hoeveelheid training of inferentie (in tijd, of FLOP, of mogelijk tokens) licentiëren om door een gebruiker te worden uitgevoerd, waarna nieuwe toestemming vereist is. Als de stappen klein zijn, dan is relatief continue herlicentie van een model vereist. Het model kan dan worden "uitgeschakeld" door simpelweg dit licentiesignaal te onthouden.[^13]
- *Snelheidslimiet*: Een model wordt verhinderd om te draaien met hogere inferentiesnelheid dan een limiet die wordt bepaald door een governeur of anderszins. Dit zou kunnen worden geïmplementeerd via een beperkte set toegestane verbindingen, of door meer geavanceerde middelen.
- *Bevestigde training*: Een trainingsprocedure kan cryptografisch veilig bewijs opleveren dat een bepaalde set codes, gegevens, en hoeveelheid rekenkrachtgebruik werden toegepast in de generatie van het model.

## Hoe je geen superintelligentie bouwt: wereldwijde limieten op training en inferentie rekenkracht

Met deze overwegingen – vooral betreffende berekening – kunnen we bespreken hoe de Poorten naar kunstmatige superintelligentie te sluiten; we zullen dan keren naar het voorkomen van volledige AGI, en het beheren van AI-modellen terwijl ze menselijke capaciteit in verschillende aspecten benaderen en overstijgen.

Het eerste ingredient is natuurlijk het begrip dat superintelligentie niet controleerbaar zou zijn, en dat de gevolgen ervan fundamenteel onvoorspelbaar zijn. Ten minste China en de VS moeten onafhankelijk beslissen, voor dit of andere doeleinden, om geen superintelligentie te bouwen.[^14] Dan is een internationaal akkoord tussen hen en anderen, met een sterk verificatie- en handhavingsmechanisme, nodig om alle partijen te verzekeren dat hun rivalen niet afvallen en besluiten om met de dobbelstenen te gooien.

Om verifieerbaar en afdwingbaar te zijn moeten de limieten harde limieten zijn, en zo ondubbelzinnig mogelijk. Dit lijkt een virtueel onmogelijk probleem: het beperken van de capaciteiten van complexe software met onvoorspelbare eigenschappen, wereldwijd. Gelukkig is de situatie veel beter dan dit, omdat precies datgene wat geavanceerde AI mogelijk heeft gemaakt – een enorme hoeveelheid rekenkracht – veel, veel gemakkelijker te controleren is. Hoewel het misschien nog steeds enkele krachtige en gevaarlijke systemen zou toestaan, kan *ontspoorde superintelligentie* waarschijnlijk worden voorkomen door een harde limiet op de hoeveelheid berekening die in een neuraal netwerk gaat, samen met een snelheidslimiet op de hoeveelheid inferentie die een AI-systeem (van verbonden neurale netwerken en andere software) kan uitvoeren. Een specifieke versie hiervan wordt hieronder voorgesteld.

Het zou kunnen lijken dat het plaatsen van harde wereldwijde limieten op AI-berekening enorme niveaus van internationale coördinatie en indringende, privacy-vernietigende surveillance zou vereisen. Gelukkig zou dat niet het geval zijn. De extreem [krappe en gebottleneckte toeleveringsketen](https://arxiv.org/abs/2402.08797) zorgt ervoor dat zodra een limiet wettelijk is vastgesteld (of door wet of uitvoerend besluit), verificatie van naleving van die limiet alleen betrokkenheid en medewerking van een handvol grote bedrijven zou vereisen.[^15]

Een dergelijk plan heeft een aantal zeer wenselijke eigenschappen. Het is minimaal indringend in de zin dat alleen een paar grote bedrijven vereisten opgelegd krijgen, en alleen vrij significante clusters van berekening zouden worden bestuurd. De relevante chips bevatten al de hardwarecapaciteiten die nodig zijn voor een eerste versie.[^16] Zowel implementatie als handhaving steunen op standaard juridische beperkingen. Maar deze worden ondersteund door gebruiksvoorwaarden van de hardware en door hardwarecontroles, wat handhaving enorm vereenvoudigt en valsspelen door bedrijven, private groepen, of zelfs landen verhindert. Er is ruim precedent voor hardwarebedrijven die externe beperkingen op hun hardwaregebruik plaatsen, en bepaalde capaciteiten extern vergrendelen/ontgrendelen,[^17] inclusief zelfs in krachtige CPU's in datacenters.[^18] Zelfs voor de vrij kleine fractie van hardware en organisaties die getroffen worden, zou het toezicht beperkt kunnen blijven tot telemetrie, zonder directe toegang tot gegevens of modellen zelf; en de software hiervoor zou open kunnen zijn voor inspectie om aan te tonen dat er geen aanvullende gegevens worden geregistreerd. Het schema is internationaal en coöperatief, en vrij flexibel en uitbreidbaar. Omdat de limiet hoofdzakelijk op hardware ligt in plaats van software, is het relatief agnostisch ten aanzien van hoe AI-software-ontwikkeling en -implementatie plaatsvindt, en is het compatibel met verschillende paradigma's inclusief meer "gedecentraliseerde" of "publieke" AI gericht op het bestrijden van AI-gedreven machtconcentratie.

Een op berekening gebaseerde Poortsluiting heeft ook nadelen. Ten eerste is het verre van een volledige oplossing voor het probleem van AI-governance in het algemeen. Ten tweede, naarmate computerhardware sneller wordt, zou het systeem "meer en meer" hardware in kleinere en kleinere clusters (of zelfs individuele GPU's) "vangen".[^19] Het is ook mogelijk dat door algoritmische verbeteringen een nog lagere berekeningslimiet mettertijd noodzakelijk zou zijn,[^20] of dat berekeningshoeveelheid grotendeels irrelevant wordt en het sluiten van de Poort in plaats daarvan een meer gedetailleerd risico-gebaseerd of capaciteit-gebaseerd governanceregime voor AI zou vereisen. Ten derde, ongeacht de garanties en het kleine aantal getroffen entiteiten, is zo'n systeem gedoemd om tegenstand te creëren betreffende privacy en surveillance, onder andere zorgen.[^21]

Natuurlijk zal het ontwikkelen en implementeren van een rekenkracht-beperkend governanceschema in een korte tijdsperiode behoorlijk uitdagend zijn. Maar het is absoluut haalbaar.

## A-G-I: Het drievoudige snijpunt als basis van risico, en van beleid

Laten we nu keren naar AGI. Harde lijnen en definities zijn hier moeilijker, omdat we zeker intelligentie hebben die kunstmatig en algemeen is, en volgens geen bestaande definitie zal iedereen het eens zijn of wanneer het bestaat. Bovendien is een rekenkracht- of inferentielimiet een enigszins bot instrument (rekenkracht als proxy voor capaciteit, wat dan een proxy is voor risico) dat – tenzij het behoorlijk laag is – waarschijnlijk geen AGI zal voorkomen die krachtig genoeg is om sociale of beschavingsverstoringen of acute risico's te veroorzaken.

Ik heb beargumenteerd dat de meest acute risico's voortkomen uit het drievoudige snijpunt van zeer hoge capaciteit, hoge autonomie, en grote algemeenheid. Dit zijn de systemen die – als ze überhaupt worden ontwikkeld – met enorme zorg moeten worden beheerd. Door stringente standaarden te creëren (door aansprakelijkheid en regelgeving) voor systemen die alle drie eigenschappen combineren, kunnen we AI-ontwikkeling kanaliseren naar veiligere alternatieven.

Net als bij andere industrieën en producten die potentieel consumenten of het publiek kunnen schaden, vereisen AI-systemen zorgvuldige regelgeving door effectieve en bevoegde overheidsinstanties. Deze regelgeving moet de inherente risico's van AGI erkennen, en voorkomen dat onaanvaardbaar riskante krachtige AI-systemen worden ontwikkeld.[^22]

Echter, grootschalige regelgeving, vooral met echte tanden die zeker door de industrie zullen worden tegengewerkt,[^23] kost tijd[^24] evenals politieke overtuiging dat het noodzakelijk is.[^25] Gezien het tempo van vooruitgang, kan dit meer tijd kosten dan we beschikbaar hebben.

Op een veel snellere tijdschaal en terwijl regulatoire maatregelen worden ontwikkeld, kunnen we bedrijven de noodzakelijke prikkels geven om (a) af te zien van zeer hoog-risico activiteiten en (b) uitgebreide systemen te ontwikkelen voor het beoordelen en mitigeren van risico, door het verduidelijken en verhogen van aansprakelijkheidsniveaus voor de meest gevaarlijke systemen. Het idee zou zijn om de allerhoogste niveaus van aansprakelijkheid op te leggen – strikt en in sommige gevallen persoonlijk crimineel – voor systemen in het drievoudige snijpunt van hoge autonomie-algemeenheid-intelligentie, maar "veilige havens" te bieden naar meer typische fout-gebaseerde aansprakelijkheid voor systemen waarin een van die eigenschappen ontbreekt of gegarandeerd beheersbaar is. Dat wil zeggen, bijvoorbeeld, een "zwak" systeem dat algemeen en autonoom is (zoals een capabele en betrouwbare maar beperkte persoonlijke assistent) zou onderhevig zijn aan lagere aansprakelijkheidsniveaus. Evenzo zou een smal en autonoom systeem zoals een zelfrijdende auto nog steeds onderhevig zijn aan de significante regelgeving die het al heeft, maar niet verhoogde aansprakelijkheid. Hetzelfde geldt voor een zeer capabel en algemeen systeem dat "passief" is en grotendeels niet in staat tot onafhankelijke actie. Systemen die *twee* van de drie eigenschappen missen zijn nog beheersbaarder en veilige havens zouden nog gemakkelijker te claimen zijn. Deze benadering weerspiegelt hoe we omgaan met andere potentieel gevaarlijke technologieën:[^26] hogere aansprakelijkheid voor gevaarlijkere configuraties creëert natuurlijke prikkels voor veiligere alternatieven.

De standaarduitkomst van dergelijke hoge niveaus van aansprakelijkheid, die AGI-risico *internaliseren* bij bedrijven in plaats van het af te schuiven op het publiek, is waarschijnlijk (en hopelijk!) dat bedrijven simpelweg geen volledige AGI ontwikkelen totdat en tenzij ze het echt betrouwbaar, veilig en controleerbaar kunnen maken gegeven dat *hun eigen leiderschap* de partijen zijn die risico lopen. (Voor het geval dit niet voldoende is, zou de wetgeving die aansprakelijkheid verduidelijkt ook expliciet ruimte moeten bieden voor gerechtelijke bevelen, d.w.z. een rechter die een stop beveelt, voor activiteiten die duidelijk in de gevarenzone zijn en aantoonbaar een publiek risico vormen.) Naarmate regelgeving van kracht wordt, kan het naleven van regelgeving de veilige haven worden, en kunnen de veilige havens van lage autonomie, nauwheid, of zwakte van AI-systemen omvormen tot relatief lichtere regulatoire regimes.

## Kernbepalingen van een Poortsluiting

Met bovenstaande discussie in gedachten, biedt deze sectie voorstellen voor kernbepalingen die verbod op volledige AGI en superintelligentie zouden implementeren en handhaven, en beheer van menselijk-competitieve of expert-competitieve algemene AI nabij de volledige AGI-drempel.[^27] Het heeft vier sleutelelementen: 1) rekenkracht-boekhouding en toezicht, 2) rekenkrachtlimieten in training en werking van AI, 3) een aansprakelijkheidskader, en 4) gelaagde veiligheids- en beveiligingstandaarden gedefinieerd die harde regulatoire vereisten bevatten. Deze worden beknopt beschreven hieronder, met verdere details of implementatievoorbeelden gegeven in drie bijbehorende tabellen. Belangrijk is op te merken dat dit verre van alles is wat nodig zal zijn om geavanceerde AI-systemen te besturen; hoewel ze aanvullende veiligheids- en beveiligingsvoordelen zullen hebben, zijn ze gericht op het sluiten van de Poort naar intelligentie-ontsporing, en het heroriënteren van AI-ontwikkeling in een betere richting.

### 1. Rekenkracht-boekhouding, en transparantie

- Een standaardorganisatie (bijvoorbeeld NIST in de VS gevolgd door ISO/IEEE internationaal) zou een gedetailleerde technische standaard moeten codificeren voor de totale rekenkracht gebruikt in training en werking van AI-modellen, in FLOP, en de snelheid in FLOP/s waarmee ze opereren. Details voor hoe dit eruit zou kunnen zien worden gegeven in Bijlage A.[^28]
- Een vereiste – ofwel door nieuwe wetgeving of onder bestaande autoriteit[^29] – zou door jurisdicties waarin grootschalige AI-training plaatsvindt moeten worden opgelegd om de totale FLOP gebruikt in training en werking van alle modellen boven een drempel van 10<sup>25</sup> FLOP of 10<sup>18</sup> FLOP/s te berekenen en te rapporteren aan een regulatoir orgaan of andere instantie.[^30]
- Deze vereisten zouden geleidelijk moeten worden ingevoerd, aanvankelijk goed-gedocumenteerde te goeder trouw schattingen op kwartaalbasis vereisend, met latere fasen die progressief hogere standaarden vereisen, tot cryptografisch bevestigde totale FLOP en FLOP/s verbonden aan elke model*output*.
- Deze rapporten zouden moeten worden aangevuld met goed-gedocumenteerde schattingen van marginale energie- en financiële kosten gebruikt bij het genereren van elke AI-output.

Rationale: Deze goed-berekende en transparant gerapporteerde getallen zouden de basis vormen voor training- en werkingslimieten, evenals een veilige haven van hogere aansprakelijkheidsmaatregelen (zie Bijlagen C en D).

### 2. Training en werking rekenkrachtlimieten

- Jurisdicties die AI-systemen hosten zouden een harde limiet moeten opleggen op de totale rekenkracht die in elke AI-model output gaat, beginnend bij 10<sup>27</sup> FLOP[^31] en aanpasbaar waar gepast.
- Jurisdicties die AI-systemen hosten zouden een harde limiet moeten opleggen op de rekenkrachtsnelheid van AI-model outputs, beginnend bij 10<sup>20</sup> FLOP/s en aanpasbaar waar gepast.

Rationale: Totale berekening is, hoewel zeer imperfect, een proxy voor AI-capaciteit (en risico) die concreet meetbaar en verifieerbaar is, dus biedt een harde achtergrond voor het beperken van capaciteiten. Een concreet implementatievoorstel wordt gegeven in Bijlage B.

### 3. Verhoogde aansprakelijkheid voor gevaarlijke systemen

- Creatie en werking[^32] van een geavanceerd AI-systeem dat zeer algemeen, capabel, en autonoom is, zou wettelijk via wetgeving verduidelijkt moeten worden als onderhevig aan strikte, gezamenlijke en hoofdelijke, in plaats van enkelvoudige fout-gebaseerde, aansprakelijkheid.[^33]
- Een juridisch proces zou beschikbaar moeten zijn om bevestigende veiligheidscases te maken, die veilige haven zouden verlenen van strikte aansprakelijkheid voor systemen die klein zijn (in termen van rekenkracht), zwak, smal, passief, of die voldoende veiligheids-, beveiligings-, en controleerbaarheidsgaranties hebben.
- Een expliciet pad en set condities voor gerechtelijke bevelen om AI-training en inferentie-activiteiten die een publiek gevaar vormen te stoppen zou geschetst moeten worden.

Rationale: AI-systemen kunnen niet verantwoordelijk worden gehouden, dus moeten we menselijke individuen en organisaties verantwoordelijk houden voor schade die ze veroorzaken (aansprakelijkheid).[^34] Oncontroleerbare AGI is een bedreiging voor samenleving en beschaving en zou bij afwezigheid van een veiligheidscase als "abnormaal gevaarlijk" moeten worden beschouwd. Het leggen van de verantwoordelijkheid bij ontwikkelaars om te tonen dat krachtige modellen veilig genoeg zijn om niet als "abnormaal gevaarlijk" te worden beschouwd prikkelt veilige ontwikkeling, samen met transparantie en documentatie om die veilige havens te claimen. Regelgeving kan dan schade voorkomen waar afschrikking door aansprakelijkheid onvoldoende is. Ten slotte zijn AI-ontwikkelaars al aansprakelijk voor schade die ze veroorzaken, dus het wettelijk verduidelijken van aansprakelijkheid voor de meest riskante systemen kan onmiddellijk worden gedaan, zonder dat zeer gedetailleerde standaarden ontwikkeld hoeven te worden; deze kunnen dan mettertijd ontwikkelen. Details worden gegeven in Bijlage C.

### 4. Veiligheidsregelgeving voor AI

Een regulatoir systeem dat grootschalige acute risico's van AI adresseert zal minimaal vereisen:

- De identificatie of creatie van een geschikte set regulatoire organen, waarschijnlijk een nieuwe instantie;
- Een uitgebreid risicoboordelingskader;[^35]
- Een kader voor bevestigende veiligheidscases, deels gebaseerd op het risicoboordelingskader, te maken door ontwikkelaars, en voor auditing door *onafhankelijke* groepen en instanties;
- Een gelaagd licentiesysteem, met lagen die niveaus van capaciteit volgen.[^36] Licenties zouden worden verleend op basis van veiligheidscases en audits, voor ontwikkeling en implementatie van systemen. Vereisten zouden variëren van notificatie aan de onderkant, tot kwantitatieve veiligheids-, beveiligings-, en controleerbaarheidsgaranties vóór ontwikkeling, aan de bovenkant. Deze zouden release van systemen voorkomen totdat ze als veilig zijn aangetoond, en de ontwikkeling van intrinsiek onveilige systemen verbieden. Bijlage D biedt een voorstel voor wat dergelijke veiligheids- en beveiligingstandaarden zouden kunnen inhouden.
- Akkoorden om dergelijke maatregelen naar het internationale niveau te brengen, inclusief internationale organen om normen en standaarden te harmoniseren, en potentieel internationale instanties om veiligheidscases te beoordelen.

Rationale: Uiteindelijk is aansprakelijkheid niet het juiste mechanisme voor het voorkomen van grootschalig risico voor het publiek van een nieuwe technologie. Uitgebreide regelgeving, met bevoegde regulatoire organen, zal nodig zijn voor AI net zoals voor elke andere grote industrie die risico vormt voor het publiek.[^37]

Regelgeving gericht op het voorkomen van andere doordringende maar minder acute risico's zal waarschijnlijk variëren in vorm van jurisdictie tot jurisdictie. Het cruciale is om te voorkomen dat de AI-systemen worden ontwikkeld die zo riskant zijn dat deze risico's onbeheersbaar zijn.

## En dan?

Gedurende het komende decennium, naarmate AI meer doordrindend wordt en de kerntechnologie vordert, zullen twee sleutelzaken waarschijnlijk gebeuren. Ten eerste zal regelgeving van bestaande krachtige AI-systemen moeilijker worden, maar nog noodzakelijker. Het is waarschijnlijk dat ten minste sommige maatregelen die grootschalige veiligheidsrisico's adresseren akkoord op internationaal niveau zullen vereisen, met individuele jurisdicties die regels handhaven gebaseerd op internationale akkoorden.

Ten tweede zullen training- en werking rekenkrachtlimieten moeilijker te handhaven worden naarmate hardware goedkoper en kostenefficiënter wordt; ze kunnen ook minder relevant worden (of moeten nog strakker zijn) met vooruitgang in algoritmes en architecturen.

Dat het controleren van AI moeilijker wordt betekent niet dat we moeten opgeven! Het implementeren van het plan geschetst in dit essay zou ons zowel waardevolle tijd als cruciale controle over het proces geven dat ons in een véél, véél betere positie zou zetten om het existentiële risico van AI voor onze samenleving, beschaving en soort te vermijden.

Op nog langere termijn zullen er keuzes zijn te maken over wat we toestaan. We kunnen er nog voor kiezen om een vorm van echt controleerbare AGI te creëren, voor zover dit mogelijk blijkt. Of we kunnen besluiten dat het runnen van de wereld beter aan de machines kan worden overgelaten, als we onszelf ervan kunnen overtuigen dat ze er een betere baan van zullen maken, en ons goed zullen behandelen. Maar dit zouden beslissingen moeten zijn genomen met diep wetenschappelijk begrip van AI in de hand, en na betekenisvolle wereldwijde inclusieve discussie, niet in een race tussen techgiganten met het grootste deel van de mensheid volledig onbetrokken en onbewust.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) Samenvatting van A-G-I en superintelligentie-governance via aansprakelijkheid en regelgeving. Aansprakelijkheid is het hoogst, en regelgeving het sterkst, bij het drievoudige snijpunt van Autonomie, Algemeenheid, en Intelligentie. Veilige havens van strikte aansprakelijkheid en sterke regelgeving kunnen worden verkregen via bevestigende veiligheidscases die aantonen dat een systeem zwak en/of smal en/of passief is. Limieten op totale Training Rekenkracht en Inferentie Rekankrachtsnelheid, geverifieerd en afgedwongen wettelijk en gebruik makend van hardware- en cryptografische beveiligingsmaatregelen, ondersteunen veiligheid door volledige AGI te vermijden en superintelligentie effectief te verbieden.

[^1]: Zeer waarschijnlijk zal de verspreiding van dit besef ofwel intense inspanning van onderwijs- en belangenbehartigingsgroepen die deze zaak maken vereisen, of een behoorlijk significante AI-veroorzaakte ramp. We kunnen hopen dat het het eerste zal zijn.

[^2]: Paradoxaal genoeg zijn we gewend dat de Natuur onze technologie beperkt door het zeer moeilijk te ontwikkelen te maken, vooral wetenschappelijk. Maar dat is niet langer het geval voor AI: de belangrijkste wetenschappelijke problemen blijken gemakkelijker te zijn dan verwacht. We kunnen er niet op rekenen dat de Natuur ons van onszelf redt hier – we zullen dat zelf moeten doen.

[^3]: Waar precies stoppen we met het ontwikkelen van nieuwe systemen? Hier zouden we een voorzorgsprincipe moeten aannemen. Zodra een systeem wordt gedeployed, en vooral zodra dat niveau van systeemcapaciteit prolifereert, is het uiterst moeilijk om terug te gaan. En als een systeem wordt *ontwikkeld* (vooral met grote kosten en inspanning), zal er enorme druk zijn om het te gebruiken of te deployen, en verleiding voor het om gelekt of gestolen te worden. Systemen ontwikkelen en *dan* beslissen of ze diep onveilig zijn is een gevaarlijke weg.

[^4]: Het zou ook verstandig zijn om AI-ontwikkeling te verbieden die intrinsiek gevaarlijk is, zoals zelf-replicerende en evoluerende systemen, die ontworpen zijn om te ontsnappen uit opsluiting, die autonoom zichzelf kunnen verbeteren, opzettelijk misleidende en kwaadaardige AI, enz.

[^5]: Merk op dat dit niet noodzakelijk betekent *afgedwongen* op internationaal niveau door een soort wereldwijd orgaan: in plaats daarvan zouden soevereine naties overeengekomen regels kunnen afdwingen, zoals in vele verdragen.

[^6]: Zoals we hieronder zullen zien, zou de aard van AI-berekening iets van een hybride toestaan; maar internationale samenwerking zal nog steeds nodig zijn.

[^7]: Bijvoorbeeld, de machines die vereist zijn om AI-relevante chips te etsen worden gemaakt door slechts één firma, ASML (ondanks vele andere pogingen om dit te doen), de overgrote meerderheid van relevante chips wordt vervaardigd door één firma, TSMC (ondanks anderen die proberen te concurreren), en het ontwerp en de constructie van hardware van die chips gedaan door slechts een paar inclusief NVIDIA, AMD, en Google.

[^8]: Het belangrijkste is dat elke chip een unieke en ontoegankelijke cryptografische private sleutel houdt die het kan gebruiken om dingen te "ondertekenen".

[^9]: Standaard zou dit het bedrijf zijn dat de chips verkoopt, maar andere modellen zijn mogelijk en potentieel nuttig.

[^10]: Een governeur kan de locatie van een chip vaststellen door de uitwisseling van ondertekende berichten ermee te timen: de eindige snelheid van het licht vereist dat de chip binnen een gegeven straal *r* van een "station" is als het een ondertekend bericht kan terugsturen in een tijd minder dan *r* / *c*, waar *c* de lichtsnelheid is. Met gebruik van meerdere stations, en enig begrip van netwerkkarakteristieken, kan de locatie van de chip worden bepaald. De schoonheid van deze methode is dat het grootste deel van zijn beveiliging wordt geleverd door de wetten van de fysica. Andere methoden zouden GPS, inertiële tracking, en vergelijkbare technologieën kunnen gebruiken.

[^11]: Alternatief zouden paren chips elkaar alleen mogen bereiken via expliciete toestemming van een governeur.

[^12]: Dit is cruciaal omdat tenminste momenteel zeer hoge bandbreedte verbinding tussen chips nodig is om grote AI-modellen erop te trainen.

[^13]: Dit zou ook kunnen worden opgezet om ondertekende berichten van *N* van *M* verschillende governeurs te vereisen, waardoor meerdere partijen governance kunnen delen.

[^14]: Dit is verre van ongekend – bijvoorbeeld hebben militairen geen legers van gekloonde of genetisch gemanipuleerde supersoldaten ontwikkeld, hoewel dit waarschijnlijk technologisch mogelijk is. Maar ze hebben ervoor *gekozen* om dit niet te doen, in plaats van door anderen te worden verhinderd. De staat van dienst is niet geweldig voor grote wereldmachten die worden verhinderd een technologie te ontwikkelen die ze sterk wensen te ontwikkelen.

[^15]: Met een paar opvallende uitzonderingen (in het bijzonder NVIDIA) is de AI-gespecialiseerde hardware een relatief klein deel van deze bedrijven' totale zakelijke en inkomstenmodel. Bovendien is de kloof tussen hardware gebruikt in geavanceerde AI en "consumenten-grade" hardware significant, dus zouden de meeste consumenten van computerhardware grotendeels onaangetast blijven.

[^16]: Voor meer gedetailleerde analyse, zie de recente rapporten van [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) en [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). Deze focussen op technische haalbaarheid, vooral in de context van Amerikaanse exportcontroles die proberen andere landen' capaciteit in hoogwaardige berekening te beperken; maar dit heeft duidelijke overlap met de wereldwijde beperking die hier wordt voorgesteld.

[^17]: Apple-apparaten, bijvoorbeeld, worden op afstand en veilig vergrendeld wanneer ze als verloren of gestolen worden gerapporteerd, en kunnen op afstand worden heractiveerd. Dit steunt op dezelfde hardwarebeveiligingsfeatures die hier besproken worden.

[^18]: Zie bijvoorbeeld IBM's [capacity on demand](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) aanbod, Intel's [Intel on demand.](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html), en Apple's [private cloud compute](https://security.apple.com/blog/private-cloud-compute/).

[^19]: [Deze studie](https://epochai.org/trends#hardware-trends-section) toont dat historisch dezelfde prestatie is bereikt met ongeveer 30% minder dollars per jaar. Als deze trend doorzet, kan er significante overlap zijn tussen AI en "consumenten" chipgebruik, en in het algemeen zou de hoeveelheid benodigde hardware voor krachtige AI-systemen ongemakkelijk klein kunnen worden.

[^20]: Per [dezelfde studie](https://epochai.org/trends#hardware-trends-section), heeft gegeven prestatie op beeldherkenning 2.5x minder berekening per jaar gevergd. Als dit ook zou gelden voor de meest capabele AI-systemen, zou een berekeningslimiet niet lang een nuttige zijn.

[^21]: In het bijzonder, op landenniveau ziet dit er veel uit zoals een nationalisatie van berekening, in dat de regering veel controle zou hebben over hoe rekenkracht wordt gebruikt. Echter, voor degenen die zich zorgen maken over overheidsbetrokkenheid, lijkt dit veel veiliger dan en verkieslijk boven de krachtigste AI-software *zelf* die wordt genationaliseerd via een fusie tussen grote AI-bedrijven en nationale regeringen, zoals sommigen beginnen te pleiten.

[^22]: Een grote regulatoire stap in Europa werd genomen met de 2024 passage van de [EU AI Act.](https://artificialintelligenceact.eu/) Het classificeert AI op risico: het verbieden van onaanvaardbare systemen, het reguleren van hoog-risico systemen, en het opleggen van transparantieregels, of geen maatregelen, op laag-risico systemen. Het zal sommige AI-risico's significant verminderen, en AI-transparantie verhogen zelfs voor Amerikaanse firmas, maar heeft twee sleuteltekortkomingen. Ten eerste, beperkt bereik: hoewel het van toepassing is op elk bedrijf dat AI in de EU levert, is handhaving over Amerikaanse firmas zwak, en militaire AI is vrijgesteld. Ten tweede, hoewel het GPAI dekt, faalt het om AGI of superintelligentie als onaanvaardbare risico's te herkennen of hun ontwikkeling te voorkomen—alleen hun EU-implementatie. Als gevolg doet het weinig om de risico's van AGI of superintelligentie in te dammen.

[^23]: Bedrijven vertegenwoordigen vaak dat ze voorstander zijn van redelijke regelgeving. Maar op de een of andere manier lijken ze bijna altijd tegen elke *specifieke* regelgeving te zijn; getuige de strijd over de behoorlijk lage-touch SB1047, waar [de meeste AI-bedrijven publiekelijk of privé tegen waren.](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/)

[^24]: Het was ongeveer 3½ jaar vanaf het moment dat de EU AI act werd voorgesteld totdat het van kracht ging.

[^25]: Het wordt soms uitgedrukt dat het "te vroeg" is om AI te gaan reguleren. Gezien de laatste opmerking lijkt dat nauwelijks waarschijnlijk. Een andere uitgedrukte zorg is dat regelgeving "innovatie zou schaden." Maar goede regelgeving verandert alleen de richting, niet hoeveelheid, van innovatie.

[^26]: Een interessant precedent is in het transport van gevaarlijke materialen, die zouden kunnen ontsnappen en schade veroorzaken. Hier hebben [regelgeving](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) en [jurisprudentie](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) strikte aansprakelijkheid vastgesteld voor zeer gevaarlijke materialen zoals explosieven, benzine, gifstoffen, infectieuze agenten, en radioactief afval. Andere voorbeelden zijn [waarschuwingen op farmaceutica](https://www.medicalnewstoday.com/articles/boxed-warnings), [klassen van medische apparaten,](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification) enz.

[^27]: Een ander uitgebreid voorstel met vergelijkbare doelen uiteengezet in ["A Narrow Path"](https://www.narrowpath.co/) pleit voor een meer gecentraliseerde, verbods-gebaseerde benadering die alle frontier AI-ontwikkeling door een enkele internationale entiteit leidt, onder toezicht van sterke internationale instellingen, met duidelijke categorische verboden in plaats van graduele beperkingen. Ik zou dat plan ook ondersteunen; echter het zal nog meer politieke wil en coördinatie vergen dan het hier voorgestelde.

[^28]: Enkele richtlijnen voor zo'n standaard werden [gepubliceerd](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/) door het Frontier Model Forum. Relatief tot het hier voorgestelde voorstel, falen die aan de kant van minder precisie en minder rekenkracht inbegrepen in de telling.

[^29]: Het 2023 Amerikaanse AI uitvoerend besluit (nu ingetrokken) vereiste vergelijkbare maar minder fijnkorrelige rapportage. Dit zou moeten worden versterkt door een vervangend besluit.

[^30]: Zeer grof, voor nu-gewone H100 chips komt dit overeen met clusters van ongeveer 1000 die inferentie doen; het is ongeveer 100 (ongeveer USD $5M waard) van de allernieuwste top-of-the-line NVIDIA B200 chips die inferentie doen. In beide gevallen komt het trainingsgetal overeen met die cluster die enkele maanden rekent.

[^31]: Deze hoeveelheid is groter dan enig huidig getraind AI-systeem; een groter of kleiner getal zou kunnen worden gerechtvaardigd naarmate we beter begrijpen hoe AI-capaciteit schaalt met rekankracht.

[^32]: Dit geldt voor degenen die de modellen creëren en leveren/hosten, niet eindgebruikers.

[^33]: Ruwweg betekent "strikte" aansprakelijkheid dat ontwikkelaars *standaard* verantwoordelijk worden gehouden voor schade gedaan door een product en is een standaard gebruikt voor "abnormaal gevaarlijke" producten, en (enigszins amusant maar gepast) wilde dieren. "Gezamenlijke en hoofdelijke" aansprakelijkheid betekent dat aansprakelijkheid wordt toegewezen aan alle partijen verantwoordelijk voor een product, en die partijen moeten onder elkaar uitzoeken wie welke verantwoordelijkheid draagt. Dit is belangrijk voor systemen zoals AI met een lange en complexe waardeketen.

[^34]: Standaard fout-gebaseerde enkelvoudige aansprakelijkheid is niet genoeg: fout zal zowel moeilijk te traceren als toe te wijzen zijn omdat AI-systemen complex zijn, hun werking niet wordt begrepen, en vele partijen kunnen betrokken zijn bij creatie van een gevaarlijk systeem of output. Daarnaast zullen rechtszaken jaren duren om te berechten en waarschijnlijk alleen resulteren in boetes die onbeduidend zijn voor deze bedrijven, dus persoonlijke aansprakelijkheid voor executives is ook belangrijk.

[^35]: Er zou geen vrijstelling van veiligheidscriteria mogen zijn voor open-gewicht modellen. Bovendien, bij het beoordelen van risico zou moeten worden aangenomen dat veiligheidshekken die kunnen worden weggenomen zullen worden weggenomen van breed beschikbare modellen, en dat zelfs gesloten modellen zullen prolifereren tenzij er zeer hoge zekerheid is dat ze veilig zullen blijven.

[^36]: Het hier voorgestelde schema heeft regulatoire scrutinie getriggerd op algemene capaciteit; echter het heeft zin voor sommige vooral riskante gebruikssituaties om meer scrutinie te triggeren – bijvoorbeeld een expert virologie AI-systeem, zelfs als smal en passief, zou waarschijnlijk in een hogere tier moeten gaan. Het voormalige Amerikaanse uitvoerend besluit had iets van deze structuur voor biologische capaciteiten.

[^37]: Twee duidelijke voorbeelden zijn luchtvaart en medicijnen, gereguleerd door de FAA en FDA, en vergelijkbare instanties in andere landen. Deze instanties zijn imperfect, maar zijn absoluut vitaal geweest voor het functioneren en succes van die industrieën.
