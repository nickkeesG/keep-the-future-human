# Hoofdstuk 9 - De toekomst vormgeven — wat we in plaats daarvan zouden moeten doen

AI kan ongelooflijk veel goeds doen in de wereld. Om alle voordelen te krijgen zonder de risico's, moeten we ervoor zorgen dat AI een menselijk gereedschap blijft.

Als we er succesvol voor kiezen om de mensheid niet te laten vervangen door machines – althans voorlopig! – wat kunnen we dan in plaats daarvan doen? Geven we de enorme belofte van AI als technologie op? Op een bepaald niveau is het antwoord simpelweg *nee:* sluit de Poorten naar oncontroleerbare AGI en superintelligentie, maar bouw *wel* vele andere vormen van AI, evenals de governance-structuren en instituties die we nodig hebben om ze te beheren.

Maar er valt nog veel te zeggen; dit realiseren zou een centrale bezigheid van de mensheid worden. Dit hoofdstuk verkent verschillende kernthema's:

- Hoe we "Tool"-AI kunnen karakteriseren en welke vormen het kan aannemen.
- Dat we (bijna) alles kunnen krijgen wat de mensheid wil zonder AGI, met Tool-AI.
- Dat Tool-AI-systemen (waarschijnlijk, in principe) beheersbaar zijn.
- Dat afstand nemen van AGI niet betekent dat we nationale veiligheid in de waagschaal stellen – integendeel.
- Dat machtconcentratie een reële zorg is. Kunnen we dit beperken zonder veiligheid en beveiliging te ondermijnen?
- Dat we nieuwe governance- en sociale structuren zullen willen – en nodig hebben, en dat AI daarbij kan helpen.

## AI binnen de Poorten: Tool-AI

Het drievoudige-intersectiediagram geeft een goede manier om af te bakenen wat we "Tool-AI" kunnen noemen: AI die een controleerbaar gereedschap is voor menselijk gebruik, in plaats van een oncontroleerbare rivaal of vervanger. De minst problematische AI-systemen zijn die welke autonoom zijn maar niet algemeen of supercapabel (zoals een veilingbiedbot), of algemeen maar niet autonoom of capabel (zoals een klein taalmodel), of capabel maar beperkt en zeer controleerbaar (zoals AlphaGo).[^1] Systemen met twee kruisende kenmerken hebben bredere toepassing maar hoger risico en zullen grote inspanningen vereisen om te beheren. (Dat een AI-systeem meer een gereedschap is betekent niet dat het inherent veilig is, alleen dat het niet inherent *onveilig* is – denk aan een kettingzaag versus een huistijger.) De Poort moet gesloten blijven voor (volledige) AGI en superintelligentie op de drievoudige intersectie, en er moet enorme voorzichtigheid worden betracht bij AI-systemen die die drempel naderen.

Maar dit laat nog veel krachtige AI over! We kunnen enorm veel nut halen uit slimme en algemene passieve "orakels" en beperkte systemen, algemene systemen op menselijk maar niet bovenmenselijk niveau, enzovoort. Veel techbedrijven en ontwikkelaars bouwen actief aan dit soort gereedschappen en zouden daarmee moeten doorgaan; zoals de meeste mensen nemen zij impliciet *aan* dat de Poorten naar AGI en superintelligentie gesloten zullen worden.[^2]

Bovendien kunnen AI-systemen effectief worden gecombineerd tot samengestelde systemen die menselijk toezicht behouden terwijl ze de capaciteit verbeteren. In plaats van te vertrouwen op ondoorgrondelijke black boxes, kunnen we systemen bouwen waarbij meerdere componenten – zowel AI als traditionele software – samenwerken op manieren die mensen kunnen monitoren en begrijpen.[^3] Hoewel sommige componenten black boxes zouden kunnen zijn, zou geen enkele dicht bij AGI komen – alleen het samengestelde systeem als geheel zou zowel zeer algemeen als zeer capabel zijn, en op een strikt controleerbare manier.[^4]

### Betekenisvolle en gegarandeerde menselijke controle

Wat betekent "strikt controleerbaar"? Een kernidee van het "Tool"-raamwerk is systemen toe te staan – zelfs als ze vrij algemeen en krachtig zijn – die gegarandeerd onder betekenisvolle menselijke controle staan. Wat betekent dit? Het houdt twee aspecten in. Ten eerste is er een ontwerpoverweging: mensen zouden diep en centraal betrokken moeten zijn bij wat het systeem doet, *zonder* belangrijke beslissingen aan de AI te delegeren. Dit is het karakter van de meeste huidige AI-systemen. Ten tweede, voor zover AI-systemen autonoom zijn, moeten ze garanties hebben die hun handelingsruimte beperken. Een garantie zou een *getal* moeten zijn dat de waarschijnlijkheid van iets karakteriseert, en een reden om dat getal te geloven. Dit is wat we eisen in andere veiligheidskritieke velden, waar getallen zoals "gemiddelde tijd tussen storingen" en verwachte aantallen ongevallen worden berekend, ondersteund en gepubliceerd in veiligheidsdossiers.[^5] Het ideale getal voor storingen is natuurlijk nul. En het goede nieuws is dat we vrij dichtbij kunnen komen, zij het met andere AI-architecturen, door gebruik te maken van ideeën van *formeel geverifieerde* eigenschappen van programma's (inclusief AI). Het idee, uitvoerig verkend door Omohundro, Tegmark, Bengio, Dalrymple en anderen (zie [hier](https://arxiv.org/abs/2309.01933) en [hier](https://arxiv.org/abs/2405.06624)), is om een programma te construeren met bepaalde eigenschappen (bijvoorbeeld: dat een mens het kan uitschakelen) en formeel te *bewijzen* dat die eigenschappen gelden. Dit kan nu gedaan worden voor vrij korte programma's en eenvoudige eigenschappen, maar de (komende) kracht van AI-aangedreven bewijssoftware zou het mogelijk kunnen maken voor veel complexere programma's (bijvoorbeeld wrappers) en zelfs AI zelf. Dit is een zeer ambitieus programma, maar naarmate de druk op de Poorten toeneemt, zullen we enkele krachtige materialen nodig hebben om ze te versterken. Wiskundig bewijs zou een van de weinige kunnen zijn die sterk genoeg is.

### Waar de AI-industrie naartoe gaat

Met omgeleide AI-vooruitgang zou Tool-AI nog steeds een enorme industrie zijn. Wat hardware betreft, zelfs met rekenkrachtlimieten om superintelligentie te voorkomen, zullen training en inferentie in kleinere modellen nog steeds enorme hoeveelheden gespecialiseerde componenten vereisen. Aan de softwarekant zou het ontmantelen van de explosie in AI-model- en berekeningsgrootte er simpelweg toe moeten leiden dat bedrijven middelen herinrichten naar het verbeteren van kleinere systemen – ze beter, diverser en gespecialiseerder maken, in plaats van ze gewoon groter te maken.[^6] Er zou genoeg ruimte zijn – waarschijnlijk meer – voor al die winstgevende Silicon Valley-startups.[^7]

## Tool-AI kan (bijna) alles opleveren wat de mensheid wil, zonder AGI

Intelligentie, zowel biologisch als machinaal, kan breed worden beschouwd als het vermogen om activiteiten te plannen en uit te voeren die toekomsten teweegbrengen die meer in lijn zijn met een set doelen. Als zodanig is intelligentie van enorm voordeel wanneer gebruikt voor het nastreven van wijs gekozen doelen. Kunstmatige intelligentie trekt enorme investeringen van tijd en energie aan grotendeels vanwege de beloofde voordelen. Dus we moeten ons afvragen: in welke mate zouden we de voordelen van AI nog steeds binnenhalen als we de doorloop naar superintelligentie bevatten? Het antwoord: we verliezen mogelijk verrassend weinig.

Overweeg eerst dat huidige AI-systemen al zeer krachtig zijn, en we hebben eigenlijk nog maar het oppervlak geschraapt van wat ermee gedaan kan worden.[^8] Ze zijn redelijk capabel om "de show te runnen" wat betreft het "begrijpen" van een vraag of taak die aan hen wordt voorgelegd, en wat er nodig zou zijn om die vraag te beantwoorden of die taak uit te voeren.

Vervolgens is veel van de opwinding over moderne AI-systemen te danken aan hun algemeenheid; maar enkele van de meest capabele AI-systemen – zoals die spraak of beelden genereren of herkennen, wetenschappelijke voorspelling en modellering doen, spellen spelen, enz. – zijn veel beperkter en goed "binnen de Poorten" wat betreft berekening.[^9] Deze systemen zijn bovenmenselijk in de specifieke taken die ze doen. Ze hebben mogelijk zwakke punten in randgevallen[^10] (of [uitbuitsbare](https://arxiv.org/abs/2211.00241) zwakheden) vanwege hun beperking; echter *totaal* beperkt of *volledig* algemeen zijn niet de enige beschikbare opties: er zijn veel architecturen ertussen.[^11]

Deze AI-gereedschappen kunnen de vooruitgang in andere positieve technologieën sterk versnellen, zonder AGI. Om betere kernfysica te doen, hebben we geen AI nodig die een kernfysicus is – die hebben we al! Als we de geneeskunde willen versnellen, geef de biologen, medische onderzoekers en chemici krachtige gereedschappen. Ze willen ze en zullen ze met enorm voordeel gebruiken. We hebben geen serverfarm vol een miljoen digitale genieën nodig; we hebben miljoenen mensen wier genialiteit AI kan helpen naar boven brengen. Ja, het zal langer duren om onsterfelijkheid en de genezing van alle ziekten te krijgen. Dit is een echte kost. Maar zelfs de meest veelbelovende gezondheidsinnovaties zouden van weinig nut zijn als AI-gedreven instabiliteit leidt tot wereldwijde conflicten of maatschappelijke ineenstorting. We zijn het aan onszelf verschuldigd om AI-aangedreven mensen eerst een kans te geven aan het probleem.

En stel dat er inderdaad enorm voordeel is aan AGI dat niet verkregen kan worden door de mensheid die binnen-de-Poorten-gereedschappen gebruikt. Verliezen we die door *nooit* AGI en superintelligentie te bouwen? Bij het afwegen van de risico's en beloningen hier is er een enorm asymmetrisch voordeel in wachten versus haasten: we kunnen wachten tot het op een gegarandeerd veilige en voordelige manier kan worden gedaan, en bijna iedereen zal nog steeds de beloningen kunnen oogsten; als we haasten, zou het – in de woorden van OpenAI CEO Sam Altman – [lichten uit kunnen zijn voor *ons allemaal*.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

Maar als niet-AGI-gereedschappen potentieel zo krachtig zijn, kunnen we ze dan beheren? Het antwoord is een duidelijke... misschien.

## Tool-AI-systemen zijn (waarschijnlijk, in principe) beheersbaar

Maar het zal niet gemakkelijk zijn. Huidige geavanceerde AI-systemen kunnen mensen en instellingen sterk versterken in het bereiken van hun doelen. Dit is over het algemeen een goede zaak! Er zijn echter natuurlijke dynamieken van het hebben van dergelijke systemen tot onze beschikking – plotseling en zonder veel tijd voor de samenleving om zich aan te passen – die ernstige risico's bieden die beheerd moeten worden. Het is de moeite waard om enkele grote klassen van dergelijke risico's te bespreken, en hoe ze verminderd kunnen worden, uitgaande van een Poortsluiting.

Een klasse van risico's is dat krachtige Tool-AI toegang mogelijk maakt tot kennis of capaciteit die eerder gebonden was aan een persoon of organisatie, waardoor een combinatie van hoge capaciteit plus hoge loyaliteit beschikbaar wordt voor een zeer breed scala aan actoren. Vandaag de dag zou een kwaadwillende persoon met genoeg geld een team chemici kunnen inhuren om nieuwe chemische wapens te ontwerpen en produceren – maar het is niet zo heel gemakkelijk om dat geld te hebben of het team te vinden/samenstellen en hen te overtuigen om iets te doen dat duidelijk illegaal, onethisch en gevaarlijk is. Om te voorkomen dat AI-systemen zo'n rol spelen, kunnen verbeteringen op huidige methoden goed volstaan,[^12] zolang al die systemen en toegang ertoe verantwoord worden beheerd. Anderzijds, als krachtige systemen worden vrijgegeven voor algemeen gebruik en modificatie, zijn ingebouwde veiligheidsmaatregelen waarschijnlijk verwijderbaar. Dus om risico's in deze klasse te vermijden, zullen sterke beperkingen op wat publiekelijk kan worden vrijgegeven – vergelijkbaar met beperkingen op details van nucleaire, explosieve en andere gevaarlijke technologieën – vereist zijn.[^13]

Een tweede klasse van risico's komt voort uit het opschalen van machines die zich gedragen als of zich voordoen als mensen. Op het niveau van schade aan individuele mensen omvatten deze risico's veel effectievere oplichting, spam en phishing, en de verspreiding van niet-consensuele deepfakes.[^14] Op collectief niveau omvatten ze ontwrichting van kernmaatschappelijke processen zoals publieke discussie en debat, onze maatschappelijke informatie- en kennisverzameling, -verwerking en -verspreidingssystemen, en onze politieke keuzesystemen. Het beperken van dit risico zal waarschijnlijk inhouden (a) wetten die het zich voordoen als mensen door AI-systemen beperken, en AI-ontwikkelaars aansprakelijk stellen die systemen creëren die dergelijke imitaties genereren, (b) watermerking- en herkomstsystemen die (verantwoord) gegenereerde AI-content identificeren en classificeren, en (c) nieuwe socio-technische epistemische systemen die een vertrouwde keten kunnen creëren van gegevens (bijvoorbeeld camera's en opnames) via feiten, begrip en goede wereldmodellen.[^15] Dit alles is mogelijk, en AI kan helpen bij sommige onderdelen ervan.

Een derde algemeen risico is dat in de mate waarin sommige taken worden geautomatiseerd, de mensen die nu die taken doen minder financiële waarde kunnen hebben als arbeidskracht. Historisch gezien heeft het automatiseren van taken de dingen die door die taken mogelijk werden gemaakt goedkoper en overvloediger gemaakt, terwijl de mensen die eerder die taken deden werden gesorteerd in degenen die nog steeds betrokken zijn bij de geautomatiseerde versie (over het algemeen op hoger vaardigheids-/loonniveau), en degenen wier arbeid minder of weinig waard is. Netto is het moeilijk te voorspellen in welke sectoren meer versus minder menselijke arbeid vereist zal zijn in de resulterende grotere maar efficiëntere sector. Parallel neigt de automatiseringsdynamiek de ongelijkheid en algemene productiviteit te verhogen, de kosten van bepaalde goederen en diensten te verlagen (via efficiëntieverhogingen), en de kosten van anderen te verhogen (via [kostenziekte](https://en.wikipedia.org/wiki/Baumol_effect)). Voor degenen aan de ongunstige kant van de ongelijkheidstoeename is het diep onduidelijk of de kostendaling in die bepaalde goederen en diensten opweegt tegen de stijging in anderen, en leidt tot algemeen groter welzijn. Dus hoe zal dit gaan voor AI? Vanwege het relatieve gemak waarmee menselijke intellectuele arbeid kan worden vervangen door algemene AI, kunnen we een snelle versie hiervan verwachten met menselijk-competitieve algemene AI.[^16] Als we de Poort naar AGI sluiten, zullen veel minder banen volledig vervangen worden door AI-agenten; maar enorme arbeidsplaatsverplaatsing is nog steeds waarschijnlijk over een periode van jaren.[^17] Om wijdverspreide economische pijn te vermijden, zal het waarschijnlijk nodig zijn om zowel een vorm van universele basisactiva of -inkomen te implementeren, als ook een culturele verschuiving te bewerkstelligen naar het waarderen en belonen van mensgerichte arbeid die moeilijker te automatiseren is (in plaats van te zien dat arbeidsprijzen dalen door de toename van beschikbare arbeid die uit andere delen van de economie wordt weggedrukt.) Andere constructies, zoals die van ["datadigniteit"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society) (waarin de menselijke producenten van trainingsdata automatisch royalty's krijgen voor de waarde gecreëerd door die data in AI) kunnen helpen. Automatisering door AI heeft ook een tweede potentieel nadelig effect, namelijk van *ongepaste* automatisering. Samen met toepassingen waar AI gewoon slechter werk doet, zou dit die omvatten waar AI-systemen waarschijnlijk morele, ethische of juridische voorschriften schenden – bijvoorbeeld bij leven-en-dood-beslissingen, en in juridische aangelegenheden. Deze moeten worden behandeld door onze huidige juridische kaders toe te passen en uit te breiden.

Ten slotte is een significante bedreiging van AI binnen de poorten het gebruik ervan in gepersonaliseerde overtuiging, aandachtvangst en manipulatie. We hebben in sociale media en andere online platforms de groei gezien van een diep verankerde aandachteconomie (waar online diensten fel strijden om gebruikersaandacht) en ["surveillance-kapitalisme"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism)-systemen (waarin gebruikersinformatie en profilering wordt toegevoegd aan de commercialisering van aandacht.) Het is vrijwel zeker dat meer AI in dienst van beide zal worden gesteld. AI wordt al zwaar gebruikt in verslavende feed-algoritmes, maar dit zal evolueren naar verslavende AI-gegenereerde content, aangepast om dwangmatig geconsumeerd te worden door een enkele persoon. En die persoon's input, reacties en data zullen in de aandachts-/advertentiemachine worden gevoerd om de vicieuze cirkel voort te zetten. Bovendien, naarmate AI-helpers geleverd door techbedrijven de interface worden voor meer online leven, zullen ze waarschijnlijk zoekmachines en feeds vervangen als het mechanisme waardoor overtuiging en geldverdienen aan klanten plaatsvindt. Het falen van onze samenleving om deze dynamieken tot nu toe te controleren voorspelt niet veel goeds. Een deel van deze dynamiek kan worden verminderd via regelgeving betreffende privacy, datarechten en manipulatie. Om meer tot de wortel van het probleem te komen kan het verschillende perspectieven vereisen, zoals dat van loyale AI-assistenten (hieronder besproken.)

De uitkomst van deze discussie is er een van hoop: systemen binnen de Poorten op gereedschapsbasis – althans zolang ze vergelijkbaar in kracht en capaciteit blijven met de meest geavanceerde systemen van vandaag – zijn waarschijnlijk beheersbaar als er wil en coördinatie is om dat te doen. Fatsoenlijke menselijke instellingen, versterkt door AI-gereedschappen,[^18] kunnen het doen. We zouden er ook in kunnen falen. Maar het is moeilijk in te zien hoe het toestaan van krachtigere systemen zou helpen – behalve door ze de leiding te laten nemen en het beste ervan te hopen.

## Nationale veiligheid

Races voor AI-suprematie – gedreven door nationale veiligheid of andere motivaties – drijven ons naar ongecontroleerde krachtige AI-systemen die de neiging zouden hebben macht te absorberen, in plaats van te verlenen. Een AGI-race tussen de VS en China is een race om te bepalen welke natie eerst superintelligentie krijgt.

Dus wat zouden degenen die verantwoordelijk zijn voor nationale veiligheid in plaats daarvan moeten doen? Overheden hebben sterke ervaring in het bouwen van controleerbare en veilige systemen, en ze zouden daarop moeten inzetten in AI, door het ondersteunen van het soort infrastructuurprojecten die het beste slagen wanneer ze op schaal en met overheidsaanzien worden gedaan.

In plaats van een roekeloos "Manhattan-project" naar AGI,[^19] zou de Amerikaanse regering een Apollo-project kunnen lanceren voor controleerbare, veilige, betrouwbare systemen. Dit zou bijvoorbeeld kunnen omvatten:

- Een groot programma om (a) de on-chip hardware-beveiligingsmechanismen te ontwikkelen en (b) de infrastructuur, om de rekenkrachtkant van krachtige AI te beheren. Deze zouden kunnen voortbouwen op de Amerikaanse [CHIPS-wet](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) en [exportcontroleregime](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion).
- Een grootschalig initiatief om formele verificatietechnieken te ontwikkelen zodat bepaalde kenmerken van AI-systemen (zoals een uit-schakelaar) *bewezen* kunnen worden aanwezig of afwezig te zijn. Dit kan AI zelf benutten om bewijzen van eigenschappen te ontwikkelen.
- Een nationale inspanning om software te creëren die verifieerbaar veilig is, aangedreven door AI-gereedschappen die bestaande software kunnen hercoderen in verifieerbaar veilige kaders.
- Een nationale investeringsproject in wetenschappelijke vooruitgang met AI,[^20] als partnerschap tussen DOE, NSF en NIH.

Over het algemeen is er een enorm aanvalsoppervlak op onze samenleving dat ons kwetsbaar maakt voor risico's van AI en het misbruik ervan. Bescherming tegen sommige van deze risico's zal overheidsgrote investeringen en standaardisatie vereisen. Deze zouden veel meer veiligheid bieden dan benzine op het vuur van races naar AGI gooien. En als AI ingebouwd gaat worden in bewapening en commando-en-controlesystemen, is het cruciaal dat de AI betrouwbaar en veilig is, wat huidige AI simpelweg niet is.

## Machtconcentratie en haar beperkingen

Dit essay heeft zich gericht op het idee van menselijke controle over AI en het mogelijke falen daarvan. Maar een andere geldige lens waardoor we de AI-situatie kunnen bekijken is door *machtconcentratie.* De ontwikkeling van zeer krachtige AI dreigt macht te concentreren ofwel in de zeer weinige en zeer grote bedrijfshanden die het hebben ontwikkeld en zullen controleren, ofwel in overheden die AI gebruiken als nieuw middel om hun eigen macht en controle te behouden, ofwel in de AI-systemen zelf. Of een onheilige mix van het bovenstaande. In al deze gevallen verliest het grootste deel van de mensheid macht, controle en handelingsbevoegdheid. Hoe kunnen we dit bestrijden?

De allereerste en belangrijkste stap is natuurlijk een Poortsluiting naar slimmer-dan-menselijke AGI en superintelligentie. Deze kunnen expliciet direct mensen en groepen mensen vervangen. Als ze onder bedrijfs- of overheidscontrole staan zullen ze macht concentreren in die bedrijven of overheden; als ze "vrij" zijn zullen ze macht in zichzelf concentreren. Dus laten we aannemen dat de Poorten gesloten zijn. Wat dan?

Een voorgestelde oplossing voor machtconcentratie is "open-source" AI, waar modelgewichten vrij of breed beschikbaar zijn. Maar zoals eerder vermeld, zodra een model open is, kunnen de meeste veiligheidsmaatregelen of vangrails (en worden ze over het algemeen) weggehaald. Er is dus een acute spanning tussen enerzijds decentralisatie, en anderzijds veiligheid, beveiliging en menselijke controle over AI-systemen. Er zijn ook redenen om sceptisch te zijn dat open modellen op zichzelf op betekenisvolle wijze machtconcentratie in AI zullen bestrijden, niet meer dan ze hebben gedaan bij besturingssystemen (nog steeds gedomineerd door Microsoft, Apple en Google ondanks open alternatieven).[^21]

Toch kunnen er manieren zijn om deze cirkel te kwadrateren – om risico's te centraliseren en beperken terwijl capaciteit en economische beloning worden gedecentraliseerd. Dit vereist herdenken van zowel hoe AI wordt ontwikkeld als hoe de voordelen ervan worden verdeeld.

Nieuwe modellen van publieke AI-ontwikkeling en -eigendom zouden helpen. Dit zou verschillende vormen kunnen aannemen: overheid-ontwikkelde AI (onderhevig aan democratisch toezicht),[^22] non-profit AI-ontwikkelingsorganisaties (zoals Mozilla voor browsers), of structuren die zeer wijdverspreide eigendom en governance mogelijk maken. Cruciaal is dat deze instellingen expliciet gecharterd zouden worden om het publieke belang te dienen terwijl ze onder sterke veiligheidsbeperkingen opereren.[^23] Goed vervaardigde regulerings- en standaarden-/certificeringsregimes zullen ook vitaal zijn, zodat AI-producten aangeboden door een levendige markt echt nuttig blijven in plaats van uitbuitend richting hun gebruikers.

Wat betreft economische machtconcentratie kunnen we herkomsttracking en "datadigniteit" gebruiken om ervoor te zorgen dat economische voordelen breder stromen. In het bijzonder komt de meeste AI-macht nu (en in de toekomst als we de Poorten gesloten houden) voort uit door mensen gegenereerde data, of directe trainingsdata of menselijke feedback. Als AI-bedrijven verplicht zouden worden om dataproviders eerlijk te compenseren,[^24] zou dit op zijn minst kunnen helpen om de economische beloningen breder te verdelen. Hiernaast zou een ander model publieke eigendom van significante fracties van grote AI-bedrijven kunnen zijn. Bijvoorbeeld, overheden die in staat zijn AI-bedrijven te belasten zouden een fractie van de ontvangsten kunnen investeren in een soeverein welvaartsfonds dat aandelen in de bedrijven houdt, en dividenden aan de bevolking betaalt.[^25]

Cruciaal in deze mechanismen is het gebruiken van de kracht van AI zelf om macht beter te verdelen, in plaats van AI-gedreven machtconcentratie simpelweg te bestrijden met niet-AI-middelen. Een krachtige benadering zou zijn door middel van goed ontworpen AI-assistenten die opereren met echte fiduciaire plicht aan hun gebruikers – de belangen van gebruikers voorop stellen, vooral boven die van bedrijfsaanbieders.[^26] Deze assistenten moeten werkelijk betrouwbaar zijn, technisch competent maar passend beperkt gebaseerd op gebruiksgeval en risiconiveau, en breed beschikbaar voor iedereen via publieke, non-profit, of gecertificeerde for-profit kanalen. Net zoals we nooit een menselijke assistent zouden accepteren die heimelijk tegen onze belangen werkt voor een andere partij, zouden we geen AI-assistenten moeten accepteren die hun gebruikers surveilleren, manipuleren of waarde onttrekken voor bedrijfsvoordeel.

Zo'n transformatie zou de huidige dynamiek fundamenteel veranderen waarbij individuen alleen moeten onderhandelen met uitgestrekte (AI-aangedreven) bedrijfs- en bureaucratische machines die waarde-extractie prioriteit geven boven menselijk welzijn. Hoewel er veel mogelijke benaderingen zijn om AI-gedreven macht breder te herverdelen, zal geen daarvan vanzelf ontstaan: ze moeten bewust worden ontwikkeld en bestuurd met mechanismen zoals fiduciaire eisen, publieke voorziening, en gelaagde toegang gebaseerd op risico.

Benaderingen om machtconcentratie te beperken kunnen significante tegenwind ondervinden van gevestigde machten.[^27] Maar er zijn paden naar AI-ontwikkeling die niet vereisen dat we kiezen tussen veiligheid en geconcentreerde macht. Door nu de juiste instellingen te bouwen, zouden we ervoor kunnen zorgen dat AI's voordelen breed worden gedeeld terwijl de risico's zorgvuldig worden beheerd.

## Nieuwe governance- en sociale structuren

Onze huidige governance-structuren worstelen: ze reageren traag, zijn vaak gevangen door speciale belangen, en [krijgen steeds minder vertrouwen van het publiek.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) Toch is dit geen reden om ze op te geven – integendeel. Sommige instellingen hebben misschien vervanging nodig, maar breder hebben we nieuwe mechanismen nodig die onze bestaande structuren kunnen verbeteren en aanvullen, hen helpen beter te functioneren in onze snel evoluerende wereld.

Veel van onze institutionele zwakte komt niet voort uit formele overheidsstructuren, maar uit gedegradeerde sociale instellingen: onze systemen voor het ontwikkelen van gedeeld begrip, het coördineren van actie, en het voeren van betekenisvolle discourse. Tot nu toe heeft AI deze degradatie versneld, onze informatiekanalen overspoeld met gegenereerde content, ons gewezen naar de meest polariserende en verdeeldheid zaaiende content, en het moeilijker gemaakt om waarheid van fictie te onderscheiden.

Maar AI zou deze sociale instellingen eigenlijk kunnen helpen herbouwen en versterken. Overweeg drie cruciale gebieden:

Ten eerste zou AI kunnen helpen het vertrouwen in onze epistemische systemen te herstellen – onze manieren om te weten wat waar is. We zouden AI-aangedreven systemen kunnen ontwikkelen die de herkomst van informatie volgen en verifiëren, van ruwe data via analyse tot conclusies. Deze systemen zouden cryptografische verificatie kunnen combineren met geavanceerde analyse om mensen te helpen begrijpen niet alleen of iets waar is, maar hoe we weten dat het waar is.[^28] Loyale AI-assistenten zouden belast kunnen worden met het volgen van de details om ervoor te zorgen dat ze kloppen.

Ten tweede zou AI nieuwe vormen van grootschalige coördinatie mogelijk kunnen maken. Veel van onze meest dringende problemen – van klimaatverandering tot antibioticaresistentie – zijn fundamenteel coördinatieproblemen. We zitten [vast in situaties die slechter zijn dan ze zouden kunnen zijn voor bijna iedereen](https://equilibriabook.com/), omdat geen individu of groep zich de eerste zet kan veroorloven. AI-systemen zouden kunnen helpen door complexe incentivestructuren te modelleren, haalbare paden naar betere uitkomsten te identificeren, en het vertrouwensopbouw- en commitmentmechanismen te faciliteren die nodig zijn om daar te komen.

Misschien het meest intrigerend, AI zou geheel nieuwe vormen van maatschappelijke discourse mogelijk kunnen maken. Stel je voor dat je kunt "praten met een stad"[^29] – niet alleen statistieken bekijken, maar een betekenisvolle dialoog voeren met een AI-systeem dat de meningen, ervaringen, behoeften en aspiraties van miljoenen inwoners verwerkt en synthetiseert. Of overweeg hoe AI echte dialoog zou kunnen faciliteren tussen groepen die nu langs elkaar heen praten, door elke kant te helpen de werkelijke zorgen en waarden van de ander beter te begrijpen in plaats van hun karikaturen van elkaar.[^30] Of AI zou bekwame, geloofwaardig neutrale bemiddeling kunnen bieden van geschillen tussen mensen of zelfs grote groepen mensen (die allemaal direct en individueel ermee kunnen interacteren!) Huidige AI is totaal capabel om dit werk te doen, maar de gereedschappen om dat te doen zullen niet vanzelf ontstaan, of via marktprikkels.

Deze mogelijkheden klinken misschien utopisch, vooral gezien AI's huidige rol in het degraderen van discourse en vertrouwen. Maar dat is precies waarom we deze positieve toepassingen actief moeten ontwikkelen. Door de Poorten naar oncontroleerbare AGI te sluiten en AI te prioriteren die menselijke handelingsbevoegdheid versterkt, kunnen we technologische vooruitgang sturen naar een toekomst waarin AI dient als kracht voor versterking, veerkracht en collectieve vooruitgang.

[^1]: Dat gezegd hebbende, wegblijven van de drievoudige intersectie is helaas niet zo gemakkelijk als men zou wensen. Het hard pushen van capaciteit in een van de drie aspecten heeft de neiging het in de anderen te verhogen. In het bijzonder kan het moeilijk zijn een extreem algemene en capabele intelligentie te creëren die niet gemakkelijk autonoom kan worden gemaakt. Een benadering is het trainen van ["myopische"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) systemen met gebrekkig planningsvermogen. Een andere zou zijn om te focussen op het ontwikkelen van pure ["orakel"](https://arxiv.org/abs/1711.05541)-systemen die zouden wegschrikken van het beantwoorden van actiegericht vragen.

[^2]: Veel bedrijven falen te beseffen dat ook zij uiteindelijk zouden worden vervangen door AGI, zelfs als het langer duurt – als ze dat wel zouden doen, zouden ze misschien wat minder hard op die Poorten duwen!

[^3]: AI-systemen zouden kunnen communiceren op meer efficiënte maar minder begrijpelijke manieren, maar het behouden van menselijk begrip zou prioriteit moeten hebben.

[^4]: Dit idee van modulaire, interpreteerbare AI is in detail ontwikkeld door verschillende onderzoekers; zie bijvoorbeeld het ["Comprehensive AI Services"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) model door Drexler, de ["Open Agency Architecture"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai) van Dalrymple en anderen. Hoewel dergelijke systemen meer engineering-inspanning zouden kunnen vereisen dan monolithische neurale netwerken getraind met massale berekening, is dat precies waar berekeningstlimieten helpen – door het veiligere, transparantere pad ook het praktischere te maken.

[^5]: Over veiligheidsdossiers in het algemeen zie [dit handboek](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). Betreffende AI in het bijzonder, zie [Wasil et al.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [Clymer et al.](https://arxiv.org/abs/2403.10462), [Buhl et al.](https://arxiv.org/abs/2410.21572), en [Balesni et al.](https://arxiv.org/abs/2411.03336)

[^6]: We zien dit trend al gedreven door alleen de hoge kosten van inferentie: kleinere en meer gespecialiseerde modellen "gedistilleerd" van grotere en capabel om te draaien op minder dure hardware.

[^7]: Ik begrijp waarom degenen die enthousiast zijn over het AI-tech-ecosysteem zich kunnen verzetten tegen wat zij zien als belastende regulering op hun industrie. Maar het is ronduit verbijsterend voor mij waarom, laten we zeggen, een venture capitalist runaway naar AGI en superintelligentie zou willen toestaan. Die systemen (en bedrijven, zolang ze onder bedrijfscontrole blijven) zullen *alle startups als snack opeten*. Waarschijnlijk zelfs *eerder* dan andere industrieën opeten. Iedereen geïnvesteerd in een bloeiend AI-ecosysteem zou prioriteit moeten geven aan ervoor zorgen dat AGI-ontwikkeling niet leidt tot monopolisering door een paar dominante spelers.

[^8]: Zoals econoom en voormalig Deepmind-onderzoeker Michael Webb [het stelde](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/), "Ik denk dat als we alle ontwikkeling van grotere taalmodellen vandaag stopten, dus GPT-4 en Claude en wat dan ook, en dat zijn de laatste dingen die we van die grootte trainen – dus we staan veel meer iteratie toe op dingen van die grootte en allerlei fine-tuning, maar niets groters dan dat, geen grotere vooruitgangen – alleen wat we vandaag hebben denk ik genoeg is om 20 of 30 jaar van ongelofelijke economische groei aan te drijven."

[^9]: Bijvoorbeeld, DeepMind's alphafold-systeem gebruikte slechts 100.000ste van GPT-4's FLOP-getal.

[^10]: De moeilijkheid van zelfrijdende auto's is hier belangrijk om op te merken: hoewel nominaal een beperkte taak, en haalbaar met redelijke betrouwbaarheid met relatief kleine AI-systemen, is uitgebreide werkelijke wereldkennis en begrip nodig om betrouwbaarheid te krijgen op het niveau dat nodig is in zo'n veiligheidskritieke taak.

[^11]: Bijvoorbeeld, gegeven een berekeningsbudget, zouden we waarschijnlijk GPAI-modellen zien vooraf getraind op (zeg) de helft van dat budget, en de andere helft gebruikt om zeer hoge capaciteit op te trainen in een meer beperkte reeks taken. Dit zou bovenmenselijke beperkte capaciteit geven ondersteund door bijna-menselijke algemene intelligentie.

[^12]: De huidige dominante alignment-techniek is "reinforcement learning by human feedback" [(RLHF)](https://arxiv.org/abs/1706.03741) en gebruikt menselijke feedback om een belonings-/straffignaal te creëren voor reinforcement learning van het AI-model. Deze en gerelateerde technieken zoals [constitutional AI](https://arxiv.org/abs/2212.08073) werken verrassend goed (hoewel ze robuustheid missen en kunnen worden omzeild met bescheiden inspanning.) Daarnaast zijn huidige taalmodellen over het algemeen competent genoeg in gezond verstand redeneren dat ze geen dwaas morele fouten zullen maken. Dit is iets van een sweet spot: slim genoeg om te begrijpen wat mensen willen (voor zover het gedefinieerd kan worden), maar niet slim genoeg om uitgebreide bedrog te plannen of enorme schade te veroorzaken wanneer ze het verkeerd krijgen.

[^13]: Op de lange termijn zal waarschijnlijk elk niveau van AI-capaciteit dat wordt ontwikkeld prolifereren, aangezien het uiteindelijk software is, en nuttig. We zullen robuuste mechanismen nodig hebben om te verdedigen tegen de risico's die dergelijke systemen stellen. Maar we *hebben dat nu niet* dus we moeten zeer afgemeten zijn in hoeveel krachtige AI-modellen mogen prolifereren.

[^14]: De overgrote meerderheid hiervan zijn niet-consensuele pornografische deepfakes, inclusief van minderjarigen.

[^15]: Veel ingrediënten voor dergelijke oplossingen bestaan, in de vorm van "bot-of-niet" wetten (in de EU AI-wet onder andere plekken), [industrie herkomst-tracking technologieën](https://c2pa.org/), [innovatieve nieuwsaggregatoren](https://www.improvethenews.org/), voorspelling [aggregatoren](https://metaculus.com/) en markten, enz.

[^16]: De automatiseringsgolf volgt misschien niet vorige patronen, in dat relatief *hoge*-vaardigheid taken zoals kwaliteitsschrijven, wet interpreteren, of medisch advies geven, net zo veel of zelfs meer kwetsbaar kunnen zijn voor automatisering dan lagere-vaardigheid taken.

[^17]: Voor zorgvuldige modellering van het effect van AGI op lonen, zie het rapport [hier](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek), en bloederige details [hier](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0), van Anton Korinek en medewerkers. Ze vinden dat naarmate meer stukken van banen worden geautomatiseerd, productiviteit en lonen omhooggaan – tot op een punt. Zodra *te veel* is geautomatiseerd, blijft productiviteit stijgen, maar instorten lonen omdat mensen vervangen worden door efficiënte AI. Dit is waarom Poorten sluiten zo nuttig is: we krijgen de productiviteit zonder de verdwenen menselijke lonen.

[^18]: Er zijn veel manieren waarop AI kan worden gebruikt als, en om te helpen bouwen, "defensieve" technologieën om beschermingen en beheer robuuster te maken. Zie [deze](https://vitalik.eth.limo/general/2025/01/05/dacc2.html) invloedrijke post die deze "D/acc" agenda beschrijft.

[^19]: Enigszins ironisch zou een Amerikaans Manhattan-project waarschijnlijk weinig doen om tijdlijnen naar AGI te versnellen – de wijzer van menselijke en fiscale investering in AI-vooruitgang staat al op 11. De primaire resultaten zouden zijn om een vergelijkbaar project in China te inspireren (dat uitblinkt in nationale infrastructuurprojecten), om internationale akkoorden die AI's risico beperken veel moeilijker te maken, en om andere geopolitieke tegenstanders van de VS zoals Rusland te alarmeren.

[^20]: Het ["National AI Research Resource"](https://nairrpilot.org/) programma is een goede huidige stap in deze richting en zou moeten worden uitgebreid.

[^21]: Zie [deze analyse](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807) van de verschillende betekenissen en implicaties van "open" in techproducten en hoe sommigen hebben geleid tot meer, in plaats van minder, verankering van dominantie.

[^22]: Plannen in de VS voor een [National AI Research Resource](https://nairratdoe.ornl.gov/) en de recente lancering van een [Europese AI Foundation](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/) zijn interessante stappen in deze richting.

[^23]: De uitdaging hier is niet technisch maar institutioneel – we hebben dringend praktijkvoorbeelden en experimenten nodig in hoe publieke-belang AI-ontwikkeling eruit zou kunnen zien.

[^24]: Dit gaat in tegen huidige big tech bedrijfsmodellen en zou zowel juridische actie als nieuwe normen vereisen.

[^25]: Alleen sommige overheden zullen dit kunnen doen. Een meer radicaal idee is [een universeel fonds van dit type, onder gezamenlijke eigendom van alle mensen.](https://futureoflife.org/project/the-windfall-trust/)

[^26]: Voor een uitvoerige uiteenzetting van deze zaak zie [dit paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338) over AI-loyaliteit. Helaas is het standaard traject van AI-assistenten waarschijnlijk er een waar ze steeds disloyaler worden.

[^27]: Enigszins ironisch staan veel gevestigde machten ook voor het risico van AI-gesteunde ontvoogding; maar het kan moeilijk voor hen zijn om dit waar te nemen totdat en tenzij het proces behoorlijk ver komt.

[^28]: Enkele interessante inspanningen in deze richting worden vertegenwoordigd door [de c2pa coalitie](https://c2pa.org/) over cryptografische verificatie; [Verity](https://www.improvethenews.org/) en [Ground news](https://ground.news/) over betere nieuwsepistemiek; en [Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com) en voorspellingsmarkten over het gronden van discourse in falsifieerbare voorspellingen.

[^29]: Zie [dit](https://talktothecity.org/) fascinerende pilotproject.

[^30]: Zie [Kialo](https://www.kialo-edu.com/), en inspanningen van het [Collective Intelligence Project](https://www.cip.org/) voor enkele voorbeelden.