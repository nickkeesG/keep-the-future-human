# Résumé exécutif

Un aperçu général de l'essai. Si vous manquez de temps, découvrez tous les points principaux en seulement 10 minutes.

Les avancées spectaculaires de l'intelligence artificielle au cours de la dernière décennie (pour l'IA spécialisée) et de ces dernières années (pour l'IA généraliste) ont transformé l'IA d'un domaine académique de niche en stratégie commerciale centrale de nombreuses des plus grandes entreprises mondiales, avec des centaines de milliards de dollars d'investissement annuel dans les techniques et technologies visant à faire progresser les capacités de l'IA.

Nous arrivons maintenant à un moment critique. Alors que les capacités des nouveaux systèmes d'IA commencent à égaler et dépasser celles des humains dans de nombreux domaines cognitifs, l'humanité doit décider : jusqu'où allons-nous, et dans quelle direction ?

L'IA, comme toute technologie, a commencé avec l'objectif d'améliorer les choses pour son créateur. Mais notre trajectoire actuelle, et notre choix implicite, consiste en une course effrénée vers des systèmes toujours plus puissants, motivée par les incitations économiques de quelques entreprises technologiques géantes cherchant à automatiser de larges pans de l'activité économique actuelle et du travail humain. Si cette course se poursuit encore longtemps, il y a un vainqueur inévitable : l'IA elle-même – une alternative plus rapide, plus intelligente et moins chère aux humains dans notre économie, notre réflexion, nos décisions, et finalement aux commandes de notre civilisation.

Mais nous pouvons faire un autre choix : par l'intermédiaire de nos gouvernements, nous pouvons reprendre le contrôle du processus de développement de l'IA pour imposer des limites claires, des lignes que nous ne franchirons pas, et des choses que nous ne ferons tout simplement pas – comme nous l'avons fait pour les technologies nucléaires, les armes de destruction massive, les armes spatiales, les processus destructeurs pour l'environnement, la bioingénierie humaine et l'eugénisme. Plus important encore, nous pouvons nous assurer que l'IA reste un outil pour autonomiser les humains, plutôt qu'une nouvelle espèce qui nous remplace et finit par nous supplanter.

Cet essai soutient que nous devrions *garder l'avenir humain* en fermant les « portes » à l'IA autonome et généraliste plus intelligente que les humains – parfois appelée « IAG » – et en particulier à la version hautement surhumaine parfois appelée « superintelligence ». Au lieu de cela, nous devrions nous concentrer sur des outils d'IA puissants et fiables qui peuvent autonomiser les individus et améliorer de façon transformatrice les capacités des sociétés humaines à faire ce qu'elles font de mieux. La structure de cet argument suit brièvement.

## L'IA est différente

Les systèmes d'IA sont fondamentalement différents des autres technologies. Alors que les logiciels traditionnels suivent des instructions précises, les systèmes d'IA apprennent à atteindre des objectifs sans qu'on leur dise explicitement comment faire. Cela les rend puissants : si nous pouvons définir clairement l'objectif ou une mesure de réussite, dans la plupart des cas un système d'IA peut apprendre à l'atteindre. Mais cela les rend aussi intrinsèquement imprévisibles : nous ne pouvons pas déterminer de manière fiable quelles actions ils prendront pour atteindre leurs objectifs.

Ils sont également largement inexplicables : bien qu'ils soient en partie du code, ils sont surtout un ensemble énorme de nombres insondables – les « pondérations » des réseaux de neurones – qui ne peuvent être analysés ; nous ne sommes guère meilleurs pour comprendre leur fonctionnement interne que pour discerner des pensées en scrutant l'intérieur d'un cerveau biologique.

Ce mode central d'entraînement des réseaux de neurones numériques gagne rapidement en complexité. Les systèmes d'IA les plus puissants sont créés par des expériences computationnelles massives, utilisant du matériel spécialisé pour entraîner les réseaux de neurones sur d'énormes ensembles de données, qui sont ensuite augmentés d'outils logiciels et de superstructures.

Cela a mené à la création d'outils très puissants pour créer et traiter le texte et les images, effectuer un raisonnement mathématique et scientifique, agréger l'information, et interroger de manière interactive un vaste stock de connaissances humaines.

Malheureusement, bien que le développement d'outils technologiques plus puissants et plus fiables soit ce que nous *devrions* faire, et ce que pratiquement tout le monde veut et dit vouloir, ce n'est pas la trajectoire sur laquelle nous sommes réellement.

## L'IAG et la superintelligence

Depuis l'aube du domaine, la recherche en IA s'est plutôt concentrée sur un objectif différent : l'Intelligence Artificielle Générale. Cette orientation est maintenant devenue le focus des entreprises titanesques qui mènent le développement de l'IA.

Qu'est-ce que l'IAG ? Elle est souvent vaguement définie comme « l'IA au niveau humain », mais c'est problématique : quels humains, et pour quelles capacités est-elle au niveau humain ? Et qu'en est-il des capacités surhumaines qu'elle a déjà ? Une façon plus utile de comprendre l'IAG passe par l'intersection de trois propriétés clés : haute **A**utonomie (indépendance d'action), haute **G**énéralité (portée large et adaptabilité), et haute **I**ntelligence (compétence aux tâches cognitives). Les systèmes d'IA actuels peuvent être très capables mais étroits, ou généraux mais nécessitant une supervision humaine constante, ou autonomes mais limités en portée.

Une IAG complète combinerait ces trois propriétés à des niveaux égalant ou dépassant les capacités humaines de pointe. De manière critique, c'est cette combinaison qui rend les humains si efficaces et si différents des logiciels actuels ; c'est aussi ce qui permettrait aux gens d'être remplacés en bloc par des systèmes numériques.

Bien que l'intelligence humaine soit spéciale, elle ne constitue en aucun cas une limite. Des systèmes artificiels « superintelligents » pourraient opérer des centaines de fois plus vite, analyser vastement plus de données et garder d'énormes quantités « à l'esprit » en même temps, et former des agrégats bien plus grands et plus efficaces que des collectifs d'humains. Ils pourraient supplanter non pas des individus mais des entreprises, des nations, ou notre civilisation dans son ensemble.

## Nous sommes au seuil

Il existe un fort consensus scientifique que l'IAG est *possible*. L'IA dépasse déjà les performances humaines dans de nombreux tests généraux de capacité intellectuelle, y compris récemment le raisonnement et la résolution de problèmes de haut niveau. Les capacités en retard – comme l'apprentissage continu, la planification, la conscience de soi et l'originalité – existent toutes à un certain niveau dans les systèmes d'IA actuels, et des techniques connues existent qui sont susceptibles de toutes les améliorer.

Alors que jusqu'à il y a quelques années de nombreux chercheurs voyaient l'IAG comme étant à des décennies de distance, actuellement les preuves de délais courts vers l'IAG sont fortes :

- Les « lois d'échelle » empiriquement vérifiées relient l'apport computationnel à la capacité de l'IA, et les entreprises sont en voie d'augmenter l'apport computationnel de plusieurs ordres de grandeur au cours des prochaines années. Les ressources humaines et financières dédiées à l'avancement de l'IA égalent maintenant celles d'une douzaine de Projets Manhattan et de plusieurs Projets Apollo.
- Les entreprises d'IA et leurs dirigeants croient publiquement et privément que l'IAG (selon une certaine définition) est réalisable dans quelques années. Ces entreprises ont des informations que le public n'a pas, notamment certaines ayant la prochaine génération de systèmes d'IA en main.
- Les prédicteurs experts avec des historiques prouvés assignent 25% de probabilité à l'arrivée de l'IAG (selon une certaine définition) dans 1-2 ans, et 50% pour 2-5 ans (voir les prédictions Metaculus pour l'IAG ['faible'](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) et ['complète'](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/)).
- L'autonomie (incluant la planification flexible à long terme) accuse un retard dans les systèmes d'IA, mais les grandes entreprises concentrent maintenant leurs vastes ressources sur le développement de systèmes d'IA autonomes et ont informellement nommé 2025 l'[« année de l'agent »](https://techinformed.com/2025-informed-the-year-of-agentic-ai/).
- L'IA contribue de plus en plus à sa propre amélioration. Une fois que les systèmes d'IA seront aussi compétents que les chercheurs humains en IA pour faire de la recherche en IA, un seuil critique pour un progrès rapide vers des systèmes d'IA beaucoup plus puissants sera atteint et mènera probablement à un emballement des capacités de l'IA. (On pourrait soutenir que cet emballement a déjà commencé.)

L'idée que l'IAG plus intelligente que les humains est à des décennies de distance ou plus n'est simplement plus tenable pour la grande majorité des experts du domaine. Les désaccords portent maintenant sur le nombre de mois ou d'années que cela prendra si nous restons sur cette voie. La question centrale à laquelle nous faisons face est : devrions-nous ?

## Ce qui alimente la course à l'IAG

La course vers l'IAG est alimentée par de multiples forces, chacune rendant la situation plus dangereuse. Les grandes entreprises technologiques voient l'IAG comme la technologie d'automatisation ultime – non seulement augmentant les travailleurs humains mais les remplaçant largement ou entièrement. Pour les entreprises, l'enjeu est énorme : l'opportunité de capturer une fraction significative des 100 billions de dollars de production économique annuelle mondiale en automatisant les coûts de main-d'œuvre humaine.

Les nations se sentent contraintes de rejoindre cette course, citant publiquement le leadership économique et scientifique, mais considérant privément l'IAG comme une révolution potentielle dans les affaires militaires comparable aux armes nucléaires. La peur que les rivaux puissent obtenir un avantage stratégique décisif crée une dynamique classique de course aux armements.

Ceux qui poursuivent la superintelligence citent souvent de grandes visions : guérir toutes les maladies, inverser le vieillissement, réaliser des percées dans l'énergie et les voyages spatiaux, ou créer des capacités de planification surhumaines.

De façon moins charitable, ce qui alimente la course c'est le pouvoir. Chaque participant – qu'il s'agisse d'une entreprise ou d'un pays – croit que l'intelligence égale le pouvoir, et qu'il sera le meilleur gardien de ce pouvoir.

Je soutiens que ces motivations sont réelles mais fondamentalement mal orientées : l'IAG *absorbera* et *recherchera* le pouvoir plutôt que de l'accorder ; les technologies créées par l'IA seront *aussi* fortement à double tranchant, et là où elles sont bénéfiques elles peuvent être créées avec des outils d'IA et sans IAG ; et même dans la mesure où l'IAG et ses productions restent sous contrôle, ces dynamiques de course – à la fois corporatives et géopolitiques – rendent les risques à grande échelle pour notre société presque inévitables à moins d'être interrompus de manière décisive.

## L'IAG et la superintelligence posent une menace dramatique à la civilisation

Malgré leur attrait, l'IAG et la superintelligence posent des menaces dramatiques à la civilisation par de multiples voies qui se renforcent :

*Concentration du pouvoir :* l'IA surhumaine pourrait désautonomiser la grande majorité de l'humanité en absorbant d'énormes pans d'activité sociale et économique dans des systèmes d'IA gérés par une poignée d'entreprises géantes (qui peuvent à leur tour soit être reprises par les gouvernements, soit effectivement les reprendre).

*Perturbation massive :* l'automatisation en bloc de la plupart des emplois cognitifs, le remplacement de nos systèmes épistémiques actuels, et le déploiement de vastes nombres d'agents non-humains actifs bouleverseraient la plupart de nos systèmes civilisationnels actuels dans une période relativement courte.

*Catastrophes :* en proliférant la capacité – potentiellement au-dessus du niveau humain – de créer de nouvelles technologies militaires et destructrices et en la découplant des systèmes sociaux et légaux fondant la responsabilité, les catastrophes physiques dues aux armes de destruction massive deviennent dramatiquement plus probables.

*Géopolitique et guerre :* les grandes puissances mondiales ne resteront pas les bras croisés si elles sentent qu'une technologie qui pourrait fournir un « avantage stratégique décisif » est développée par leurs adversaires.

*Emballement et perte de contrôle :* À moins que cela ne soit spécifiquement empêché, l'IA surhumaine aura toutes les incitations à s'améliorer davantage et pourrait largement dépasser les humains en vitesse, traitement de données, et sophistication de la pensée. Il n'y a aucune façon significative dans laquelle nous pouvons être aux commandes d'un tel système. Une telle IA n'accordera pas le pouvoir aux humains ; nous lui accorderons le pouvoir, ou elle le prendra.

Beaucoup de ces risques demeurent même si le problème technique d'« alignement » – s'assurer que l'IA avancée fait de manière fiable ce que les humains veulent qu'elle fasse – est résolu. L'IA présente un énorme défi dans la façon dont elle sera gérée, et de très nombreux aspects de cette gestion deviennent incroyablement difficiles ou insolubles une fois que l'intelligence humaine est dépassée.

Plus fondamentalement, le type d'IA généraliste surhumaine actuellement poursuivi aurait, par sa nature même, des objectifs, une agence et des capacités dépassant les nôtres. Elle serait intrinsèquement incontrôlable – comment pouvons-nous contrôler quelque chose que nous ne pouvons ni comprendre ni prédire ? Elle ne serait pas un outil technologique pour l'usage humain, mais une seconde espèce d'intelligence sur Terre à côté de la nôtre. Si on lui permettait de progresser davantage, elle ne constituerait pas seulement une seconde espèce mais une espèce de remplacement.

Peut-être nous traiterait-elle bien, peut-être pas. Mais l'avenir lui appartiendrait, pas à nous. L'ère humaine serait terminée.

## Ce n'est pas inévitable ; l'humanité peut, très concrètement, décider de ne pas construire son remplacement.

La création d'une IAG surhumaine est loin d'être inévitable. Nous pouvons l'empêcher par un ensemble coordonné de mesures de gouvernance :

D'abord, nous avons besoin d'une comptabilité robuste et d'une supervision du calcul IA (la « puissance de calcul »), qui est un facilitateur fondamental et un levier pour gouverner les systèmes d'IA à grande échelle. Cela nécessite à son tour une mesure et un rapport standardisés du calcul total utilisé pour entraîner les modèles d'IA et les faire fonctionner, et des méthodes techniques de comptage, certification et vérification du calcul utilisé.

Deuxièmement, nous devrions implémenter des plafonds stricts sur le calcul IA, à la fois pour l'entraînement et pour l'opération ; ceux-ci empêchent l'IA d'être à la fois trop puissante et d'opérer trop rapidement. Ces plafonds peuvent être implémentés par des exigences légales et des mesures de sécurité basées sur le matériel intégrées dans les puces spécialisées pour l'IA, analogues aux fonctions de sécurité dans les téléphones modernes. Parce que le matériel spécialisé pour l'IA est fabriqué par seulement une poignée d'entreprises, la vérification et l'application sont faisables par la chaîne d'approvisionnement existante.

Troisièmement, nous avons besoin d'une responsabilité renforcée pour les systèmes d'IA les plus dangereux. Ceux qui développent une IA qui combine haute autonomie, large généralité et intelligence supérieure devraient faire face à une responsabilité objective pour les dommages, tandis que des régimes d'exonération de cette responsabilité encourageraient le développement de systèmes plus limités et contrôlables.

Quatrièmement, nous avons besoin d'une réglementation à niveaux basée sur les niveaux de risque. Les systèmes les plus capables et dangereux nécessiteraient des garanties étendues de sécurité et contrôlabilité avant développement et déploiement, tandis que les systèmes moins puissants ou plus spécialisés feraient face à une supervision proportionnée. Ce cadre réglementaire devrait éventuellement opérer aux niveaux national et international.

Cette approche – avec une spécification détaillée donnée dans le document complet – est pratique : bien qu'une coordination internationale soit nécessaire, la vérification et l'application peuvent fonctionner par le petit nombre d'entreprises contrôlant la chaîne d'approvisionnement de matériel spécialisé. Elle est aussi flexible : les entreprises peuvent encore innover et profiter du développement de l'IA, juste avec des limites claires sur les systèmes les plus dangereux.

Le confinement à long terme du pouvoir et du risque de l'IA nécessiterait des accords internationaux basés sur l'intérêt propre et commun, tout comme le contrôle de la prolifération des armes nucléaires le fait maintenant. Mais nous pouvons commencer immédiatement avec une supervision et une responsabilité renforcées, tout en construisant vers une gouvernance plus complète.

L'ingrédient clé manquant est la volonté politique et sociale de reprendre le contrôle du processus de développement de l'IA. La source de cette volonté, si elle vient à temps, sera la réalité elle-même – c'est-à-dire, de la prise de conscience généralisée des vraies implications de ce que nous faisons.

## Nous pouvons concevoir une IA-outil pour autonomiser l'humanité

Plutôt que de poursuivre une IAG incontrôlable, nous pouvons développer une « IA-outil » puissante qui améliore les capacités humaines tout en restant sous un contrôle humain significatif. Les systèmes d'IA-outil peuvent être extrêmement capables tout en évitant la dangereuse triple intersection de haute autonomie, large généralité et intelligence surhumaine, tant que nous les concevons pour être contrôlables à un niveau proportionné à leur capacité. Ils peuvent aussi être combinés en systèmes sophistiqués qui maintiennent la supervision humaine tout en délivrant des bénéfices transformateurs.

L'IA-outil peut révolutionner la médecine, accélérer la découverte scientifique, améliorer l'éducation et perfectionner les processus démocratiques. Quand elle est correctement gouvernée, elle peut rendre les experts humains et les institutions plus efficaces plutôt que de les remplacer. Bien que de tels systèmes soient encore hautement perturbateurs et nécessitent une gestion attentive, les risques qu'ils posent sont fondamentalement différents de l'IAG : ce sont des risques que nous pouvons gouverner, comme ceux d'autres technologies puissantes, pas des menaces existentielles à l'agence humaine et à la civilisation. Et crucialement, quand elle est sagement développée, l'IA-outil peut aider les gens à gouverner l'IA puissante et gérer ses effets.

Cette approche nécessite de repenser à la fois comment l'IA est développée et comment ses bénéfices sont distribués. De nouveaux modèles de développement d'IA public et à but non lucratif, des cadres réglementaires robustes, et des mécanismes pour distribuer les bénéfices économiques plus largement peuvent aider à s'assurer que l'IA autonomise l'humanité dans son ensemble plutôt que de concentrer le pouvoir en quelques mains. L'IA elle-même peut aider à construire de meilleures institutions sociales et de gouvernance, permettant de nouvelles formes de coordination et de discours qui renforcent plutôt qu'affaiblissent la société humaine. Les établissements de sécurité nationale peuvent mobiliser leur expertise pour rendre les systèmes d'IA-outil véritablement sûrs et fiables, et une vraie source de défense ainsi que de puissance nationale.

Nous pourrions éventuellement choisir de développer des systèmes encore plus puissants et plus souverains qui ressemblent moins à des outils et – nous pouvons l'espérer – plus à des bienfaiteurs sages et puissants. Mais nous ne devrions le faire qu'après avoir développé la compréhension scientifique et la capacité de gouvernance pour le faire en sécurité. Une décision si monumentale et irréversible devrait être prise délibérément par l'humanité dans son ensemble, pas par défaut dans une course entre entreprises technologiques et nations.

## Entre les mains humaines

Les gens veulent le bien qui vient de l'IA : des outils utiles qui les autonomisent, dynamisent les opportunités et la croissance économiques, et promettent des percées en science, technologie et éducation. Pourquoi pas ? Mais quand on leur demande, d'écrasantes majorités du grand public [veulent un développement de l'IA plus lent et plus prudent](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation), et ne veulent pas d'IA plus intelligente que les humains qui les remplacera dans leurs emplois et ailleurs, remplira leur culture et bien commun informationnel de contenu non-humain, concentrera le pouvoir dans un petit ensemble d'entreprises, posera des risques extrêmes à grande échelle globale, et finalement menacera de désautonomiser ou remplacer leur espèce. Pourquoi le voudraient-ils ?

Nous *pouvons* avoir l'un sans l'autre. Cela commence par décider que notre destin n'est pas dans la supposée inévitabilité d'une technologie ou entre les mains de quelques PDG de la Silicon Valley, mais dans le reste de nos mains si nous nous en saisissons. Fermons les Portes, et gardons l'avenir humain.