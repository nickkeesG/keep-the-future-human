# Annexes

Informations complémentaires, notamment : détails techniques sur la comptabilisation du calcul, exemple de mise en œuvre d'une « fermeture des portes », détails d'un régime de responsabilité stricte pour l'IAG, et approche par niveaux pour les normes de sécurité et de sûreté de l'IAG.

## Annexe A : Détails techniques de la comptabilisation du calcul

Une méthode détaillée pour établir à la fois la « vérité de terrain » ainsi que de bonnes approximations du calcul total utilisé pour l'entraînement et l'inférence est nécessaire pour des contrôles significatifs basés sur le calcul. Voici un exemple de la façon dont la « vérité de terrain » pourrait être comptabilisée au niveau technique.

**Définitions :**

*Graphe causal de calcul :* Pour une sortie O donnée d'un modèle d'IA, il existe un ensemble de calculs numériques pour lesquels modifier le résultat de ce calcul pourrait potentiellement changer O. (Ceci devrait être présumé de manière prudente, c'est-à-dire qu'il devrait y avoir une raison claire de croire qu'un calcul est indépendant d'un précurseur qui se produit à la fois plus tôt dans le temps et a un chemin causal physique potentiel d'effet.) Cela inclut les calculs effectués par le modèle d'IA pendant l'inférence, ainsi que les calculs qui ont contribué à l'entrée, la préparation des données et l'entraînement du modèle. Comme chacun de ceux-ci peut lui-même être la sortie d'un modèle d'IA, ceci est calculé récursivement, interrompu là où un humain a apporté un changement significatif à l'entrée.

*Calcul d'entraînement :* Le calcul total, en FLOP ou autres unités, impliqué par le graphe causal de calcul d'un réseau de neurones (incluant la préparation des données, l'entraînement et l'ajustement fin, et tous autres calculs.)

*Calcul de sortie :* Le calcul total dans le graphe causal de calcul d'une sortie d'IA donnée, incluant tous les réseaux de neurones (et incluant leur calcul d'entraînement) et autres calculs contribuant à cette sortie.

*Taux de calcul d'inférence :* Dans une série de sorties, le taux de changement (en FLOP/s ou autres unités) du calcul de sortie entre les sorties, c'est-à-dire le calcul utilisé pour produire la sortie suivante, divisé par l'intervalle de temps entre les sorties.

**Exemples et approximations :**

- Pour un réseau de neurones unique entraîné sur des données créées par des humains, le calcul d'entraînement est simplement le calcul d'entraînement total tel que rapporté habituellement.
- Pour un tel réseau de neurones effectuant l'inférence à un rythme régulier, le taux de calcul d'inférence est approximativement la vitesse totale du cluster de calcul effectuant l'inférence en FLOP/s.
- Pour l'ajustement fin de modèle, le calcul d'entraînement du modèle complet est donné par le calcul d'entraînement du modèle non ajusté plus le calcul effectué pendant l'ajustement fin et pour préparer toute donnée utilisée dans l'ajustement fin.
- Pour un modèle distillé, le calcul d'entraînement du modèle complet inclut l'entraînement à la fois du modèle distillé et du modèle plus large utilisé pour fournir des données synthétiques ou autre entrée d'entraînement.
- Si plusieurs modèles sont entraînés, mais que de nombreux « essais » sont écartés sur la base du jugement humain, ceux-ci ne comptent pas dans le calcul d'entraînement ou de sortie du modèle retenu.

## Annexe B : Exemple de mise en œuvre d'une fermeture des portes

**Exemple de mise en œuvre :** Voici un exemple de la façon dont une fermeture des portes pourrait fonctionner, avec une limite de 10<sup>27</sup> FLOP pour l'entraînement et 10<sup>20</sup> FLOP/s pour l'inférence (faire fonctionner l'IA) :

**1. Pause :** Pour des raisons de sécurité nationale, l'exécutif américain demande à toutes les entreprises basées aux États-Unis, faisant des affaires aux États-Unis, ou utilisant des puces fabriquées aux États-Unis, de cesser tout nouvel entraînement d'IA qui pourrait dépasser la limite de 10<sup>27</sup> FLOP de calcul d'entraînement. Les États-Unis devraient entamer des discussions avec d'autres pays hébergeant le développement d'IA, les encourageant fortement à prendre des mesures similaires et indiquant que la pause américaine pourrait être levée s'ils choisissent de ne pas se conformer.

**2. Supervision et licences américaines :** Par décret exécutif ou action d'une agence réglementaire existante, les États-Unis exigent que dans un délai d'(disons) un an :

- Tous les entraînements d'IA estimés au-dessus de 10<sup>25</sup> FLOP effectués par des entreprises opérant aux États-Unis soient enregistrés dans une base de données maintenue par une agence réglementaire américaine. (Note : Une version légèrement plus faible de ceci avait déjà été incluse dans le décret exécutif américain de 2023 sur l'IA, maintenant abrogé, exigeant l'enregistrement pour les modèles au-dessus de 10<sup>26</sup> FLOP.)
- Tous les fabricants de matériel pertinent pour l'IA opérant aux États-Unis ou faisant affaire avec le gouvernement américain adhèrent à un ensemble d'exigences sur leur matériel spécialisé et le logiciel qui le pilote. (Beaucoup de ces exigences pourraient être intégrées dans des mises à jour logicielles et de micrologiciel pour le matériel existant, mais des solutions à long terme et robustes nécessiteraient des changements aux générations ultérieures de matériel.) Parmi celles-ci figure une exigence que si le matériel fait partie d'un cluster interconnecté haute vitesse capable d'exécuter 10<sup>18</sup> FLOP/s de calcul, un niveau de vérification plus élevé est requis, qui inclut une permission régulière par un « gouverneur » distant qui reçoit à la fois la télémétrie et les demandes d'effectuer du calcul supplémentaire.
- Le dépositaire rapporte le calcul total effectué sur son matériel à l'agence maintenant la base de données américaine.
- Des exigences plus fortes sont progressivement mises en place pour permettre une supervision et un système de permissions à la fois plus sûrs et plus flexibles.

**3. Supervision internationale :**

- Les États-Unis, la Chine, et tout autre pays hébergeant une capacité avancée de fabrication de puces négocient un accord international.
- Cet accord crée une nouvelle agence internationale, analogue à l'Agence internationale de l'énergie atomique, chargée de superviser l'entraînement et l'exécution d'IA.
- Les pays signataires doivent exiger que leurs fabricants nationaux de matériel d'IA se conforment à un ensemble d'exigences au moins aussi strictes que celles imposées aux États-Unis.
- Les dépositaires sont maintenant tenus de rapporter les chiffres de calcul d'IA à la fois aux agences dans leurs pays d'origine ainsi qu'à un nouveau bureau au sein de l'agence internationale.
- Les pays supplémentaires sont fortement encouragés à rejoindre l'accord international existant : les contrôles à l'exportation par les pays signataires restreignent l'accès au matériel haut de gamme par les non-signataires tandis que les signataires peuvent recevoir un soutien technique dans la gestion de leurs systèmes d'IA.

**4. Vérification et application internationales :**

- Le système de vérification matérielle est mis à jour de sorte qu'il rapporte l'utilisation du calcul à la fois au dépositaire original et aussi directement au bureau de l'agence internationale.
- L'agence, via la discussion avec les signataires de l'accord international, s'accorde sur des limitations de calcul qui prennent alors force de loi dans les pays signataires.
- En parallèle, un ensemble de normes internationales peut être développé de sorte que l'entraînement et le fonctionnement d'IA au-dessus d'un seuil de calcul (mais en dessous de la limite) soient tenus d'adhérer à ces normes.
- L'agence peut, si nécessaire pour compenser de meilleurs algorithmes etc., abaisser la limite de calcul. Ou, si c'est jugé sûr et souhaitable (au niveau par exemple de garanties de sécurité prouvables), relever la limite de calcul.

## Annexe C : Détails d'un régime de responsabilité stricte pour l'IAG

**Détails d'un régime de responsabilité stricte pour l'IAG**

- La création et l'exploitation d'un système d'IA avancé qui est hautement général, capable et autonome, est considérée comme une activité « anormalement dangereuse ».
- En tant que telle, la responsabilité par défaut pour l'entraînement et l'exploitation de tels systèmes est une responsabilité objective, conjointe et solidaire (ou son équivalent hors États-Unis) pour tous dommages causés par le modèle ou ses sorties/actions.
- Une responsabilité personnelle sera imposée aux dirigeants et membres du conseil d'administration en cas de négligence grave ou de faute intentionnelle. Ceci devrait inclure des sanctions pénales pour les cas les plus flagrants.
- Il existe de nombreux régimes d'exonération sous lesquels la responsabilité revient à la responsabilité par défaut (basée sur la faute, aux États-Unis) à laquelle les personnes et entreprises seraient normalement soumises.
	- Modèles entraînés et exploités en dessous d'un certain seuil de calcul (qui serait au moins 10 fois inférieur aux plafonds décrits ci-dessus.)
	- IA qui est « faible » (grossièrement, en dessous du niveau d'expert humain aux tâches pour lesquelles elle est destinée) et/ou
	- IA qui est « étroite » (ayant une portée fixe et assez limitée de tâches et d'opérations pour lesquelles elle est spécifiquement conçue et entraînée) et/ou
	- IA qui est « passive » (très limitée dans sa capacité – même sous modification modeste – à entreprendre des actions ou effectuer des tâches complexes multi-étapes sans implication et contrôle humains directs.)
	- Une IA qui est garantie d'être sûre, sécurisée et contrôlable (prouvablement sûre, ou une analyse de risque indique un niveau négligeable de dommage attendu.)
- Les régimes d'exonération peuvent être revendiqués sur la base d'un [dossier de sécurité](https://arxiv.org/abs/2410.21572) préparé par le développeur d'IA et approuvé par une agence ou un auditeur accrédité par une agence. Pour revendiquer un régime d'exonération basé sur le calcul, le développeur doit simplement fournir des estimations crédibles du calcul d'entraînement total et du taux d'inférence maximal
- La législation définirait explicitement les situations dans lesquelles une réparation par injonction du développement de systèmes d'IA avec un risque élevé de préjudice public serait appropriée.
- Les consortiums d'entreprises, travaillant avec les ONG et agences gouvernementales, devraient développer des normes et standards définissant ces termes, comment les régulateurs devraient accorder les régimes d'exonération, comment les développeurs d'IA devraient développer les dossiers de sécurité, et comment les tribunaux devraient interpréter la responsabilité là où les régimes d'exonération ne sont pas proactivement revendiqués.

## Annexe D : Une approche par niveaux pour les normes de sécurité et de sûreté de l'IAG

**Une approche par niveaux pour les normes de sécurité et de sûreté de l'IAG**

| Niveau de risque | Déclencheur(s) | Exigences pour l'entraînement | Exigences pour le déploiement |
| --- | --- | --- | --- |
| NR-0 | IA faible en autonomie, généralité et intelligence | aucune | aucune |
| NR-1 | IA forte dans l'un des domaines : autonomie, généralité et intelligence | aucune | Basé sur le risque et l'utilisation, potentiellement dossiers de sécurité approuvés par les autorités nationales partout où le modèle peut être utilisé |
| NR-2 | IA forte dans deux des domaines : autonomie, généralité et intelligence | Enregistrement auprès de l'autorité nationale ayant juridiction sur le développeur | Dossier de sécurité bornant le risque de dommage majeur en dessous des niveaux autorisés plus audits de sécurité indépendants (incluant tests d'intrusion boîte noire et boîte blanche) approuvés par les autorités nationales partout où le modèle peut être utilisé |
| NR-3 | IAG forte en autonomie, généralité et intelligence | Pré-approbation du plan de sécurité et de sûreté par l'autorité nationale ayant juridiction sur le développeur | Dossier de sécurité garantissant un risque borné de dommage majeur en dessous des niveaux autorisés ainsi que des spécifications requises, incluant cybersécurité, contrôlabilité, un interrupteur d'urgence non-amovible, alignement avec les valeurs humaines, et robustesse à l'usage malveillant. |
| NR-4 | Tout modèle qui dépasse également soit 10<sup>27</sup> FLOP d'entraînement soit 10<sup>20</sup> FLOP/s d'inférence | Interdit en attendant la levée convenue internationalement du plafond de calcul | Interdit en attendant la levée convenue internationalement du plafond de calcul |

Classifications de risque et normes de sécurité/sûreté, avec niveaux basés sur les seuils de calcul ainsi que les combinaisons d'autonomie, généralité et intelligence élevées :

- *Autonomie forte* s'applique si le système est capable d'effectuer, ou peut facilement être amené à effectuer, des tâches multi-étapes et/ou entreprendre des actions complexes qui sont pertinentes pour le monde réel, sans supervision ou intervention humaine significative. Exemples : véhicules autonomes et robots ; bots de trading financier. Contre-exemples : GPT-4 ; classificateurs d'images
- *Généralité forte* indique une large portée d'application, performance de tâches pour lesquelles le modèle n'a pas été délibérément et spécifiquement entraîné, et capacité significative à apprendre de nouvelles tâches. Exemples : GPT-4 ; mu-zero. Contre-exemples : AlphaFold ; véhicules autonomes ; générateurs d'images
- *Intelligence forte* correspond à égaler la performance de niveau expert humain sur les tâches pour lesquelles le modèle performe le mieux (et pour un modèle général, à travers une large gamme de tâches.) Exemples : AlphaFold ; mu-zero ; o3. Contre-exemples : GPT-4 ; Siri