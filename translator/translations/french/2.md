# Chapitre 2 - Les bases des réseaux de neurones en IA

Comment fonctionnent les systèmes d'IA modernes, et à quoi peut-on s'attendre de la prochaine génération d'IA ?

Pour comprendre comment se déploieront les conséquences du développement d'une IA plus puissante, il est essentiel d'intégrer quelques notions fondamentales. Cette section et les deux suivantes développent ces concepts, couvrant tour à tour ce qu'est l'IA moderne, comment elle exploite des calculs massifs, et les façons dont elle croît rapidement en généralité et en capacité.[^1]

Il existe de nombreuses façons de définir l'intelligence artificielle, mais pour nos propos, la propriété clé de l'IA est que tandis qu'un programme informatique standard est une liste d'instructions pour accomplir une tâche, un système d'IA apprend à partir de données ou d'expériences pour accomplir des tâches *sans qu'on lui dise explicitement comment procéder.*

Presque toute l'IA moderne significative repose sur des réseaux de neurones. Il s'agit de structures mathématiques/computationnelles, représentées par un très grand ensemble (milliards ou billions) de nombres (« poids »), qui accomplissent bien une tâche d'entraînement. Ces poids sont façonnés (ou peut-être « cultivés » ou « découverts ») en les ajustant de manière itérative pour que le réseau de neurones améliore un score numérique (appelé « perte ») défini pour bien accomplir une ou plusieurs tâches.[^2] Ce processus est appelé *entraînement* du réseau de neurones.[^3]

Il existe de nombreuses techniques pour cet entraînement, mais ces détails sont bien moins pertinents que les façons dont le score est défini, et comment celles-ci aboutissent aux différentes tâches que le réseau de neurones accomplit bien. Une distinction clé a historiquement été établie entre l'IA « étroite » et « générale ».

L'IA étroite est délibérément entraînée pour faire une tâche particulière ou un petit ensemble de tâches (comme reconnaître des images ou jouer aux échecs) ; elle nécessite un réentraînement pour de nouvelles tâches, et a un champ de capacités restreint. Nous avons de l'IA étroite surhumaine, ce qui signifie que pour presque toute tâche discrète et bien définie qu'une personne peut faire, nous pouvons probablement construire un score puis entraîner avec succès un système d'IA étroite à le faire mieux qu'un humain.

Les systèmes d'IA à usage général (IAUG) peuvent accomplir un large éventail de tâches, y compris beaucoup pour lesquelles ils n'ont pas été explicitement entraînés ; ils peuvent aussi apprendre de nouvelles tâches dans le cadre de leur fonctionnement. Les « modèles multimodaux » actuels[^4] comme ChatGPT illustrent cela : entraînés sur un très large corpus de texte et d'images, ils peuvent s'engager dans un raisonnement complexe, écrire du code, analyser des images, et assister dans un vaste éventail de tâches intellectuelles. Bien qu'encore assez différents de l'intelligence humaine de façons que nous verrons en détail ci-dessous, leur généralité a causé une révolution en IA.[^5]

## L'imprévisibilité : une caractéristique clé des systèmes d'IA

Une différence clé entre les systèmes d'IA et les logiciels conventionnels réside dans la prévisibilité. La sortie d'un logiciel standard peut être imprévisible – en effet, c'est parfois pour cela que nous écrivons des logiciels, pour obtenir des résultats que nous n'aurions pas pu prédire. Mais les logiciels conventionnels font rarement quelque chose pour lequel ils n'ont pas été programmés – leur portée et leur comportement sont généralement conformes à leur conception. Un programme d'échecs de premier plan peut faire des coups qu'aucun humain ne pourrait prédire (sinon ils pourraient battre ce programme d'échecs !) mais il ne fera généralement rien d'autre que jouer aux échecs.

Comme les logiciels conventionnels, l'IA étroite a une portée et un comportement prévisibles mais peut avoir des résultats imprévisibles. C'est en réalité juste une autre façon de définir l'IA étroite : comme une IA qui ressemble aux logiciels conventionnels dans sa prévisibilité et son champ d'action.

L'IA à usage général est différente : sa portée (les domaines auxquels elle s'applique), son comportement (les types de choses qu'elle fait), et ses résultats (ses sorties réelles) peuvent tous être imprévisibles.[^6] GPT-4 a été entraîné uniquement pour générer du texte avec précision, mais a développé de nombreuses capacités que ses entraîneurs n'avaient pas prédites ou voulues. Cette imprévisibilité découle de la complexité de l'entraînement : parce que les données d'entraînement contiennent des sorties de nombreuses tâches différentes, l'IA doit effectivement apprendre à accomplir ces tâches pour bien prédire.

Cette imprévisibilité des systèmes d'IA générale est assez fondamentale. Bien qu'en principe il soit possible de construire soigneusement des systèmes d'IA qui ont des limites garanties sur leur comportement (comme mentionné plus tard dans l'essai), la façon dont les systèmes d'IA sont créés maintenant les rend imprévisibles en pratique et même en principe.

## IA passive, agents, systèmes autonomes, et alignement

Cette imprévisibilité devient particulièrement importante quand nous considérons comment les systèmes d'IA sont réellement déployés et utilisés pour atteindre divers objectifs.

De nombreux systèmes d'IA sont relativement passifs en ce sens qu'ils fournissent principalement de l'information, et l'utilisateur prend des actions. D'autres, communément appelés *agents*, prennent eux-mêmes des actions, avec des niveaux variables d'implication de l'utilisateur. Ceux qui prennent des actions avec relativement moins d'input ou de supervision externes peuvent être qualifiés de plus *autonomes*. Cela forme un spectre en termes d'indépendance d'action, d'outils passifs à agents autonomes.[^7]

Quant aux objectifs des systèmes d'IA, ceux-ci peuvent être directement liés à leur objectif d'entraînement (par exemple l'objectif de « gagner » pour un système qui joue au Go est aussi explicitement ce pour quoi il a été entraîné). Ou ils peuvent ne pas l'être : l'objectif d'entraînement de ChatGPT est en partie de prédire le texte, en partie d'être un assistant utile. Mais quand il fait une tâche donnée, son objectif lui est fourni par l'utilisateur. Les objectifs peuvent aussi être créés par un système d'IA lui-même, seulement très indirectement liés à son objectif d'entraînement.[^8]

Les objectifs sont étroitement liés à la question de l'« alignement », c'est-à-dire la question de savoir si les systèmes d'IA vont *faire ce que nous voulons qu'ils fassent*. Cette question simple cache un niveau énorme de subtilité.[^9] Pour l'instant, notez que « nous » dans cette phrase pourrait se référer à de nombreuses personnes et groupes différents, menant à différents types d'alignement. Par exemple, une IA pourrait être très *obéissante* (ou [« loyale »](https://arxiv.org/abs/2003.11157)) à son utilisateur – ici « nous » c'est « chacun de nous ». Ou elle pourrait être plus *souveraine*, étant principalement guidée par ses propres objectifs et contraintes, mais agissant toujours largement dans l'intérêt commun du bien-être humain – « nous » c'est alors « l'humanité » ou « la société ». Entre les deux se trouve un spectre où une IA serait largement obéissante, mais pourrait refuser de prendre des actions qui nuisent aux autres ou à la société, violent la loi, etc.

Ces deux axes – niveau d'autonomie et type d'alignement – ne sont pas entièrement indépendants. Par exemple, un système passif souverain, bien que pas tout à fait auto-contradictoire, est un concept en tension, comme l'est un agent autonome obéissant.[^10] Il y a un sens clair dans lequel l'autonomie et la souveraineté tendent à aller de pair. Dans la même veine, la prévisibilité tend à être plus élevée dans les systèmes d'IA « passifs » et « obéissants », tandis que les souverains ou autonomes tendront à être plus imprévisibles. Tout cela sera crucial pour comprendre les ramifications de l'IAG et de la superintelligence potentielles.

Créer une IA vraiment alignée, de quelque type que ce soit, nécessite de résoudre trois défis distincts :

1. Comprendre ce que « nous » voulons – ce qui est complexe que « nous » signifie une personne ou organisation spécifique (loyauté) ou l'humanité au sens large (souveraineté) ;
2. Construire des systèmes qui agissent régulièrement en accord avec ces volontés – essentiellement créer un comportement positif cohérent ;
3. Plus fondamentalement, faire des systèmes qui « se soucient » genuinement de ces volontés plutôt que d'agir simplement comme s'ils s'en souciaient.

La distinction entre comportement fiable et souci genuine est cruciale. Tout comme un employé humain pourrait suivre les ordres parfaitement tout en manquant de véritable engagement envers la mission de l'organisation, un système d'IA pourrait agir de façon alignée sans vraiment valoriser les préférences humaines. Nous pouvons entraîner les systèmes d'IA à dire et faire des choses par le feedback, et ils peuvent apprendre à raisonner sur ce que les humains veulent. Mais les faire *genuinement* valoriser les préférences humaines est un défi bien plus profond.[^11]

Les difficultés profondes à résoudre ces défis d'alignement, et leurs implications pour le risque d'IA, seront explorées plus loin ci-dessous. Pour l'instant, comprenez que l'alignement n'est pas juste une caractéristique technique que nous ajoutons aux systèmes d'IA, mais un aspect fondamental de leur architecture qui façonne leur relation avec l'humanité.

[^1]: Pour une introduction douce mais technique à l'apprentissage automatique et à l'IA, particulièrement les modèles de langage, voir [ce site.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) Pour un autre guide moderne sur les risques d'extinction de l'IA, voir [cette pièce.](https://www.thecompendium.ai/) Pour une analyse scientifique complète et faisant autorité de l'état de la sécurité de l'IA, voir le récent [Rapport international sur la sécurité de l'IA.](https://arxiv.org/abs/2501.17805)

[^2]: L'entraînement se produit typiquement en cherchant un maximum local du score dans un espace de haute dimension donné par les poids du modèle. En vérifiant comment le score change quand les poids sont ajustés, l'algorithme d'entraînement identifie quels ajustements améliorent le score le plus, et déplace les poids dans cette direction.

[^3]: Par exemple, dans un problème de reconnaissance d'image, le réseau de neurones sortirait des probabilités pour les étiquettes de l'image. Un score serait lié à la probabilité que l'IA accorde à la bonne réponse. La procédure d'entraînement ajusterait alors les poids pour que la prochaine fois, l'IA sorte une probabilité plus élevée pour l'étiquette correcte pour cette image. Ceci est alors répété un énorme nombre de fois. La même procédure de base est utilisée dans l'entraînement d'essentiellement tous les réseaux de neurones modernes, quoique avec des mécanismes de score plus complexes.

[^4]: La plupart des modèles multimodaux utilisent l'architecture « transformateur » pour traiter et générer plusieurs types de données (texte, images, son). Ceux-ci peuvent tous être décomposés en, puis traités sur le même pied, comme différents types de « jetons ». Les modèles multimodaux sont entraînés d'abord pour prédire précisément les jetons dans des ensembles de données massifs, puis raffinés par apprentissage par renforcement pour améliorer les capacités et façonner les comportements.

[^5]: Que les modèles de langage soient entraînés pour faire une chose – prédire les mots – a amené certains à les appeler IA étroite. Mais c'est trompeur : parce que bien prédire le texte nécessite tant de capacités différentes, cette tâche d'entraînement mène à un système étonnamment général. Notez aussi que ces systèmes sont extensivement entraînés par apprentissage par renforcement, représentant effectivement des milliers de personnes donnant au modèle un signal de récompense quand il fait du bon travail dans n'importe laquelle des nombreuses choses qu'il fait. Il hérite alors d'une généralité significative des gens donnant ce feedback.

[^6]: Il y a plusieurs façons dont l'IA est imprévisible. Une est que dans le cas général on ne peut prédire ce qu'un algorithme fera sans réellement l'exécuter ; il y a des [théorèmes](https://arxiv.org/abs/1310.3225) à cet effet. Ceci peut être vrai juste parce que la sortie d'algorithmes peut être complexe. Mais c'est particulièrement clair et pertinent dans le cas (comme aux échecs ou au Go) où la prédiction impliquerait une capacité (battre l'IA) que le prédicteur potentiel n'a pas. Deuxième, un système d'IA donné ne produira pas toujours la même sortie même avec la même entrée – ses sorties contiennent de l'aléatoire ; ceci se couple aussi avec l'imprévisibilité algorithmique. Troisième, des capacités inattendues et émergentes peuvent surgir de l'entraînement, signifiant que même les *types* de choses qu'un système d'IA peut et va faire sont imprévisibles ; Ce dernier type est particulièrement important pour les considérations de sécurité.

[^7]: Voir [ici](https://arxiv.org/abs/2502.02649) pour une revue en profondeur de ce qu'on entend par « agent autonome » (avec des arguments éthiques contre leur construction).

[^8]: Vous pouvez parfois entendre « l'IA ne peut pas avoir ses propres objectifs ». C'est du pur non-sens. Il est facile de générer des exemples où l'IA a ou développe des objectifs qui ne lui ont jamais été donnés et ne sont connus que d'elle-même. Vous ne voyez pas beaucoup cela dans les modèles multimodaux populaires actuels parce que c'est entraîné hors d'eux ; cela pourrait tout aussi facilement être entraîné en eux.

[^9]: Il y a une large littérature. Sur le problème général voir [*The Alignment Problem*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821) de Christian, et [*Human-Compatible*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) de Russell. Du côté plus technique voir par exemple [cet article](https://arxiv.org/abs/2209.00626).

[^10]: Nous verrons plus tard que tandis que de tels systèmes vont contre la tendance, cela les rend en fait très intéressants et utiles.

[^11]: Ceci ne veut pas dire que nous nécessitons des émotions ou de la sentience. Plutôt, il est énormément difficile de l'extérieur d'un système de savoir quels sont ses objectifs internes, préférences, et valeurs. « Genuine » ici signifierait que nous avons des raisons assez fortes de nous y fier que dans le cas de systèmes critiques nous pouvons parier nos vies dessus.