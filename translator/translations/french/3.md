# Chapitre 3 - Aspects clés de la fabrication des systèmes d'IA générale modernes

La plupart des systèmes d'IA les plus avancés au monde sont créés selon des méthodes étonnamment similaires. Voici les principes de base.

Pour vraiment comprendre un être humain, il faut connaître certains aspects de la biologie, de l'évolution, de l'éducation des enfants, et bien plus encore ; pour comprendre l'IA, il faut aussi connaître sa méthode de fabrication. Au cours des cinq dernières années, les systèmes d'IA ont énormément évolué tant en capacités qu'en complexité. Un facteur clé de cette évolution a été la disponibilité de très grandes quantités de calcul (ou familièrement « puissance de calcul » dans le contexte de l'IA).

Les chiffres sont stupéfiants. Environ 10 <sup>25</sup> -10 <sup>26</sup> « opérations en virgule flottante » (FLOP) [^1] sont utilisées pour l'entraînement de modèles comme la série GPT, Claude, Gemini, etc.[^2] (À titre de comparaison, si chaque être humain sur Terre travaillait sans arrêt en effectuant un calcul toutes les cinq secondes, il faudrait environ un milliard d'années pour accomplir cela.) Cette énorme quantité de calcul permet l'entraînement de modèles comportant jusqu'à des billions de paramètres sur des téraoctets de données – une grande fraction de tout le texte de qualité qui ait jamais été écrit, accompagnée de vastes bibliothèques de sons, d'images et de vidéos. En complétant cet entraînement par un entraînement supplémentaire approfondi renforçant les préférences humaines et les bonnes performances de tâches, les modèles ainsi entraînés présentent des performances rivalisant avec l'humain sur un éventail significatif de tâches intellectuelles de base, incluant le raisonnement et la résolution de problèmes.

Nous savons également (très, très approximativement) quelle vitesse de calcul, en opérations par seconde, est suffisante pour que la vitesse d'*inférence* [^3] d'un tel système égale la *vitesse* de traitement de texte humain. Elle est d'environ 10 <sup>15</sup> -10 <sup>16</sup> FLOP par seconde.[^4]

Bien qu'ils soient puissants, ces modèles sont par nature limités de manières importantes, tout à fait analogues à la façon dont un individu humain serait limité s'il était forcé de simplement produire du texte à un rythme fixe de mots par minute, sans s'arrêter pour réfléchir ou utiliser des outils supplémentaires. Les systèmes d'IA plus récents s'attaquent à ces limitations par un processus et une architecture plus complexes combinant plusieurs éléments clés :

- Un ou plusieurs réseaux de neurones, avec un modèle fournissant la capacité cognitive centrale, et jusqu'à plusieurs autres effectuant d'autres tâches plus spécialisées ;
- Des *outils* fournis au modèle et utilisables par celui-ci – par exemple la capacité de rechercher sur le web, créer ou modifier des documents, exécuter des programmes, etc.
- Une *architecture de support* qui connecte les entrées et sorties des réseaux de neurones. Une architecture très simple pourrait simplement permettre à deux « instances » d'un modèle d'IA de converser entre elles, ou à l'une de vérifier le travail de l'autre.[^5]
- Les techniques de *chaîne de raisonnement* et d'incitation connexes font quelque chose de similaire, amenant un modèle à par exemple générer de nombreuses approches à un problème, puis traiter ces approches pour une réponse agrégée.
- Le *ré-entraînement* des modèles pour mieux utiliser les outils, l'architecture de support, et la chaîne de raisonnement.

Parce que ces extensions peuvent être très puissantes (et incluent des systèmes d'IA eux-mêmes), ces systèmes composites peuvent être assez sophistiqués et améliorer considérablement les capacités de l'IA.[^6] Et récemment, des techniques d'architecture de support et surtout d'incitation par chaîne de raisonnement (et de réintégration des résultats dans le ré-entraînement des modèles pour mieux les utiliser) ont été développées et employées dans [o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/), et [DeepSeek R1](https://api-docs.deepseek.com/news/news250120) pour effectuer de nombreux passages d'inférence en réponse à une requête donnée.[^7] Ceci permet effectivement au modèle de « réfléchir » à sa réponse et améliore considérablement la capacité de ces modèles à effectuer des raisonnements de haut niveau dans les tâches scientifiques, mathématiques et de programmation.[^8]

Pour une architecture d'IA donnée, les augmentations de calcul d'entraînement [peuvent être traduites de manière fiable](https://arxiv.org/abs/2405.10938) en améliorations dans un ensemble de métriques clairement définies. Pour des capacités générales moins précisément définies (comme celles discutées ci-dessous), la traduction est moins claire et prédictive, mais il est presque certain que des modèles plus larges avec plus de calcul d'entraînement auront des capacités nouvelles et meilleures, même s'il est difficile de prédire lesquelles.

De même, les systèmes composites et surtout les avancées dans la « chaîne de raisonnement » (et l'entraînement de modèles qui fonctionnent bien avec elle) ont débloqué la montée en puissance dans le calcul d'*inférence* : pour un modèle central entraîné donné, au moins certaines capacités du système d'IA augmentent lorsque plus de calcul est appliqué, ce qui leur permet de « réfléchir plus intensément et plus longtemps » sur des problèmes complexes. Ceci se fait au prix d'un coût de vitesse de calcul élevé, nécessitant des centaines ou des milliers de FLOP/s supplémentaires pour égaler les performances humaines.[^9]

Bien qu'elle ne soit qu'une partie de ce qui conduit aux progrès rapides de l'IA,[^10] le rôle du calcul et la possibilité de systèmes composites s'avéreront cruciaux tant pour empêcher une IAG incontrôlable que pour développer des alternatives plus sûres.


[^1]: 10 <sup>25</sup> signifie 1 suivi de 25 zéros, ou dix mille milliards de milliards. Un FLOP est simplement une addition ou multiplication arithmétique de nombres avec une certaine précision. Notez que les performances du matériel d'IA peuvent varier d'un facteur de dix selon la précision de l'arithmétique et l'architecture de l'ordinateur. Compter les opérations de portes logiques (ET, OU, ET NON) serait fondamental mais celles-ci ne sont pas communément disponibles ou étalonnées ; aux fins présentes il est utile de standardiser sur les opérations 16-bit (FP16), bien que des facteurs de conversion appropriés devraient être établis.

[^2]: Une collection d'estimations et de données concrètes est disponible chez [Epoch AI](https://epochai.org/data/large-scale-ai-models) et indique environ 2×10 <sup>25</sup> FLOP 16-bit pour GPT-4 ; ceci correspond grossièrement aux [chiffres qui ont fuité](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) pour GPT-4. Les estimations pour d'autres modèles de mi-2024 sont toutes dans un facteur de quelques-uns de GPT-4.

[^3]: L'inférence est simplement le processus de génération d'une sortie à partir d'un réseau de neurones. L'entraînement peut être considéré comme une succession de nombreuses inférences et d'ajustements de poids du modèle.

[^4]: Pour la production de texte, le GPT-4 original nécessitait 560 TFLOP par jeton généré. Environ 7 jetons/s sont nécessaires pour suivre la pensée humaine, ce qui donne ≈3×10 <sup>15</sup> FLOP/s. Mais les efficacités ont fait chuter ce chiffre ; [cette brochure NVIDIA](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/) par exemple indique aussi peu que 3×10 <sup>14</sup> FLOP/s pour un modèle Llama 405B aux performances comparables.

[^5]: Comme exemple légèrement plus complexe, un système d'IA pourrait d'abord générer plusieurs solutions possibles à un problème mathématique, puis utiliser une autre instance pour vérifier chaque solution, et finalement utiliser une troisième pour synthétiser les résultats en une explication claire. Ceci permet une résolution de problèmes plus approfondie et fiable qu'un passage unique.

[^6]: Voir par exemple les détails sur [« Operator » d'OpenAI](https://openai.com/index/introducing-operator/), [les capacités d'outils de Claude](https://docs.anthropic.com/en/docs/build-with-claude/computer-use), et [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). [Deep Research](https://openai.com/index/introducing-deep-research/) d'OpenAI a probablement une architecture assez sophistiquée mais les détails ne sont pas disponibles.

[^7]: Deepseek R1 s'appuie sur l'entraînement et l'incitation itératifs du modèle de sorte que le modèle final entraîné crée un raisonnement de chaîne de pensée étendu. Les détails architecturaux ne sont pas disponibles pour o1 ou o3, cependant Deepseek a révélé qu'il n'y a pas de « sauce secrète » particulière requise pour débloquer la montée en puissance des capacités avec l'inférence. Mais malgré avoir reçu beaucoup de presse comme bouleversant le « statu quo » en IA, cela n'impacte pas les affirmations centrales de cet essai.

[^8]: Ces modèles surpassent significativement les modèles standard sur les références de raisonnement. Par exemple, dans le GPQA Diamond Benchmark—un test rigoureux de questions scientifiques de niveau doctorat—GPT-4o a [obtenu](https://openai.com/index/learning-to-reason-with-llms/) 56%, tandis que o1 et o3 ont atteint 78% et 88%, respectivement, dépassant largement le score moyen de 70% des experts humains.

[^9]: L'O3 d'OpenAI a probablement dépensé ∼10 <sup>21</sup> -10 <sup>22</sup> FLOP [pour compléter chacune des questions du défi ARC-AGI](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai), que des humains compétents peuvent faire en (disons) 10-100 secondes, donnant un chiffre plus proche de ∼10 <sup>20</sup> FLOP/s.

[^10]: Bien que le calcul soit une mesure clé de la capacité des systèmes d'IA, il interagit avec la qualité des données et les améliorations algorithmiques. De meilleures données ou algorithmes peuvent réduire les exigences computationnelles, tandis que plus de calcul peut parfois compenser des données ou algorithmes plus faibles.