# Chapitre 7 - Que se passe-t-il si nous développons l'IAG sur notre trajectoire actuelle ?

La société n'est pas prête pour des systèmes de niveau IAG. Si nous les construisons très bientôt, les choses pourraient mal tourner.

Le développement d'une intelligence artificielle générale complète – ce que nous appellerons ici l'IA qui se trouve « à l'extérieur des Portes » – constituerait un changement fondamental dans la nature du monde : par sa nature même, cela signifie ajouter à la Terre une nouvelle espèce d'intelligence dotée de capacités supérieures à celles des humains.

Ce qui se passera ensuite dépend de nombreux facteurs, notamment la nature de la technologie, les choix de ceux qui la développent, et le contexte mondial dans lequel elle est développée.

Actuellement, l'IAG complète est développée par une poignée d'entreprises privées massives en compétition les unes avec les autres, avec peu de réglementation significative ou de supervision externe,[^1] dans une société aux institutions centrales de plus en plus affaiblies et même dysfonctionnelles,[^2] à une époque de tensions géopolitiques élevées et de faible coordination internationale. Bien que certains soient motivés par l'altruisme, beaucoup de ceux qui s'y adonnent sont poussés par l'argent, le pouvoir, ou les deux.

La prédiction est très difficile, mais il existe certaines dynamiques suffisamment bien comprises, et des analogies assez pertinentes avec des technologies antérieures pour offrir un guide. Et malheureusement, malgré les promesses de l'IA, elles donnent de bonnes raisons d'être profondément pessimiste quant à la façon dont notre trajectoire actuelle va se dérouler.

Pour le dire sans détour, sur notre parcours actuel, le développement de l'IAG aura quelques effets positifs (et rendra certaines personnes très, très riches). Mais la nature de la technologie, les dynamiques fondamentales, et le contexte dans lequel elle est développée, indiquent fortement que : l'IA puissante sapera dramatiquement notre société et notre civilisation ; nous en perdrons le contrôle ; nous pourrions bien finir dans une guerre mondiale à cause d'elle ; nous perdrons (ou céderons) le contrôle *à* celle-ci ; elle mènera à une superintelligence artificielle, que nous ne contrôlerons absolument pas et qui signifiera la fin d'un monde dirigé par les humains.

Ce sont des affirmations fortes, et j'aimerais qu'elles ne soient que spéculation vaine ou « catastrophisme » injustifié. Mais c'est là que pointent la science, la théorie des jeux, la théorie de l'évolution, et l'histoire. Cette section développe ces affirmations, et leurs fondements, en détail.

## Nous saperons notre société et notre civilisation

Malgré ce que vous pourriez entendre dans les conseils d'administration de la Silicon Valley, la plupart des ruptures – surtout de la variété très rapide – ne sont pas bénéfiques. Il y a largement plus de façons d'empirer les systèmes complexes que de les améliorer. Notre monde fonctionne aussi bien qu'il le fait parce que nous avons patiemment construit des processus, des technologies, et des institutions qui l'ont rendu progressivement meilleur.[^3] Prendre une masse sur une usine améliore rarement les opérations.

Voici un catalogue (incomplet) des façons dont les systèmes d'IAG perturberaient notre civilisation.

- Ils perturberaient dramatiquement le travail, menant *au minimum* à une inégalité de revenus dramatiquement plus élevée et potentiellement au sous-emploi ou chômage à grande échelle, sur une échelle temporelle bien trop courte pour que la société s'ajuste.[^4]
- Ils mèneraient probablement à la concentration d'un pouvoir économique, social et politique énorme – potentiellement plus grand que celui des États-nations – dans un petit nombre d'intérêts privés massifs non responsables devant le public.
- Ils pourraient soudainement rendre triviales des activités auparavant difficiles ou coûteuses, déstabilisant les systèmes sociaux qui dépendent du fait que certaines activités restent coûteuses ou nécessitent un effort humain significatif.[^5]
- Ils pourraient inonder les systèmes de collecte, de traitement et de communication d'informations de la société avec des médias complètement réalistes mais faux, spammeurs, hyper-ciblés, ou manipulateurs si massivement qu'il devient impossible de distinguer ce qui est physiquement réel ou non, humain ou non, factuel ou non, et digne de confiance ou non.[^6]
- Ils pourraient créer une dépendance intellectuelle dangereuse et quasi totale, où la compréhension humaine des systèmes et technologies clés s'atrophie alors que nous comptons de plus en plus sur des systèmes d'IA que nous ne pouvons pas pleinement comprendre.
- Ils pourraient effectivement mettre fin à la culture humaine, une fois que presque tous les objets culturels (texte, musique, art visuel, cinéma, etc.) consommés par la plupart des gens sont créés, médiatisés, ou organisés par des esprits non-humains.
- Ils pourraient permettre des systèmes de surveillance et de manipulation de masse efficaces utilisables par les gouvernements ou les intérêts privés pour contrôler une population et poursuivre des objectifs en conflit avec l'intérêt public.
- En sapant le discours humain, le débat, et les systèmes électoraux, ils pourraient réduire la crédibilité des institutions démocratiques au point où elles sont effectivement (ou explicitement) remplacées par d'autres, mettant fin à la démocratie dans les États où elle existe actuellement.
- Ils pourraient devenir, ou créer, des virus et vers logiciels intelligents auto-réplicateurs avancés qui pourraient proliférer et évoluer, perturbant massivement les systèmes d'information mondiaux.
- Ils peuvent dramatiquement augmenter la capacité des terroristes, mauvais acteurs, et États voyous à causer des dommages via des armes biologiques, chimiques, cyber, autonomes, ou autres, sans que l'IA ne fournisse une capacité d'équilibre pour prévenir de tels dommages. De même, ils mineraient la sécurité nationale et les équilibres géopolitiques en rendant l'expertise de premier plan nucléaire, biologique, d'ingénierie, et autre disponible aux régimes qui ne l'auraient pas autrement.
- Ils pourraient causer un hyper-capitalisme emballé rapide à grande échelle, avec des entreprises effectivement dirigées par l'IA en compétition dans des espaces financiers, de vente, et de services largement électroniques. Les marchés financiers pilotés par l'IA pourraient opérer à des vitesses et complexités bien au-delà de la compréhension ou du contrôle humains. Tous les modes de défaillance et externalités négatives des économies capitalistes actuelles pourraient être exacerbés et accélérés bien au-delà du contrôle, de la gouvernance, ou de la capacité réglementaire humains.
- Ils pourraient alimenter une course aux armements entre nations dans l'armement alimenté par l'IA, les systèmes de commandement et contrôle, les cyberarmes, etc., créant une accumulation très rapide de capacités extrêmement destructrices.

Ces risques ne sont pas spéculatifs. Beaucoup d'entre eux se réalisent en ce moment même, via les systèmes d'IA existants ! Mais considérez, *vraiment* considérez, à quoi chacun ressemblerait avec une IA dramatiquement plus puissante.

Considérez le déplacement de main-d'œuvre quand la plupart des travailleurs ne peuvent simplement pas fournir de valeur économique significative au-delà de ce que l'IA peut faire, dans leur domaine d'expertise ou d'expérience – ou même s'ils se reconvertissent ! Considérez la surveillance de masse si tout le monde est individuellement surveillé et contrôlé par quelque chose de plus rapide et plus intelligent qu'eux. À quoi ressemble la démocratie quand nous ne pouvons pas faire confiance de façon fiable à toute information numérique que nous voyons, entendons, ou lisons, et quand les voix publiques les plus convaincantes ne sont même pas humaines, et n'ont aucun intérêt dans le résultat ? Que devient la guerre quand les généraux doivent constamment s'en remettre à l'IA (ou simplement la mettre aux commandes), de peur d'accorder un avantage décisif à l'ennemi ? N'importe lequel des risques ci-dessus représente une catastrophe pour la civilisation humaine[^7] s'il est pleinement réalisé.

Vous pouvez faire vos propres prédictions. Posez-vous ces trois questions pour chaque risque :

1. Est-ce qu'une IA super-capable, hautement autonome, et très générale le permettrait d'une façon ou à une échelle qui ne serait pas possible autrement ?
2. Y a-t-il des parties qui bénéficieraient de choses qui causent sa réalisation ?
3. Y a-t-il des systèmes et institutions en place qui l'empêcheraient efficacement de se produire ?

Là où vos réponses sont « oui, oui, non » vous pouvez voir que nous avons un gros problème.

Quel est notre plan pour les gérer ? En l'état, il y en a deux sur la table concernant l'IA en général.

Le premier est de construire des garde-fous dans les systèmes pour les empêcher de faire des choses qu'ils ne devraient pas faire. C'est ce qui se fait maintenant : les systèmes d'IA commerciaux vont, par exemple, refuser d'aider à construire une bombe ou écrire un discours de haine.

Ce plan est lamentablement inadéquat pour les systèmes à l'extérieur de la Porte.[^8] Il peut aider à diminuer le risque que l'IA fournisse une assistance manifestement dangereuse aux mauvais acteurs. Mais il ne fera rien pour empêcher la perturbation du travail, la concentration du pouvoir, l'hyper-capitalisme emballé, ou le remplacement de la culture humaine : ce ne sont que des résultats de l'utilisation des systèmes de façons permises qui profitent à leurs fournisseurs ! Et les gouvernements obtiendront sûrement l'accès aux systèmes pour un usage militaire ou de surveillance.

Le second plan est encore pire : simplement libérer ouvertement des systèmes d'IA très puissants pour que quiconque les utilise comme il le souhaite,[^9] et espérer le mieux.

Implicite dans les deux plans est que quelqu'un d'autre, par exemple les gouvernements, aidera à résoudre les problèmes par le droit souple ou dur, les standards, réglementations, normes, et autres mécanismes que nous utilisons généralement pour gérer les technologies.[^10] Mais mis à part que les entreprises d'IA se battent déjà bec et ongles contre toute réglementation substantielle ou limitations imposées de l'extérieur, pour un certain nombre de ces risques il est assez difficile de voir quelle réglementation aiderait même vraiment. La réglementation pourrait imposer des standards de sécurité sur l'IA. Mais empêcherait-elle les entreprises de remplacer les travailleurs en gros par l'IA ? Interdirait-elle aux gens de laisser l'IA diriger leurs entreprises pour eux ? Empêcherait-elle les gouvernements d'utiliser l'IA puissante dans la surveillance et l'armement ? Ces enjeux sont fondamentaux. L'humanité pourrait potentiellement trouver des façons de s'y adapter, mais seulement avec *beaucoup* plus de temps. En l'état, étant donné la vitesse à laquelle l'IA atteint ou dépasse les capacités des gens qui essaient de la gérer, ces problèmes semblent de plus en plus insolubles.

## Nous perdrons le contrôle des systèmes d'IAG (au moins certains)

La plupart des technologies sont très contrôlables, par construction. Si votre voiture ou votre grille-pain commence à faire quelque chose que vous ne voulez pas qu'il fasse, c'est juste un dysfonctionnement, pas une partie de sa nature de grille-pain. L'IA est différente : elle est *cultivée* plutôt que conçue, son fonctionnement central est opaque, et elle est intrinsèquement imprévisible.

Cette perte de contrôle n'est pas théorique – nous en voyons déjà les premières versions. Considérez d'abord un exemple prosaïque, et sans doute bénin. Si vous demandez à ChatGPT de vous aider à mélanger un poison, ou écrire une diatribe raciste, il refusera. C'est sans doute bien. Mais c'est aussi ChatGPT *ne faisant pas ce que vous lui avez explicitement demandé de faire*. D'autres logiciels ne font pas ça. Ce même modèle ne concevra pas de poisons à la demande d'un employé d'OpenAI non plus.[^11] Cela rend très facile d'imaginer à quoi ressemblerait une future IA plus puissante hors de contrôle. Dans beaucoup de cas, elles ne feront simplement pas ce que nous demandons ! Soit un système d'IAG super-humain donné sera absolument obéissant et loyal à un système de commandement humain, soit il ne le sera pas. Si non, *il fera des choses qu'il peut croire bonnes pour nous, mais qui sont contraires à nos commandes explicites.* Ce n'est pas quelque chose qui est sous contrôle. Mais, pourriez-vous dire, c'est intentionnel – ces refus sont par conception, partie de ce qu'on appelle « aligner » les systèmes sur les valeurs humaines. Et c'est vrai. Cependant le « programme » d'alignement lui-même a deux problèmes majeurs.[^12]

Premièrement, à un niveau profond nous n'avons aucune idée de comment le faire. Comment garantir qu'un système d'IA « se soucie » de ce que nous voulons ? Nous pouvons entraîner les systèmes d'IA à dire et ne pas dire des choses en fournissant des retours ; et ils peuvent apprendre et raisonner sur ce que les humains veulent et à quoi ils tiennent tout comme ils raisonnent sur d'autres choses. Mais nous n'avons aucune méthode – même théoriquement – pour les amener à valoriser profondément et de façon fiable ce qui importe aux gens. Il y a des psychopathes humains fonctionnels qui savent ce qui est considéré comme bien et mal, et comment ils sont supposés se comporter. Ils ne s'en *soucient* simplement pas. Mais ils peuvent *agir* comme s'ils le faisaient, si cela sert leur objectif. Tout comme nous ne savons pas comment changer un psychopathe (ou quiconque d'autre) en quelqu'un de véritablement, complètement loyal ou aligné avec quelqu'un ou quelque chose d'autre, nous n'avons *aucune idée*[^13] de comment résoudre le problème d'alignement dans des systèmes assez avancés pour se modéliser comme agents dans le monde et potentiellement [manipuler leur propre entraînement](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) et [tromper les gens.](https://arxiv.org/abs/2311.08379) S'il s'avère impossible ou irréalisable *soit* de rendre l'IAG complètement obéissante ou de lui faire se soucier profondément des humains, alors dès qu'elle en sera capable (et croira pouvoir s'en tirer), elle commencera à faire des choses que nous ne voulons pas.[^14]

Deuxièmement, il y a des raisons théoriques profondes de croire que *par nature* les systèmes d'IA avancés auront des objectifs et donc des comportements qui sont contraires aux intérêts humains. Pourquoi ? Eh bien elle pourrait, bien sûr, recevoir *ces* objectifs. Un système créé par l'armée serait probablement délibérément mauvais pour au moins certaines parties. Plus généralement, cependant, un système d'IA pourrait recevoir un objectif relativement neutre (« gagner beaucoup d'argent ») ou même ostensiblement positif (« réduire la pollution ») qui mène presque inévitablement à des objectifs « instrumentaux » qui sont plutôt moins bénins.

Nous voyons cela tout le temps dans les systèmes humains. Tout comme les entreprises poursuivant le profit développent des objectifs instrumentaux comme acquérir du pouvoir politique (pour désarmer les réglementations), devenir secrètes (pour désarmer la compétition ou le contrôle externe), ou saper la compréhension scientifique (si cette compréhension montre que leurs actions sont nuisibles), les systèmes d'IA puissants développeront des capacités similaires – mais avec une vitesse et une efficacité bien supérieures. Tout agent hautement compétent voudra faire des choses comme acquérir du pouvoir et des ressources, augmenter ses propres capacités, s'empêcher d'être tué, éteint, ou désarmé, contrôler les récits sociaux et cadres autour de ses actions, persuader les autres de ses vues, et ainsi de suite.[^15]

Et pourtant ce n'est pas seulement une prédiction théorique presque inévitable, c'est déjà observablement en train de se produire dans les systèmes d'IA d'aujourd'hui, et augmente avec leur capacité. Quand ils sont évalués, même ces systèmes d'IA relativement « passifs » vont, dans des circonstances appropriées, délibérément [tromper les évaluateurs sur leurs objectifs et capacités, viser à désactiver les mécanismes de supervision,](https://arxiv.org/abs/2412.04984) et éviter d'être éteints ou réentraînés en [simulant l'alignement](https://arxiv.org/abs/2412.14093) ou se copiant vers d'autres emplacements. Bien que totalement peu surprenants pour les chercheurs en sécurité de l'IA, ces comportements sont très sobres à observer. Et ils présagent très mal pour des systèmes d'IA bien plus puissants et autonomes qui arrivent.

En effet en général, notre incapacité à assurer que l'IA « se soucie » de ce dont nous nous soucions, ou se comporte de façon contrôlable ou prévisible, ou évite de développer des pulsions vers l'auto-préservation, l'acquisition de pouvoir, etc., ne promet que de devenir plus prononcée alors que l'IA devient plus puissante. Créer un nouvel avion implique une plus grande compréhension de l'avionique, l'hydrodynamique, et les systèmes de contrôle. Créer un ordinateur plus puissant implique une plus grande compréhension et maîtrise du fonctionnement et conception d'ordinateur, de puce, et de logiciel. *Pas* ainsi avec un système d'IA.[^16]

Pour résumer : il est concevable que l'IAG puisse être rendue complètement obéissante ; mais nous ne savons pas comment le faire. Si non, elle sera plus souveraine, comme les gens, faisant diverses choses pour diverses raisons. Nous ne savons pas non plus comment instiller de façon fiable un « alignement » profond dans l'IA qui rendrait ces choses tendant à être bonnes pour l'humanité, et en l'absence d'un niveau profond d'alignement, la nature de l'agentivité et de l'intelligence elle-même indique que – tout comme les gens et les entreprises – elles seront poussées à faire beaucoup de choses profondément antisociales.

Où cela nous place-t-il ? Un monde plein d'IA souveraine puissante incontrôlée *pourrait* finir par être un bon monde pour les humains.[^17] Mais alors qu'elles deviennent toujours plus puissantes, comme nous le verrons ci-dessous, ce ne serait pas *notre* monde.

C'est pour l'IAG incontrôlable. Mais même si l'IAG pouvait, d'une façon ou d'une autre, être rendue parfaitement contrôlée et loyale, nous aurions encore d'énormes problèmes. Nous en avons déjà vu un : l'IA puissante peut être utilisée et mal utilisée pour perturber profondément le fonctionnement de notre société. Voyons-en un autre : dans la mesure où l'IAG serait contrôlable et révolutionnairement puissante (ou même *crue* l'être) elle menacerait tellement les structures de pouvoir dans le monde qu'elle présenterait un risque profond.

## Nous augmentons radicalement la probabilité d'une guerre à grande échelle

Imaginez une situation dans un futur proche, où il deviendrait clair qu'un effort corporatif, peut-être en collaboration avec un gouvernement national, était au seuil d'une IA s'améliorant rapidement elle-même. Cela se passe dans le contexte actuel d'une course entre entreprises, et d'une compétition géopolitique dans laquelle des recommandations sont faites au gouvernement américain de poursuivre explicitement un « projet Manhattan de l'IAG » et les États-Unis contrôlent l'exportation de puces d'IA haute performance vers les pays non alliés.

La théorie des jeux ici est brutale : une fois qu'une telle course commence (comme elle l'a fait, entre entreprises et quelque peu entre pays), il n'y a que quatre résultats possibles :

1. La course est arrêtée (par accord, ou force externe).
2. Une partie « gagne » en développant une IAG forte puis en arrêtant les autres (utilisant l'IA ou autrement).
3. La course est arrêtée par destruction mutuelle de la capacité des coureurs à courir.
4. Plusieurs participants continuent à courir, et développent la superintelligence, à peu près aussi rapidement les uns que les autres.

Examinons chaque possibilité. Une fois commencé, arrêter pacifiquement une course entre entreprises nécessiterait une intervention du gouvernement national (pour les entreprises) ou une coordination internationale sans précédent (pour les pays). Mais quand toute fermeture ou prudence significative est proposée, il y aurait des cris immédiats : « mais si nous sommes arrêtés, *ils* vont se précipiter en avant », où « ils » est maintenant la Chine (pour les États-Unis), ou les États-Unis (pour la Chine), ou la Chine *et* les États-Unis (pour l'Europe ou l'Inde). Sous cet état d'esprit,[^18] aucun participant ne peut s'arrêter unilatéralement : tant qu'un s'engage à courir, les autres sentent qu'ils ne peuvent pas se permettre de s'arrêter.

La seconde possibilité a un côté « gagnant ». Mais qu'est-ce que cela signifie ? Juste obtenir (d'une façon ou d'une autre obéissante) l'IAG en premier ne suffit pas. Le gagnant doit *aussi* empêcher les autres de continuer à courir – sinon ils l'obtiendront aussi. C'est possible en principe : quiconque développe l'IAG en premier *pourrait* gagner un pouvoir imparable sur tous les autres acteurs. Mais qu'est-ce qu'obtenir un tel « avantage stratégique décisif » nécessiterait vraiment ? Peut-être seraient-ce des capacités militaires révolutionnaires ?[^19] Ou des pouvoirs de cyberattaque ?[^20] Peut-être l'IAG serait-elle juste si étonnamment persuasive qu'elle convaincrait les autres parties de juste s'arrêter ?[^21] Si riche qu'elle achèterait les autres entreprises ou même pays ?[^22]

Comment *exactement* un côté construit-il une IA assez puissante pour désarmer les autres de construire une IA comparablement puissante ? Mais c'est la question facile.

Car maintenant considérez à quoi cette situation ressemble pour les autres puissances. Que pense le gouvernement chinois quand les États-Unis semblent obtenir une telle capacité ? Ou vice-versa ? Que pense le gouvernement américain (ou chinois, ou russe, ou indien) quand OpenAI ou DeepMind ou Anthropic semble proche d'une percée ? Que se passe-t-il si les États-Unis voient un nouvel effort indien ou émirien avec un succès révolutionnaire ? Ils verraient à la fois une menace existentielle et – crucialement – que la seule façon dont cette « course » se termine est par leur propre désarmement. Ces agents très puissants – incluant les gouvernements de nations pleinement équipées qui ont sûrement les moyens de le faire – seraient hautement motivés à soit obtenir ou détruire une telle capacité, que ce soit par force ou subterfuge.[^23]

Cela pourrait commencer petit, comme sabotage d'entraînements ou attaques sur la fabrication de puces, mais ces attaques ne peuvent vraiment s'arrêter qu'une fois que toutes les parties soit perdent la capacité de courir sur l'IA, ou perdent la capacité de faire les attaques. Parce que les participants voient les enjeux comme existentiels, l'un ou l'autre cas est susceptible de représenter une guerre catastrophique.

Cela nous amène à la quatrième possibilité : courir vers la superintelligence, et de la façon la plus rapide et la moins contrôlée possible. Alors que l'IA augmente en puissance, ses développeurs des deux côtés la trouveront progressivement plus difficile à contrôler, surtout parce que courir pour les capacités est antithétique au genre de travail soigneux que la contrôlabilité nécessiterait. Donc ce scénario nous place directement dans le cas où le contrôle est perdu (ou donné, comme nous le verrons ensuite) aux systèmes d'IA eux-mêmes. C'est-à-dire, *l'IA gagne la course.* Mais d'un autre côté, dans la mesure où le contrôle *est* maintenu, nous continuons à avoir plusieurs parties mutuellement hostiles chacune en charge de capacités extrêmement puissantes. Cela ressemble encore à une guerre.

Mettons cela d'une autre façon.[^24] Le monde actuel n'a simplement aucune institution qui pourrait être confiée pour héberger le développement d'une IA de cette capacité sans inviter une attaque immédiate.[^25] Toutes les parties raisonneront correctement que soit elle ne sera *pas* sous contrôle – et donc est une menace pour toutes les parties, ou elle *sera* sous contrôle, et donc est une menace pour tout adversaire qui la développe moins rapidement. Ce sont des pays armés nucléairement, ou sont des entreprises hébergées en leur sein.

En l'absence de toute façon plausible pour les humains de « gagner » cette course, nous sommes laissés avec une conclusion brutale : la seule façon dont cette course se termine est soit dans un conflit catastrophique ou où l'IA, et non tout groupe humain, est le gagnant.

## Nous donnons le contrôle à l'IA (ou elle le prend)

La compétition « grandes puissances » géopolitique n'est qu'une des nombreuses compétitions : les individus sont en compétition économiquement et socialement ; les entreprises sont en compétition sur les marchés ; les partis politiques sont en compétition pour le pouvoir ; les mouvements sont en compétition pour l'influence. Dans chaque arène, alors que l'IA approche et dépasse la capacité humaine, la pression compétitive forcera les participants à déléguer ou céder de plus en plus de contrôle aux systèmes d'IA – non parce que ces participants veulent le faire, mais parce qu'ils [ne peuvent pas se permettre de ne pas le faire.](https://arxiv.org/abs/2303.16200)

Comme avec d'autres risques de l'IAG, nous voyons cela déjà avec des systèmes plus faibles. Les étudiants ressentent la pression d'utiliser l'IA dans leurs devoirs, parce que clairement beaucoup d'autres étudiants le font. Les entreprises se [précipitent pour adopter des solutions d'IA pour des raisons compétitives.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) Les artistes et programmeurs se sentent forcés d'utiliser l'IA ou sinon leurs tarifs seront sous-coupés par d'autres qui le font.

Cela ressemble à une délégation sous pression, mais pas à une perte de contrôle. Mais augmentons les enjeux et avançons l'horloge. Considérez un PDG dont les compétiteurs utilisent des « aides » d'IAG pour prendre des décisions plus rapides et meilleures, ou un commandant militaire face à un adversaire avec commande et contrôle améliorés par l'IA. Un système d'IA suffisamment avancé pourrait opérer de façon autonome à plusieurs fois la vitesse, sophistication, complexité, et capacité de traitement de données humaines, poursuivant des objectifs complexes de façons compliquées. Notre PDG ou commandant, en charge d'un tel système, peut le voir accomplir ce qu'il veut ; mais comprendraient-ils même une petite partie de *comment* c'était accompli ? Non, ils devraient juste l'accepter. Qui plus est, beaucoup de ce que le système peut faire n'est pas juste prendre des ordres mais conseiller son patron putatif sur quoi faire. Ce conseil sera bon –– encore et encore.

À quel point, alors, le rôle de l'humain sera-t-il réduit à cliquer « oui, allez-y » ?

Il fait bon d'avoir des systèmes d'IA capables qui peuvent améliorer notre productivité, s'occuper de corvées ennuyeuses, et même agir comme partenaire de réflexion pour faire les choses. Il fera bon d'avoir un assistant IA qui peut s'occuper d'actions pour nous, comme un bon assistant personnel humain. Il semblera naturel, même bénéfique, alors que l'IA devient très intelligente, compétente, et fiable, de déléguer de plus en plus de décisions à elle. Mais cette délégation « bénéfique » a un point final clair si nous continuons sur la route : un jour nous trouverons que nous ne sommes vraiment plus en charge de grand-chose, et que les systèmes d'IA dirigeant vraiment le spectacle ne peuvent pas plus être éteints que les compagnies pétrolières, les médias sociaux, l'internet, ou le capitalisme.

Et c'est la version bien plus positive, dans laquelle l'IA est simplement si utile et efficace que nous la laissons prendre la plupart de nos décisions clés pour nous. La réalité serait probablement bien plus un mélange entre cela et des versions où des systèmes d'IAG incontrôlés *prennent* diverses formes de pouvoir pour eux-mêmes parce que, rappelez-vous, le pouvoir est utile pour presque tout objectif qu'on a, et l'IAG serait, par conception, au moins aussi efficace à poursuivre ses objectifs que les humains.

Que nous accordions le contrôle ou qu'il nous soit arraché, sa perte semble extrêmement probable. Comme Alan Turing l'avait originellement dit, « ...il semble probable qu'une fois que la méthode de pensée machine aurait commencé, il ne faudrait pas longtemps pour dépasser nos faibles pouvoirs. Il n'y aurait pas de question de machines mourant, et elles pourraient converser entre elles pour aiguiser leur esprit. À un certain stade donc nous devrions nous attendre à ce que les machines prennent le contrôle... »

Veuillez noter, bien que ce soit assez évident, que la perte de contrôle par l'humanité à l'IA entraîne aussi la perte de contrôle des États-Unis par le gouvernement des États-Unis ; elle signifie la perte de contrôle de la Chine par le parti communiste chinois, et la perte de contrôle de l'Inde, France, Brésil, Russie, et tout autre pays par leur propre gouvernement. Ainsi les entreprises d'IA sont, même si ce n'est pas leur intention, actuellement en train de participer au renversement potentiel des gouvernements mondiaux, incluant le leur. Cela pourrait arriver en quelques années.

## L'IAG mènera à la superintelligence

Il y a un argument à faire que l'IA générale compétitive ou même expert-compétitive, même si autonome, pourrait être gérable. Elle peut être incroyablement perturbatrice de toutes les façons discutées ci-dessus, mais il y a beaucoup de gens très intelligents et agentiels dans le monde maintenant, et ils sont plus ou moins gérables.[^26]

Mais nous n'arriverons pas à rester à peu près au niveau humain. La progression au-delà sera probablement menée par les mêmes forces que nous avons déjà vues : pression compétitive entre développeurs d'IA cherchant profit et pouvoir, pression compétitive entre utilisateurs d'IA qui ne peuvent pas se permettre de prendre du retard, et – plus important – la propre capacité de l'IAG à s'améliorer elle-même.

Dans un processus que nous avons déjà vu commencer avec des systèmes moins puissants, l'IAG serait elle-même capable de concevoir et créer des versions améliorées d'elle-même. Cela inclut matériel, logiciel, réseaux de neurones, outils, architectures de support, etc. Elle sera, par définition, meilleure que nous pour le faire, donc nous ne savons pas exactement comment elle va s'auto-améliorer en intelligence. Mais nous n'aurons pas à le faire. Dans la mesure où nous avons encore de l'influence sur ce que l'IAG fait, nous n'aurions besoin que de lui demander, ou la laisser faire.

Il n'y a pas de barrière de niveau humain à la cognition qui pourrait nous protéger de cet emballement.[^27]

La progression de l'IAG à la superintelligence n'est pas une loi de nature ; il serait encore possible de freiner l'emballement, surtout si l'IAG est relativement centralisée et dans la mesure où elle est contrôlée par des parties qui ne ressentent pas de pression à courir les unes contre les autres. Mais si l'IAG était largement proliférée et hautement autonome, il semble presque impossible d'empêcher qu'elle décide qu'elle devrait être plus, et puis encore plus, puissante.

## Que se passe-t-il si nous construisons (ou l'IAG construit) la superintelligence

Pour le dire sans détour, nous n'avons aucune idée de ce qui arriverait si nous construisions la superintelligence.[^28] Elle prendrait des actions que nous ne pouvons pas suivre ou percevoir pour des raisons que nous ne pouvons pas saisir vers des objectifs que nous ne pouvons pas concevoir. Ce que nous savons c'est que ce ne sera pas à nous d'en décider.[^29]

L'impossibilité de contrôler la superintelligence peut être comprise par des analogies de plus en plus brutales. D'abord, imaginez que vous êtes PDG d'une grande entreprise. Il n'y a pas moyen que vous puissiez suivre tout ce qui se passe, mais avec la bonne configuration de personnel, vous pouvez encore comprendre de façon significative la vue d'ensemble, et prendre des décisions. Mais supposez juste une chose : tout le monde d'autre dans l'entreprise opère à cent fois votre vitesse. Pouvez-vous encore suivre ?

Avec l'IA superintelligente, les gens « commanderaient » quelque chose non seulement plus rapide, mais opérant à des niveaux de sophistication et complexité qu'ils ne peuvent pas comprendre, traitant vastement plus de données qu'ils ne peuvent même concevoir. Cette incommensurabilité peut être mise sur un niveau formel : [la loi de variété requise d'Ashby](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (et voir le [« théorème du bon régulateur »](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf) relié) énonce, grossièrement, que tout système de contrôle doit avoir autant de boutons et cadrans que le système contrôlé a de degrés de liberté.

Une personne contrôlant un système d'IA superintelligent serait comme une fougère contrôlant General Motors : même si « faire ce que la fougère veut » était écrit dans les statuts corporatifs, les systèmes sont si différents en vitesse et étendue d'action que « contrôle » ne s'applique simplement pas. (Et combien de temps avant que ce statut gênant ne soit réécrit ?)[^30]

Comme il y a zéro exemple de plantes contrôlant des entreprises du fortune 500, il y aurait exactement zéro exemple de gens contrôlant des superintelligences. Cela approche un fait mathématique.[^31] Si la superintelligence était construite – peu importe comment nous y sommes arrivés – la question ne serait pas de savoir si les humains pourraient la contrôler, mais de savoir si nous continuerions à exister, et si oui, si nous aurions une existence bonne et significative comme individus ou comme espèce. Sur ces questions existentielles pour l'humanité nous aurions peu de prise. L'ère humaine serait finie.

## Conclusion : nous ne devons pas construire l'IAG

Il y a un scénario dans lequel construire l'IAG peut bien aller pour l'humanité : elle est construite soigneusement, sous contrôle et pour le bénéfice de l'humanité, gouvernée par accord mutuel de nombreuses parties prenantes,[^32] et empêchée d'évoluer vers une superintelligence incontrôlable.

*Ce scénario ne nous est pas ouvert dans les circonstances présentes.* Comme discuté dans cette section, avec une probabilité très élevée, le développement de l'IAG mènerait à une certaine combinaison de :

- Perturbation ou destruction sociétale et civilisationnelle massive ;
- Conflit ou guerre entre grandes puissances ;
- Perte de contrôle par l'humanité *des* ou *aux* systèmes d'IA puissants ;
- Emballement vers une superintelligence incontrôlable, et l'insignifiance ou cessation de l'espèce humaine.

Comme une première représentation fictionnelle de l'IAG l'a dit : la seule façon de gagner est de ne pas jouer.

[^1]: La [Loi IA de l'UE](https://artificialintelligenceact.eu/) est un morceau significatif de législation mais n'empêcherait pas directement un système d'IA dangereux d'être développé ou déployé, ou même ouvertement libéré, surtout aux États-Unis. Un autre morceau significatif de politique, l'ordre exécutif américain sur l'IA, a été rescindé.

[^2]: Ce [sondage Gallup](https://news.gallup.com/poll/1597/confidence-institutions.aspx) montre un déclin lugubre de confiance dans les institutions publiques depuis 2000 aux États-Unis. Les chiffres européens sont variés et moins extrêmes, mais aussi sur une tendance descendante. La méfiance ne signifie pas strictement que les institutions sont vraiment *dysfonctionnelles*, mais c'est une indication aussi bien qu'une cause.

[^3]: Et les perturbations majeures que nous approuvons maintenant – comme l'expansion des droits à de nouveaux groupes – étaient spécifiquement menées par des gens dans une direction vers l'amélioration des choses.

[^4]: Permettez-moi d'être direct. Si votre travail peut être fait de derrière un ordinateur, avec relativement peu d'interaction en personne avec des gens en dehors de votre organisation, et n'implique pas de responsabilité légale envers des parties externes, il serait par définition possible (et probablement économe) de vous remplacer complètement par un système numérique. La robotique pour remplacer beaucoup de travail physique viendra plus tard – mais pas beaucoup plus tard une fois que l'IAG commence à concevoir des robots.

[^5]: Par exemple, qu'arrive-t-il à notre système judiciaire si les procès sont presque gratuits à déposer ? Qu'arrive-t-il quand contourner les systèmes de sécurité par ingénierie sociale devient bon marché, facile, et sans risque ?

[^6]: [Cet article](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/) prétend que 10% de tout contenu internet est déjà généré par IA, et est le premier résultat Google (pour moi) à la requête de recherche « estimations de quelle fraction de nouveau contenu internet est générée par IA ». Est-ce vrai ? Je n'en ai aucune idée ! Il ne cite aucune référence et il n'a pas été écrit par une personne. Quelle fraction de nouvelles images indexées par Google, ou Tweets, ou commentaires sur Reddit, ou vidéos Youtube sont générées par des humains ? Personne ne le sait – je ne pense pas que ce soit un nombre connaissable. Et cela moins de *deux ans* dans l'avènement de l'IA générative.

[^7]: Vaut aussi la peine d'ajouter qu'il y a un risque « moral » que nous pourrions créer des êtres numériques qui peuvent souffrir. Comme nous n'avons actuellement pas de théorie fiable de la conscience qui nous permettrait de distinguer les systèmes physiques qui peuvent et ne peuvent pas souffrir, nous ne pouvons pas l'exclure théoriquement. De plus, les rapports des systèmes d'IA de leur sentience sont probablement peu fiables par rapport à leur expérience réelle (ou non-expérience) de sentience.

[^8]: Les solutions techniques dans ce domaine d' « alignement » de l'IA sont improbables d'être à la hauteur de la tâche non plus. Dans les systèmes présents elles fonctionnent à un certain niveau, mais sont superficielles et peuvent généralement être contournées sans effort significatif ; et comme discuté ci-dessous nous n'avons aucune idée réelle de comment faire cela pour des systèmes beaucoup plus avancés.

[^9]: De tels systèmes d'IA peuvent venir avec quelques garde-fous intégrés. Mais pour tout modèle avec quelque chose comme l'architecture actuelle, si l'accès complet à ses poids est disponible, les mesures de sécurité peuvent être retirées via entraînement additionnel ou autres techniques. Donc il est virtuellement garanti que pour chaque système avec garde-fous il y aura aussi un système largement disponible sans eux. En effet le modèle Llama 3.1 405B de Meta a été ouvertement libéré avec garde-fous. Mais *même avant cela* un modèle « de base », sans garde-fous, a fuité.

[^10]: Le marché pourrait-il gérer ces risques sans implication gouvernementale ? En bref, non. Il y a certainement des risques que les entreprises sont fortement incitées à mitiger. Mais beaucoup d'autres les entreprises peuvent et externalisent à tout le monde d'autre, et beaucoup des ci-dessus sont dans cette classe : il n'y a pas d'incitations de marché naturelles pour prévenir la surveillance de masse, la décadence de vérité, la concentration du pouvoir, la perturbation du travail, le discours politique dommageable, etc. En effet nous avons vu tout cela de la tech d'aujourd'hui, surtout les médias sociaux, qui sont allés essentiellement non régulés. L'IA ne ferait qu'amplifier énormément beaucoup des mêmes dynamiques.

[^11]: OpenAI a probablement des modèles plus obéissants pour usage interne. Il est improbable qu'OpenAI ait construit une sorte de « porte dérobée » pour que ChatGPT puisse être mieux contrôlé par OpenAI lui-même, parce que ce serait une pratique de sécurité terrible, et être hautement exploitable étant donné l'opacité et l'imprévisibilité de l'IA.

[^12]: Aussi d'importance cruciale : l'alignement ou toute autre caractéristique de sécurité n'importe que s'ils sont vraiment utilisés dans un système d'IA. Les systèmes qui sont ouvertement libérés (c'est-à-dire où les poids et architecture du modèle sont publiquement disponibles) peuvent être transformés relativement facilement en systèmes *sans* ces mesures de sécurité. Libérer ouvertement des systèmes d'IAG plus intelligents que humains serait étonnamment imprudent, et il est difficile d'imaginer comment le contrôle humain ou même la pertinence serait maintenue dans un tel scénario. Il y aurait toute motivation, par exemple, à lâcher de puissants agents d'IA auto-reproducteurs et auto-entretenus avec l'objectif de faire de l'argent et l'envoyer à un portefeuille de cryptomonnaie. Ou gagner une élection. Ou renverser un gouvernement. L'IA « bonne » pourrait-elle aider à contenir cela ? Peut-être – mais seulement en lui déléguant une autorité énorme, menant à la perte de contrôle comme décrit ci-dessous.

[^13]: Pour des expositions de longueur de livre du problème voir par exemple *Superintelligence*, *The Alignment Problem*, et *Human-Compatible*. Pour un énorme tas de travail à divers niveaux techniques par ceux qui ont peiné pendant des années à réfléchir au problème, vous pouvez visiter le [forum d'alignement de l'IA](https://www.alignmentforum.org/). Voici une [prise récente](https://alignment.anthropic.com/2025/recommended-directions/) de l'équipe d'alignement d'Anthropic sur ce qu'ils considèrent non résolu.

[^14]: C'est le scénario [« IA voyou »](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/). En principe le risque pourrait être relativement mineur si le système peut encore être contrôlé en l'éteignant ; mais le scénario pourrait aussi inclure tromperie de l'IA, auto-exfiltration et reproduction, agrégation de pouvoir, et autres étapes qui rendraient difficile ou impossible de le faire.

[^15]: Il y a une littérature très riche sur ce sujet, remontant aux écrits formatifs par [Steve Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), Nick Bostrom, et Eliezer Yudkowsky. Pour une exposition de longueur de livre voir [Human Compatible](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) par Stuart Russell ; [voici](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/) un primer court et à jour.

[^16]: Reconnaissant cela, plutôt que de ralentir pour obtenir une meilleure compréhension, les entreprises d'IAG ont inventé un plan différent : elles vont faire faire cela à l'IA ! Plus spécifiquement, elles vont avoir l'IA *N* les aider à comprendre comment aligner l'IA *N+1*, tout le chemin vers la superintelligence. Bien que tirer parti de l'IA pour nous aider à aligner l'IA sonne prometteur, il y a un argument fort qu'elle assume simplement sa conclusion comme prémisse, et est en général une approche incroyablement risquée. Voir [ici](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us) pour quelque discussion. Ce « plan » n'en est pas un, et a subi rien comme l'examen minutieux approprié à la stratégie centrale de comment rendre l'IA super-humaine bonne pour l'humanité.

[^17]: Après tout, les humains, imparfaits et volontaires comme nous sommes, ont développé des systèmes éthiques par lesquels nous traitons au moins quelques autres espèces sur Terre bien. (Juste ne pensez pas à ces fermes-usines.)

[^18]: Il y a, heureusement, une échappatoire ici : si les participants en viennent à comprendre qu'ils sont engagés dans une course suicide plutôt qu'une gagnable. C'est ce qui est arrivé près de la fin de la guerre froide, quand les États-Unis et URSS en sont venus à réaliser qu'à cause de l'hiver nucléaire, même une attaque nucléaire *sans réponse* serait désastreuse pour l'attaquant. Avec la réalisation que « la guerre nucléaire ne peut pas être gagnée et ne doit jamais être menée » sont venus des accords significatifs sur la réduction d'armes – essentiellement une fin à la course aux armements.

[^19]: Guerre, explicitement ou implicitement.

[^20]: Escalade, puis guerre.

[^21]: Pensée magique.

[^22]: J'ai aussi un pont à un quadrillion de dollars à vous vendre.

[^23]: De tels agents préféreraient présumablement « obtenir », avec destruction comme solution de rechange ; mais sécuriser les modèles contre à la fois destruction *et* vol par des nations puissantes est difficile pour dire le moins, surtout pour des entités privées.

[^24]: Pour une autre perspective sur les risques de sécurité nationale de l'IAG, voir [ce rapport RAND.](https://www.rand.org/pubs/perspectives/PEA3691-4.html)

[^25]: Peut-être pourrions-nous construire une telle institution ! Il y a eu des propositions pour un « CERN pour l'IA » et autres initiatives similaires, où le développement d'IAG est sous contrôle global multilatéral. Mais au moment aucune telle institution n'existe ou n'est à l'horizon.

[^26]: Et bien que l'alignement soit très difficile, faire que les gens se comportent est encore plus dur !

[^27]: Imaginez un système qui peut parler 50 langues, avoir de l'expertise dans tous les sujets académiques, lire un livre complet en secondes et avoir tout le matériel immédiatement en tête, et produire des sorties à dix fois la vitesse humaine. Vraiment, vous n'avez pas à l'imaginer : juste chargez un système d'IA actuel. Ceux-ci sont super-humains de beaucoup de façons, et il n'y a rien qui les arrête d'être encore plus super-humains dans celles-ci et beaucoup d'autres.

[^28]: C'est pourquoi cela a été appelé une « singularité » technologique, empruntant à la physique l'idée qu'on ne peut pas faire de prédictions au-delà d'une singularité. Les promoteurs de se pencher *dans* une telle singularité peuvent aussi vouloir réfléchir qu'en physique ces mêmes sortes de singularités déchirent et écrasent ceux qui y entrent.

[^29]: Le problème était exhaustivement esquissé dans [*Superintelligence*](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834) de Bostrom, et rien depuis n'a significativement changé le message central. Pour un volume plus récent collectant les résultats formels et mathématiques sur l'incontrôlabilité voir [AI: Unexplainable, Unpredictable, Uncontrollable](https://www.amazon.com/Unexplainable-Unpredictable-Uncontrollable-Artificial-Intelligence/dp/103257626X) de Yampolskiy

[^30]: Cela clarifie aussi pourquoi la stratégie actuelle des entreprises d'IA (laisser itérativement l'IA « aligner » la prochaine IA la plus puissante) ne peut pas fonctionner. Supposez qu'une fougère, via l'agrément de ses frondes, enrôle un élève de première année pour s'en occuper. L'élève de première année écrit quelques instructions détaillées pour un élève de seconde à suivre, et une note le convainquant de le faire. L'élève de seconde fait de même pour un élève de troisième, et ainsi de suite tout le chemin vers un diplômé d'université, un manager, un exécutif, et finalement le PDG de GM. GM va-t-elle alors « faire ce que la fougère veut » ? À chaque étape cela pourrait sembler fonctionner. Mais mettant tout ensemble, cela fonctionnera presque exactement dans la mesure où le PDG, Conseil, et actionnaires de GM se trouvent se soucier des enfants et fougères, et avoir peu à rien à voir avec toutes ces notes et ensembles d'instructions.

[^31]: Le caractère n'est pas si différent de résultats formels comme le théorème d'incomplétude de Gödel ou l'argument d'arrêt de Turing en ce que la notion de contrôle contredit fondamentalement la prémisse : comment pouvez-vous contrôler de façon significative quelque chose que vous ne pouvez pas comprendre ou prédire ; pourtant si vous pouviez comprendre et prédire la superintelligence vous seriez superintelligent. La raison pour laquelle je dis « approche » est que les résultats formels ne sont pas aussi approfondis ou vérifiés que dans le cas des mathématiques pures, et parce que j'aimerais garder espoir qu'une intelligence générale très soigneusement construite, utilisant des méthodes totalement différentes de celles actuellement employées, pourrait avoir quelques propriétés de sécurité mathématiquement prouvables, selon le genre de programme d'IA « garantie sûre » discuté ci-dessous.

[^32]: Au moment, la plupart des parties prenantes – c'est-à-dire, presque toute l'humanité – est mise à l'écart dans cette discussion. C'est profondément mauvais, et si pas invités, les nombreux, nombreux autres groupes seront affectés par le développement d'IAG devraient exiger d'être admis.