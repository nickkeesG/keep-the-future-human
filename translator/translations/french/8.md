# Chapitre 8 - Comment ne pas construire d'IAG

L'IAG n'est pas inévitable – nous nous trouvons aujourd'hui à un carrefour. Ce chapitre présente une proposition sur la manière dont nous pourrions empêcher sa construction.

Si la voie sur laquelle nous nous trouvons actuellement mène probablement à la fin de notre civilisation, comment changer de direction ?

Supposons que le désir d'arrêter le développement de l'IAG et de la superintelligence soit répandu et puissant,[^1] parce qu'il devient communément admis que l'IAG absorberait le pouvoir plutôt qu'elle ne l'accorderait, et qu'elle représente un danger profond pour la société et l'humanité. Comment fermerions-nous les Portes ?

À l'heure actuelle, nous ne connaissons qu'une seule façon de *créer* une IA puissante et générale, à savoir par des calculs vraiment massifs de réseaux de neurones profonds. Comme il s'agit d'opérations incroyablement difficiles et coûteuses à réaliser, il y a un sens dans lequel *ne pas* les faire est facile.[^2] Mais nous avons déjà vu les forces qui poussent vers l'IAG, et la dynamique de théorie des jeux qui rend très difficile pour toute partie d'arrêter unilatéralement. Il faudrait donc une combinaison d'intervention de l'extérieur (c'est-à-dire des gouvernements) pour arrêter les entreprises, et d'accords entre gouvernements pour s'arrêter eux-mêmes.[^3] À quoi cela pourrait-il ressembler ?

Il est utile de distinguer d'abord entre les développements d'IA qui doivent être *empêchés* ou *interdits*, et ceux qui doivent être *gérés*. Les premiers seraient principalement l'emballement vers la superintelligence.[^4] Pour le développement interdit, les définitions devraient être aussi précises que possible, et tant la vérification que l'application devraient être pratiques. Ce qui doit être *géré* serait les systèmes d'IA généraux et puissants – que nous avons déjà, et qui présenteront de nombreuses zones grises, nuances et complexités. Pour ceux-ci, des institutions solides et efficaces sont cruciales.

Nous pouvons aussi utilement délimiter les questions qui doivent être abordées au niveau international (y compris entre rivaux ou adversaires géopolitiques)[^5] de celles que des juridictions individuelles, des pays, ou des groupes de pays peuvent gérer. Le développement interdit relève largement de la catégorie « internationale », car une interdiction locale du développement d'une technologie peut généralement être contournée en changeant de lieu.[^6]

Enfin, nous pouvons considérer les outils dans la boîte à outils. Il y en a beaucoup, notamment les outils techniques, le droit souple (normes, standards, etc.), le droit dur (réglementations et exigences), la responsabilité, les incitations de marché, et ainsi de suite. Portons une attention particulière à un qui est spécifique à l'IA.

## Sécurité matérielle et gouvernance computationnelle

Un outil central dans la gouvernance de l'IA de haute puissance sera le matériel qu'elle nécessite. Le logiciel prolifère facilement, a un coût marginal de production quasi nul, traverse les frontières trivialement, et peut être instantanément modifié ; rien de tout cela n'est vrai pour le matériel. Pourtant, comme nous l'avons discuté, d'énormes quantités de cette « puissance de calcul » sont nécessaires à la fois pendant l'entraînement des systèmes d'IA et pendant l'inférence pour atteindre les systèmes les plus capables. Le calcul peut être facilement quantifié, comptabilisé et audité, avec relativement peu d'ambiguïté une fois que de bonnes règles pour le faire sont développées. Plus crucial encore, de grandes quantités de calcul sont, comme l'uranium enrichi, une ressource très rare, coûteuse et difficile à produire. Bien que les puces informatiques soient omniprésentes, le matériel requis pour l'IA est cher et énormément difficile à fabriquer.[^7]

Ce qui rend les puces spécialisées en IA *beaucoup plus* gérables comme ressource rare que l'uranium, c'est qu'elles peuvent inclure des mécanismes de sécurité basés sur le matériel. La plupart des téléphones portables modernes, et certains ordinateurs portables, ont des fonctionnalités matérielles spécialisées sur puce qui leur permettent de s'assurer qu'ils n'installent que des logiciels de système d'exploitation et des mises à jour approuvés, qu'ils conservent et protègent des données biométriques sensibles sur l'appareil, et qu'ils peuvent être rendus inutiles à quiconque d'autre que leur propriétaire s'ils sont perdus ou volés. Au cours des dernières années, de telles mesures de sécurité matérielle sont devenues bien établies et largement adoptées, et se sont généralement révélées assez sécurisées.

La nouveauté clé de ces fonctionnalités est qu'elles lient matériel et logiciel ensemble en utilisant la cryptographie.[^8] C'est-à-dire que le simple fait d'avoir un morceau particulier de matériel informatique ne signifie pas qu'un utilisateur peut faire tout ce qu'il veut avec en appliquant différents logiciels. Et cette liaison fournit aussi une sécurité puissante parce que de nombreuses attaques nécessiteraient une violation de la sécurité *matérielle* plutôt que seulement *logicielle*.

Plusieurs rapports récents (par exemple de [GovAI et collaborateurs](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips), et [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) ont souligné que des fonctionnalités matérielles similaires intégrées dans le matériel informatique de pointe pertinent pour l'IA pourraient jouer un rôle extrêmement utile dans la sécurité et la gouvernance de l'IA. Elles permettent un certain nombre de fonctions disponibles à un « gouverneur »[^9] que l'on ne pourrait pas deviner être disponibles ou même possibles. Voici quelques exemples clés :

- *Géolocalisation* : Les systèmes peuvent être configurés de sorte que les puces aient un emplacement connu, et puissent agir différemment (ou être complètement éteintes) selon l'emplacement.[^10]
- *Connexions sur liste blanche* : chaque puce peut être configurée avec une liste blanche appliquée par le matériel de puces particulières avec lesquelles elle peut se connecter en réseau, et être incapable de se connecter avec toute puce ne figurant pas sur cette liste.[^11] Cela peut limiter la taille des grappes communicantes de puces.[^12]
- *Inférence ou entraînement mesurés (et arrêt automatique)* : Un gouverneur peut licencier seulement une certaine quantité d'entraînement ou d'inférence (en temps, ou FLOP, ou possiblement tokens) à effectuer par un utilisateur, après quoi une nouvelle permission est requise. Si les incréments sont petits, alors un re-licenciement relativement continu d'un modèle est requis. Le modèle peut alors être « éteint » simplement en retenant ce signal de licence.[^13]
- *Limitation de vitesse* : Un modèle est empêché de fonctionner à une vitesse d'inférence supérieure à une certaine limite qui est déterminée par un gouverneur ou autrement. Cela pourrait être implémenté via un ensemble limité de connexions sur liste blanche, ou par des moyens plus sophistiqués.
- *Entraînement attesté* : Une procédure d'entraînement peut produire une preuve cryptographiquement sécurisée qu'un ensemble particulier de codes, de données, et de quantité d'usage de calcul ont été employés dans la génération du modèle.

## Comment ne pas construire la superintelligence : limites mondiales sur le calcul d'entraînement et d'inférence

Avec ces considérations – en particulier concernant le calcul – en place, nous pouvons discuter de la façon de fermer les Portes à la superintelligence artificielle ; nous nous tournerons ensuite vers la prévention de l'IAG complète, et la gestion des modèles d'IA alors qu'ils approchent et dépassent les capacités humaines dans différents aspects.

Le premier ingrédient est, bien sûr, la compréhension que la superintelligence ne serait pas contrôlable, et que ses conséquences sont fondamentalement imprévisibles. Au moins la Chine et les États-Unis doivent décider indépendamment, pour cette raison ou d'autres, de ne pas construire la superintelligence.[^14] Puis un accord international entre eux et d'autres, avec un mécanisme de vérification et d'application solide, est nécessaire pour assurer toutes les parties que leurs rivaux ne font pas défection et ne décident pas de jouer aux dés.

Pour être vérifiables et applicables, les limites devraient être des limites dures, et aussi non ambiguës que possible. Cela semble être un problème virtuellement impossible : limiter les capacités de logiciels complexes aux propriétés imprévisibles, dans le monde entier. Heureusement, la situation est beaucoup mieux que cela, car la chose même qui a rendu l'IA avancée possible – une énorme quantité de calcul – est beaucoup, beaucoup plus facile à contrôler. Bien qu'elle puisse encore permettre certains systèmes puissants et dangereux, l'*emballement de la superintelligence* peut probablement être empêché par un plafond dur sur la quantité de calcul qui entre dans un réseau de neurones, ainsi qu'une limite de taux sur la quantité d'inférence qu'un système d'IA (de réseaux de neurones connectés et d'autres logiciels) peut effectuer. Une version spécifique de ceci est proposée ci-dessous.

Il peut sembler que placer des limites mondiales dures sur le calcul d'IA nécessiterait d'énormes niveaux de coordination internationale et une surveillance intrusive qui brise la vie privée. Heureusement, ce ne serait pas le cas. La [chaîne d'approvisionnement extrêmement serrée et à goulot d'étranglement](https://arxiv.org/abs/2402.08797) fait qu'une fois qu'une limite est fixée légalement (que ce soit par la loi ou un décret exécutif), la vérification de conformité à cette limite ne nécessiterait que l'implication et la coopération d'une poignée de grandes entreprises.[^15]

Un plan comme celui-ci a un certain nombre de caractéristiques hautement désirables. Il est minimalement invasif dans le sens où seules quelques grandes entreprises ont des exigences qui leur sont imposées, et seules des grappes de calcul assez significatives seraient gouvernées. Les puces pertinentes contiennent déjà les capacités matérielles nécessaires pour une première version.[^16] L'implémentation et l'application reposent toutes deux sur des restrictions légales standard. Mais celles-ci sont soutenues par les conditions d'utilisation du matériel et par des contrôles matériels, simplifiant vastement l'application et prévenant la triche par les entreprises, groupes privés, ou même pays. Il y a un précédent ample pour les entreprises de matériel plaçant des restrictions à distance sur l'usage de leur matériel, et verrouillant/déverrouillant des capacités particulières extérieurement,[^17] y compris même dans des CPU de haute puissance dans des centres de données.[^18] Même pour la fraction plutôt petite de matériel et d'organisations affectées, la supervision pourrait être limitée à la télémétrie, sans accès direct aux données ou modèles eux-mêmes ; et le logiciel pour cela pourrait être ouvert à l'inspection pour montrer qu'aucune donnée supplémentaire n'est enregistrée. Le schéma est international et coopératif, et assez flexible et extensible. Parce que la limite porte principalement sur le matériel plutôt que le logiciel, elle est relativement agnostique quant à la façon dont le développement et le déploiement de logiciels d'IA se déroulent, et est compatible avec une variété de paradigmes incluant l'IA plus « décentralisée » ou « publique » visant à combattre la concentration du pouvoir entraînée par l'IA.

Une fermeture de Porte basée sur le calcul a aussi des inconvénients. Premièrement, c'est loin d'être une solution complète au problème de la gouvernance de l'IA en général. Deuxièmement, alors que le matériel informatique devient plus rapide, le système « attraperait » de plus en plus de matériel dans des grappes de plus en plus petites (ou même des GPU individuels).[^19] Il est aussi possible qu'en raison d'améliorations algorithmiques une limite de calcul encore plus basse soit nécessaire avec le temps,[^20] ou que la quantité de calcul devienne largement non pertinente et que fermer la Porte nécessite au contraire un régime de gouvernance plus détaillé basé sur le risque ou les capacités pour l'IA. Troisièmement, quelles que soient les garanties et le petit nombre d'entités affectées, un tel système est destiné à créer une résistance concernant la vie privée et la surveillance, entre autres préoccupations.[^21]

Bien sûr, développer et implémenter un schéma de gouvernance limitant le calcul dans une courte période de temps sera assez difficile. Mais c'est absolument faisable.

## A-I-G : La triple-intersection comme base du risque et de la politique

Tournons-nous maintenant vers l'IAG. Les lignes dures et les définitions ici sont plus difficiles, car nous avons certainement de l'intelligence qui est artificielle et générale, et selon aucune définition existante tout le monde ne sera d'accord si ou quand elle existe. De plus, une limite de calcul ou d'inférence est un outil quelque peu grossier (le calcul étant un proxy pour la capacité, qui est alors un proxy pour le risque) qui – à moins qu'elle ne soit assez basse – est peu susceptible d'empêcher l'IAG qui est assez puissante pour causer une perturbation sociale ou civilisationnelle ou des risques aigus.

J'ai soutenu que les risques les plus aigus émergent de la triple-intersection de très haute capacité, haute autonomie, et grande généralité. Ce sont les systèmes qui – s'ils sont développés du tout – doivent être gérés avec un soin énorme. En créant des standards stricts (par la responsabilité et la réglementation) pour les systèmes combinant les trois propriétés, nous pouvons canaliser le développement d'IA vers des alternatives plus sûres.

Comme avec d'autres industries et produits qui pourraient potentiellement nuire aux consommateurs ou au public, les systèmes d'IA nécessitent une réglementation soigneuse par des agences gouvernementales efficaces et habilitées. Cette réglementation devrait reconnaître les risques inhérents de l'IAG, et empêcher que des systèmes d'IA de haute puissance inacceptablement risqués ne soient développés.[^22]

Cependant, la réglementation à grande échelle, en particulier avec de vraies dents qui sont sûres d'être opposées par l'industrie,[^23] prend du temps[^24] ainsi qu'une conviction politique qu'elle est nécessaire.[^25] Étant donné le rythme des progrès, cela peut prendre plus de temps que nous n'en avons à disposition.

Sur une échelle de temps beaucoup plus rapide et alors que les mesures réglementaires sont développées, nous pouvons donner aux entreprises les incitations nécessaires pour (a) s'abstenir d'activités très à haut risque et (b) développer des systèmes complets pour évaluer et atténuer le risque, en clarifiant et augmentant les niveaux de responsabilité pour les systèmes les plus dangereux. L'idée serait d'imposer les plus hauts niveaux de responsabilité – stricte et dans certains cas criminelle personnelle – pour les systèmes dans la triple-intersection de haute autonomie-généralité-intelligence, mais de fournir des « régimes d'exonération » à une responsabilité plus typique basée sur la faute pour les systèmes dans lesquels une de ces propriétés manque ou est garantie d'être gérable. C'est-à-dire, par exemple, qu'un système « faible » qui est général et autonome (comme un assistant personnel capable et fiable mais limité) serait soumis à des niveaux de responsabilité plus bas. De même un système étroit et autonome comme une voiture autonome serait encore soumis à la réglementation significative à laquelle elle l'est déjà, mais pas à une responsabilité renforcée. Similairement pour un système très capable et général qui est « passif » et largement incapable d'action indépendante. Les systèmes manquant de *deux* des trois propriétés sont encore plus gérables et les régimes d'exonération seraient encore plus faciles à revendiquer. Cette approche reflète comment nous gérons d'autres technologies potentiellement dangereuses :[^26] une responsabilité plus élevée pour des configurations plus dangereuses crée des incitations naturelles pour des alternatives plus sûres.

Le résultat par défaut de tels hauts niveaux de responsabilité, qui agissent pour *internaliser* le risque d'IAG aux entreprises plutôt que de le décharger sur le public, est probable (et espérons-le !) que les entreprises ne développent simplement pas l'IAG complète jusqu'à et à moins qu'elles puissent réellement la rendre digne de confiance, sûre et contrôlable étant donné que *leur propre direction* sont les parties à risque. (Au cas où cela ne serait pas suffisant, la législation clarifiant la responsabilité devrait aussi explicitement permettre un recours injonctif, c'est-à-dire qu'un juge ordonne un arrêt, pour les activités qui sont clairement dans la zone de danger et posent de manière discutable un risque public.) Alors que la réglementation se met en place, se conformer à la réglementation peut devenir le régime d'exonération, et les régimes d'exonération de faible autonomie, étroitesse, ou faiblesse des systèmes d'IA peuvent se convertir en régimes réglementaires relativement plus légers.

## Dispositions clés d'une fermeture de Porte

Avec la discussion ci-dessus à l'esprit, cette section fournit des propositions pour des dispositions clés qui mettraient en œuvre et maintiendraient l'interdiction sur l'IAG complète et la superintelligence, et la gestion de l'IA généraliste compétitive avec l'humain ou compétitive avec l'expert près du seuil d'IAG complète.[^27] Elle a quatre pièces clés : 1) comptabilité et supervision du calcul, 2) plafonds de calcul dans l'entraînement et l'opération de l'IA, 3) un cadre de responsabilité, et 4) standards de sécurité et de sûreté échelonnés définis qui incluent des exigences réglementaires dures. Ceux-ci sont succinctement décrits ensuite, avec des détails supplémentaires ou des exemples d'implémentation donnés dans trois tableaux accompagnants. Important, notez que ceux-ci sont loin d'être tout ce qui sera nécessaire pour gouverner les systèmes d'IA avancés ; bien qu'ils aient des bénéfices de sécurité et de sûreté supplémentaires, ils visent à fermer la Porte à l'emballement de l'intelligence, et à rediriger le développement d'IA dans une meilleure direction.

### 1\. Comptabilité du calcul et transparence

- Une organisation de standards (par exemple NIST aux États-Unis suivi par ISO/IEEE internationalement) devrait codifier un standard technique détaillé pour le calcul total utilisé dans l'entraînement et l'opération des modèles d'IA, en FLOP, et la vitesse en FLOP/s à laquelle ils opèrent. Des détails sur à quoi cela pourrait ressembler sont donnés en Annexe A.[^28]
- Une exigence – soit par nouvelle législation soit sous autorité existante[^29] – devrait être imposée par les juridictions dans lesquelles l'entraînement d'IA à grande échelle a lieu de calculer et rapporter à un corps réglementaire ou autre agence le total de FLOP utilisés dans l'entraînement et l'opération de tous les modèles au-dessus d'un seuil de 10<sup>25</sup> FLOP ou 10<sup>18</sup> FLOP/s.[^30]
- Ces exigences devraient être introduites par phases, nécessitant initialement des estimations de bonne foi bien documentées sur une base trimestrielle, avec des phases ultérieures nécessitant des standards progressivement plus élevés, jusqu'à des FLOP totaux et FLOP/s cryptographiquement attestés attachés à chaque *sortie* de modèle.
- Ces rapports devraient être complétés par des estimations bien documentées du coût énergétique et financier marginal utilisé dans la génération de chaque sortie d'IA.

Justification : Ces nombres bien calculés et rapportés transparemment fourniraient la base pour les plafonds d'entraînement et d'opération, ainsi qu'un régime d'exonération des mesures de responsabilité plus élevées (voir Annexes C et D).

### 2\. Plafonds de calcul d'entraînement et d'opération

- Les juridictions hébergeant des systèmes d'IA devraient imposer une limite dure sur le calcul total entrant dans toute sortie de modèle d'IA, commençant à 10<sup>27</sup> FLOP[^31] et ajustable selon l'approprié.
- Les juridictions hébergeant des systèmes d'IA devraient imposer une limite dure sur le taux de calcul des sorties de modèles d'IA, commençant à 10<sup>20</sup> FLOP/s et ajustable selon l'approprié.

Justification : Le calcul total, bien que très imparfait, est un proxy pour la capacité d'IA (et le risque) qui est concrètement mesurable et vérifiable, donc fournit un arrêt dur pour limiter les capacités. Une proposition d'implémentation concrète est donnée en Annexe B.

### 3\. Responsabilité renforcée pour les systèmes dangereux

- La création et l'opération[^32] d'un système d'IA avancé qui est hautement général, capable et autonome, devrait être clarifiée légalement via la législation pour être sujette à une responsabilité stricte, solidaire, plutôt qu'une responsabilité basée sur la faute d'une seule partie.[^33]
- Un processus légal devrait être disponible pour faire des cas de sécurité affirmatifs, qui accorderaient un régime d'exonération de la responsabilité stricte pour les systèmes qui sont petits (en termes de calcul), faibles, étroits, passifs, ou qui ont des garanties suffisantes de sécurité, sûreté et contrôlabilité.
- Une voie explicite et un ensemble de conditions pour un recours injonctif pour arrêter les activités d'entraînement et d'inférence d'IA qui constituent un danger public devraient être délimitées.

Justification : Les systèmes d'IA ne peuvent pas être tenus responsables, donc nous devons tenir les individus et organisations humains responsables du mal qu'ils causent (responsabilité).[^34] L'IAG incontrôlable est une menace pour la société et la civilisation et en l'absence d'un cas de sécurité devrait être considérée anormalement dangereuse. Mettre le fardeau de la responsabilité sur les développeurs pour montrer que les modèles puissants sont assez sûrs pour ne pas être considérés « anormalement dangereux » incite au développement sûr, ainsi qu'à la transparence et à la tenue de registres pour revendiquer ces régimes d'exonération. La réglementation peut alors empêcher le mal là où la dissuasion de la responsabilité est insuffisante. Finalement, les développeurs d'IA sont déjà responsables des dommages qu'ils causent, donc clarifier légalement la responsabilité pour les systèmes les plus risqués peut être fait immédiatement, sans que des standards hautement détaillés soient développés ; ceux-ci peuvent alors se développer avec le temps. Des détails sont donnés en Annexe C.

### 4\. Réglementation de sécurité pour l'IA

Un système réglementaire qui aborde les risques aigus à grande échelle de l'IA nécessitera au minimum :

- L'identification ou la création d'un ensemble approprié de corps réglementaires, probablement une nouvelle agence ;
- Un cadre complet d'évaluation des risques ;[^35]
- Un cadre pour les cas de sécurité affirmatifs, basé en partie sur le cadre d'évaluation des risques, à faire par les développeurs, et pour l'audit par des groupes et agences *indépendants* ;
- Un système de licences échelonné, avec des échelons suivant les niveaux de capacité.[^36] Les licences seraient accordées sur la base de cas de sécurité et d'audits, pour le développement et le déploiement de systèmes. Les exigences iraient de la notification à l'extrémité basse, à des garanties quantitatives de sécurité, sûreté et contrôlabilité avant le développement, à l'extrémité haute. Celles-ci empêcheraient la sortie de systèmes jusqu'à ce qu'ils soient démontrés sûrs, et interdiraient le développement de systèmes intrinsèquement dangereux. L'Annexe D fournit une proposition sur ce que de tels standards de sécurité et sûreté pourraient impliquer.
- Des accords pour amener de telles mesures au niveau international, incluant des corps internationaux pour harmoniser les normes et standards, et potentiellement des agences internationales pour réviser les cas de sécurité.

Justification : Ultimement, la responsabilité n'est pas le bon mécanisme pour empêcher le risque à grande échelle au public d'une nouvelle technologie. Une réglementation compréhensive, avec des corps réglementaires habilités, sera nécessaire pour l'IA tout comme pour chaque autre industrie majeure posant un risque au public.[^37]

La réglementation vers la prévention d'autres risques pervasifs mais moins aigus variera probablement dans sa forme d'une juridiction à l'autre. La chose cruciale est d'éviter de développer les systèmes d'IA qui sont si risqués que ces risques sont ingérables.

## Et ensuite ?

Au cours de la prochaine décennie, alors que l'IA devient plus pervasive et que la technologie de base avance, deux choses clés sont susceptibles de se produire. Premièrement, la réglementation des systèmes d'IA puissants existants deviendra plus difficile, mais encore plus nécessaire. Il est probable qu'au moins certaines mesures abordant les risques de sécurité à grande échelle nécessiteront un accord au niveau international, avec des juridictions individuelles appliquant des règles basées sur des accords internationaux.

Deuxièmement, les plafonds de calcul d'entraînement et d'opération deviendront plus difficiles à maintenir alors que le matériel devient moins cher et plus rentable ; ils peuvent aussi devenir moins pertinents (ou nécessiter d'être encore plus serrés) avec les avances en algorithmes et architectures.

Que contrôler l'IA deviendra plus difficile ne signifie pas que nous devrions abandonner ! Implémenter le plan décrit dans cet essai nous donnerait à la fois du temps précieux et un contrôle crucial sur le processus qui nous mettrait dans une position bien, bien meilleure pour éviter le risque existentiel de l'IA pour notre société, civilisation et espèce.

À plus long terme encore, il y aura des choix à faire quant à ce que nous permettons. Nous pouvons choisir de créer encore une forme d'IAG genuinement contrôlable, dans la mesure où cela s'avère possible. Ou nous pouvons décider que diriger le monde est mieux laissé aux machines, si nous pouvons nous convaincre qu'elles feront un meilleur travail, et nous traiteront bien. Mais ces devraient être des décisions prises avec une compréhension scientifique profonde de l'IA en main, et après une discussion globale inclusive significative, pas dans une course entre magnats de la technologie avec la plupart de l'humanité complètement non impliquée et inconsciente.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) Résumé de la gouvernance de l'A-I-G et de la superintelligence via la responsabilité et la réglementation. La responsabilité est la plus élevée, et la réglementation la plus forte, à la triple-intersection d'Autonomie, Généralité et Intelligence. Des régimes d'exonération de la responsabilité stricte et de la réglementation forte peuvent être obtenus via des cas de sécurité affirmatifs démontrant qu'un système est faible et/ou étroit et/ou passif. Des plafonds sur le Calcul Total d'Entraînement et le taux de Calcul d'Inférence, vérifiés et appliqués légalement et utilisant des mesures de sécurité matérielles et cryptographiques, soutiennent la sécurité en évitant l'IAG complète et interdisant efficacement la superintelligence.

[^1]: Très probablement, la diffusion de cette réalisation prendra soit un effort intense par des groupes d'éducation et de plaidoyer faisant ce cas, soit un désastre assez significant causé par l'IA. Nous pouvons espérer que ce sera le premier.

[^2]: Paradoxalement, nous sommes habitués à ce que la Nature limite notre technologie en la rendant très difficile à développer, en particulier scientifiquement. Mais ce n'est plus le cas pour l'IA : les problèmes scientifiques clés s'avèrent être plus faciles qu'anticipé. Nous ne pouvons pas compter sur la Nature pour nous sauver de nous-mêmes ici – nous devrons le faire.

[^3]: Où, exactement, nous arrêtons-nous dans le développement de nouveaux systèmes ? Ici, nous devrions adopter un principe de précaution. Une fois qu'un système est déployé, et en particulier une fois que ce niveau de capacité de système prolifère, il est extrêmement difficile de faire marche arrière. Et si un système est *développé* (en particulier à grand coût et effort), il y aura une pression énorme pour l'utiliser ou le déployer, et la tentation qu'il soit divulgué ou volé. Développer des systèmes et *puis* décider s'ils sont profondément dangereux est une route dangereuse.

[^4]: Il serait aussi sage d'interdire le développement d'IA qui est intrinsèquement dangereuse, comme les systèmes auto-réplicants et évolutifs, ceux conçus pour échapper à l'enfermement, ceux qui peuvent s'auto-améliorer de manière autonome, l'IA délibérément trompeuse et malicieuse, etc.

[^5]: Notez que cela ne signifie pas nécessairement *appliqué* au niveau international par une sorte de corps global : au lieu de cela les nations souveraines pourraient appliquer des règles convenues, comme dans de nombreux traités.

[^6]: Comme nous le verrons ci-dessous, la nature du calcul d'IA permettrait quelque chose d'un hybride ; mais la coopération internationale sera encore nécessaire.

[^7]: Par exemple, les machines requises pour graver les puces pertinentes pour l'IA sont fabriquées par une seule firme, ASML (malgré de nombreuses autres tentatives de le faire), la grande majorité des puces pertinentes sont manufacturées par une firme, TSMC (malgré d'autres tentant de concourir), et la conception et construction de matériel à partir de ces puces faite par seulement quelques-unes incluant NVIDIA, AMD, et Google.

[^8]: Plus important, chaque puce détient une clé privée cryptographique unique et inaccessible qu'elle peut utiliser pour « signer » des choses.

[^9]: Par défaut ce serait l'entreprise vendant les puces, mais d'autres modèles sont possibles et potentiellement utiles.

[^10]: Un gouverneur peut déterminer l'emplacement d'une puce en chronométrant l'échange de messages signés avec elle : la vitesse finie de la lumière nécessite que la puce soit dans un rayon donné *r* d'une « station » si elle peut retourner un message signé dans un temps inférieur à *r* / *c*, où *c* est la vitesse de la lumière. En utilisant plusieurs stations, et une certaine compréhension des caractéristiques du réseau, l'emplacement de la puce peut être déterminé. La beauté de cette méthode est que la plupart de sa sécurité est fournie par les lois de la physique. D'autres méthodes pourraient utiliser GPS, suivi inertiel, et des technologies similaires.

[^11]: Alternativement, des paires de puces pourraient être autorisées à communiquer l'une avec l'autre seulement via permission explicite d'un gouverneur.

[^12]: Ceci est crucial car au moins actuellement, une connexion à très haute largeur de bande entre puces est nécessaire pour entraîner de grands modèles d'IA sur elles.

[^13]: Ceci pourrait aussi être configuré pour nécessiter des messages signés de *N* des *M* différents gouverneurs, permettant à plusieurs parties de partager la gouvernance.

[^14]: Ceci est loin d'être sans précédent – par exemple les militaires n'ont pas développé d'armées de super-soldats clonés ou génétiquement modifiés, bien que cela soit probablement technologiquement possible. Mais ils ont *choisi* de ne pas faire cela, plutôt que d'être empêchés par d'autres. Le bilan n'est pas génial pour que des puissances mondiales majeures soient empêchées de développer une technologie qu'elles souhaitent fortement développer.

[^15]: Avec quelques exceptions notables (en particulier NVIDIA) le matériel spécialisé en IA est une partie relativement petite du modèle d'affaires global et de revenus de ces entreprises. De plus, l'écart entre le matériel utilisé dans l'IA avancée et le matériel « grand public » est significatif, donc la plupart des consommateurs de matériel informatique seraient largement non affectés.

[^16]: Pour une analyse plus détaillée, voir les rapports récents de [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) et [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). Ceux-ci se concentrent sur la faisabilité technique, en particulier dans le contexte des contrôles d'exportation américains cherchant à contraindre la capacité d'autres pays en calcul de haut niveau ; mais cela a un chevauchement évident avec la contrainte globale envisagée ici.

[^17]: Les appareils Apple, par exemple, sont verrouillés à distance et sécurisés quand rapportés perdus ou volés, et peuvent être réactivés à distance. Cela repose sur les mêmes fonctionnalités de sécurité matérielle discutées ici.

[^18]: Voir par exemple l'offre [capacity on demand](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) d'IBM, [Intel on demand](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html) d'Intel, et [private cloud compute](https://security.apple.com/blog/private-cloud-compute/) d'Apple.

[^19]: [Cette étude](https://epochai.org/trends#hardware-trends-section) montre qu'historiquement la même performance a été atteinte en utilisant environ 30% de moins de dollars par année. Si cette tendance continue, il peut y avoir un chevauchement significatif entre l'usage de puces d'IA et « grand public », et en général la quantité de matériel nécessaire pour les systèmes d'IA de haute puissance pourrait devenir inconfortablement petite.

[^20]: Selon la [même étude](https://epochai.org/trends#hardware-trends-section), une performance donnée sur la reconnaissance d'images a nécessité 2,5x moins de calcul chaque année. Si cela devait aussi tenir pour les systèmes d'IA les plus capables aussi bien, une limite de calcul ne serait pas utile très longtemps.

[^21]: En particulier, au niveau du pays cela ressemble beaucoup à une nationalisation du calcul, dans le sens où le gouvernement aurait beaucoup de contrôle sur la façon dont la puissance computationnelle est utilisée. Cependant, pour ceux inquiets de l'implication gouvernementale, cela semble bien plus sûr que et préférable au logiciel d'IA le plus puissant *lui-même* étant nationalisé via une fusion entre les entreprises d'IA majeures et les gouvernements nationaux, comme certains commencent à plaider pour.

[^22]: Une étape réglementaire majeure en Europe a été prise avec le passage en 2024 de la [Loi sur l'IA de l'UE](https://artificialintelligenceact.eu/). Elle classifie l'IA par risque : interdisant les systèmes inacceptables, réglementant ceux à haut risque, et imposant des règles de transparence, ou aucune mesure du tout, sur les systèmes à faible risque. Elle réduira significativement certains risques d'IA, et stimulera la transparence de l'IA même pour les firmes américaines, mais a deux défauts clés. Premièrement, portée limitée : bien qu'elle s'applique à toute entreprise fournissant de l'IA dans l'UE, l'application sur les firmes basées aux États-Unis est faible, et l'IA militaire est exemptée. Deuxièmement, bien qu'elle couvre l'IAGP, elle échoue à reconnaître l'IAG ou la superintelligence comme des risques inacceptables ou à empêcher leur développement—seulement leur déploiement dans l'UE. En résultat, elle fait peu pour freiner les risques de l'IAG ou de la superintelligence.

[^23]: Les entreprises représentent souvent qu'elles sont en faveur d'une réglementation raisonnable. Mais d'une façon ou d'une autre elles semblent presque toujours s'opposer à toute réglementation *particulière* ; témoin la lutte sur la SB1047 assez peu intrusive, à laquelle [la plupart des entreprises d'IA se sont opposées publiquement ou privément](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/).

[^24]: Il a fallu environ 3 ans et demi depuis le moment où la loi sur l'IA de l'UE a été proposée jusqu'à ce qu'elle entre en vigueur.

[^25]: Il est parfois exprimé qu'il est « trop tôt » pour commencer à réglementer l'IA. Étant donné la note précédente, cela ne semble guère probable. Une autre préoccupation exprimée est que la réglementation « nuirait à l'innovation ». Mais une bonne réglementation change seulement la direction, pas la quantité, d'innovation.

[^26]: Un précédent intéressant est dans le transport de matériaux dangereux, qui pourraient s'échapper et causer des dommages. Ici, la [réglementation](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) et la [jurisprudence](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) ont établi une responsabilité stricte pour des matériaux très dangereux comme les explosifs, l'essence, les poisons, les agents infectieux, et les déchets radioactifs. D'autres exemples incluent les [avertissements sur les médicaments](https://www.medicalnewstoday.com/articles/boxed-warnings), les [classes d'appareils médicaux](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification), etc.

[^27]: Une autre proposition complète avec des buts similaires mise en avant dans [« A Narrow Path »](https://www.narrowpath.co/) plaide pour une approche plus centralisée, basée sur l'interdiction qui canalise tout développement d'IA de pointe à travers une seule entité internationale, supervisée par de fortes institutions internationales, avec des interdictions catégoriques claires plutôt que des restrictions graduées. J'endosserais aussi ce plan ; cependant il prendra encore plus de volonté politique et de coordination que celui proposé ici.

[^28]: Quelques lignes directrices pour un tel standard ont été [publiées](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/) par le Frontier Model Forum. Relativement à la proposition ici, celles-ci pèchent du côté de moins de précision et moins de calcul inclus dans le décompte.

[^29]: Le décret exécutif américain sur l'IA de 2023 (maintenant rescindé) nécessitait un rapport similaire mais moins fin. Ceci devrait être renforcé par un ordre de remplacement.

[^30]: Très approximativement, pour les puces H100 maintenant communes cela correspond à des grappes d'environ 1000 faisant de l'inférence ; c'est environ 100 (environ 5M USD de valeur) des toutes nouvelles puces NVIDIA B200 de pointe faisant de l'inférence. Dans les deux cas le nombre d'entraînement correspond à cette grappe calculant pendant plusieurs mois.

[^31]: Cette quantité est plus grande que tout système d'IA actuellement entraîné ; un nombre plus grand ou plus petit pourrait être justifié alors que nous comprenons mieux comment la capacité d'IA s'échelonne avec le calcul.

[^32]: Ceci s'applique à ceux créant et fournissant/hébergeant les modèles, pas aux utilisateurs finaux.

[^33]: Grossièrement, la responsabilité « stricte » signifie que les développeurs sont tenus responsables des dommages faits par un produit *par défaut* et est un standard utilisé pour les produits « anormalement dangereux », et (quelque peu amusant mais approprié) les animaux sauvages. La responsabilité « solidaire » signifie que la responsabilité est assignée à toutes les parties responsables d'un produit, et ces parties doivent régler entre elles qui porte quelle responsabilité. Ceci est important pour les systèmes comme l'IA avec une chaîne de valeur longue et complexe.

[^34]: La responsabilité standard basée sur la faute d'une seule partie n'est pas suffisante : la faute sera à la fois difficile à tracer et assigner parce que les systèmes d'IA sont complexes, leur opération n'est pas comprise, et de nombreuses parties peuvent être impliquées dans la création d'un système ou d'une sortie dangereuse. De plus, les poursuites prendront des années à adjuger et résulteront probablement simplement en amendes qui sont sans conséquence pour ces entreprises, donc la responsabilité personnelle pour les cadres est importante aussi bien.

[^35]: Il ne devrait y avoir aucune exemption des critères de sécurité pour les modèles à poids ouverts. De plus, en évaluant le risque il devrait être assumé que les garde-fous qui peuvent être enlevés le seront des modèles largement disponibles, et que même les modèles fermés proliféreront à moins qu'il y ait une assurance très élevée qu'ils resteront sécurisés.

[^36]: Le schéma proposé ici a un examen réglementaire déclenché sur la capacité générale ; cependant il est sensé que certains cas d'usage spécialement risqués déclenchent plus d'examen – par exemple un système d'IA de virologie expert, même s'il est étroit et passif, devrait probablement aller dans un échelon plus élevé. L'ancien décret exécutif américain avait une partie de cette structure pour les capacités biologiques.

[^37]: Deux exemples clairs sont l'aviation et les médicaments, réglementés par la FAA et la FDA, et des agences similaires dans d'autres pays. Ces agences sont imparfaites, mais ont été absolument vitales pour le fonctionnement et le succès de ces industries.