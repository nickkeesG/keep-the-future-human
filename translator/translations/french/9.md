# Chapitre 9 - Façonner l'avenir — ce que nous devrions faire à la place

L'IA peut accomplir des merveilles dans le monde. Pour obtenir tous les bénéfices sans les risques, nous devons nous assurer que l'IA reste un outil humain.

Si nous choisissons avec succès de ne pas remplacer l'humanité par des machines – du moins pour un temps ! – que pouvons-nous faire à la place ? Devons-nous renoncer à l'immense promesse de l'IA en tant que technologie ? À un certain niveau, la réponse est un simple *non* : fermons les Portes à l'IAG incontrôlable et à la superintelligence, mais *construisons* bien d'autres formes d'IA, ainsi que les structures de gouvernance et les institutions nécessaires pour les gérer.

Mais il y a encore beaucoup à dire ; réaliser cela constituerait une préoccupation centrale de l'humanité. Cette section explore plusieurs thèmes clés :

- Comment nous pouvons caractériser l'IA « outil » et les formes qu'elle peut prendre.
- Que nous pouvons obtenir (presque) tout ce que l'humanité désire sans IAG, avec l'IA-outil.
- Que les systèmes d'IA-outil sont (probablement, en principe) gérables.
- Que s'éloigner de l'IAG ne signifie pas compromettre la sécurité nationale – bien au contraire.
- Que la concentration du pouvoir est une préoccupation réelle. Pouvons-nous l'atténuer sans compromettre la sûreté et la sécurité ?
- Que nous voudrons – et aurons besoin – de nouvelles structures de gouvernance et sociales, et l'IA peut effectivement nous aider.

## L'IA à l'intérieur des Portes : l'IA-outil

Le diagramme de triple intersection offre un bon moyen de délimiter ce que nous pouvons appeler « l'IA-outil » : une IA qui est un outil contrôlable pour l'usage humain, plutôt qu'un rival ou remplaçant incontrôlable. Les systèmes d'IA les moins problématiques sont ceux qui sont autonomes mais ni généraux ni super-capables (comme un robot d'enchères automatiques), ou généraux mais ni autonomes ni capables (comme un petit modèle de langage), ou capables mais étroits et très contrôlables (comme AlphaGo).[^1] Ceux ayant deux caractéristiques qui se recoupent ont une application plus large mais un risque plus élevé et nécessiteront des efforts majeurs de gestion. (Ce n'est pas parce qu'un système d'IA est davantage un outil qu'il est intrinsèquement sûr, seulement qu'il n'est pas intrinsèquement *dangereux* – considérez une tronçonneuse, par rapport à un tigre domestique.) La Porte doit rester fermée à l'IAG (complète) et à la superintelligence à la triple intersection, et une précaution énorme doit être prise avec les systèmes d'IA approchant ce seuil.

Mais cela laisse place à beaucoup d'IA puissante ! Nous pouvons tirer une utilité énorme d'« oracles » passifs intelligents et généraux ainsi que de systèmes étroits, de systèmes généraux au niveau humain mais pas surhumain, et ainsi de suite. De nombreuses entreprises technologiques et développeurs construisent activement ce genre d'outils et devraient continuer ; comme la plupart des gens, ils *supposent* implicitement que les Portes vers l'IAG et la superintelligence resteront fermées.[^2]

De plus, les systèmes d'IA peuvent être efficacement combinés en systèmes composites qui maintiennent la supervision humaine tout en améliorant les capacités. Plutôt que de nous appuyer sur des boîtes noires inscrutables, nous pouvons construire des systèmes où plusieurs composants – incluant à la fois l'IA et les logiciels traditionnels – travaillent ensemble de manière que les humains puissent surveiller et comprendre.[^3] Bien que certains composants puissent être des boîtes noires, aucun ne serait proche de l'IAG – seul le système composite dans son ensemble serait à la fois hautement général et hautement capable, et de manière strictement contrôlable.[^4]

### Contrôle humain significatif et garanti

Que signifie « strictement contrôlable » ? Une idée clé du cadre « Outil » est de permettre des systèmes – même s'ils sont assez généraux et puissants – qui sont garantis d'être sous contrôle humain significatif. Que signifie ceci ? Cela implique deux aspects. Premièrement, une considération de conception : les humains devraient être profondément et centralement impliqués dans ce que fait le système, *sans* déléguer les décisions importantes clés à l'IA. C'est le caractère de la plupart des systèmes d'IA actuels. Deuxièmement, dans la mesure où les systèmes d'IA sont autonomes, ils doivent avoir des garanties qui limitent leur portée d'action. Une garantie devrait être un *nombre* caractérisant la probabilité que quelque chose se produise, et une raison de croire en ce nombre. C'est ce que nous exigeons dans d'autres domaines critiques pour la sécurité, où des nombres comme les « temps moyens entre défaillances » et les nombres attendus d'accidents sont calculés, soutenus, et publiés dans des dossiers de sécurité.[^5] Le nombre idéal pour les défaillances est zéro, bien sûr. Et la bonne nouvelle est que nous pourrions nous en approcher assez, bien qu'en utilisant des architectures d'IA assez différentes, en utilisant des idées de propriétés *formellement vérifiées* de programmes (y compris l'IA). L'idée, explorée en détail par Omohundro, Tegmark, Bengio, Dalrymple, et d'autres (voir [ici](https://arxiv.org/abs/2309.01933) et [ici](https://arxiv.org/abs/2405.06624)) est de construire un programme avec certaines propriétés (par exemple : qu'un humain puisse l'arrêter) et de *prouver* formellement que ces propriétés se maintiennent. Cela peut être fait maintenant pour des programmes assez courts et des propriétés simples, mais la puissance (à venir) des logiciels de preuve assistés par IA pourrait le permettre pour des programmes beaucoup plus complexes (par exemple des wrappers) et même l'IA elle-même. C'est un programme très ambitieux, mais à mesure que la pression s'accroît sur les Portes, nous allons avoir besoin de matériaux puissants pour les renforcer. La preuve mathématique pourrait être l'un des rares assez solides.

### Que devient l'industrie de l'IA

Avec le progrès de l'IA redirigé, l'IA-outil resterait encore une industrie énorme. En termes de matériel, même avec des plafonds de calcul pour prévenir la superintelligence, l'entraînement et l'inférence dans des modèles plus petits nécessiteront encore d'énormes quantités de composants spécialisés. Du côté logiciel, désamorcer l'explosion dans la taille des modèles et de la puissance de calcul d'IA devrait simplement amener les entreprises à rediriger les ressources vers l'amélioration des systèmes plus petits, les rendant meilleurs, plus diversifiés, et plus spécialisés, plutôt que simplement plus gros.[^6] Il y aurait amplement de place – plus probablement – pour toutes ces startups lucratives de la Silicon Valley.[^7]

## L'IA-outil peut produire (presque) tout ce que l'humanité désire, sans IAG

L'intelligence, qu'elle soit biologique ou artificielle, peut être largement considérée comme la capacité de planifier et d'exécuter des activités amenant des futurs plus en ligne avec un ensemble d'objectifs. En tant que telle, l'intelligence est d'un bénéfice énorme quand elle est utilisée à la poursuite d'objectifs sagement choisis. L'intelligence artificielle attire d'énormes investissements de temps et d'efforts largement à cause de ses bénéfices promis. Nous devrions donc nous demander : dans quelle mesure récolterions-nous encore les bénéfices de l'IA si nous contenons sa fuite vers la superintelligence ? La réponse : nous pourrions perdre étonnamment peu.

Considérons d'abord que les systèmes d'IA actuels sont déjà très puissants, et nous n'avons vraiment fait qu'effleurer la surface de ce qui peut être fait avec eux.[^8] Ils sont raisonnablement capables de « mener la danse » en termes de « compréhension » d'une question ou tâche qui leur est présentée, et ce qu'il faudrait pour répondre à cette question ou faire cette tâche.

Ensuite, une grande part de l'enthousiasme concernant les systèmes d'IA modernes est due à leur généralité ; mais certains des systèmes d'IA les plus capables – comme ceux qui génèrent ou reconnaissent la parole ou les images, font de la prédiction et modélisation scientifique, jouent à des jeux, etc. – sont beaucoup plus étroits et bien « à l'intérieur des Portes » en termes de calcul.[^9] Ces systèmes sont surhumains aux tâches particulières qu'ils font. Ils peuvent avoir des faiblesses de cas limites[^10] (ou [exploitables](https://arxiv.org/abs/2211.00241)) dues à leur étroitesse ; cependant *totalement* étroit ou *pleinement* général ne sont pas les seules options disponibles : il y a de nombreuses architectures entre les deux.[^11]

Ces outils d'IA peuvent grandement accélérer l'avancement dans d'autres technologies positives, sans IAG. Pour mieux faire de la physique nucléaire, nous n'avons pas besoin que l'IA soit un physicien nucléaire – nous en avons ! Si nous voulons accélérer la médecine, donnons aux biologistes, chercheurs médicaux, et chimistes des outils puissants. Ils les veulent et les utiliseront à un bénéfice énorme. Nous n'avons pas besoin d'une ferme de serveurs pleine d'un million de génies numériques ; nous avons des millions d'humains dont l'IA peut aider à faire ressortir le génie. Oui, il faudra plus de temps pour obtenir l'immortalité et le remède à toutes les maladies. C'est un coût réel. Mais même les innovations sanitaires les plus prometteuses seraient de peu d'utilité si l'instabilité menée par l'IA conduit à un conflit global ou un effondrement sociétal. Nous nous devons de donner d'abord une chance aux humains assistés par l'IA de s'attaquer au problème.

Et supposons qu'il y ait, en fait, quelque énorme avantage à l'IAG qui ne puisse être obtenu par l'humanité utilisant des outils intra-Portes. Perdons-nous ceux-là en ne construisant *jamais* l'IAG et la superintelligence ? En pesant les risques et récompenses ici, il y a un bénéfice asymétrique énorme à attendre plutôt qu'à se précipiter : nous pouvons attendre jusqu'à ce que cela puisse être fait de manière garantie sûre et bénéfique, et presque tout le monde pourra encore récolter les récompenses ; si nous nous précipitons, cela pourrait être – dans les mots du PDG d'OpenAI Sam Altman – [extinction pour *tous* d'entre nous.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

Mais si les outils non-IAG sont potentiellement si puissants, pouvons-nous les gérer ? La réponse est un clair... peut-être.

## Les systèmes d'IA-outil sont (probablement, en principe) gérables

Mais ce ne sera pas facile. Les systèmes d'IA de pointe actuels peuvent grandement renforcer les personnes et institutions dans l'atteinte de leurs objectifs. C'est, en général, une bonne chose ! Cependant, il y a des dynamiques naturelles d'avoir de tels systèmes à notre disposition – soudainement et sans beaucoup de temps pour que la société s'adapte – qui offrent des risques sérieux qui doivent être gérés. Il vaut la peine de discuter quelques classes majeures de tels risques, et comment ils peuvent être diminués, en supposant une fermeture de Porte.

Une classe de risques est celle d'IA-outil de haute puissance permettant l'accès à la connaissance ou capacité qui avait précédemment été liée à une personne ou organisation, rendant une combinaison de haute capacité plus haute loyauté disponible à un éventail très large d'acteurs. Aujourd'hui, avec assez d'argent une personne de mauvaise intention pourrait embaucher une équipe de chimistes pour concevoir et produire de nouvelles armes chimiques – mais il n'est pas si facile d'avoir cet argent ou de trouver/assembler l'équipe et la convaincre de faire quelque chose d'assez clairement illégal, non éthique, et dangereux. Pour empêcher les systèmes d'IA de jouer un tel rôle, des améliorations sur les méthodes actuelles pourraient bien suffire,[^12] tant que tous ces systèmes et l'accès à eux sont gérés de manière responsable. D'autre part, si des systèmes puissants sont libérés pour usage et modification généraux, toute mesure de sécurité intégrée est probablement supprimable. Donc pour éviter les risques dans cette classe, de fortes restrictions quant à ce qui peut être publiquement libéré – analogues aux restrictions sur les détails des technologies nucléaires, explosives, et autres dangereuses – seront requises.[^13]

Une seconde classe de risques découle de la montée en échelle de machines qui agissent comme ou se font passer pour des personnes. Au niveau des dommages aux personnes individuelles, ces risques incluent des arnaques, spam, et phishing beaucoup plus efficaces, et la prolifération de deepfakes non consensuels.[^14] À un niveau collectif, ils incluent la perturbation de processus sociaux centraux comme la discussion et débat publics, nos systèmes sociétaux de collecte, traitement, et dissémination d'information et de connaissance, et nos systèmes de choix politiques. Atténuer ce risque impliquera probablement (a) des lois restreignant l'imitation de personnes par les systèmes d'IA, et tenant responsables les développeurs d'IA qui créent des systèmes qui génèrent de telles imitations, (b) des systèmes de filigrane et provenance qui identifient et classifient (de manière responsable) le contenu d'IA généré, et (c) de nouveaux systèmes épistémiques socio-techniques qui peuvent créer une chaîne de confiance des données (par exemple caméras et enregistrements) à travers les faits, compréhension, et bons modèles du monde.[^15] Tout ceci est possible, et l'IA peut aider avec certaines parties.

Un troisième risque général est que dans la mesure où certaines tâches sont automatisées, les humains faisant présentement ces tâches peuvent avoir moins de valeur financière en tant que travail. Historiquement, automatiser les tâches a rendu les choses rendues possibles par ces tâches moins chères et plus abondantes, tout en triant les personnes faisant précédemment ces tâches entre celles encore impliquées dans la version automatisée (généralement à compétence/salaire plus élevés), et celles dont le travail vaut moins ou peu. Au net, il est difficile de prédire dans quels secteurs plus versus moins de travail humain sera requis dans le secteur résultant plus large mais plus efficace. En parallèle, la dynamique d'automatisation tend à augmenter l'inégalité et la productivité générale, diminuer le coût de certains biens et services (via les augmentations d'efficacité), et augmenter le coût d'autres (via [la maladie des coûts](https://en.wikipedia.org/wiki/Baumol_effect)). Pour ceux du côté défavorisé de l'augmentation d'inégalité, il est profondément incertain si la diminution de coût dans ces certains biens et services l'emporte sur l'augmentation dans d'autres, et mène à un bien-être global plus grand. Alors comment cela ira-t-il pour l'IA ? À cause de la facilité relative avec laquelle le travail intellectuel humain peut être remplacé par l'IA générale, nous pouvons nous attendre à une version rapide de ceci avec l'IA généraliste compétitive humaine.[^16] Si nous fermons la Porte à l'IAG, beaucoup moins d'emplois seront remplacés en gros par des agents IA ; mais un déplacement massif de main-d'œuvre reste probable sur une période d'années.[^17] Pour éviter une souffrance économique généralisée, il sera probablement nécessaire d'implémenter à la fois une forme d'actifs de base universels ou revenu, et aussi d'ingénier un changement culturel vers la valorisation et récompense du travail centré humain qui est plus difficile à automatiser (plutôt que de voir les prix du travail chuter due à l'augmentation dans la main-d'œuvre disponible poussée hors d'autres parties de l'économie.) D'autres construits, tels que celui de [« dignité des données »](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society) (où les producteurs humains de données d'entraînement se voient automatiquement accordés des royalties pour la valeur créée par ces données dans l'IA) peuvent aider. L'automatisation par l'IA a aussi un second effet adverse potentiel, qui est celui d'automatisation *inappropriée*. Avec les applications où l'IA fait simplement un travail pire, cela inclurait celles où les systèmes d'IA sont susceptibles de violer des préceptes moraux, éthiques, ou légaux – par exemple dans les décisions de vie et de mort, et en matières judiciaires. Celles-ci doivent être traitées en appliquant et étendant nos cadres légaux actuels.

Finalement, une menace significative de l'IA intra-portes est son usage dans la persuasion personnalisée, capture d'attention, et manipulation. Nous avons vu dans les médias sociaux et autres plateformes en ligne la croissance d'une économie de l'attention profondément enracinée (où les services en ligne se battent férocement pour l'attention des utilisateurs) et des systèmes de [« capitalisme de surveillance »](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism) (dans lesquels l'information et profilage des utilisateurs est ajouté à la marchandisation de l'attention.) Il est presque certain que plus d'IA sera mise au service des deux. L'IA est déjà massivement utilisée dans les algorithmes de flux addictifs, mais ceci évoluera vers du contenu généré par IA addictif, personnalisé pour être consommé compulsivement par une seule personne. Et les entrées, réponses, et données de cette personne, seront alimentées dans la machine attention/publicité pour continuer le cercle vicieux. De même, à mesure que les assistants IA fournis par les entreprises technologiques deviennent l'interface pour plus de vie en ligne, ils remplaceront probablement les moteurs de recherche et flux comme mécanisme par lequel la persuasion et monétisation des clients se produit. L'échec de notre société à contrôler ces dynamiques jusqu'ici n'augure rien de bon. Une partie de cette dynamique peut être réduite via des régulations concernant la vie privée, droits des données, et manipulation. Arriver plus à la racine du problème peut nécessiter différentes perspectives, telles que celle d'assistants IA loyaux (discutée ci-dessous.)

Le résultat de cette discussion est celui d'espoir : les systèmes basés sur des outils intra-Portes – au moins tant qu'ils restent comparables en puissance et capacité aux systèmes de pointe d'aujourd'hui – sont probablement gérables s'il y a volonté et coordination pour le faire. Des institutions humaines décentes, renforcées par des outils d'IA,[^18] peuvent le faire. Nous pourrions aussi échouer à le faire. Mais il est difficile de voir comment permettre des systèmes plus puissants aiderait – autre qu'en les mettant aux commandes et espérer le mieux.

## Sécurité nationale

Les courses vers la suprématie IA – menées par la sécurité nationale ou d'autres motivations – nous poussent vers des systèmes d'IA puissants incontrôlés qui tendraient à absorber, plutôt qu'octroyer, le pouvoir. Une course IAG entre les États-Unis et la Chine est une course pour déterminer quelle nation obtient la superintelligence en premier.

Alors que devraient faire ceux responsables de la sécurité nationale à la place ? Les gouvernements ont une forte expérience dans la construction de systèmes contrôlables et sécurisés, et ils devraient redoubler d'efforts pour le faire dans l'IA, soutenant le genre de projets d'infrastructure qui réussissent le mieux quand fait à l'échelle et avec l'approbation gouvernementale.

Au lieu d'un « projet Manhattan » téméraire vers l'IAG,[^19] le gouvernement américain pourrait lancer un projet Apollo pour des systèmes contrôlables, sécurisés, fiables. Ceci pourrait inclure par exemple :

- Un programme majeur pour (a) développer les mécanismes de sécurité matérielle sur puce et (b) l'infrastructure, pour gérer le côté calcul de l'IA puissante. Ceux-ci pourraient s'appuyer sur l'[acte CHIPS](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) américain et le [régime de contrôle d'exportation](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion).
- Une initiative à grande échelle pour développer des techniques de vérification formelle afin que des caractéristiques particulières des systèmes d'IA (comme un interrupteur d'arrêt) puissent être *prouvées* présentes ou absentes. Ceci peut exploiter l'IA elle-même pour développer des preuves de propriétés.
- Un effort à l'échelle nationale pour créer des logiciels vérifiablement sécurisés, propulsé par des outils d'IA qui peuvent recoder les logiciels existants dans des cadres vérifiablement sécurisés.
- Un projet d'investissement national dans l'avancement scientifique utilisant l'IA,[^20] fonctionnant comme un partenariat entre le DOE, NSF, et NIH.

En général, il y a une énorme surface d'attaque sur notre société qui nous rend vulnérables aux risques de l'IA et de sa mauvaise utilisation. Se protéger de certains de ces risques nécessitera un investissement et une standardisation de taille gouvernementale. Ceux-ci fourniraient vastement plus de sécurité que jeter de l'essence sur le feu des courses vers l'IAG. Et si l'IA va être intégrée dans l'armement et les systèmes de commandement et contrôle, il est crucial que l'IA soit fiable et sécurisée, ce que l'IA actuelle n'est simplement pas.

## Concentration du pouvoir et ses atténuations

Cet essai s'est concentré sur l'idée du contrôle humain de l'IA et de son échec potentiel. Mais une autre perspective valide à travers laquelle voir la situation de l'IA est à travers la *concentration du pouvoir.* Le développement d'IA très puissante menace de concentrer le pouvoir soit dans les mains corporatives très peu nombreuses et très larges qui l'ont développée et la contrôleront, soit dans les gouvernements utilisant l'IA comme nouveau moyen de maintenir leur propre pouvoir et contrôle, soit dans les systèmes d'IA eux-mêmes. Ou quelque mélange impie de ce qui précède. Dans tous ces cas, la plupart de l'humanité perd pouvoir, contrôle, et agency. Comment pourrions-nous combattre cela ?

La toute première et plus importante étape, bien sûr, est une fermeture de Porte à l'IAG et superintelligence plus intelligente que l'humain. Celles-ci peuvent explicitement remplacer directement les humains et groupes d'humains. Si elles sont sous contrôle corporatif ou gouvernemental elles concentreront le pouvoir dans ces corporations ou gouvernements ; si elles sont « libres » elles concentreront le pouvoir en elles-mêmes. Supposons donc que les Portes sont fermées. Et alors ?

Une solution proposée à la concentration de pouvoir est l'IA « open-source », où les poids des modèles sont disponibles gratuitement ou largement. Mais comme mentionné précédemment, une fois qu'un modèle est ouvert, la plupart des mesures de sécurité ou garde-fous peuvent être (et sont généralement) supprimés. Il y a donc une tension aiguë entre d'une part la décentralisation, et d'autre part la sécurité, sûreté, et contrôle humain des systèmes d'IA. Il y a aussi des raisons d'être sceptique que les modèles ouverts combattront par eux-mêmes significativement la concentration de pouvoir dans l'IA plus qu'ils l'ont fait dans les systèmes d'exploitation (encore dominés par Microsoft, Apple, et Google malgré les alternatives ouvertes).[^21]

Pourtant il peut y avoir des moyens de résoudre ce cercle – de centraliser et atténuer les risques tout en décentralisant la capacité et récompense économique. Ceci nécessite de repenser à la fois comment l'IA est développée et comment ses bénéfices sont distribués.

De nouveaux modèles de développement et propriété d'IA publique aideraient. Ceci pourrait prendre plusieurs formes : IA développée par le gouvernement (sujette à supervision démocratique),[^22] organisations de développement d'IA à but non lucratif (comme Mozilla pour les navigateurs), ou structures permettant une propriété et gouvernance très largement répandues. La clé est que ces institutions seraient explicitement mandatées pour servir l'intérêt public tout en opérant sous de fortes contraintes de sécurité.[^23] Des régimes réglementaires et de standards/certifications bien conçus seront aussi vitaux, afin que les produits d'IA offerts par un marché vibrant restent genuinement utiles plutôt qu'exploitants envers leurs utilisateurs.

En termes de concentration du pouvoir économique, nous pouvons utiliser le suivi de provenance et la « dignité des données » pour assurer que les bénéfices économiques coulent plus largement. En particulier, la plupart du pouvoir d'IA maintenant (et dans le futur si nous gardons les Portes fermées) découle de données générées par l'humain, que ce soient des données d'entraînement directes ou des retours humains. Si les entreprises d'IA étaient requises de compenser équitablement les fournisseurs de données,[^24] ceci pourrait au moins aider à distribuer les récompenses économiques plus largement. Au-delà de ceci, un autre modèle pourrait être la propriété publique de fractions significatives de grandes entreprises d'IA. Par exemple, les gouvernements capables de taxer les entreprises d'IA pourraient investir une fraction des recettes dans un fonds souverain qui détient des actions dans les entreprises, et paie des dividendes à la population.[^25]

Crucial dans ces mécanismes est d'utiliser le pouvoir de l'IA elle-même pour aider à mieux distribuer le pouvoir, plutôt que de simplement combattre la concentration de pouvoir menée par l'IA en utilisant des moyens non-IA. Une approche puissante serait à travers des assistants IA bien conçus qui opèrent avec un véritable devoir fiduciaire envers leurs utilisateurs – mettant les intérêts des utilisateurs en premier, surtout au-dessus de ceux des fournisseurs corporatifs.[^26] Ces assistants doivent être vraiment dignes de confiance, techniquement compétents mais appropriément limités basé sur le cas d'usage et niveau de risque, et largement disponibles à tous à travers des canaux publics, à but non lucratif, ou à but lucratif certifiés. Tout comme nous n'accepterions jamais un assistant humain qui travaille secrètement contre nos intérêts pour une autre partie, nous ne devrions pas accepter des assistants IA qui surveillent, manipulent, ou extraient de la valeur de leurs utilisateurs pour le bénéfice corporatif.

Une telle transformation changerait fondamentalement la dynamique actuelle où les individus sont laissés à négocier seuls avec de vastes machines corporatives et bureaucratiques (propulsées par l'IA) qui priorisent l'extraction de valeur sur le bien-être humain. Bien qu'il y ait de nombreuses approches possibles pour redistribuer plus largement le pouvoir mené par l'IA, aucune n'émergera par défaut : elles doivent être délibérément conçues et gouvernées avec des mécanismes comme les exigences fiduciaires, provision publique, et accès à niveaux basé sur le risque.

Les approches pour atténuer la concentration de pouvoir peuvent faire face à des vents contraires significatifs des pouvoirs en place.[^27] Mais il y a des chemins vers le développement d'IA qui ne nécessitent pas de choisir entre sécurité et pouvoir concentré. En construisant les bonnes institutions maintenant, nous pourrions assurer que les bénéfices de l'IA soient largement partagés tandis que ses risques sont soigneusement gérés.

## Nouvelles structures de gouvernance et sociales

Nos structures de gouvernance actuelles peinent : elles sont lentes à répondre, souvent capturées par des intérêts spéciaux, et [de plus en plus méfiées par le public.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) Pourtant ce n'est pas une raison de les abandonner – tout à fait le contraire. Certaines institutions peuvent avoir besoin d'être remplacées, mais plus largement nous avons besoin de nouveaux mécanismes qui peuvent améliorer et supplémenter nos structures existantes, les aidant à mieux fonctionner dans notre monde en évolution rapide.

Beaucoup de notre faiblesse institutionnelle découle non des structures gouvernementales formelles, mais d'institutions sociales dégradées : nos systèmes pour développer une compréhension partagée, coordonner l'action, et conduire un discours significatif. Jusqu'ici, l'IA a accéléré cette dégradation, inondant nos canaux d'information avec du contenu généré, nous pointant vers le contenu le plus polarisant et diviseur, et rendant plus difficile de distinguer la vérité de la fiction.

Mais l'IA pourrait en fait aider à reconstruire et renforcer ces institutions sociales. Considérons trois domaines cruciaux :

Premièrement, l'IA pourrait aider à restaurer la confiance dans nos systèmes épistémiques – nos façons de savoir ce qui est vrai. Nous pourrions développer des systèmes propulsés par IA qui suivent et vérifient la provenance de l'information, des données brutes à travers l'analyse aux conclusions. Ces systèmes pourraient combiner la vérification cryptographique avec une analyse sophistiquée pour aider les gens à comprendre non seulement si quelque chose est vrai, mais comment nous savons que c'est vrai.[^28] Des assistants IA loyaux pourraient être chargés de suivre les détails pour s'assurer qu'ils se vérifient.

Deuxièmement, l'IA pourrait permettre de nouvelles formes de coordination à grande échelle. Beaucoup de nos problèmes les plus pressants – du changement climatique à la résistance aux antibiotiques – sont fondamentalement des problèmes de coordination. Nous sommes [coincés dans des situations qui sont pires qu'elles ne pourraient être pour presque tout le monde](https://equilibriabook.com/), parce qu'aucun individu ou groupe ne peut se permettre de faire le premier pas. Les systèmes d'IA pourraient aider en modélisant des structures d'incitations complexes, identifiant des chemins viables vers de meilleurs résultats, et facilitant les mécanismes de construction de confiance et d'engagement nécessaires pour y arriver.

Peut-être plus intriguant, l'IA pourrait permettre des formes entièrement nouvelles de discours social. Imaginez pouvoir « parler à une ville »[^29] – pas seulement voir des statistiques, mais avoir un dialogue significatif avec un système d'IA qui traite et synthétise les vues, expériences, besoins, et aspirations de millions de résidents. Ou considérez comment l'IA pourrait faciliter un dialogue genuine entre des groupes qui actuellement se parlent sans s'entendre, en aidant chaque côté à mieux comprendre les préoccupations et valeurs actuelles de l'autre plutôt que leurs caricatures l'un de l'autre.[^30] Ou l'IA pourrait offrir une intermédiation compétente et crédiblement neutre des disputes entre personnes ou même de grands groupes de personnes (qui pourraient tous interagir avec elle directement et individuellement !) L'IA actuelle est totalement capable de faire ce travail, mais les outils pour le faire ne viendront pas à l'existence par eux-mêmes, ou via des incitations de marché.

Ces possibilités pourraient sembler utopiques, surtout donné le rôle actuel de l'IA dans la dégradation du discours et de la confiance. Mais c'est précisément pourquoi nous devons activement développer ces applications positives. En fermant les Portes à l'IAG incontrôlable et priorisant l'IA qui améliore l'agency humaine, nous pouvons orienter le progrès technologique vers un futur où l'IA sert comme force pour l'autonomisation, la résilience, et l'avancement collectif.

[^1]: Cela dit, rester loin de la triple-intersection n'est malheureusement pas aussi facile qu'on pourrait l'aimer. Pousser très fort la capacité dans n'importe lequel des trois aspects tend à l'augmenter dans les autres. En particulier, il peut être difficile de créer une intelligence extrêmement générale et capable qui ne peut pas être facilement rendue autonome. Une approche est d'entraîner des modèles [« myopes »](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) avec une capacité de planification entravée. Une autre serait de se concentrer sur l'ingénierie de purs systèmes [« oracle »](https://arxiv.org/abs/1711.05541) qui s'écarteraient de répondre aux questions orientées action.

[^2]: Beaucoup d'entreprises ne réalisent pas qu'elles aussi seraient éventuellement déplacées par l'IAG, même si cela prend plus de temps – si elles le faisaient, elles pousseraient peut-être un peu moins sur ces Portes !

[^3]: Les systèmes d'IA pourraient communiquer de manières plus efficaces mais moins intelligibles, mais maintenir la compréhension humaine devrait prendre priorité.

[^4]: Cette idée d'IA modulaire, interprétable a été développée en détail par plusieurs chercheurs ; voir par exemple le modèle [« Comprehensive AI Services »](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) de Drexler, l'[« Open Agency Architecture »](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai) de Dalrymple et autres. Bien que de tels systèmes puissent nécessiter plus d'effort d'ingénierie que des réseaux de neurones monolithiques entraînés avec calcul massif, c'est précisément où les limites de calcul aident – en rendant le chemin plus sûr, plus transparent aussi le plus pratique.

[^5]: Sur les dossiers de sécurité en général voir [ce manuel](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). Concernant l'IA en particulier, voir [Wasil et al.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [Clymer et al.](https://arxiv.org/abs/2403.10462), [Buhl et al.](https://arxiv.org/abs/2410.21572), et [Balesni et al.](https://arxiv.org/abs/2411.03336)

[^6]: Nous voyons en fait déjà cette tendance menée juste par le coût élevé de l'inférence : des modèles plus petits et plus spécialisés « distillés » des plus grands et capables de fonctionner sur du matériel moins cher.

[^7]: Je comprends pourquoi ceux enthousiasmés par l'écosystème technologique de l'IA peuvent s'opposer à ce qu'ils voient comme une régulation onéreuse sur leur industrie. Mais c'est franchement déconcertant pour moi pourquoi, disons, un capital-risqueur voudrait permettre une fuite vers l'IAG et la superintelligence. Ces systèmes (et entreprises, tant qu'elles restent sous contrôle d'entreprise) *dévoreront toutes les startups comme collation*. Probablement même *plus tôt* que dévorer d'autres industries. Quiconque investi dans un écosystème d'IA prospère devrait prioriser assurer que le développement d'IAG ne mène pas à une monopolisation par quelques acteurs dominants.

[^8]: Comme l'économiste et ancien chercheur Deepmind Michael Webb [l'a dit](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/), « Je pense que si nous arrêtions tout développement de plus grands modèles de langage aujourd'hui, donc GPT-4 et Claude et peu importe, et ils sont les dernières choses que nous entraînons de cette taille – donc nous permettons beaucoup plus d'itération sur des choses de cette taille et toutes sortes de réglage fin, mais rien de plus grand que ça, pas d'avancements plus grands – juste ce que nous avons aujourd'hui je pense est assez pour propulser 20 ou 30 ans de croissance économique incroyable. »

[^9]: Par exemple, le système alphafold de DeepMind a utilisé seulement 100 000e du nombre de FLOP de GPT-4.

[^10]: La difficulté des voitures autonomes est importante à noter ici : bien que nominalement une tâche étroite, et réalisable avec fiabilité raisonnable avec des systèmes d'IA relativement petits, une connaissance et compréhension étendues du monde réel sont nécessaires pour obtenir la fiabilité au niveau nécessaire dans une tâche si critique pour la sécurité.

[^11]: Par exemple, donné un budget de calcul, nous verrions probablement des modèles GPAI pré-entraînés à (disons) la moitié de ce budget, et l'autre moitié utilisée pour entraîner une très haute capacité dans une gamme plus étroite de tâches. Ceci donnerait une capacité étroite surhumaine soutenue par une intelligence générale quasi-humaine.

[^12]: La technique d'alignement dominante actuelle est « apprentissage par renforcement par retour humain » [(RLHF)](https://arxiv.org/abs/1706.03741) et utilise le retour humain pour créer un signal de récompense/punition pour l'apprentissage par renforcement du modèle d'IA. Cette technique et les techniques reliées comme [IA constitutionnelle](https://arxiv.org/abs/2212.08073) marchent étonnamment bien (bien qu'elles manquent de robustesse et puissent être contournées avec un effort modeste.) De plus, les modèles de langage actuels sont généralement assez compétents au raisonnement de sens commun qu'ils ne feront pas d'erreurs morales stupides. C'est quelque chose d'un point optimal : assez intelligents pour comprendre ce que les gens veulent (dans la mesure où cela peut être défini), mais pas assez intelligents pour planifier des déceptions élaborées ou causer d'énormes dommages quand ils se trompent.

[^13]: À long terme, tout niveau de capacité d'IA qui est développé est susceptible de proliférer, puisqu'ultimement c'est du logiciel, et utile. Nous devrons avoir des mécanismes robustes pour nous défendre contre les risques que de tels systèmes posent. Mais nous *n'avons pas cela maintenant* donc nous devons être très mesurés dans combien de modèles d'IA puissants sont autorisés à proliférer.

[^14]: La vaste majorité de ceux-ci sont des deepfakes pornographiques non consensuels, incluant de mineurs.

[^15]: Beaucoup d'ingrédients pour de telles solutions existent, sous la forme de lois « bot-ou-pas » (dans l'acte d'IA de l'UE entre autres endroits), [technologies de suivi de provenance industrielles](https://c2pa.org/), [agrégateurs de nouvelles innovants](https://www.improvethenews.org/), [agrégateurs](https://metaculus.com/) de prédiction et marchés, etc.

[^16]: La vague d'automatisation peut ne pas suivre les patterns précédents, en ce que les tâches relativement de *haute* compétence telles que l'écriture de qualité, interpréter la loi, ou donner des conseils médicaux, peuvent être autant ou même plus vulnérables à l'automatisation que les tâches de compétence inférieure.

[^17]: Pour une modélisation soigneuse de l'effet de l'IAG sur les salaires, voir le rapport [ici](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek), et les détails sanglants [ici](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0), d'Anton Korinek et collaborateurs. Ils trouvent qu'à mesure que plus de pièces d'emplois sont automatisées, productivité et salaires montent – jusqu'à un point. Une fois que *trop* est automatisé, la productivité continue d'augmenter, mais les salaires s'effondrent parce que les gens sont remplacés en gros par l'IA efficace. C'est pourquoi fermer les Portes est si utile : nous obtenons la productivité sans les salaires humains évanouis.

[^18]: Il y a de nombreuses façons dont l'IA peut être utilisée comme, et pour aider à construire, des technologies « défensives » pour rendre les protections et gestion plus robustes. Voir [ce](https://vitalik.eth.limo/general/2025/01/05/dacc2.html) poste influent décrivant cet agenda « D/acc ».

[^19]: Quelque peu ironiquement, un projet Manhattan américain ferait probablement peu pour accélérer les calendriers vers l'IAG – le cadran d'investissement humain et fiscal dans le progrès de l'IA est déjà épinglé à 11. Les résultats primaires seraient d'inspirer un projet similaire en Chine (qui excelle aux projets d'infrastructure au niveau national), de rendre les accords internationaux limitant le risque de l'IA beaucoup plus difficiles, et d'alarmer d'autres adversaires géopolitiques des États-Unis tels que la Russie.

[^20]: Le programme [« National AI Research Resource »](https://nairrpilot.org/) est un bon pas actuel dans cette direction et devrait être étendu.

[^21]: Voir [cette analyse](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807) des diverses significations et implications d'« ouvert » dans les produits technologiques et comment certains ont mené à plus, plutôt que moins, d'enracinement de dominance.

[^22]: Les plans aux États-Unis pour une [National AI Research Resource](https://nairratdoe.ornl.gov/) et le lancement récent d'une [Fondation AI Européenne](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/) sont des pas intéressants dans cette direction.

[^23]: Le défi ici n'est pas technique mais institutionnel – nous avons un besoin urgent d'exemples et expériences du monde réel de ce à quoi le développement d'IA d'intérêt public pourrait ressembler.

[^24]: Ceci va à l'encontre des modèles d'affaires actuels des grandes technologies et nécessiterait à la fois action légale et nouvelles normes.

[^25]: Seuls certains gouvernements seront capables de le faire. Une idée plus radicale est [un fonds universel de ce type, sous propriété conjointe de tous les humains.](https://futureoflife.org/project/the-windfall-trust/)

[^26]: Pour une exposition longue de ce cas voir [ce papier](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338) sur la loyauté de l'IA. Malheureusement la trajectoire par défaut des assistants IA est probablement d'être de plus en plus déloyaux.

[^27]: Quelque peu ironiquement, beaucoup de pouvoirs en place sont aussi à risque de désempowerment soutenu par l'IA ; mais il peut être difficile pour eux de percevoir ceci jusqu'à moins que le processus n'aille assez loin.

[^28]: Quelques efforts intéressants dans cette direction sont représentés par [la coalition c2pa](https://c2pa.org/) sur la vérification cryptographique ; [Verity](https://www.improvethenews.org/) et [Ground news](https://ground.news/) sur de meilleures épistémologies de nouvelles ; et [Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com) et marchés de prédiction sur l'ancrage du discours dans des prédictions falsifiables.

[^29]: Voir [ce](https://talktothecity.org/) projet pilote fascinant.

[^30]: Voir [Kialo](https://www.kialo-edu.com/), et efforts du [Collective Intelligence Project](https://www.cip.org/) pour quelques exemples.