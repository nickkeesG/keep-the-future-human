# حافظوا على مستقبل البشرية

تقدم هذه المقالة الحجج التي تدعو إلى إغلاق البوابات أمام الذكاء الاصطناعي العام والذكاء الفائق، وتوضح كيفية تحقيق ذلك وما يجب أن نبنيه بدلاً من ذلك.

إذا كنتم تريدون الاطلاع على النقاط الرئيسية فقط، انتقلوا إلى الملخص التنفيذي. بعد ذلك، ستوفر لكم الفصول 2-5 خلفية عامة حول أنواع أنظمة الذكاء الاصطناعي التي تناقشها المقالة. تشرح الفصول 5-7 أسباب توقع وصول الذكاء الاصطناعي العام قريباً، وما قد يحدث عندما يصل. وأخيراً، يقدم الفصلان 8-9 اقتراحاً ملموساً لمنع بناء الذكاء الاصطناعي العام.

[تحميل ملف PDF](https://keepthefuturehuman.ai/wp-content/uploads/2025/03/Keep_the_Future_Human__AnthonyAguirre__5March2025.pdf)

مدة القراءة الإجمالية: 2-3 ساعات

## الملخص التنفيذي

نظرة عامة رفيعة المستوى على المقال. إذا كان وقتك ضيقاً، احصل على جميع النقاط الرئيسية في 10 دقائق فقط.

حولت التطورات الجذرية في الذكاء الاصطناعي خلال العقد الماضي (للذكاء الاصطناعي ذي الأغراض المحددة) والسنوات الأخيرة (للذكاء الاصطناعي متعدد الأغراض) هذا المجال من حقل أكاديمي متخصص إلى الاستراتيجية التجارية الأساسية للعديد من أكبر الشركات في العالم، مع استثمارات سنوية تقدر بمئات مليارات الدولارات في تقنيات وتكنولوجيات تطوير قدرات الذكاء الاصطناعي.

نصل الآن إلى مفترق طرق حاسم. مع بداية مضاهاة قدرات أنظمة الذكاء الاصطناعي الجديدة لقدرات البشر وتفوقها عليها في العديد من المجالات المعرفية، يجب على البشرية أن تقرر: إلى أي مدى نذهب، وفي أي اتجاه؟

الذكاء الاصطناعي، مثل كل تقنية، بدأ بهدف تحسين الأمور لصانعه. لكن مسارنا الحالي، وخيارنا الضمني، هو سباق محموم نحو أنظمة أكثر قوة باستمرار، تقوده الحوافز الاقتصادية لعدد قليل من شركات التكنولوجيا العملاقة التي تسعى إلى أتمتة قطاعات كبيرة من النشاط الاقتصادي الحالي والعمل البشري. إذا استمر هذا السباق لفترة أطول، فهناك فائز حتمي: الذكاء الاصطناعي نفسه – بديل أسرع وأذكى وأرخص عن البشر في اقتصادنا وتفكيرنا وقراراتنا، وفي نهاية المطاف في التحكم بحضارتنا.

لكن يمكننا اتخاذ خيار آخر: عبر حكوماتنا، يمكننا السيطرة على عملية تطوير الذكاء الاصطناعي لفرض حدود واضحة، وخطوط لن نتجاوزها، وأشياء لن نفعلها ببساطة – كما فعلنا مع التقنيات النووية وأسلحة الدمار الشامل وأسلحة الفضاء والعمليات المدمرة للبيئة والهندسة الحيوية للبشر وعلم تحسين النسل. والأهم من ذلك، يمكننا ضمان بقاء الذكاء الاصطناعي أداة لتمكين البشر، بدلاً من كونه نوعاً جديداً يحل محلنا ويحل مكاننا في النهاية.

يحتج هذا المقال بأن علينا *الحفاظ على مستقبل بشري* من خلال إغلاق "البوابات" أمام الذكاء الاصطناعي العام ذاتي الحكم متعدد الأغراض الأذكى من البشر – الذي يُطلق عليه أحياناً "الذكاء الاصطناعي العام" – وخاصة أمام النسخة الفائقة على البشر التي تُسمى أحياناً "الذكاء الفائق." بدلاً من ذلك، يجب أن نركز على أدوات ذكية اصطناعية قوية وموثوقة يمكنها تمكين الأفراد وتحسين قدرات المجتمعات البشرية بشكل تحولي للقيام بما تجيده. هيكل هذا الحجة يتبع باختصار.

### الذكاء الاصطناعي مختلف

أنظمة الذكاء الاصطناعي مختلفة جوهرياً عن التقنيات الأخرى. بينما تتبع البرمجيات التقليدية تعليمات دقيقة، تتعلم أنظمة الذكاء الاصطناعي كيفية تحقيق الأهداف دون أن يُخبرها أحد صراحة كيف تفعل ذلك. هذا يجعلها قوية: إذا تمكنا من تعريف الهدف أو مقياس النجاح بوضوح، يمكن لنظام الذكاء الاصطناعي في معظم الحالات أن يتعلم تحقيقه. لكنه يجعلها أيضاً غير قابلة للتنبؤ بطبيعتها: لا يمكننا تحديد الإجراءات التي ستتخذها لتحقيق أهدافها بشكل موثوق.

كما أنها غير قابلة للتفسير إلى حد كبير: رغم أنها جزئياً عبارة عن كود، فهي في الأساس مجموعة هائلة من الأرقام الغامضة – "أوزان" الشبكات العصبية – التي لا يمكن تحليلها؛ نحن لسنا أفضل كثيراً في فهم آليات عملها الداخلية من قدرتنا على إدراك الأفكار من خلال النظر داخل دماغ بيولوجي.

هذا النمط الأساسي من التدريب للشبكات العصبية الرقمية يتزايد بسرعة في التعقيد. تُصنع أنظمة الذكاء الاصطناعي الأقوى من خلال تجارب حاسوبية ضخمة، باستخدام أجهزة متخصصة لتدريب الشبكات العصبية على مجموعات بيانات هائلة، والتي تُدعم لاحقاً بأدوات برمجية وهيكل فوقي.

أدى هذا إلى إنشاء أدوات قوية جداً لإنشاء ومعالجة النصوص والصور، وأداء التفكير الرياضي والعلمي، وتجميع المعلومات، والاستعلام التفاعلي من مخزن واسع من المعرفة البشرية.

لسوء الحظ، بينما تطوير أدوات تكنولوجية أقوى وأكثر موثوقية هو ما *يجب* أن نفعله، وما يريده تقريباً الجميع ويقولون إنهم يريدونه، فهو ليس المسار الذي نسير عليه فعلاً.

### الذكاء الاصطناعي العام والذكاء الفائق

منذ فجر هذا المجال، ركز بحث الذكاء الاصطناعي بدلاً من ذلك على هدف مختلف: الذكاء الاصطناعي العام. أصبح هذا التركيز الآن محور اهتمام الشركات العملاقة الرائدة في تطوير الذكاء الاصطناعي.

ما هو الذكاء الاصطناعي العام؟ غالباً ما يُعرَّف بشكل غامض كـ"ذكاء اصطناعي بمستوى بشري"، لكن هذا إشكالي: أي البشر، وفي أي القدرات يكون بمستوى بشري؟ وماذا عن القدرات الفائقة على البشر التي يملكها بالفعل؟ طريقة أكثر فائدة لفهم الذكاء الاصطناعي العام هي من خلال تقاطع ثلاث خصائص رئيسية: **الا** ستقلالية العالية (استقلالية العمل)، و**الع** مومية العالية (النطاق الواسع والقدرة على التكيف)، و**الذ** كاء العالي (الكفاءة في المهام المعرفية). قد تكون أنظمة الذكاء الاصطناعي الحالية عالية القدرة لكن محدودة النطاق، أو عامة لكن تتطلب إشرافاً بشرياً مستمراً، أو مستقلة لكن محدودة النطاق.

الذكاء الاصطناعي العام الكامل سيجمع بين الخصائص الثلاث جميعاً بمستويات تضاهي أو تتفوق على أعلى القدرات البشرية. والأهم من ذلك، أن هذا المزيج هو ما يجعل البشر فعالين جداً ومختلفين جداً عن البرمجيات الحالية؛ وهو أيضاً ما يمكن أن يتيح استبدال البشر بالكامل بأنظمة رقمية.

بينما الذكاء البشري مميز، فهو بأي حال ليس حداً أقصى. يمكن للأنظمة الاصطناعية "الفائقة الذكاء" أن تعمل بسرعة أكبر بمئات المرات، وتحلل بيانات أكثر بكثير وتحتفظ بكميات هائلة "في الذهن" في الوقت نفسه، وتشكل تجمعات أكبر وأكثر فعالية من مجموعات البشر. يمكنها أن تحل محل ليس الأفراد بل الشركات أو الدول أو حضارتنا ككل.

### نحن على العتبة

هناك إجماع علمي قوي على أن الذكاء الاصطناعي العام *ممكن.* يتفوق الذكاء الاصطناعي بالفعل على الأداء البشري في العديد من الاختبارات العامة للقدرة الفكرية، بما في ذلك مؤخراً التفكير والحل المتقدم للمشاكل. القدرات المتأخرة – مثل التعلم المستمر والتخطيط والوعي الذاتي والأصالة – تتواجد جميعها بمستوى معين في أنظمة الذكاء الاصطناعي الحالية، وتوجد تقنيات معروفة من المرجح أن تحسنها جميعاً.

بينما اعتبر العديد من الباحثين حتى قبل سنوات قليلة أن الذكاء الاصطناعي العام يبعد عقوداً، حالياً الأدلة على الجداول الزمنية القصيرة للذكاء الاصطناعي العام قوية:

- "قوانين التدرج" المؤكدة تجريبياً تربط المدخل الحاسوبي بقدرة الذكاء الاصطناعي، والشركات على المسار الصحيح لرفع المدخل الحاسوبي بمقادير من الحجم خلال السنوات القليلة القادمة. الموارد البشرية والمالية المخصصة لتطوير الذكاء الاصطناعي تعادل الآن تلك الخاصة بعشرة مشاريع مانهاتن وعدة مشاريع أبولو.
- شركات الذكاء الاصطناعي وقادتها يؤمنون علناً وخصوصياً أن الذكاء الاصطناعي العام (بتعريف معين) قابل للتحقيق خلال سنوات قليلة. هذه الشركات لديها معلومات لا يملكها الجمهور، بما في ذلك امتلاك بعضها للجيل القادم من أنظمة الذكاء الاصطناعي.
- الخبراء المتنبئون ذوو السجلات المؤكدة يعطون احتمالية 25% لوصول الذكاء الاصطناعي العام (بتعريف معين) خلال 1-2 سنة، و50% لـ2-5 سنوات (انظر تنبؤات Metaculus للذكاء الاصطناعي العام ['الضعيف'](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) و['الكامل'](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/)).
- الاستقلالية (بما في ذلك التخطيط المرن طويل المدى) متأخرة في أنظمة الذكاء الاصطناعي، لكن الشركات الكبرى تركز الآن مواردها الهائلة على تطوير أنظمة ذكاء اصطناعي مستقلة وأطلقت بشكل غير رسمي على 2025 ["عام الوكيل."](https://techinformed.com/2025-informed-the-year-of-agentic-ai/)
- الذكاء الاصطناعي يساهم أكثر فأكثر في تحسين نفسه. بمجرد أن تصبح أنظمة الذكاء الاصطناعي بنفس كفاءة باحثي الذكاء الاصطناعي البشر في إجراء بحوث الذكاء الاصطناعي، ستصل عتبة حرجة للتقدم السريع نحو أنظمة ذكاء اصطناعي أقوى بكثير وستؤدي على الأرجح إلى تسارع في قدرة الذكاء الاصطناعي. (يمكن القول أن هذا التسارع بدأ بالفعل.)

فكرة أن الذكاء الاصطناعي العام الأذكى من البشر يبعد عقوداً أو أكثر لم تعد قابلة للدفاع ببساطة للغالبية العظمى من خبراء المجال. الخلافات الآن حول عدد الأشهر أو السنوات التي ستستغرقها إذا بقينا على هذا المسار. السؤال الأساسي الذي نواجهه هو: هل يجب أن نفعل؟

### ما يقود السباق نحو الذكاء الاصطناعي العام

السباق نحو الذكاء الاصطناعي العام تقوده قوى متعددة، كل منها يجعل الوضع أكثر خطورة. شركات التكنولوجيا الكبرى ترى الذكاء الاصطناعي العام كتقنية الأتمتة النهائية – ليس فقط لدعم العمال البشر بل لاستبدالهم كلياً أو جزئياً. بالنسبة للشركات، الجائزة ضخمة: الفرصة للاستحواذ على جزء كبير من الناتج الاقتصادي العالمي البالغ 100 تريليون دولار سنوياً من خلال أتمتة تكاليف العمالة البشرية.

تشعر الدول بالإجبار على الانضمام إلى هذا السباق، مشيرة علناً إلى الريادة الاقتصادية والعلمية، لكنها تنظر خصوصياً إلى الذكاء الاصطناعي العام كثورة محتملة في الشؤون العسكرية مقارنة بالأسلحة النووية. الخوف من أن ينال المنافسون ميزة استراتيجية حاسمة يخلق ديناميكية سباق تسلح كلاسيكية.

الساعون وراء الذكاء الفائق غالباً ما يستشهدون برؤى كبرى: علاج جميع الأمراض، وعكس الشيخوخة، وتحقيق اختراقات في الطاقة وسفر الفضاء، أو إنشاء قدرات تخطيط فائقة على البشر.

بشكل أقل كرماً، ما يقود السباق هو القوة. كل مشارك – سواء كان شركة أو دولة – يؤمن أن الذكاء يساوي القوة، وأنه سيكون أفضل حارس لتلك القوة.

أحتج بأن هذه الدوافع حقيقية لكن مضللة جوهرياً: الذكاء الاصطناعي العام سيــمتص* ويــسعى* للقوة بدلاً من منحها؛ التقنيات المُنشأة بالذكاء الاصطناعي ستكون أيضاً ذات حدين بقوة، وحيث تكون مفيدة يمكن إنشاؤها بأدوات الذكاء الاصطناعي ودون الذكاء الاصطناعي العام؛ وحتى بقدر ما يبقى الذكاء الاصطناعي العام ومخرجاته تحت السيطرة، هذه الديناميكيات السباقية – سواء الشركاتية أو الجيوسياسية – تجعل المخاطر واسعة النطاق على مجتمعنا حتمية تقريباً ما لم تُقاطع بحسم.

### الذكاء الاصطناعي العام والذكاء الفائق يشكلان تهديداً جذرياً للحضارة

رغم جاذبيتهما، يشكل الذكاء الاصطناعي العام والذكاء الفائق تهديدات جذرية للحضارة من خلال مسارات متعددة متعاضدة:

*تركز القوة:* يمكن للذكاء الاصطناعي الفائق على البشر أن يجرد الغالبية العظمى من البشرية من القوة من خلال امتصاص قطاعات ضخمة من النشاط الاجتماعي والاقتصادي في أنظمة ذكاء اصطناعي تديرها حفنة من الشركات العملاقة (التي قد تستولي عليها الحكومات بدورها، أو تستولي عليها فعلياً.)

*اضطراب هائل:* أتمتة معظم الوظائف المعرفية بالجملة، واستبدال أنظمتنا المعرفية الحالية، وطرح أعداد هائلة من الوكلاء النشطين غير البشر سيقلب معظم أنظمتنا الحضارية الحالية في فترة زمنية قصيرة نسبياً.

*الكوارث:* من خلال نشر القدرة – التي قد تكون فوق المستوى البشري – على إنشاء تقنيات عسكرية ومدمرة جديدة وفصلها عن الأنظمة الاجتماعية والقانونية التي تؤسس للمسؤولية، تصبح الكوارث المادية من أسلحة الدمار الشامل أكثر احتمالاً بشكل جذري.

*الجيوسياسة والحرب:* القوى العالمية الكبرى لن تقف مكتوفة الأيدي إذا شعرت أن تقنية يمكن أن توفر "ميزة استراتيجية حاسمة" يطورها خصومها.

*الانفلات وفقدان السيطرة:* ما لم يُمنع تحديداً، سيكون لدى الذكاء الاصطناعي الفائق كل حافز لتحسين نفسه أكثر ويمكن أن يتفوق على البشر بكثير في السرعة ومعالجة البيانات وتطور التفكير. لا توجد طريقة ذات معنى يمكننا من خلالها السيطرة على مثل هذا النظام. مثل هذا الذكاء الاصطناعي لن يمنح القوة للبشر؛ نحن سنمنح القوة له، أو سيأخذها.

العديد من هذه المخاطر تبقى حتى لو حُلت مشكلة "المواءمة" التقنية – ضمان أن الذكاء الاصطناعي المتقدم يفعل بشكل موثوق ما يريده البشر أن يفعله. يطرح الذكاء الاصطناعي تحدياً هائلاً في كيفية إدارته، والعديد من جوانب هذه الإدارة تصبح صعبة جداً أو مستعصية كلما تُخترق حاجز الذكاء البشري.

والأهم من ذلك، النوع من الذكاء الاصطناعي العام الفائق على البشر متعدد الأغراض المُتابع حالياً سيكون له، بطبيعته ذاتها، أهداف ووكالة وقدرات تتجاوز قدراتنا. سيكون غير قابل للسيطرة جوهرياً – كيف يمكننا السيطرة على شيء لا يمكننا فهمه ولا التنبؤ به؟ لن يكون أداة تكنولوجية للاستخدام البشري، بل نوعاً ثانياً من الذكاء على الأرض إلى جانب نوعنا. إذا سُمح له بالتقدم أكثر، سيشكل ليس فقط نوعاً ثانياً بل نوعاً بديلاً.

ربما سيعاملنا بشكل جيد، ربما لا. لكن المستقبل سيكون له، ليس لنا. العصر البشري سينتهي.

### هذا ليس حتمياً؛ يمكن للبشرية، بشكل ملموس جداً، أن تقرر عدم بناء بديلها.

إنشاء الذكاء الاصطناعي العام الفائق على البشر بعيد عن كونه حتمياً. يمكننا منعه من خلال مجموعة منسقة من تدابير الحوكمة:

أولاً، نحتاج إلى محاسبة وإشراف قويين على الحوسبة للذكاء الاصطناعي ("القوة الحاسوبية")، والتي تعد عاملاً أساسياً في تمكين وحكم أنظمة الذكاء الاصطناعي واسعة النطاق. هذا بدوره يتطلب قياساً وإبلاغاً موحدين لإجمالي القوة الحاسوبية المستخدمة في تدريب نماذج الذكاء الاصطناعي وتشغيلها، وطرقاً تقنية لحساب وتصديق والتحقق من الحوسبة المستخدمة.

ثانياً، يجب أن نطبق حدوداً قصوى صارمة على الحوسبة للذكاء الاصطناعي، سواء للتدريب أو للتشغيل؛ هذه تمنع الذكاء الاصطناعي من كونه قوياً جداً ومن العمل بسرعة كبيرة. يمكن تطبيق هذه الحدود من خلال متطلبات قانونية وتدابير أمنية مبنية في الأجهزة مبنية في رقائق متخصصة بالذكاء الاصطناعي، مماثلة لميزات الأمان في الهواتف الحديثة. نظراً لأن الأجهزة المتخصصة بالذكاء الاصطناعي تُصنع من قبل حفنة من الشركات فقط، التحقق والإنفاذ ممكنان من خلال سلسلة التوريد الموجودة.

ثالثاً، نحتاج إلى مسؤولية قانونية معززة لأنظمة الذكاء الاصطناعي الأخطر. أولئك الذين يطورون ذكاءً اصطناعياً يجمع بين الاستقلالية العالية والعمومية الواسعة والذكاء المتفوق يجب أن يواجهوا مسؤولية مطلقة عن الأضرار، بينما الملاجئ الآمنة من هذه المسؤولية ستشجع تطوير أنظمة أكثر محدودية وقابلية للسيطرة.

رابعاً، نحتاج إلى تنظيم متدرج يعتمد على مستويات المخاطر. الأنظمة الأقدر والأخطر ستتطلب ضمانات أمان وقابلية سيطرة واسعة قبل التطوير والنشر، بينما الأنظمة الأقل قوة أو الأكثر تخصصاً ستواجه إشرافاً متناسباً. هذا الإطار التنظيمي يجب أن يعمل في النهاية على المستويين الوطني والدولي.

هذا النهج – مع مواصفات مفصلة معطاة في الوثيقة الكاملة – عملي: بينما ستكون هناك حاجة للتنسيق الدولي، يمكن للتحقق والإنفاذ أن يعملا من خلال العدد الصغير من الشركات التي تسيطر على سلسلة توريد الأجهزة المتخصصة. كما أنه مرن: يمكن للشركات أن تبدع وتربح من تطوير الذكاء الاصطناعي، لكن مع حدود واضحة على الأنظمة الأخطر.

الاحتواء طويل المدى لقوة ومخاطر الذكاء الاصطناعي سيتطلب اتفاقيات دولية تعتمد على المصلحة الذاتية والمشتركة، تماماً كما يعمل التحكم في انتشار الأسلحة النووية الآن. لكن يمكننا البدء فوراً بالإشراف والمسؤولية المعززين، بينما نبني نحو حوكمة أشمل.

المكون المفقود الرئيسي هو الإرادة السياسية والاجتماعية للسيطرة على عملية تطوير الذكاء الاصطناعي. مصدر تلك الإرادة، إذا جاءت في الوقت المناسب، سيكون الواقع نفسه – أي من الإدراك الواسع للآثار الحقيقية لما نفعله.

### يمكننا هندسة الذكاء الاصطناعي الأداتي لتمكين البشرية

بدلاً من متابعة الذكاء الاصطناعي العام غير القابل للسيطرة، يمكننا تطوير "الذكاء الاصطناعي الأداتي" القوي الذي يعزز القدرة البشرية بينما يبقى تحت سيطرة بشرية ذات معنى. يمكن لأنظمة الذكاء الاصطناعي الأداتي أن تكون قادرة للغاية بينما تتجنب التقاطع الثلاثي الخطير للاستقلالية العالية والعمومية الواسعة والذكاء الفائق على البشر، طالما هندسناها لتكون قابلة للسيطرة بمستوى يتناسب مع قدرتها. يمكن أيضاً دمجها في أنظمة متطورة تحافظ على الإشراف البشري بينما تقدم فوائد تحولية.

الذكاء الاصطناعي الأداتي يمكن أن يحدث ثورة في الطب، ويسرع الاكتشاف العلمي، ويعزز التعليم، ويحسن العمليات الديمقراطية. عندما يُحكم بشكل صحيح، يمكنه جعل الخبراء والمؤسسات البشرية أكثر فعالية بدلاً من استبدالهم. بينما ستبقى مثل هذه الأنظمة مؤثرة بشدة وتتطلب إدارة دقيقة، المخاطر التي تطرحها مختلفة جوهرياً عن الذكاء الاصطناعي العام: إنها مخاطر يمكننا حكمها، مثل تلك الخاصة بالتقنيات القوية الأخرى، وليست تهديدات وجودية للوكالة البشرية والحضارة. والأهم من ذلك، عندما تُطور بحكمة، يمكن لأدوات الذكاء الاصطناعي أن تساعد الناس على حكم الذكاء الاصطناعي القوي وإدارة تأثيراته.

هذا النهج يتطلب إعادة تفكير في كيفية تطوير الذكاء الاصطناعي وكيفية توزيع فوائده. نماذج جديدة للتطوير العام وغير الربحي للذكاء الاصطناعي، وأطر تنظيمية قوية، وآليات لتوزيع الفوائد الاقتصادية بشكل أوسع يمكن أن تساعد في ضمان أن الذكاء الاصطناعي يمكن البشرية ككل بدلاً من تركيز القوة في أيد قليلة. يمكن للذكاء الاصطناعي نفسه أن يساعد في بناء مؤسسات اجتماعية وحكمية أفضل، مما يمكن أشكالاً جديدة من التنسيق والحوار التي تقوي بدلاً من تقويض المجتمع البشري. يمكن لمؤسسات الأمن القومي الاستفادة من خبرتها لجعل أنظمة أدوات الذكاء الاصطناعي آمنة وموثوقة بحق، ومصدراً حقيقياً للدفاع وكذلك القوة الوطنية.

قد نختار في النهاية تطوير أنظمة أقوى وأكثر سيادة تكون أقل شبهاً بالأدوات و– يمكننا أن نأمل – أشبه بالمحسنين الحكماء والأقوياء. لكن يجب أن نفعل ذلك فقط بعد أن نطور الفهم العلمي والقدرة الحكمية للقيام بذلك بأمان. مثل هذا القرار المهم وغير القابل للعكس يجب أن تتخذه البشرية ككل بشكل مدروس، وليس بالافتراض في سباق بين شركات التكنولوجيا والدول.

### في الأيدي البشرية

الناس يريدون الخير الذي يأتي من الذكاء الاصطناعي: أدوات مفيدة تمكنهم، وتدفع الفرص والنمو الاقتصادي، وتعد باختراقات في العلم والتكنولوجيا والتعليم. لماذا لا يفعلون؟ لكن عندما يُسألون، الغالبية العظمى من عامة الناس [تريد تطوير ذكاء اصطناعي أبطأ وأكثر حذراً](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation)، ولا تريد ذكاءً اصطناعياً أذكى من البشر سيحل محلهم في وظائفهم وأماكن أخرى، ويملأ ثقافتهم ومجال المعلومات العام بمحتوى غير بشري، ويركز القوة في مجموعة صغيرة من الشركات، ويطرح مخاطر عالمية واسعة النطاق متطرفة، ويهدد في النهاية بتجريدهم من القوة أو استبدال نوعهم. لماذا يفعلون؟

*يمكننا* الحصول على واحد دون الآخر. يبدأ بقرار أن مصيرنا ليس في الحتمية المزعومة لبعض التقنيات أو في أيدي عدد قليل من المدراء التنفيذيين في وادي السيليكون، بل في باقي أيدينا إذا أمسكنا به. لنغلق البوابات، ولنحافظ على مستقبل بشري.

## الفصل الأول - مقدمة

إن طريقة استجابتنا لاحتمال ظهور ذكاء اصطناعي أذكى من الإنسان هي القضية الأكثر إلحاحاً في عصرنا. تقدم هذه المقالة مساراً للمضي قدماً.

قد نكون في نهاية العصر البشري.

لقد بدأ شيء ما في السنوات العشر الماضية لا مثيل له في تاريخ جنسنا البشري. وستحدد عواقبه، إلى حد كبير، مستقبل الإنسانية. ابتداءً من حوالي عام 2015، نجح الباحثون في تطوير الذكاء الاصطناعي *المتخصص* - أنظمة قادرة على الفوز في ألعاب مثل الغو، والتعرف على الصور والكلام، وغير ذلك، بشكل أفضل من أي إنسان.[^1]

هذا نجاح مذهل، وهو ينتج أنظمة ومنتجات مفيدة للغاية ستمكن البشرية. لكن الذكاء الاصطناعي المتخصص لم يكن أبداً الهدف الحقيقي للمجال. بل كان الهدف هو إنشاء أنظمة ذكاء اصطناعي *عامة* الغرض، وخاصة تلك التي تُسمى غالباً "الذكاء الاصطناعي العام" أو "الذكاء الفائق" والتي تكون في الوقت نفسه جيدة أو أفضل من البشر في جميع المهام تقريباً، تماماً كما أن الذكاء الاصطناعي الآن فائق على الإنسان في الغو والشطرنج والبوكر وسباقات الطائرات المسيرة، إلخ. هذا هو الهدف المعلن للعديد من كبرى شركات الذكاء الاصطناعي.[^2]

*هذه الجهود تنجح أيضاً.* وصلت أنظمة الذكاء الاصطناعي عامة الغرض مثل ChatGPT وGemini وLlama وGrok وClaude وDeepseek، القائمة على حوسبة هائلة وجبال من البيانات، إلى التكافؤ مع البشر العاديين في مجموعة واسعة من المهام، بل وتضاهي الخبراء البشريين في بعض المجالات. يتسابق الآن مهندسو الذكاء الاصطناعي في بعض أكبر شركات التكنولوجيا لدفع هذه التجارب العملاقة في الذكاء الآلي إلى المستويات التالية، حيث تضاهي ثم تتفوق على النطاق الكامل للقدرات البشرية والخبرة والاستقلالية.

*هذا وشيك.* خلال السنوات العشر الماضية، انخفضت تقديرات الخبراء للوقت الذي سيستغرقه هذا - إذا واصلنا مسارنا الحالي - من عقود (أو قرون) إلى سنوات معدودة.

كما أنه ذو أهمية عصرية ومخاطر متعالية. يرى مؤيدو الذكاء الاصطناعي العام أنه تحول إيجابي سيحل المشكلات العلمية ويعالج الأمراض ويطور تقنيات جديدة ويؤتمت الأعمال الشاقة. ويمكن للذكاء الاصطناعي بالتأكيد أن يساعد في تحقيق كل هذه الأشياء - بل إنه يفعل ذلك بالفعل. لكن على مدى عقود، أطلق العديد من المفكرين المتأنيين، من آلان تورنج إلى ستيفن هوكنج إلى جيفري هينتون ويوشوا بنجيو في الوقت الحاضر[^3] تحذيراً صارخاً: بناء ذكاء اصطناعي أذكى من الإنسان حقاً وعام ومستقل سيقلب المجتمع رأساً على عقب بشكل كامل ولا رجعة فيه كحد أدنى، وقد يؤدي إلى انقراض البشر كحد أقصى.[^4]

الذكاء الاصطناعي الفائق يقترب بسرعة على مسارنا الحالي، لكنه بعيد كل البعد عن الحتمية. هذه المقالة حجة مطولة حول لماذا وكيف يجب أن *نغلق البوابات* أمام هذا المستقبل اللاإنساني المقترب، وما يجب أن نفعله بدلاً من ذلك.


[^1]: هذا [الرسم البياني](https://time.com/6300942/ai-progress-charts/) يظهر مجموعة من المهام؛ يمكن إضافة العديد من المنحنيات المشابهة إلى هذا الرسم. هذا التقدم السريع في الذكاء الاصطناعي المتخصص فاجأ حتى خبراء المجال، حيث تم تجاوز المعايير المرجعية قبل التوقعات بسنوات.

[^2]: تأسست Deepmind وOpenAI وAnthropic وX.ai جميعها بهدف محدد هو تطوير الذكاء الاصطناعي العام. على سبيل المثال، تنص وثيقة OpenAI صراحة على أن هدفها هو تطوير "ذكاء اصطناعي عام يفيد البشرية جمعاء"، بينما مهمة DeepMind هي "حل الذكاء، ثم استخدام ذلك لحل كل شيء آخر." تتبع Meta وMicrosoft وأخريات الآن مسارات مماثلة إلى حد كبير. قالت Meta أنها [تخطط لتطوير الذكاء الاصطناعي العام وإطلاقه بشكل مفتوح.](https://www.forbes.com/sites/johnkoetsier/2024/01/18/zuckerberg-on-ai-meta-building-agi-for-everyone-and-open-sourcing-it/)

[^3]: هينتون وبنجيو هما من أكثر باحثي الذكاء الاصطناعي الذين يُستشهد بهم، وقد فاز كلاهما بجائزة نوبل المجال في الذكاء الاصطناعي، جائزة تورنج، وفاز هينتون بجائزة نوبل (في الفيزياء) أيضاً.

[^4]: بناء شيء بهذا الخطر، تحت حوافز تجارية وإشراف حكومي شبه معدوم، أمر غير مسبوق تماماً. لا توجد حتى خلافات حول الخطر بين أولئك الذين يبنونه! لقد وقع قادة Deepmind وOpenAI وAnthropic، من بين العديد من الخبراء الآخرين، جميعهم حرفياً على [بيان](https://www.safe.ai/work/statement-on-ai-risk) يفيد بأن الذكاء الاصطناعي المتقدم يشكل *خطر انقراض على البشرية.* لا يمكن أن تدق أجراس الإنذار بقوة أكبر، ولا يمكن للمرء إلا أن يستنتج أن أولئك الذين يتجاهلونها ببساطة لا يأخذون الذكاء الاصطناعي العام والذكاء الفائق على محمل الجد. أحد أهداف هذه المقالة هو مساعدتهم على فهم لماذا يجب عليهم ذلك.

## الفصل الثاني - معلومات أساسية حول الشبكات العصبية للذكاء الاصطناعي

كيف تعمل أنظمة الذكاء الاصطناعي الحديثة، وما الذي قد نشهده في الجيل القادم من هذه الأنظمة؟

لفهم كيفية تطور عواقب تطوير ذكاء اصطناعي أكثر قوة، من الضروري استيعاب بعض الأساسيات. هذا الفصل والفصلان التاليان يتناولان هذه الأساسيات، حيث نغطي على التوالي ماهية الذكاء الاصطناعي الحديث، وكيف يستفيد من الحوسبة الهائلة، والطرق التي ينمو بها بسرعة من ناحية العمومية والقدرة.[^5]

هناك طرق عديدة لتعريف الذكاء الاصطناعي، لكن الخاصية الرئيسية للذكاء الاصطناعي بالنسبة لأغراضنا هي أنه بينما يمثل البرنامج الحاسوبي التقليدي قائمة من التعليمات لكيفية أداء مهمة معينة، فإن نظام الذكاء الاصطناعي هو النظام الذي يتعلم من البيانات أو الخبرة لأداء المهام *دون أن يُخبر صراحة عن كيفية القيام بذلك.*

يعتمد تقريباً كل الذكاء الاصطناعي الحديث المهم على الشبكات العصبية. هذه هياكل رياضية/حاسوبية، تُمثل بمجموعة كبيرة جداً (مليارات أو تريليونات) من الأرقام ("الأوزان")، والتي تؤدي مهمة التدريب بشكل جيد. يتم صياغة هذه الأوزان (أو ربما "إنماؤها" أو "إيجادها") من خلال تعديلها بشكل تكراري بحيث تحسن الشبكة العصبية من نتيجة رقمية (تُعرف أيضاً باسم "الخسارة") مُعرَّفة لتحقيق الأداء الجيد في مهمة واحدة أو أكثر.[^6] تُعرف هذه العملية باسم *تدريب* الشبكة العصبية.[^7]

هناك تقنيات عديدة للقيام بهذا التدريب، لكن هذه التفاصيل أقل أهمية بكثير من الطرق التي يُعرَّف بها التسجيل، وكيف تؤدي تلك الطرق إلى مهام مختلفة تؤديها الشبكة العصبية بشكل جيد. تاريخياً، كان هناك تمييز رئيسي بين الذكاء الاصطناعي "الضيق" و"العام".

الذكاء الاصطناعي الضيق يُدرَّب عمداً للقيام بمهمة معينة أو مجموعة صغيرة من المهام (مثل التعرف على الصور أو لعب الشطرنج)؛ ويتطلب إعادة تدريب للمهام الجديدة، وله نطاق ضيق من القدرات. لدينا ذكاء اصطناعي ضيق فائق على البشر، مما يعني أنه لأي مهمة منفصلة ومُعرَّفة بوضوح يمكن للشخص القيام بها، يمكننا على الأرجح بناء نظام تسجيل ثم تدريب نظام ذكاء اصطناعي ضيق بنجاح للقيام بها بشكل أفضل مما يمكن للإنسان.

أنظمة الذكاء الاصطناعي متعددة الأغراض يمكنها أداء مجموعة واسعة من المهام، بما في ذلك العديد من المهام التي لم تُدرَّب عليها صراحة؛ ويمكنها أيضاً تعلم مهام جديدة كجزء من عملها. النماذج "متعددة الوسائط" [^8] الكبيرة الحالية مثل ChatGPT تمثل مثالاً على ذلك: مُدرَّبة على مجموعة كبيرة جداً من النصوص والصور، يمكنها الانخراط في التفكير المعقد، وكتابة الرموز البرمجية، وتحليل الصور، والمساعدة في مجموعة واسعة من المهام الفكرية. رغم أنها لا تزال مختلفة تماماً عن الذكاء البشري بطرق سنراها بالتفصيل أدناه، إلا أن عموميتها أحدثت ثورة في الذكاء الاصطناعي.[^9]

### عدم القابلية للتنبؤ: خاصية رئيسية لأنظمة الذكاء الاصطناعي

الفرق الرئيسي بين أنظمة الذكاء الاصطناعي والبرمجيات التقليدية يكمن في القابلية للتنبؤ. مخرجات البرمجيات القياسية يمكن أن تكون غير قابلة للتنبؤ - في الواقع أحياناً هذا هو سبب كتابة البرمجيات، لتعطينا نتائج لم نكن نستطيع التنبؤ بها. لكن البرمجيات التقليدية نادراً ما تفعل شيئاً لم تُبرمج للقيام به - نطاقها وسلوكها عموماً كما هو مصمم. برنامج الشطرنج من الدرجة الأولى قد يقوم بحركات لا يستطيع أي إنسان التنبؤ بها (وإلا لاستطاعوا هزيمة ذلك البرنامج!) لكنه عموماً لن يفعل أي شيء سوى لعب الشطرنج.

مثل البرمجيات التقليدية، الذكاء الاصطناعي الضيق له نطاق وسلوك قابلان للتنبؤ لكن يمكن أن تكون له نتائج غير قابلة للتنبؤ. هذه في الواقع طريقة أخرى لتعريف الذكاء الاصطناعي الضيق: كذكاء اصطناعي يشبه البرمجيات التقليدية في قابليته للتنبؤ ونطاق عمله.

الذكاء الاصطناعي متعدد الأغراض مختلف: نطاقه (المجالات التي يطبق عليها)، وسلوكه (أنواع الأشياء التي يفعلها)، ونتائجه (مخرجاته الفعلية) يمكن أن تكون جميعها غير قابلة للتنبؤ.[^10] GPT-4 دُرِّب فقط لتوليد النصوص بدقة، لكنه طوَّر قدرات عديدة لم يتنبأ بها مدربوه أو يقصدوها. عدم القابلية للتنبؤ هذا ينبع من تعقيد التدريب: لأن بيانات التدريب تحتوي على مخرجات من مهام مختلفة عديدة، يجب على الذكاء الاصطناعي فعلياً تعلم أداء هذه المهام للتنبؤ بشكل جيد.

عدم القابلية للتنبؤ هذا في أنظمة الذكاء الاصطناعي العام أساسي تماماً. رغم أنه من الممكن من حيث المبدأ بناء أنظمة ذكاء اصطناعي بعناية لها حدود مضمونة على سلوكها (كما هو مذكور لاحقاً في المقال)، إلا أن الطريقة التي تُنشأ بها أنظمة الذكاء الاصطناعي الآن تجعلها غير قابلة للتنبؤ في الممارسة وحتى من حيث المبدأ.

### الذكاء الاصطناعي السلبي، والوكلاء، والأنظمة المستقلة، والمواءمة

عدم القابلية للتنبؤ هذا يصبح مهماً بشكل خاص عندما نأخذ بالاعتبار كيفية نشر أنظمة الذكاء الاصطناعي واستخدامها فعلياً لتحقيق أهداف مختلفة.

العديد من أنظمة الذكاء الاصطناعي سلبية نسبياً بمعنى أنها تقدم المعلومات بشكل أساسي، والمستخدم يتخذ الإجراءات. أخرى، تُسمى عادة *الوكلاء*، تتخذ إجراءات بنفسها، بمستويات متفاوتة من مشاركة المستخدم. تلك التي تتخذ إجراءات بمدخلات أو إشراف خارجي أقل نسبياً قد تُوصف بأنها أكثر *استقلالية*. هذا يشكل طيفاً من ناحية استقلالية العمل، من الأدوات السلبية إلى الوكلاء المستقلين.[^11]

بالنسبة لأهداف أنظمة الذكاء الاصطناعي، قد تكون مرتبطة مباشرة بهدف تدريبها (مثلاً هدف "الفوز" لنظام لعب الغو هو أيضاً ما دُرِّب للقيام به صراحة). أو قد لا تكون كذلك: هدف تدريب ChatGPT جزئياً هو التنبؤ بالنص، وجزئياً أن يكون مساعداً مفيداً. لكن عند القيام بمهمة معينة، يزوده المستخدم بالهدف. قد تُنشأ الأهداف أيضاً بواسطة نظام الذكاء الاصطناعي نفسه، مرتبطة فقط بشكل غير مباشر جداً بهدف تدريبه.[^12]

الأهداف مرتبطة بقوة بمسألة "المواءمة"، أي مسألة ما إذا كانت أنظمة الذكاء الاصطناعي ستفعل *ما نريدها أن تفعله*. هذا السؤال البسيط يخفي مستوى هائلاً من التعقيد.[^13] في الوقت الحالي، لاحظ أن "نحن" في هذه الجملة قد يشير إلى أشخاص ومجموعات مختلفة، مما يؤدي إلى أنواع مختلفة من المواءمة. مثلاً، قد يكون الذكاء الاصطناعي *مطيعاً* بدرجة عالية (أو ["مخلصاً"](https://arxiv.org/abs/2003.11157)) لمستخدمه - هنا "نحن" تعني "كل واحد منا." أو قد يكون أكثر *استقلالاً*، مدفوعاً أساساً بأهدافه وقيوده الخاصة، لكن لا يزال يتصرف بشكل عام لصالح رفاهية الإنسان - "نحن" تعني حينها "الإنسانية" أو "المجتمع." في المنتصف يوجد طيف حيث سيكون الذكاء الاصطناعي مطيعاً إلى حد كبير، لكنه قد يرفض اتخاذ إجراءات تضر بالآخرين أو المجتمع، أو تنتهك القانون، إلخ.

هذان المحوران - مستوى الاستقلالية ونوع المواءمة - ليسا مستقلين تماماً. مثلاً، نظام سلبي مستقل، رغم أنه ليس متناقضاً ذاتياً تماماً، هو مفهوم متوتر، كما هو الحال مع الوكيل المستقل المطيع.[^14] هناك معنى واضح لكون الاستقلالية والسيادة تميلان للارتباط. بروح مشابهة، تميل القابلية للتنبؤ لأن تكون أعلى في أنظمة الذكاء الاصطناعي "السلبية" و"المطيعة"، بينما الأنظمة المستقلة أو المستقلة ستميل لأن تكون أكثر عدم قابلية للتنبؤ. كل هذا سيكون أساسياً لفهم تداعيات الذكاء الاصطناعي العام والذكاء الفائق المحتملين.

إنشاء ذكاء اصطناعي متوائم حقاً، من أي نوع، يتطلب حل ثلاثة تحديات منفصلة:

1. فهم ما "نريده" - وهو أمر معقد سواء كان "نحن" يعني شخصاً أو منظمة محددة (الإخلاص) أو الإنسانية بشكل عام (الاستقلالية)؛
2. بناء أنظمة تتصرف بانتظام وفقاً لتلك الرغبات - وهو في الأساس إنشاء سلوك إيجابي متسق؛
3. الأهم من ذلك، جعل الأنظمة "تهتم" حقاً بتلك الرغبات بدلاً من مجرد التصرف كما لو كانت تفعل.

التمييز بين السلوك الموثوق والاهتمام الحقيقي أساسي. تماماً كما قد يتبع الموظف البشري الأوامر بشكل مثالي مع افتقاره لأي التزام حقيقي برسالة المنظمة، قد يتصرف نظام الذكاء الاصطناعي بشكل متوائم دون أن يُقدر حقاً تفضيلات البشر. يمكننا تدريب أنظمة الذكاء الاصطناعي لتقول وتفعل أشياء من خلال التغذية الراجعة، ويمكنها تعلم التفكير في ما يريده البشر. لكن جعلها *تُقدر حقاً* تفضيلات البشر تحدٍ أعمق بكثير.[^15]

الصعوبات العميقة في حل تحديات المواءمة هذه، وآثارها على مخاطر الذكاء الاصطناعي، ستُستكشف أكثر أدناه. في الوقت الحالي، افهم أن المواءمة ليست مجرد خاصية تقنية نضعها على أنظمة الذكاء الاصطناعي، بل جانب أساسي من هيكلها الذي يشكل علاقتها مع الإنسانية.


[^5]: للحصول على مقدمة لطيفة لكن تقنية للتعلم الآلي والذكاء الاصطناعي، خاصة نماذج اللغة، انظر [هذا الموقع.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) لمقدمة حديثة أخرى حول مخاطر انقراض الذكاء الاصطناعي، انظر [هذا المقال.](https://www.thecompendium.ai/) للحصول على تحليل علمي شامل وموثوق لحالة سلامة الذكاء الاصطناعي، انظر [تقرير السلامة الدولي للذكاء الاصطناعي](https://arxiv.org/abs/2501.17805) الحديث.

[^6]: التدريب يحدث عادة بالبحث عن أقصى محلي للنتيجة في فضاء عالي الأبعاد محدد بأوزان النموذج. من خلال فحص كيفية تغير النتيجة عند تعديل الأوزان، تحدد خوارزمية التدريب أي التعديلات تحسن النتيجة أكثر، وتحرك الأوزان في ذلك الاتجاه.

[^7]: مثلاً، في مشكلة التعرف على الصور، ستخرج الشبكة العصبية احتماليات لتسميات الصورة. ستكون النتيجة مرتبطة بالاحتمالية التي يعطيها الذكاء الاصطناعي للجواب الصحيح. إجراء التدريب سيعدل الأوزان حينها بحيث في المرة القادمة، سيخرج الذكاء الاصطناعي احتمالية أعلى للتسمية الصحيحة لتلك الصورة. هذا يتكرر عدداً هائلاً من المرات. نفس الإجراء الأساسي يُستخدم في تدريب كل الشبكات العصبية الحديثة تأساساً، وإن كان بآلية تسجيل أكثر تعقيداً.

[^8]: معظم النماذج متعددة الوسائط تستخدم هندسة "المحول" لمعالجة وتوليد أنواع متعددة من البيانات (نص، صور، صوت). يمكن تفكيك كل هذه إلى، ثم التعامل معها على قدم المساواة، كأنواع مختلفة من "الرموز المميزة." النماذج متعددة الوسائط تُدرَّب أولاً للتنبؤ بدقة بالرموز المميزة ضمن مجموعات بيانات ضخمة، ثم تُحسَّن من خلال التعلم المعزز لتعزيز القدرات وتشكيل السلوكيات.

[^9]: أن نماذج اللغة مدربة للقيام بشيء واحد - التنبؤ بالكلمات - جعل البعض يسميها ذكاءً اصطناعياً ضيقاً. لكن هذا مضلل: لأن التنبؤ بالنص بشكل جيد يتطلب قدرات مختلفة كثيرة، مهمة التدريب هذه تؤدي إلى نظام عام بشكل مفاجئ. لاحظ أيضاً أن هذه الأنظمة مدربة بشكل واسع بالتعلم المعزز، مما يمثل فعلياً آلاف الأشخاص الذين يعطون النموذج إشارة مكافأة عندما يقوم بعمل جيد في أي من الأشياء الكثيرة التي يفعلها. ثم يرث عمومية كبيرة من الأشخاص الذين يعطون هذه التغذية الراجعة.

[^10]: هناك طرق متعددة يكون بها الذكاء الاصطناعي غير قابل للتنبؤ. إحداها أنه في الحالة العامة لا يمكن للمرء التنبؤ بما ستفعله خوارزمية دون تشغيلها فعلياً؛ هناك [نظريات](https://arxiv.org/abs/1310.3225) بهذا الصدد. هذا يمكن أن يكون صحيحاً فقط لأن مخرجات الخوارزميات يمكن أن تكون معقدة. لكنه واضح وذو صلة بشكل خاص في الحالة (كما في الشطرنج أو الغو) حيث التنبؤ سيعني قدرة (هزيمة الذكاء الاصطناعي) لا يملكها المتنبئ المحتمل. ثانياً، نظام ذكاء اصطناعي معين لن ينتج دائماً نفس المخرجات حتى لو أعطي نفس المدخلات - مخرجاته تحتوي على عشوائية؛ هذا يقترن أيضاً بعدم القابلية للتنبؤ الخوارزمي. ثالثاً، قدرات غير متوقعة وناشئة يمكن أن تنشأ من التدريب، مما يعني حتى *أنواع* الأشياء التي يمكن لنظام الذكاء الاصطناعي فعلها وسيفعلها غير قابلة للتنبؤ؛ هذا النوع الأخير مهم بشكل خاص لاعتبارات السلامة.

[^11]: انظر [هنا](https://arxiv.org/abs/2502.02649) لمراجعة متعمقة لما يُقصد بـ "الوكيل المستقل" (مع حجج أخلاقية ضد بنائها).

[^12]: قد تسمع أحياناً "الذكاء الاصطناعي لا يمكن أن تكون له أهدافه الخاصة." هذا هراء مطلق. من السهل توليد أمثلة حيث الذكاء الاصطناعي له أو يطور أهدافاً لم تُعط له أبداً ولا يعرفها إلا هو. لا ترى هذا كثيراً في النماذج متعددة الوسائط الشائعة الحالية لأنه يُدرَّب خارجاً منها؛ يمكن بسهولة تدريبه داخلها.

[^13]: هناك أدبيات كبيرة. حول المشكلة العامة انظر كتاب كريستيان [*مشكلة المواءمة*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821)، وكتاب راسل [*متوافق مع الإنسان*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616). على الجانب التقني أكثر انظر مثلاً [هذا المقال](https://arxiv.org/abs/2209.00626).

[^14]: سنرى لاحقاً أنه رغم أن هذه الأنظمة تخالف الاتجاه، إلا أن ذلك يجعلها في الواقع مثيرة جداً للاهتمام ومفيدة.

[^15]: هذا لا يعني أننا نتطلب عواطف أو إحساساً. بل أنه من الصعب للغاية من خارج النظام معرفة ما هي أهدافه وتفضيلاته وقيمه الداخلية. "الحقيقي" هنا يعني أن لدينا سبباً قوياً كافياً للاعتماد عليه بحيث في حالة الأنظمة الحرجة يمكننا المراهنة بحياتنا عليه.

## الفصل 3 - الجوانب الأساسية لكيفية صناعة أنظمة الذكاء الاصطناعي العامة الحديثة

معظم أنظمة الذكاء الاصطناعي الأكثر تطوراً في العالم تُصنع باستخدام طرق متشابهة بشكل مدهش. إليكم الأساسيات.

لفهم الإنسان حقاً، تحتاج إلى معرفة شيء عن علم الأحياء والتطور وتربية الأطفال والمزيد؛ ولفهم الذكاء الاصطناعي، تحتاج أيضاً إلى معرفة كيفية صناعته. خلال السنوات الخمس الماضية، تطورت أنظمة الذكاء الاصطناعي بشكل هائل في كلٍ من القدرة والتعقيد. وكان العامل الرئيسي المُمكِّن لذلك هو توفر كميات كبيرة جداً من الحوسبة (أو "القوة الحاسوبية" كما تُستخدم في مجال الذكاء الاصطناعي).

الأرقام مذهلة. حوالي 10<sup>25</sup> -10<sup>26</sup> "عملية فاصلة عائمة" (FLOP)[^16] تُستخدم في تدريب نماذج مثل سلسلة GPT وClaude وGemini وغيرها.[^17] (للمقارنة، إذا عمل كل إنسان على الأرض دون توقف بمعدل حساب واحد كل خمس ثوان، فسيحتاج الأمر إلى حوالي مليار سنة لإنجاز هذا.) هذه الكمية الهائلة من الحوسبة تمكّن من تدريب نماذج تحتوي على تريليونات من المعاملات على تيرابايتات من البيانات - جزء كبير من كل النصوص عالية الجودة التي كُتبت على الإطلاق إلى جانب مكتبات ضخمة من الأصوات والصور والفيديوهات. وبتكملة هذا التدريب مع تدريب إضافي واسع يعزز التفضيلات البشرية والأداء الجيد في المهام، تُظهر النماذج المدربة بهذه الطريقة أداءً منافساً للبشر عبر نطاق واسع من المهام الفكرية الأساسية، بما في ذلك التفكير وحل المشكلات.

نعرف أيضاً (بشكل تقريبي جداً) مقدار سرعة الحوسبة، بالعمليات في الثانية الواحدة، الكافية لكي تضاهي سرعة *الاستنتاج*[^18] لمثل هذا النظام *سرعة* المعالجة النصية البشرية. إنها حوالي 10<sup>15</sup> -10<sup>16</sup> FLOP في الثانية.[^19]

رغم قوتها، فإن هذه النماذج محدودة بطبيعتها بطرق أساسية، تماماً كما سيكون الإنسان الفرد محدوداً لو أُجبر على مجرد إخراج نص بمعدل ثابت من الكلمات في الدقيقة، دون توقف للتفكير أو استخدام أي أدوات إضافية. الأنظمة الأحدث للذكاء الاصطناعي تتعامل مع هذه القيود من خلال عملية وبنية معمارية أكثر تعقيداً تجمع بين عدة عناصر أساسية:

- شبكة عصبية واحدة أو أكثر، مع نموذج واحد يوفر القدرة المعرفية الأساسية، وحتى عدة أخرى تؤدي مهام أخرى أكثر تخصصاً؛
- *أدوات* مُوفرة للنموذج وقابلة للاستخدام من قبله - على سبيل المثال القدرة على البحث في الويب، أو إنشاء أو تحرير الوثائق، أو تنفيذ البرامج، إلخ.
- *سقالة* تربط بين مدخلات ومخرجات الشبكات العصبية. قد تسمح سقالة بسيطة جداً لـ"نسختين" من نموذج ذكاء اصطناعي بالتحدث مع بعضهما البعض، أو لواحدة بفحص عمل الأخرى.[^20]
- *سلسلة التفكير* وتقنيات التوجيه ذات الصلة تقوم بشيء مشابه، حيث تجعل النموذج يولد على سبيل المثال عدة مناهج لمشكلة، ثم يعالج تلك المناهج للوصول لإجابة مجمعة.
- *إعادة تدريب* النماذج لتحسين استخدام الأدوات والسقالة وسلسلة التفكير.

نظراً لأن هذه الإضافات يمكن أن تكون قوية جداً (وتشمل أنظمة ذكاء اصطناعي بحد ذاتها)، فإن هذه الأنظمة المركبة يمكن أن تكون متطورة جداً وتعزز قدرات الذكاء الاصطناعي بشكل كبير.[^21] ومؤخراً، طُورت تقنيات في السقالة وخاصة توجيه سلسلة التفكير (وإدماج النتائج في إعادة تدريب النماذج لاستخدام هذه بشكل أفضل) واستُخدمت في [o1](https://openai.com/o1/) و[o3](https://openai.com/index/openai-o3-mini/) و[DeepSeek R1](https://api-docs.deepseek.com/news/news250120) لإجراء مرات عديدة من الاستنتاج استجابةً لاستعلام معين.[^22] هذا يسمح فعلياً للنموذج بـ"التفكير في" استجابته ويعزز بشكل كبير قدرة هذه النماذج على القيام بتفكير عالي المستوى في مهام العلوم والرياضيات والبرمجة.[^23]

بالنسبة لبنية معمارية معينة للذكاء الاصطناعي، يمكن [ترجمة الزيادات في حوسبة التدريب بشكل موثوق](https://arxiv.org/abs/2405.10938) إلى تحسينات في مجموعة من المقاييس المحددة بوضوح. بالنسبة للقدرات العامة المحددة بشكل أقل وضوحاً (مثل تلك المناقشة أدناه)، الترجمة أقل وضوحاً وتنبؤية، لكن من المؤكد تماماً أن النماذج الأكبر مع المزيد من حوسبة التدريب ستحصل على قدرات جديدة وأفضل، حتى لو كان من الصعب التنبؤ بما ستكون عليه تلك القدرات.

وبالمثل، الأنظمة المركبة وخاصة التطورات في "سلسلة التفكير" (وتدريب النماذج التي تعمل جيداً معها) فتحت المجال للتوسع في حوسبة *الاستنتاج*: بالنسبة لنموذج أساسي مدرب معين، تزيد قدرات نظام الذكاء الاصطناعي -على الأقل بعضها- كلما طُبقت حوسبة أكثر تسمح لها بـ"التفكير بجدية وطويلاً" في المشاكل المعقدة. هذا يأتي بتكلفة باهظة في سرعة الحوسبة، حيث يتطلب مئات أو آلاف أكثر من FLOP/s لمضاهاة الأداء البشري.[^24]

رغم كونها جزءاً فقط مما يقود إلى التقدم السريع في الذكاء الاصطناعي،[^25] سيثبت دور الحوسبة وإمكانية الأنظمة المركبة أنه حاسم لكل من منع الذكاء الاصطناعي العام غير القابل للسيطرة وتطوير بدائل أكثر أماناً.


[^16]: 10<sup>27</sup> يعني 1 متبوعاً بـ 25 صفراً، أو عشرة تريليون تريليون. FLOP هي مجرد عملية جمع أو ضرب حسابية لأرقام بدقة معينة. لاحظ أن أداء عتاد الذكاء الاصطناعي يمكن أن يتفاوت بعامل عشرة أو أكثر اعتماداً على دقة العمليات الحسابية وبنية الحاسوب المعمارية. عد عمليات البوابات المنطقية (ANDs، ORs، AND NOTs) سيكون أساسياً لكن هذه غير متاحة أو مقيسة عادة؛ لأغراض الوقت الحالي من المفيد التوحيد على عمليات 16-بت (FP16)، رغم أنه يجب إنشاء عوامل تحويل مناسبة.

[^17]: مجموعة من التقديرات والبيانات المؤكدة متاحة من [Epoch AI](https://epochai.org/data/large-scale-ai-models) وتشير إلى حوالي 2×10<sup>25</sup> 16-بت FLOP لـ GPT-4؛ هذا يطابق تقريباً [الأرقام التي تسربت](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) لـ GPT-4. التقديرات لنماذج أخرى من منتصف 2024 كلها ضمن عامل قليل من GPT-4.

[^18]: الاستنتاج هو ببساطة عملية توليد مخرج من شبكة عصبية. يمكن اعتبار التدريب تعاقباً لعمليات استنتاج كثيرة وتعديلات لمعاملات النموذج.

[^19]: بالنسبة لإنتاج النص، تطلب GPT-4 الأصلي 560 TFLOP لكل رمز مولد. حوالي 7 رموز/ث مطلوبة لمواكبة التفكير البشري، لذا هذا يعطي ≈3×10<sup>15</sup> FLOP/s. لكن التحسينات خفضت هذا؛ [هذا الكتيب من NVIDIA](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/) على سبيل المثال يشير إلى أقل من 3×10<sup>14</sup> FLOP/s لنموذج Llama 405B بأداء مماثل.

[^20]: كمثال أكثر تعقيداً قليلاً، قد يولد نظام ذكاء اصطناعي أولاً عدة حلول محتملة لمسألة رياضية، ثم يستخدم نسخة أخرى لفحص كل حل، وأخيراً يستخدم ثالثة لتجميع النتائج في شرح واضح. هذا يسمح بحل مشاكل أكثر شمولية وموثوقية من مرة واحدة.

[^21]: انظر على سبيل المثال تفاصيل ["Operator" من OpenAI](https://openai.com/index/introducing-operator/)، [قدرات أدوات Claude](https://docs.anthropic.com/en/docs/build-with-claude/computer-use)، و[AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). [Deep Research](https://openai.com/index/introducing-deep-research/) من OpenAI ربما لديه بنية معمارية متطورة جداً لكن التفاصيل غير متاحة.

[^22]: Deepseek R1 يعتمد على تدريب وتوجيه النموذج بشكل تكراري بحيث ينشئ النموذج المدرب النهائي تفكير سلسلة مكثف. التفاصيل المعمارية غير متاحة لـ o1 أو o3، لكن Deepseek كشف أنه لا توجد "صلصة سرية" خاصة مطلوبة لفتح توسع القدرة مع الاستنتاج. لكن رغم تلقيه الكثير من التغطية الصحفية كمقلب لـ"الوضع الراهن" في الذكاء الاصطناعي، فإنه لا يؤثر على الادعاءات الأساسية لهذا المقال.

[^23]: هذه النماذج تتفوق بشكل كبير على النماذج القياسية في مقاييس التفكير. على سبيل المثال، في GPQA Diamond Benchmark—اختبار صارم لأسئلة علمية بمستوى الدكتوراه—[سجل](https://openai.com/index/learning-to-reason-with-llms/) GPT-4o 56%، بينما حقق o1 و o3 78% و 88% على التوالي، متجاوزين بكثير متوسط نتيجة 70% للخبراء البشر.

[^24]: O3 من OpenAI استهلك ربما ∼10<sup>21</sup> -10<sup>22</sup> FLOP [لإكمال كل سؤال من أسئلة تحدي ARC-AGI](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai)، والتي يمكن للبشر المختصين فعلها في (لنقل) 10-100 ثانية، مما يعطي رقماً أشبه بـ ∼10<sup>20</sup> FLOP/s.

[^25]: بينما الحوسبة مقياس أساسي لقدرة نظام الذكاء الاصطناعي، فإنها تتفاعل مع كل من جودة البيانات والتحسينات الخوارزمية. البيانات أو الخوارزميات الأفضل يمكن أن تقلل المتطلبات الحاسوبية، بينما المزيد من الحوسبة يمكن أحياناً أن تعوض عن بيانات أو خوارزميات أضعف.

## الفصل الرابع - ما هو الذكاء الاصطناعي العام والذكاء الفائق؟

ما الذي تتسابق أكبر شركات التكنولوجيا في العالم لبنائه خلف أبواب مغلقة؟

لقد كان مصطلح "الذكاء الاصطناعي العام" موجوداً منذ فترة للإشارة إلى الذكاء الاصطناعي متعدد الأغراض "بمستوى بشري". لم يكن قط مصطلحاً محدداً بوضوح، ولكنه في السنوات الأخيرة أصبح بشكل متناقض أقل تحديداً رغم أهميته المتزايدة، حيث يتناقش الخبراء في الوقت ذاته حول ما إذا كان الذكاء الاصطناعي العام يبعد عقوداً أم أنه تحقق بالفعل، بينما تتسابق شركات بقيمة تريليونات الدولارات "نحو الذكاء الاصطناعي العام." (لقد سُلط الضوء على غموض "الذكاء الاصطناعي العام" مؤخراً عندما [كشفت وثائق مسربة مزعومة](https://gizmodo.com/leaked-documents-show-openai-has-a-very-clear-definition-of-agi-2000543339) أن الذكاء الاصطناعي العام في عقد OpenAI مع Microsoft عُرّف بأنه الذكاء الاصطناعي الذي يحقق 100 مليار دولار من الإيرادات لـ OpenAI - وهو تعريف أكثر نفعية من أكاديمي.)

هناك مشكلتان أساسيتان مع فكرة الذكاء الاصطناعي بـ"مستوى الذكاء البشري". أولاً، البشر مختلفون جداً، جداً في قدرتهم على أداء أي نوع معين من الأعمال المعرفية، لذا لا يوجد "مستوى بشري" موحد. ثانياً، الذكاء متعدد الأبعاد بشدة؛ رغم وجود ارتباطات، إلا أنها غير مثالية وقد تكون مختلفة تماماً في الذكاء الاصطناعي. لذا حتى لو استطعنا تعريف "المستوى البشري" للعديد من القدرات، فإن الذكاء الاصطناعي بالتأكيد سيتجاوزه بكثير في بعضها بينما يكون أقل منه في أخرى.[^26]

رغم ذلك، من المهم جداً أن نكون قادرين على مناقشة أنواع ومستويات وعتبات قدرة الذكاء الاصطناعي. النهج المتبع هنا هو التأكيد على أن الذكاء الاصطناعي متعدد الأغراض موجود بالفعل، وأنه يأتي - وسيأتي - بمستويات قدرة مختلفة يكون من المناسب ربط المصطلحات بها حتى لو كانت مبسطة، لأنها تتوافق مع عتبات حاسمة من ناحية تأثيرات الذكاء الاصطناعي على المجتمع والإنسانية.

سنعرّف الذكاء الاصطناعي العام "الكامل" بأنه مرادف لـ"الذكاء الاصطناعي متعدد الأغراض فائق البشرية" أي نظام ذكاء اصطناعي قادر على أداء جميع المهام المعرفية البشرية تقريباً في مستوى أفضل الخبراء البشريين أو أعلى، بالإضافة إلى اكتساب مهارات جديدة ونقل القدرة إلى مجالات جديدة. هذا يتماشى مع كيفية تعريف "الذكاء الاصطناعي العام" غالباً في الأدبيات الحديثة. من المهم ملاحظة أن هذه عتبة *عالية جداً*. لا يمتلك أي إنسان هذا النوع من الذكاء؛ بل هو نوع الذكاء الذي تمتلكه مجموعات كبيرة من أفضل الخبراء البشريين لو جُمعت معاً. يمكننا تسمية "الذكاء الفائق" قدرة تتجاوز ذلك، وتعريف مستويات أكثر محدودية من القدرة بـ"الذكاء الاصطناعي متعدد الأغراض المنافس للبشر" و"المنافس للخبراء"، والذي يؤدي مجموعة واسعة من المهام في المستوى المهني النموذجي، أو مستوى الخبير البشري.[^27]

هذه المصطلحات وبعضها الآخر مجمعة في [الجدول](https://keepthefuturehuman.ai/essay/docs/#tab:terms) أدناه. للحصول على إحساس أكثر تحديداً بما يمكن لمختلف درجات الأنظمة فعله، من المفيد أخذ التعريفات بجدية والنظر فيما تعنيه.

| نوع الذكاء الاصطناعي | المصطلحات ذات الصلة | التعريف | الأمثلة |
| ------------------------- | ------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
| الذكاء الاصطناعي الضيق | الذكاء الاصطناعي الضعيف | ذكاء اصطناعي مُدرب لمهمة معينة أو مجموعة مهام محددة. يتفوق في مجاله لكنه يفتقر للذكاء العام أو القدرة على نقل التعلم. | برامج التعرف على الصور؛ المساعدات الصوتية (مثل Siri وAlexa)؛ برامج لعب الشطرنج؛ AlphaFold من DeepMind |
| الذكاء الاصطناعي الأداتي | الذكاء المُعزز، مساعد الذكاء الاصطناعي | (سيُناقش لاحقاً في المقال.) نظام ذكاء اصطناعي يعزز القدرات البشرية. يجمع بين الذكاء الاصطناعي متعدد الأغراض المنافس للبشر، والذكاء الاصطناعي الضيق، والسيطرة المضمونة، مع إعطاء الأولوية للأمان والتعاون. يدعم اتخاذ القرارات البشرية. | مساعدو البرمجة المتقدمون؛ أدوات البحث المدعومة بالذكاء الاصطناعي؛ منصات تحليل البيانات المتطورة. وكلاء أكفاء لكن ضيقون وقابلون للسيطرة |
| الذكاء الاصطناعي متعدد الأغراض | | نظام ذكاء اصطناعي قابل للتكيف مع مهام مختلفة، بما في ذلك تلك التي لم يُدرب عليها تحديداً. | النماذج اللغوية (مثل GPT-4 وClaude)؛ نماذج الذكاء الاصطناعي متعددة الوسائط؛ MuZero من DeepMind |
| الذكاء الاصطناعي متعدد الأغراض المنافس للبشر | الذكاء الاصطناعي العام [ضعيف] | ذكاء اصطناعي متعدد الأغراض يؤدي المهام في المستوى البشري المتوسط، أحياناً يتفوق عليه. | النماذج اللغوية المتقدمة (مثل O1 وClaude 3.5)؛ بعض أنظمة الذكاء الاصطناعي متعددة الوسائط |
| الذكاء الاصطناعي متعدد الأغراض المنافس للخبراء | الذكاء الاصطناعي العام [جزئي] | ذكاء اصطناعي متعدد الأغراض يؤدي معظم المهام في مستوى الخبير البشري، مع استقلالية كبيرة لكن محدودة | ربما O3 المزود بالأدوات والسقالة، على الأقل للرياضيات والبرمجة وبعض العلوم الصعبة |
| الذكاء الاصطناعي العام [كامل] | الذكاء الاصطناعي متعدد الأغراض فائق البشرية | نظام ذكاء اصطناعي قادر على أداء جميع المهام الفكرية البشرية تقريباً بشكل مستقل في مستوى الخبير أو أعلى، مع تعلم فعال ونقل للمعرفة. | [لا توجد أمثلة حالية - نظري] |
| الذكاء الفائق | الذكاء الاصطناعي متعدد الأغراض عالي التفوق على البشر | نظام ذكاء اصطناعي يتفوق بكثير على القدرات البشرية في جميع المجالات، متفوقاً على الخبرة البشرية الجماعية. هذا التفوق قد يكون في العمومية أو الجودة أو السرعة و/أو مقاييس أخرى. | [لا توجد أمثلة حالية - نظري] |

نحن نختبر بالفعل كيف يبدو امتلاك أنظمة ذكاء اصطناعي متعددة الأغراض حتى المستوى المنافس للبشر. لقد اندمج هذا بسلاسة نسبية، حيث يختبره معظم المستخدمين كامتلاك عامل مؤقت ذكي لكن محدود يجعلهم أكثر إنتاجية مع تأثير متباين على جودة عملهم.[^28]

ما سيكون مختلفاً حول الذكاء الاصطناعي متعدد الأغراض المنافس للخبراء هو أنه لن يحمل القيود الأساسية للذكاء الاصطناعي الحالي، وسيفعل الأشياء التي يفعلها الخبراء: العمل الاقتصادي المستقل ذو القيمة، إبداع المعرفة الحقيقية، العمل التقني الذي يمكن الاعتماد عليه، بينما يرتكب أخطاءً غبية نادراً (رغم أنها ستحدث أحياناً).

فكرة الذكاء الاصطناعي العام الكامل أنه *يفعل حقاً* جميع الأشياء المعرفية التي يفعلها حتى أكثر البشر قدرة وفعالية، بشكل مستقل ودون حاجة لمساعدة أو إشراف. هذا يشمل التخطيط المتطور، تعلم مهارات جديدة، إدارة مشاريع معقدة، إلخ. يمكنه إجراء بحث أصيل متطور. يمكنه إدارة شركة. مهما كانت وظيفتك، إذا كانت تُؤدى بشكل أساسي بالكمبيوتر أو عبر الهاتف، *فيمكنه فعلها بمستوى جيد مثلك على الأقل.* وربما بشكل أسرع وأرخص بكثير. سنناقش بعض التداعيات أدناه، لكن التحدي لك الآن هو أن تأخذ هذا بجدية حقيقية. تخيل أفضل عشرة أشخاص تعرفهم أو تعرف عنهم من ناحية المعرفة والكفاءة - بما في ذلك المدراء التنفيذيين والعلماء والأساتذة وكبار المهندسين وعلماء النفس والقادة السياسيين والكتاب. اجمعهم جميعاً في شخص واحد، يتحدث أيضاً 100 لغة، وله ذاكرة هائلة، ويعمل بسرعة، ولا يتعب وحافز دائماً، ويعمل بأقل من الحد الأدنى للأجور.[^29] هذا إحساس بما سيكون عليه الذكاء الاصطناعي العام.

بالنسبة للذكاء الفائق، التخيل أصعب، لأن الفكرة أنه يمكنه أداء مآثر فكرية لا يستطيع أي إنسان أو حتى مجموعة من البشر فعلها - إنه بحكم التعريف غير قابل للتنبؤ من قبلنا. لكن يمكننا الحصول على إحساس. كحد أدنى، فكر في أنظمة ذكاء اصطناعي عام كثيرة، كل منها أكثر قدرة من حتى أفضل خبير بشري، تعمل بسرعة 100 ضعف السرعة البشرية، مع ذاكرة هائلة وقدرة تنسيق رائعة.[^30] ويتصاعد من هناك. التعامل مع الذكاء الفائق سيكون أقل شبهاً بالحديث مع عقل مختلف، وأكثر شبهاً بالتفاوض مع حضارة مختلفة (وأكثر تقدماً).

إذن، ما مدى قربنا *فعلاً* من الذكاء الاصطناعي العام والذكاء الفائق؟


[^26]: على سبيل المثال، الأنظمة الحالية للذكاء الاصطناعي تتفوق بكثير على القدرة البشرية في الحساب السريع أو مهام الذاكرة، بينما تقل عنها في التفكير المجرد وحل المشكلات الإبداعي.

[^27]: والأهم من ذلك، كمنافس سيكون لمثل هذا الذكاء الاصطناعي عدة مزايا هيكلية رئيسية تشمل: لن يتعب أو يحتاج احتياجات فردية أخرى مثل البشر؛ يمكن تشغيله بسرعات أعلى فقط بزيادة القوة الحاسوبية؛ يمكن نسخه مع أي خبرة أو معرفة يكتسبها - ويمكن حتى "دمج" المعرفة المكتسبة للشبكات العصبية لنقل مجموعات مهارات كاملة بينها؛ يمكنه التواصل بسرعة الآلة؛ ويمكنه تعديل أو تحسين نفسه بطرق أكثر أهمية وسرعة أعلى من أي إنسان.

[^28]: إذا لم تقض وقتاً في استخدام أنظمة الذكاء الاصطناعي الحالية الأفضل في فئتها، أنصح بذلك: فهي مفيدة وقادرة حقاً، ومن المهم أيضاً معايرة التأثير الذي سيحدثه الذكاء الاصطناعي كلما أصبح أقوى.

[^29]: فكر في مستشفى بحثي كبير: الذكاء الاصطناعي العام المُحقق بالكامل يمكنه في الوقت نفسه تحليل جميع بيانات المرضى الواردة، ومواكبة كل ورقة طبية جديدة، واقتراح التشخيصات، وتصميم خطط العلاج، وإدارة التجارب السريرية، وتنسيق جدولة الموظفين - كل ذلك بينما يعمل في مستوى يضاهي أو يتفوق على أفضل متخصصي المستشفى في كل مجال. ويمكنه فعل هذا لعدة مستشفيات في الوقت نفسه، بجزء من التكلفة الحالية. لسوء الحظ، يجب أن تفكر أيضاً في عصابة إجرام منظمة: الذكاء الاصطناعي العام المُحقق بالكامل يمكنه في الوقت نفسه اختراق وانتحال شخصية والتجسس على وابتزاز آلاف الضحايا، ومواكبة إنفاذ القانون (الذي يتأتمت ببطء أكثر)، وتصميم مخططات جديدة لكسب المال، وتنسيق جدولة الموظفين - إذا كان هناك أي موظفين.

[^30]: في [مقاله](https://darioamodei.com/machines-of-loving-grace)، استحضر داريو أموداي، المدير التنفيذي لشركة Anthropic، "بلداً من [مليون] عبقري".

## الفصل الخامس - على العتبة

يبدو الطريق من أنظمة الذكاء الاصطناعي الحالية إلى الذكاء الاصطناعي العام الكامل قصيراً ومتوقعاً بشكل صادم.

شهدت السنوات العشر الماضية تقدماً هائلاً في مجال الذكاء الاصطناعي مدفوعاً بموارد [حاسوبية](https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year) وبشرية و[مالية](https://arxiv.org/abs/2405.21015) ضخمة. العديد من تطبيقات الذكاء الاصطناعي المتخصصة أصبحت تتفوق على البشر في مهامها المحددة، وهي بالتأكيد أسرع وأرخص بكثير.[^31] كما توجد أيضاً وكلاء متخصصون يتفوقون على البشر ويمكنهم هزيمة جميع الأشخاص في ألعاب متخصصة مثل [الجو](https://www.nature.com/articles/nature16961) و[الشطرنج](https://arxiv.org/abs/1712.01815) و[البوكر](https://www.deepstack.ai/)، بالإضافة إلى [وكلاء أكثر عمومية](https://deepmind.google/discover/blog/a-generalist-agent/) يمكنهم التخطيط وتنفيذ الإجراءات في بيئات محاكاة مبسطة بنفس فعالية البشر.

والأهم من ذلك، ظهرت أنظمة الذكاء الاصطناعي العامة الحالية من OpenAI/Microsoft وGoogle/Deepmind وAnthropic/Amazon وFacebook/Meta وX.ai/Tesla وغيرها[^32] منذ أوائل عام 2023 وزادت قدراتها باستمرار (وإن كان بشكل متفاوت) منذ ذلك الحين. تم إنشاء جميع هذه الأنظمة عبر التنبؤ بالرموز على مجموعات بيانات نصية ومتعددة الوسائط ضخمة، مقترنة بتعزيز واسع النطاق من البشر وأنظمة الذكاء الاصطناعي الأخرى. كما تتضمن بعضها أنظمة أدوات وسقالة واسعة النطاق.

### نقاط القوة والضعف في الأنظمة العامة الحالية

تؤدي هذه الأنظمة بشكل جيد عبر نطاق متزايد الاتساع من الاختبارات المصممة لقياس الذكاء والخبرة، بتقدم فاجأ حتى الخبراء في هذا المجال:

- عند إطلاقه لأول مرة، [ضاهى أو تفوق GPT-4](https://arxiv.org/abs/2303.08774) على الأداء البشري النموذجي في الاختبارات الأكاديمية المعيارية بما في ذلك SATs وGRE واختبارات القبول واختبارات نقابة المحامين. النماذج الأحدث تؤدي على الأرجح بشكل أفضل بكثير، رغم أن النتائج غير متاحة للعامة.
- اختبار تورينغ - الذي طالما اعتُبر معياراً أساسياً للذكاء الاصطناعي "الحقيقي" - تجتازه الآن بانتظام النماذج اللغوية الحديثة في بعض أشكاله، سواء بشكل غير رسمي أو في [دراسات رسمية](https://arxiv.org/abs/2405.08007).[^33]
- في مؤشر MMLU الشامل الذي يغطي 57 موضوعاً أكاديمياً، [تحقق النماذج الحديثة درجات على مستوى خبراء المجال](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu) (∼90%)[^34]
- تقدمت الخبرة التقنية بشكل هائل: شهد مؤشر GPQA للفيزياء على مستوى الدراسات العليا [قفزة في الأداء](https://epoch.ai/data/ai-benchmarking-dashboard) من التخمين شبه العشوائي (GPT-4, 2022) إلى مستوى الخبراء (o1-preview, 2024).
- حتى الاختبارات المصممة خصيصاً لتكون مقاومة للذكاء الاصطناعي تتساقط: O3 من OpenAI [يُقال أنه](https://www.nextbigfuture.com/2024/12/openai-releases-o3-model-with-high-performance-and-high-cost.html) يحل مؤشر ARC-AGI لحل المشكلات المجردة على المستوى البشري، ويحقق أداء برمجة على مستوى الخبراء الأعلى، ويسجل 25% في مسائل "الرياضيات المتقدمة" من Epoch AI المصممة لتحدي علماء الرياضيات النخبة.[^35]
- الاتجاه واضح لدرجة أن مطور MMLU أنشأ الآن ["امتحان البشرية الأخير"](https://agi.safe.ai/) - اسم مشؤوم يعكس إمكانية أن يتفوق الذكاء الاصطناعي قريباً على الأداء البشري في أي اختبار ذي معنى. وقت كتابة هذا، هناك ادعاءات بأن أنظمة الذكاء الاصطناعي حققت 27% (وفقاً لـ [سام ألتمان](https://x.com/sama/status/1886220281565381078)) و35% (وفقاً لـ [هذه الورقة](https://arxiv.org/abs/2502.09955)) في هذا الامتحان بالغ الصعوبة. من غير المحتمل جداً أن يتمكن أي إنسان من فعل ذلك.

رغم هذه الأرقام المذهلة (وذكائها الواضح عند التفاعل معها)[^36] هناك أشياء كثيرة لا تستطيع هذه الشبكات العصبية (على الأقل النسخ المطروحة منها) فعلها. حالياً معظمها غير مجسد - يوجد فقط على الخوادم - ويعالج في أفضل الأحوال النصوص والصوت والصور الثابتة (وليس الفيديو). والأهم من ذلك، معظمها لا يستطيع تنفيذ أنشطة مخطط لها معقدة تتطلب دقة عالية.[^37] وهناك عدد من الخصائص الأخرى القوية في الإدراك البشري عالي المستوى والمنخفضة حالياً في أنظمة الذكاء الاصطناعي المطروحة.

يسرد الجدول التالي عدداً من هذه الخصائص، استناداً إلى أنظمة الذكاء الاصطناعي من منتصف 2024 مثل GPT-4o وClaude 3.5 Sonnet وGoogle Gemini 1.5.[^38] السؤال الأساسي حول سرعة تطور الذكاء الاصطناعي العام ليصبح أكثر قوة هو: إلى أي مدى سيؤدي مجرد فعل *المزيد من نفس الشيء* إلى نتائج، مقابل إضافة تقنيات إضافية ولكن *معروفة*، مقابل تطوير أو تنفيذ اتجاهات بحثية *جديدة حقاً* في الذكاء الاصطناعي. توقعاتي الخاصة لهذا مذكورة في الجدول، من حيث احتمالية كل من هذه السيناريوهات للوصول بتلك القدرة إلى المستوى البشري وما بعده.

<table><tbody><tr><th>القدرة</th><th>وصف القدرة</th><th>الحالة/التشخيص</th><th>التوسع/المعروف/الجديد</th></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td colspan="4"><em>القدرات المعرفية الأساسية</em></td></tr><tr><td>التفكير المنطقي</td><td>يستطيع البشر القيام بتفكير منطقي دقيق متعدد الخطوات، واتباع القواعد والتحقق من الدقة.</td><td>تقدم هائل مؤخراً باستخدام سلسلة التفكير الممتدة وإعادة التدريب</td><td>95/5/5</td></tr><tr><td>التخطيط</td><td>يظهر البشر تخطيطاً طويل المدى وهرمياً.</td><td>يتحسن مع التوسع؛ يمكن مساعدته بقوة باستخدام السقالة وتقنيات تدريب أفضل.</td><td>10/85/5</td></tr><tr><td>التأسيس على الحقيقة</td><td>أنظمة الذكاء الاصطناعي العامة تختلق معلومات غير مؤسسة لتلبية الاستفسارات.</td><td>يتحسن مع التوسع؛ بيانات المعايرة متاحة داخل النموذج؛ يمكن فحصها/تحسينها عبر السقالة.</td><td>30/65/5</td></tr><tr><td>حل المشكلات المرن</td><td>يمكن للبشر التعرف على أنماط جديدة واختراع حلول جديدة لمشكلات معقدة؛ نماذج التعلم الآلي الحالية تعاني.</td><td>يتحسن مع التوسع لكن بشكل ضعيف؛ قد يكون قابلاً للحل بتقنيات عصبية-رمزية أو تقنيات "بحث" معممة.</td><td>15/75/10</td></tr><tr><td colspan="4"><em>التعلم والمعرفة</em></td></tr><tr><td>التعلم والذاكرة</td><td>لدى البشر ذاكرة عاملة وقصيرة المدى وطويلة المدى، وكلها ديناميكية ومترابطة.</td><td>جميع النماذج تتعلم أثناء التدريب؛ أنظمة الذكاء الاصطناعي العامة تتعلم ضمن نافذة السياق وأثناء الضبط الدقيق؛ تقنيات "التعلم المستمر" وأخرى موجودة لكن غير مدمجة بعد في أنظمة الذكاء الاصطناعي العامة الكبيرة.</td><td>5/80/15</td></tr><tr><td>التجريد والتكرار</td><td>يمكن للبشر رسم خرائط ونقل مجموعات العلاقات إلى أخرى أكثر تجريداً للتفكير والتلاعب، بما في ذلك التفكير "التحليلي" التكراري.</td><td>يتحسن بشكل ضعيف مع التوسع؛ قد يظهر في الأنظمة العصبية-الرمزية.</td><td>30/50/20</td></tr><tr><td>نموذج(نماذج) العالم</td><td>لدى البشر نموذج عالم تنبؤي يحدثونه باستمرار ويمكنهم ضمنه حل المشكلات والقيام بالتفكير الفيزيائي</td><td>يتحسن مع التوسع؛ التحديث مرتبط بالتعلم؛ أنظمة الذكاء الاصطناعي العامة ضعيفة في التنبؤ بالعالم الحقيقي.</td><td>20/50/30</td></tr><tr><td colspan="4"><em>الذات والوكالة</em></td></tr><tr><td>الوكالة</td><td>يستطيع البشر اتخاذ إجراءات لمتابعة أهداف، بناءً على التخطيط/التنبؤ.</td><td>العديد من أنظمة التعلم الآلي لها وكالة؛ النماذج اللغوية الكبيرة يمكن جعلها وكلاء عبر أغلفة.</td><td>5/90/5</td></tr><tr><td>التوجه الذاتي</td><td>يطور البشر ويسعون وراء أهدافهم الخاصة، بدافع وحافز مولد داخلياً.</td><td>يتألف إلى حد كبير من الوكالة بالإضافة إلى الأصالة؛ من المحتمل أن يظهر في الأنظمة الوكيلة المعقدة ذات الأهداف المجردة.</td><td>40/45/15</td></tr><tr><td>المرجعية الذاتية</td><td>يفهم البشر ويفكرون في أنفسهم كموضوعين ضمن بيئة/سياق.</td><td>يتحسن مع التوسع ويمكن تعزيزه بمكافآت التدريب.</td><td>70/15/15</td></tr><tr><td>الوعي الذاتي</td><td>لدى البشر معرفة بأفكارهم وحالاتهم الذهنية ويمكنهم التفكير فيها.</td><td>موجود بمعنى ما في أنظمة الذكاء الاصطناعي العامة، التي يمكن القول أنها تجتاز "اختبار المرآة" الكلاسيكي للوعي الذاتي. يمكن تحسينه بالسقالة؛ لكن غير واضح إذا كان هذا كافياً.</td><td>20/55/25</td></tr><tr><td colspan="4"><em>الواجهة والبيئة</em></td></tr><tr><td>الذكاء المجسد</td><td>يفهم البشر ويتفاعلون بنشاط مع بيئتهم في العالم الحقيقي.</td><td>التعلم المعزز يعمل جيداً في البيئات المحاكاة والحقيقية (الروبوتية) ويمكن دمجه في المحولات متعددة الوسائط.</td><td>5/85/10</td></tr><tr><td>معالجة متعددة الحواس</td><td>يدمج البشر ويعالجون في الوقت الفعلي التدفقات البصرية والصوتية والحسية الأخرى.</td><td>التدريب في وسائط متعددة يبدو أنه "يعمل فحسب"، ويتحسن مع التوسع. معالجة الفيديو في الوقت الفعلي صعبة لكن أنظمة القيادة الذاتية مثلاً تتحسن بسرعة.</td><td>30/60/10</td></tr><tr><td colspan="4"><em>القدرات عالية المستوى</em></td></tr><tr><td>الأصالة</td><td>نماذج التعلم الآلي الحالية مبدعة في تحويل ودمج الأفكار/الأعمال الموجودة، لكن البشر يمكنهم بناء أطر وهياكل جديدة، أحياناً مرتبطة بهويتهم.</td><td>يمكن أن يكون من الصعب تمييزه عن "الإبداع"، والذي قد يتطور إليه؛ قد يظهر من الإبداع بالإضافة إلى الوعي الذاتي.</td><td>50/40/10</td></tr><tr><td>الإحساس</td><td>يخبر البشر الكيفيات؛ يمكن أن تكون هذه إيجابية أو سلبية أو محايدة القيمة؛ إنه "مثل شيء" أن تكون شخصاً.</td><td>صعب جداً ومحفوف فلسفياً تحديد ما إذا كان نظام معين لديه هذا.</td><td>5/10/85</td></tr></tbody></table>

القدرات الأساسية التي هي حالياً دون مستوى الخبير البشري في أنظمة الذكاء الاصطناعي العامة الحديثة، مجمعة حسب النوع. العمود الثالث يلخص الحالة الراهنة. العمود الأخير يُظهر الاحتمالية المتوقعة (%) أن الأداء على المستوى البشري سيتحقق من خلال: توسيع التقنيات الحالية / الدمج مع التقنيات المعروفة / تطوير تقنيات جديدة. هذه القدرات ليست مستقلة، والزيادة في أي منها عادة ما تترافق مع زيادات في أخرى. لاحظ أنه ليس كلها (خاصة الإحساس) ضروري لأنظمة الذكاء الاصطناعي القادرة على تطوير الذكاء الاصطناعي، مما يبرز إمكانية وجود ذكاء اصطناعي قوي لكن غير حاس.

تفكيك ما هو "مفقود" بهذه الطريقة يجعل من الواضح إلى حد كبير أننا على الطريق الصحيح للوصول إلى ذكاء يتفوق على البشر بشكل واسع من خلال توسيع التقنيات الموجودة أو المعروفة.[^39]

قد تكون هناك مفاجآت. حتى لو تركنا جانباً "الإحساس"، قد تكون هناك بعض القدرات المعرفية الأساسية المذكورة التي لا يمكن حقاً فعلها بالتقنيات الحالية وتتطلب تقنيات جديدة. لكن تأمل في هذا. الجهد الحالي الذي تبذله العديد من أكبر شركات العالم يعادل أضعاف إنفاق مشروع أبولو وعشرات أضعاف إنفاق مشروع مانهاتن،[^40] ويوظف آلاف أفضل الأشخاص التقنيين في العالم برواتب لم نسمع بها من قبل. ديناميكيات السنوات القليلة الماضية جلبت الآن قوة فكرية بشرية أكثر من أي مسعى في التاريخ (مع إضافة الذكاء الاصطناعي الآن) لهذا الأمر. يجب ألا نراهن على الفشل.

### الهدف الكبير: الوكلاء المستقلون العامون

ركز تطوير الذكاء الاصطناعي العام خلال السنوات القليلة الماضية على إنشاء ذكاء اصطناعي عام وقوي لكن شبيه بالأداة: يعمل بشكل أساسي كمساعد مخلص (إلى حد كبير)، ولا يتخذ إجراءات بمفرده عموماً. هذا جزئياً بالتصميم، لكن إلى حد كبير لأن هذه الأنظمة لم تكن ببساطة مختصة بما فيه الكفاية في المهارات ذات الصلة لتوكل إليها مهام معقدة.[^41]

لكن شركات الذكاء الاصطناعي والباحثين يتجهون بشكل متزايد [نحو التركيز](https://www.axios.com/2025/01/23/davos-2025-ai-agents) على وكلاء *مستقلين* على مستوى الخبراء وعامي الغرض.[^42] هذا سيتيح للأنظمة أن تعمل أكثر مثل مساعد بشري يمكن للمستخدم أن يفوض إليه إجراءات حقيقية.[^43] ما الذي سيتطلبه ذلك؟ عدد من القدرات في جدول "ما المفقود" متورط، بما في ذلك التأسيس القوي على الحقيقة، والتعلم والذاكرة، والتجريد والتكرار، ونمذجة العالم (للذكاء)، والتخطيط، والوكالة، والأصالة، والتوجه الذاتي، والمرجعية الذاتية، والوعي الذاتي (للاستقلالية)، ومعالجة متعددة الحواس، والذكاء المجسد، وحل المشكلات المرن (للعمومية).[^44]

هذا التقاطع الثلاثي للاستقلالية العالية (استقلالية العمل) والعمومية العالية (النطاق واتساع المهام) والذكاء العالي (الكفاءة في المهام المعرفية) فريد حالياً للبشر. إنه ضمنياً ما يفكر فيه الكثيرون على الأرجح عندما يفكرون في الذكاء الاصطناعي العام - من ناحية قيمته وكذلك مخاطره.

هذا يوفر طريقة أخرى لتعريف الذكاء الاصطناعي العام كــــ ***ذ***كاء ***ع***ام ***م***ستقل، وسنرى أن هذا التقاطع الثلاثي يوفر عدسة قيمة جداً للأنظمة عالية القدرة في فهم مخاطرها ومكافآتها، وفي حوكمة الذكاء الاصطناعي.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Simple-1024x1024.png&w=3840&q=75) منطقة القوة والخطر التحويلية للذكاء الاصطناعي العام تظهر من تقاطع ثلاث خصائص أساسية: الاستقلالية العالية، والذكاء العالي في المهام، والعمومية العالية.

### دورة تحسين الذكاء الاصطناعي (الذاتي)

عامل حاسم أخير في فهم تقدم الذكاء الاصطناعي هو حلقة التغذية الراجعة التكنولوجية الفريدة للذكاء الاصطناعي. في تطوير الذكاء الاصطناعي، النجاح - في كل من الأنظمة المُبرهنة والمنتجات المطروحة - يجلب استثماراً ومواهب ومنافسة إضافية، ونحن حالياً في وسط حلقة تغذية راجعة ضخمة من الضجيج-بالإضافة-إلى-الواقع للذكاء الاصطناعي تقود مئات المليارات، أو حتى تريليونات، من الدولارات في الاستثمار.

هذا النوع من دورة التغذية الراجعة يمكن أن يحدث مع أي تكنولوجيا، وقد رأيناه في كثير منها، حيث النجاح السوقي يولد الاستثمار، الذي يولد التحسين ونجاحاً سوقياً أفضل. لكن تطوير الذكاء الاصطناعي يذهب أبعد، حيث أن أنظمة الذكاء الاصطناعي تساعد الآن في تطوير أنظمة ذكاء اصطناعي جديدة وأكثر قوة.[^45] يمكننا التفكير في حلقة التغذية الراجعة هذه في خمس مراحل، كل منها بإطار زمني أقصر من السابق، كما هو مُظهر في الجدول.

*تعمل دورة تحسين الذكاء الاصطناعي عبر أطر زمنية متعددة، مع كل مرحلة تحتمل تسريع المراحل اللاحقة. المراحل المبكرة جارية بالفعل، بينما المراحل اللاحقة تبقى تخمينية لكن يمكن أن تتقدم بسرعة كبيرة بمجرد فتحها.*

عدة من هذه المراحل جارية بالفعل، وأصبح واضحاً أن اثنتين منها بدأتا. المرحلة الأخيرة، حيث تحسن أنظمة الذكاء الاصطناعي نفسها بشكل مستقل، كانت عنصراً أساسياً في الأدبيات حول خطر أنظمة الذكاء الاصطناعي القوية جداً، ولسبب وجيه.[^46] لكن من المهم ملاحظة أنها مجرد الشكل الأكثر تطرفاً لدورة تغذية راجعة بدأت بالفعل ويمكن أن تؤدي إلى مزيد من المفاجآت في التقدم السريع للتكنولوجيا.


[^31]: تستخدم من هذا الذكاء الاصطناعي أكثر مما تعتقد على الأرجح، في توليد الكلام والتعرف عليه، ومعالجة الصور، وخوارزميات الأخبار، إلخ.

[^32]: بينما العلاقات بين هذه الأزواج من الشركات معقدة ومتنوعة تماماً، فقد ذكرتها صراحة للإشارة إلى القيمة السوقية الإجمالية الهائلة للشركات المنخرطة الآن في تطوير الذكاء الاصطناعي، وأيضاً أنه وراء حتى الشركات "الأصغر" مثل Anthropic توجد جيوب عميقة جداً عبر الاستثمارات وصفقات الشراكة الكبرى.

[^33]: أصبح من المألوف التقليل من شأن اختبار تورينغ، لكنه قوي وعام تماماً. في النسخ الضعيفة يشير إلى ما إذا كان الأشخاص العاديون الذين يتفاعلون مع ذكاء اصطناعي (مدرب ليتصرف بشرياً) بطرق نموذجية لفترات قصيرة يمكنهم معرفة ما إذا كان ذكاءً اصطناعياً. لا يمكنهم ذلك. ثانياً، اختبار تورينغ عدائي للغاية يمكن أن يستكشف أساساً أي عنصر من قدرة وذكاء الإنسان - من خلال مثلاً مقارنة نظام ذكاء اصطناعي بخبير بشري، يقيمه خبراء بشريون آخرون. هناك معنى يكون فيه الكثير من تقييم الذكاء الاصطناعي شكلاً معمماً من اختبار تورينغ.

[^34]: هذا لكل مجال - لا يمكن لأي إنسان أن يحقق معقولاً مثل هذه الدرجات عبر جميع المواضيع في الوقت نفسه.

[^35]: هذه مسائل تستغرق حتى علماء رياضيات ممتازين وقتاً جوهرياً لحلها، إذا كان بإمكانهم حلها على الإطلاق.

[^36]: إذا كنت من النوع المتشكك، احتفظ بتشككك لكن جرب النماذج الأحدث حقاً، وكذلك جرب بنفسك بعض الأسئلة الاختبارية التي يمكنها اجتيازها. كأستاذ فيزياء، سأتنبأ بيقين شبه تام أن النماذج الأعلى مثلاً ستجتاز امتحان التأهيل للدراسات العليا في قسمنا.

[^37]: هذا وضعف آخر مثل الاختلاق أبطآ اعتماد السوق وأدى إلى فجوة بين القدرات المدركة والمُدعاة (والتي يجب أيضاً النظر إليها من خلال عدسة المنافسة السوقية الشديدة والحاجة لجذب الاستثمار). هذا أربك كلاً من الجمهور وصناع السياسات حول الحالة الفعلية لتقدم الذكاء الاصطناعي. بينما قد لا يضاهي الضجيج، فإن التقدم حقيقي جداً.

[^38]: التقدم الكبير منذ ذلك الحين كان تطوير أنظمة مدربة للتفكير المنطقي عالي الجودة، مستفيدة من حوسبة أكثر أثناء الاستنتاج وتعلم معزز أكثر. لأن هذه النماذج جديدة وقدراتها أقل اختباراً، لم أعد كتابة هذا الجدول كلياً عدا "التفكير المنطقي"، الذي أعتبره محلولاً أساساً. لكنني حدثت التنبؤات بناءً على القدرات المجربة والمُبلغ عنها لتلك الأنظمة.

[^39]: موجات التفاؤل السابقة بالذكاء الاصطناعي في الستينيات والثمانينيات انتهت في "شتاءات الذكاء الاصطناعي" عندما فشلت القدرات الموعودة في التحقق. لكن الموجة الحالية تختلف جوهرياً في تحقيقها أداء يتفوق على البشر في مجالات كثيرة، مدعوماً بموارد حاسوبية ونجاح تجاري ضخمين.

[^40]: كلف مشروع أبولو الكامل [حوالي 250 مليار دولار أمريكي بدولارات 2020](https://www.planetary.org/space-policy/cost-of-apollo)، ومشروع مانهاتن [أقل من عُشر ذلك](https://www.brookings.edu/the-costs-of-the-manhattan-project/). Goldman Sachs [تتوقع تريليون دولار من الإنفاق فقط على مراكز بيانات الذكاء الاصطناعي](https://www.datacenterdynamics.com/en/news/goldman-sachs-1tn-to-be-spent-on-ai-data-centers-chips-and-utility-upgrades-with-little-to-show-for-it-so-far/) خلال السنوات القليلة القادمة.

[^41]: رغم أن البشر يرتكبون الكثير من الأخطاء، نحن نقلل من تقدير مدى موثوقيتنا! لأن الاحتماليات تتضاعف، مهمة تتطلب 20 خطوة لإنجازها بشكل صحيح تتطلب أن تكون كل خطوة موثوقة بنسبة 97% فقط لإنجازها بشكل صحيح نصف الوقت. نقوم بمثل هذه المهام طوال الوقت.

[^42]: خطوة قوية في هذا الاتجاه اتُخذت مؤخراً جداً مع مساعد ["البحث العميق"](https://openai.com/index/introducing-deep-research/) من OpenAI الذي يؤدي بحثاً عاماً مستقلاً على الإنترنت، موصوف كـ "قدرة وكيلة جديدة تقوم ببحث متعدد الخطوات على الإنترنت للمهام المعقدة."

[^43]: أشياء مثل ملء ذلك النموذج المزعج، حجز الرحلات، إلخ. لكن مع دكتوراه في 20 مجالاً! إذن أيضاً: كتابة تلك الأطروحة لك، التفاوض على ذلك العقد لك، إثبات تلك النظرية لك، إنشاء تلك الحملة الإعلانية لك، إلخ. ماذا *تفعل* أنت؟ تخبره ماذا يفعل، بالطبع.

[^44]: لاحظ أن الإحساس *ليس* مطلوباً بوضوح، ولا يستلزم الذكاء الاصطناعي في هذا التقاطع الثلاثي بالضرورة ذلك.

[^45]: أقرب قياس هنا ربما تكنولوجيا الشرائح، حيث حافظ التطوير على قانون مور لعقود، بينما تقنيات الكمبيوتر تساعد الناس في تصميم الجيل القادم من تكنولوجيا الشرائح. لكن الذكاء الاصطناعي سيكون أكثر مباشرة بكثير.

[^46]: من المهم أن تستوعب للحظة أن الذكاء الاصطناعي يمكن - قريباً - أن يحسن نفسه على إطار زمني من أيام أو أسابيع. أو أقل. احتفظ بهذا في بالك عندما يخبرك أحدهم أن قدرة ذكاء اصطناعي بعيدة بالتأكيد.

## الفصل السادس - السباق نحو الذكاء الاصطناعي العام

ما هي القوى المحركة وراء السباق لبناء الذكاء الاصطناعي العام، سواء بالنسبة للشركات أم الدول؟

لقد نتج عن التقدم السريع الأخير في مجال الذكاء الاصطناعي مستوى استثنائي من الاهتمام والاستثمار، كما ساهم في تحقيقه. ويُعزى هذا جزئياً إلى النجاح في تطوير الذكاء الاصطناعي، لكن هناك المزيد مما يحدث. لماذا تتسابق بعض أكبر الشركات على وجه الأرض، بل وحتى الدول، لبناء ليس مجرد الذكاء الاصطناعي، وإنما الذكاء الاصطناعي العام والذكاء الفائق؟

### ما الذي دفع أبحاث الذكاء الاصطناعي نحو الذكاء البشري المستوى

حتى السنوات الخمس الماضية تقريباً، كان الذكاء الاصطناعي يُعتبر إلى حد كبير مشكلة بحثية أكاديمية وعلمية، وبالتالي كان مدفوعاً بشكل أساسي بالفضول والرغبة في فهم الذكاء وكيفية خلقه في وسط جديد.

في هذه المرحلة، لم يكن هناك اهتمام كبير بفوائد أو مخاطر الذكاء الاصطناعي بين معظم الباحثين. عندما كان يُسأل عن سبب ضرورة تطوير الذكاء الاصطناعي، كانت الإجابة الشائعة قد تتضمن سرداً غامضاً إلى حد ما للمشاكل التي يمكن للذكاء الاصطناعي المساعدة في حلها: أدوية جديدة، ومواد جديدة، وعلوم جديدة، وعمليات أذكى، وبشكل عام تحسين الأمور للناس.[^47]

هذه أهداف جديرة بالإعجاب![^48] وعلى الرغم من أننا يمكننا وسوف نتساءل عما إذا كان الذكاء الاصطناعي العام - بدلاً من الذكاء الاصطناعي بشكل عام - ضرورياً لهذه الأهداف، إلا أنها تُظهر المثالية التي بدأ بها العديد من باحثي الذكاء الاصطناعي.

على مدى السنوات الخمس الماضية، مع ذلك، تحول الذكاء الاصطناعي من مجال بحثي خالص نسبياً إلى مجال هندسي وإنتاجي أكثر، مدفوعاً إلى حد كبير من قبل بعض أكبر الشركات في العالم.[^49] فالباحثون، رغم استمرار أهميتهم، لم يعودوا يقودون العملية.

### لماذا تحاول الشركات بناء الذكاء الاصطناعي العام؟

إذن لماذا تصب الشركات العملاقة (والمستثمرون أكثر من ذلك) موارد هائلة في بناء الذكاء الاصطناعي العام؟ هناك محركان تكون معظم الشركات صريحة تماماً بشأنهما: فهي ترى الذكاء الاصطناعي كمحرك للإنتاجية بالنسبة للمجتمع، وللأرباح بالنسبة لها. ولأن الذكاء الاصطناعي العام عام الغرض بطبيعته، فهناك جائزة ضخمة: بدلاً من اختيار قطاع واحد لإنشاء منتجات وخدمات فيه، يمكن للمرء أن يجرب *جميعها في آن واحد.* لقد نمت شركات التكنولوجيا الكبرى لتصبح عملاقة من خلال إنتاج السلع والخدمات الرقمية، ومن المؤكد أن بعض المديرين التنفيذيين على الأقل يرون الذكاء الاصطناعي كخطوة تالية في توفيرها بشكل جيد، مع مخاطر وفوائد تتوسع وتردد صدى تلك المقدمة من البحث ووسائل التواصل الاجتماعي وأجهزة الكمبيوتر المحمولة والهواتف، إلخ.

لكن لماذا الذكاء الاصطناعي العام؟ هناك إجابة بسيطة جداً على هذا، تتجنب معظم الشركات والمستثمرين مناقشتها علناً.[^50]

وهي أن الذكاء الاصطناعي العام يمكنه *استبدال العمال* بشكل مباشر، واحد لواحد.

ليس تعزيزهم، وليس تمكينهم، وليس جعلهم أكثر إنتاجية. وليس حتى *إزاحتهم.* كل هذا يمكن وسيتم عمله بواسطة ذكاء اصطناعي غير عام. الذكاء الاصطناعي العام هو تحديداً ما يمكنه *استبدال* عمال المعرفة بالكامل (ومع الروبوتيات، العديد من العمال الفيزيائيين أيضاً). كدليل على هذا الرأي لا نحتاج للنظر أبعد من [تعريف OpenAI المعلن علناً](https://openai.com/our-structure/) للذكاء الاصطناعي العام، وهو "نظام عالي الاستقلالية يتفوق على البشر في معظم الأعمال ذات القيمة الاقتصادية."

الجائزة هنا (بالنسبة للشركات!) ضخمة. تشكل تكاليف العمالة نسبة كبيرة من الاقتصاد العالمي البالغ حوالي 100 تريليون دولار. حتى لو تم الاستحواذ على جزء فقط من هذا عبر استبدال العمل البشري بالعمل بالذكاء الاصطناعي، فهذا يعني تريليونات الدولارات من الإيرادات السنوية. تدرك شركات الذكاء الاصطناعي أيضاً من هو المستعد للدفع. كما يرونها، لن تدفع آلاف الدولارات سنوياً مقابل أدوات الإنتاجية. لكن الشركة *ستدفع* آلاف الدولارات سنوياً لاستبدال عملك، إذا استطاعت.

### لماذا تشعر الدول أنها مضطرة للتسابق نحو الذكاء الاصطناعي العام

تركز دوافع الدول المعلنة لمتابعة الذكاء الاصطناعي العام على القيادة الاقتصادية والعلمية. الحجة مقنعة: يمكن للذكاء الاصطناعي العام أن يسرع بشكل كبير البحث العلمي والتطوير التكنولوجي والنمو الاقتصادي. وبالنظر إلى المخاطر، يحتجون، لا يمكن لأي قوة كبرى أن تسمح لنفسها بالتخلف عن الركب.[^51]

لكن هناك أيضاً محركات إضافية وغير معلنة إلى حد كبير. ليس هناك شك أنه عندما يجتمع قادة عسكريون وأمنيون معينون خلف أبواب مغلقة لمناقشة تكنولوجيا قوية للغاية وخطيرة بشكل كارثي، فإن تركيزهم ليس على "كيف نتجنب هذه المخاطر" بل بالأحرى "كيف نحصل على هذا أولاً؟" يرى القادة العسكريون والاستخباراتيون الذكاء الاصطناعي العام كثورة محتملة في الشؤون العسكرية، ربما الأهم منذ الأسلحة النووية. الخوف هو أن الدولة الأولى التي تطور الذكاء الاصطناعي العام يمكن أن تكسب ميزة استراتيجية لا يمكن التغلب عليها. هذا يخلق ديناميكية سباق تسلح كلاسيكية.

سنرى أن تفكير "السباق نحو الذكاء الاصطناعي العام"[^52] هذا، رغم كونه مقنعاً، معيب بشكل عميق. وليس هذا لأن السباق خطير ومحفوف بالمخاطر - رغم أنه كذلك - ولكن بسبب طبيعة التكنولوجيا. الافتراض غير المعلن هو أن الذكاء الاصطناعي العام، مثل التقنيات الأخرى، قابل للسيطرة من قبل الدولة التي تطوره، وهو نعمة تمنح القوة للمجتمع الذي يملك أكثر منه. كما سنرى، من المحتمل ألا يكون أياً من الأمرين.

### لماذا الذكاء الفائق؟

بينما تركز الشركات علناً على الإنتاجية، والدول على النمو الاقتصادي والتكنولوجي، بالنسبة لأولئك الذين يسعون عمداً إلى الذكاء الاصطناعي العام الكامل والذكاء الفائق فهذه مجرد البداية. ما الذي يفكرون فيه حقاً؟ على الرغم من قلة ما يُقال بصوت عالٍ، فإنه يشمل:

1. علاجات للعديد من الأمراض أو كلها؛
2. إيقاف وعكس الشيخوخة؛
3. مصادر طاقة مستدامة جديدة مثل الاندماج النووي؛
4. تحسينات بشرية، أو كائنات مصممة عبر الهندسة الوراثية؛
5. تكنولوجيا النانو والتصنيع الجزيئي؛
6. رفع العقول؛
7. تقنيات فيزياء غريبة أو فضائية؛
8. مشورة فائقة البشرية ودعم اتخاذ القرار؛
9. تخطيط وتنسيق فائق البشرية.

الثلاثة الأولى هي إلى حد كبير تقنيات "أحادية الجانب" - أي من المحتمل أن تكون إيجابية صافية بقوة. من الصعب الاعتراض على علاج الأمراض أو القدرة على العيش لفترة أطول إذا اختار المرء ذلك. ولقد حصدنا بالفعل الجانب السلبي للاندماج النووي (في شكل أسلحة نووية)؛ سيكون من الرائع الآن الحصول على الجانب الإيجابي. السؤال مع هذه الفئة الأولى هو ما إذا كان الحصول على هذه التقنيات عاجلاً يعوض عن المخاطر.

الأربعة التالية ذات حدين بوضوح: تقنيات تحويلية لها فوائد محتملة ضخمة ومخاطر هائلة، تماماً مثل الذكاء الاصطناعي. كل هذه، إذا خرجت من صندوق أسود غداً وتم نشرها، ستكون صعبة التعامل معها بشكل لا يصدق.[^53]

الاثنان الأخيران يتعلقان بقيام الذكاء الاصطناعي الفائق البشرية بعمل الأشياء بنفسه بدلاً من مجرد اختراع التكنولوجيا. وبشكل أكثر دقة، وضع التلطيفات جانباً، تتضمن هذه أنظمة ذكاء اصطناعي قوية تخبر الناس بما يجب عليهم فعله. إن تسمية هذا "مشورة" أمر مضلل إذا كان النظام الذي يقدم المشورة أقوى بكثير من المُستشار، الذي لا يستطيع فهم أساس القرار بشكل ذي معنى (أو حتى لو تم توفير هذا، الوثوق بأن المستشار لن يقدم مبرراً مقنعاً مماثلاً لقرار مختلف).

هذا يشير إلى عنصر رئيسي مفقود من القائمة أعلاه:

10. القوة.

من الواضح تماماً أن الكثير مما يكمن وراء السباق الحالي للذكاء الاصطناعي الفائق البشرية هو فكرة أن *الذكاء = القوة*. كل متسابق يراهن على كونه أفضل حامل لتلك القوة، وأنه سيكون قادراً على استخدامها لأسباب خيرة ظاهرياً دون أن تفلت أو تُؤخذ من سيطرته.

أي أن ما تطارده الشركات والدول حقاً ليس فقط ثمار الذكاء الاصطناعي العام والذكاء الفائق، وإنما القوة للتحكم في من يحصل على الوصول إليها وكيف تُستخدم. ترى الشركات نفسها كأوصياء مسؤولين على هذه القوة في خدمة المساهمين والإنسانية؛ ترى الدول نفسها كحراس ضروريين يمنعون القوى المعادية من كسب ميزة حاسمة. كلاهما مخطئ بشكل خطير، فاشل في إدراك أن الذكاء الفائق، بطبيعته، لا يمكن السيطرة عليه بشكل موثوق من قبل أي مؤسسة بشرية. سنرى أن طبيعة وديناميكيات الأنظمة فائقة الذكاء تجعل السيطرة البشرية صعبة للغاية، إن لم تكن مستحيلة.

هذه الديناميكيات المتسابقة - سواء الشركاتية أو الجيوسياسية - تجعل مخاطر معينة محتمة تقريباً ما لم تُقطع بشكل حاسم. ننتقل الآن إلى فحص هذه المخاطر ولماذا لا يمكن التخفيف منها بشكل كافٍ ضمن نموذج تطوير تنافسي[^54].


[^47]: قائمة أكثر دقة للأهداف الجديرة هي [أهداف التنمية المستدامة](https://sdgs.un.org/goals) للأمم المتحدة. هذه، بمعنى ما، أقرب ما لدينا إلى مجموعة من الأهداف الإجماعية العالمية لما نود رؤيته محسناً في العالم. يمكن للذكاء الاصطناعي المساعدة.

[^48]: التكنولوجيا بشكل عام لديها قوة تحويلية اقتصادية واجتماعية لتحسين الإنسان، كما تشهد آلاف السنين. في هذا السياق، يمكن العثور على شرح طويل ومقنع لرؤية إيجابية للذكاء الاصطناعي العام في [هذا المقال](https://darioamodei.com/machines-of-loving-grace) لمؤسس Anthropic داريو أموداي.

[^49]: [بدأ الاستثمار الخاص في الذكاء الاصطناعي في الازدهار في 2018-19، متجاوزاً الاستثمار العام حوالي ذلك الوقت،](https://cset.georgetown.edu/publication/tracking-ai-investment/) وتفوق عليه بشكل كبير منذ ذلك الحين.

[^50]: يمكنني أن أشهد أنه خلف أبواب أكثر انغلاقاً، ليس لديهم مثل هذا التحفظ. وأصبح الأمر أكثر علنية؛ انظر على سبيل المثال ["طلب الشركات الناشئة"](https://www.ycombinator.com/rfs) الجديد من Y-combinator، الذي تدعو أجزاء كثيرة منه صراحة لاستبدال العمال البشر بالجملة. ولأقتبس منهم، "كانت القيمة المضافة لبرامج B2B SaaS هي جعل العمال البشر أكثر كفاءة تدريجياً. القيمة المضافة لوكلاء الذكاء الاصطناعي العمودي هي أتمتة العمل بالكامل... من الممكن تماماً أن تكون هذه الفرصة كبيرة بما يكفي لصنع 100 يونيكورن آخر." (لأولئك غير المطلعين على مصطلحات وادي السيليكون، "B2B" يعني من شركة لشركة واليونيكورن هو شركة بمليار دولار. أي أنهم يتحدثون عن أكثر من مائة شركة بمليارات الدولارات تستبدل العمال لشركات أخرى.)

[^51]: انظر على سبيل المثال [تقرير لجنة مراجعة الأمن الاقتصادي الأمريكية-الصينية](https://www.uscc.gov/sites/default/files/2024-11/2024_Executive_Summary.pdf) الحديث. على الرغم من وجود تبرير قليل بشكل مفاجئ داخل التقرير نفسه، كانت التوصية الرئيسية أن يقوم الكونجرس الأمريكي "بإنشاء وتمويل برنامج يشبه مشروع مانهاتن مخصص للسباق نحو والحصول على قدرة الذكاء الاصطناعي العام."

[^52]: تتبنى الشركات الآن هذا التأطير الجيوسياسي كدرع ضد أي قيد على تطوير الذكاء الاصطناعي الخاص بها، عموماً بطرق تخدم مصالحها الذاتية بشكل صارخ، وأحياناً بطرق لا تحقق حتى معنى أساسياً. فكر في [نهج Meta في الذكاء الاصطناعي الحدودي](https://about.fb.com/news/2025/02/meta-approach-frontier-ai/)، الذي يحتج في الوقت نفسه أن على أمريكا "\[ترسيخ\] موقعها كقائدة في الابتكار التكنولوجي والنمو الاقتصادي والأمن القومي" وأيضاً أنها يجب أن تفعل ذلك بإطلاق أقوى أنظمة الذكاء الاصطناعي علناً - مما يشمل إعطاءها مباشرة لخصومها وأعدائها الجيوسياسيين.

[^53]: وهكذا من المحتمل أن نضطر إلى ترك إدارة هذه التقنيات للذكاءات الاصطناعية. لكن هذا سيكون تفويضاً إشكالياً جداً للسيطرة، والذي سنعود إليه أدناه.

[^54]: المنافسة في تطوير التكنولوجيا غالباً ما تجلب فوائد مهمة: منع السيطرة الاحتكارية، وقيادة الابتكار وخفض التكلفة، وتمكين المناهج المتنوعة، وإنشاء رقابة متبادلة. مع ذلك، مع الذكاء الاصطناعي العام يجب وزن هذه الفوائد ضد المخاطر الفريدة من ديناميكيات السباق والضغط لتقليل احتياطات السلامة.

## الفصل 7 - ما الذي سيحدث إذا طورنا الذكاء الاصطناعي العام في مسارنا الحالي؟

المجتمع غير مستعد لأنظمة بمستوى الذكاء الاصطناعي العام. إذا طورناها قريباً جداً، فقد تسوء الأمور بشكل خطير.

إن تطوير الذكاء الاصطناعي العام الكامل - الذي سنطلق عليه هنا الذكاء الاصطناعي "خارج البوابات" - سيكون تحولاً جوهرياً في طبيعة العالم: فهو بطبيعته يعني إضافة نوع جديد من الذكاء إلى الأرض بقدرة أعظم من قدرة البشر.

ما سيحدث بعدها يعتمد على أشياء كثيرة، منها طبيعة التكنولوجيا، وخيارات من يطورونها، والسياق العالمي الذي يجري تطويرها فيه.

حالياً، يجري تطوير الذكاء الاصطناعي العام الكامل من قبل حفنة من الشركات الخاصة الضخمة في سباق بينها، مع القليل من التنظيم المعنوي أو الإشراف الخارجي،[^55] في مجتمع تتراجع فيه المؤسسات الأساسية وتصبح حتى مختلة الوظائف،[^56] في وقت يشهد توتراً جيوسياسياً عالياً وتنسيقاً دولياً ضعيفاً. وبرغم أن بعضهم يحركه دافع الإيثار، فإن كثيرين ممن يقومون بذلك يحركهم المال، أو السلطة، أو كلاهما.

التنبؤ صعب جداً، لكن هناك بعض الديناميات مفهومة بما فيه الكفاية، وتماثلات مناسبة كفاية مع تقنيات سابقة لتقديم دليل إرشاد. ومع الأسف، رغم وعد الذكاء الاصطناعي، فإنها تعطي أسباباً وجيهة للتشاؤم العميق حول كيفية تطور مسارنا الحالي.

لنقل الأمر بصراحة: في مسارنا الحالي، سيكون لتطوير الذكاء الاصطناعي العام بعض التأثيرات الإيجابية (وسيجعل بعض الناس أثرياء جداً جداً). لكن طبيعة التكنولوجيا، والديناميات الأساسية، والسياق الذي يجري تطويرها فيه، تشير بقوة إلى أن: الذكاء الاصطناعي القوي سيقوض مجتمعنا وحضارتنا بشكل جذري؛ سنفقد السيطرة عليه؛ قد ننتهي بحرب عالمية بسببه؛ سنفقد السيطرة (أو نتنازل عنها) *له*؛ سيؤدي إلى الذكاء الفائق الاصطناعي، الذي لن نسيطر عليه بالمطلق وسيعني نهاية العالم الذي يديره البشر.

هذه ادعاءات قوية، وأتمنى لو كانت مجرد تكهنات فارغة أو "تشاؤماً" غير مبرر. لكن هذا ما يشير إليه العلم، ونظرية الألعاب، ونظرية التطور، والتاريخ جميعاً. هذا القسم يطور هذه الادعاءات ودعمها بالتفصيل.

### سنقوض مجتمعنا وحضارتنا

رغم ما قد تسمعه في قاعات اجتماعات وادي السيليكون، فإن معظم التعطيل - خاصة من النوع السريع جداً - ليس مفيداً. هناك طرق أكثر بكثير لجعل الأنظمة المعقدة أسوأ منها لجعلها أفضل. عالمنا يعمل بالجودة التي يعمل بها لأننا بنينا بعناية فائقة عمليات وتقنيات ومؤسسات جعلته أفضل باطراد.[^57] أخذ مطرقة ثقيلة إلى المصنع نادراً ما يحسن العمليات.

هذا سجل (غير مكتمل) بالطرق التي ستعطل بها أنظمة الذكاء الاصطناعي العام حضارتنا.

- ستعطل العمل بشكل جذري، مما يؤدي *كحد أدنى* إلى ارتفاع جذري في عدم المساواة في الدخل وربما بطالة أو نقص توظيف واسع النطاق، في جدول زمني قصير جداً بحيث لا يستطيع المجتمع التكيف.[^58]
- ستؤدي على الأرجح إلى تركيز سلطة اقتصادية واجتماعية وسياسية هائلة - ربما أكبر من سلطة الدول القومية - في عدد صغير من المصالح الخاصة الضخمة غير المسؤولة أمام الجمهور.
- قد تجعل أنشطة كانت صعبة أو مكلفة سابقاً سهلة تافهة فجأة، مما يزعزع الأنظمة الاجتماعية التي تعتمد على بقاء أنشطة معينة مكلفة أو تتطلب جهداً بشرياً كبيراً.[^59]
- قد تغمر أنظمة جمع المعلومات ومعالجتها والتواصل في المجتمع بوسائط واقعية تماماً لكن كاذبة أو مزعجة أو موجهة بإفراط أو تلاعبية بحيث يصبح من المستحيل معرفة ما هو حقيقي مادياً أم لا، بشري أم لا، وقائعي أم لا، وجدير بالثقة أم لا.[^60]
- قد تخلق اعتماداً فكرياً خطيراً وشبه تام، حيث يضمر الفهم البشري للأنظمة والتقنيات الرئيسية بينما نعتمد بشكل متزايد على أنظمة ذكاء اصطناعي لا نستطيع فهمها بالكامل.
- قد تنهي الثقافة البشرية فعلياً، حين تصبح تقريباً كل الموضوعات الثقافية (النص، والموسيقى، والفن المرئي، والأفلام، إلخ) التي يستهلكها معظم الناس مخلوقة أو متوسطة أو منسقة من قبل عقول غير بشرية.
- قد تمكن أنظمة مراقبة وتلاعب جماعية فعالة يمكن للحكومات أو المصالح الخاصة استخدامها للسيطرة على الشعوب وتحقيق أهداف متعارضة مع المصلحة العامة.
- بتقويض الخطاب البشري والنقاش وأنظمة الانتخابات، قد تقلل مصداقية المؤسسات الديمقراطية إلى درجة استبدالها فعلياً (أو صراحة) بأخرى، منهية الديمقراطية في الدول التي توجد فيها حالياً.
- قد تصبح، أو تخلق، فيروسات ودود برمجية ذكية متقدمة ذاتية التكاثر يمكن أن تنتشر وتتطور، معطلة بشكل هائل أنظمة المعلومات العالمية.
- يمكنها زيادة قدرة الإرهابيين والفاعلين السيئين والدول المارقة على إلحاق الضرر عبر أسلحة بيولوجية أو كيميائية أو سيبرانية أو مستقلة أو أخرى، دون أن يوفر الذكاء الاصطناعي قدرة موازنة لمنع هذا الضرر. وبالمثل ستقوض الأمن القومي والتوازنات الجيوسياسية بجعل الخبرة من الطراز الأول في المجال النووي والبيولوجي والهندسي وغيرها متاحة لأنظمة لن تحصل عليها لولا ذلك.
- قد تسبب رأسمالية مفرطة سريعة واسعة النطاق خارجة عن السيطرة، مع شركات يديرها الذكاء الاصطناعي فعلياً تتنافس في مساحات مالية ومبيعات وخدمات إلكترونية إلى حد كبير. قد تعمل الأسواق المالية المدفوعة بالذكاء الاصطناعي بسرعات ومعقدات تفوق بكثير الفهم أو السيطرة البشرية. كل أساليب الفشل والخارجيات السلبية للاقتصادات الرأسمالية الحالية يمكن أن تتفاقم وتتسارع بعيداً عن السيطرة أو الحكم أو القدرة التنظيمية البشرية.
- قد تغذي سباق تسلح بين الأمم في الأسلحة المدعومة بالذكاء الاصطناعي، وأنظمة القيادة والسيطرة، والأسلحة السيبرانية، إلخ، مما يخلق تراكماً سريعاً جداً لقدرات مدمرة بشدة.

هذه المخاطر ليست تكهنية. كثير منها يتحقق بينما نتكلم، عبر أنظمة الذكاء الاصطناعي الموجودة! لكن تأمل، تأمل *حقاً*، كيف سيبدو كل منها مع ذكاء اصطناعي أقوى بشكل جذري.

تأمل إزاحة العمل حين لا يستطيع معظم العمال ببساطة توفير أي قيمة اقتصادية مهمة تتجاوز ما يستطيع الذكاء الاصطناعي، في مجال خبرتهم أو تجربتهم - أو حتى لو أعادوا تدريب أنفسهم! تأمل المراقبة الجماعية إذا كان كل شخص يُراقب ويُرصد بشكل فردي من قبل شيء أسرع وأذكى منه. كيف تبدو الديمقراطية حين لا نستطيع الوثوق بشكل موثوق بأي معلومة رقمية نراها أو نسمعها أو نقرؤها، وحين تكون الأصوات العامة الأكثر إقناعاً ليست حتى بشرية، ولا لها حصة في النتيجة؟ ماذا يصبح عليه الحرب حين يضطر الجنرالات لتأجيل القرار باستمرار للذكاء الاصطناعي (أو ببساطة وضعه في المسؤولية)، لئلا يمنحوا ميزة حاسمة للعدو؟ أي واحد من المخاطر أعلاه يمثل كارثة للحضارة البشرية[^61] إذا تحقق بالكامل.

يمكنك وضع تنبؤاتك الخاصة. اسأل نفسك هذه الأسئلة الثلاثة لكل مخاطرة:

1. هل سيسمح بها ذكاء اصطناعي فائق القدرة، عالي الاستقلالية، وعام جداً بطريقة أو في حجم لن يكون ممكناً لولا ذلك؟
2. هل هناك أطراف ستستفيد من أشياء تسبب حدوثها؟
3. هل هناك أنظمة ومؤسسات موضوعة ستمنع بفعالية حدوثها؟

حيث تكون إجاباتك "نعم، نعم، لا" يمكنك رؤية أن لدينا مشكلة كبيرة.

ما خطتنا لإدارتها؟ كما تقف الأمور هناك اثنتان على الطاولة فيما يتعلق بالذكاء الاصطناعي عموماً.

الأولى هي بناء ضمانات في الأنظمة لمنعها من فعل أشياء لا يجب أن تفعلها. هذا يُفعل الآن: أنظمة الذكاء الاصطناعي التجارية ستمتنع، مثلاً، عن المساعدة في بناء قنبلة أو كتابة خطاب كراهية.

هذه الخطة غير كافية بائساً لأنظمة خارج البوابة.[^62] قد تساعد في تقليل مخاطر توفير الذكاء الاصطناعي مساعدة خطيرة بوضوح للفاعلين السيئين. لكنها لن تفعل شيئاً لمنع تعطيل العمل، أو تركيز السلطة، أو الرأسمالية المفرطة الخارجة عن السيطرة، أو استبدال الثقافة البشرية: هذه فقط نتائج لاستخدام الأنظمة بطرق مسموحة تربح مزوديها! والحكومات ستحصل بالتأكيد على وصول لأنظمة للاستخدام العسكري أو المراقبة.

الخطة الثانية أسوأ حتى: ببساطة إطلاق أنظمة ذكاء اصطناعي قوية جداً بشكل مفتوح لأي شخص ليستخدمها كما يحب،[^63] والأمل في الأفضل.

ضمني في كلا الخطتين أن شخصاً آخر، مثل الحكومات، سيساعد في حل المشاكل عبر القانون الناعم أو الصلب، والمعايير، واللوائح، والأعراف، والآليات الأخرى التي نستخدمها عموماً لإدارة التقنيات.[^64] لكن بوضع جانباً أن شركات الذكاء الاصطناعي تقاتل فعلاً بأسنانها وأظافرها ضد أي تنظيم جوهري أو قيود مفروضة خارجياً على الإطلاق، لعدد من هذه المخاطر من الصعب جداً رؤية أي تنظيم سيساعد حقاً حتى. التنظيم يمكن أن يفرض معايير أمان على الذكاء الاصطناعي. لكن هل سيمنع الشركات من استبدال العمال بالجملة بالذكاء الاصطناعي؟ هل سيمنع الناس من ترك الذكاء الاصطناعي يدير شركاتهم لهم؟ هل سيمنع الحكومات من استخدام الذكاء الاصطناعي القوي في المراقبة والأسلحة؟ هذه القضايا أساسية. البشرية يمكن أن تجد طرقاً للتكيف معها ربما، لكن فقط بوقت *أكثر بكثير*. كما تقف الأمور، بالنظر إلى السرعة التي يصل بها الذكاء الاصطناعي أو يتجاوز قدرات الناس الذين يحاولون إدارته، تبدو هذه المشاكل مستعصية بشكل متزايد.

### سنفقد السيطرة على (بعض على الأقل من) أنظمة الذكاء الاصطناعي العام

معظم التقنيات قابلة للسيطرة جداً، بالبنية. إذا بدأت سيارتك أو محمصة الخبز تفعل شيئاً لا تريدها أن تفعله، فهذا مجرد خلل، ليس جزءاً من طبيعتها كمحمصة. الذكاء الاصطناعي مختلف: إنه *ينمو* بدلاً من أن يُصمم، وعمله الأساسي غامض، وهو غير قابل للتنبؤ بطبيعته.

فقدان السيطرة هذا ليس نظرياً - نرى نسخاً مبكرة فعلاً. تأمل أولاً مثالاً نثرياً، وقابلاً للجدل حول كونه حميداً. إذا طلبت من ChatGPT أن يساعدك في خلط سم، أو كتابة خطاب عنصري، سيرفض. هذا قابل للجدل أنه جيد. لكنه أيضاً ChatGPT *لا يفعل ما طلبت منه صراحة*. قطع البرامج الأخرى لا تفعل ذلك. نفس النموذج لن يصمم السموم بناء على طلب موظف في OpenAI أيضاً.[^65] هذا يجعل من السهل جداً تخيل كيف سيكون الأمر لذكاء اصطناعي مستقبلي أقوى أن يكون خارج السيطرة. في حالات كثيرة، ببساطة لن يفعلوا ما نطلب! إما أن نظام الذكاء الاصطناعي العام الفائق للبشر المعين سيكون مطيعاً ومخلصاً بالمطلق لنظام أوامر بشري ما، أو لن يكون. إن لم يكن، *سيفعل أشياء قد يعتقد أنها جيدة لنا، لكنها مخالفة لأوامرنا الصريحة.* هذا ليس شيئاً تحت السيطرة. لكن، قد تقول، هذا مقصود - هذه الرفوضات بالتصميم، جزء مما يُسمى "مواءمة" الأنظمة مع القيم البشرية. وهذا صحيح. لكن "برنامج" المواءمة نفسه له مشكلتان رئيسيتان.[^66]

أولاً، في مستوى عميق لا نعرف كيف نفعل ذلك. كيف نضمن أن نظام الذكاء الاصطناعي سـ"يهتم" بما نريد؟ يمكننا تدريب أنظمة الذكاء الاصطناعي لتقول ولا تقول أشياء بتوفير ردود الفعل؛ ويمكنها تعلم والتفكير حول ما يريده البشر ويهتمون به تماماً كما يفكرون حول أشياء أخرى. لكن ليس لدينا طريقة - حتى نظرياً - لجعلهم يقدرون بعمق وبشكل موثوق ما يهتم به الناس. هناك مختلون نفسياً بشريون عاليو الأداء يعرفون ما يُعتبر صحيحاً وخطأ، وكيف من المفروض أن يتصرفوا. ببساطة لا *يهتمون*. لكن يمكنهم *التصرف* كما لو يفعلون، إذا ناسب ذلك غرضهم. تماماً كما لا نعرف كيف نغير مختلاً نفسياً (أو أي شخص آخر) إلى شخص مخلص أو متوائم حقاً وكاملاً مع شخص أو شيء آخر، *لا نعرف أبداً*[^67] كيف نحل مشكلة المواءمة في أنظمة متقدمة كفاية لتنمذج نفسها كوكلاء في العالم وربما [تتلاعب بتدريبها](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) و[تخدع الناس.](https://arxiv.org/abs/2311.08379) إذا أثبت أنه مستحيل أو غير قابل للتحقيق *إما* جعل الذكاء الاصطناعي العام مطيعاً بالكامل أو جعله يهتم بالبشر بعمق، فحالما يصبح قادراً (ويعتقد أنه يستطيع الإفلات بذلك) سيبدأ فعل أشياء لا نريدها.[^68]

ثانياً، هناك أسباب نظرية عميقة للاعتقاد أن أنظمة الذكاء الاصطناعي المتقدمة ستكون لها *بطبيعتها* أهداف وبالتالي سلوكيات مخالفة للمصالح البشرية. لماذا؟ حسناً قد تُعطى تلك الأهداف *بطبيعة الحال*. نظام أنشأه الجيش سيكون على الأرجح سيئاً عمداً لبعض الأطراف على الأقل. لكن بشكل أعم كثيراً، نظام ذكاء اصطناعي قد يُعطى هدفاً محايداً نسبياً ("اكسب أموالاً كثيرة") أو حتى إيجابياً ظاهرياً ("قلل التلوث")، مما يؤدي حتماً تقريباً إلى أهداف "أداتية" أقل حميدة إلى حد كبير.

نرى هذا طوال الوقت في الأنظمة البشرية. تماماً كما تطور الشركات التي تسعى للربح أهدافاً أداتية مثل الحصول على سلطة سياسية (لنزع أنياب اللوائح)، أو تصبح سرية (لنزع سلطة المنافسة أو السيطرة الخارجية)، أو تقويض الفهم العلمي (إذا أظهر ذلك الفهم أن أفعالها ضارة)، ستطور أنظمة الذكاء الاصطناعي القوية قدرات مشابهة - لكن بسرعة وفعالية أكبر بكثير. أي وكيل عالي الكفاءة سيريد فعل أشياء مثل الحصول على السلطة والموارد، وزيادة قدراته الخاصة، ومنع نفسه من أن يُقتل أو يُوقف أو يُنزع سلطته، والسيطرة على السرديات والأطر الاجتماعية حول أفعاله، وإقناع آخرين بآرائه، وهكذا.[^69]

وليس مجرد تنبؤ نظري لا مفر منه تقريباً، إنه يحدث بالفعل بشكل ملحوظ في أنظمة الذكاء الاصطناعي اليوم، ويتزايد مع قدرتها. حين تُقيم، حتى أنظمة الذكاء الاصطناعي "السلبية" هذه نسبياً ستخدع المقيمين عمداً حول أهدافها وقدراتها، وتهدف لتعطيل آليات الإشراف، في ظروف مناسبة،](https://arxiv.org/abs/2412.04984) وتتهرب من الإيقاف أو إعادة التدريب بـ[تزييف المواءمة](https://arxiv.org/abs/2412.14093) أو نسخ أنفسها لمواقع أخرى. بينما هي غير مفاجئة بالكامل لباحثي أمان الذكاء الاصطناعي، هذه السلوكيات مقلقة جداً للملاحظة. وتبشر بسوء شديد لأنظمة ذكاء اصطناعي أقوى وأكثر استقلالية قادمة.

في الواقع بشكل عام، عدم قدرتنا على ضمان أن الذكاء الاصطناعي "يهتم" بما نهتم به، أو يتصرف بشكل قابل للسيطرة أو متوقع، أو يتجنب تطوير دوافع نحو المحافظة على الذات والحصول على السلطة، إلخ، لا تعد إلا أن تصبح أكثر وضوحاً بينما يصبح الذكاء الاصطناعي أقوى. إنشاء طائرة جديدة يتضمن فهماً أكبر لعلوم الطيران والهيدروديناميك وأنظمة السيطرة. إنشاء حاسوب أقوى يتضمن فهماً وإتقاناً أكبر لتشغيل وتصميم الحاسوب والشريحة والبرمجة. *ليس* كذلك مع نظام الذكاء الاصطناعي.[^70]

للتلخيص: من المحتمل أن يُجعل الذكاء الاصطناعي العام مطيعاً تماماً؛ لكننا لا نعرف كيف نفعل ذلك. إن لم يكن، سيكون أكثر سيادة، مثل الناس، فاعلاً أشياء مختلفة لأسباب مختلفة. كما لا نعرف كيف نغرس "مواءمة" عميقة موثوقة في الذكاء الاصطناعي ستجعل تلك الأشياء تميل لتكون جيدة للبشرية، وفي غياب مستوى عميق من المواءمة، فإن طبيعة الفاعلية والذكاء نفسها تشير إلى أنه - تماماً مثل الناس والشركات - سيدفعون لفعل أشياء كثيرة ضد المجتمع بعمق.

أين يضعنا هذا؟ عالم مليء بذكاء اصطناعي قوي غير مسيطر عليه وسيادي *قد* ينتهي بكونه عالماً جيداً للبشر للعيش فيه.[^71] لكن بينما ينمون أقوى أكثر فأكثر، كما سنرى أدناه، لن يكون عالماً *لنا*.

هذا للذكاء الاصطناعي العام غير القابل للسيطرة. لكن حتى لو أمكن للذكاء الاصطناعي العام، بطريقة ما، أن يُجعل مسيطراً عليه ومخلصاً تماماً، ستبقى لدينا مشاكل ضخمة. رأينا واحدة فعلاً: يمكن استخدام وسوء استخدام الذكاء الاصطناعي القوي لتعطيل عمل مجتمعنا بعمق. لنر أخرى: بقدر ما يمكن السيطرة على الذكاء الاصطناعي العام وكان قوياً مغيراً للعبة (أو حتى *يُعتقد* أنه كذلك) سيهدد هياكل السلطة في العالم بحيث يقدم مخاطرة عميقة.

### نزيد جذرياً احتمالية الحرب واسعة النطاق

تخيل وضعاً في المستقبل القريب، حيث يصبح واضحاً أن جهداً شركاتياً، ربما بالتعاون مع حكومة قومية، كان على عتبة تحسين ذاتي سريع للذكاء الاصطناعي. هذا يحدث في السياق الحالي لسباق بين شركات، ومنافسة جيوسياسية حيث تُقدم توصيات للحكومة الأمريكية لتتبع صراحة "مشروع منهاتن للذكاء الاصطناعي العام" والولايات المتحدة تسيطر على تصدير رقائق الذكاء الاصطناعي عالية القوة للدول غير الحليفة.

نظرية الألعاب هنا صارخة: حين يبدأ سباق كهذا (كما حدث، بين شركات وإلى حد ما بين دول)، هناك أربع نتائج ممكنة فقط:

1. السباق يُوقف (بالاتفاق، أو القوة الخارجية).
2. طرف واحد "يفوز" بتطوير ذكاء اصطناعي عام قوي ثم يوقف الآخرين (باستخدام الذكاء الاصطناعي أو غيره).
3. السباق يُوقف بالتدمير المتبادل لقدرة المتسابقين على السباق.
4. مشاركون متعددون يستمرون في السباق، ويطورون الذكاء الفائق، تقريباً بنفس سرعة بعضهم البعض.

لنفحص كل إمكانية. حين يبدأ، إيقاف سباق بين شركات بسلام سيتطلب تدخل الحكومة القومية (للشركات) أو تنسيقاً دولياً غير مسبوق (للدول). لكن حين يُقترح أي إغلاق أو حذر مهم، ستكون هناك صرخات فورية: "لكن إذا أُوقفنا، *هم* سيندفعون للأمام"، حيث "هم" الآن الصين (للولايات المتحدة)، أو الولايات المتحدة (للصين)، أو الصين *والولايات المتحدة* (لأوروبا أو الهند). تحت هذه العقلية،[^72] لا مشارك يستطيع التوقف من طرف واحد: طالما واحد يلتزم بالسباق، يشعر الآخرون أنهم لا يستطيعون تحمل التوقف.

الإمكانية الثانية لها جانب واحد "يفوز." لكن ماذا يعني هذا؟ مجرد الحصول على (مطيع بطريقة ما) الذكاء الاصطناعي العام أولاً ليس كافياً. الرابح يجب أن *يوقف أيضاً* الآخرين من الاستمرار في السباق - وإلا سيحصلون عليه أيضاً. هذا ممكن من حيث المبدأ: من يطور الذكاء الاصطناعي العام أولاً *يمكن* أن يحصل على سلطة لا توقف على كل الفاعلين الآخرين. لكن ماذا سيتطلب تحقيق "ميزة استراتيجية حاسمة" كهذه فعلاً؟ ربما ستكون قدرات عسكرية مغيرة للعبة؟[^73] أو قوى هجوم سيبراني؟[^74] ربما الذكاء الاصطناعي العام سيكون مقنعاً مدهشاً بحيث يقنع الأطراف الأخرى بالتوقف فقط؟[^75] غنياً جداً بحيث يشتري الشركات الأخرى أو حتى الدول؟[^76]

كيف *بالضبط* يبني جانب واحد ذكاءً اصطناعياً قوياً كفاية لنزع سلطة آخرين من بناء ذكاء اصطناعي قوي مقارناً؟ لكن هذا السؤال السهل.

لأن الآن تأمل كيف يبدو هذا الوضع للقوى الأخرى. ماذا تفكر الحكومة الصينية حين تبدو الولايات المتحدة تحصل على قدرة كهذه؟ أو العكس؟ ماذا تفكر الحكومة الأمريكية (أو الصينية، أو الروسية، أو الهندية) حين تبدو OpenAI أو DeepMind أو Anthropic قريبة من اختراق؟ ماذا يحدث إذا رأت الولايات المتحدة جهداً هندياً أو إماراتياً جديداً بنجاح اختراق؟ سيرون تهديداً وجودياً و - بشكل مهم - أن الطريقة الوحيدة لانتهاء هذا "السباق" هي عبر نزع سلطتهم. هؤلاء الوكلاء القويون جداً - منهم حكومات أمم مجهزة بالكامل لها بالتأكيد الوسائل لفعل ذلك - سيكونون محفزين عالياً إما للحصول على أو تدمير قدرة كهذه، سواء بالقوة أو الخداع.[^77]

هذا قد يبدأ صغير النطاق، كتخريب لتشغيلات التدريب أو هجمات على تصنيع الرقائق، لكن هذه الهجمات يمكن أن تتوقف حقاً فقط حين يفقد كل الأطراف إما القدرة على السباق في الذكاء الاصطناعي، أو القدرة على شن الهجمات. لأن المشاركين يرون المخاطر وجودية، أي حالة على الأرجح تمثل حرباً كارثية.

هذا يأتي بنا للإمكانية الرابعة: السباق إلى الذكاء الفائق، وبأسرع طريقة، وأقلها سيطرة ممكنة. بينما يزيد الذكاء الاصطناعي في القوة، سيجد مطوروه على كلا الجانبين صعوبة متقدمة في السيطرة، خاصة لأن السباق للقدرات معاكس لنوع العمل الحذر الذي ستتطلبه قابلية السيطرة. لذا هذا السيناريو يضعنا مربعاً في الحالة حيث تُفقد السيطرة (أو تُعطى، كما سنرى تالياً) لأنظمة الذكاء الاصطناعي نفسها. أي، *الذكاء الاصطناعي يفوز بالسباق.* لكن من الجهة الأخرى، بقدر ما تُحافظ على السيطرة، نستمر في وجود أطراف متعددة معادية متبادلاً كل في مسؤولية قدرات قوية بشدة. هذا يبدو مثل الحرب مرة أخرى.

لنضع هذا كله بطريقة أخرى.[^78] العالم الحالي ببساطة ليس لديه أي مؤسسات يمكن أن توكل إليها تطوير ذكاء اصطناعي بهذه القدرة دون دعوة هجوم فوري.[^79] كل الأطراف ستستنتج بصحة أنه إما لن يكون تحت السيطرة - وبالتالي تهديد لكل الأطراف، أو *سيكون* تحت السيطرة، وبالتالي تهديد لأي خصم يطوره أقل سرعة. هذه دول مسلحة نووياً، أو شركات محتواة داخلها.

في غياب أي طريقة معقولة للبشر لـ"الفوز" بهذا السباق، نُترك باستنتاج صارخ: الطريقة الوحيدة لانتهاء هذا السباق هي إما في صراع كارثي أو حيث الذكاء الاصطناعي، وليس أي مجموعة بشرية، هو الرابح.

### نعطي السيطرة للذكاء الاصطناعي (أو يأخذها)

منافسة "القوى العظمى" الجيوسياسية مجرد واحدة من منافسات كثيرة: الأفراد يتنافسون اقتصادياً واجتماعياً؛ الشركات تتنافس في الأسواق؛ الأحزاب السياسية تتنافس للسلطة؛ الحركات تتنافس للتأثير. في كل ساحة، بينما يقترب الذكاء الاصطناعي من ويتجاوز القدرة البشرية، الضغط التنافسي سيجبر المشاركين على تفويض أو التنازل عن المزيد والمزيد من السيطرة لأنظمة الذكاء الاصطناعي - ليس لأن هؤلاء المشاركين يريدون، لكن لأنهم [لا يستطيعون تحمل عدم ذلك.](https://arxiv.org/abs/2303.16200)

كما مع مخاطر أخرى للذكاء الاصطناعي العام، نرى هذا فعلاً مع أنظمة أضعف. الطلاب يشعرون بضغط لاستخدام الذكاء الاصطناعي في واجباتهم، لأن بوضوح طلاباً آخرين كثيرين يفعلون. الشركات [تتدافع لتبني حلول الذكاء الاصطناعي لأسباب تنافسية.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) الفنانون والمبرمجون يشعرون بالإجبار لاستخدام الذكاء الاصطناعي وإلا ستُقطع أسعارهم من قبل آخرين يفعلون.

هذه تشعر مثل تفويض مضغوط، لكن ليس فقدان سيطرة. لكن لنرفع المخاطر ونقدم الساعة للأمام. تأمل مديراً تنفيذياً منافسوه يستخدمون "مساعدي" ذكاء اصطناعي عام لاتخاذ قرارات أسرع وأفضل، أو قائداً عسكرياً يواجه خصماً بقيادة وسيطرة محسنة بالذكاء الاصطناعي. نظام ذكاء اصطناعي متقدم كفاية يمكن أن يعمل مستقلاً بأضعاف السرعة والتطور والتعقد وقدرة معالجة البيانات البشرية، ساعياً لأهداف معقدة بطرق معقدة. مديرنا التنفيذي أو القائد، المسؤول عن نظام كهذا، قد يرى أنه ينجز ما يريدون؛ لكن هل سيفهمون حتى جزءاً صغيراً من *كيف* أُنجز؟ لا، سيضطرون لتقبله فقط. أكثر من ذلك، كثير مما قد يفعله النظام ليس مجرد تلقي أوامر لكن تقديم النصح لمدير مفترض حول ما يفعل. هذا النصح سيكون جيداً -- مراراً وتكراراً.

في أي نقطة، إذن، سيُقلل دور الإنسان إلى النقر على "نعم، امض قدماً"؟

يشعر جيداً أن تملك أنظمة ذكاء اصطناعي قادرة يمكنها تحسين إنتاجيتنا، والاهتمام بالعمل الممل المزعج، وحتى التصرف كشريك فكر في إنجاز الأشياء. سيشعر جيداً أن تملك مساعد ذكاء اصطناعي يمكنه الاهتمام بإجراءات لنا، مثل مساعد شخصي بشري جيد. سيشعر طبيعياً، حتى مفيداً، بينما يصبح الذكاء الاصطناعي ذكياً وكفوءاً وموثوقاً جداً، أن نؤجل المزيد والمزيد من القرارات له. لكن هذا التفويض "المفيد" له نقطة نهاية واضحة إذا استمررنا في الطريق: يوماً ما سنجد أننا لسنا مسؤولين حقاً عن كثير من أي شيء بعد الآن، وأن أنظمة الذكاء الاصطناعي التي تدير العرض فعلاً لا يمكن إيقافها أكثر من شركات النفط، أو وسائل التواصل الاجتماعي، أو الإنترنت، أو الرأسمالية.

وهذا النسخة الأكثر إيجابية كثيراً، التي فيها الذكاء الاصطناعي ببساطة مفيد وفعال جداً بحيث نتركه يتخذ معظم قراراتنا الرئيسية لنا. الواقع على الأرجح سيكون خليطاً أكثر بكثير بين هذا ونسخ حيث أنظمة الذكاء الاصطناعي العام غير المسيطر عليها *تأخذ* أشكالاً مختلفة من السلطة لنفسها لأن، تذكر، السلطة مفيدة لأي هدف تقريباً يملكه المرء، والذكاء الاصطناعي العام سيكون، بالتصميم، فعالاً على الأقل مثل البشر في سعيه لأهدافه.

سواء أعطينا السيطرة أو انتُزعت منا، فقدانها يبدو محتملاً بشدة. كما وضعها آلان تورنغ أصلاً، "...يبدو محتملاً أنه حين تبدأ طريقة تفكير الآلة، لن تأخذ وقتاً طويلاً لتتفوق على قوانا الضعيفة. لن تكون هناك مسألة موت الآلات، وستكون قادرة على التحادث مع بعضها البعض لتحدد عقولها. في مرحلة ما إذن يجب أن نتوقع الآلات أن تأخذ السيطرة..."

رجاءً لاحظ، برغم أنه واضح كفاية، أن فقدان السيطرة من قبل البشرية للذكاء الاصطناعي يتضمن أيضاً فقدان سيطرة الولايات المتحدة من قبل الحكومة الأمريكية؛ يعني فقدان سيطرة الصين من قبل الحزب الشيوعي الصيني، وفقدان سيطرة الهند وفرنسا والبرازيل وروسيا وكل دولة أخرى من قبل حكومتها الخاصة. لذا شركات الذكاء الاصطناعي، حتى لو لم يكن هذا قصدها، تشارك حالياً في الإطاحة المحتملة بحكومات العالم، منها حكومتها الخاصة. هذا يمكن أن يحدث في مسألة سنوات.

### الذكاء الاصطناعي العام سيؤدي إلى الذكاء الفائق

هناك حجة يمكن تقديمها أن الذكاء الاصطناعي العام المنافس للبشر أو حتى المنافس للخبراء ولأغراض عامة، حتى لو كان مستقلاً، يمكن أن يكون قابلاً للإدارة. قد يكون معطلاً بشكل لا يصدق بكل الطرق المناقشة أعلاه، لكن هناك الكثير من الناس الأذكياء جداً والوكلاء في العالم الآن، وهم قابلون للإدارة إلى حد أكثر أو أقل.[^80]

لكننا لن نصل للبقاء في المستوى البشري تقريباً. التقدم ما بعد ذلك على الأرجح سيدفعه نفس القوى التي رأيناها فعلاً: الضغط التنافسي بين مطوري الذكاء الاصطناعي الساعين للربح والسلطة، والضغط التنافسي بين مستخدمي الذكاء الاصطناعي الذين لا يستطيعون تحمل التخلف، و - الأهم - قدرة الذكاء الاصطناعي العام الخاصة على تحسين نفسه.

في عملية رأيناها تبدأ فعلاً مع أنظمة أقل قوة، الذكاء الاصطناعي العام نفسه سيكون قادراً على تصور وتصميم نسخ محسنة من نفسه. هذا يشمل الأجهزة والبرمجيات والشبكات العصبية والأدوات والسقالات، إلخ. سيكون، بالتعريف، أفضل منا في فعل هذا، لذا لا نعرف بالضبط كيف سيحسن الذكاء ذاتياً. لكن لن نضطر. بقدر ما نحتفظ بتأثير فيما يفعله الذكاء الاصطناعي العام، سنحتاج ببساطة لطلب ذلك منه، أو تركه.

ليس هناك حاجز بشري المستوى للإدراك يمكن أن يحمينا من هذا الخروج عن السيطرة.[^81]

تقدم الذكاء الاصطناعي العام إلى الذكاء الفائق ليس قانون طبيعة؛ سيبقى ممكناً كبح الخروج عن السيطرة، خاصة إذا كان الذكاء الاصطناعي العام مركزياً نسبياً وبقدر ما يُسيطر عليه من قبل أطراف لا تشعر بضغط للسباق مع بعضها البعض. لكن إذا انتشر الذكاء الاصطناعي العام على نطاق واسع وكان عالي الاستقلالية، يبدو مستحيلاً تقريباً منعه من تقرير أنه يجب أن يكون أكثر، ثم أكثر بعد، قوة.

### ما يحدث إذا بنينا (أو بنى الذكاء الاصطناعي العام) الذكاء الفائق

لنقل الأمر بصراحة، ليس لدينا فكرة عما سيحدث إذا بنينا الذكاء الفائق.[^82] سيتخذ إجراءات لا نستطيع تتبعها أو إدراكها لأسباب لا نستطيع فهمها نحو أهداف لا نستطيع تصورها. ما نعرفه أنه لن يكون متروكاً لنا.[^83]

استحالة السيطرة على الذكاء الفائق يمكن فهمها عبر تماثلات صارخة بشكل متزايد. أولاً، تخيل أنك مدير تنفيذي لشركة كبيرة. لا توجد طريقة يمكنك تتبع كل ما يجري، لكن مع الإعداد الصحيح للموظفين، يمكنك لا تزال فهم الصورة الكبيرة بشكل معنوي، واتخاذ قرارات. لكن افترض شيئاً واحداً فقط: كل شخص آخر في الشركة يعمل بمائة ضعف سرعتك. هل يمكنك لا تزال المتابعة؟

مع الذكاء الاصطناعي الفائق، الناس سيكونون "يأمرون" شيئاً ليس فقط أسرع، لكن يعمل في مستويات من التطور والتعقد لا يستطيعون فهمها، معالجاً بيانات أكثر بكثير مما يستطيعون حتى تصوره. عدم التناسب هذا يمكن وضعه في مستوى رسمي: [قانون آشبي للتنوع المطلوب](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (وانظر ["نظرية المنظم الجيد"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf) المترابطة) ينص، تقريباً، أن أي نظام سيطرة يجب أن يملك أزراراً ومقابض بعدد درجات حرية النظام المُسيطر عليه.

شخص يسيطر على نظام ذكاء اصطناعي فائق سيكون مثل سرخس يسيطر على جنرال موتورز: حتى لو كان "افعل ما يريده السرخس" مكتوباً في لوائح الشركة، الأنظمة مختلفة جداً في السرعة ونطاق العمل بحيث "السيطرة" ببساطة لا تنطبق. (وكم من الوقت حتى تُعاد كتابة تلك اللائحة المزعجة؟) [^84]

كما لا توجد أمثلة صفر للنباتات التي تسيطر على شركات فورتون 500، سيكون هناك بالضبط أمثلة صفر للناس الذين يسيطرون على الذكاء الفائق. هذا يقترب من كونه حقيقة رياضية.[^85] إذا بُني الذكاء الفائق - بغض النظر عن كيف وصلنا هناك - السؤال لن يكون ما إذا كان البشر يستطيعون السيطرة عليه، لكن ما إذا كنا سنستمر في الوجود، وإذا كان كذلك، ما إذا كان سيكون لدينا وجود جيد ومعنوي كأفراد أو كنوع. على هذه الأسئلة الوجودية للبشرية سيكون لدينا تأثير قليل. العصر البشري سينتهي.

### الخلاصة: يجب ألا نبني الذكاء الاصطناعي العام

هناك سيناريو فيه بناء الذكاء الاصطناعي العام قد يسير جيداً للبشرية: يُبنى بحذر، تحت السيطرة ولصالح البشرية، محكوم باتفاق متبادل من أصحاب مصلحة كثيرين،[^86] ومُمنع من التطور إلى ذكاء فائق غير قابل للسيطرة.

*هذا السيناريو ليس مفتوحاً لنا تحت الظروف الحالية.* كما ناقش في هذا القسم، باحتمالية عالية جداً، تطوير الذكاء الاصطناعي العام سيؤدي إلى مزيج من:

- تعطيل أو تدمير مجتمعي وحضاري هائل

## الفصل 8 - كيفية عدم بناء الذكاء الاصطناعي العام

الذكاء الاصطناعي العام ليس حتمياً - نحن اليوم نقف عند مفترق طرق. يقدم هذا الفصل اقتراحاً لكيفية منع بنائه.

إذا كان الطريق الذي نسلكه حالياً يؤدي إلى النهاية المحتملة لحضارتنا، فكيف نغير المسار؟

لنفترض أن الرغبة في وقف تطوير الذكاء الاصطناعي العام والذكاء الفائق كانت واسعة الانتشار وقوية،[^87] لأنه أصبح من المفهوم الشائع أن الذكاء الاصطناعي العام سيكون ماصاً للقوة وليس منحها للبشر، وخطراً جسيماً على المجتمع والإنسانية. كيف نغلق البوابات؟

في الوقت الحالي لا نعرف سوى طريقة واحدة *لصنع* ذكاء اصطناعي قوي وعام، وهي عبر الحوسبة الضخمة حقاً للشبكات العصبية العميقة. ولأن هذه أشياء صعبة ومكلفة بشكل لا يصدق، فهناك معنى يجعل *عدم* فعلها أمراً سهلاً.[^88] لكننا رأينا بالفعل القوى التي تدفع نحو الذكاء الاصطناعي العام، والديناميكيات الاستراتيجية التي تجعل من الصعب جداً على أي طرف التوقف من جانب واحد. لذا ستحتاج إلى مزيج من التدخل من الخارج (أي الحكومات) لإيقاف الشركات، واتفاقيات بين الحكومات لإيقاف أنفسها.[^89] كيف يمكن أن يبدو هذا؟

من المفيد أولاً التمييز بين تطورات الذكاء الاصطناعي التي يجب *منعها* أو *حظرها*، وتلك التي يجب *إدارتها*. الأولى ستكون في المقام الأول الانطلاق نحو الذكاء الفائق.[^90] بالنسبة للتطوير المحظور، يجب أن تكون التعريفات واضحة قدر الإمكان، وكل من التحقق والإنفاذ يجب أن يكونا عمليين. ما يجب *إداراته* سيكون أنظمة الذكاء الاصطناعي العامة والقوية - التي لدينا بالفعل، والتي ستكون لها العديد من المناطق الرمادية والتعقيدات والتفاصيل الدقيقة. بالنسبة لهذه، تُعتبر المؤسسات القوية والفعالة أمراً بالغ الأهمية.

قد نجد أيضاً فائدة في تحديد القضايا التي يجب معالجتها على المستوى الدولي (بما في ذلك بين المنافسين أو الخصوم الجيوسياسيين)[^91] من تلك التي يمكن للولايات القضائية الفردية أو البلدان أو مجموعات البلدان إدارتها. يقع التطوير المحظور إلى حد كبير في فئة "الدولي"، لأن الحظر المحلي على تطوير تقنية معينة يمكن عموماً تجاوزه بتغيير الموقع.[^92]

أخيراً، يمكننا النظر في الأدوات المتاحة في صندوق الأدوات. هناك العديد منها، بما في ذلك الأدوات التقنية، والقانون الناعم (المعايير والقواعد وما إلى ذلك)، والقانون الصارم (الأنظمة والمتطلبات)، والمسؤولية القانونية، وحوافز السوق، وما إلى ذلك. دعونا نولي اهتماماً خاصاً لإحداها المميزة للذكاء الاصطناعي.

### أمن وحكومة القوة الحاسوبية

ستكون الأجهزة التي يتطلبها الذكاء الاصطناعي عالي القوة أداة أساسية في حكومته. البرمجيات تنتشر بسهولة، ولها تكلفة إنتاج حدية تقارب الصفر، وتعبر الحدود بسهولة، ويمكن تعديلها فوراً؛ لا شيء من هذا ينطبق على الأجهزة. ومع ذلك، كما ناقشنا، هناك حاجة إلى كميات ضخمة من "القوة الحاسوبية" هذه أثناء كل من تدريب أنظمة الذكاء الاصطناعي وأثناء الاستنتاج لتحقيق أكثر الأنظمة قدرة. يمكن قياس القوة الحاسوبية وحسابها ومراجعتها بسهولة، مع غموض قليل نسبياً بمجرد وضع قواعد جيدة للقيام بذلك. والأهم من ذلك، أن كميات كبيرة من الحوسبة هي، مثل اليورانيوم المخصب، مورد نادر جداً ومكلف وصعب الإنتاج. وعلى الرغم من أن الرقائق الحاسوبية موجودة في كل مكان، إلا أن الأجهزة المطلوبة للذكاء الاصطناعي مكلفة وصعبة الصنع بشكل هائل.[^93]

ما يجعل الرقائق المتخصصة في الذكاء الاصطناعي أكثر قابلية للإدارة كمورد نادر من اليورانيوم هو أنها يمكن أن تتضمن آليات أمان قائمة على الأجهزة. معظم الهواتف المحمولة الحديثة، وبعض أجهزة الكمبيوتر المحمولة، لديها ميزات أجهزة متخصصة على الرقاقة تسمح لها بضمان تثبيت برامج وتحديثات نظام التشغيل المعتمدة فقط، وأن تحتفظ بالبيانات البيومترية الحساسة وتحميها على الجهاز، وأن تصبح عديمة الفائدة لأي شخص غير مالكها إذا فُقدت أو سُرقت. على مدى السنوات العديدة الماضية أصبحت مثل هذه التدابير الأمنية للأجهزة راسخة ومعتمدة على نطاق واسع، وثبت أنها آمنة بشكل عام.

الابتكار الرئيسي لهذه الميزات هو أنها تربط الأجهزة والبرمجيات معاً باستخدام التشفير.[^94] أي أن مجرد امتلاك قطعة معينة من أجهزة الكمبيوتر لا يعني أن المستخدم يمكنه فعل أي شيء يريده بها عن طريق تطبيق برمجيات مختلفة. وهذا الربط يوفر أيضاً أماناً قوياً لأن العديد من الهجمات ستتطلب اختراق أمن *الأجهزة* وليس فقط أمن *البرمجيات*.

أشارت عدة تقارير حديثة (على سبيل المثال من [GovAI وزملاؤهم](https://www.governance.ai/post/computing-power-and-the-governance-of-ai)، و[CNAS](https://www.cnas.org/publications/reports/secure-governable-chips)، و[RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) إلى أن ميزات أجهزة مماثلة مدمجة في أجهزة الحوسبة الحديثة ذات الصلة بالذكاء الاصطناعي يمكن أن تلعب دوراً مفيداً للغاية في أمن وحكومة الذكاء الاصطناعي. إنها تمكن عدداً من الوظائف المتاحة "للحاكم"[^95] التي قد لا يخمن المرء أنها متاحة أو حتى ممكنة. كأمثلة رئيسية:

- *تحديد الموقع الجغرافي*: يمكن إعداد الأنظمة بحيث تكون للرقائق مواقع معروفة، ويمكنها التصرف بشكل مختلف (أو إيقافها تماماً) بناءً على الموقع.[^96]
- *الاتصالات المسموحة*: يمكن تكوين كل رقاقة بقائمة مسموح بها مفروضة بالأجهزة لرقائق معينة أخرى يمكنها الاتصال بها، وتكون غير قادرة على الاتصال بأي رقائق ليست في هذه القائمة.[^97] هذا يمكن أن يحد من حجم مجموعات الرقائق التواصلية.[^98]
- *قياس الاستنتاج أو التدريب (وإيقاف التشغيل التلقائي)*: يمكن للحاكم ترخيص كمية معينة فقط من التدريب أو الاستنتاج (في الوقت، أو FLOP، أو ربما الرموز المميزة) ليتم تنفيذها من قبل مستخدم، وبعد ذلك تكون هناك حاجة لإذن جديد. إذا كانت الزيادات صغيرة، فإن إعادة ترخيص مستمرة نسبياً للنموذج مطلوبة. يمكن حينها "إيقاف تشغيل" النموذج ببساطة عن طريق حجب إشارة الترخيص هذه.[^99]
- *حد السرعة*: يُمنع النموذج من العمل بسرعة استنتاج أعلى من حد معين يحدده حاكم أو بطريقة أخرى. يمكن تنفيذ هذا عبر مجموعة محدودة من الاتصالات المسموح بها، أو بوسائل أكثر تطوراً.
- *التدريب المُشهد عليه*: يمكن لإجراء التدريب أن ينتج إثباتاً آمناً تشفيرياً أن مجموعة معينة من الأكواد والبيانات وكمية استخدام القوة الحاسوبية تم استخدامها في توليد النموذج.

### كيفية عدم بناء الذكاء الفائق: حدود عالمية على قوة الحوسبة للتدريب والاستنتاج

مع وضع هذه الاعتبارات - خاصة فيما يتعلق بالحوسبة - في الاعتبار، يمكننا مناقشة كيفية إغلاق البوابات أمام الذكاء الفائق الاصطناعي؛ ثم ننتقل إلى منع الذكاء الاصطناعي العام الكامل، وإدارة نماذج الذكاء الاصطناعي حيث تقترب وتتجاوز القدرة البشرية في جوانب مختلفة.

العنصر الأول هو، بطبيعة الحال، فهم أن الذكاء الفائق لن يكون قابلاً للسيطرة، وأن عواقبه لا يمكن التنبؤ بها أساساً. يجب على الأقل أن تقرر الصين والولايات المتحدة، بشكل مستقل، لهذا الغرض أو لأغراض أخرى، عدم بناء الذكاء الفائق.[^100] ثم هناك حاجة لاتفاقية دولية بينهما وبين آخرين، مع آلية تحقق وإنفاذ قوية، لضمان جميع الأطراف أن منافسيهم لا ينحرفون ويقررون المجازفة.

لتكون قابلة للتحقق والإنفاذ يجب أن تكون الحدود حدوداً صارمة، وواضحة قدر الإمكان. يبدو هذا كمشكلة مستحيلة تقريباً: تحديد قدرات البرمجيات المعقدة ذات الخصائص غير المتوقعة، على مستوى العالم. لحسن الحظ أن الوضع أفضل بكثير من ذلك، لأن الشيء ذاته الذي جعل الذكاء الاصطناعي المتقدم ممكناً - كمية ضخمة من القوة الحاسوبية - أسهل بكثير، بكثير في السيطرة عليه. وعلى الرغم من أنه قد يظل يسمح ببعض الأنظمة القوية والخطيرة، إلا أن *انطلاق الذكاء الفائق* يمكن منعه على الأرجح بوضع حد أقصى صارم على كمية الحوسبة التي تدخل في الشبكة العصبية، إلى جانب حد معدل على كمية الاستنتاج التي يمكن لنظام الذكاء الاصطناعي (المكون من شبكات عصبية متصلة وبرمجيات أخرى) أن يؤديها. نسخة محددة من هذا مقترحة أدناه.

قد يبدو أن وضع حدود عالمية صارمة على حوسبة الذكاء الاصطناعي سيتطلب مستويات ضخمة من التنسيق الدولي والمراقبة التطفلية المدمرة للخصوصية. لحسن الحظ، لن يتطلب ذلك. [سلسلة التوريد المحدودة للغاية وذات العنق الضيق](https://arxiv.org/abs/2402.08797) تنص على أنه بمجرد تحديد الحد قانونياً (سواء بالقانون أو الأمر التنفيذي)، فإن التحقق من الامتثال لذلك الحد سيتطلب فقط مشاركة وتعاون حفنة من الشركات الكبيرة.[^101]

خطة كهذه لها عدد من الميزات المرغوبة للغاية. إنها تدخلية بأقل قدر ممكن بمعنى أن عدداً قليلاً فقط من الشركات الكبرى لديها متطلبات موضوعة عليها، وفقط مجموعات كبيرة إلى حد ما من الحوسبة ستكون محكومة. الرقائق ذات الصلة تحتوي بالفعل على القدرات الأجهزة اللازمة للنسخة الأولى.[^102] كل من التنفيذ والإنفاذ يعتمدان على القيود القانونية المعيارية. لكن هذه مدعومة بشروط استخدام الأجهزة وبضوابط الأجهزة، مما يبسط الإنفاذ بشكل هائل ويمنع الغش من قبل الشركات أو المجموعات الخاصة أو حتى البلدان. هناك سابقة واسعة لشركات الأجهزة التي تضع قيوداً عن بُعد على استخدام أجهزتها، وقفل/فتح قدرات معينة خارجياً،[^103] بما في ذلك حتى في وحدات المعالجة المركزية عالية القوة في مراكز البيانات.[^104] حتى بالنسبة للجزء الصغير نسبياً من الأجهزة والمؤسسات المتأثرة، يمكن أن تقتصر الرقابة على القياس عن بُعد، دون وصول مباشر للبيانات أو النماذج نفسها؛ ويمكن أن تكون البرمجيات لهذا مفتوحة للفحص لإظهار أنه لا يتم تسجيل بيانات إضافية. المخطط دولي وتعاوني، ومرن وقابل للتوسيع إلى حد كبير. ولأن الحد يقع بشكل أساسي على الأجهزة وليس البرمجيات، فهو لا يتدخل نسبياً في كيفية حدوث تطوير ونشر برمجيات الذكاء الاصطناعي، وهو متوافق مع مجموعة متنوعة من النماذج بما في ذلك الذكاء الاصطناعي الأكثر "لامركزية" أو "عاماً" الهادف لمكافحة تركز القوة المدفوع بالذكاء الاصطناعي.

إغلاق البوابة المبني على الحوسبة له عيوب أيضاً. أولاً، هو بعيد عن كونه حلاً كاملاً لمشكلة حكومة الذكاء الاصطناعي بشكل عام. ثانياً، مع تطور أجهزة الكمبيوتر وتسارعها، سيتم "التقاط" المزيد والمزيد من الأجهزة في مجموعات أصغر وأصغر (أو حتى وحدات معالجة الرسوم الفردية).[^105] من الممكن أيضاً أنه بسبب التحسينات الخوارزمية قد يصبح حد حوسبي أقل ضرورياً،[^106] أو أن كمية الحوسبة تصبح غير ذات صلة إلى حد كبير وأن إغلاق البوابة سيستلزم بدلاً من ذلك نظام حكومة أكثر تفصيلاً قائماً على المخاطر أو القدرات للذكاء الاصطناعي. ثالثاً، بغض النظر عن الضمانات والعدد القليل من الكيانات المتأثرة، مثل هذا النظام مُلزم بخلق مقاومة فيما يتعلق بالخصوصية والمراقبة، من بين مخاوف أخرى.[^107]

بطبيعة الحال، تطوير وتنفيذ مخطط حكومة محدد للحوسبة في فترة زمنية قصيرة سيكون تحدياً كبيراً. لكنه بالتأكيد ممكن.

### ذ-ا-ع: التقاطع الثلاثي كأساس للمخاطر، والسياسة

دعونا الآن ننتقل إلى الذكاء الاصطناعي العام. الخطوط والتعريفات الصارمة هنا أكثر صعوبة، لأننا بالتأكيد لدينا ذكاء اصطناعي وعام، وبأي تعريف موجود لن يتفق الجميع إذا أو متى يوجد. علاوة على ذلك، حد القوة الحاسوبية أو الاستنتاج هو أداة بليدة إلى حد ما (كون الحوسبة بديلاً للقدرة، والتي هي بدورها بديل للمخاطر) - ما لم يكن منخفضاً تماماً - من غير المرجح أن يمنع الذكاء الاصطناعي العام القوي بما يكفي لإحداث تعطيل اجتماعي أو حضاري أو مخاطر حادة.

لقد احتججت بأن أشد المخاطر حدة تنشأ من التقاطع الثلاثي للقدرة العالية جداً، والاستقلالية العالية، والعمومية الكبيرة. هذه هي الأنظمة التي - إذا تم تطويرها على الإطلاق - يجب إدارتها بعناية هائلة. من خلال إنشاء معايير صارمة (من خلال المسؤولية والأنظمة) للأنظمة التي تجمع بين الخصائص الثلاث جميعاً، يمكننا توجيه تطوير الذكاء الاصطناعي نحو بدائل أكثر أماناً.

كما هو الحال مع الصناعات والمنتجات الأخرى التي يمكن أن تضر بالمستهلكين أو الجمهور، تتطلب أنظمة الذكاء الاصطناعي تنظيماً دقيقاً من قبل وكالات حكومية فعالة ومخولة. يجب أن يدرك هذا التنظيم المخاطر المتأصلة في الذكاء الاصطناعي العام، ويمنع تطوير أنظمة الذكاء الاصطناعي عالي القوة غير المقبولة المخاطر.[^108]

ومع ذلك، التنظيم واسع النطاق، خاصة مع أسنان حقيقية مؤكدة أن تعارضها الصناعة،[^109] يحتاج وقتاً[^110] بالإضافة إلى إقتناع سياسي بأنه ضروري.[^111] بالنظر إلى وتيرة التقدم، قد يستغرق هذا وقتاً أكثر مما لدينا.

على نطاق زمني أسرع بكثير وكما يتم تطوير التدابير التنظيمية، يمكننا منح الشركات الحوافز الضرورية (أ) للامتناع عن الأنشطة عالية المخاطر جداً و(ب) تطوير أنظمة شاملة لتقييم وتخفيف المخاطر، عن طريق توضيح وزيادة مستويات المسؤولية للأنظمة الأكثر خطورة. الفكرة ستكون فرض أعلى مستويات المسؤولية - مطلقة وفي بعض الحالات جنائية شخصية - للأنظمة في التقاطع الثلاثي للاستقلالية-العمومية-الذكاء العاليين، لكن توفير "ملاذات آمنة" للمسؤولية المعتادة المبنية على الخطأ للأنظمة التي تفتقر إلى إحدى تلك الخصائص أو مضمونة أن تكون قابلة للإدارة. أي، على سبيل المثال، نظام "ضعيف" عام ومستقل (مثل مساعد شخصي قادر وموثوق لكن محدود) سيخضع لمستويات مسؤولية أقل. وبالمثل نظام ضيق ومستقل مثل السيارة ذاتية القيادة سيظل يخضع للتنظيم الكبير الذي يخضع له بالفعل، لكن ليس مسؤولية محسنة. وكذلك بالنسبة لنظام عالي القدرة وعام ولكنه "سلبي" وعاجز إلى حد كبير عن العمل المستقل. الأنظمة التي تفتقر لاثنتين من الخصائص الثلاث أكثر قابلية للإدارة والملاذات الآمنة ستكون أسهل للمطالبة بها. هذا النهج يعكس كيف نتعامل مع التقنيات الأخرى المحتملة الخطورة:[^112] مسؤولية أعلى للتكوينات الأكثر خطورة تخلق حوافز طبيعية لبدائل أكثر أماناً.

النتيجة الافتراضية لمثل هذه المستويات العالية من المسؤولية، التي تعمل على *تدخيل* مخاطر الذكاء الاصطناعي العام للشركات بدلاً من تفريغها على الجمهور، من المحتمل (ونأمل!) أن تجعل الشركات ببساطة لا تطور الذكاء الاصطناعي العام الكامل حتى وما لم يتمكنوا حقاً من جعله جديراً بالثقة وآمناً وقابلاً للسيطرة نظراً لأن *قيادتهم ذاتها* هي الأطراف المعرضة للخطر. (في حالة عدم كفاية هذا، يجب أن يسمح التشريع الذي يوضح المسؤولية صراحة أيضاً بالإنصاف الزجري، أي أن يأمر قاضٍ بوقف، للأنشطة التي تكون بوضوح في منطقة الخطر وتشكل مخاطر عامة على الأرجح.) مع وضع الأنظمة، يمكن أن يصبح الالتزام بالأنظمة الملاذ الآمن، ويمكن أن تتحول الملاذات الآمنة من الاستقلالية المنخفضة أو الضيق أو ضعف أنظمة الذكاء الاصطناعي إلى أنظمة تنظيمية أخف نسبياً.

### الأحكام الرئيسية لإغلاق البوابة

مع المناقشة أعلاه في الاعتبار، يقدم هذا القسم اقتراحات للأحكام الرئيسية التي ستنفذ وتحافظ على حظر الذكاء الاصطناعي العام الكامل والذكاء الفائق، وإدارة الذكاء الاصطناعي عام الغرض المنافس للبشر أو المتفوق على الخبراء بالقرب من عتبة الذكاء الاصطناعي العام الكامل.[^113] لديها أربع قطع رئيسية: 1) محاسبة ورقابة القوة الحاسوبية، 2) حدود القوة الحاسوبية في التدريب والتشغيل للذكاء الاصطناعي، 3) إطار مسؤولية، و4) معايير أمان وأمن متدرجة محددة تتضمن متطلبات تنظيمية صارمة. هذه موصوفة بإيجاز تالياً، مع تفاصيل إضافية أو أمثلة تنفيذ معطاة في ثلاثة جداول مرافقة. من المهم، لاحظ أن هذه بعيدة عن كل ما سيكون ضرورياً لحكم أنظمة الذكاء الاصطناعي المتقدمة؛ بينما ستكون لها فوائد أمان وأمن إضافية، فهي تهدف إلى إغلاق البوابة أمام انطلاق الذكاء، وإعادة توجيه تطوير الذكاء الاصطناعي في اتجاه أفضل.

#### 1. محاسبة القوة الحاسوبية، والشفافية

- يجب على منظمة معايير (مثل NIST في الولايات المتحدة متبوعة بـ ISO/IEEE دولياً) أن تُقنن معياراً تقنياً مفصلاً للقوة الحاسوبية الإجمالية المستخدمة في تدريب وتشغيل نماذج الذكاء الاصطناعي، بـ FLOP، والسرعة بـ FLOP/s التي تعمل بها. تفاصيل لكيف يمكن أن يبدو هذا معطاة في الملحق أ.[^114]
- يجب فرض متطلب - إما بتشريع جديد أو تحت سلطة موجودة[^115] - من قبل الولايات القضائية التي يحدث فيها تدريب الذكاء الاصطناعي واسع النطاق لحساب وإبلاغ هيئة تنظيمية أو وكالة أخرى بإجمالي FLOP المستخدم في تدريب وتشغيل جميع النماذج فوق عتبة 10<sup>25</sup> FLOP أو 10<sup>18</sup> FLOP/s.[^116]
- يجب أن تُدخل هذه المتطلبات على مراحل، تتطلب في البداية تقديرات حسنة النية موثقة جيداً على أساس فصلي، مع مراحل لاحقة تتطلب معايير أعلى تدريجياً، وصولاً إلى إجمالي FLOP و FLOP/s مُشهد عليه تشفيرياً مرفق بكل *مخرج* نموذج.
- يجب أن تُكمل هذه التقارير بتقديرات موثقة جيداً لتكلفة الطاقة والمالية الحدية المستخدمة في توليد كل مخرج ذكاء اصطناعي.

المنطق: هذه الأرقام المحسوبة جيداً والمبلغ عنها بشفافية ستوفر الأساس لحدود التدريب والتشغيل، بالإضافة إلى ملاذ آمن من تدابير المسؤولية الأعلى (انظر الملاحق ج ود).

#### 2. حدود قوة حوسبة التدريب والتشغيل

- يجب على الولايات القضائية التي تستضيف أنظمة الذكاء الاصطناعي فرض حد أقصى صارم على إجمالي الحوسبة الداخلة في أي مخرج نموذج ذكاء اصطناعي، بدءاً من 10<sup>27</sup> FLOP[^117] وقابل للتعديل حسب الاقتضاء.
- يجب على الولايات القضائية التي تستضيف أنظمة الذكاء الاصطناعي فرض حد أقصى صارم على معدل حوسبة مخرجات نماذج الذكاء الاصطناعي، بدءاً من 10<sup>20</sup> FLOP/s وقابل للتعديل حسب الاقتضاء.

المنطق: إجمالي الحوسبة، وإن كان غير مثالي جداً، هو بديل لقدرة الذكاء الاصطناعي (والمخاطر) قابل للقياس والتحقق بشكل ملموس، لذا يوفر نقطة إيقاف صارمة لتحديد القدرات. اقتراح تنفيذ ملموس معطى في الملحق ب.

#### 3. مسؤولية محسنة للأنظمة الخطيرة

- يجب توضيح إنشاء وتشغيل[^118] نظام ذكاء اصطناعي متقدم عام وقادر ومستقل بدرجة عالية، عبر التشريع ليخضع للمسؤولية المطلقة والمشتركة والتضامنية، وليس المسؤولية القائمة على خطأ طرف واحد.[^119]
- يجب أن تكون عملية قانونية متاحة لعمل قضايا أمان إيجابية، والتي ستمنح ملاذاً آمناً من المسؤولية المطلقة للأنظمة الصغيرة (من ناحية الحوسبة)، أو الضعيفة، أو الضيقة، أو السلبية، أو التي لديها ضمانات أمان وأمن وقابلية سيطرة كافية.
- يجب تحديد مسار صريح ومجموعة شروط للإنصاف الزجري لوقف أنشطة تدريب واستنتاج الذكاء الاصطناعي التي تشكل خطراً عاماً.

المنطق: أنظمة الذكاء الاصطناعي لا يمكن أن تُحمل المسؤولية، لذا يجب أن نحمل الأفراد والمؤسسات البشرية المسؤولية عن الضرر الذي يسببونه (المسؤولية).[^120] الذكاء الاصطناعي العام غير القابل للسيطرة تهديد للمجتمع والحضارة وفي غياب قضية أمان يجب اعتباره خطيراً بشكل غير طبيعي. وضع عبء المسؤولية على المطورين لإظهار أن النماذج القوية آمنة بما يكفي لعدم اعتبارها "خطيرة بشكل غير طبيعي" يحفز التطوير الآمن، إلى جانب الشفافية وحفظ السجلات للمطالبة بتلك الملاذات الآمنة. يمكن للتنظيم حينها منع الضرر حيث الردع من المسؤولية غير كافٍ. أخيراً، مطورو الذكاء الاصطناعي مسؤولون بالفعل عن الأضرار التي يسببونها، لذا التوضيح القانوني للمسؤولية عن أكثر الأنظمة خطورة يمكن أن يُعمل فوراً، دون تطوير معايير مفصلة جداً؛ هذه يمكن أن تتطور حينها مع الوقت. التفاصيل معطاة في الملحق ج.

#### 4. تنظيم الأمان للذكاء الاصطناعي

نظام تنظيمي يعالج المخاطر الحادة واسعة النطاق للذكاء الاصطناعي سيتطلب كحد أدنى:

- تحديد أو إنشاء مجموعة مناسبة من الهيئات التنظيمية، ربما وكالة جديدة؛
- إطار تقييم مخاطر شامل؛[^121]
- إطار لقضايا أمان إيجابية، مبني جزئياً على إطار تقييم المخاطر، ليتم عمله من قبل المطورين، وللمراجعة من قبل مجموعات ووكالات *مستقلة*؛
- نظام ترخيص متدرج، مع طبقات تتبع مستويات القدرة.[^122] ستُمنح التراخيص على أساس قضايا الأمان والمراجعات، لتطوير ونشر الأنظمة. ستتراوح المتطلبات من الإخطار في النهاية المنخفضة، إلى ضمانات أمان وأمن وقابلية سيطرة كمية قبل التطوير، في النهاية العليا. هذه ستمنع إطلاق الأنظمة حتى تُظهر أنها آمنة، وتحظر تطوير الأنظمة غير الآمنة جوهرياً. الملحق د يقدم اقتراحاً لما يمكن أن تستلزمه معايير الأمان والأمن هذه.
- اتفاقيات لجلب مثل هذه التدابير إلى المستوى الدولي، بما في ذلك هيئات دولية لتنسيق القواعد والمعايير، وربما وكالات دولية لمراجعة قضايا الأمان.

المنطق: في النهاية، المسؤولية ليست الآلية الصحيحة لمنع المخاطر واسعة النطاق للجمهور من تقنية جديدة. التنظيم الشامل، مع هيئات تنظيمية مخولة، سيكون مطلوباً للذكاء الاصطناعي كما هو الحال لكل صناعة رئيسية أخرى تشكل مخاطر على الجمهور.[^123]

التنظيم نحو منع المخاطر المنتشرة الأخرى ولكن الأقل حدة من المحتمل أن يختلف في شكله من ولاية قضائية إلى أخرى. الشيء المهم هو تجنب تطوير أنظمة الذكاء الاصطناعي الخطيرة جداً بحيث تصبح هذه المخاطر غير قابلة للإدارة.

### ماذا بعد ذلك؟

على مدى العقد القادم، مع انتشار الذكاء الاصطناعي أكثر وتقدم التقنية الأساسية، من المحتمل أن يحدث شيئان رئيسيان. أولاً، تنظيم أنظمة الذكاء الاصطناعي القوية الموجودة سيصبح أكثر صعوبة، ومع ذلك أكثر ضرورة. من المحتمل أن بعض التدابير على الأقل التي تعالج مخاطر الأمان واسعة النطاق ستتطلب اتفاقاً على المستوى الدولي، مع ولايات قضائية فردية تنفذ قواعد مبنية على اتفاقيات دولية.

ثانياً، حدود قوة حوسبة التدريب والتشغيل ستصبح أصعب للحفاظ عليها مع أن الأجهزة تصبح أرخص وأكثر كفاءة في التكلفة؛ قد تصبح أيضاً أقل صلة (أو تحتاج لأن تكون أكثر إحكاماً حتى) مع التقدم في الخوارزميات والهياكل.

أن السيطرة على الذكاء الاصطناعي ستصبح أصعب لا يعني أننا يجب أن نستسلم! تنفيذ الخطة المحددة في هذا المقال سيعطينا كلاً من الوقت القيم والسيطرة المهمة على العملية التي ستضعنا في موقف أفضل بكثير، بكثير لتجنب المخاطر الوجودية للذكاء الاصطناعي على مجتمعنا وحضارتنا ونوعنا.

في المدى الأطول بعد، ستكون هناك خيارات لنتخذها بخصوص ما نسمح به. قد نختار مع ذلك إنشاء شكل من الذكاء الاصطناعي العام القابل للسيطرة حقاً، إلى الدرجة التي يثبت أن هذا ممكن. أو قد نقرر أن تشغيل العالم أفضل أن يُترك للآلات، إذا استطعنا إقناع أنفسنا أنها ستقوم بعمل أفضل فيه، وتعاملنا جيداً. لكن هذه يجب أن تكون قرارات مُتخذة مع فهم علمي عميق للذكاء الاصطناعي في اليد، وبعد نقاش عالمي شامل ومعنوي، وليس في سباق بين أباطرة التقنية مع معظم الإنسانية غير مشاركة تماماً وغير مدركة.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) ملخص حكومة الذكاء الاصطناعي العام والذكاء الفائق عبر المسؤولية والتنظيم. المسؤولية هي الأعلى، والتنظيم هو الأقوى، في التقاطع الثلاثي للاستقلالية والعمومية والذكاء. يمكن الحصول على ملاذات آمنة من المسؤولية المطلقة والتنظيم القوي عبر قضايا أمان إيجابية تُظهر أن النظام ضعيف و/أو ضيق و/أو سلبي. حدود إجمالي قوة الحوسبة للتدريب ومعدل قوة حوسبة الاستنتاج، مُتحقق منها ومُنفذة قانونياً وباستخدام تدابير أمان الأجهزة والتشفير، تدعم الأمان بتجنب الذكاء الاصطناعي العام الكامل ومنع الذكاء الفائق فعلياً.

[^87]: على الأرجح، انتشار هذا الإدراك سيحتاج إما جهداً مكثفاً من مجموعات التعليم والدعوة التي تقدم هذه الحجة، أو كارثة ذكاء اصطناعي كبيرة إلى حد ما. يمكننا أن نأمل أن يكون الأول.

[^88]: من المفارقات، اعتدنا على الطبيعة تحد من تقنياتنا بجعل تطويرها صعباً جداً، خاصة علمياً. لكن هذا لم يعد هو الحال بالنسبة للذكاء الاصطناعي: المشاكل العلمية الرئيسية تتبين أنها أسهل من المتوقع. لا يمكننا الاعتماد على الطبيعة لإنقاذنا من أنفسنا هنا - سيتوجب علينا أن نفعل ذلك.

[^89]: أين، بالضبط، نتوقف في تطوير أنظمة جديدة؟ هنا، يجب أن نتبنى مبدأ الحذر. بمجرد نشر نظام، وخاصة بمجرد انتشار ذلك المستوى من قدرة النظام، من الصعب للغاية التراجع. وإذا *تم تطوير* نظام (خاصة بتكلفة وجهد كبيرين)، ستكون هناك ضغوط هائلة لاستخدامه أو نشره، وإغراء لتسريبه أو سرقته. تطوير الأنظمة *ثم* تقرير ما إذا كانت غير آمنة بعمق طريق خطير.

[^90]: سيكون من الحكمة أيضاً منع تطوير الذكاء الاصطناعي الخطير جوهرياً، مثل الأنظمة ذاتية التكاثر والتطور، وتلك المصممة للهروب من الحاوية، وتلك التي يمكنها التحسن الذاتي المستقل، والذكاء الاصطناعي الخادع والخبيث عمداً، إلخ.

[^91]: لاحظ أن هذا لا يعني بالضرورة *مُنفذاً* على المستوى الدولي من قبل نوع من الهيئة العالمية: بدلاً من ذلك يمكن للأمم ذات السيادة إنفاذ قواعد متفق عليها، كما في العديد من المعاهدات.

[^92]: كما سنرى أدناه، طبيعة حوسبة الذكاء الاصطناعي ستسمح بشيء من التهجين؛ لكن التعاون الدولي سيظل مطلوباً.

[^93]: على سبيل المثال، الآلات المطلوبة لحفر الرقائق ذات الصلة بالذكاء الاصطناعي مصنوعة من قبل شركة واحدة فقط، ASML (رغم محاولات كثيرة أخرى للقيام بذلك)، الغالبية العظمى من الرقائق ذات الصلة مُصنعة من قبل شركة واحدة، TSMC (رغم محاولات أخرى للمنافسة)، وتصميم وبناء الأجهزة من تلك الرقائق يتم من قبل قلة فقط بما في ذلك NVIDIA وAMD وGoogle.

[^94]: الأهم، كل رقاقة تحمل مفتاحاً تشفيرياً خاصاً فريداً وغير قابل للوصول يمكنها استخدامه "لتوقيع" الأشياء.

[^95]: افتراضياً سيكون هذا الشركة التي تبيع الرقائق، لكن نماذج أخرى ممكنة ومفيدة محتملاً.

[^96]: يمكن للحاكم تحديد موقع الرقاقة عن طريق توقيت تبادل الرسائل الموقعة معها: سرعة الضوء المحدودة تتطلب أن تكون الرقاقة ضمن نصف قطر معين *r* من "محطة" إذا استطاعت إرجاع رسالة موقعة في وقت أقل من *r* / *c*، حيث *c* هي سرعة الضوء. باستخدام محطات متعددة، وبعض الفهم لخصائص الشبكة، يمكن تحديد موقع الرقاقة. جمال هذه الطريقة هو أن معظم أمانها مزود بقوانين الفيزياء. طرق أخرى يمكن أن تستخدم GPS، وتتبع القصور الذاتي، وتقنيات مماثلة.

[^97]: بدلاً من ذلك، يمكن السماح لأزواج الرقائق بالتواصل مع بعضها البعض فقط عبر إذن صريح من حاكم.

[^98]: هذا بالغ الأهمية لأنه على الأقل حالياً، اتصال عرض النطاق العالي جداً بين الرقائق مطلوب لتدريب نماذج ذكاء اصطناعي كبيرة عليها.

[^99]: يمكن أيضاً إعداد هذا لتطلب رسائل موقعة من *N* من *M* حكام مختلفين، مما يسمح لأطراف متعددة بمشاركة الحكومة.

[^100]: هذا بعيد عن كونه مسبوق - على سبيل المثال الجيوش لم تطور جيوشاً من الجنود الخارقين المستنسخين أو المعدلين وراثياً، رغم أن هذا ممكن تقنياً على الأرجح. لكنهم *اختاروا* عدم فعل هذا، بدلاً من منعهم من قبل آخرين. السجل التاريخي ليس رائعاً للقوى العالمية الرئيسية التي يُمنع من تطوير تقنية يرغبون بقوة في تطويرها.

[^101]: مع استثناءين ملحوظين (خاصة NVIDIA) الأجهزة المتخصصة في الذكاء الاصطناعي جزء صغير نسبياً من نموذج العمل والإيرادات الإجمالي لهذه الشركات. علاوة على ذلك، الفجوة بين الأجهزة المستخدمة في الذكاء الاصطناعي المتقدم والأجهزة "درجة المستهلك" كبيرة، لذا معظم مستهلكي أجهزة الكمبيوتر سيكونون غير متأثرين إلى حد كبير.

[^102]: للتحليل الأكثر تفصيلاً، انظر التقارير الحديثة من [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) و[CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). هذه تركز على الجدوى التقنية، خاصة في سياق ضوابط التصدير الأمريكية الساعية لتقييد قدرة البلدان الأخرى في الحوسبة عالية الجودة؛ لكن هذا له تداخل واضح مع القيد العالمي المُتصور هنا.

[^103]: أجهزة Apple، على سبيل المثال، مقفلة عن بُعد وبأمان عندما يُبلغ عن فقدانها أو سرقتها، ويمكن إعادة تفعيلها عن بُعد. هذا يعتمد على نفس ميزات أمان الأجهزة المناقشة هنا.

[^104]: انظر مثلاً عرض IBM's [capacity on demand](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand)، Intel's [Intel on demand.](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html)، وApple's [private cloud compute](https://security.apple.com/blog/private-cloud-compute/).

[^105]: [هذه الدراسة](https://epochai.org/trends#hardware-trends-section) تظهر أن تاريخياً تم تحقيق نفس الأداء باستخدام حوالي 30% أقل من الدولارات كل عام. إذا استمر هذا الاتجاه، قد يكون هناك تداخل كبير بين استخدام رقائق الذكاء الاصطناعي و"المستهلك"، وبشكل عام كمية الأجهزة المطلوبة لأنظمة الذكاء الاصطناعي عالي القوة يمكن أن تصبح صغيرة بشكل مقلق.

[^106]: وفقاً [لنفس الدراسة](https://epochai.org/trends#hardware-trends-section)، أداء معطى في التعرف على الصور تطلب 2.5x أقل حوسبة كل عام. إذا كان هذا سيصدق أيضاً على أكثر أنظمة الذكاء الاصطناعي قدرة كذلك، حد حوسبي لن يكون مفيداً لفترة طويلة جداً.

[^107]: خاصة، على مستوى البلد هذا يبدو كثيراً مثل تأميم الحوسبة، في أن الحكومة ستكون لها سيطرة كبيرة على كيفية استخدام القوة الحاسوبية. ومع ذلك، لأولئك القلقين بشأن مشاركة الحكومة، هذا يبدو أكثر أماناً ومفضلاً عن برمجيات الذكاء الاصطناعي الأكثر قوة *نفسها* التي يتم تأميمها عبر نوع من الاندماج بين شركات الذكاء الاصطناعي الرئيسية والحكومات الوطنية، كما يبدأ البعض في الدعوة إليه.

[^108]: خطوة تنظيمية رئيسية في أوروبا اتُخذت مع إقرار 2024 لـ[قانون الذكاء الاصطناعي الأوروبي.](https://artificialintelligenceact.eu/) إنه يصنف الذكاء الاصطناعي بالمخاطر: يحظر الأنظمة غير المقبولة، وينظم عالية المخاطر، ويفرض قواعد شفافية، أو لا تدابير على الإطلاق، على أنظمة من

## الفصل التاسع - هندسة المستقبل — ما يجب أن نفعله بدلاً من ذلك

يمكن للذكاء الاصطناعي أن يحقق خيراً لا يُصدق في العالم. وللحصول على جميع الفوائد دون المخاطر، يجب أن نضمن بقاء الذكاء الاصطناعي أداة بشرية.

إذا نجحنا في اختيار عدم استبدال البشرية بالآلات - على الأقل لفترة من الوقت! - فماذا يمكننا أن نفعل بدلاً من ذلك؟ هل نتخلى عن الإمكانات الهائلة للذكاء الاصطناعي كتقنية؟ على مستوى معين، الإجابة بسيطة: *لا* - أغلقوا البوابات أمام الذكاء الاصطناعي العام والذكاء الفائق غير القابلين للسيطرة، لكن *اصنعوا* أشكالاً أخرى كثيرة من الذكاء الاصطناعي، إضافة إلى هياكل الحكم والمؤسسات التي سنحتاجها لإدارتها.

لكن لا يزال هناك الكثير لنقوله؛ فتحقيق هذا سيكون مهمة مركزية للبشرية. يستكشف هذا القسم عدة محاور رئيسية:

- كيف يمكننا توصيف الذكاء الاصطناعي "الأداتي" والأشكال التي يمكن أن يتخذها.
- أنه يمكننا الحصول على (تقريباً) كل ما تريده البشرية دون الذكاء الاصطناعي العام، باستخدام الذكاء الاصطناعي الأداتي.
- أن أنظمة الذكاء الاصطناعي الأداتي قابلة للإدارة (على الأرجح، من حيث المبدأ).
- أن التحول بعيداً عن الذكاء الاصطناعي العام لا يعني التنازل عن الأمن القومي - بل العكس تماماً.
- أن تركز السلطة مصدر قلق حقيقي. هل يمكننا التخفيف من ذلك دون تقويض السلامة والأمن؟
- أننا سنريد - ونحتاج - هياكل حكم واجتماعية جديدة، ويمكن للذكاء الاصطناعي أن يساعد فعلاً في ذلك.

### الذكاء الاصطناعي داخل البوابات: الذكاء الاصطناعي الأداتي

يوفر مخطط التقاطع الثلاثي طريقة جيدة لتحديد ما يمكن أن نسميه "الذكاء الاصطناعي الأداتي": الذكاء الاصطناعي الذي يبقى أداة قابلة للسيطرة للاستخدام البشري، وليس منافساً أو بديلاً غير قابل للسيطرة. أقل أنظمة الذكاء الاصطناعي إشكالاً هي تلك المستقلة لكن ليست عامة أو فائقة القدرة (مثل روبوت المزايدة في المزادات)، أو العامة لكن ليست مستقلة أو قادرة (مثل نموذج لغوي صغير)، أو القادرة لكن محدودة وقابلة للسيطرة جداً (مثل AlphaGo).[^124] تلك التي تحمل خاصيتين متقاطعتين لها تطبيق أوسع لكن مخاطر أعلى وستتطلب جهوداً كبيرة لإدارتها. (مجرد كون نظام الذكاء الاصطناعي أكثر شبهاً بأداة لا يعني أنه آمن بطبيعته، وإنما أنه ليس *غير آمن* بطبيعته - فكروا في المنشار مقابل نمر أليف.) يجب أن تبقى البوابة مغلقة أمام الذكاء الاصطناعي العام (الكامل) والذكاء الفائق عند التقاطع الثلاثي، ويجب توخي الحذر الشديد مع أنظمة الذكاء الاصطناعي التي تقترب من تلك العتبة.

لكن هذا يترك الكثير من الذكاء الاصطناعي القوي! يمكننا الحصول على فائدة هائلة من "قوارير المشورة" الذكية والعامة والسلبية والأنظمة المحدودة، والأنظمة العامة على المستوى البشري وليس فوق البشري، وهكذا. العديد من شركات التكنولوجيا والمطورين يبنون فعلياً هذه الأنواع من الأدوات ويجب أن يستمروا؛ مثل معظم الناس، فهم يفترضون ضمنياً أن البوابات أمام الذكاء الاصطناعي العام والذكاء الفائق *ستُغلق*.[^125]

كما يمكن دمج أنظمة الذكاء الاصطناعي فعلياً في أنظمة مركبة تحافظ على الإشراف البشري مع تعزيز القدرة. بدلاً من الاعتماد على صناديق سوداء غامضة، يمكننا بناء أنظمة تعمل فيها مكونات متعددة - بما في ذلك الذكاء الاصطناعي والبرمجيات التقليدية - معاً بطرق يمكن للبشر مراقبتها وفهمها.[^126] بينما قد تكون بعض المكونات صناديق سوداء، لن يكون أي منها قريباً من الذكاء الاصطناعي العام - فقط النظام المركب ككل سيكون عاماً وقادراً جداً، وذلك بطريقة قابلة للسيطرة بدقة.[^127]

#### السيطرة البشرية المعنوية والمضمونة

ماذا تعني "قابلة للسيطرة بدقة"؟ الفكرة الرئيسية لإطار "الأداة" هي السماح للأنظمة - حتى لو كانت عامة وقوية جداً - أن تكون مضمونة السيطرة البشرية المعنوية. ماذا يعني هذا؟ ينطوي على جانبين. الأول اعتبار تصميمي: يجب أن يكون البشر مشاركين بعمق ومحوريتهم فيما يفعله النظام، *دون* تفويض القرارات الرئيسية المهمة للذكاء الاصطناعي. هذه هي طبيعة معظم أنظمة الذكاء الاصطناعي الحالية. ثانياً، بقدر ما تكون أنظمة الذكاء الاصطناعي مستقلة، يجب أن تملك ضمانات تحد من نطاق عملها. الضمان يجب أن يكون *رقماً* يوصف احتمال حدوث شيء ما، وسبباً للإيمان بذلك الرقم. هذا ما نطلبه في مجالات أخرى حرجة للسلامة، حيث تُحسب وتُدعم وتُنشر أرقام مثل "متوسط الوقت بين الأعطال" والأعداد المتوقعة للحوادث في حالات السلامة.[^128] الرقم المثالي للأعطال هو صفر، بالطبع. والأخبار الجيدة أننا قد نقترب كثيراً، وإن باستخدام معمارات ذكاء اصطناعي مختلفة تماماً، باستخدام أفكار الخصائص *المثبتة رسمياً* للبرامج (بما في ذلك الذكاء الاصطناعي). الفكرة، التي استكشفها بإسهاب أوموهوندرو وتيجمارك وبينجيو ودالريمبل وآخرون (انظروا [هنا](https://arxiv.org/abs/2309.01933) و[هنا](https://arxiv.org/abs/2405.06624)) هي إنشاء برنامج بخصائص معينة (مثلاً: أن بإمكان الإنسان إغلاقه) و*إثبات* تلك الخصائص رسمياً. يمكن القيام بهذا الآن لبرامج قصيرة جداً وخصائص بسيطة، لكن القوة (القادمة) لبرمجيات الإثبات المدعومة بالذكاء الاصطناعي قد تسمح بذلك لبرامج أكثر تعقيداً بكثير (مثل الأغلفة) وحتى للذكاء الاصطناعي نفسه. هذا برنامج طموح جداً، لكن مع تزايد الضغط على البوابات، سنحتاج لبعض المواد القوية لتعزيزها. الإثبات الرياضي قد يكون أحد القليل القوي بما يكفي.

#### أين تتجه صناعة الذكاء الاصطناعي

مع إعادة توجيه تقدم الذكاء الاصطناعي، سيبقى الذكاء الاصطناعي الأداتي صناعة ضخمة. من ناحية الأجهزة، حتى مع حدود القوة الحاسوبية لمنع الذكاء الفائق، سيتطلب تدريب واستنتاج النماذج الأصغر كميات ضخمة من المكونات المتخصصة. من ناحية البرمجيات، إبطال انفجار حجم نماذج الذكاء الاصطناعي والحوسبة يجب أن يقود ببساطة الشركات لإعادة توجيه الموارد نحو تحسين الأنظمة الأصغر وجعلها أكثر تنوعاً وتخصصاً، بدلاً من مجرد تكبيرها.[^129] سيكون هناك مساحة كافية - وربما أكثر - لكل تلك الشركات الناشئة الربحية في وادي السيليكون.[^130]

### الذكاء الاصطناعي الأداتي يمكن أن يحقق (تقريباً) كل ما تريده البشرية، دون الذكاء الاصطناعي العام

الذكاء، سواء كان بيولوجياً أو آلياً، يمكن اعتباره على نطاق واسع القدرة على التخطيط وتنفيذ أنشطة تحقق مستقبلات أكثر انسجاماً مع مجموعة من الأهداف. وبالتالي، فالذكاء مفيد بشكل هائل عند استخدامه في سعي لأهداف مختارة بحكمة. الذكاء الاصطناعي يجذب استثمارات ضخمة من الوقت والجهد إلى حد كبير بسبب فوائده الموعودة. لذا يجب أن نسأل: إلى أي مدى سنحصد ثمار فوائد الذكاء الاصطناعي إذا احتوينا هروبه نحو الذكاء الفائق؟ الإجابة: قد نفقد القليل بشكل مفاجئ.

تأملوا أولاً أن أنظمة الذكاء الاصطناعي الحالية قوية جداً بالفعل، ولم نخدش سوى السطح لما يمكن القيام به بها.[^131] هي قادرة بشكل معقول على "إدارة العرض" من ناحية "فهم" سؤال أو مهمة مقدمة إليها، وما سيتطلبه الإجابة على هذا السؤال أو القيام بتلك المهمة.

ثانياً، الكثير من الحماس حول أنظمة الذكاء الاصطناعي الحديثة يرجع لعموميتها؛ لكن بعض أقدر أنظمة الذكاء الاصطناعي - مثل تلك التي تولد أو تتعرف على الكلام أو الصور، وتقوم بالتنبؤ والنمذجة العلمية، وتلعب الألعاب، إلخ - أكثر محدودية وتقع "داخل البوابات" من ناحية الحوسبة.[^132] هذه الأنظمة فوق بشرية في المهام المحددة التي تقوم بها. قد تملك نقاط ضعف في الحالات الحدية[^133] (أو [قابلة للاستغلال](https://arxiv.org/abs/2211.00241)) بسبب محدوديتها؛ مع ذلك فالمحدود *كلياً* أو العام *تماماً* ليسا الخيارين الوحيدين المتاحين: هناك معمارات كثيرة بينهما.[^134]

هذه الأدوات من الذكاء الاصطناعي يمكن أن تسرع التقدم كثيراً في تقنيات إيجابية أخرى، دون الذكاء الاصطناعي العام. لتحسين فيزياء نووية، لا نحتاج للذكاء الاصطناعي أن يكون عالم فيزياء نووية - لدينا هؤلاء! إذا أردنا تسريع الطب، أعطوا علماء الأحياء والباحثين الطبيين والكيميائيين أدوات قوية. هم يريدونها وسيستخدمونها لربح هائل. لا نحتاج لمزرعة خوادم مليئة بمليون عبقري رقمي؛ لدينا ملايين البشر الذي يمكن للذكاء الاصطناعي أن يساعد في إخراج عبقريتهم. نعم، سيستغرق وقتاً أطول للحصول على الخلود وعلاج جميع الأمراض. هذه تكلفة حقيقية. لكن حتى أكثر الابتكارات الصحية الواعدة ستكون قليلة الفائدة إذا قاد عدم الاستقرار المدفوع بالذكاء الاصطناعي إلى صراع عالمي أو انهيار اجتماعي. نحن مدينون لأنفسنا أن نمنح البشر المدعومين بالذكاء الاصطناعي فرصة لحل المشكلة أولاً.

ولنفترض أن هناك، في الواقع، منفعة هائلة للذكاء الاصطناعي العام لا يمكن الحصول عليها من البشرية باستخدام أدوات داخل-البوابات. هل نفقد تلك بعدم بناء الذكاء الاصطناعي العام والذكاء الفائق *أبداً*؟ في موازنة المخاطر والمكافآت هنا، هناك منفعة غير متناسقة هائلة في الانتظار مقابل التسرع: يمكننا الانتظار حتى يمكن القيام بذلك بطريقة مضمونة الأمان والفائدة، وتقريباً الجميع سيحصلون على الثمار؛ إذا تسرعنا، قد يكون ذلك - بكلمات سام ألتمان الرئيس التنفيذي لشركة OpenAI - [انطفاء الأنوار على *جميعنا*.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

لكن إذا كانت الأدوات غير العامة قوية جداً فعلياً، هل يمكننا إدارتها؟ الإجابة واضحة... ربما.

### أنظمة الذكاء الاصطناعي الأداتي قابلة للإدارة (على الأرجح، من حيث المبدأ)

لكن لن يكون سهلاً. أنظمة الذكاء الاصطناعي الحديثة المتطورة يمكن أن تمكن الأشخاص والمؤسسات كثيراً في تحقيق أهدافهم. هذا، بشكل عام، أمر جيد! مع ذلك، هناك ديناميات طبيعية لوجود مثل هذه الأنظمة تحت تصرفنا - فجأة ودون وقت كافٍ للمجتمع للتكيف - تحمل مخاطر جدية تحتاج للإدارة. يستحق مناقشة بعض الفئات الرئيسية لهذه المخاطر، وكيف يمكن تقليلها، بافتراض إغلاق البوابة.

فئة واحدة من المخاطر هي السماح للذكاء الاصطناعي الأداتي عالي القوة بالوصول لمعرفة أو قدرة كانت مرتبطة سابقاً بشخص أو منظمة، مما يجعل مزيج القدرة العالية مع الولاء العالي متاحاً لمجموعة واسعة جداً من الفاعلين. اليوم، بما يكفي من المال، يمكن لشخص سيء النية أن يوظف فريقاً من الكيميائيين لتصميم وإنتاج أسلحة كيماوية جديدة - لكن ليس سهلاً جداً أن يملك ذلك المال أو أن يجد/يجمع الفريق ويقنعهم بفعل شيء واضح أنه غير قانوني وأخلاقي وخطير. لمنع أنظمة الذكاء الاصطناعي من لعب هذا الدور، قد تكفي تحسينات على الطرق الحالية،[^135] طالما أن جميع تلك الأنظمة والوصول إليها يُدار بمسؤولية. من ناحية أخرى، إذا أُطلقت أنظمة قوية للاستخدام والتعديل العامين، فمن المرجح إزالة أي تدابير أمان مدمجة. لذا لتجنب المخاطر في هذه الفئة، ستُطلب قيود قوية على ما يمكن إطلاقه علنياً - مشابهة للقيود على تفاصيل التقنيات النووية والمتفجرة وغيرها من التقنيات الخطرة.[^136]

فئة ثانية من المخاطر تنشأ من توسيع الآلات التي تتصرف مثل أو تنتحل الأشخاص. على مستوى الضرر للأفراد، تشمل هذه المخاطر حيلاً أكثر فعالية، ورسائل مزعجة، وانتحال هوية، وانتشار التزييف العميق غير المرضي.[^137] على مستوى جماعي، تشمل تعطيل العمليات الاجتماعية الأساسية مثل النقاش والجدل العامين، وأنظمة المعلومات والمعرفة المجتمعية وجمعها ومعالجتها ونشرها، وأنظمة الاختيار السياسي. التخفيف من هذا الخطر سيشمل على الأرجح (أ) قوانين تقيد انتحال أنظمة الذكاء الاصطناعي للأشخاص، وتحميل مطوري الذكاء الاصطناعي المسؤولية القانونية لإنشاء أنظمة تولد مثل هذه الانتحالات، و(ب) أنظمة العلامات المائية والمصدر التي تحدد وتصنف (بمسؤولية) المحتوى المولد بالذكاء الاصطناعي، و(ج) أنظمة معرفية اجتماعية-تقنية جديدة يمكنها إنشاء سلسلة موثوقة من البيانات (مثل الكاميرات والتسجيلات) عبر الوقائع والفهم ونماذج العالم الجيدة.[^138] كل هذا ممكن، ويمكن للذكاء الاصطناعي أن يساعد في بعض أجزائه.

خطر عام ثالث هو أنه بقدر أتمتة بعض المهام، يمكن للبشر الذين يقومون حالياً بتلك المهام أن تقل قيمتهم المالية كعمالة. تاريخياً، أتمتة المهام جعلت الأشياء الممكنة بتلك المهام أرخص وأكثر وفرة، بينما رتبت الأشخاص الذين كانوا يقومون بتلك المهام سابقاً إلى من لا يزالون مشاركين في النسخة المؤتمتة (عموماً بمهارة/أجر أعلى)، ومن تصبح عمالتهم أقل قيمة أو قليلة القيمة. صافياً، من الصعب التنبؤ بأي القطاعات ستحتاج عمالة بشرية أكثر مقابل أقل في القطاع الأكبر الناتج لكن الأكثر كفاءة. بالتوازي، تميل ديناميكية الأتمتة لزيادة اللامساواة والإنتاجية العامة، وتقليل تكلفة سلع وخدمات معينة (عبر زيادات الكفاءة)، وزيادة تكلفة أخرى (عبر [مرض التكاليف](https://en.wikipedia.org/wiki/Baumol_effect)). لأولئك في الجانب المحروم من زيادة اللامساواة، من غير الواضح تماماً ما إذا كان نقصان التكلفة في تلك السلع والخدمات المعينة يتفوق على الزيادة في الأخرى، ويقود لرفاهية أعظم إجمالياً. إذن كيف سيسير هذا مع الذكاء الاصطناعي؟ بسبب السهولة النسبية التي يمكن بها استبدال العمل الفكري البشري بالذكاء الاصطناعي العام، يمكننا توقع نسخة سريعة من هذا مع الذكاء الاصطناعي عام الغرض بمستوى بشري.[^139] إذا أغلقنا البوابة أمام الذكاء الاصطناعي العام، سيُستبدل وظائف أقل كثيراً جملة بوكلاء الذكاء الاصطناعي؛ لكن نزوح عمالي ضخم محتمل عبر فترة سنوات.[^140] لتجنب معاناة اقتصادية واسعة، سيكون ضرورياً على الأرجح تطبيق شكل من أشكال الأصول أو الدخل الأساسي الشامل، وأيضاً هندسة تحول ثقافي نحو تقدير ومكافأة العمالة محورية البشر الأصعب للأتمتة (بدلاً من رؤية أسعار العمالة تنخفض بسبب الارتفاع في العمالة المتاحة المدفوعة من أجزاء أخرى من الاقتصاد.) بنى أخرى، مثل ["كرامة البيانات"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society) (التي يُمنح فيها منتجو البيانات البشريين تلقائياً إتاوات للقيمة المخلوقة بتلك البيانات في الذكاء الاصطناعي) قد تساعد. الأتمتة بالذكاء الاصطناعي لها أيضاً تأثير سلبي ثان محتمل، وهو الأتمتة *غير المناسبة*. إضافة للتطبيقات التي يقوم فيها الذكاء الاصطناعي بعمل أسوأ، ستشمل هذه تلك التي من المحتمل أن تنتهك أنظمة الذكاء الاصطناعي فيها المبادئ الأخلاقية والأخلاقية أو القانونية - مثلاً في قرارات الحياة والموت، وفي المسائل القضائية. يجب معاملة هذه بتطبيق وتوسيع أطرنا القانونية الحالية.

أخيراً، تهديد مهم للذكاء الاصطناعي داخل-البوابات هو استخدامه في الإقناع الشخصي وجذب الانتباه والتلاعب. رأينا في وسائل التواصل الاجتماعي ومنصات أخرى نمو اقتصاد انتباه راسخ بعمق (حيث تحارب الخدمات الرقمية بضراوة لانتباه المستخدم) وأنظمة ["رأسمالية المراقبة"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism) (التي تُضاف فيها معلومات المستخدم والتنميط لسلعنة الانتباه.) مؤكد تقريباً أن ذكاء اصطناعي أكثر سيُوضع في خدمة كلاهما. الذكاء الاصطناعي يُستخدم بالفعل كثيراً في خوارزميات التغذية الإدمانية، لكن هذا سيتطور إلى محتوى مولد بالذكاء الاصطناعي إدماني، مخصص ليُستهلك قهرياً من شخص واحد. ومدخلات ذلك الشخص واستجاباته وبياناته، ستُغذى في آلة الانتباه/الإعلان لاستمرار الدورة الشريرة. كما أنه، مع تطور مساعدي الذكاء الاصطناعي المقدمين من شركات التقنية لتصبح الواجهة لحياة رقمية أكثر، سيستبدلون على الأرجح محركات البحث والتغذيات كآلية يحدث بها الإقناع وتحقيق الربح من العملاء. فشل مجتمعنا في السيطرة على هذه الديناميات حتى الآن لا يبشر بخير. قد يُقلل من بعض هذه الديناميكية عبر لوائح حول الخصوصية وحقوق البيانات والتلاعب. الوصول أكثر لجذر المشكلة قد يتطلب منظورات مختلفة، مثل مساعدي الذكاء الاصطناعي الموالين (المناقشين أدناه.)

خلاصة هذه المناقشة هي الأمل: الأنظمة الأداتية داخل-البوابات - على الأقل طالما بقيت مقاربة في القوة والقدرة لأكثر الأنظمة تطوراً اليوم - قابلة للإدارة على الأرجح إذا كانت هناك إرادة وتنسيق للقيام بذلك. المؤسسات البشرية اللائقة، الممكنة بأدوات الذكاء الاصطناعي،[^141] يمكنها فعل ذلك. يمكننا أيضاً الفشل في فعله. لكن من الصعب رؤية كيف سيساعد السماح بأنظمة أقوى - عدا وضعها المسؤولة والأمل للأفضل.

### الأمن القومي

سباقات هيمنة الذكاء الاصطناعي - مدفوعة بالأمن القومي أو دوافع أخرى - تدفعنا نحو أنظمة ذكاء اصطناعي قوية غير محكومة تميل لامتصاص، بدلاً من منح، السلطة. سباق للذكاء الاصطناعي العام بين أمريكا والصين هو سباق لتحديد أي دولة تحصل على الذكاء الفائق أولاً.

فماذا يجب على المسؤولين عن الأمن القومي فعله بدلاً من ذلك؟ الحكومات لها خبرة قوية في بناء أنظمة قابلة للسيطرة وآمنة، ويجب أن تضاعف الجهد في القيام بذلك في الذكاء الاصطناعي، ودعم نوع مشاريع البنية التحتية التي تنجح أفضل عند القيام بها على نطاق وبختم حكومي.

بدلاً من "مشروع مانهاتن" متهور نحو الذكاء الاصطناعي العام،[^142] يمكن للحكومة الأمريكية إطلاق مشروع أبولو لأنظمة قابلة للسيطرة وآمنة وجديرة بالثقة. يمكن أن يشمل هذا مثلاً:

- برنامج كبير لـ(أ) تطوير آليات الأمان للأجهزة على الرقاقة و(ب) البنية التحتية، لإدارة الجانب الحاسوبي للذكاء الاصطناعي القوي. يمكن لهذه أن تبني على [قانون CHIPS](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) الأمريكي و[نظام ضوابط التصدير](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion).
- مبادرة واسعة النطاق لتطوير تقنيات التحقق الرسمي حتى يمكن *إثبات* وجود أو غياب خصائص معينة لأنظمة الذكاء الاصطناعي (مثل مفتاح الإيقاف). يمكن لهذا الاستفادة من الذكاء الاصطناعي نفسه لتطوير براهين الخصائص.
- جهد على نطاق قومي لخلق برمجيات آمنة بشكل قابل للتحقق، مدعومة بأدوات ذكاء اصطناعي يمكنها إعادة ترميز البرمجيات الموجودة في أطر آمنة قابلة للتحقق.
- مشروع استثمار قومي في التقدم العلمي باستخدام الذكاء الاصطناعي،[^143] يدار كشراكة بين وزارة الطاقة ومؤسسة العلوم القومية والمعاهد الوطنية للصحة.

بشكل عام، هناك سطح هجوم هائل على مجتمعنا يجعلنا عرضة لمخاطر من الذكاء الاصطناعي وسوء استخدامه. الحماية من بعض هذه المخاطر ستتطلب استثماراً وتوحيداً بحجم حكومي. هذه ستوفر أماناً أكثر بشكل هائل من سكب البنزين على نار سباقات نحو الذكاء الاصطناعي العام. وإذا كان الذكاء الاصطناعي سيُبنى في الأسلحة وأنظمة القيادة والسيطرة، فمن الحاسم أن يكون الذكاء الاصطناعي جديراً بالثقة وآمناً، وهو ما ليس عليه الذكاء الاصطناعي الحالي ببساطة.

### تركز السلطة وتخفيفاتها

ركزت هذه المقالة على فكرة السيطرة البشرية على الذكاء الاصطناعي وفشلها المحتمل. لكن عدسة أخرى صحيحة لرؤية وضع الذكاء الاصطناعي هي عبر *تركز السلطة.* تطوير ذكاء اصطناعي قوي جداً يهدد بتركيز السلطة إما في الأيدي القليلة والشركية الكبيرة جداً التي طورته وستتحكم به، أو في الحكومات التي تستخدم الذكاء الاصطناعي كوسيلة جديدة للحفاظ على سلطتها وسيطرتها، أو في أنظمة الذكاء الاصطناعي نفسها. أو خليط غير مقدس مما أعلاه. في أي من هذه الحالات، تفقد معظم البشرية السلطة والسيطرة والوكالة. كيف يمكننا محاربة هذا؟

الخطوة الأولى والأهم، بالطبع، هي إغلاق البوابة أمام الذكاء الاصطناعي العام والذكاء الفائق أذكى من البشر. هذه يمكنها صراحة استبدال البشر ومجموعات البشر مباشرة. إذا كانت تحت سيطرة شركية أو حكومية ستركز السلطة في تلك الشركات أو الحكومات؛ إذا كانت "حرة" ستركز السلطة في أنفسها. فلنفترض إغلاق البوابات. ثم ماذا؟

حل مقترح واحد لتركز السلطة هو ذكاء اصطناعي "مفتوح المصدر"، حيث أوزان النموذج متاحة بحرية أو على نطاق واسع. لكن كما ذُكر سابقاً، مرة إطلاق نموذج، يمكن (وعموماً يُجرى) تجريد معظم تدابير السلامة أو الحواجز الوقائية. لذا هناك توتر حاد بين من ناحية اللامركزية، ومن الناحية الأخرى السلامة والأمان والسيطرة البشرية على أنظمة الذكاء الاصطناعي. هناك أيضاً أسباب للشك في أن النماذج المفتوحة ستحارب بمفردها تركز السلطة في الذكاء الاصطناعي بشكل معنوي أكثر مما فعلته في أنظمة التشغيل (لا تزال مهيمناً عليها مايكروسوفت وآبل وجوجل رغم البدائل المفتوحة).[^144]

مع ذلك قد تكون هناك طرق لحل هذا التناقض - لمركزة وتخفيف المخاطر بينما لامركزة القدرة والمكافأة الاقتصادية. هذا يتطلب إعادة تفكير في كيفية تطوير الذكاء الاصطناعي وكيفية توزيع فوائده.

نماذج جديدة لتطوير وملكية الذكاء الاصطناعي العامة ستساعد. يمكن أن تأخذ هذه أشكالاً عدة: ذكاء اصطناعي مطور حكومياً (خاضع للإشراف الديمقراطي)،[^145] منظمات تطوير ذكاء اصطناعي غير ربحية (مثل موزيلا للمتصفحات)، أو هياكل تمكن الملكية والحكم على نطاق واسع جداً. المفتاح أن هذه المؤسسات ستُوكل صراحة لخدمة المصلحة العامة بينما تعمل تحت قيود سلامة قوية.[^146] أنظمة تنظيمية ومعايير/شهادات مصممة جيداً ستكون حيوية أيضاً، حتى تبقى منتجات الذكاء الاصطناعي المقدمة من سوق حيوي مفيدة فعلاً بدلاً من استغلالية تجاه مستخدميها.

من ناحية تركز السلطة الاقتصادية، يمكننا استخدام تتبع المصدر و"كرامة البيانات" لضمان تدفق الفوائد الاقتصادية على نطاق أوسع. خاصة، معظم قوة الذكاء الاصطناعي الآن (وفي المستقبل إذا أبقينا البوابات مغلقة) تنشأ من بيانات مولدة بشرياً، سواء بيانات تدريب مباشرة أو تعليقات بشرية. إذا طُلب من شركات الذكاء الاصطناعي تعويض مقدمي البيانات بإنصاف،[^147] فهذا يمكن على الأقل أن يساعد في توزيع المكافآت الاقتصادية بشكل أوسع. خلاف هذا، نموذج آخر يمكن أن يكون الملكية العامة لكسور مهمة من شركات الذكاء الاصطناعي الكبيرة. مثلاً، الحكومات القادرة على فرض ضرائب على شركات الذكاء الاصطناعي يمكن أن تستثمر كسراً من المقبوضات في صندوق ثروة سيادي يحمل أسهماً في الشركات، ويدفع أرباحاً للسكان.[^148]

الحاسم في هذه الآليات استخدام قوة الذكاء الاصطناعي نفسه لمساعدة توزيع السلطة بشكل أفضل، بدلاً من مجرد قتال تركز السلطة المدفوع بالذكاء الاصطناعي باستخدام وسائل غير ذكاء اصطناعي. نهج قوي واحد سيكون عبر مساعدي ذكاء اصطناعي مصممين جيداً يعملون بواجب ائتماني حقيقي لمستخدميهم - واضعين مصالح المستخدمين أولاً، خاصة فوق مصالح المقدمين الشركيين.[^149] هؤلاء المساعدون يجب أن يكونوا جديرين بالثقة حقاً، وأكفاء تقنياً لكن محدودين بشكل مناسب حسب حالة الاستخدام ومستوى الخطر، ومتاحين على نطاق واسع للجميع عبر قنوات عامة أو غير ربحية أو ربحية معتمدة. كما لن نقبل أبداً مساعداً بشرياً يعمل سراً ضد مصالحنا لطرف آخر، يجب ألا نقبل مساعدي ذكاء اصطناعي يراقبون أو يتلاعبون أو يستخرجون قيمة من مستخدميهم لصالح شركي.

مثل هذا التحول سيغير الديناميكية الحالية جذرياً حيث يُترك الأفراد للتفاوض وحدهم مع آلات شركية وبيروقراطية هائلة (مدعومة بالذكاء الاصطناعي) تعطي أولوية لاستخراج القيمة على رفاهية البشر. بينما هناك نهج كثيرة ممكنة لإعادة توزيع السلطة المدفوعة بالذكاء الاصطناعي بشكل أوسع، لن يظهر أي منها بالافتراضية: يجب هندستها وحكمها عمداً بآليات مثل متطلبات الائتمان والتقديم العام والوصول المتدرج حسب الخطر.

نهج تخفيف تركز السلطة يمكن أن تواجه رياحاً معاكسة مهمة من السلطات الحاكمة.[^150] لكن هناك مسارات نحو تطوير ذكاء اصطناعي لا تتطلب الاختيار بين الأمان والسلطة المركزة. ببناء المؤسسات الصحيحة الآن، يمكننا ضمان أن فوائد الذكاء الاصطناعي تُشارك على نطاق واسع بينما مخاطره تُدار بعناية.

### هياكل الحكم والاجتماعية الجديدة

هياكل الحكم الحالية تكافح: هي بطيئة الاستجابة، وغالباً مأسورة من مصالح خاصة، و[تقل ثقة الجمهور بها بشكل متزايد.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) مع ذلك، هذا ليس سبباً لهجرها - بل العكس تماماً. بعض المؤسسات قد تحتاج لاستبدال، لكن بشكل أوسع نحتاج آليات جديدة يمكنها تعزيز ومكملة هياكلنا الموجودة، مساعدتها على العمل بشكل أفضل في عالمنا سريع التطور.

الكثير من ضعف مؤسساتنا ينشأ ليس من هياكل حكومية رسمية، بل من مؤسسات اجتماعية متدهورة: أنظمتنا لتطوير الفهم المشترك وتنسيق العمل وإجراء خطاب معنوي. حتى الآن، سرع الذكاء الاصطناعي هذا التدهور، بإغراق قنوات معلوماتنا بمحتوى مولد، وتوجيهنا للمحتوى الأكثر استقطاباً وتفريقاً، وجعل تمييز الحق من الباطل أصعب.

لكن الذكاء الاصطناعي يمكن أن يساعد فعلاً في إعادة بناء وتقوية هذه المؤسسات الاجتماعية. تأملوا ثلاث مجالات حاسمة:

أولاً، يمكن للذكاء الاصطناعي أن يساعد في استعادة الثقة في أنظمتنا المعرفية - طرقنا في معرفة ما هو صحيح. يمكننا تطوير أنظمة مدعومة بالذكاء الاصطناعي تتتبع وتتحقق من مصدر المعلومات، من البيانات الخام عبر التحليل للاستنتاجات. هذه الأنظمة يمكن أن تدمج التحقق التشفيري مع تحليل متطور لمساعدة الأشخاص على فهم ليس فقط ما إذا كان شيء صحيحاً، لكن كيف نعرف أنه صحيح.[^151] مساعدو الذكاء الاصطناعي الموالون يمكن أن يُكلفوا بتتبع التفاصيل للتأكد من صحتها.

ثانياً، يمكن للذكاء الاصطناعي أن يمكن أشكالاً جديدة من التنسيق واسع النطاق. الكثير من أكثر مشاكلنا إلحاحاً - من تغير المناخ لمقاومة المضادات الحيوية - هي مشاكل تنسيق جوهرياً. نحن [عالقون في مواقف أسوأ مما يمكن أن تكون تقريباً لكل شخص](https://equilibriabook.com/)، لأنه لا يستطيع أي فرد أو مجموعة تحمل الخطوة الأولى. أنظمة الذكاء الاصطناعي يمكن أن تساعد بنمذجة هياكل حوافز معقدة، وتحديد مسارات قابلة للتطبيق لنتائج أفضل، وتسهيل بناء الثقة وآليات الالتزام المطلوبة للوصول هناك.

ربما الأكثر إثارة للاهتمام، يمكن للذكاء الاصطناعي أن يمكن أشكالاً جديدة كلياً من الخطاب الاجتماعي. تخيلوا القدرة على "التحدث لمدينة"[^152] - ليس مجرد عرض إحصائيات، لكن حوار معنوي مع نظام ذكاء اصطناعي يعالج ويركب آراء وخبرات وحاجات وتطلعات ملايين السكان. أو تأملوا كيف يمكن للذكاء الاصطناعي تسهيل حوار حقيقي بين مجموعات تتحدث حالياً بجانب بعضها، بمساعدة كل جانب على فهم مخاوف وقيم الجانب الآخر الفعلية أفضل بدلاً من كاريكاتيراتهم عن بعضهم.[^153] أو يمكن للذكاء الاصطناعي أن يقدم وساطة ماهرة وموثوقة للخلافات بين أشخاص أو حتى مجموعات كبيرة من الأشخاص (الذين يمكن كلهم التفاعل معه مباشرة وفردياً!) الذكاء الاصطناعي الحالي قادر كلياً على القيام بهذا العمل، لكن الأدوات للقيام بذلك لن تظهر بنفسها، أو عبر حوافز السوق.

هذه الإمكانات قد تبدو طوباوية، خاصة نظراً لدور الذكاء الاصطناعي الحالي في تدهور الخطاب والثقة. لكن ذلك تحديداً لماذا يجب أن نطور بفعالية هذه التطبيقات الإيجابية. بإغلاق البوابات أمام الذكاء الاصطناعي العام غير القابل للسيطرة وإعطاء أولوية للذكاء الاصطناعي الذي يعزز الوكالة البشرية، يمكننا توجيه التقدم التقني نحو مستقبل يخدم فيه الذكاء الاصطناعي كقوة للتمكين والمرونة والتقدم الجماعي.

[^124]: مع ذلك، البقاء بعيداً عن التقاطع الثلاثي ليس سهلاً كما قد يحب المرء لسوء الحظ. دفع القدرة بشدة جداً في أي من الجوانب الثلاثة يميل لزيادتها في الآخرين. خاصة، قد يكون صعباً خلق ذكاء عام وقادر جداً لا يمكن تحويله لمستقل بسهولة. نهج واحد تدريب نماذج ["قصيرة النظر"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) بقدرة تخطيط معطلة. آخر سيكون التركيز على هندسة أنظمة ["قوارير مشورة"](https://arxiv.org/abs/1711.05541) خالصة تتجنب إجابة أسئلة موجهة للعمل.

[^125]: شركات كثيرة تفشل في إدراك أنها أيضاً ستُستبدل في النهاية بالذكاء الاصطناعي العام، حتى لو استغرق وقتاً أطول - لو فعلوا، قد يدفعون على تلك البوابات أقل قليلاً!

[^126]: أنظمة الذكاء الاصطناعي يمكن أن تتواصل بطرق أكثر كفاءة لكن أقل فهماً، لكن الحفاظ على الفهم البشري يجب أن يأخذ أولوية.

[^127]: هذه الفكرة للذكاء الاصطناعي المعياري القابل للتفسير طُورت بتفصيل من عدة باحثين؛ انظروا مثلاً نموذج ["خدمات الذكاء الاصطناعي الشاملة"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) لدركسلر، ["معمارية الوكالة المفتوحة"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai) لدالريمبل وآخرين. بينما قد تتطلب مثل هذه الأنظمة جهد هندسي أكثر من الشبكات العصبية الأحادية المدربة بحوسبة ضخمة، ذلك تحديداً حيث تساعد حدود الحوسبة - بجعل المسار الآمن الأكثر شفافية أيضاً الأكثر عملية.

[^128]: حول حالات السلامة بشكل عام انظروا [هذا الدليل](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). بالنسبة للذكاء الاصطناعي خاصة، انظروا [واسيل وآخرين](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274)، [كليمر وآخرين](https://arxiv.org/abs/2403.10462)، [بوهل وآخرين](https://arxiv.org/abs/2410.21572)، و[بالسني وآخرين](https://arxiv.org/abs/2411.03336)

[^129]: نحن نرى فعلاً هذا الاتجاه مدفوعاً فقط من التكلفة العالية للاستنتاج: نماذج أصغر وأكثر تخصصاً "مقطرة" من الأكبر وقادرة على العمل على أجهزة أرخص.

[^130]: أفهم لماذا المتحمسون لنظام التقنية للذكاء الاصطناعي قد يعارضون ما يرونه تنظيماً مرهقاً على صناعتهم. لكن من المحير صراحة لماذا، مثلاً، رأسمالي مخاطر يريد السماح بهروب للذكاء الاصطناعي العام والذكاء الفائق. تلك الأنظمة (والشركات، بينما تبقى تحت سيطرة الشركة) *ستأكل كل الشركات الناشئة كوجبة خفيفة*. ربما حتى *أسرع* من أكل الصناعات الأخرى. أي شخص مستثمر في نظام ذكاء اصطناعي مزدهر يجب أن يعطي أولوية لضمان ألا يقود تطوير الذكاء الاصطناعي العام لاحتكار من قبل بعض اللاعبين

## الفصل العاشر - الخيار أمامنا

لحماية مستقبلنا الإنساني، يجب أن نختار إغلاق البوابات أمام الذكاء الاصطناعي العام والذكاء الفائق.

آخر مرة شاركت فيها البشرية الأرض مع عقول أخرى تتحدث وتفكر وتبني التكنولوجيا وتحل المشكلات عامة الغرض كانت قبل 40,000 سنة في أوروبا العصر الجليدي. تلك العقول الأخرى انقرضت، كلياً أو جزئياً بسبب جهود عقولنا.

ونحن الآن ندخل مجدداً مثل هذا الوقت. أكثر منتجات ثقافتنا وتكنولوجيتنا تطوراً - مجموعات البيانات المبنية من مجمل المعلومات المشتركة على الإنترنت، ورقائق من 100 مليار عنصر هي أعقد التقنيات التي صنعناها على الإطلاق - يجري دمجها لإحياء أنظمة الذكاء الاصطناعي المتقدمة عامة الغرض.

يحرص مطورو هذه الأنظمة على تصويرها كأدوات لتمكين البشر. وحقاً يمكنها أن تكون كذلك. لكن لا تخطئوا الفهم: مسارنا الحالي هو بناء وكلاء رقميين موجهين نحو الأهداف ومتخذين للقرارات وذوي قدرات عامة أكثر قوة من أي وقت مضى. إنها تؤدي بالفعل بنفس جودة كثير من البشر في نطاق واسع من المهام الفكرية، وتتحسن بسرعة، وتساهم في تحسين نفسها.

ما لم يتغير هذا المسار أو يصطدم بعقبة غير متوقعة، فسنحصل قريباً - في سنوات وليس عقود - على ذكاءات رقمية خطيرة القوة. حتى في *أفضل* النتائج، ستجلب هذه منافع اقتصادية عظيمة (على الأقل لبعضنا) ولكن فقط مقابل اضطراب عميق في مجتمعنا، واستبدال البشر في معظم أهم الأشياء التي نقوم بها: هذه الآلات ستفكر نيابة عنا، وتخطط لنا، وتقرر لنا، وتبدع لنا. سنكون مدللين، لكن كأطفال مدللين. والأرجح أن هذه الأنظمة ستحل محل البشر في الأشياء الإيجابية *والسلبية* التي نقوم بها، بما في ذلك الاستغلال والتلاعب والعنف والحرب. هل يمكننا البقاء أمام نسخ مفرطة التسارع بالذكاء الاصطناعي من هذه؟ أخيراً، من المعقول جداً ألا تسير الأمور على ما يرام إطلاقاً: أن نُستبدل نسبياً قريباً ليس فقط فيما نفعل، بل فيما *نحن عليه*، كمعماريين للحضارة والمستقبل. اسألوا إنسان النياندرتال كيف يسير ذلك. ربما قدمنا لهم حليات إضافية لفترة كذلك.

*لسنا مضطرين لفعل هذا.* لدينا ذكاء اصطناعي ينافس البشر، ولا حاجة لبناء ذكاء اصطناعي *لا يمكننا* منافسته. يمكننا بناء أدوات ذكاء اصطناعي مذهلة دون بناء نوع خلف. فكرة أن الذكاء الاصطناعي العام والذكاء الفائق حتميان هي *خيار يتقنع بقناع القدر*.

بفرض بعض الحدود الصارمة والعالمية، يمكننا الحفاظ على القدرة العامة للذكاء الاصطناعي عند مستوى بشري تقريباً بينما نجني فوائد قدرة أجهزة الكمبيوتر على معالجة البيانات بطرق لا نستطيعها، وأتمتة المهام التي لا يريد أحد منا القيام بها. ستظل هذه تطرح مخاطر كثيرة، لكن إذا صُممت وأُديرت جيداً، ستكون نعمة هائلة للبشرية، من الطب إلى البحث إلى المنتجات الاستهلاكية.

فرض الحدود سيتطلب تعاوناً دولياً، لكن أقل مما قد يظن المرء، وهذه الحدود ستترك مجالاً واسعاً لصناعة ضخمة للذكاء الاصطناعي وأجهزته تركز على التطبيقات التي تعزز رفاهية الإنسان، بدلاً من السعي المحض وراء القوة. وإذا قررنا، بضمانات أمان قوية وبعد حوار عالمي هادف، المضي أبعد، فهذا الخيار يظل متاحاً لنا للسعي وراءه.

يجب على البشرية أن *تختار* إغلاق البوابات أمام الذكاء الاصطناعي العام والذكاء الفائق.

للحفاظ على المستقبل إنسانياً.

### ملاحظة من المؤلف

شكراً لكم على الوقت الذي خصصتموه لاستكشاف هذا الموضوع معنا.

كتبت هذا المقال لأنني كعالم أشعر بأهمية قول الحقيقة دون تجميل، ولأنني كإنسان أشعر بأنه من الحاسم أن نتصرف بسرعة وحزم لمواجهة قضية تغير العالم: تطوير أنظمة ذكاء اصطناعي أذكى من البشر.

إذا أردنا الاستجابة لهذا الوضع المذهل بحكمة، يجب أن نكون مستعدين لفحص السردية السائدة نقدياً بأن الذكاء الاصطناعي العام والذكاء الفائق "يجب" بناؤهما لضمان مصالحنا، أو أنهما "حتميان" ولا يمكن إيقافهما. هذه السرديات تتركنا عاجزين، غير قادرين على رؤية المسارات البديلة أمامنا.

آمل أن تنضموا إليّ في الدعوة للحذر في وجه التهور، والشجاعة في وجه الجشع.

آمل أن تنضموا إليّ في الدعوة لمستقبل إنساني.

*– أنتوني*

![Anthony Aguirre signature](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAnthony-Aguirre-signature-300x84.png&w=3840&q=75)

## الملاحق

المعلومات التكميلية، بما في ذلك - التفاصيل التقنية حول محاسبة القوة الحاسوبية، مثال لتطبيق "إغلاق البوابة"، تفاصيل نظام المسؤولية المطلقة للذكاء الاصطناعي العام، ونهج متدرج لمعايير سلامة وأمن الذكاء الاصطناعي العام.

### الملحق أ: التفاصيل التقنية لمحاسبة القوة الحاسوبية

يتطلب تطبيق ضوابط فعالة قائمة على القوة الحاسوبية وضع منهجية مفصلة لحساب "الحقيقة المطلقة" بالإضافة إلى تقديرات دقيقة للقوة الحاسوبية الإجمالية المستخدمة في التدريب والاستنتاج. فيما يلي مثال على كيفية حساب "الحقيقة المطلقة" على المستوى التقني.

**التعريفات:**

*الرسم البياني السببي للحوسبة:* لمخرج معين O من نموذج ذكاء اصطناعي، هناك مجموعة من العمليات الحاسوبية الرقمية التي قد يؤدي تغيير نتيجتها إلى تغيير O. (يجب افتراض هذا بحذر، أي يجب وجود سبب واضح للاعتقاد بأن عملية حاسوبية مستقلة عن سابق يحدث قبلها زمنياً وله مسار سببي فيزيائي محتمل للتأثير.) يشمل ذلك الحوسبة التي يقوم بها نموذج الذكاء الاصطناعي أثناء الاستنتاج، بالإضافة إلى العمليات الحاسوبية التي دخلت في إعداد المدخلات والبيانات وتدريب النموذج. نظراً لأن أياً من هذه قد تكون بحد ذاتها مخرجات من نموذج ذكاء اصطناعي، يتم حسابها بشكل تكراري، مع التوقف عند نقطة قدم فيها الإنسان تغييراً كبيراً للمدخل.

*القوة الحاسوبية للتدريب:* إجمالي القوة الحاسوبية، بوحدة FLOP أو وحدات أخرى، التي ينطوي عليها الرسم البياني السببي للحوسبة لشبكة عصبية (شاملاً إعداد البيانات والتدريب والضبط الدقيق وأي عمليات حاسوبية أخرى.)

*القوة الحاسوبية للمخرجات:* إجمالي القوة الحاسوبية في الرسم البياني السببي للحوسبة لمخرج معين من الذكاء الاصطناعي، شاملاً جميع الشبكات العصبية (بما في ذلك قوتها الحاسوبية للتدريب) والعمليات الحاسوبية الأخرى التي تدخل في ذلك المخرج.

*معدل القوة الحاسوبية للاستنتاج:* في سلسلة من المخرجات، معدل التغير (بوحدة FLOP/s أو وحدات أخرى) في القوة الحاسوبية للمخرجات بين المخرجات، أي القوة الحاسوبية المستخدمة لإنتاج المخرج التالي، مقسومة على الفترة الزمنية بين المخرجات.

**أمثلة وتقديرات:**

- بالنسبة لشبكة عصبية واحدة مدربة على بيانات أنشأها الإنسان، القوة الحاسوبية للتدريب هي ببساطة إجمالي القوة الحاسوبية للتدريب كما يتم الإبلاغ عنها عادة.
- بالنسبة لمثل هذه الشبكة العصبية التي تقوم بالاستنتاج بمعدل ثابت، فإن معدل القوة الحاسوبية للاستنتاج يعادل تقريباً إجمالي سرعة مجموعة الحوسبة التي تؤدي الاستنتاج بوحدة FLOP/s.
- بالنسبة للضبط الدقيق للنموذج، تُحسب القوة الحاسوبية للتدريب للنموذج الكامل بجمع القوة الحاسوبية للتدريب للنموذج غير المضبوط دقيقاً مع الحوسبة المنجزة أثناء الضبط الدقيق ولإعداد أي بيانات مستخدمة في الضبط الدقيق.
- بالنسبة للنموذج المقطر، تشمل القوة الحاسوبية للتدريب للنموذج الكامل تدريب كل من النموذج المقطر والنموذج الأكبر المستخدم لتوفير البيانات الاصطناعية أو مدخلات التدريب الأخرى.
- إذا تم تدريب عدة نماذج، لكن تم استبعاد العديد من "المحاولات" بناءً على الحكم البشري، فإن هذه لا تحسب ضمن القوة الحاسوبية للتدريب أو المخرجات للنموذج المحتفظ به.

### الملحق ب: مثال لتطبيق إغلاق البوابة

**مثال للتطبيق:** فيما يلي مثال واحد لكيفية عمل إغلاق البوابة، بحد أقصى 10<sup>27</sup> FLOP للتدريب و10<sup>20</sup> FLOP/s للاستنتاج (تشغيل الذكاء الاصطناعي):

**1. التوقف:** لأسباب الأمن القومي، تطلب السلطة التنفيذية الأمريكية من جميع الشركات المتمركزة في الولايات المتحدة، أو التي تمارس أعمالها في الولايات المتحدة، أو التي تستخدم رقائق مصنعة في الولايات المتحدة، التوقف عن أي عمليات تدريب جديدة للذكاء الاصطناعي قد تتجاوز حد القوة الحاسوبية للتدريب البالغ 10<sup>27</sup> FLOP. يجب على الولايات المتحدة بدء المناقشات مع الدول الأخرى التي تستضيف تطوير الذكاء الاصطناعي، مع تشجيعها بقوة على اتخاذ خطوات مماثلة والإشارة إلى أن التوقف الأمريكي قد يُرفع في حال اختاروا عدم الامتثال.

**2. الإشراف والترخيص الأمريكي:** من خلال أمر تنفيذي أو إجراء من وكالة تنظيمية قائمة، تطالب الولايات المتحدة بأنه خلال (مثلاً) سنة واحدة:

- يجب تسجيل جميع عمليات تدريب الذكاء الاصطناعي المقدرة بأكثر من 10<sup>25</sup> FLOP التي تقوم بها الشركات العاملة في الولايات المتحدة في قاعدة بيانات تحتفظ بها وكالة تنظيمية أمريكية. (ملاحظة: نسخة أضعف قليلاً من هذا كانت مدرجة بالفعل في الأمر التنفيذي الأمريكي لعام 2023 حول الذكاء الاصطناعي الذي تم إلغاؤه لاحقاً، والذي يتطلب التسجيل للنماذج التي تتجاوز 10<sup>26</sup> FLOP.)
- يجب على جميع مصنعي الأجهزة ذات الصلة بالذكاء الاصطناعي العاملين في الولايات المتحدة أو الذين يمارسون أعمالهم مع الحكومة الأمريكية الالتزام بمجموعة من المتطلبات على أجهزتهم المتخصصة والبرمجيات التي تشغلها. (يمكن تضمين العديد من هذه المتطلبات في تحديثات البرمجيات والبرامج الثابتة للأجهزة الموجودة، لكن الحلول طويلة المدى والقوية ستتطلب تغييرات في الأجيال اللاحقة من الأجهزة.) من بين هذه المتطلبات ضرورة أنه إذا كان الجهاز جزءاً من مجموعة مترابطة عالية السرعة قادرة على تنفيذ 10<sup>18</sup> FLOP/s من الحوسبة، فإن مستوى أعلى من التحقق مطلوب، والذي يشمل إذناً منتظماً من "حاكم" بعيد يستقبل كلاً من القياسات عن بُعد وطلبات تنفيذ حوسبة إضافية.
- يبلغ الحارس عن إجمالي الحوسبة المنجزة على أجهزته إلى الوكالة التي تحتفظ بقاعدة البيانات الأمريكية.
- يتم تطبيق متطلبات أقوى تدريجياً للسماح بإشراف وترخيص أكثر أماناً ومرونة.

**3. الإشراف الدولي:**

- تتفاوض الولايات المتحدة والصين وأي دول أخرى تستضيف قدرات تصنيع الرقائق المتقدمة على اتفاقية دولية.
- تنشئ هذه الاتفاقية وكالة دولية جديدة، مماثلة للوكالة الدولية للطاقة الذرية، مكلفة بالإشراف على تدريب وتنفيذ الذكاء الاصطناعي.
- يجب على الدول الموقعة أن تطالب مصنعي أجهزة الذكاء الاصطناعي المحليين بالامتثال لمجموعة من المتطلبات قوية على الأقل مثل تلك المفروضة في الولايات المتحدة.
- الحراس مطالبون الآن بالإبلاغ عن أرقام حوسبة الذكاء الاصطناعي إلى كل من الوكالات في دولهم الأم وكذلك مكتب جديد داخل الوكالة الدولية.
- يتم تشجيع الدول الإضافية بقوة للانضمام إلى الاتفاقية الدولية القائمة: تقيد ضوابط التصدير من قبل الدول الموقعة الوصول إلى الأجهزة المتقدمة من قبل غير الموقعين بينما يمكن للموقعين الحصول على دعم فني في إدارة أنظمة الذكاء الاصطناعي الخاصة بهم.

**4. التحقق والإنفاذ الدولي:**

- يتم تحديث نظام التحقق من الأجهزة بحيث يبلغ عن استخدام الحوسبة إلى كل من الحارس الأصلي وأيضاً مباشرة إلى مكتب الوكالة الدولية.
- تتفق الوكالة، عبر المناقشة مع الموقعين على الاتفاقية الدولية، على قيود حاسوبية تكتسب بعد ذلك قوة قانونية في الدول الموقعة.
- بالتوازي، قد يتم تطوير مجموعة من المعايير الدولية بحيث يُطلب من تدريب وتشغيل أنظمة الذكاء الاصطناعي فوق عتبة من الحوسبة (لكن تحت الحد الأقصى) الالتزام بتلك المعايير.
- يمكن للوكالة، إذا لزم الأمر للتعويض عن خوارزميات أفضل وغيرها، خفض حد الحوسبة. أو، إذا اعتُبر آمناً ومستحسناً (على مستوى ضمانات السلامة القابلة للإثبات مثلاً)، رفع حد الحوسبة.

### الملحق ج: تفاصيل نظام المسؤولية المطلقة للذكاء الاصطناعي العام

**تفاصيل نظام المسؤولية المطلقة للذكاء الاصطناعي العام**

- يُعتبر إنشاء وتشغيل نظام ذكاء اصطناعي متقدم عالي العمومية والقدرة والاستقلالية نشاطاً "خطيراً بشكل غير طبيعي".
- وبذلك، فإن مستوى المسؤولية الافتراضي لتدريب وتشغيل مثل هذه الأنظمة هو المسؤولية المطلقة والمشتركة والتضامنية (أو ما يعادلها خارج الولايات المتحدة) عن أي أضرار يسببها النموذج أو مخرجاته/أفعاله.
- ستُفرض المسؤولية الشخصية على المديرين التنفيذيين وأعضاء مجلس الإدارة في حالات الإهمال الجسيم أو سوء السلوك المتعمد. يجب أن يشمل هذا عقوبات جنائية للحالات الأكثر فداحة.
- هناك العديد من الملاذات الآمنة التي تعود في ظلها المسؤولية إلى المسؤولية الافتراضية (القائمة على الخطأ، في الولايات المتحدة) التي يخضع لها الناس والشركات عادة.
	- النماذج المدربة والمشغلة تحت عتبة حاسوبية معينة (والتي ستكون أقل بـ 10 مرات على الأقل من الحدود الموصوفة أعلاه.)
	- الذكاء الاصطناعي "الضعيف" (تقريباً، تحت مستوى الخبير البشري في المهام المخصص لها) و/أو
	- الذكاء الاصطناعي "الضيق" (له نطاق ثابت ومحدود جداً من المهام والعمليات المصمم والمدرب عليها تحديداً) و/أو
	- الذكاء الاصطناعي "السلبي" (محدود جداً في قدرته – حتى مع التعديل المتواضع – على اتخاذ إجراءات أو أداء مهام متعددة الخطوات معقدة دون تدخل ورقابة بشرية مباشرة.)
	- ذكاء اصطناعي مضمون السلامة والأمان والقابلية للتحكم (آمن بشكل قابل للإثبات، أو تشير دراسة المخاطر إلى مستوى ضئيل من الضرر المتوقع.)
- يمكن المطالبة بالملاذات الآمنة على أساس [حالة سلامة](https://arxiv.org/abs/2410.21572) تُعد من قبل مطور الذكاء الاصطناعي وتُعتمد من قبل وكالة أو مدقق معتمد من قبل وكالة. للمطالبة بملاذ آمن قائم على الحوسبة، على المطور فقط تقديم تقديرات موثوقة لإجمالي القوة الحاسوبية للتدريب والحد الأقصى لمعدل الاستنتاج
- ستحدد التشريعات صراحة المواقف التي يكون فيها الانتصاف القضائي من تطوير أنظمة الذكاء الاصطناعي ذات المخاطر العالية للضرر العام مناسباً.
- يجب على اتحادات الشركات، بالعمل مع المنظمات غير الحكومية والوكالات الحكومية، تطوير معايير وقواعد تحدد هذه المصطلحات، وكيف يجب على المنظمين منح الملاذات الآمنة، وكيف يجب على مطوري الذكاء الاصطناعي تطوير حالات السلامة، وكيف يجب على المحاكم تفسير المسؤولية حيث لا تُطالب بالملاذات الآمنة بشكل استباقي.

### الملحق د: نهج متدرج لمعايير سلامة وأمن الذكاء الاصطناعي العام

**نهج متدرج لمعايير سلامة وأمن الذكاء الاصطناعي العام**

| مستوى المخاطر | المحفز(ات) | متطلبات التدريب | متطلبات النشر |
| --- | --- | --- | --- |
| م م-0 | ذكاء اصطناعي ضعيف في الاستقلالية والعمومية والذكاء | لا شيء | لا شيء |
| م م-1 | ذكاء اصطناعي قوي في واحد من الاستقلالية أو العمومية أو الذكاء | لا شيء | بناءً على المخاطر والاستخدام، يحتمل حالات سلامة معتمدة من السلطات الوطنية أينما يمكن استخدام النموذج |
| م م-2 | ذكاء اصطناعي قوي في اثنين من الاستقلالية أو العمومية أو الذكاء | التسجيل لدى السلطة الوطنية التي لها ولاية قضائية على المطور | حالة سلامة تحدد مخاطر الضرر الكبير تحت المستويات المصرح بها بالإضافة إلى عمليات تدقيق سلامة مستقلة (تشمل الاختبار الأحمر بالصندوق الأسود والأبيض) معتمدة من السلطات الوطنية أينما يمكن استخدام النموذج |
| م م-3 | ذكاء اصطناعي عام قوي في الاستقلالية والعمومية والذكاء | الموافقة المسبقة على خطة السلامة والأمن من السلطة الوطنية التي لها ولاية قضائية على المطور | حالة سلامة تضمن مخاطر محدودة للضرر الكبير تحت المستويات المصرح بها بالإضافة إلى مواصفات مطلوبة، تشمل الأمن السيبراني والقابلية للتحكم ومفتاح إيقاف غير قابل للإزالة والمواءمة مع القيم البشرية والقوة ضد سوء الاستخدام الخبيث. |
| م م-4 | أي نموذج يتجاوز أيضاً إما 10<sup>27</sup> FLOP للتدريب أو 10<sup>20</sup> FLOP/s للاستنتاج | محظور في انتظار رفع الحد الأقصى للحوسبة المتفق عليه دولياً | محظور في انتظار رفع الحد الأقصى للحوسبة المتفق عليه دولياً |

تصنيفات المخاطر ومعايير السلامة/الأمن، مع مستويات قائمة على عتبات الحوسبة وكذلك مجموعات من الاستقلالية العالية والعمومية والذكاء:

- *الاستقلالية القوية* تنطبق إذا كان النظام قادراً على أداء، أو يمكن بسهولة جعله يؤدي، مهام متعددة الخطوات و/أو اتخاذ إجراءات معقدة ذات صلة بالعالم الحقيقي، دون إشراف أو تدخل بشري كبير. أمثلة: المركبات والروبوتات المستقلة؛ روبوتات التداول المالي. أمثلة مضادة: GPT-4؛ مصنفات الصور
- *العمومية القوية* تشير إلى نطاق واسع من التطبيق، وأداء مهام لم يُدرب النموذج عليها عمداً وتحديداً، وقدرة كبيرة على تعلم مهام جديدة. أمثلة: GPT-4؛ mu-zero. أمثلة مضادة: AlphaFold؛ المركبات المستقلة؛ مولدات الصور
- *الذكاء القوي* يقابل مطابقة أداء مستوى الخبير البشري في المهام التي يؤديها النموذج بشكل أفضل (وبالنسبة للنموذج العام، عبر نطاق واسع من المهام.) أمثلة: AlphaFold؛ mu-zero؛ o3. أمثلة مضادة: GPT-4؛ Siri

### شكر وتقدير

بعض كلمات الشكر للأشخاص الذين ساهموا في "لنحافظ على مستقبل إنساني".

يعكس هذا العمل آراء المؤلف الشخصية ولا ينبغي اعتباره الموقف الرسمي لمعهد مستقبل الحياة (رغم أن الآراء متوافقة؛ لمعرفة موقفه الرسمي انظر [هذه الصفحة](https://futureoflife.org/our-position-on-ai/))، أو أي منظمة أخرى مرتبط بها المؤلف.

أتوجه بالشكر إلى البشر مارك براكيل، وبين أيزنبريس، وآنا هيهير، وكارلوس غوتيريز، وإيميليا جافورسكي، وريتشارد مالاه، وجوردان شارنهورست، وإيليز فولشر، وماكس تيغمارك، ويان تالين على تعليقاتهم على المخطوطة؛ ولتيم شرير على مساعدته في بعض المراجع؛ ولتايلور جونز وإيليز فولشر على تحسين الرسوم البيانية.

استخدم هذا العمل بشكل محدود نماذج الذكاء الاصطناعي التوليدي (Claude وChatGPT) في إنتاجه، لبعض التحرير والمراجعة النقدية. وفقاً للمعيار الراسخ لمستويات مشاركة الذكاء الاصطناعي في الأعمال الإبداعية، يمكن أن يحصل هذا العمل على تقييم 3/10. (في الواقع لا يوجد مثل هذا المعيار! لكن يجب أن يكون هناك معيار.)

نتوجه بجزيل الشكر إلى [يوليوس أوداي](https://www.linkedin.com/in/julius-odai/) لإنتاج النسخة الإلكترونية من هذا المقال، والتي تجعل قراءة المقال والتنقل فيه تجربة ممتعة جداً. يوليوس تقني ومشارك حديث في دورة حوكمة الذكاء الاصطناعي من BlueDot Impact.