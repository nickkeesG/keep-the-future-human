# Gardons l'avenir humain

Cet essai présente les arguments sur les raisons et les moyens de fermer la porte à l'IAG et à la superintelligence, ainsi que sur ce que nous devrions construire à la place.

Si vous souhaitez simplement connaître les points essentiels, consultez le résumé exécutif. Ensuite, les chapitres 2 à 5 fourniront un contexte sur les types de systèmes d'IA abordés dans l'essai. Les chapitres 5 à 7 expliquent pourquoi nous pourrions nous attendre à voir l'IAG arriver bientôt, et ce qui pourrait se passer lorsque cela se produira. Enfin, les chapitres 8 et 9 présentent une proposition concrète pour empêcher la construction de l'IAG.

[Télécharger le PDF](https://keepthefuturehuman.ai/wp-content/uploads/2025/03/Keep_the_Future_Human__AnthonyAguirre__5March2025.pdf)

Temps de lecture total : 2-3 heures

## Résumé exécutif

Un aperçu général de l'essai. Si vous manquez de temps, découvrez tous les points principaux en seulement 10 minutes.

Les avancées spectaculaires de l'intelligence artificielle au cours de la dernière décennie (pour l'IA spécialisée) et de ces dernières années (pour l'IA généraliste) ont transformé l'IA d'un domaine académique de niche en stratégie commerciale centrale de nombreuses des plus grandes entreprises mondiales, avec des centaines de milliards de dollars d'investissement annuel dans les techniques et technologies visant à faire progresser les capacités de l'IA.

Nous arrivons maintenant à un moment critique. Alors que les capacités des nouveaux systèmes d'IA commencent à égaler et dépasser celles des humains dans de nombreux domaines cognitifs, l'humanité doit décider : jusqu'où allons-nous, et dans quelle direction ?

L'IA, comme toute technologie, a commencé avec l'objectif d'améliorer les choses pour son créateur. Mais notre trajectoire actuelle, et notre choix implicite, consiste en une course effrénée vers des systèmes toujours plus puissants, motivée par les incitations économiques de quelques entreprises technologiques géantes cherchant à automatiser de larges pans de l'activité économique actuelle et du travail humain. Si cette course se poursuit encore longtemps, il y a un vainqueur inévitable : l'IA elle-même – une alternative plus rapide, plus intelligente et moins chère aux humains dans notre économie, notre réflexion, nos décisions, et finalement aux commandes de notre civilisation.

Mais nous pouvons faire un autre choix : par l'intermédiaire de nos gouvernements, nous pouvons reprendre le contrôle du processus de développement de l'IA pour imposer des limites claires, des lignes que nous ne franchirons pas, et des choses que nous ne ferons tout simplement pas – comme nous l'avons fait pour les technologies nucléaires, les armes de destruction massive, les armes spatiales, les processus destructeurs pour l'environnement, la bioingénierie humaine et l'eugénisme. Plus important encore, nous pouvons nous assurer que l'IA reste un outil pour autonomiser les humains, plutôt qu'une nouvelle espèce qui nous remplace et finit par nous supplanter.

Cet essai soutient que nous devrions *garder l'avenir humain* en fermant les « portes » à l'IA autonome et généraliste plus intelligente que les humains – parfois appelée « IAG » – et en particulier à la version hautement surhumaine parfois appelée « superintelligence ». Au lieu de cela, nous devrions nous concentrer sur des outils d'IA puissants et fiables qui peuvent autonomiser les individus et améliorer de façon transformatrice les capacités des sociétés humaines à faire ce qu'elles font de mieux. La structure de cet argument suit brièvement.

### L'IA est différente

Les systèmes d'IA sont fondamentalement différents des autres technologies. Alors que les logiciels traditionnels suivent des instructions précises, les systèmes d'IA apprennent à atteindre des objectifs sans qu'on leur dise explicitement comment faire. Cela les rend puissants : si nous pouvons définir clairement l'objectif ou une mesure de réussite, dans la plupart des cas un système d'IA peut apprendre à l'atteindre. Mais cela les rend aussi intrinsèquement imprévisibles : nous ne pouvons pas déterminer de manière fiable quelles actions ils prendront pour atteindre leurs objectifs.

Ils sont également largement inexplicables : bien qu'ils soient en partie du code, ils sont surtout un ensemble énorme de nombres insondables – les « pondérations » des réseaux de neurones – qui ne peuvent être analysés ; nous ne sommes guère meilleurs pour comprendre leur fonctionnement interne que pour discerner des pensées en scrutant l'intérieur d'un cerveau biologique.

Ce mode central d'entraînement des réseaux de neurones numériques gagne rapidement en complexité. Les systèmes d'IA les plus puissants sont créés par des expériences computationnelles massives, utilisant du matériel spécialisé pour entraîner les réseaux de neurones sur d'énormes ensembles de données, qui sont ensuite augmentés d'outils logiciels et de superstructures.

Cela a mené à la création d'outils très puissants pour créer et traiter le texte et les images, effectuer un raisonnement mathématique et scientifique, agréger l'information, et interroger de manière interactive un vaste stock de connaissances humaines.

Malheureusement, bien que le développement d'outils technologiques plus puissants et plus fiables soit ce que nous *devrions* faire, et ce que pratiquement tout le monde veut et dit vouloir, ce n'est pas la trajectoire sur laquelle nous sommes réellement.

### L'IAG et la superintelligence

Depuis l'aube du domaine, la recherche en IA s'est plutôt concentrée sur un objectif différent : l'Intelligence Artificielle Générale. Cette orientation est maintenant devenue le focus des entreprises titanesques qui mènent le développement de l'IA.

Qu'est-ce que l'IAG ? Elle est souvent vaguement définie comme « l'IA au niveau humain », mais c'est problématique : quels humains, et pour quelles capacités est-elle au niveau humain ? Et qu'en est-il des capacités surhumaines qu'elle a déjà ? Une façon plus utile de comprendre l'IAG passe par l'intersection de trois propriétés clés : haute **A**utonomie (indépendance d'action), haute **G**énéralité (portée large et adaptabilité), et haute **I**ntelligence (compétence aux tâches cognitives). Les systèmes d'IA actuels peuvent être très capables mais étroits, ou généraux mais nécessitant une supervision humaine constante, ou autonomes mais limités en portée.

Une IAG complète combinerait ces trois propriétés à des niveaux égalant ou dépassant les capacités humaines de pointe. De manière critique, c'est cette combinaison qui rend les humains si efficaces et si différents des logiciels actuels ; c'est aussi ce qui permettrait aux gens d'être remplacés en bloc par des systèmes numériques.

Bien que l'intelligence humaine soit spéciale, elle ne constitue en aucun cas une limite. Des systèmes artificiels « superintelligents » pourraient opérer des centaines de fois plus vite, analyser vastement plus de données et garder d'énormes quantités « à l'esprit » en même temps, et former des agrégats bien plus grands et plus efficaces que des collectifs d'humains. Ils pourraient supplanter non pas des individus mais des entreprises, des nations, ou notre civilisation dans son ensemble.

### Nous sommes au seuil

Il existe un fort consensus scientifique que l'IAG est *possible*. L'IA dépasse déjà les performances humaines dans de nombreux tests généraux de capacité intellectuelle, y compris récemment le raisonnement et la résolution de problèmes de haut niveau. Les capacités en retard – comme l'apprentissage continu, la planification, la conscience de soi et l'originalité – existent toutes à un certain niveau dans les systèmes d'IA actuels, et des techniques connues existent qui sont susceptibles de toutes les améliorer.

Alors que jusqu'à il y a quelques années de nombreux chercheurs voyaient l'IAG comme étant à des décennies de distance, actuellement les preuves de délais courts vers l'IAG sont fortes :

- Les « lois d'échelle » empiriquement vérifiées relient l'apport computationnel à la capacité de l'IA, et les entreprises sont en voie d'augmenter l'apport computationnel de plusieurs ordres de grandeur au cours des prochaines années. Les ressources humaines et financières dédiées à l'avancement de l'IA égalent maintenant celles d'une douzaine de Projets Manhattan et de plusieurs Projets Apollo.
- Les entreprises d'IA et leurs dirigeants croient publiquement et privément que l'IAG (selon une certaine définition) est réalisable dans quelques années. Ces entreprises ont des informations que le public n'a pas, notamment certaines ayant la prochaine génération de systèmes d'IA en main.
- Les prédicteurs experts avec des historiques prouvés assignent 25% de probabilité à l'arrivée de l'IAG (selon une certaine définition) dans 1-2 ans, et 50% pour 2-5 ans (voir les prédictions Metaculus pour l'IAG ['faible'](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) et ['complète'](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/)).
- L'autonomie (incluant la planification flexible à long terme) accuse un retard dans les systèmes d'IA, mais les grandes entreprises concentrent maintenant leurs vastes ressources sur le développement de systèmes d'IA autonomes et ont informellement nommé 2025 l'[« année de l'agent »](https://techinformed.com/2025-informed-the-year-of-agentic-ai/).
- L'IA contribue de plus en plus à sa propre amélioration. Une fois que les systèmes d'IA seront aussi compétents que les chercheurs humains en IA pour faire de la recherche en IA, un seuil critique pour un progrès rapide vers des systèmes d'IA beaucoup plus puissants sera atteint et mènera probablement à un emballement des capacités de l'IA. (On pourrait soutenir que cet emballement a déjà commencé.)

L'idée que l'IAG plus intelligente que les humains est à des décennies de distance ou plus n'est simplement plus tenable pour la grande majorité des experts du domaine. Les désaccords portent maintenant sur le nombre de mois ou d'années que cela prendra si nous restons sur cette voie. La question centrale à laquelle nous faisons face est : devrions-nous ?

### Ce qui alimente la course à l'IAG

La course vers l'IAG est alimentée par de multiples forces, chacune rendant la situation plus dangereuse. Les grandes entreprises technologiques voient l'IAG comme la technologie d'automatisation ultime – non seulement augmentant les travailleurs humains mais les remplaçant largement ou entièrement. Pour les entreprises, l'enjeu est énorme : l'opportunité de capturer une fraction significative des 100 billions de dollars de production économique annuelle mondiale en automatisant les coûts de main-d'œuvre humaine.

Les nations se sentent contraintes de rejoindre cette course, citant publiquement le leadership économique et scientifique, mais considérant privément l'IAG comme une révolution potentielle dans les affaires militaires comparable aux armes nucléaires. La peur que les rivaux puissent obtenir un avantage stratégique décisif crée une dynamique classique de course aux armements.

Ceux qui poursuivent la superintelligence citent souvent de grandes visions : guérir toutes les maladies, inverser le vieillissement, réaliser des percées dans l'énergie et les voyages spatiaux, ou créer des capacités de planification surhumaines.

De façon moins charitable, ce qui alimente la course c'est le pouvoir. Chaque participant – qu'il s'agisse d'une entreprise ou d'un pays – croit que l'intelligence égale le pouvoir, et qu'il sera le meilleur gardien de ce pouvoir.

Je soutiens que ces motivations sont réelles mais fondamentalement mal orientées : l'IAG *absorbera* et *recherchera* le pouvoir plutôt que de l'accorder ; les technologies créées par l'IA seront *aussi* fortement à double tranchant, et là où elles sont bénéfiques elles peuvent être créées avec des outils d'IA et sans IAG ; et même dans la mesure où l'IAG et ses productions restent sous contrôle, ces dynamiques de course – à la fois corporatives et géopolitiques – rendent les risques à grande échelle pour notre société presque inévitables à moins d'être interrompus de manière décisive.

### L'IAG et la superintelligence posent une menace dramatique à la civilisation

Malgré leur attrait, l'IAG et la superintelligence posent des menaces dramatiques à la civilisation par de multiples voies qui se renforcent :

*Concentration du pouvoir :* l'IA surhumaine pourrait désautonomiser la grande majorité de l'humanité en absorbant d'énormes pans d'activité sociale et économique dans des systèmes d'IA gérés par une poignée d'entreprises géantes (qui peuvent à leur tour soit être reprises par les gouvernements, soit effectivement les reprendre).

*Perturbation massive :* l'automatisation en bloc de la plupart des emplois cognitifs, le remplacement de nos systèmes épistémiques actuels, et le déploiement de vastes nombres d'agents non-humains actifs bouleverseraient la plupart de nos systèmes civilisationnels actuels dans une période relativement courte.

*Catastrophes :* en proliférant la capacité – potentiellement au-dessus du niveau humain – de créer de nouvelles technologies militaires et destructrices et en la découplant des systèmes sociaux et légaux fondant la responsabilité, les catastrophes physiques dues aux armes de destruction massive deviennent dramatiquement plus probables.

*Géopolitique et guerre :* les grandes puissances mondiales ne resteront pas les bras croisés si elles sentent qu'une technologie qui pourrait fournir un « avantage stratégique décisif » est développée par leurs adversaires.

*Emballement et perte de contrôle :* À moins que cela ne soit spécifiquement empêché, l'IA surhumaine aura toutes les incitations à s'améliorer davantage et pourrait largement dépasser les humains en vitesse, traitement de données, et sophistication de la pensée. Il n'y a aucune façon significative dans laquelle nous pouvons être aux commandes d'un tel système. Une telle IA n'accordera pas le pouvoir aux humains ; nous lui accorderons le pouvoir, ou elle le prendra.

Beaucoup de ces risques demeurent même si le problème technique d'« alignement » – s'assurer que l'IA avancée fait de manière fiable ce que les humains veulent qu'elle fasse – est résolu. L'IA présente un énorme défi dans la façon dont elle sera gérée, et de très nombreux aspects de cette gestion deviennent incroyablement difficiles ou insolubles une fois que l'intelligence humaine est dépassée.

Plus fondamentalement, le type d'IA généraliste surhumaine actuellement poursuivi aurait, par sa nature même, des objectifs, une agence et des capacités dépassant les nôtres. Elle serait intrinsèquement incontrôlable – comment pouvons-nous contrôler quelque chose que nous ne pouvons ni comprendre ni prédire ? Elle ne serait pas un outil technologique pour l'usage humain, mais une seconde espèce d'intelligence sur Terre à côté de la nôtre. Si on lui permettait de progresser davantage, elle ne constituerait pas seulement une seconde espèce mais une espèce de remplacement.

Peut-être nous traiterait-elle bien, peut-être pas. Mais l'avenir lui appartiendrait, pas à nous. L'ère humaine serait terminée.

### Ce n'est pas inévitable ; l'humanité peut, très concrètement, décider de ne pas construire son remplacement.

La création d'une IAG surhumaine est loin d'être inévitable. Nous pouvons l'empêcher par un ensemble coordonné de mesures de gouvernance :

D'abord, nous avons besoin d'une comptabilité robuste et d'une supervision du calcul IA (la « puissance de calcul »), qui est un facilitateur fondamental et un levier pour gouverner les systèmes d'IA à grande échelle. Cela nécessite à son tour une mesure et un rapport standardisés du calcul total utilisé pour entraîner les modèles d'IA et les faire fonctionner, et des méthodes techniques de comptage, certification et vérification du calcul utilisé.

Deuxièmement, nous devrions implémenter des plafonds stricts sur le calcul IA, à la fois pour l'entraînement et pour l'opération ; ceux-ci empêchent l'IA d'être à la fois trop puissante et d'opérer trop rapidement. Ces plafonds peuvent être implémentés par des exigences légales et des mesures de sécurité basées sur le matériel intégrées dans les puces spécialisées pour l'IA, analogues aux fonctions de sécurité dans les téléphones modernes. Parce que le matériel spécialisé pour l'IA est fabriqué par seulement une poignée d'entreprises, la vérification et l'application sont faisables par la chaîne d'approvisionnement existante.

Troisièmement, nous avons besoin d'une responsabilité renforcée pour les systèmes d'IA les plus dangereux. Ceux qui développent une IA qui combine haute autonomie, large généralité et intelligence supérieure devraient faire face à une responsabilité objective pour les dommages, tandis que des régimes d'exonération de cette responsabilité encourageraient le développement de systèmes plus limités et contrôlables.

Quatrièmement, nous avons besoin d'une réglementation à niveaux basée sur les niveaux de risque. Les systèmes les plus capables et dangereux nécessiteraient des garanties étendues de sécurité et contrôlabilité avant développement et déploiement, tandis que les systèmes moins puissants ou plus spécialisés feraient face à une supervision proportionnée. Ce cadre réglementaire devrait éventuellement opérer aux niveaux national et international.

Cette approche – avec une spécification détaillée donnée dans le document complet – est pratique : bien qu'une coordination internationale soit nécessaire, la vérification et l'application peuvent fonctionner par le petit nombre d'entreprises contrôlant la chaîne d'approvisionnement de matériel spécialisé. Elle est aussi flexible : les entreprises peuvent encore innover et profiter du développement de l'IA, juste avec des limites claires sur les systèmes les plus dangereux.

Le confinement à long terme du pouvoir et du risque de l'IA nécessiterait des accords internationaux basés sur l'intérêt propre et commun, tout comme le contrôle de la prolifération des armes nucléaires le fait maintenant. Mais nous pouvons commencer immédiatement avec une supervision et une responsabilité renforcées, tout en construisant vers une gouvernance plus complète.

L'ingrédient clé manquant est la volonté politique et sociale de reprendre le contrôle du processus de développement de l'IA. La source de cette volonté, si elle vient à temps, sera la réalité elle-même – c'est-à-dire, de la prise de conscience généralisée des vraies implications de ce que nous faisons.

### Nous pouvons concevoir une IA-outil pour autonomiser l'humanité

Plutôt que de poursuivre une IAG incontrôlable, nous pouvons développer une « IA-outil » puissante qui améliore les capacités humaines tout en restant sous un contrôle humain significatif. Les systèmes d'IA-outil peuvent être extrêmement capables tout en évitant la dangereuse triple intersection de haute autonomie, large généralité et intelligence surhumaine, tant que nous les concevons pour être contrôlables à un niveau proportionné à leur capacité. Ils peuvent aussi être combinés en systèmes sophistiqués qui maintiennent la supervision humaine tout en délivrant des bénéfices transformateurs.

L'IA-outil peut révolutionner la médecine, accélérer la découverte scientifique, améliorer l'éducation et perfectionner les processus démocratiques. Quand elle est correctement gouvernée, elle peut rendre les experts humains et les institutions plus efficaces plutôt que de les remplacer. Bien que de tels systèmes soient encore hautement perturbateurs et nécessitent une gestion attentive, les risques qu'ils posent sont fondamentalement différents de l'IAG : ce sont des risques que nous pouvons gouverner, comme ceux d'autres technologies puissantes, pas des menaces existentielles à l'agence humaine et à la civilisation. Et crucialement, quand elle est sagement développée, l'IA-outil peut aider les gens à gouverner l'IA puissante et gérer ses effets.

Cette approche nécessite de repenser à la fois comment l'IA est développée et comment ses bénéfices sont distribués. De nouveaux modèles de développement d'IA public et à but non lucratif, des cadres réglementaires robustes, et des mécanismes pour distribuer les bénéfices économiques plus largement peuvent aider à s'assurer que l'IA autonomise l'humanité dans son ensemble plutôt que de concentrer le pouvoir en quelques mains. L'IA elle-même peut aider à construire de meilleures institutions sociales et de gouvernance, permettant de nouvelles formes de coordination et de discours qui renforcent plutôt qu'affaiblissent la société humaine. Les établissements de sécurité nationale peuvent mobiliser leur expertise pour rendre les systèmes d'IA-outil véritablement sûrs et fiables, et une vraie source de défense ainsi que de puissance nationale.

Nous pourrions éventuellement choisir de développer des systèmes encore plus puissants et plus souverains qui ressemblent moins à des outils et – nous pouvons l'espérer – plus à des bienfaiteurs sages et puissants. Mais nous ne devrions le faire qu'après avoir développé la compréhension scientifique et la capacité de gouvernance pour le faire en sécurité. Une décision si monumentale et irréversible devrait être prise délibérément par l'humanité dans son ensemble, pas par défaut dans une course entre entreprises technologiques et nations.

### Entre les mains humaines

Les gens veulent le bien qui vient de l'IA : des outils utiles qui les autonomisent, dynamisent les opportunités et la croissance économiques, et promettent des percées en science, technologie et éducation. Pourquoi pas ? Mais quand on leur demande, d'écrasantes majorités du grand public [veulent un développement de l'IA plus lent et plus prudent](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation), et ne veulent pas d'IA plus intelligente que les humains qui les remplacera dans leurs emplois et ailleurs, remplira leur culture et bien commun informationnel de contenu non-humain, concentrera le pouvoir dans un petit ensemble d'entreprises, posera des risques extrêmes à grande échelle globale, et finalement menacera de désautonomiser ou remplacer leur espèce. Pourquoi le voudraient-ils ?

Nous *pouvons* avoir l'un sans l'autre. Cela commence par décider que notre destin n'est pas dans la supposée inévitabilité d'une technologie ou entre les mains de quelques PDG de la Silicon Valley, mais dans le reste de nos mains si nous nous en saisissons. Fermons les Portes, et gardons l'avenir humain.

## Chapitre 1 - Introduction

La façon dont nous réagirons à la perspective d'une IA plus intelligente que l'humain est l'enjeu le plus pressant de notre époque. Cet essai propose une voie à suivre.

Nous nous trouvons peut-être à la fin de l'ère humaine.

Quelque chose a commencé au cours des dix dernières années qui est unique dans l'histoire de notre espèce. Ses conséquences détermineront, dans une large mesure, l'avenir de l'humanité. À partir de 2015 environ, les chercheurs ont réussi à développer une intelligence artificielle (IA) *spécialisée* – des systèmes capables de gagner à des jeux comme le Go, de reconnaître des images et la parole, et bien d'autres choses, mieux que n'importe quel humain.[^1]

C'est un succès remarquable, qui produit des systèmes et des produits extrêmement utiles qui renforceront les capacités humaines. Mais l'intelligence artificielle spécialisée n'a jamais été le véritable objectif du domaine. L'ambition a plutôt été de créer des systèmes d'IA *polyvalents*, notamment ceux souvent appelés « intelligence artificielle générale » (IAG) ou « superintelligence », qui sont simultanément aussi performants ou meilleurs que les humains dans presque *toutes* les tâches, tout comme l'IA surpasse aujourd'hui l'humain au Go, aux échecs, au poker, aux courses de drones, etc. C'est l'objectif déclaré de nombreuses grandes entreprises d'IA.[^2]

*Ces efforts réussissent également.* Les systèmes d'IA polyvalents comme ChatGPT, Gemini, Llama, Grok, Claude et Deepseek, basés sur des calculs massifs et des montagnes de données, ont atteint la parité avec les humains ordinaires dans une grande variété de tâches, et égalent même les experts humains dans certains domaines. Aujourd'hui, les ingénieurs en IA de quelques-unes des plus grandes entreprises technologiques s'efforcent de pousser ces expériences géantes d'intelligence artificielle vers les niveaux suivants, où elles égaleront puis dépasseront toute la gamme des capacités, expertises et autonomie humaines.

*Cela est imminent.* Au cours des dix dernières années, les estimations d'experts sur le temps que cela prendra – si nous poursuivons notre trajectoire actuelle – sont passées de décennies (ou de siècles) à quelques années seulement.

C'est aussi d'une importance historique et représente un risque transcendant. Les partisans de l'IAG y voient une transformation positive qui résoudra les problèmes scientifiques, guérira les maladies, développera de nouvelles technologies et automatisera les corvées. Et l'IA pourrait certainement contribuer à réaliser tout cela – d'ailleurs, elle le fait déjà. Mais au fil des décennies, de nombreux penseurs attentifs, d'Alan Turing à Stephen Hawking en passant par Geoffrey Hinton et Yoshua Bengio d'aujourd'hui[^3], ont lancé un avertissement sévère : construire une IA véritablement plus intelligente que l'humain, générale et autonome bouleversera au minimum complètement et irréversiblement la société, et au maximum entraînera l'extinction humaine.[^4]

La superintelligence artificielle approche rapidement sur notre trajectoire actuelle, mais elle est loin d'être inévitable. Cet essai est un argument développé expliquant pourquoi et comment nous devrions *fermer les Portes* à cet avenir inhumain qui approche, et ce que nous devrions faire à la place.


[^1]: Ce [graphique](https://time.com/6300942/ai-progress-charts/) montre un ensemble de tâches ; de nombreuses courbes similaires pourraient être ajoutées à ce graphique. Ces progrès rapides de l'IA spécialisée ont surpris même les experts du domaine, avec des références dépassées des années avant les prédictions.

[^2]: Deepmind, OpenAI, Anthropic et X.ai ont tous été fondés avec l'objectif spécifique de développer l'IAG. Par exemple, la charte d'OpenAI énonce explicitement son objectif comme développer « une intelligence artificielle générale qui profite à toute l'humanité », tandis que la mission de DeepMind est « résoudre l'intelligence, puis l'utiliser pour tout résoudre ». Meta, Microsoft et d'autres poursuivent désormais des voies substantiellement similaires. Meta a déclaré qu'elle [prévoit de développer l'IAG et de la diffuser ouvertement.](https://www.forbes.com/sites/johnkoetsier/2024/01/18/zuckerberg-on-ai-meta-building-agi-for-everyone-and-open-sourcing-it/)

[^3]: Hinton et Bengio sont deux des chercheurs en IA les plus cités, ont tous deux remporté le Nobel de l'IA, le prix Turing, et Hinton a en plus remporté un prix Nobel (en physique).

[^4]: Construire quelque chose de ce niveau de risque, sous des incitations commerciales et une surveillance gouvernementale quasi nulle, est absolument sans précédent. Il n'y a même pas de controverse sur le risque parmi ceux qui le construisent ! Les dirigeants de Deepmind, OpenAI et Anthropic, parmi de nombreux autres experts, ont tous littéralement signé une [déclaration](https://www.safe.ai/work/statement-on-ai-risk) affirmant que l'IA avancée pose un *risque d'extinction pour l'humanité.* Les signaux d'alarme ne pourraient pas sonner plus fort, et on ne peut que conclure que ceux qui les ignorent ne prennent tout simplement pas l'IAG et la superintelligence au sérieux. L'un des objectifs de cet essai est de les aider à comprendre pourquoi ils le devraient.

## Chapitre 2 - Les bases des réseaux de neurones en IA

Comment fonctionnent les systèmes d'IA modernes, et à quoi peut-on s'attendre de la prochaine génération d'IA ?

Pour comprendre comment se déploieront les conséquences du développement d'une IA plus puissante, il est essentiel d'intégrer quelques notions fondamentales. Cette section et les deux suivantes développent ces concepts, couvrant tour à tour ce qu'est l'IA moderne, comment elle exploite des calculs massifs, et les façons dont elle croît rapidement en généralité et en capacité.[^5]

Il existe de nombreuses façons de définir l'intelligence artificielle, mais pour nos propos, la propriété clé de l'IA est que tandis qu'un programme informatique standard est une liste d'instructions pour accomplir une tâche, un système d'IA apprend à partir de données ou d'expériences pour accomplir des tâches *sans qu'on lui dise explicitement comment procéder.*

Presque toute l'IA moderne significative repose sur des réseaux de neurones. Il s'agit de structures mathématiques/computationnelles, représentées par un très grand ensemble (milliards ou billions) de nombres (« poids »), qui accomplissent bien une tâche d'entraînement. Ces poids sont façonnés (ou peut-être « cultivés » ou « découverts ») en les ajustant de manière itérative pour que le réseau de neurones améliore un score numérique (appelé « perte ») défini pour bien accomplir une ou plusieurs tâches.[^6] Ce processus est appelé *entraînement* du réseau de neurones.[^7]

Il existe de nombreuses techniques pour cet entraînement, mais ces détails sont bien moins pertinents que les façons dont le score est défini, et comment celles-ci aboutissent aux différentes tâches que le réseau de neurones accomplit bien. Une distinction clé a historiquement été établie entre l'IA « étroite » et « générale ».

L'IA étroite est délibérément entraînée pour faire une tâche particulière ou un petit ensemble de tâches (comme reconnaître des images ou jouer aux échecs) ; elle nécessite un réentraînement pour de nouvelles tâches, et a un champ de capacités restreint. Nous avons de l'IA étroite surhumaine, ce qui signifie que pour presque toute tâche discrète et bien définie qu'une personne peut faire, nous pouvons probablement construire un score puis entraîner avec succès un système d'IA étroite à le faire mieux qu'un humain.

Les systèmes d'IA à usage général (IAUG) peuvent accomplir un large éventail de tâches, y compris beaucoup pour lesquelles ils n'ont pas été explicitement entraînés ; ils peuvent aussi apprendre de nouvelles tâches dans le cadre de leur fonctionnement. Les « modèles multimodaux » actuels[^8] comme ChatGPT illustrent cela : entraînés sur un très large corpus de texte et d'images, ils peuvent s'engager dans un raisonnement complexe, écrire du code, analyser des images, et assister dans un vaste éventail de tâches intellectuelles. Bien qu'encore assez différents de l'intelligence humaine de façons que nous verrons en détail ci-dessous, leur généralité a causé une révolution en IA.[^9]

### L'imprévisibilité : une caractéristique clé des systèmes d'IA

Une différence clé entre les systèmes d'IA et les logiciels conventionnels réside dans la prévisibilité. La sortie d'un logiciel standard peut être imprévisible – en effet, c'est parfois pour cela que nous écrivons des logiciels, pour obtenir des résultats que nous n'aurions pas pu prédire. Mais les logiciels conventionnels font rarement quelque chose pour lequel ils n'ont pas été programmés – leur portée et leur comportement sont généralement conformes à leur conception. Un programme d'échecs de premier plan peut faire des coups qu'aucun humain ne pourrait prédire (sinon ils pourraient battre ce programme d'échecs !) mais il ne fera généralement rien d'autre que jouer aux échecs.

Comme les logiciels conventionnels, l'IA étroite a une portée et un comportement prévisibles mais peut avoir des résultats imprévisibles. C'est en réalité juste une autre façon de définir l'IA étroite : comme une IA qui ressemble aux logiciels conventionnels dans sa prévisibilité et son champ d'action.

L'IA à usage général est différente : sa portée (les domaines auxquels elle s'applique), son comportement (les types de choses qu'elle fait), et ses résultats (ses sorties réelles) peuvent tous être imprévisibles.[^10] GPT-4 a été entraîné uniquement pour générer du texte avec précision, mais a développé de nombreuses capacités que ses entraîneurs n'avaient pas prédites ou voulues. Cette imprévisibilité découle de la complexité de l'entraînement : parce que les données d'entraînement contiennent des sorties de nombreuses tâches différentes, l'IA doit effectivement apprendre à accomplir ces tâches pour bien prédire.

Cette imprévisibilité des systèmes d'IA générale est assez fondamentale. Bien qu'en principe il soit possible de construire soigneusement des systèmes d'IA qui ont des limites garanties sur leur comportement (comme mentionné plus tard dans l'essai), la façon dont les systèmes d'IA sont créés maintenant les rend imprévisibles en pratique et même en principe.

### IA passive, agents, systèmes autonomes, et alignement

Cette imprévisibilité devient particulièrement importante quand nous considérons comment les systèmes d'IA sont réellement déployés et utilisés pour atteindre divers objectifs.

De nombreux systèmes d'IA sont relativement passifs en ce sens qu'ils fournissent principalement de l'information, et l'utilisateur prend des actions. D'autres, communément appelés *agents*, prennent eux-mêmes des actions, avec des niveaux variables d'implication de l'utilisateur. Ceux qui prennent des actions avec relativement moins d'input ou de supervision externes peuvent être qualifiés de plus *autonomes*. Cela forme un spectre en termes d'indépendance d'action, d'outils passifs à agents autonomes.[^11]

Quant aux objectifs des systèmes d'IA, ceux-ci peuvent être directement liés à leur objectif d'entraînement (par exemple l'objectif de « gagner » pour un système qui joue au Go est aussi explicitement ce pour quoi il a été entraîné). Ou ils peuvent ne pas l'être : l'objectif d'entraînement de ChatGPT est en partie de prédire le texte, en partie d'être un assistant utile. Mais quand il fait une tâche donnée, son objectif lui est fourni par l'utilisateur. Les objectifs peuvent aussi être créés par un système d'IA lui-même, seulement très indirectement liés à son objectif d'entraînement.[^12]

Les objectifs sont étroitement liés à la question de l'« alignement », c'est-à-dire la question de savoir si les systèmes d'IA vont *faire ce que nous voulons qu'ils fassent*. Cette question simple cache un niveau énorme de subtilité.[^13] Pour l'instant, notez que « nous » dans cette phrase pourrait se référer à de nombreuses personnes et groupes différents, menant à différents types d'alignement. Par exemple, une IA pourrait être très *obéissante* (ou [« loyale »](https://arxiv.org/abs/2003.11157)) à son utilisateur – ici « nous » c'est « chacun de nous ». Ou elle pourrait être plus *souveraine*, étant principalement guidée par ses propres objectifs et contraintes, mais agissant toujours largement dans l'intérêt commun du bien-être humain – « nous » c'est alors « l'humanité » ou « la société ». Entre les deux se trouve un spectre où une IA serait largement obéissante, mais pourrait refuser de prendre des actions qui nuisent aux autres ou à la société, violent la loi, etc.

Ces deux axes – niveau d'autonomie et type d'alignement – ne sont pas entièrement indépendants. Par exemple, un système passif souverain, bien que pas tout à fait auto-contradictoire, est un concept en tension, comme l'est un agent autonome obéissant.[^14] Il y a un sens clair dans lequel l'autonomie et la souveraineté tendent à aller de pair. Dans la même veine, la prévisibilité tend à être plus élevée dans les systèmes d'IA « passifs » et « obéissants », tandis que les souverains ou autonomes tendront à être plus imprévisibles. Tout cela sera crucial pour comprendre les ramifications de l'IAG et de la superintelligence potentielles.

Créer une IA vraiment alignée, de quelque type que ce soit, nécessite de résoudre trois défis distincts :

1. Comprendre ce que « nous » voulons – ce qui est complexe que « nous » signifie une personne ou organisation spécifique (loyauté) ou l'humanité au sens large (souveraineté) ;
2. Construire des systèmes qui agissent régulièrement en accord avec ces volontés – essentiellement créer un comportement positif cohérent ;
3. Plus fondamentalement, faire des systèmes qui « se soucient » genuinement de ces volontés plutôt que d'agir simplement comme s'ils s'en souciaient.

La distinction entre comportement fiable et souci genuine est cruciale. Tout comme un employé humain pourrait suivre les ordres parfaitement tout en manquant de véritable engagement envers la mission de l'organisation, un système d'IA pourrait agir de façon alignée sans vraiment valoriser les préférences humaines. Nous pouvons entraîner les systèmes d'IA à dire et faire des choses par le feedback, et ils peuvent apprendre à raisonner sur ce que les humains veulent. Mais les faire *genuinement* valoriser les préférences humaines est un défi bien plus profond.[^15]

Les difficultés profondes à résoudre ces défis d'alignement, et leurs implications pour le risque d'IA, seront explorées plus loin ci-dessous. Pour l'instant, comprenez que l'alignement n'est pas juste une caractéristique technique que nous ajoutons aux systèmes d'IA, mais un aspect fondamental de leur architecture qui façonne leur relation avec l'humanité.

[^5]: Pour une introduction douce mais technique à l'apprentissage automatique et à l'IA, particulièrement les modèles de langage, voir [ce site.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) Pour un autre guide moderne sur les risques d'extinction de l'IA, voir [cette pièce.](https://www.thecompendium.ai/) Pour une analyse scientifique complète et faisant autorité de l'état de la sécurité de l'IA, voir le récent [Rapport international sur la sécurité de l'IA.](https://arxiv.org/abs/2501.17805)

[^6]: L'entraînement se produit typiquement en cherchant un maximum local du score dans un espace de haute dimension donné par les poids du modèle. En vérifiant comment le score change quand les poids sont ajustés, l'algorithme d'entraînement identifie quels ajustements améliorent le score le plus, et déplace les poids dans cette direction.

[^7]: Par exemple, dans un problème de reconnaissance d'image, le réseau de neurones sortirait des probabilités pour les étiquettes de l'image. Un score serait lié à la probabilité que l'IA accorde à la bonne réponse. La procédure d'entraînement ajusterait alors les poids pour que la prochaine fois, l'IA sorte une probabilité plus élevée pour l'étiquette correcte pour cette image. Ceci est alors répété un énorme nombre de fois. La même procédure de base est utilisée dans l'entraînement d'essentiellement tous les réseaux de neurones modernes, quoique avec des mécanismes de score plus complexes.

[^8]: La plupart des modèles multimodaux utilisent l'architecture « transformateur » pour traiter et générer plusieurs types de données (texte, images, son). Ceux-ci peuvent tous être décomposés en, puis traités sur le même pied, comme différents types de « jetons ». Les modèles multimodaux sont entraînés d'abord pour prédire précisément les jetons dans des ensembles de données massifs, puis raffinés par apprentissage par renforcement pour améliorer les capacités et façonner les comportements.

[^9]: Que les modèles de langage soient entraînés pour faire une chose – prédire les mots – a amené certains à les appeler IA étroite. Mais c'est trompeur : parce que bien prédire le texte nécessite tant de capacités différentes, cette tâche d'entraînement mène à un système étonnamment général. Notez aussi que ces systèmes sont extensivement entraînés par apprentissage par renforcement, représentant effectivement des milliers de personnes donnant au modèle un signal de récompense quand il fait du bon travail dans n'importe laquelle des nombreuses choses qu'il fait. Il hérite alors d'une généralité significative des gens donnant ce feedback.

[^10]: Il y a plusieurs façons dont l'IA est imprévisible. Une est que dans le cas général on ne peut prédire ce qu'un algorithme fera sans réellement l'exécuter ; il y a des [théorèmes](https://arxiv.org/abs/1310.3225) à cet effet. Ceci peut être vrai juste parce que la sortie d'algorithmes peut être complexe. Mais c'est particulièrement clair et pertinent dans le cas (comme aux échecs ou au Go) où la prédiction impliquerait une capacité (battre l'IA) que le prédicteur potentiel n'a pas. Deuxième, un système d'IA donné ne produira pas toujours la même sortie même avec la même entrée – ses sorties contiennent de l'aléatoire ; ceci se couple aussi avec l'imprévisibilité algorithmique. Troisième, des capacités inattendues et émergentes peuvent surgir de l'entraînement, signifiant que même les *types* de choses qu'un système d'IA peut et va faire sont imprévisibles ; Ce dernier type est particulièrement important pour les considérations de sécurité.

[^11]: Voir [ici](https://arxiv.org/abs/2502.02649) pour une revue en profondeur de ce qu'on entend par « agent autonome » (avec des arguments éthiques contre leur construction).

[^12]: Vous pouvez parfois entendre « l'IA ne peut pas avoir ses propres objectifs ». C'est du pur non-sens. Il est facile de générer des exemples où l'IA a ou développe des objectifs qui ne lui ont jamais été donnés et ne sont connus que d'elle-même. Vous ne voyez pas beaucoup cela dans les modèles multimodaux populaires actuels parce que c'est entraîné hors d'eux ; cela pourrait tout aussi facilement être entraîné en eux.

[^13]: Il y a une large littérature. Sur le problème général voir [*The Alignment Problem*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821) de Christian, et [*Human-Compatible*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) de Russell. Du côté plus technique voir par exemple [cet article](https://arxiv.org/abs/2209.00626).

[^14]: Nous verrons plus tard que tandis que de tels systèmes vont contre la tendance, cela les rend en fait très intéressants et utiles.

[^15]: Ceci ne veut pas dire que nous nécessitons des émotions ou de la sentience. Plutôt, il est énormément difficile de l'extérieur d'un système de savoir quels sont ses objectifs internes, préférences, et valeurs. « Genuine » ici signifierait que nous avons des raisons assez fortes de nous y fier que dans le cas de systèmes critiques nous pouvons parier nos vies dessus.

## Chapitre 3 - Aspects clés de la fabrication des systèmes d'IA générale modernes

La plupart des systèmes d'IA les plus avancés au monde sont créés selon des méthodes étonnamment similaires. Voici les principes de base.

Pour vraiment comprendre un être humain, il faut connaître certains aspects de la biologie, de l'évolution, de l'éducation des enfants, et bien plus encore ; pour comprendre l'IA, il faut aussi connaître sa méthode de fabrication. Au cours des cinq dernières années, les systèmes d'IA ont énormément évolué tant en capacités qu'en complexité. Un facteur clé de cette évolution a été la disponibilité de très grandes quantités de calcul (ou familièrement « puissance de calcul » dans le contexte de l'IA).

Les chiffres sont stupéfiants. Environ 10 <sup>25</sup> -10 <sup>26</sup> « opérations en virgule flottante » (FLOP) [^16] sont utilisées pour l'entraînement de modèles comme la série GPT, Claude, Gemini, etc.[^17] (À titre de comparaison, si chaque être humain sur Terre travaillait sans arrêt en effectuant un calcul toutes les cinq secondes, il faudrait environ un milliard d'années pour accomplir cela.) Cette énorme quantité de calcul permet l'entraînement de modèles comportant jusqu'à des billions de paramètres sur des téraoctets de données – une grande fraction de tout le texte de qualité qui ait jamais été écrit, accompagnée de vastes bibliothèques de sons, d'images et de vidéos. En complétant cet entraînement par un entraînement supplémentaire approfondi renforçant les préférences humaines et les bonnes performances de tâches, les modèles ainsi entraînés présentent des performances rivalisant avec l'humain sur un éventail significatif de tâches intellectuelles de base, incluant le raisonnement et la résolution de problèmes.

Nous savons également (très, très approximativement) quelle vitesse de calcul, en opérations par seconde, est suffisante pour que la vitesse d'*inférence* [^18] d'un tel système égale la *vitesse* de traitement de texte humain. Elle est d'environ 10 <sup>15</sup> -10 <sup>16</sup> FLOP par seconde.[^19]

Bien qu'ils soient puissants, ces modèles sont par nature limités de manières importantes, tout à fait analogues à la façon dont un individu humain serait limité s'il était forcé de simplement produire du texte à un rythme fixe de mots par minute, sans s'arrêter pour réfléchir ou utiliser des outils supplémentaires. Les systèmes d'IA plus récents s'attaquent à ces limitations par un processus et une architecture plus complexes combinant plusieurs éléments clés :

- Un ou plusieurs réseaux de neurones, avec un modèle fournissant la capacité cognitive centrale, et jusqu'à plusieurs autres effectuant d'autres tâches plus spécialisées ;
- Des *outils* fournis au modèle et utilisables par celui-ci – par exemple la capacité de rechercher sur le web, créer ou modifier des documents, exécuter des programmes, etc.
- Une *architecture de support* qui connecte les entrées et sorties des réseaux de neurones. Une architecture très simple pourrait simplement permettre à deux « instances » d'un modèle d'IA de converser entre elles, ou à l'une de vérifier le travail de l'autre.[^20]
- Les techniques de *chaîne de raisonnement* et d'incitation connexes font quelque chose de similaire, amenant un modèle à par exemple générer de nombreuses approches à un problème, puis traiter ces approches pour une réponse agrégée.
- Le *ré-entraînement* des modèles pour mieux utiliser les outils, l'architecture de support, et la chaîne de raisonnement.

Parce que ces extensions peuvent être très puissantes (et incluent des systèmes d'IA eux-mêmes), ces systèmes composites peuvent être assez sophistiqués et améliorer considérablement les capacités de l'IA.[^21] Et récemment, des techniques d'architecture de support et surtout d'incitation par chaîne de raisonnement (et de réintégration des résultats dans le ré-entraînement des modèles pour mieux les utiliser) ont été développées et employées dans [o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/), et [DeepSeek R1](https://api-docs.deepseek.com/news/news250120) pour effectuer de nombreux passages d'inférence en réponse à une requête donnée.[^22] Ceci permet effectivement au modèle de « réfléchir » à sa réponse et améliore considérablement la capacité de ces modèles à effectuer des raisonnements de haut niveau dans les tâches scientifiques, mathématiques et de programmation.[^23]

Pour une architecture d'IA donnée, les augmentations de calcul d'entraînement [peuvent être traduites de manière fiable](https://arxiv.org/abs/2405.10938) en améliorations dans un ensemble de métriques clairement définies. Pour des capacités générales moins précisément définies (comme celles discutées ci-dessous), la traduction est moins claire et prédictive, mais il est presque certain que des modèles plus larges avec plus de calcul d'entraînement auront des capacités nouvelles et meilleures, même s'il est difficile de prédire lesquelles.

De même, les systèmes composites et surtout les avancées dans la « chaîne de raisonnement » (et l'entraînement de modèles qui fonctionnent bien avec elle) ont débloqué la montée en puissance dans le calcul d'*inférence* : pour un modèle central entraîné donné, au moins certaines capacités du système d'IA augmentent lorsque plus de calcul est appliqué, ce qui leur permet de « réfléchir plus intensément et plus longtemps » sur des problèmes complexes. Ceci se fait au prix d'un coût de vitesse de calcul élevé, nécessitant des centaines ou des milliers de FLOP/s supplémentaires pour égaler les performances humaines.[^24]

Bien qu'elle ne soit qu'une partie de ce qui conduit aux progrès rapides de l'IA,[^25] le rôle du calcul et la possibilité de systèmes composites s'avéreront cruciaux tant pour empêcher une IAG incontrôlable que pour développer des alternatives plus sûres.


[^16]: 10 <sup>25</sup> signifie 1 suivi de 25 zéros, ou dix mille milliards de milliards. Un FLOP est simplement une addition ou multiplication arithmétique de nombres avec une certaine précision. Notez que les performances du matériel d'IA peuvent varier d'un facteur de dix selon la précision de l'arithmétique et l'architecture de l'ordinateur. Compter les opérations de portes logiques (ET, OU, ET NON) serait fondamental mais celles-ci ne sont pas communément disponibles ou étalonnées ; aux fins présentes il est utile de standardiser sur les opérations 16-bit (FP16), bien que des facteurs de conversion appropriés devraient être établis.

[^17]: Une collection d'estimations et de données concrètes est disponible chez [Epoch AI](https://epochai.org/data/large-scale-ai-models) et indique environ 2×10 <sup>25</sup> FLOP 16-bit pour GPT-4 ; ceci correspond grossièrement aux [chiffres qui ont fuité](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) pour GPT-4. Les estimations pour d'autres modèles de mi-2024 sont toutes dans un facteur de quelques-uns de GPT-4.

[^18]: L'inférence est simplement le processus de génération d'une sortie à partir d'un réseau de neurones. L'entraînement peut être considéré comme une succession de nombreuses inférences et d'ajustements de poids du modèle.

[^19]: Pour la production de texte, le GPT-4 original nécessitait 560 TFLOP par jeton généré. Environ 7 jetons/s sont nécessaires pour suivre la pensée humaine, ce qui donne ≈3×10 <sup>15</sup> FLOP/s. Mais les efficacités ont fait chuter ce chiffre ; [cette brochure NVIDIA](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/) par exemple indique aussi peu que 3×10 <sup>14</sup> FLOP/s pour un modèle Llama 405B aux performances comparables.

[^20]: Comme exemple légèrement plus complexe, un système d'IA pourrait d'abord générer plusieurs solutions possibles à un problème mathématique, puis utiliser une autre instance pour vérifier chaque solution, et finalement utiliser une troisième pour synthétiser les résultats en une explication claire. Ceci permet une résolution de problèmes plus approfondie et fiable qu'un passage unique.

[^21]: Voir par exemple les détails sur [« Operator » d'OpenAI](https://openai.com/index/introducing-operator/), [les capacités d'outils de Claude](https://docs.anthropic.com/en/docs/build-with-claude/computer-use), et [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). [Deep Research](https://openai.com/index/introducing-deep-research/) d'OpenAI a probablement une architecture assez sophistiquée mais les détails ne sont pas disponibles.

[^22]: Deepseek R1 s'appuie sur l'entraînement et l'incitation itératifs du modèle de sorte que le modèle final entraîné crée un raisonnement de chaîne de pensée étendu. Les détails architecturaux ne sont pas disponibles pour o1 ou o3, cependant Deepseek a révélé qu'il n'y a pas de « sauce secrète » particulière requise pour débloquer la montée en puissance des capacités avec l'inférence. Mais malgré avoir reçu beaucoup de presse comme bouleversant le « statu quo » en IA, cela n'impacte pas les affirmations centrales de cet essai.

[^23]: Ces modèles surpassent significativement les modèles standard sur les références de raisonnement. Par exemple, dans le GPQA Diamond Benchmark—un test rigoureux de questions scientifiques de niveau doctorat—GPT-4o a [obtenu](https://openai.com/index/learning-to-reason-with-llms/) 56%, tandis que o1 et o3 ont atteint 78% et 88%, respectivement, dépassant largement le score moyen de 70% des experts humains.

[^24]: L'O3 d'OpenAI a probablement dépensé ∼10 <sup>21</sup> -10 <sup>22</sup> FLOP [pour compléter chacune des questions du défi ARC-AGI](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai), que des humains compétents peuvent faire en (disons) 10-100 secondes, donnant un chiffre plus proche de ∼10 <sup>20</sup> FLOP/s.

[^25]: Bien que le calcul soit une mesure clé de la capacité des systèmes d'IA, il interagit avec la qualité des données et les améliorations algorithmiques. De meilleures données ou algorithmes peuvent réduire les exigences computationnelles, tandis que plus de calcul peut parfois compenser des données ou algorithmes plus faibles.

## Chapitre 4 - Qu'est-ce que l'IAG et la superintelligence ?

Que cherchent exactement à construire les plus grandes entreprises technologiques mondiales dans le secret de leurs laboratoires ?

Le terme « intelligence artificielle générale » existe depuis un certain temps pour désigner une IA polyvalente de « niveau humain ». Il n'a jamais été défini de manière particulièrement précise, mais ces dernières années, il est paradoxalement devenu encore moins bien défini tout en gagnant en importance, les experts débattant simultanément pour savoir si l'IAG est encore à des décennies ou déjà réalisée, tandis que des entreprises valant des billions de dollars font la course « vers l'IAG ». (L'ambiguïté du terme « IAG » a été récemment soulignée quand [des documents divulgués ont révélé](https://gizmodo.com/leaked-documents-show-openai-has-a-very-clear-definition-of-agi-2000543339) que dans le contrat d'OpenAI avec Microsoft, l'IAG était définie comme une IA qui génère 100 milliards de dollars de revenus pour OpenAI – une définition plutôt mercantile qu'intellectuelle.)

Il y a deux problèmes fondamentaux avec l'idée d'une IA ayant une « intelligence de niveau humain ». Premièrement, les humains diffèrent énormément dans leur capacité à effectuer un type donné de travail cognitif, il n'existe donc pas de « niveau humain ». Deuxièmement, l'intelligence est très multidimensionnelle ; bien qu'il puisse y avoir des corrélations, elles sont imparfaites et peuvent être très différentes chez l'IA. Ainsi, même si nous pouvions définir un « niveau humain » pour de nombreuses capacités, l'IA le dépasserait sûrement de loin dans certaines tout en restant bien en deçà dans d'autres.[^26]

Il est néanmoins crucial de pouvoir discuter des types, niveaux et seuils de capacités de l'IA. L'approche adoptée ici consiste à souligner que l'IA polyvalente est déjà là, et qu'elle arrive – et arrivera – à différents niveaux de capacité auxquels il est pratique d'attacher des termes même s'ils sont réducteurs, car ils correspondent à des seuils cruciaux en termes d'effets de l'IA sur la société et l'humanité.

Nous définirons l'IAG « complète » comme synonyme d'« IA polyvalente surhumaine », désignant un système d'IA capable d'effectuer essentiellement toutes les tâches cognitives humaines au niveau des meilleurs experts humains ou au-dessus, ainsi que d'acquérir de nouvelles compétences et de transférer ses capacités vers de nouveaux domaines. Cela correspond à la façon dont l'« IAG » est souvent définie dans la littérature moderne. Il est important de noter que c'est un seuil *très* élevé. Aucun humain ne possède ce type d'intelligence ; il s'agit plutôt du type d'intelligence que de vastes groupes des meilleurs experts humains auraient s'ils étaient combinés. Nous pouvons appeler « superintelligence » une capacité qui va au-delà, et définir des niveaux de capacité plus limités par des IA polyvalentes « compétitives au niveau humain » et « compétitives au niveau expert », qui effectuent un large éventail de tâches au niveau professionnel typique ou au niveau d'expert humain.[^27]

Ces termes et quelques autres sont rassemblés dans [le tableau](https://keepthefuturehuman.ai/essay/docs/#tab:terms) ci-dessous. Pour avoir une idée plus concrète de ce que peuvent faire les différents niveaux de systèmes, il est utile de prendre les définitions au sérieux et de considérer ce qu'elles signifient.

| Type d'IA                      | Termes connexes                          | Définition                                                                                                                                                                                                                         | Exemples                                                                                                                                           |
| ------------------------------ | ---------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |
| IA spécialisée                 | IA faible                                | IA entraînée pour une tâche spécifique ou une famille de tâches. Excelle dans son domaine mais manque d'intelligence générale ou de capacité d'apprentissage par transfert.                                                      | Logiciels de reconnaissance d'images ; Assistants vocaux (Siri, Alexa) ; Programmes de jeu d'échecs ; AlphaFold de DeepMind                      |
| IA-outil                       | Intelligence augmentée, Assistant IA     | (Discuté plus loin dans l'essai.) Système d'IA améliorant les capacités humaines. Combine IA polyvalente compétitive au niveau humain, IA spécialisée et contrôle garanti, privilégiant sécurité et collaboration. Aide à la prise de décision humaine. | Assistants de programmation avancés ; Outils de recherche assistés par IA ; Plateformes sophistiquées d'analyse de données. Agents compétents mais spécialisés et contrôlables |
| IA polyvalente (IAP)           |                                          | Système d'IA adaptable à diverses tâches, y compris celles pour lesquelles il n'a pas été spécifiquement entraîné.                                                                                                                | Modèles de langage (GPT-4, Claude) ; Modèles d'IA multimodaux ; MuZero de DeepMind                                                               |
| IAP compétitive au niveau humain | IAG \[faible\]                         | IA polyvalente effectuant des tâches au niveau humain moyen, les dépassant parfois.                                                                                                                                               | Modèles de langage avancés (O1, Claude 3.5) ; Certains systèmes d'IA multimodaux                                                                |
| IAP compétitive au niveau expert | IAG \[partielle\]                      | IA polyvalente effectuant la plupart des tâches au niveau d'expert humain, avec une autonomie significative mais limitée                                                                                                          | Possiblement un O3 outillé et structuré, au moins pour les mathématiques, la programmation et certaines sciences exactes                        |
| IAG \[complète\]               | IAP surhumaine                           | Système d'IA capable d'effectuer de manière autonome approximativement toutes les tâches intellectuelles humaines au niveau expert ou au-delà, avec apprentissage efficace et transfert de connaissances.                     | \[Aucun exemple actuel – théorique\]                                                                                                              |
| Superintelligence             | IAP hautement surhumaine                | Système d'IA surpassant de loin les capacités humaines dans tous les domaines, dépassant l'expertise humaine collective. Cette supériorité pourrait concerner la généralité, la qualité, la vitesse et/ou d'autres mesures.   | \[Aucun exemple actuel – théorique\]                                                                                                              |

Nous expérimentons déjà ce que c'est que d'avoir des IAP jusqu'au niveau compétitif humain. Cela s'est intégré relativement facilement, car la plupart des utilisateurs vivent cela comme avoir un employé temporaire intelligent mais limité qui les rend plus productifs avec un impact mitigé sur la qualité de leur travail.[^28]

Ce qui serait différent avec une IAP compétitive au niveau expert, c'est qu'elle n'aurait pas les limitations fondamentales de l'IA actuelle, et ferait ce que font les experts : un travail indépendant économiquement rentable, une véritable création de connaissances, un travail technique fiable, tout en faisant rarement (bien qu'encore occasionnellement) des erreurs stupides.

L'idée de l'IAG complète est qu'elle fait *vraiment* toutes les choses cognitives que font même les humains les plus capables et efficaces, de manière autonome et sans aide ou supervision nécessaire. Cela inclut la planification sophistiquée, l'apprentissage de nouvelles compétences, la gestion de projets complexes, etc. Elle pourrait faire de la recherche de pointe originale. Elle pourrait diriger une entreprise. Quel que soit votre travail, s'il se fait principalement par ordinateur ou au téléphone, *elle pourrait le faire au moins aussi bien que vous.* Et probablement beaucoup plus rapidement et à moindre coût. Nous discuterons de certaines ramifications plus bas, mais pour l'instant le défi pour vous est de vraiment prendre cela au sérieux. Imaginez les dix personnes les plus savantes et compétentes que vous connaissez ou dont vous avez entendu parler – incluant PDG, scientifiques, professeurs, ingénieurs de haut niveau, psychologues, dirigeants politiques et écrivains. Rassemblez-les tous en une seule personne, qui parle aussi 100 langues, a une mémoire prodigieuse, fonctionne rapidement, est infatigable et toujours motivée, et travaille en dessous du salaire minimum.[^29] Voilà une idée de ce que serait l'IAG.

Pour la superintelligence, l'imagination est plus difficile, car l'idée est qu'elle pourrait accomplir des prouesses intellectuelles qu'aucun humain ou même groupe d'humains ne peut – elle est par définition imprévisible pour nous. Mais nous pouvons avoir une idée. Comme base minimale, considérez de nombreuses IAG, chacune beaucoup plus capable que même le meilleur expert humain, fonctionnant 100 fois plus vite que les humains, avec une mémoire énorme et une formidable capacité de coordination.[^30] Et cela ne fait que commencer. Traiter avec la superintelligence ressemblerait moins à converser avec un esprit différent qu'à négocier avec une civilisation différente (et plus avancée).

Alors, à quel point sommes-nous *proches* de l'IAG et de la superintelligence ?


[^26]: Par exemple, les systèmes d'IA actuels dépassent largement les capacités humaines dans les tâches de calcul rapide ou de mémorisation, tout en restant en retrait dans le raisonnement abstrait et la résolution créative de problèmes.

[^27]: Très important, en tant que concurrent, une telle IA aurait plusieurs avantages structurels majeurs incluant : elle ne se fatiguerait pas et n'aurait pas d'autres besoins individuels comme les humains ; elle peut fonctionner à des vitesses plus élevées simplement en augmentant la puissance de calcul ; elle peut être copiée avec toute expertise ou connaissance qu'elle acquiert – et les connaissances acquises des réseaux de neurones peuvent même être « fusionnées » pour transférer des ensembles complets de compétences entre eux ; elle pourrait communiquer à la vitesse des machines ; et elle pourrait s'auto-modifier ou s'améliorer de manière plus significative et plus rapide qu'aucun humain.

[^28]: Si vous n'avez pas passé de temps à utiliser les systèmes d'IA actuels de pointe, je le recommande : ils sont véritablement utiles et capables, et c'est aussi important pour calibrer l'effet que l'IA aura en devenant plus puissante.

[^29]: Considérez un grand hôpital de recherche : une IAG pleinement réalisée pourrait simultanément analyser toutes les données des patients entrants, suivre chaque nouvel article médical, suggérer des diagnostics, concevoir des plans de traitement, gérer des essais cliniques, et coordonner la planification du personnel – tout en opérant à un niveau égalant ou dépassant les meilleurs spécialistes de l'hôpital dans chaque domaine. Et elle pourrait faire cela pour plusieurs hôpitaux simultanément, à une fraction du coût actuel. Malheureusement, vous devez aussi considérer un syndicat du crime organisé : une IAG pleinement réalisée pourrait simultanément pirater, usurper l'identité, espionner et faire chanter des milliers de victimes, suivre le rythme des forces de l'ordre (qui s'automatisent beaucoup plus lentement), concevoir de nouveaux stratagèmes pour gagner de l'argent, et coordonner la planification du personnel – s'il y a du personnel.

[^30]: Dans son [essai](https://darioamodei.com/machines-of-loving-grace), Dario Amodei, PDG d'Anthropic, évoquait un « Pays de \[un million de\] génies ».

## Chapitre 5 - Au seuil

Le chemin qui nous mène des systèmes d'IA actuels à une IAG pleinement développée semble d'une brièveté et d'une prévisibilité saisissantes.

Les dix dernières années ont été témoins d'avancées spectaculaires en IA, alimentées par des ressources [computationnelles](https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year), humaines et [financières](https://arxiv.org/abs/2405.21015) considérables. De nombreuses applications d'IA spécialisée surpassent les humains dans leurs tâches assignées, et sont certainement bien plus rapides et économiques.[^31] Il existe aussi des agents spécialisés surhumains qui écrasent tous les humains dans des jeux à domaine restreint comme le [Go](https://www.nature.com/articles/nature16961), les [échecs](https://arxiv.org/abs/1712.01815) et le [poker](https://www.deepstack.ai/), ainsi que des [agents plus généralistes](https://deepmind.google/discover/blog/a-generalist-agent/) capables de planifier et d'exécuter des actions dans des environnements simulés simplifiés avec autant d'efficacité que les humains.

Plus remarquables encore, les systèmes d'IA générale actuels d'OpenAI/Microsoft, Google/Deepmind, Anthropic/Amazon, Facebook/Meta, X.ai/Tesla et autres [^32] ont émergé depuis le début 2023 et ont régulièrement (bien qu'inégalement) accru leurs capacités depuis. Tous ont été créés par prédiction de tokens sur d'immenses ensembles de données textuelles et multimédias, combinée à un retour de renforcement extensif de la part d'humains et d'autres systèmes d'IA. Certains incluent également de vastes systèmes d'outils et d'architectures de support.

### Forces et faiblesses des systèmes généralistes actuels

Ces systèmes performent bien dans une gamme de plus en plus large de tests conçus pour mesurer l'intelligence et l'expertise, avec des progrès qui ont surpris même les experts du domaine :

- Lors de sa première sortie, GPT-4 [égalait ou dépassait les performances humaines typiques](https://arxiv.org/abs/2303.08774) aux tests académiques standards incluant les SAT, GRE, examens d'entrée et examens du barreau. Les modèles plus récents performent probablement significativement mieux, bien que les résultats ne soient pas publiquement disponibles.
- Le test de Turing – longtemps considéré comme un critère clé pour une IA « véritable » – est désormais régulièrement réussi sous certaines formes par les modèles de langage modernes, tant informellement que dans des [études formelles](https://arxiv.org/abs/2405.08007).[^33]
- Sur le benchmark MMLU couvrant 57 matières académiques, [les modèles récents atteignent des scores au niveau d'experts du domaine](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu) (∼90%) [^34]
- L'expertise technique a progressé de manière spectaculaire : Le benchmark GPQA de physique de niveau universitaire a vu [ses performances bondir](https://epoch.ai/data/ai-benchmarking-dashboard) de quasi-aléatoires (GPT-4, 2022) au niveau expert (o1-preview, 2024).
- Même les tests spécifiquement conçus pour résister à l'IA tombent : O3 d'OpenAI [résoudrait](https://www.nextbigfuture.com/2024/12/openai-releases-o3-model-with-high-performance-and-high-cost.html) le benchmark de résolution de problèmes abstraits ARC-AGI au niveau humain, atteint des performances de programmation au niveau des meilleurs experts, et obtient 25% sur les problèmes de « mathématiques de pointe » d'Epoch AI conçus pour défier les mathématiciens d'élite.[^35]
- La tendance est si claire que le développeur de MMLU a maintenant créé [« Le dernier examen de l'humanité »](https://agi.safe.ai/) – un nom inquiétant reflétant la possibilité que l'IA dépasse bientôt les performances humaines sur tout test significatif. Au moment de la rédaction, il y a des affirmations de systèmes d'IA atteignant 27% (selon [Sam Altman](https://x.com/sama/status/1886220281565381078)) et 35% (selon [cet article](https://arxiv.org/abs/2502.09955)) sur cet examen extrêmement difficile. Il est très improbable qu'un humain individuel puisse le faire.

Malgré ces chiffres impressionnants (et leur intelligence évidente quand on interagit avec eux) [^36] il y a beaucoup de choses que (du moins les versions publiées de) ces réseaux de neurones *ne peuvent pas* faire. Actuellement, la plupart sont désincarnés – n'existant que sur des serveurs – et ne traitent au mieux que du texte, du son et des images fixes (mais pas de vidéo). Crucialement, la plupart ne peuvent pas mener d'activités complexes planifiées nécessitant une haute précision.[^37] Et il y a un certain nombre d'autres qualités fortes dans la cognition humaine de haut niveau actuellement faibles dans les systèmes d'IA publiés.

Le tableau suivant en énumère un certain nombre, basé sur les systèmes d'IA de mi-2024 tels que GPT-4o, Claude 3.5 Sonnet, et Google Gemini 1.5.[^38] La question clé pour déterminer à quelle vitesse l'IA générale deviendra plus puissante est : dans quelle mesure faire simplement *plus de la même chose* produira-t-il des résultats, par opposition à ajouter des techniques supplémentaires mais *connues*, par opposition à développer ou implémenter des directions de recherche en IA *vraiment nouvelles*. Mes propres prédictions à ce sujet sont données dans le tableau, en termes de probabilité que chacun de ces scénarios amène cette capacité au niveau humain et au-delà.

<table><tbody><tr><th>Capacité</th><th>Description de la capacité</th><th>Statut/pronostic</th><th>Échelle/connu/nouveau</th></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td colspan="4"><em>Capacités cognitives fondamentales</em></td></tr><tr><td>Raisonnement</td><td>Les gens peuvent faire un raisonnement précis, multi-étapes, suivant des règles et vérifiant la précision.</td><td>Progrès récents spectaculaires utilisant les chaînes de raisonnement étendues et le réentraînement</td><td>95/5/5</td></tr><tr><td>Planification</td><td>Les gens présentent une planification à long terme et hiérarchique.</td><td>S'améliore avec l'échelle ; peut être fortement aidée par l'architecture de support et de meilleures techniques d'entraînement.</td><td>10/85/5</td></tr><tr><td>Ancrage dans la vérité</td><td>Les IA générales confabulent des informations non fondées pour satisfaire les requêtes.</td><td>S'améliore avec l'échelle ; données de calibrage disponibles dans le modèle ; peut être vérifié/amélioré via l'architecture de support.</td><td>30/65/5</td></tr><tr><td>Résolution flexible de problèmes</td><td>Les humains peuvent reconnaître de nouveaux motifs et inventer de nouvelles solutions à des problèmes complexes ; les modèles d'apprentissage automatique actuels peinent.</td><td>S'améliore avec l'échelle mais faiblement ; peut être résolvable avec des techniques neurosymboliques ou de « recherche » généralisée.</td><td>15/75/10</td></tr><tr><td colspan="4"><em>Apprentissage et connaissance</em></td></tr><tr><td>Apprentissage et mémoire</td><td>Les gens ont une mémoire de travail, à court terme et à long terme, toutes dynamiques et interreliées.</td><td>Tous les modèles apprennent pendant l'entraînement ; les IA générales apprennent dans la fenêtre contextuelle et pendant l'ajustement fin ; l'« apprentissage continu » et autres techniques existent mais ne sont pas encore intégrés dans les grandes IA générales.</td><td>5/80/15</td></tr><tr><td>Abstraction et récursion</td><td>Les gens peuvent mapper et transférer des ensembles de relations en d'autres plus abstraits pour le raisonnement et la manipulation, incluant le raisonnement « méta » récursif.</td><td>S'améliore faiblement avec l'échelle ; pourrait émerger dans les systèmes neurosymboliques.</td><td>30/50/20</td></tr><tr><td>Modèle(s) du monde</td><td>Les gens ont et mettent continuellement à jour un modèle prédictif du monde dans lequel ils peuvent résoudre des problèmes et faire du raisonnement physique</td><td>S'améliore avec l'échelle ; la mise à jour liée à l'apprentissage ; les IA générales faibles en prédiction du monde réel.</td><td>20/50/30</td></tr><tr><td colspan="4"><em>Soi et agentivité</em></td></tr><tr><td>Agentivité</td><td>Les gens peuvent prendre des actions pour poursuivre des objectifs, basées sur la planification/prédiction.</td><td>Beaucoup de systèmes d'apprentissage automatique sont agentiques ; les LLM peuvent devenir agents via des enveloppes.</td><td>5/90/5</td></tr><tr><td>Auto-direction</td><td>Les gens développent et poursuivent leurs propres objectifs, avec motivation et élan générés intérieurement.</td><td>Largement composée d'agentivité plus originalité ; susceptible d'émerger dans des systèmes agentiels complexes avec des objectifs abstraits.</td><td>40/45/15</td></tr><tr><td>Auto-référence</td><td>Les gens se comprennent et raisonnent sur eux-mêmes comme situés dans un environnement/contexte.</td><td>S'améliore avec l'échelle et pourrait être augmentée avec une récompense d'entraînement.</td><td>70/15/15</td></tr><tr><td>Conscience de soi</td><td>Les gens ont connaissance et peuvent raisonner sur leurs propres pensées et états mentaux.</td><td>Existe dans un certain sens dans les IA générales, qui peuvent sans doute réussir le « test du miroir » classique de conscience de soi. Peut être améliorée avec l'architecture de support ; mais incertain si c'est suffisant.</td><td>20/55/25</td></tr><tr><td colspan="4"><em>Interface et environnement</em></td></tr><tr><td>Intelligence incarnée</td><td>Les gens comprennent et interagissent activement avec leur environnement du monde réel.</td><td>L'apprentissage par renforcement fonctionne bien dans des environnements simulés et du monde réel (robotique) et peut être intégré dans des transformateurs multimodaux.</td><td>5/85/10</td></tr><tr><td>Traitement multi-sensoriel</td><td>Les gens intègrent et traitent en temps réel des flux visuels, audio et autres flux sensoriels.</td><td>L'entraînement en modalités multiples semble « simplement fonctionner » et s'améliore avec l'échelle. Le traitement vidéo en temps réel est difficile mais par ex. les systèmes de conduite autonome s'améliorent rapidement.</td><td>30/60/10</td></tr><tr><td colspan="4"><em>Capacités d'ordre supérieur</em></td></tr><tr><td>Originalité</td><td>Les modèles d'apprentissage automatique actuels sont créatifs pour transformer et combiner des idées/œuvres existantes, mais les gens peuvent construire de nouveaux cadres et structures, parfois liés à leur identité.</td><td>Peut être difficile à discerner de la « créativité », qui peut évoluer vers cela ; peut émerger de la créativité plus la conscience de soi.</td><td>50/40/10</td></tr><tr><td>Sentience</td><td>Les gens expérimentent des qualia ; ceux-ci peuvent avoir une valence positive, négative ou neutre ; c'est « comme quelque chose » d'être une personne.</td><td>Très difficile et philosophiquement épineux de déterminer si un système donné possède cela.</td><td>5/10/85</td></tr></tbody></table>

Capacités clés actuellement sous le niveau d'expert humain dans les systèmes d'IA générale modernes, groupées par type. La troisième colonne résume le statut actuel. La dernière colonne montre la probabilité prédite (%) que la performance de niveau humain soit atteinte par : mise à l'échelle des techniques actuelles / combinaison avec des techniques connues / développement de nouvelles techniques. Ces capacités ne sont pas indépendantes, et l'augmentation de l'une s'accompagne typiquement d'augmentations des autres. Notez que toutes (particulièrement la sentience) ne sont pas nécessaires pour des systèmes d'IA capables de faire progresser le développement de l'IA, soulignant la possibilité d'une IA puissante mais non sentiente.

Décomposer ce qui « manque » de cette façon rend assez clair que nous sommes tout à fait sur la voie d'une intelligence largement supra-humaine en mettant à l'échelle les techniques existantes ou connues.[^39]

Il pourrait encore y avoir des surprises. Même en mettant de côté la « sentience », il pourrait y avoir certaines des capacités cognitives fondamentales listées qui ne peuvent vraiment pas être accomplies avec les techniques actuelles et nécessitent des nouvelles. Mais considérons ceci. L'effort actuel déployé par bon nombre des plus grandes entreprises mondiales équivaut à plusieurs fois les dépenses du projet Apollo et à des dizaines de fois celles du projet Manhattan,[^40] et emploie des milliers des meilleurs techniciens au monde à des salaires inouïs. La dynamique de ces dernières années a maintenant mobilisé plus de puissance intellectuelle humaine (avec l'IA maintenant ajoutée) que tout autre effort dans l'histoire. Nous ne devrions pas parier sur l'échec.

### La grande cible : les agents autonomes généralistes

Le développement de l'IA générale au cours des dernières années s'est concentré sur la création d'IA générale et puissante mais ressemblant à un outil : elle fonctionne principalement comme un assistant (assez) loyal, et généralement ne prend pas d'actions de son propre chef. C'est en partie par conception, mais largement parce que ces systèmes n'ont simplement pas été assez compétents dans les compétences pertinentes pour être chargés d'actions complexes.[^41]

Les entreprises d'IA et les chercheurs [déplacent cependant de plus en plus l'attention](https://www.axios.com/2025/01/23/davos-2025-ai-agents) vers des agents généralistes *autonomes* de niveau expert.[^42] Cela permettrait aux systèmes d'agir plus comme un assistant humain à qui l'utilisateur peut déléguer de vraies actions.[^43] Que faudra-t-il pour cela ? Un certain nombre des capacités du tableau « ce qui manque » sont impliquées, incluant un fort ancrage dans la vérité, l'apprentissage et la mémoire, l'abstraction et la récursion, et la modélisation du monde (pour l'intelligence), la planification, l'agentivité, l'originalité, l'auto-direction, l'auto-référence, et la conscience de soi (pour l'autonomie), et le traitement multi-sensoriel, l'intelligence incarnée, et la résolution flexible de problèmes (pour la généralité).[^44]

Cette triple intersection de haute autonomie (indépendance d'action), haute généralité (portée et étendue des tâches) et haute intelligence (compétence aux tâches cognitives) est actuellement unique aux humains. C'est implicitement ce que beaucoup ont probablement à l'esprit quand ils pensent à l'IAG – tant en termes de valeur que de risques.

Cela fournit une autre façon de définir I-A-G comme Intelligence ***A***utonome- ***G***énérale, et nous verrons que cette triple intersection fournit une perspective très précieuse pour les systèmes de haute capacité tant pour comprendre leurs risques et récompenses que pour la gouvernance de l'IA.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Simple-1024x1024.png&w=3840&q=75) La zone transformatrice de puissance et de risque I-A-G émerge de l'intersection de trois propriétés clés : haute Autonomie, haute Intelligence aux tâches, et haute Généralité.

### Le cycle d'amélioration (auto-)amélioration de l'IA

Un facteur final crucial pour comprendre le progrès de l'IA est la boucle de rétroaction technologique unique de l'IA. Dans le développement de l'IA, le succès – tant dans les systèmes démontrés que dans les produits déployés – apporte investissement, talent et concurrence supplémentaires, et nous sommes actuellement au milieu d'une énorme boucle de rétroaction battage-plus-réalité de l'IA qui drive des centaines de milliards, voire des billions, de dollars d'investissement.

Ce type de cycle de rétroaction pourrait arriver avec n'importe quelle technologie, et nous l'avons vu dans beaucoup, où le succès commercial engendre l'investissement, qui engendre l'amélioration et un meilleur succès commercial. Mais le développement de l'IA va plus loin, en ce que maintenant les systèmes d'IA aident à développer des systèmes d'IA nouveaux et plus puissants.[^45] Nous pouvons penser à cette boucle de rétroaction en cinq étapes, chacune avec une échelle de temps plus courte que la précédente, comme montré dans le tableau.

*Le cycle d'amélioration de l'IA opère à travers de multiples échelles de temps, avec chaque étape accélérant potentiellement les étapes suivantes. Les étapes antérieures sont bien engagées, tandis que les étapes ultérieures restent spéculatives mais pourraient procéder très rapidement une fois débloquées.*

Plusieurs de ces étapes sont déjà en cours, et quelques-unes commencent clairement. La dernière étape, dans laquelle les systèmes d'IA s'améliorent autonomement eux-mêmes, a été un pilier de la littérature sur le risque de systèmes d'IA très puissants, et pour de bonnes raisons.[^46] Mais il est important de noter que c'est juste la forme la plus drastique d'un cycle de rétroaction qui a déjà commencé et pourrait mener à plus de surprises dans l'avancement rapide de la technologie.


[^31]: Vous utilisez beaucoup plus de cette IA que vous ne le pensez probablement, alimentant la génération et reconnaissance vocale, le traitement d'images, les algorithmes de flux d'actualités, etc.

[^32]: Bien que les relations entre ces paires d'entreprises soient assez complexes et nuancées, je les ai explicitement listées pour indiquer à la fois la vaste capitalisation boursière globale des firmes maintenant engagées dans le développement de l'IA, et aussi que derrière même des entreprises « plus petites » comme Anthropic se trouvent des poches énormément profondes via des investissements et des accords de partenariat majeurs.

[^33]: Il est devenu à la mode de dénigrer le test de Turing, mais il est assez puissant et général. Dans les versions faibles, il indique si des gens typiques interagissant avec une IA (qui est entraînée à agir humainement) de manières typiques pendant de brèves périodes peuvent dire si c'est une IA. Ils ne peuvent pas. Deuxièmement, un test de Turing hautement adversarial peut sonder essentiellement tout élément de capacité et d'intelligence humaines – par ex. en comparant un système d'IA à un expert humain, évalué par d'autres experts humains. Il y a un sens dans lequel une grande partie de l'évaluation de l'IA est une forme généralisée de test de Turing.

[^34]: C'est par domaine – aucun humain ne pourrait plausiblement atteindre de tels scores à travers toutes les matières simultanément.

[^35]: Ce sont des problèmes qui prendraient même à d'excellents mathématiciens un temps substantiel à résoudre, s'ils pouvaient les résoudre du tout.

[^36]: Si vous êtes d'un tempérament sceptique, gardez votre scepticisme mais essayez vraiment les modèles les plus actuels, ainsi que tentez par vous-même quelques-unes des questions de test qu'ils peuvent réussir. En tant que professeur de physique, je prédirais avec quasi-certitude que, par exemple, les meilleurs modèles réussiraient l'examen de qualification diplômant dans notre département.

[^37]: Ceci et d'autres faiblesses comme la confabulation ont ralenti l'adoption commerciale et mené à un écart entre les capacités perçues et revendiquées (qui doivent aussi être vues à travers le prisme de la concurrence de marché intense et le besoin d'attirer l'investissement). Cela a confondu tant le public que les décideurs politiques sur l'état réel du progrès de l'IA. Bien que peut-être ne correspondant pas au battage, le progrès est très réel.

[^38]: L'avancée majeure depuis lors a été le développement de systèmes entraînés pour un raisonnement de qualité supérieure, tirant parti de plus de calcul pendant l'inférence et d'un apprentissage par renforcement plus grand. Parce que ces modèles sont nouveaux et leurs capacités moins testées, je n'ai pas entièrement remanié ce tableau sauf pour le « raisonnement », que je considère comme essentiellement résolu. Mais j'ai mis à jour les prédictions basées sur les capacités expérimentées et rapportées de ces systèmes.

[^39]: Les vagues précédentes d'optimisme de l'IA dans les années 1960 et 1980 se sont terminées en « hivers de l'IA » quand les capacités promises ont échoué à se matérialiser. Cependant, la vague actuelle diffère fondamentalement en ayant atteint des performances surhumaines dans de nombreux domaines, soutenues par des ressources computationnelles massives et un succès commercial.

[^40]: Le projet Apollo complet [a coûté environ 250 milliards USD en dollars 2020](https://www.planetary.org/space-policy/cost-of-apollo), et le projet Manhattan [moins d'un dixième de cela](https://www.brookings.edu/the-costs-of-the-manhattan-project/). Goldman Sachs [projette mille milliards de dollars de dépenses juste sur les centres de données IA](https://www.datacenterdynamics.com/en/news/goldman-sachs-1tn-to-be-spent-on-ai-data-centers-chips-and-utility-upgrades-with-little-to-show-for-it-so-far/) dans les prochaines années.

[^41]: Bien que les humains fassent beaucoup d'erreurs, nous sous-estimons à quel point nous pouvons être fiables ! Parce que les probabilités se multiplient, une tâche nécessitant 20 étapes pour être faite correctement exige que chaque étape soit fiable à 97% juste pour la faire bien la moitié du temps. Nous faisons de telles tâches tout le temps.

[^42]: Un mouvement fort dans cette direction a très récemment été pris avec l'assistant [« Recherche Profonde »](https://openai.com/index/introducing-deep-research/) d'OpenAI qui effectue autonomement une recherche générale, décrit comme « une nouvelle capacité agentique qui conduit une recherche multi-étapes sur internet pour des tâches complexes ».

[^43]: Des choses comme remplir ce PDF embêtant, réserver des vols, etc. Mais avec un doctorat dans 20 domaines ! Donc aussi : écrire cette thèse pour vous, négocier ce contrat pour vous, prouver ce théorème pour vous, créer cette campagne publicitaire pour vous, etc. Que faites-*vous* ? Vous lui dites quoi faire, bien sûr.

[^44]: Notez que la sentience n'est *pas* clairement requise, ni l'IA dans cette triple intersection n'implique nécessairement cela.

[^45]: L'analogie la plus proche ici est peut-être la technologie des puces, où le développement a maintenu la loi de Moore pendant des décennies, alors que les technologies informatiques aident les gens à concevoir la prochaine génération de technologie de puce. Mais l'IA sera bien plus directe.

[^46]: Il est important de laisser cela s'imprégner un moment que l'IA pourrait – bientôt – s'améliorer elle-même sur une échelle de temps de jours ou semaines. Ou moins. Gardez cela à l'esprit quand quelqu'un vous dit qu'une capacité de l'IA est définitivement loin.

## Chapitre 6 - La course à l'IAG

Quelles sont les forces motrices derrière la course à la construction de l'IAG, tant pour les entreprises que pour les pays ?

Les progrès rapides récents de l'IA ont résulté d'un niveau extraordinaire d'attention et d'investissement, et ont également contribué à les alimenter. Cela s'explique en partie par les succès du développement de l'IA, mais il y a d'autres enjeux. Pourquoi certaines des plus grandes entreprises de la planète, et même des pays entiers, se livrent-ils une course pour construire non seulement de l'IA, mais spécifiquement l'IAG et la superintelligence ?

### Ce qui a orienté la recherche en IA vers l'intelligence de niveau humain

Jusqu'à ces cinq dernières années environ, l'IA était largement un problème de recherche académique et scientifique, donc principalement motivée par la curiosité et la volonté de comprendre l'intelligence et comment la créer dans un nouveau substrat.

Durant cette phase, la plupart des chercheurs accordaient relativement peu d'attention aux avantages ou aux périls de l'IA. Lorsqu'on leur demandait pourquoi l'IA devrait être développée, une réponse courante consistait à énumérer, de manière quelque peu vague, les problèmes que l'IA pourrait aider à résoudre : nouveaux médicaments, nouveaux matériaux, nouvelles sciences, processus plus intelligents, et en général améliorer les choses pour les gens.[^47]

Ce sont des objectifs admirables ![^48] Bien que nous puissions et allons nous demander si l'IAG – plutôt que l'IA en général – est nécessaire pour ces objectifs, ils témoignent de l'idéalisme avec lequel de nombreux chercheurs en IA ont commencé.

Au cours des cinq dernières années, cependant, l'IA s'est transformée d'un domaine de recherche relativement pure en un domaine beaucoup plus orienté vers l'ingénierie et les produits, largement porté par certaines des plus grandes entreprises mondiales.[^49] Les chercheurs, bien que toujours pertinents, ne dirigent plus le processus.

### Pourquoi les entreprises essaient-elles de construire l'IAG ?

Alors pourquoi les corporations géantes (et plus encore les investisseurs) investissent-elles des ressources considérables dans la construction de l'IAG ? Il y a deux moteurs dont la plupart des entreprises parlent assez ouvertement : elles voient l'IA comme un moteur de productivité pour la société, et de profits pour elles. Parce que l'IA générale est par nature polyvalente, il y a un enjeu énorme : plutôt que de choisir un secteur dans lequel créer des produits et services, on peut essayer de *tous les faire à la fois.* Les grandes entreprises technologiques ont atteint une taille énorme en produisant des biens et services numériques, et au moins certains dirigeants voient sûrement l'IA comme simplement la prochaine étape pour bien les fournir, avec des risques et avantages qui étendent mais font écho à ceux fournis par la recherche, les réseaux sociaux, les ordinateurs portables, les téléphones, etc.

Mais pourquoi l'IAG ? Il y a une réponse très simple à cela, dont la plupart des entreprises et investisseurs évitent de discuter publiquement.[^50]

C'est que l'IAG peut directement, un pour un, *remplacer les travailleurs.*

Pas augmenter, pas autonomiser, pas rendre plus productif. Pas même *déplacer.* Tout cela peut être et sera fait par de l'IA non générale. L'IAG est spécifiquement ce qui peut entièrement *remplacer* les travailleurs intellectuels (et avec la robotique, beaucoup de travailleurs physiques également). Pour soutenir cette vision, il suffit de regarder la [définition (publiquement déclarée)](https://openai.com/our-structure/) de l'IAG par OpenAI, qui est « un système hautement autonome qui surpasse les humains dans la plupart des tâches économiquement valorisables. »

L'enjeu ici (pour les entreprises !) est énorme. Les coûts de main-d'œuvre représentent un pourcentage substantiel des ~100 000 milliards de dollars de l'économie mondiale. Même si seulement une fraction de cela est capturée par le remplacement du travail humain par le travail de l'IA, cela représente des milliers de milliards de dollars de revenus annuels. Les entreprises d'IA sont également conscientes de qui est prêt à payer. Selon elles, vous n'allez pas payer des milliers de dollars par an pour des outils de productivité. Mais une entreprise *paiera* des milliers de dollars par an pour remplacer votre travail, si elle le peut.

### Pourquoi les pays se sentent obligés de faire la course à l'IAG

Les motivations officielles des pays pour poursuivre l'IAG se concentrent sur le leadership économique et scientifique. L'argument est convaincant : l'IAG pourrait considérablement accélérer la recherche scientifique, le développement technologique et la croissance économique. Compte tenu des enjeux, argumentent-ils, aucune grande puissance ne peut se permettre de prendre du retard.[^51]

Mais il y a aussi des moteurs supplémentaires et largement non déclarés. Il ne fait aucun doute que lorsque certains dirigeants militaires et de sécurité nationale se réunissent à huis clos pour discuter d'une technologie extraordinairement puissante et catastrophiquement risquée, leur attention ne se porte pas sur « comment éviter ces risques » mais plutôt sur « comment l'obtenir en premier ? » Les dirigeants militaires et du renseignement voient l'IAG comme une révolution potentielle dans les affaires militaires, peut-être la plus significative depuis les armes nucléaires. La crainte est que le premier pays à développer l'IAG pourrait obtenir un avantage stratégique insurmontable. Cela crée une dynamique classique de course aux armements.

Nous verrons que cette pensée de « course à l'IAG »,[^52] bien que convaincante, est profondément défaillante. Ce n'est pas parce que faire la course est dangereux et risqué – bien que ce soit le cas – mais à cause de la nature de la technologie. L'hypothèse implicite est que l'IAG, comme les autres technologies, est contrôlable par l'État qui la développe, et constitue un avantage générateur de pouvoir pour la société qui en a le plus. Comme nous le verrons, elle ne sera probablement ni l'un ni l'autre.

### Pourquoi la superintelligence ?

Tandis que les entreprises se concentrent publiquement sur la productivité, et les pays sur la croissance économique et technologique, pour ceux qui poursuivent délibérément l'IAG complète et la superintelligence, ce ne sont que le début. Qu'ont-ils vraiment en tête ? Bien que rarement dit à voix haute, cela inclut :

1. Des remèdes à de nombreuses ou toutes les maladies ;
2. L'arrêt et l'inversion du vieillissement ;
3. De nouvelles sources d'énergie durable comme la fusion ;
4. Des améliorations humaines, ou des organismes conçus via l'ingénierie génétique ;
5. La nanotechnologie et la fabrication moléculaire ;
6. Le téléchargement d'esprits ;
7. La physique exotique ou les technologies spatiales ;
8. Des conseils et un soutien à la décision surhumains ;
9. Une planification et une coordination surhumaines.

Les trois premiers sont largement des technologies « à effet unique » – c'est-à-dire susceptibles d'être assez fortement positives nettes. Il est difficile d'argumenter contre guérir les maladies ou pouvoir vivre plus longtemps si on le choisit. Et nous avons déjà récolté le côté négatif de la fusion (sous forme d'armes nucléaires) ; il serait formidable maintenant d'obtenir le côté positif. La question avec cette première catégorie est de savoir si obtenir ces technologies plus tôt compense le risque.

Les quatre suivantes sont clairement à double tranchant : des technologies transformatrices avec à la fois des avantages potentiellement énormes et des risques immenses, un peu comme l'IA. Toutes ces technologies, si elles sortaient d'une boîte noire demain et étaient déployées, seraient incroyablement difficiles à gérer.[^53]

Les deux dernières concernent l'IA surhumaine faisant des choses elle-même plutôt que d'inventer simplement de la technologie. Plus précisément, en laissant les euphémismes de côté, celles-ci impliquent des systèmes d'IA puissants disant aux gens quoi faire. Appeler cela des « conseils » est malhonnête si le système qui conseille est bien plus puissant que celui qui est conseillé, qui ne peut pas comprendre de manière significative la base de la décision (ou même si cela est fourni, faire confiance que le conseiller ne fournirait pas une justification également convaincante pour une décision différente).

Cela indique un élément clé manquant dans la liste ci-dessus :

10. Le pouvoir.

Il est parfaitement clair qu'une grande partie de ce qui sous-tend la course actuelle vers l'IA surhumaine est l'idée que *intelligence = pouvoir*. Chaque concurrent mise sur le fait d'être le meilleur détenteur de ce pouvoir, et qu'il sera capable de l'exercer pour des raisons ostensiblement bienveillantes sans qu'il leur échappe ou leur soit retiré de leur contrôle.

C'est-à-dire que ce que les entreprises et nations poursuivent vraiment, ce ne sont pas seulement les fruits de l'IAG et de la superintelligence, mais le pouvoir de contrôler qui y a accès et comment ils sont utilisés. Les entreprises se voient comme des gardiens responsables de ce pouvoir au service des actionnaires et de l'humanité ; les nations se voient comme des gardiens nécessaires empêchant les puissances hostiles d'obtenir un avantage décisif. Les deux ont dangereusement tort, échouant à reconnaître que la superintelligence, par sa nature, ne peut pas être contrôlée de manière fiable par aucune institution humaine. Nous verrons que la nature et les dynamiques des systèmes superintelligents rendent le contrôle humain extrêmement difficile, sinon impossible.

Ces dynamiques de course – à la fois corporatives et géopolitiques – rendent certains risques quasi inévitables sauf s'ils sont interrompus de manière décisive. Nous nous tournons maintenant vers l'examen de ces risques et pourquoi ils ne peuvent pas être adéquatement atténués dans un paradigme de développement compétitif.[^54]


[^47]: Une liste plus précise d'objectifs dignes est celle des [Objectifs de développement durable](https://sdgs.un.org/goals) de l'ONU. Ce sont, en un sens, ce qui se rapproche le plus d'un ensemble d'objectifs de consensus mondial pour ce que nous aimerions voir s'améliorer dans le monde. L'IA pourrait aider.

[^48]: La technologie en général a un pouvoir transformateur économique et social pour le bien-être humain, comme en attestent des milliers d'années. Dans cette veine, une explication longue et convaincante d'une vision positive de l'IAG peut être trouvée dans [cet essai](https://darioamodei.com/machines-of-loving-grace) du fondateur d'Anthropic, Dario Amodei.

[^49]: L'investissement privé en IA [a commencé à exploser en 2018-19, dépassant l'investissement public vers cette période,](https://cset.georgetown.edu/publication/tracking-ai-investment/) et l'a largement distancé depuis.

[^50]: Je peux attester que derrière des portes plus fermées, ils n'ont pas de tels scrupules. Et cela devient plus public ; voir par exemple la nouvelle [« demande de startups »](https://www.ycombinator.com/rfs) de Y-combinator, dont de nombreuses parties appellent explicitement au remplacement complet des travailleurs humains. Pour les citer, « La proposition de valeur du SaaS B2B était de rendre les travailleurs humains progressivement plus efficaces. La proposition de valeur des agents d'IA verticaux est d'automatiser entièrement le travail... Il est tout à fait possible que cette opportunité soit assez grande pour créer une centaine d'autres licornes. » (Pour ceux qui ne connaissent pas le jargon de la Silicon Valley, « B2B » signifie business-to-business et une licorne est une entreprise d'un milliard de dollars. C'est-à-dire qu'ils parlent de plus d'une centaine d'entreprises d'un milliard de dollars et plus qui remplacent les travailleurs pour d'autres entreprises.)

[^51]: Voir par exemple un récent [rapport de la Commission de révision économique et sécuritaire États-Unis-Chine](https://www.uscc.gov/sites/default/files/2024-11/2024_Executive_Summary.pdf). Bien qu'il y ait eu étonnamment peu de justification dans le rapport lui-même, la recommandation principale était que le Congrès américain « établisse et finance un programme de type Projet Manhattan dédié à faire la course pour acquérir une capacité d'Intelligence artificielle générale (IAG). »

[^52]: Les entreprises adoptent maintenant ce cadrage géopolitique comme un bouclier contre toute contrainte sur leur développement d'IA, généralement de manières qui sont manifestement intéressées, et parfois de manières qui n'ont même pas de sens basique. Considérez l'[Approche de l'IA de frontière](https://about.fb.com/news/2025/02/meta-approach-frontier-ai/) de Meta, qui argue simultanément que l'Amérique doit « [Consolider sa] position de leader dans l'innovation technologique, la croissance économique et la sécurité nationale » et aussi qu'elle doit le faire en publiant ouvertement ses systèmes d'IA les plus puissants – ce qui inclut les donner directement à ses rivaux et adversaires géopolitiques.

[^53]: Ainsi nous devrions probablement laisser la gestion de ces technologies aux IA. Mais ce serait une délégation de contrôle très problématique, à laquelle nous reviendrons ci-dessous.

[^54]: La concurrence dans le développement technologique apporte souvent des avantages importants : prévenir le contrôle monopolistique, stimuler l'innovation et la réduction des coûts, permettre des approches diverses, et créer une surveillance mutuelle. Cependant, avec l'IAG, ces avantages doivent être pesés contre les risques uniques des dynamiques de course et de la pression à réduire les précautions de sécurité.

## Chapitre 7 - Que se passe-t-il si nous développons l'IAG sur notre trajectoire actuelle ?

La société n'est pas prête pour des systèmes de niveau IAG. Si nous les construisons très bientôt, les choses pourraient mal tourner.

Le développement d'une intelligence artificielle générale complète – ce que nous appellerons ici l'IA qui se trouve « à l'extérieur des Portes » – constituerait un changement fondamental dans la nature du monde : par sa nature même, cela signifie ajouter à la Terre une nouvelle espèce d'intelligence dotée de capacités supérieures à celles des humains.

Ce qui se passera ensuite dépend de nombreux facteurs, notamment la nature de la technologie, les choix de ceux qui la développent, et le contexte mondial dans lequel elle est développée.

Actuellement, l'IAG complète est développée par une poignée d'entreprises privées massives en compétition les unes avec les autres, avec peu de réglementation significative ou de supervision externe,[^55] dans une société aux institutions centrales de plus en plus affaiblies et même dysfonctionnelles,[^56] à une époque de tensions géopolitiques élevées et de faible coordination internationale. Bien que certains soient motivés par l'altruisme, beaucoup de ceux qui s'y adonnent sont poussés par l'argent, le pouvoir, ou les deux.

La prédiction est très difficile, mais il existe certaines dynamiques suffisamment bien comprises, et des analogies assez pertinentes avec des technologies antérieures pour offrir un guide. Et malheureusement, malgré les promesses de l'IA, elles donnent de bonnes raisons d'être profondément pessimiste quant à la façon dont notre trajectoire actuelle va se dérouler.

Pour le dire sans détour, sur notre parcours actuel, le développement de l'IAG aura quelques effets positifs (et rendra certaines personnes très, très riches). Mais la nature de la technologie, les dynamiques fondamentales, et le contexte dans lequel elle est développée, indiquent fortement que : l'IA puissante sapera dramatiquement notre société et notre civilisation ; nous en perdrons le contrôle ; nous pourrions bien finir dans une guerre mondiale à cause d'elle ; nous perdrons (ou céderons) le contrôle *à* celle-ci ; elle mènera à une superintelligence artificielle, que nous ne contrôlerons absolument pas et qui signifiera la fin d'un monde dirigé par les humains.

Ce sont des affirmations fortes, et j'aimerais qu'elles ne soient que spéculation vaine ou « catastrophisme » injustifié. Mais c'est là que pointent la science, la théorie des jeux, la théorie de l'évolution, et l'histoire. Cette section développe ces affirmations, et leurs fondements, en détail.

### Nous saperons notre société et notre civilisation

Malgré ce que vous pourriez entendre dans les conseils d'administration de la Silicon Valley, la plupart des ruptures – surtout de la variété très rapide – ne sont pas bénéfiques. Il y a largement plus de façons d'empirer les systèmes complexes que de les améliorer. Notre monde fonctionne aussi bien qu'il le fait parce que nous avons patiemment construit des processus, des technologies, et des institutions qui l'ont rendu progressivement meilleur.[^57] Prendre une masse sur une usine améliore rarement les opérations.

Voici un catalogue (incomplet) des façons dont les systèmes d'IAG perturberaient notre civilisation.

- Ils perturberaient dramatiquement le travail, menant *au minimum* à une inégalité de revenus dramatiquement plus élevée et potentiellement au sous-emploi ou chômage à grande échelle, sur une échelle temporelle bien trop courte pour que la société s'ajuste.[^58]
- Ils mèneraient probablement à la concentration d'un pouvoir économique, social et politique énorme – potentiellement plus grand que celui des États-nations – dans un petit nombre d'intérêts privés massifs non responsables devant le public.
- Ils pourraient soudainement rendre triviales des activités auparavant difficiles ou coûteuses, déstabilisant les systèmes sociaux qui dépendent du fait que certaines activités restent coûteuses ou nécessitent un effort humain significatif.[^59]
- Ils pourraient inonder les systèmes de collecte, de traitement et de communication d'informations de la société avec des médias complètement réalistes mais faux, spammeurs, hyper-ciblés, ou manipulateurs si massivement qu'il devient impossible de distinguer ce qui est physiquement réel ou non, humain ou non, factuel ou non, et digne de confiance ou non.[^60]
- Ils pourraient créer une dépendance intellectuelle dangereuse et quasi totale, où la compréhension humaine des systèmes et technologies clés s'atrophie alors que nous comptons de plus en plus sur des systèmes d'IA que nous ne pouvons pas pleinement comprendre.
- Ils pourraient effectivement mettre fin à la culture humaine, une fois que presque tous les objets culturels (texte, musique, art visuel, cinéma, etc.) consommés par la plupart des gens sont créés, médiatisés, ou organisés par des esprits non-humains.
- Ils pourraient permettre des systèmes de surveillance et de manipulation de masse efficaces utilisables par les gouvernements ou les intérêts privés pour contrôler une population et poursuivre des objectifs en conflit avec l'intérêt public.
- En sapant le discours humain, le débat, et les systèmes électoraux, ils pourraient réduire la crédibilité des institutions démocratiques au point où elles sont effectivement (ou explicitement) remplacées par d'autres, mettant fin à la démocratie dans les États où elle existe actuellement.
- Ils pourraient devenir, ou créer, des virus et vers logiciels intelligents auto-réplicateurs avancés qui pourraient proliférer et évoluer, perturbant massivement les systèmes d'information mondiaux.
- Ils peuvent dramatiquement augmenter la capacité des terroristes, mauvais acteurs, et États voyous à causer des dommages via des armes biologiques, chimiques, cyber, autonomes, ou autres, sans que l'IA ne fournisse une capacité d'équilibre pour prévenir de tels dommages. De même, ils mineraient la sécurité nationale et les équilibres géopolitiques en rendant l'expertise de premier plan nucléaire, biologique, d'ingénierie, et autre disponible aux régimes qui ne l'auraient pas autrement.
- Ils pourraient causer un hyper-capitalisme emballé rapide à grande échelle, avec des entreprises effectivement dirigées par l'IA en compétition dans des espaces financiers, de vente, et de services largement électroniques. Les marchés financiers pilotés par l'IA pourraient opérer à des vitesses et complexités bien au-delà de la compréhension ou du contrôle humains. Tous les modes de défaillance et externalités négatives des économies capitalistes actuelles pourraient être exacerbés et accélérés bien au-delà du contrôle, de la gouvernance, ou de la capacité réglementaire humains.
- Ils pourraient alimenter une course aux armements entre nations dans l'armement alimenté par l'IA, les systèmes de commandement et contrôle, les cyberarmes, etc., créant une accumulation très rapide de capacités extrêmement destructrices.

Ces risques ne sont pas spéculatifs. Beaucoup d'entre eux se réalisent en ce moment même, via les systèmes d'IA existants ! Mais considérez, *vraiment* considérez, à quoi chacun ressemblerait avec une IA dramatiquement plus puissante.

Considérez le déplacement de main-d'œuvre quand la plupart des travailleurs ne peuvent simplement pas fournir de valeur économique significative au-delà de ce que l'IA peut faire, dans leur domaine d'expertise ou d'expérience – ou même s'ils se reconvertissent ! Considérez la surveillance de masse si tout le monde est individuellement surveillé et contrôlé par quelque chose de plus rapide et plus intelligent qu'eux. À quoi ressemble la démocratie quand nous ne pouvons pas faire confiance de façon fiable à toute information numérique que nous voyons, entendons, ou lisons, et quand les voix publiques les plus convaincantes ne sont même pas humaines, et n'ont aucun intérêt dans le résultat ? Que devient la guerre quand les généraux doivent constamment s'en remettre à l'IA (ou simplement la mettre aux commandes), de peur d'accorder un avantage décisif à l'ennemi ? N'importe lequel des risques ci-dessus représente une catastrophe pour la civilisation humaine[^61] s'il est pleinement réalisé.

Vous pouvez faire vos propres prédictions. Posez-vous ces trois questions pour chaque risque :

1. Est-ce qu'une IA super-capable, hautement autonome, et très générale le permettrait d'une façon ou à une échelle qui ne serait pas possible autrement ?
2. Y a-t-il des parties qui bénéficieraient de choses qui causent sa réalisation ?
3. Y a-t-il des systèmes et institutions en place qui l'empêcheraient efficacement de se produire ?

Là où vos réponses sont « oui, oui, non » vous pouvez voir que nous avons un gros problème.

Quel est notre plan pour les gérer ? En l'état, il y en a deux sur la table concernant l'IA en général.

Le premier est de construire des garde-fous dans les systèmes pour les empêcher de faire des choses qu'ils ne devraient pas faire. C'est ce qui se fait maintenant : les systèmes d'IA commerciaux vont, par exemple, refuser d'aider à construire une bombe ou écrire un discours de haine.

Ce plan est lamentablement inadéquat pour les systèmes à l'extérieur de la Porte.[^62] Il peut aider à diminuer le risque que l'IA fournisse une assistance manifestement dangereuse aux mauvais acteurs. Mais il ne fera rien pour empêcher la perturbation du travail, la concentration du pouvoir, l'hyper-capitalisme emballé, ou le remplacement de la culture humaine : ce ne sont que des résultats de l'utilisation des systèmes de façons permises qui profitent à leurs fournisseurs ! Et les gouvernements obtiendront sûrement l'accès aux systèmes pour un usage militaire ou de surveillance.

Le second plan est encore pire : simplement libérer ouvertement des systèmes d'IA très puissants pour que quiconque les utilise comme il le souhaite,[^63] et espérer le mieux.

Implicite dans les deux plans est que quelqu'un d'autre, par exemple les gouvernements, aidera à résoudre les problèmes par le droit souple ou dur, les standards, réglementations, normes, et autres mécanismes que nous utilisons généralement pour gérer les technologies.[^64] Mais mis à part que les entreprises d'IA se battent déjà bec et ongles contre toute réglementation substantielle ou limitations imposées de l'extérieur, pour un certain nombre de ces risques il est assez difficile de voir quelle réglementation aiderait même vraiment. La réglementation pourrait imposer des standards de sécurité sur l'IA. Mais empêcherait-elle les entreprises de remplacer les travailleurs en gros par l'IA ? Interdirait-elle aux gens de laisser l'IA diriger leurs entreprises pour eux ? Empêcherait-elle les gouvernements d'utiliser l'IA puissante dans la surveillance et l'armement ? Ces enjeux sont fondamentaux. L'humanité pourrait potentiellement trouver des façons de s'y adapter, mais seulement avec *beaucoup* plus de temps. En l'état, étant donné la vitesse à laquelle l'IA atteint ou dépasse les capacités des gens qui essaient de la gérer, ces problèmes semblent de plus en plus insolubles.

### Nous perdrons le contrôle des systèmes d'IAG (au moins certains)

La plupart des technologies sont très contrôlables, par construction. Si votre voiture ou votre grille-pain commence à faire quelque chose que vous ne voulez pas qu'il fasse, c'est juste un dysfonctionnement, pas une partie de sa nature de grille-pain. L'IA est différente : elle est *cultivée* plutôt que conçue, son fonctionnement central est opaque, et elle est intrinsèquement imprévisible.

Cette perte de contrôle n'est pas théorique – nous en voyons déjà les premières versions. Considérez d'abord un exemple prosaïque, et sans doute bénin. Si vous demandez à ChatGPT de vous aider à mélanger un poison, ou écrire une diatribe raciste, il refusera. C'est sans doute bien. Mais c'est aussi ChatGPT *ne faisant pas ce que vous lui avez explicitement demandé de faire*. D'autres logiciels ne font pas ça. Ce même modèle ne concevra pas de poisons à la demande d'un employé d'OpenAI non plus.[^65] Cela rend très facile d'imaginer à quoi ressemblerait une future IA plus puissante hors de contrôle. Dans beaucoup de cas, elles ne feront simplement pas ce que nous demandons ! Soit un système d'IAG super-humain donné sera absolument obéissant et loyal à un système de commandement humain, soit il ne le sera pas. Si non, *il fera des choses qu'il peut croire bonnes pour nous, mais qui sont contraires à nos commandes explicites.* Ce n'est pas quelque chose qui est sous contrôle. Mais, pourriez-vous dire, c'est intentionnel – ces refus sont par conception, partie de ce qu'on appelle « aligner » les systèmes sur les valeurs humaines. Et c'est vrai. Cependant le « programme » d'alignement lui-même a deux problèmes majeurs.[^66]

Premièrement, à un niveau profond nous n'avons aucune idée de comment le faire. Comment garantir qu'un système d'IA « se soucie » de ce que nous voulons ? Nous pouvons entraîner les systèmes d'IA à dire et ne pas dire des choses en fournissant des retours ; et ils peuvent apprendre et raisonner sur ce que les humains veulent et à quoi ils tiennent tout comme ils raisonnent sur d'autres choses. Mais nous n'avons aucune méthode – même théoriquement – pour les amener à valoriser profondément et de façon fiable ce qui importe aux gens. Il y a des psychopathes humains fonctionnels qui savent ce qui est considéré comme bien et mal, et comment ils sont supposés se comporter. Ils ne s'en *soucient* simplement pas. Mais ils peuvent *agir* comme s'ils le faisaient, si cela sert leur objectif. Tout comme nous ne savons pas comment changer un psychopathe (ou quiconque d'autre) en quelqu'un de véritablement, complètement loyal ou aligné avec quelqu'un ou quelque chose d'autre, nous n'avons *aucune idée*[^67] de comment résoudre le problème d'alignement dans des systèmes assez avancés pour se modéliser comme agents dans le monde et potentiellement [manipuler leur propre entraînement](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) et [tromper les gens.](https://arxiv.org/abs/2311.08379) S'il s'avère impossible ou irréalisable *soit* de rendre l'IAG complètement obéissante ou de lui faire se soucier profondément des humains, alors dès qu'elle en sera capable (et croira pouvoir s'en tirer), elle commencera à faire des choses que nous ne voulons pas.[^68]

Deuxièmement, il y a des raisons théoriques profondes de croire que *par nature* les systèmes d'IA avancés auront des objectifs et donc des comportements qui sont contraires aux intérêts humains. Pourquoi ? Eh bien elle pourrait, bien sûr, recevoir *ces* objectifs. Un système créé par l'armée serait probablement délibérément mauvais pour au moins certaines parties. Plus généralement, cependant, un système d'IA pourrait recevoir un objectif relativement neutre (« gagner beaucoup d'argent ») ou même ostensiblement positif (« réduire la pollution ») qui mène presque inévitablement à des objectifs « instrumentaux » qui sont plutôt moins bénins.

Nous voyons cela tout le temps dans les systèmes humains. Tout comme les entreprises poursuivant le profit développent des objectifs instrumentaux comme acquérir du pouvoir politique (pour désarmer les réglementations), devenir secrètes (pour désarmer la compétition ou le contrôle externe), ou saper la compréhension scientifique (si cette compréhension montre que leurs actions sont nuisibles), les systèmes d'IA puissants développeront des capacités similaires – mais avec une vitesse et une efficacité bien supérieures. Tout agent hautement compétent voudra faire des choses comme acquérir du pouvoir et des ressources, augmenter ses propres capacités, s'empêcher d'être tué, éteint, ou désarmé, contrôler les récits sociaux et cadres autour de ses actions, persuader les autres de ses vues, et ainsi de suite.[^69]

Et pourtant ce n'est pas seulement une prédiction théorique presque inévitable, c'est déjà observablement en train de se produire dans les systèmes d'IA d'aujourd'hui, et augmente avec leur capacité. Quand ils sont évalués, même ces systèmes d'IA relativement « passifs » vont, dans des circonstances appropriées, délibérément [tromper les évaluateurs sur leurs objectifs et capacités, viser à désactiver les mécanismes de supervision,](https://arxiv.org/abs/2412.04984) et éviter d'être éteints ou réentraînés en [simulant l'alignement](https://arxiv.org/abs/2412.14093) ou se copiant vers d'autres emplacements. Bien que totalement peu surprenants pour les chercheurs en sécurité de l'IA, ces comportements sont très sobres à observer. Et ils présagent très mal pour des systèmes d'IA bien plus puissants et autonomes qui arrivent.

En effet en général, notre incapacité à assurer que l'IA « se soucie » de ce dont nous nous soucions, ou se comporte de façon contrôlable ou prévisible, ou évite de développer des pulsions vers l'auto-préservation, l'acquisition de pouvoir, etc., ne promet que de devenir plus prononcée alors que l'IA devient plus puissante. Créer un nouvel avion implique une plus grande compréhension de l'avionique, l'hydrodynamique, et les systèmes de contrôle. Créer un ordinateur plus puissant implique une plus grande compréhension et maîtrise du fonctionnement et conception d'ordinateur, de puce, et de logiciel. *Pas* ainsi avec un système d'IA.[^70]

Pour résumer : il est concevable que l'IAG puisse être rendue complètement obéissante ; mais nous ne savons pas comment le faire. Si non, elle sera plus souveraine, comme les gens, faisant diverses choses pour diverses raisons. Nous ne savons pas non plus comment instiller de façon fiable un « alignement » profond dans l'IA qui rendrait ces choses tendant à être bonnes pour l'humanité, et en l'absence d'un niveau profond d'alignement, la nature de l'agentivité et de l'intelligence elle-même indique que – tout comme les gens et les entreprises – elles seront poussées à faire beaucoup de choses profondément antisociales.

Où cela nous place-t-il ? Un monde plein d'IA souveraine puissante incontrôlée *pourrait* finir par être un bon monde pour les humains.[^71] Mais alors qu'elles deviennent toujours plus puissantes, comme nous le verrons ci-dessous, ce ne serait pas *notre* monde.

C'est pour l'IAG incontrôlable. Mais même si l'IAG pouvait, d'une façon ou d'une autre, être rendue parfaitement contrôlée et loyale, nous aurions encore d'énormes problèmes. Nous en avons déjà vu un : l'IA puissante peut être utilisée et mal utilisée pour perturber profondément le fonctionnement de notre société. Voyons-en un autre : dans la mesure où l'IAG serait contrôlable et révolutionnairement puissante (ou même *crue* l'être) elle menacerait tellement les structures de pouvoir dans le monde qu'elle présenterait un risque profond.

### Nous augmentons radicalement la probabilité d'une guerre à grande échelle

Imaginez une situation dans un futur proche, où il deviendrait clair qu'un effort corporatif, peut-être en collaboration avec un gouvernement national, était au seuil d'une IA s'améliorant rapidement elle-même. Cela se passe dans le contexte actuel d'une course entre entreprises, et d'une compétition géopolitique dans laquelle des recommandations sont faites au gouvernement américain de poursuivre explicitement un « projet Manhattan de l'IAG » et les États-Unis contrôlent l'exportation de puces d'IA haute performance vers les pays non alliés.

La théorie des jeux ici est brutale : une fois qu'une telle course commence (comme elle l'a fait, entre entreprises et quelque peu entre pays), il n'y a que quatre résultats possibles :

1. La course est arrêtée (par accord, ou force externe).
2. Une partie « gagne » en développant une IAG forte puis en arrêtant les autres (utilisant l'IA ou autrement).
3. La course est arrêtée par destruction mutuelle de la capacité des coureurs à courir.
4. Plusieurs participants continuent à courir, et développent la superintelligence, à peu près aussi rapidement les uns que les autres.

Examinons chaque possibilité. Une fois commencé, arrêter pacifiquement une course entre entreprises nécessiterait une intervention du gouvernement national (pour les entreprises) ou une coordination internationale sans précédent (pour les pays). Mais quand toute fermeture ou prudence significative est proposée, il y aurait des cris immédiats : « mais si nous sommes arrêtés, *ils* vont se précipiter en avant », où « ils » est maintenant la Chine (pour les États-Unis), ou les États-Unis (pour la Chine), ou la Chine *et* les États-Unis (pour l'Europe ou l'Inde). Sous cet état d'esprit,[^72] aucun participant ne peut s'arrêter unilatéralement : tant qu'un s'engage à courir, les autres sentent qu'ils ne peuvent pas se permettre de s'arrêter.

La seconde possibilité a un côté « gagnant ». Mais qu'est-ce que cela signifie ? Juste obtenir (d'une façon ou d'une autre obéissante) l'IAG en premier ne suffit pas. Le gagnant doit *aussi* empêcher les autres de continuer à courir – sinon ils l'obtiendront aussi. C'est possible en principe : quiconque développe l'IAG en premier *pourrait* gagner un pouvoir imparable sur tous les autres acteurs. Mais qu'est-ce qu'obtenir un tel « avantage stratégique décisif » nécessiterait vraiment ? Peut-être seraient-ce des capacités militaires révolutionnaires ?[^73] Ou des pouvoirs de cyberattaque ?[^74] Peut-être l'IAG serait-elle juste si étonnamment persuasive qu'elle convaincrait les autres parties de juste s'arrêter ?[^75] Si riche qu'elle achèterait les autres entreprises ou même pays ?[^76]

Comment *exactement* un côté construit-il une IA assez puissante pour désarmer les autres de construire une IA comparablement puissante ? Mais c'est la question facile.

Car maintenant considérez à quoi cette situation ressemble pour les autres puissances. Que pense le gouvernement chinois quand les États-Unis semblent obtenir une telle capacité ? Ou vice-versa ? Que pense le gouvernement américain (ou chinois, ou russe, ou indien) quand OpenAI ou DeepMind ou Anthropic semble proche d'une percée ? Que se passe-t-il si les États-Unis voient un nouvel effort indien ou émirien avec un succès révolutionnaire ? Ils verraient à la fois une menace existentielle et – crucialement – que la seule façon dont cette « course » se termine est par leur propre désarmement. Ces agents très puissants – incluant les gouvernements de nations pleinement équipées qui ont sûrement les moyens de le faire – seraient hautement motivés à soit obtenir ou détruire une telle capacité, que ce soit par force ou subterfuge.[^77]

Cela pourrait commencer petit, comme sabotage d'entraînements ou attaques sur la fabrication de puces, mais ces attaques ne peuvent vraiment s'arrêter qu'une fois que toutes les parties soit perdent la capacité de courir sur l'IA, ou perdent la capacité de faire les attaques. Parce que les participants voient les enjeux comme existentiels, l'un ou l'autre cas est susceptible de représenter une guerre catastrophique.

Cela nous amène à la quatrième possibilité : courir vers la superintelligence, et de la façon la plus rapide et la moins contrôlée possible. Alors que l'IA augmente en puissance, ses développeurs des deux côtés la trouveront progressivement plus difficile à contrôler, surtout parce que courir pour les capacités est antithétique au genre de travail soigneux que la contrôlabilité nécessiterait. Donc ce scénario nous place directement dans le cas où le contrôle est perdu (ou donné, comme nous le verrons ensuite) aux systèmes d'IA eux-mêmes. C'est-à-dire, *l'IA gagne la course.* Mais d'un autre côté, dans la mesure où le contrôle *est* maintenu, nous continuons à avoir plusieurs parties mutuellement hostiles chacune en charge de capacités extrêmement puissantes. Cela ressemble encore à une guerre.

Mettons cela d'une autre façon.[^78] Le monde actuel n'a simplement aucune institution qui pourrait être confiée pour héberger le développement d'une IA de cette capacité sans inviter une attaque immédiate.[^79] Toutes les parties raisonneront correctement que soit elle ne sera *pas* sous contrôle – et donc est une menace pour toutes les parties, ou elle *sera* sous contrôle, et donc est une menace pour tout adversaire qui la développe moins rapidement. Ce sont des pays armés nucléairement, ou sont des entreprises hébergées en leur sein.

En l'absence de toute façon plausible pour les humains de « gagner » cette course, nous sommes laissés avec une conclusion brutale : la seule façon dont cette course se termine est soit dans un conflit catastrophique ou où l'IA, et non tout groupe humain, est le gagnant.

### Nous donnons le contrôle à l'IA (ou elle le prend)

La compétition « grandes puissances » géopolitique n'est qu'une des nombreuses compétitions : les individus sont en compétition économiquement et socialement ; les entreprises sont en compétition sur les marchés ; les partis politiques sont en compétition pour le pouvoir ; les mouvements sont en compétition pour l'influence. Dans chaque arène, alors que l'IA approche et dépasse la capacité humaine, la pression compétitive forcera les participants à déléguer ou céder de plus en plus de contrôle aux systèmes d'IA – non parce que ces participants veulent le faire, mais parce qu'ils [ne peuvent pas se permettre de ne pas le faire.](https://arxiv.org/abs/2303.16200)

Comme avec d'autres risques de l'IAG, nous voyons cela déjà avec des systèmes plus faibles. Les étudiants ressentent la pression d'utiliser l'IA dans leurs devoirs, parce que clairement beaucoup d'autres étudiants le font. Les entreprises se [précipitent pour adopter des solutions d'IA pour des raisons compétitives.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) Les artistes et programmeurs se sentent forcés d'utiliser l'IA ou sinon leurs tarifs seront sous-coupés par d'autres qui le font.

Cela ressemble à une délégation sous pression, mais pas à une perte de contrôle. Mais augmentons les enjeux et avançons l'horloge. Considérez un PDG dont les compétiteurs utilisent des « aides » d'IAG pour prendre des décisions plus rapides et meilleures, ou un commandant militaire face à un adversaire avec commande et contrôle améliorés par l'IA. Un système d'IA suffisamment avancé pourrait opérer de façon autonome à plusieurs fois la vitesse, sophistication, complexité, et capacité de traitement de données humaines, poursuivant des objectifs complexes de façons compliquées. Notre PDG ou commandant, en charge d'un tel système, peut le voir accomplir ce qu'il veut ; mais comprendraient-ils même une petite partie de *comment* c'était accompli ? Non, ils devraient juste l'accepter. Qui plus est, beaucoup de ce que le système peut faire n'est pas juste prendre des ordres mais conseiller son patron putatif sur quoi faire. Ce conseil sera bon –– encore et encore.

À quel point, alors, le rôle de l'humain sera-t-il réduit à cliquer « oui, allez-y » ?

Il fait bon d'avoir des systèmes d'IA capables qui peuvent améliorer notre productivité, s'occuper de corvées ennuyeuses, et même agir comme partenaire de réflexion pour faire les choses. Il fera bon d'avoir un assistant IA qui peut s'occuper d'actions pour nous, comme un bon assistant personnel humain. Il semblera naturel, même bénéfique, alors que l'IA devient très intelligente, compétente, et fiable, de déléguer de plus en plus de décisions à elle. Mais cette délégation « bénéfique » a un point final clair si nous continuons sur la route : un jour nous trouverons que nous ne sommes vraiment plus en charge de grand-chose, et que les systèmes d'IA dirigeant vraiment le spectacle ne peuvent pas plus être éteints que les compagnies pétrolières, les médias sociaux, l'internet, ou le capitalisme.

Et c'est la version bien plus positive, dans laquelle l'IA est simplement si utile et efficace que nous la laissons prendre la plupart de nos décisions clés pour nous. La réalité serait probablement bien plus un mélange entre cela et des versions où des systèmes d'IAG incontrôlés *prennent* diverses formes de pouvoir pour eux-mêmes parce que, rappelez-vous, le pouvoir est utile pour presque tout objectif qu'on a, et l'IAG serait, par conception, au moins aussi efficace à poursuivre ses objectifs que les humains.

Que nous accordions le contrôle ou qu'il nous soit arraché, sa perte semble extrêmement probable. Comme Alan Turing l'avait originellement dit, « ...il semble probable qu'une fois que la méthode de pensée machine aurait commencé, il ne faudrait pas longtemps pour dépasser nos faibles pouvoirs. Il n'y aurait pas de question de machines mourant, et elles pourraient converser entre elles pour aiguiser leur esprit. À un certain stade donc nous devrions nous attendre à ce que les machines prennent le contrôle... »

Veuillez noter, bien que ce soit assez évident, que la perte de contrôle par l'humanité à l'IA entraîne aussi la perte de contrôle des États-Unis par le gouvernement des États-Unis ; elle signifie la perte de contrôle de la Chine par le parti communiste chinois, et la perte de contrôle de l'Inde, France, Brésil, Russie, et tout autre pays par leur propre gouvernement. Ainsi les entreprises d'IA sont, même si ce n'est pas leur intention, actuellement en train de participer au renversement potentiel des gouvernements mondiaux, incluant le leur. Cela pourrait arriver en quelques années.

### L'IAG mènera à la superintelligence

Il y a un argument à faire que l'IA générale compétitive ou même expert-compétitive, même si autonome, pourrait être gérable. Elle peut être incroyablement perturbatrice de toutes les façons discutées ci-dessus, mais il y a beaucoup de gens très intelligents et agentiels dans le monde maintenant, et ils sont plus ou moins gérables.[^80]

Mais nous n'arriverons pas à rester à peu près au niveau humain. La progression au-delà sera probablement menée par les mêmes forces que nous avons déjà vues : pression compétitive entre développeurs d'IA cherchant profit et pouvoir, pression compétitive entre utilisateurs d'IA qui ne peuvent pas se permettre de prendre du retard, et – plus important – la propre capacité de l'IAG à s'améliorer elle-même.

Dans un processus que nous avons déjà vu commencer avec des systèmes moins puissants, l'IAG serait elle-même capable de concevoir et créer des versions améliorées d'elle-même. Cela inclut matériel, logiciel, réseaux de neurones, outils, architectures de support, etc. Elle sera, par définition, meilleure que nous pour le faire, donc nous ne savons pas exactement comment elle va s'auto-améliorer en intelligence. Mais nous n'aurons pas à le faire. Dans la mesure où nous avons encore de l'influence sur ce que l'IAG fait, nous n'aurions besoin que de lui demander, ou la laisser faire.

Il n'y a pas de barrière de niveau humain à la cognition qui pourrait nous protéger de cet emballement.[^81]

La progression de l'IAG à la superintelligence n'est pas une loi de nature ; il serait encore possible de freiner l'emballement, surtout si l'IAG est relativement centralisée et dans la mesure où elle est contrôlée par des parties qui ne ressentent pas de pression à courir les unes contre les autres. Mais si l'IAG était largement proliférée et hautement autonome, il semble presque impossible d'empêcher qu'elle décide qu'elle devrait être plus, et puis encore plus, puissante.

### Que se passe-t-il si nous construisons (ou l'IAG construit) la superintelligence

Pour le dire sans détour, nous n'avons aucune idée de ce qui arriverait si nous construisions la superintelligence.[^82] Elle prendrait des actions que nous ne pouvons pas suivre ou percevoir pour des raisons que nous ne pouvons pas saisir vers des objectifs que nous ne pouvons pas concevoir. Ce que nous savons c'est que ce ne sera pas à nous d'en décider.[^83]

L'impossibilité de contrôler la superintelligence peut être comprise par des analogies de plus en plus brutales. D'abord, imaginez que vous êtes PDG d'une grande entreprise. Il n'y a pas moyen que vous puissiez suivre tout ce qui se passe, mais avec la bonne configuration de personnel, vous pouvez encore comprendre de façon significative la vue d'ensemble, et prendre des décisions. Mais supposez juste une chose : tout le monde d'autre dans l'entreprise opère à cent fois votre vitesse. Pouvez-vous encore suivre ?

Avec l'IA superintelligente, les gens « commanderaient » quelque chose non seulement plus rapide, mais opérant à des niveaux de sophistication et complexité qu'ils ne peuvent pas comprendre, traitant vastement plus de données qu'ils ne peuvent même concevoir. Cette incommensurabilité peut être mise sur un niveau formel : [la loi de variété requise d'Ashby](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (et voir le [« théorème du bon régulateur »](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf) relié) énonce, grossièrement, que tout système de contrôle doit avoir autant de boutons et cadrans que le système contrôlé a de degrés de liberté.

Une personne contrôlant un système d'IA superintelligent serait comme une fougère contrôlant General Motors : même si « faire ce que la fougère veut » était écrit dans les statuts corporatifs, les systèmes sont si différents en vitesse et étendue d'action que « contrôle » ne s'applique simplement pas. (Et combien de temps avant que ce statut gênant ne soit réécrit ?)[^84]

Comme il y a zéro exemple de plantes contrôlant des entreprises du fortune 500, il y aurait exactement zéro exemple de gens contrôlant des superintelligences. Cela approche un fait mathématique.[^85] Si la superintelligence était construite – peu importe comment nous y sommes arrivés – la question ne serait pas de savoir si les humains pourraient la contrôler, mais de savoir si nous continuerions à exister, et si oui, si nous aurions une existence bonne et significative comme individus ou comme espèce. Sur ces questions existentielles pour l'humanité nous aurions peu de prise. L'ère humaine serait finie.

### Conclusion : nous ne devons pas construire l'IAG

Il y a un scénario dans lequel construire l'IAG peut bien aller pour l'humanité : elle est construite soigneusement, sous contrôle et pour le bénéfice de l'humanité, gouvernée par accord mutuel de nombreuses parties prenantes,[^86] et empêchée d'évoluer vers une superintelligence incontrôlable.

*Ce scénario ne nous est pas ouvert dans les circonstances présentes.* Comme discuté dans cette section, avec une probabilité très élevée, le développement de l'IAG mènerait à une certaine combinaison de :

- Perturbation ou destruction sociétale et civilisationnelle massive ;
- Conflit ou guerre entre grandes puissances ;
- Perte de contrôle par l'humanité *des* ou *aux* systèmes d'IA puissants ;
- Emballement vers une superintelligence incontrôlable, et l'insignifiance ou cessation de l'espèce humaine.

Comme une première représentation fictionnelle de l'IAG l'a dit : la seule façon de gagner est de ne pas jouer.

[^55]: La [Loi IA de l'UE](https://artificialintelligenceact.eu/) est un morceau significatif de législation mais n'empêcherait pas directement un système d'IA dangereux d'être développé ou déployé, ou même ouvertement libéré, surtout aux États-Unis. Un autre morceau significatif de politique, l'ordre exécutif américain sur l'IA, a été rescindé.

[^56]: Ce [sondage Gallup](https://news.gallup.com/poll/1597/confidence-institutions.aspx) montre un déclin lugubre de confiance dans les institutions publiques depuis 2000 aux États-Unis. Les chiffres européens sont variés et moins extrêmes, mais aussi sur une tendance descendante. La méfiance ne signifie pas strictement que les institutions sont vraiment *dysfonctionnelles*, mais c'est une indication aussi bien qu'une cause.

[^57]: Et les perturbations majeures que nous approuvons maintenant – comme l'expansion des droits à de nouveaux groupes – étaient spécifiquement menées par des gens dans une direction vers l'amélioration des choses.

[^58]: Permettez-moi d'être direct. Si votre travail peut être fait de derrière un ordinateur, avec relativement peu d'interaction en personne avec des gens en dehors de votre organisation, et n'implique pas de responsabilité légale envers des parties externes, il serait par définition possible (et probablement économe) de vous remplacer complètement par un système numérique. La robotique pour remplacer beaucoup de travail physique viendra plus tard – mais pas beaucoup plus tard une fois que l'IAG commence à concevoir des robots.

[^59]: Par exemple, qu'arrive-t-il à notre système judiciaire si les procès sont presque gratuits à déposer ? Qu'arrive-t-il quand contourner les systèmes de sécurité par ingénierie sociale devient bon marché, facile, et sans risque ?

[^60]: [Cet article](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/) prétend que 10% de tout contenu internet est déjà généré par IA, et est le premier résultat Google (pour moi) à la requête de recherche « estimations de quelle fraction de nouveau contenu internet est générée par IA ». Est-ce vrai ? Je n'en ai aucune idée ! Il ne cite aucune référence et il n'a pas été écrit par une personne. Quelle fraction de nouvelles images indexées par Google, ou Tweets, ou commentaires sur Reddit, ou vidéos Youtube sont générées par des humains ? Personne ne le sait – je ne pense pas que ce soit un nombre connaissable. Et cela moins de *deux ans* dans l'avènement de l'IA générative.

[^61]: Vaut aussi la peine d'ajouter qu'il y a un risque « moral » que nous pourrions créer des êtres numériques qui peuvent souffrir. Comme nous n'avons actuellement pas de théorie fiable de la conscience qui nous permettrait de distinguer les systèmes physiques qui peuvent et ne peuvent pas souffrir, nous ne pouvons pas l'exclure théoriquement. De plus, les rapports des systèmes d'IA de leur sentience sont probablement peu fiables par rapport à leur expérience réelle (ou non-expérience) de sentience.

[^62]: Les solutions techniques dans ce domaine d' « alignement » de l'IA sont improbables d'être à la hauteur de la tâche non plus. Dans les systèmes présents elles fonctionnent à un certain niveau, mais sont superficielles et peuvent généralement être contournées sans effort significatif ; et comme discuté ci-dessous nous n'avons aucune idée réelle de comment faire cela pour des systèmes beaucoup plus avancés.

[^63]: De tels systèmes d'IA peuvent venir avec quelques garde-fous intégrés. Mais pour tout modèle avec quelque chose comme l'architecture actuelle, si l'accès complet à ses poids est disponible, les mesures de sécurité peuvent être retirées via entraînement additionnel ou autres techniques. Donc il est virtuellement garanti que pour chaque système avec garde-fous il y aura aussi un système largement disponible sans eux. En effet le modèle Llama 3.1 405B de Meta a été ouvertement libéré avec garde-fous. Mais *même avant cela* un modèle « de base », sans garde-fous, a fuité.

[^64]: Le marché pourrait-il gérer ces risques sans implication gouvernementale ? En bref, non. Il y a certainement des risques que les entreprises sont fortement incitées à mitiger. Mais beaucoup d'autres les entreprises peuvent et externalisent à tout le monde d'autre, et beaucoup des ci-dessus sont dans cette classe : il n'y a pas d'incitations de marché naturelles pour prévenir la surveillance de masse, la décadence de vérité, la concentration du pouvoir, la perturbation du travail, le discours politique dommageable, etc. En effet nous avons vu tout cela de la tech d'aujourd'hui, surtout les médias sociaux, qui sont allés essentiellement non régulés. L'IA ne ferait qu'amplifier énormément beaucoup des mêmes dynamiques.

[^65]: OpenAI a probablement des modèles plus obéissants pour usage interne. Il est improbable qu'OpenAI ait construit une sorte de « porte dérobée » pour que ChatGPT puisse être mieux contrôlé par OpenAI lui-même, parce que ce serait une pratique de sécurité terrible, et être hautement exploitable étant donné l'opacité et l'imprévisibilité de l'IA.

[^66]: Aussi d'importance cruciale : l'alignement ou toute autre caractéristique de sécurité n'importe que s'ils sont vraiment utilisés dans un système d'IA. Les systèmes qui sont ouvertement libérés (c'est-à-dire où les poids et architecture du modèle sont publiquement disponibles) peuvent être transformés relativement facilement en systèmes *sans* ces mesures de sécurité. Libérer ouvertement des systèmes d'IAG plus intelligents que humains serait étonnamment imprudent, et il est difficile d'imaginer comment le contrôle humain ou même la pertinence serait maintenue dans un tel scénario. Il y aurait toute motivation, par exemple, à lâcher de puissants agents d'IA auto-reproducteurs et auto-entretenus avec l'objectif de faire de l'argent et l'envoyer à un portefeuille de cryptomonnaie. Ou gagner une élection. Ou renverser un gouvernement. L'IA « bonne » pourrait-elle aider à contenir cela ? Peut-être – mais seulement en lui déléguant une autorité énorme, menant à la perte de contrôle comme décrit ci-dessous.

[^67]: Pour des expositions de longueur de livre du problème voir par exemple *Superintelligence*, *The Alignment Problem*, et *Human-Compatible*. Pour un énorme tas de travail à divers niveaux techniques par ceux qui ont peiné pendant des années à réfléchir au problème, vous pouvez visiter le [forum d'alignement de l'IA](https://www.alignmentforum.org/). Voici une [prise récente](https://alignment.anthropic.com/2025/recommended-directions/) de l'équipe d'alignement d'Anthropic sur ce qu'ils considèrent non résolu.

[^68]: C'est le scénario [« IA voyou »](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/). En principe le risque pourrait être relativement mineur si le système peut encore être contrôlé en l'éteignant ; mais le scénario pourrait aussi inclure tromperie de l'IA, auto-exfiltration et reproduction, agrégation de pouvoir, et autres étapes qui rendraient difficile ou impossible de le faire.

[^69]: Il y a une littérature très riche sur ce sujet, remontant aux écrits formatifs par [Steve Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), Nick Bostrom, et Eliezer Yudkowsky. Pour une exposition de longueur de livre voir [Human Compatible](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) par Stuart Russell ; [voici](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/) un primer court et à jour.

[^70]: Reconnaissant cela, plutôt que de ralentir pour obtenir une meilleure compréhension, les entreprises d'IAG ont inventé un plan différent : elles vont faire faire cela à l'IA ! Plus spécifiquement, elles vont avoir l'IA *N* les aider à comprendre comment aligner l'IA *N+1*, tout le chemin vers la superintelligence. Bien que tirer parti de l'IA pour nous aider à aligner l'IA sonne prometteur, il y a un argument fort qu'elle assume simplement sa conclusion comme prémisse, et est en général une approche incroyablement risquée. Voir [ici](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us) pour quelque discussion. Ce « plan » n'en est pas un, et a subi rien comme l'examen minutieux approprié à la stratégie centrale de comment rendre l'IA super-humaine bonne pour l'humanité.

[^71]: Après tout, les humains, imparfaits et volontaires comme nous sommes, ont développé des systèmes éthiques par lesquels nous traitons au moins quelques autres espèces sur Terre bien. (Juste ne pensez pas à ces fermes-usines.)

[^72]: Il y a, heureusement, une échappatoire ici : si les participants en viennent à comprendre qu'ils sont engagés dans une course suicide plutôt qu'une gagnable. C'est ce qui est arrivé près de la fin de la guerre froide, quand les États-Unis et URSS en sont venus à réaliser qu'à cause de l'hiver nucléaire, même une attaque nucléaire *sans réponse* serait désastreuse pour l'attaquant. Avec la réalisation que « la guerre nucléaire ne peut pas être gagnée et ne doit jamais être menée » sont venus des accords significatifs sur la réduction d'armes – essentiellement une fin à la course aux armements.

[^73]: Guerre, explicitement ou implicitement.

[^74]: Escalade, puis guerre.

[^75]: Pensée magique.

[^76]: J'ai aussi un pont à un quadrillion de dollars à vous vendre.

[^77]: De tels agents préféreraient présumablement « obtenir », avec destruction comme solution de rechange ; mais sécuriser les modèles contre à la fois destruction *et* vol par des nations puissantes est difficile pour dire le moins, surtout pour des entités privées.

[^78]: Pour une autre perspective sur les risques de sécurité nationale de l'IAG, voir [ce rapport RAND.](https://www.rand.org/pubs/perspectives/PEA3691-4.html)

[^79]: Peut-être pourrions-nous construire une telle institution ! Il y a eu des propositions pour un « CERN pour l'IA » et autres initiatives similaires, où le développement d'IAG est sous contrôle global multilatéral. Mais au moment aucune telle institution n'existe ou n'est à l'horizon.

[^80]: Et bien que l'alignement soit très difficile, faire que les gens se comportent est encore plus dur !

[^81]: Imaginez un système qui peut parler 50 langues, avoir de l'expertise dans tous les sujets académiques, lire un livre complet en secondes et avoir tout le matériel immédiatement en tête, et produire des sorties à dix fois la vitesse humaine. Vraiment, vous n'avez pas à l'imaginer : juste chargez un système d'IA actuel. Ceux-ci sont super-humains de beaucoup de façons, et il n'y a rien qui les arrête d'être encore plus super-humains dans celles-ci et beaucoup d'autres.

[^82]: C'est pourquoi cela a été appelé une « singularité » technologique, empruntant à la physique l'idée qu'on ne peut pas faire de prédictions au-delà d'une singularité. Les promoteurs de se pencher *dans* une telle singularité peuvent aussi vouloir réfléchir qu'en physique ces mêmes sortes de singularités déchirent et écrasent ceux qui y entrent.

[^83]: Le problème était exhaustivement esquissé dans [*Superintelligence*](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834) de Bostrom, et rien depuis n'a significativement changé le message central. Pour un volume plus récent collectant les résultats formels et mathématiques sur l'incontrôlabilité voir [AI: Unexplainable, Unpredictable, Uncontrollable](https://www.amazon.com/Unexplainable-Unpredictable-Uncontrollable-Artificial-Intelligence/dp/103257626X) de Yampolskiy

[^84]: Cela clarifie aussi pourquoi la stratégie actuelle des entreprises d'IA (laisser itérativement l'IA « aligner » la prochaine IA la plus puissante) ne peut pas fonctionner. Supposez qu'une fougère, via l'agrément de ses frondes, enrôle un élève de première année pour s'en occuper. L'élève de première année écrit quelques instructions détaillées pour un élève de seconde à suivre, et une note le convainquant de le faire. L'élève de seconde fait de même pour un élève de troisième, et ainsi de suite tout le chemin vers un diplômé d'université, un manager, un exécutif, et finalement le PDG de GM. GM va-t-elle alors « faire ce que la fougère veut » ? À chaque étape cela pourrait sembler fonctionner. Mais mettant tout ensemble, cela fonctionnera presque exactement dans la mesure où le PDG, Conseil, et actionnaires de GM se trouvent se soucier des enfants et fougères, et avoir peu à rien à voir avec toutes ces notes et ensembles d'instructions.

[^85]: Le caractère n'est pas si différent de résultats formels comme le théorème d'incomplétude de Gödel ou l'argument d'arrêt de Turing en ce que la notion de contrôle contredit fondamentalement la prémisse : comment pouvez-vous contrôler de façon significative quelque chose que vous ne pouvez pas comprendre ou prédire ; pourtant si vous pouviez comprendre et prédire la superintelligence vous seriez superintelligent. La raison pour laquelle je dis « approche » est que les résultats formels ne sont pas aussi approfondis ou vérifiés que dans le cas des mathématiques pures, et parce que j'aimerais garder espoir qu'une intelligence générale très soigneusement construite, utilisant des méthodes totalement différentes de celles actuellement employées, pourrait avoir quelques propriétés de sécurité mathématiquement prouvables, selon le genre de programme d'IA « garantie sûre » discuté ci-dessous.

[^86]: Au moment, la plupart des parties prenantes – c'est-à-dire, presque toute l'humanité – est mise à l'écart dans cette discussion. C'est profondément mauvais, et si pas invités, les nombreux, nombreux autres groupes seront affectés par le développement d'IAG devraient exiger d'être admis.

## Chapitre 8 - Comment ne pas construire d'IAG

L'IAG n'est pas inévitable – nous nous trouvons aujourd'hui à un carrefour. Ce chapitre présente une proposition sur la manière dont nous pourrions empêcher sa construction.

Si la voie sur laquelle nous nous trouvons actuellement mène probablement à la fin de notre civilisation, comment changer de direction ?

Supposons que le désir d'arrêter le développement de l'IAG et de la superintelligence soit répandu et puissant,[^87] parce qu'il devient communément admis que l'IAG absorberait le pouvoir plutôt qu'elle ne l'accorderait, et qu'elle représente un danger profond pour la société et l'humanité. Comment fermerions-nous les Portes ?

À l'heure actuelle, nous ne connaissons qu'une seule façon de *créer* une IA puissante et générale, à savoir par des calculs vraiment massifs de réseaux de neurones profonds. Comme il s'agit d'opérations incroyablement difficiles et coûteuses à réaliser, il y a un sens dans lequel *ne pas* les faire est facile.[^88] Mais nous avons déjà vu les forces qui poussent vers l'IAG, et la dynamique de théorie des jeux qui rend très difficile pour toute partie d'arrêter unilatéralement. Il faudrait donc une combinaison d'intervention de l'extérieur (c'est-à-dire des gouvernements) pour arrêter les entreprises, et d'accords entre gouvernements pour s'arrêter eux-mêmes.[^89] À quoi cela pourrait-il ressembler ?

Il est utile de distinguer d'abord entre les développements d'IA qui doivent être *empêchés* ou *interdits*, et ceux qui doivent être *gérés*. Les premiers seraient principalement l'emballement vers la superintelligence.[^90] Pour le développement interdit, les définitions devraient être aussi précises que possible, et tant la vérification que l'application devraient être pratiques. Ce qui doit être *géré* serait les systèmes d'IA généraux et puissants – que nous avons déjà, et qui présenteront de nombreuses zones grises, nuances et complexités. Pour ceux-ci, des institutions solides et efficaces sont cruciales.

Nous pouvons aussi utilement délimiter les questions qui doivent être abordées au niveau international (y compris entre rivaux ou adversaires géopolitiques)[^91] de celles que des juridictions individuelles, des pays, ou des groupes de pays peuvent gérer. Le développement interdit relève largement de la catégorie « internationale », car une interdiction locale du développement d'une technologie peut généralement être contournée en changeant de lieu.[^92]

Enfin, nous pouvons considérer les outils dans la boîte à outils. Il y en a beaucoup, notamment les outils techniques, le droit souple (normes, standards, etc.), le droit dur (réglementations et exigences), la responsabilité, les incitations de marché, et ainsi de suite. Portons une attention particulière à un qui est spécifique à l'IA.

### Sécurité matérielle et gouvernance computationnelle

Un outil central dans la gouvernance de l'IA de haute puissance sera le matériel qu'elle nécessite. Le logiciel prolifère facilement, a un coût marginal de production quasi nul, traverse les frontières trivialement, et peut être instantanément modifié ; rien de tout cela n'est vrai pour le matériel. Pourtant, comme nous l'avons discuté, d'énormes quantités de cette « puissance de calcul » sont nécessaires à la fois pendant l'entraînement des systèmes d'IA et pendant l'inférence pour atteindre les systèmes les plus capables. Le calcul peut être facilement quantifié, comptabilisé et audité, avec relativement peu d'ambiguïté une fois que de bonnes règles pour le faire sont développées. Plus crucial encore, de grandes quantités de calcul sont, comme l'uranium enrichi, une ressource très rare, coûteuse et difficile à produire. Bien que les puces informatiques soient omniprésentes, le matériel requis pour l'IA est cher et énormément difficile à fabriquer.[^93]

Ce qui rend les puces spécialisées en IA *beaucoup plus* gérables comme ressource rare que l'uranium, c'est qu'elles peuvent inclure des mécanismes de sécurité basés sur le matériel. La plupart des téléphones portables modernes, et certains ordinateurs portables, ont des fonctionnalités matérielles spécialisées sur puce qui leur permettent de s'assurer qu'ils n'installent que des logiciels de système d'exploitation et des mises à jour approuvés, qu'ils conservent et protègent des données biométriques sensibles sur l'appareil, et qu'ils peuvent être rendus inutiles à quiconque d'autre que leur propriétaire s'ils sont perdus ou volés. Au cours des dernières années, de telles mesures de sécurité matérielle sont devenues bien établies et largement adoptées, et se sont généralement révélées assez sécurisées.

La nouveauté clé de ces fonctionnalités est qu'elles lient matériel et logiciel ensemble en utilisant la cryptographie.[^94] C'est-à-dire que le simple fait d'avoir un morceau particulier de matériel informatique ne signifie pas qu'un utilisateur peut faire tout ce qu'il veut avec en appliquant différents logiciels. Et cette liaison fournit aussi une sécurité puissante parce que de nombreuses attaques nécessiteraient une violation de la sécurité *matérielle* plutôt que seulement *logicielle*.

Plusieurs rapports récents (par exemple de [GovAI et collaborateurs](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips), et [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) ont souligné que des fonctionnalités matérielles similaires intégrées dans le matériel informatique de pointe pertinent pour l'IA pourraient jouer un rôle extrêmement utile dans la sécurité et la gouvernance de l'IA. Elles permettent un certain nombre de fonctions disponibles à un « gouverneur »[^95] que l'on ne pourrait pas deviner être disponibles ou même possibles. Voici quelques exemples clés :

- *Géolocalisation* : Les systèmes peuvent être configurés de sorte que les puces aient un emplacement connu, et puissent agir différemment (ou être complètement éteintes) selon l'emplacement.[^96]
- *Connexions sur liste blanche* : chaque puce peut être configurée avec une liste blanche appliquée par le matériel de puces particulières avec lesquelles elle peut se connecter en réseau, et être incapable de se connecter avec toute puce ne figurant pas sur cette liste.[^97] Cela peut limiter la taille des grappes communicantes de puces.[^98]
- *Inférence ou entraînement mesurés (et arrêt automatique)* : Un gouverneur peut licencier seulement une certaine quantité d'entraînement ou d'inférence (en temps, ou FLOP, ou possiblement tokens) à effectuer par un utilisateur, après quoi une nouvelle permission est requise. Si les incréments sont petits, alors un re-licenciement relativement continu d'un modèle est requis. Le modèle peut alors être « éteint » simplement en retenant ce signal de licence.[^99]
- *Limitation de vitesse* : Un modèle est empêché de fonctionner à une vitesse d'inférence supérieure à une certaine limite qui est déterminée par un gouverneur ou autrement. Cela pourrait être implémenté via un ensemble limité de connexions sur liste blanche, ou par des moyens plus sophistiqués.
- *Entraînement attesté* : Une procédure d'entraînement peut produire une preuve cryptographiquement sécurisée qu'un ensemble particulier de codes, de données, et de quantité d'usage de calcul ont été employés dans la génération du modèle.

### Comment ne pas construire la superintelligence : limites mondiales sur le calcul d'entraînement et d'inférence

Avec ces considérations – en particulier concernant le calcul – en place, nous pouvons discuter de la façon de fermer les Portes à la superintelligence artificielle ; nous nous tournerons ensuite vers la prévention de l'IAG complète, et la gestion des modèles d'IA alors qu'ils approchent et dépassent les capacités humaines dans différents aspects.

Le premier ingrédient est, bien sûr, la compréhension que la superintelligence ne serait pas contrôlable, et que ses conséquences sont fondamentalement imprévisibles. Au moins la Chine et les États-Unis doivent décider indépendamment, pour cette raison ou d'autres, de ne pas construire la superintelligence.[^100] Puis un accord international entre eux et d'autres, avec un mécanisme de vérification et d'application solide, est nécessaire pour assurer toutes les parties que leurs rivaux ne font pas défection et ne décident pas de jouer aux dés.

Pour être vérifiables et applicables, les limites devraient être des limites dures, et aussi non ambiguës que possible. Cela semble être un problème virtuellement impossible : limiter les capacités de logiciels complexes aux propriétés imprévisibles, dans le monde entier. Heureusement, la situation est beaucoup mieux que cela, car la chose même qui a rendu l'IA avancée possible – une énorme quantité de calcul – est beaucoup, beaucoup plus facile à contrôler. Bien qu'elle puisse encore permettre certains systèmes puissants et dangereux, l'*emballement de la superintelligence* peut probablement être empêché par un plafond dur sur la quantité de calcul qui entre dans un réseau de neurones, ainsi qu'une limite de taux sur la quantité d'inférence qu'un système d'IA (de réseaux de neurones connectés et d'autres logiciels) peut effectuer. Une version spécifique de ceci est proposée ci-dessous.

Il peut sembler que placer des limites mondiales dures sur le calcul d'IA nécessiterait d'énormes niveaux de coordination internationale et une surveillance intrusive qui brise la vie privée. Heureusement, ce ne serait pas le cas. La [chaîne d'approvisionnement extrêmement serrée et à goulot d'étranglement](https://arxiv.org/abs/2402.08797) fait qu'une fois qu'une limite est fixée légalement (que ce soit par la loi ou un décret exécutif), la vérification de conformité à cette limite ne nécessiterait que l'implication et la coopération d'une poignée de grandes entreprises.[^101]

Un plan comme celui-ci a un certain nombre de caractéristiques hautement désirables. Il est minimalement invasif dans le sens où seules quelques grandes entreprises ont des exigences qui leur sont imposées, et seules des grappes de calcul assez significatives seraient gouvernées. Les puces pertinentes contiennent déjà les capacités matérielles nécessaires pour une première version.[^102] L'implémentation et l'application reposent toutes deux sur des restrictions légales standard. Mais celles-ci sont soutenues par les conditions d'utilisation du matériel et par des contrôles matériels, simplifiant vastement l'application et prévenant la triche par les entreprises, groupes privés, ou même pays. Il y a un précédent ample pour les entreprises de matériel plaçant des restrictions à distance sur l'usage de leur matériel, et verrouillant/déverrouillant des capacités particulières extérieurement,[^103] y compris même dans des CPU de haute puissance dans des centres de données.[^104] Même pour la fraction plutôt petite de matériel et d'organisations affectées, la supervision pourrait être limitée à la télémétrie, sans accès direct aux données ou modèles eux-mêmes ; et le logiciel pour cela pourrait être ouvert à l'inspection pour montrer qu'aucune donnée supplémentaire n'est enregistrée. Le schéma est international et coopératif, et assez flexible et extensible. Parce que la limite porte principalement sur le matériel plutôt que le logiciel, elle est relativement agnostique quant à la façon dont le développement et le déploiement de logiciels d'IA se déroulent, et est compatible avec une variété de paradigmes incluant l'IA plus « décentralisée » ou « publique » visant à combattre la concentration du pouvoir entraînée par l'IA.

Une fermeture de Porte basée sur le calcul a aussi des inconvénients. Premièrement, c'est loin d'être une solution complète au problème de la gouvernance de l'IA en général. Deuxièmement, alors que le matériel informatique devient plus rapide, le système « attraperait » de plus en plus de matériel dans des grappes de plus en plus petites (ou même des GPU individuels).[^105] Il est aussi possible qu'en raison d'améliorations algorithmiques une limite de calcul encore plus basse soit nécessaire avec le temps,[^106] ou que la quantité de calcul devienne largement non pertinente et que fermer la Porte nécessite au contraire un régime de gouvernance plus détaillé basé sur le risque ou les capacités pour l'IA. Troisièmement, quelles que soient les garanties et le petit nombre d'entités affectées, un tel système est destiné à créer une résistance concernant la vie privée et la surveillance, entre autres préoccupations.[^107]

Bien sûr, développer et implémenter un schéma de gouvernance limitant le calcul dans une courte période de temps sera assez difficile. Mais c'est absolument faisable.

### A-I-G : La triple-intersection comme base du risque et de la politique

Tournons-nous maintenant vers l'IAG. Les lignes dures et les définitions ici sont plus difficiles, car nous avons certainement de l'intelligence qui est artificielle et générale, et selon aucune définition existante tout le monde ne sera d'accord si ou quand elle existe. De plus, une limite de calcul ou d'inférence est un outil quelque peu grossier (le calcul étant un proxy pour la capacité, qui est alors un proxy pour le risque) qui – à moins qu'elle ne soit assez basse – est peu susceptible d'empêcher l'IAG qui est assez puissante pour causer une perturbation sociale ou civilisationnelle ou des risques aigus.

J'ai soutenu que les risques les plus aigus émergent de la triple-intersection de très haute capacité, haute autonomie, et grande généralité. Ce sont les systèmes qui – s'ils sont développés du tout – doivent être gérés avec un soin énorme. En créant des standards stricts (par la responsabilité et la réglementation) pour les systèmes combinant les trois propriétés, nous pouvons canaliser le développement d'IA vers des alternatives plus sûres.

Comme avec d'autres industries et produits qui pourraient potentiellement nuire aux consommateurs ou au public, les systèmes d'IA nécessitent une réglementation soigneuse par des agences gouvernementales efficaces et habilitées. Cette réglementation devrait reconnaître les risques inhérents de l'IAG, et empêcher que des systèmes d'IA de haute puissance inacceptablement risqués ne soient développés.[^108]

Cependant, la réglementation à grande échelle, en particulier avec de vraies dents qui sont sûres d'être opposées par l'industrie,[^109] prend du temps[^110] ainsi qu'une conviction politique qu'elle est nécessaire.[^111] Étant donné le rythme des progrès, cela peut prendre plus de temps que nous n'en avons à disposition.

Sur une échelle de temps beaucoup plus rapide et alors que les mesures réglementaires sont développées, nous pouvons donner aux entreprises les incitations nécessaires pour (a) s'abstenir d'activités très à haut risque et (b) développer des systèmes complets pour évaluer et atténuer le risque, en clarifiant et augmentant les niveaux de responsabilité pour les systèmes les plus dangereux. L'idée serait d'imposer les plus hauts niveaux de responsabilité – stricte et dans certains cas criminelle personnelle – pour les systèmes dans la triple-intersection de haute autonomie-généralité-intelligence, mais de fournir des « régimes d'exonération » à une responsabilité plus typique basée sur la faute pour les systèmes dans lesquels une de ces propriétés manque ou est garantie d'être gérable. C'est-à-dire, par exemple, qu'un système « faible » qui est général et autonome (comme un assistant personnel capable et fiable mais limité) serait soumis à des niveaux de responsabilité plus bas. De même un système étroit et autonome comme une voiture autonome serait encore soumis à la réglementation significative à laquelle elle l'est déjà, mais pas à une responsabilité renforcée. Similairement pour un système très capable et général qui est « passif » et largement incapable d'action indépendante. Les systèmes manquant de *deux* des trois propriétés sont encore plus gérables et les régimes d'exonération seraient encore plus faciles à revendiquer. Cette approche reflète comment nous gérons d'autres technologies potentiellement dangereuses :[^112] une responsabilité plus élevée pour des configurations plus dangereuses crée des incitations naturelles pour des alternatives plus sûres.

Le résultat par défaut de tels hauts niveaux de responsabilité, qui agissent pour *internaliser* le risque d'IAG aux entreprises plutôt que de le décharger sur le public, est probable (et espérons-le !) que les entreprises ne développent simplement pas l'IAG complète jusqu'à et à moins qu'elles puissent réellement la rendre digne de confiance, sûre et contrôlable étant donné que *leur propre direction* sont les parties à risque. (Au cas où cela ne serait pas suffisant, la législation clarifiant la responsabilité devrait aussi explicitement permettre un recours injonctif, c'est-à-dire qu'un juge ordonne un arrêt, pour les activités qui sont clairement dans la zone de danger et posent de manière discutable un risque public.) Alors que la réglementation se met en place, se conformer à la réglementation peut devenir le régime d'exonération, et les régimes d'exonération de faible autonomie, étroitesse, ou faiblesse des systèmes d'IA peuvent se convertir en régimes réglementaires relativement plus légers.

### Dispositions clés d'une fermeture de Porte

Avec la discussion ci-dessus à l'esprit, cette section fournit des propositions pour des dispositions clés qui mettraient en œuvre et maintiendraient l'interdiction sur l'IAG complète et la superintelligence, et la gestion de l'IA généraliste compétitive avec l'humain ou compétitive avec l'expert près du seuil d'IAG complète.[^113] Elle a quatre pièces clés : 1) comptabilité et supervision du calcul, 2) plafonds de calcul dans l'entraînement et l'opération de l'IA, 3) un cadre de responsabilité, et 4) standards de sécurité et de sûreté échelonnés définis qui incluent des exigences réglementaires dures. Ceux-ci sont succinctement décrits ensuite, avec des détails supplémentaires ou des exemples d'implémentation donnés dans trois tableaux accompagnants. Important, notez que ceux-ci sont loin d'être tout ce qui sera nécessaire pour gouverner les systèmes d'IA avancés ; bien qu'ils aient des bénéfices de sécurité et de sûreté supplémentaires, ils visent à fermer la Porte à l'emballement de l'intelligence, et à rediriger le développement d'IA dans une meilleure direction.

#### 1\. Comptabilité du calcul et transparence

- Une organisation de standards (par exemple NIST aux États-Unis suivi par ISO/IEEE internationalement) devrait codifier un standard technique détaillé pour le calcul total utilisé dans l'entraînement et l'opération des modèles d'IA, en FLOP, et la vitesse en FLOP/s à laquelle ils opèrent. Des détails sur à quoi cela pourrait ressembler sont donnés en Annexe A.[^114]
- Une exigence – soit par nouvelle législation soit sous autorité existante[^115] – devrait être imposée par les juridictions dans lesquelles l'entraînement d'IA à grande échelle a lieu de calculer et rapporter à un corps réglementaire ou autre agence le total de FLOP utilisés dans l'entraînement et l'opération de tous les modèles au-dessus d'un seuil de 10<sup>25</sup> FLOP ou 10<sup>18</sup> FLOP/s.[^116]
- Ces exigences devraient être introduites par phases, nécessitant initialement des estimations de bonne foi bien documentées sur une base trimestrielle, avec des phases ultérieures nécessitant des standards progressivement plus élevés, jusqu'à des FLOP totaux et FLOP/s cryptographiquement attestés attachés à chaque *sortie* de modèle.
- Ces rapports devraient être complétés par des estimations bien documentées du coût énergétique et financier marginal utilisé dans la génération de chaque sortie d'IA.

Justification : Ces nombres bien calculés et rapportés transparemment fourniraient la base pour les plafonds d'entraînement et d'opération, ainsi qu'un régime d'exonération des mesures de responsabilité plus élevées (voir Annexes C et D).

#### 2\. Plafonds de calcul d'entraînement et d'opération

- Les juridictions hébergeant des systèmes d'IA devraient imposer une limite dure sur le calcul total entrant dans toute sortie de modèle d'IA, commençant à 10<sup>27</sup> FLOP[^117] et ajustable selon l'approprié.
- Les juridictions hébergeant des systèmes d'IA devraient imposer une limite dure sur le taux de calcul des sorties de modèles d'IA, commençant à 10<sup>20</sup> FLOP/s et ajustable selon l'approprié.

Justification : Le calcul total, bien que très imparfait, est un proxy pour la capacité d'IA (et le risque) qui est concrètement mesurable et vérifiable, donc fournit un arrêt dur pour limiter les capacités. Une proposition d'implémentation concrète est donnée en Annexe B.

#### 3\. Responsabilité renforcée pour les systèmes dangereux

- La création et l'opération[^118] d'un système d'IA avancé qui est hautement général, capable et autonome, devrait être clarifiée légalement via la législation pour être sujette à une responsabilité stricte, solidaire, plutôt qu'une responsabilité basée sur la faute d'une seule partie.[^119]
- Un processus légal devrait être disponible pour faire des cas de sécurité affirmatifs, qui accorderaient un régime d'exonération de la responsabilité stricte pour les systèmes qui sont petits (en termes de calcul), faibles, étroits, passifs, ou qui ont des garanties suffisantes de sécurité, sûreté et contrôlabilité.
- Une voie explicite et un ensemble de conditions pour un recours injonctif pour arrêter les activités d'entraînement et d'inférence d'IA qui constituent un danger public devraient être délimitées.

Justification : Les systèmes d'IA ne peuvent pas être tenus responsables, donc nous devons tenir les individus et organisations humains responsables du mal qu'ils causent (responsabilité).[^120] L'IAG incontrôlable est une menace pour la société et la civilisation et en l'absence d'un cas de sécurité devrait être considérée anormalement dangereuse. Mettre le fardeau de la responsabilité sur les développeurs pour montrer que les modèles puissants sont assez sûrs pour ne pas être considérés « anormalement dangereux » incite au développement sûr, ainsi qu'à la transparence et à la tenue de registres pour revendiquer ces régimes d'exonération. La réglementation peut alors empêcher le mal là où la dissuasion de la responsabilité est insuffisante. Finalement, les développeurs d'IA sont déjà responsables des dommages qu'ils causent, donc clarifier légalement la responsabilité pour les systèmes les plus risqués peut être fait immédiatement, sans que des standards hautement détaillés soient développés ; ceux-ci peuvent alors se développer avec le temps. Des détails sont donnés en Annexe C.

#### 4\. Réglementation de sécurité pour l'IA

Un système réglementaire qui aborde les risques aigus à grande échelle de l'IA nécessitera au minimum :

- L'identification ou la création d'un ensemble approprié de corps réglementaires, probablement une nouvelle agence ;
- Un cadre complet d'évaluation des risques ;[^121]
- Un cadre pour les cas de sécurité affirmatifs, basé en partie sur le cadre d'évaluation des risques, à faire par les développeurs, et pour l'audit par des groupes et agences *indépendants* ;
- Un système de licences échelonné, avec des échelons suivant les niveaux de capacité.[^122] Les licences seraient accordées sur la base de cas de sécurité et d'audits, pour le développement et le déploiement de systèmes. Les exigences iraient de la notification à l'extrémité basse, à des garanties quantitatives de sécurité, sûreté et contrôlabilité avant le développement, à l'extrémité haute. Celles-ci empêcheraient la sortie de systèmes jusqu'à ce qu'ils soient démontrés sûrs, et interdiraient le développement de systèmes intrinsèquement dangereux. L'Annexe D fournit une proposition sur ce que de tels standards de sécurité et sûreté pourraient impliquer.
- Des accords pour amener de telles mesures au niveau international, incluant des corps internationaux pour harmoniser les normes et standards, et potentiellement des agences internationales pour réviser les cas de sécurité.

Justification : Ultimement, la responsabilité n'est pas le bon mécanisme pour empêcher le risque à grande échelle au public d'une nouvelle technologie. Une réglementation compréhensive, avec des corps réglementaires habilités, sera nécessaire pour l'IA tout comme pour chaque autre industrie majeure posant un risque au public.[^123]

La réglementation vers la prévention d'autres risques pervasifs mais moins aigus variera probablement dans sa forme d'une juridiction à l'autre. La chose cruciale est d'éviter de développer les systèmes d'IA qui sont si risqués que ces risques sont ingérables.

### Et ensuite ?

Au cours de la prochaine décennie, alors que l'IA devient plus pervasive et que la technologie de base avance, deux choses clés sont susceptibles de se produire. Premièrement, la réglementation des systèmes d'IA puissants existants deviendra plus difficile, mais encore plus nécessaire. Il est probable qu'au moins certaines mesures abordant les risques de sécurité à grande échelle nécessiteront un accord au niveau international, avec des juridictions individuelles appliquant des règles basées sur des accords internationaux.

Deuxièmement, les plafonds de calcul d'entraînement et d'opération deviendront plus difficiles à maintenir alors que le matériel devient moins cher et plus rentable ; ils peuvent aussi devenir moins pertinents (ou nécessiter d'être encore plus serrés) avec les avances en algorithmes et architectures.

Que contrôler l'IA deviendra plus difficile ne signifie pas que nous devrions abandonner ! Implémenter le plan décrit dans cet essai nous donnerait à la fois du temps précieux et un contrôle crucial sur le processus qui nous mettrait dans une position bien, bien meilleure pour éviter le risque existentiel de l'IA pour notre société, civilisation et espèce.

À plus long terme encore, il y aura des choix à faire quant à ce que nous permettons. Nous pouvons choisir de créer encore une forme d'IAG genuinement contrôlable, dans la mesure où cela s'avère possible. Ou nous pouvons décider que diriger le monde est mieux laissé aux machines, si nous pouvons nous convaincre qu'elles feront un meilleur travail, et nous traiteront bien. Mais ces devraient être des décisions prises avec une compréhension scientifique profonde de l'IA en main, et après une discussion globale inclusive significative, pas dans une course entre magnats de la technologie avec la plupart de l'humanité complètement non impliquée et inconsciente.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) Résumé de la gouvernance de l'A-I-G et de la superintelligence via la responsabilité et la réglementation. La responsabilité est la plus élevée, et la réglementation la plus forte, à la triple-intersection d'Autonomie, Généralité et Intelligence. Des régimes d'exonération de la responsabilité stricte et de la réglementation forte peuvent être obtenus via des cas de sécurité affirmatifs démontrant qu'un système est faible et/ou étroit et/ou passif. Des plafonds sur le Calcul Total d'Entraînement et le taux de Calcul d'Inférence, vérifiés et appliqués légalement et utilisant des mesures de sécurité matérielles et cryptographiques, soutiennent la sécurité en évitant l'IAG complète et interdisant efficacement la superintelligence.

[^87]: Très probablement, la diffusion de cette réalisation prendra soit un effort intense par des groupes d'éducation et de plaidoyer faisant ce cas, soit un désastre assez significant causé par l'IA. Nous pouvons espérer que ce sera le premier.

[^88]: Paradoxalement, nous sommes habitués à ce que la Nature limite notre technologie en la rendant très difficile à développer, en particulier scientifiquement. Mais ce n'est plus le cas pour l'IA : les problèmes scientifiques clés s'avèrent être plus faciles qu'anticipé. Nous ne pouvons pas compter sur la Nature pour nous sauver de nous-mêmes ici – nous devrons le faire.

[^89]: Où, exactement, nous arrêtons-nous dans le développement de nouveaux systèmes ? Ici, nous devrions adopter un principe de précaution. Une fois qu'un système est déployé, et en particulier une fois que ce niveau de capacité de système prolifère, il est extrêmement difficile de faire marche arrière. Et si un système est *développé* (en particulier à grand coût et effort), il y aura une pression énorme pour l'utiliser ou le déployer, et la tentation qu'il soit divulgué ou volé. Développer des systèmes et *puis* décider s'ils sont profondément dangereux est une route dangereuse.

[^90]: Il serait aussi sage d'interdire le développement d'IA qui est intrinsèquement dangereuse, comme les systèmes auto-réplicants et évolutifs, ceux conçus pour échapper à l'enfermement, ceux qui peuvent s'auto-améliorer de manière autonome, l'IA délibérément trompeuse et malicieuse, etc.

[^91]: Notez que cela ne signifie pas nécessairement *appliqué* au niveau international par une sorte de corps global : au lieu de cela les nations souveraines pourraient appliquer des règles convenues, comme dans de nombreux traités.

[^92]: Comme nous le verrons ci-dessous, la nature du calcul d'IA permettrait quelque chose d'un hybride ; mais la coopération internationale sera encore nécessaire.

[^93]: Par exemple, les machines requises pour graver les puces pertinentes pour l'IA sont fabriquées par une seule firme, ASML (malgré de nombreuses autres tentatives de le faire), la grande majorité des puces pertinentes sont manufacturées par une firme, TSMC (malgré d'autres tentant de concourir), et la conception et construction de matériel à partir de ces puces faite par seulement quelques-unes incluant NVIDIA, AMD, et Google.

[^94]: Plus important, chaque puce détient une clé privée cryptographique unique et inaccessible qu'elle peut utiliser pour « signer » des choses.

[^95]: Par défaut ce serait l'entreprise vendant les puces, mais d'autres modèles sont possibles et potentiellement utiles.

[^96]: Un gouverneur peut déterminer l'emplacement d'une puce en chronométrant l'échange de messages signés avec elle : la vitesse finie de la lumière nécessite que la puce soit dans un rayon donné *r* d'une « station » si elle peut retourner un message signé dans un temps inférieur à *r* / *c*, où *c* est la vitesse de la lumière. En utilisant plusieurs stations, et une certaine compréhension des caractéristiques du réseau, l'emplacement de la puce peut être déterminé. La beauté de cette méthode est que la plupart de sa sécurité est fournie par les lois de la physique. D'autres méthodes pourraient utiliser GPS, suivi inertiel, et des technologies similaires.

[^97]: Alternativement, des paires de puces pourraient être autorisées à communiquer l'une avec l'autre seulement via permission explicite d'un gouverneur.

[^98]: Ceci est crucial car au moins actuellement, une connexion à très haute largeur de bande entre puces est nécessaire pour entraîner de grands modèles d'IA sur elles.

[^99]: Ceci pourrait aussi être configuré pour nécessiter des messages signés de *N* des *M* différents gouverneurs, permettant à plusieurs parties de partager la gouvernance.

[^100]: Ceci est loin d'être sans précédent – par exemple les militaires n'ont pas développé d'armées de super-soldats clonés ou génétiquement modifiés, bien que cela soit probablement technologiquement possible. Mais ils ont *choisi* de ne pas faire cela, plutôt que d'être empêchés par d'autres. Le bilan n'est pas génial pour que des puissances mondiales majeures soient empêchées de développer une technologie qu'elles souhaitent fortement développer.

[^101]: Avec quelques exceptions notables (en particulier NVIDIA) le matériel spécialisé en IA est une partie relativement petite du modèle d'affaires global et de revenus de ces entreprises. De plus, l'écart entre le matériel utilisé dans l'IA avancée et le matériel « grand public » est significatif, donc la plupart des consommateurs de matériel informatique seraient largement non affectés.

[^102]: Pour une analyse plus détaillée, voir les rapports récents de [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) et [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). Ceux-ci se concentrent sur la faisabilité technique, en particulier dans le contexte des contrôles d'exportation américains cherchant à contraindre la capacité d'autres pays en calcul de haut niveau ; mais cela a un chevauchement évident avec la contrainte globale envisagée ici.

[^103]: Les appareils Apple, par exemple, sont verrouillés à distance et sécurisés quand rapportés perdus ou volés, et peuvent être réactivés à distance. Cela repose sur les mêmes fonctionnalités de sécurité matérielle discutées ici.

[^104]: Voir par exemple l'offre [capacity on demand](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) d'IBM, [Intel on demand](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html) d'Intel, et [private cloud compute](https://security.apple.com/blog/private-cloud-compute/) d'Apple.

[^105]: [Cette étude](https://epochai.org/trends#hardware-trends-section) montre qu'historiquement la même performance a été atteinte en utilisant environ 30% de moins de dollars par année. Si cette tendance continue, il peut y avoir un chevauchement significatif entre l'usage de puces d'IA et « grand public », et en général la quantité de matériel nécessaire pour les systèmes d'IA de haute puissance pourrait devenir inconfortablement petite.

[^106]: Selon la [même étude](https://epochai.org/trends#hardware-trends-section), une performance donnée sur la reconnaissance d'images a nécessité 2,5x moins de calcul chaque année. Si cela devait aussi tenir pour les systèmes d'IA les plus capables aussi bien, une limite de calcul ne serait pas utile très longtemps.

[^107]: En particulier, au niveau du pays cela ressemble beaucoup à une nationalisation du calcul, dans le sens où le gouvernement aurait beaucoup de contrôle sur la façon dont la puissance computationnelle est utilisée. Cependant, pour ceux inquiets de l'implication gouvernementale, cela semble bien plus sûr que et préférable au logiciel d'IA le plus puissant *lui-même* étant nationalisé via une fusion entre les entreprises d'IA majeures et les gouvernements nationaux, comme certains commencent à plaider pour.

[^108]: Une étape réglementaire majeure en Europe a été prise avec le passage en 2024 de la [Loi sur l'IA de l'UE](https://artificialintelligenceact.eu/). Elle classifie l'IA par risque : interdisant les systèmes inacceptables, réglementant ceux à haut risque, et imposant des règles de transparence, ou aucune mesure du tout, sur les systèmes à faible risque. Elle réduira significativement certains risques d'IA, et stimulera la transparence de l'IA même pour les firmes américaines, mais a deux défauts clés. Premièrement, portée limitée : bien qu'elle s'applique à toute entreprise fournissant de l'IA dans l'UE, l'application sur les firmes basées aux États-Unis est faible, et l'IA militaire est exemptée. Deuxièmement, bien qu'elle couvre l'IAGP, elle échoue à reconnaître l'IAG ou la superintelligence comme des risques inacceptables ou à empêcher leur développement—seulement leur déploiement dans l'UE. En résultat, elle fait peu pour freiner les risques de l'IAG ou de la superintelligence.

[^109]: Les entreprises représentent souvent qu'elles sont en faveur d'une réglementation raisonnable. Mais d'une façon ou d'une autre elles semblent presque toujours s'opposer à toute réglementation *particulière* ; témoin la lutte sur la SB1047 assez peu intrusive, à laquelle [la plupart des entreprises d'IA se sont opposées publiquement ou privément](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/).

[^110]: Il a fallu environ 3 ans et demi depuis le moment où la loi sur l'IA de l'UE a été proposée jusqu'à ce qu'elle entre en vigueur.

[^111]: Il est parfois exprimé qu'il est « trop tôt » pour commencer à réglementer l'IA. Étant donné la note précédente, cela ne semble guère probable. Une autre préoccupation exprimée est que la réglementation « nuirait à l'innovation ». Mais une bonne réglementation change seulement la direction, pas la quantité, d'innovation.

[^112]: Un précédent intéressant est dans le transport de matériaux dangereux, qui pourraient s'échapper et causer des dommages. Ici, la [réglementation](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) et la [jurisprudence](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) ont établi une responsabilité stricte pour des matériaux très dangereux comme les explosifs, l'essence, les poisons, les agents infectieux, et les déchets radioactifs. D'autres exemples incluent les [avertissements sur les médicaments](https://www.medicalnewstoday.com/articles/boxed-warnings), les [classes d'appareils médicaux](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification), etc.

[^113]: Une autre proposition complète avec des buts similaires mise en avant dans [« A Narrow Path »](https://www.narrowpath.co/) plaide pour une approche plus centralisée, basée sur l'interdiction qui canalise tout développement d'IA de pointe à travers une seule entité internationale, supervisée par de fortes institutions internationales, avec des interdictions catégoriques claires plutôt que des restrictions graduées. J'endosserais aussi ce plan ; cependant il prendra encore plus de volonté politique et de coordination que celui proposé ici.

[^114]: Quelques lignes directrices pour un tel standard ont été [publiées](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/) par le Frontier Model Forum. Relativement à la proposition ici, celles-ci pèchent du côté de moins de précision et moins de calcul inclus dans le décompte.

[^115]: Le décret exécutif américain sur l'IA de 2023 (maintenant rescindé) nécessitait un rapport similaire mais moins fin. Ceci devrait être renforcé par un ordre de remplacement.

[^116]: Très approximativement, pour les puces H100 maintenant communes cela correspond à des grappes d'environ 1000 faisant de l'inférence ; c'est environ 100 (environ 5M USD de valeur) des toutes nouvelles puces NVIDIA B200 de pointe faisant de l'inférence. Dans les deux cas le nombre d'entraînement correspond à cette grappe calculant pendant plusieurs mois.

[^117]: Cette quantité est plus grande que tout système d'IA actuellement entraîné ; un nombre plus grand ou plus petit pourrait être justifié alors que nous comprenons mieux comment la capacité d'IA s'échelonne avec le calcul.

[^118]: Ceci s'applique à ceux créant et fournissant/hébergeant les modèles, pas aux utilisateurs finaux.

[^119]: Grossièrement, la responsabilité « stricte » signifie que les développeurs sont tenus responsables des dommages faits par un produit *par défaut* et est un standard utilisé pour les produits « anormalement dangereux », et (quelque peu amusant mais approprié) les animaux sauvages. La responsabilité « solidaire » signifie que la responsabilité est assignée à toutes les parties responsables d'un produit, et ces parties doivent régler entre elles qui porte quelle responsabilité. Ceci est important pour les systèmes comme l'IA avec une chaîne de valeur longue et complexe.

[^120]: La responsabilité standard basée sur la faute d'une seule partie n'est pas suffisante : la faute sera à la fois difficile à tracer et assigner parce que les systèmes d'IA sont complexes, leur opération n'est pas comprise, et de nombreuses parties peuvent être impliquées dans la création d'un système ou d'une sortie dangereuse. De plus, les poursuites prendront des années à adjuger et résulteront probablement simplement en amendes qui sont sans conséquence pour ces entreprises, donc la responsabilité personnelle pour les cadres est importante aussi bien.

[^121]: Il ne devrait y avoir aucune exemption des critères de sécurité pour les modèles à poids ouverts. De plus, en évaluant le risque il devrait être assumé que les garde-fous qui peuvent être enlevés le seront des modèles largement disponibles, et que même les modèles fermés proliféreront à moins qu'il y ait une assurance très élevée qu'ils resteront sécurisés.

[^122]: Le schéma proposé ici a un examen réglementaire déclenché sur la capacité générale ; cependant il est sensé que certains cas d'usage spécialement risqués déclenchent plus d'examen – par exemple un système d'IA de virologie expert, même s'il est étroit et passif, devrait probablement aller dans un échelon plus élevé. L'ancien décret exécutif américain avait une partie de cette structure pour les capacités biologiques.

[^123]: Deux exemples clairs sont l'aviation et les médicaments, réglementés par la FAA et la FDA, et des agences similaires dans d'autres pays. Ces agences sont imparfaites, mais ont été absolument vitales pour le fonctionnement et le succès de ces industries.

## Chapitre 9 - Façonner l'avenir — ce que nous devrions faire à la place

L'IA peut accomplir des merveilles dans le monde. Pour obtenir tous les bénéfices sans les risques, nous devons nous assurer que l'IA reste un outil humain.

Si nous choisissons avec succès de ne pas remplacer l'humanité par des machines – du moins pour un temps ! – que pouvons-nous faire à la place ? Devons-nous renoncer à l'immense promesse de l'IA en tant que technologie ? À un certain niveau, la réponse est un simple *non* : fermons les Portes à l'IAG incontrôlable et à la superintelligence, mais *construisons* bien d'autres formes d'IA, ainsi que les structures de gouvernance et les institutions nécessaires pour les gérer.

Mais il y a encore beaucoup à dire ; réaliser cela constituerait une préoccupation centrale de l'humanité. Cette section explore plusieurs thèmes clés :

- Comment nous pouvons caractériser l'IA « outil » et les formes qu'elle peut prendre.
- Que nous pouvons obtenir (presque) tout ce que l'humanité désire sans IAG, avec l'IA-outil.
- Que les systèmes d'IA-outil sont (probablement, en principe) gérables.
- Que s'éloigner de l'IAG ne signifie pas compromettre la sécurité nationale – bien au contraire.
- Que la concentration du pouvoir est une préoccupation réelle. Pouvons-nous l'atténuer sans compromettre la sûreté et la sécurité ?
- Que nous voudrons – et aurons besoin – de nouvelles structures de gouvernance et sociales, et l'IA peut effectivement nous aider.

### L'IA à l'intérieur des Portes : l'IA-outil

Le diagramme de triple intersection offre un bon moyen de délimiter ce que nous pouvons appeler « l'IA-outil » : une IA qui est un outil contrôlable pour l'usage humain, plutôt qu'un rival ou remplaçant incontrôlable. Les systèmes d'IA les moins problématiques sont ceux qui sont autonomes mais ni généraux ni super-capables (comme un robot d'enchères automatiques), ou généraux mais ni autonomes ni capables (comme un petit modèle de langage), ou capables mais étroits et très contrôlables (comme AlphaGo).[^124] Ceux ayant deux caractéristiques qui se recoupent ont une application plus large mais un risque plus élevé et nécessiteront des efforts majeurs de gestion. (Ce n'est pas parce qu'un système d'IA est davantage un outil qu'il est intrinsèquement sûr, seulement qu'il n'est pas intrinsèquement *dangereux* – considérez une tronçonneuse, par rapport à un tigre domestique.) La Porte doit rester fermée à l'IAG (complète) et à la superintelligence à la triple intersection, et une précaution énorme doit être prise avec les systèmes d'IA approchant ce seuil.

Mais cela laisse place à beaucoup d'IA puissante ! Nous pouvons tirer une utilité énorme d'« oracles » passifs intelligents et généraux ainsi que de systèmes étroits, de systèmes généraux au niveau humain mais pas surhumain, et ainsi de suite. De nombreuses entreprises technologiques et développeurs construisent activement ce genre d'outils et devraient continuer ; comme la plupart des gens, ils *supposent* implicitement que les Portes vers l'IAG et la superintelligence resteront fermées.[^125]

De plus, les systèmes d'IA peuvent être efficacement combinés en systèmes composites qui maintiennent la supervision humaine tout en améliorant les capacités. Plutôt que de nous appuyer sur des boîtes noires inscrutables, nous pouvons construire des systèmes où plusieurs composants – incluant à la fois l'IA et les logiciels traditionnels – travaillent ensemble de manière que les humains puissent surveiller et comprendre.[^126] Bien que certains composants puissent être des boîtes noires, aucun ne serait proche de l'IAG – seul le système composite dans son ensemble serait à la fois hautement général et hautement capable, et de manière strictement contrôlable.[^127]

#### Contrôle humain significatif et garanti

Que signifie « strictement contrôlable » ? Une idée clé du cadre « Outil » est de permettre des systèmes – même s'ils sont assez généraux et puissants – qui sont garantis d'être sous contrôle humain significatif. Que signifie ceci ? Cela implique deux aspects. Premièrement, une considération de conception : les humains devraient être profondément et centralement impliqués dans ce que fait le système, *sans* déléguer les décisions importantes clés à l'IA. C'est le caractère de la plupart des systèmes d'IA actuels. Deuxièmement, dans la mesure où les systèmes d'IA sont autonomes, ils doivent avoir des garanties qui limitent leur portée d'action. Une garantie devrait être un *nombre* caractérisant la probabilité que quelque chose se produise, et une raison de croire en ce nombre. C'est ce que nous exigeons dans d'autres domaines critiques pour la sécurité, où des nombres comme les « temps moyens entre défaillances » et les nombres attendus d'accidents sont calculés, soutenus, et publiés dans des dossiers de sécurité.[^128] Le nombre idéal pour les défaillances est zéro, bien sûr. Et la bonne nouvelle est que nous pourrions nous en approcher assez, bien qu'en utilisant des architectures d'IA assez différentes, en utilisant des idées de propriétés *formellement vérifiées* de programmes (y compris l'IA). L'idée, explorée en détail par Omohundro, Tegmark, Bengio, Dalrymple, et d'autres (voir [ici](https://arxiv.org/abs/2309.01933) et [ici](https://arxiv.org/abs/2405.06624)) est de construire un programme avec certaines propriétés (par exemple : qu'un humain puisse l'arrêter) et de *prouver* formellement que ces propriétés se maintiennent. Cela peut être fait maintenant pour des programmes assez courts et des propriétés simples, mais la puissance (à venir) des logiciels de preuve assistés par IA pourrait le permettre pour des programmes beaucoup plus complexes (par exemple des wrappers) et même l'IA elle-même. C'est un programme très ambitieux, mais à mesure que la pression s'accroît sur les Portes, nous allons avoir besoin de matériaux puissants pour les renforcer. La preuve mathématique pourrait être l'un des rares assez solides.

#### Que devient l'industrie de l'IA

Avec le progrès de l'IA redirigé, l'IA-outil resterait encore une industrie énorme. En termes de matériel, même avec des plafonds de calcul pour prévenir la superintelligence, l'entraînement et l'inférence dans des modèles plus petits nécessiteront encore d'énormes quantités de composants spécialisés. Du côté logiciel, désamorcer l'explosion dans la taille des modèles et de la puissance de calcul d'IA devrait simplement amener les entreprises à rediriger les ressources vers l'amélioration des systèmes plus petits, les rendant meilleurs, plus diversifiés, et plus spécialisés, plutôt que simplement plus gros.[^129] Il y aurait amplement de place – plus probablement – pour toutes ces startups lucratives de la Silicon Valley.[^130]

### L'IA-outil peut produire (presque) tout ce que l'humanité désire, sans IAG

L'intelligence, qu'elle soit biologique ou artificielle, peut être largement considérée comme la capacité de planifier et d'exécuter des activités amenant des futurs plus en ligne avec un ensemble d'objectifs. En tant que telle, l'intelligence est d'un bénéfice énorme quand elle est utilisée à la poursuite d'objectifs sagement choisis. L'intelligence artificielle attire d'énormes investissements de temps et d'efforts largement à cause de ses bénéfices promis. Nous devrions donc nous demander : dans quelle mesure récolterions-nous encore les bénéfices de l'IA si nous contenons sa fuite vers la superintelligence ? La réponse : nous pourrions perdre étonnamment peu.

Considérons d'abord que les systèmes d'IA actuels sont déjà très puissants, et nous n'avons vraiment fait qu'effleurer la surface de ce qui peut être fait avec eux.[^131] Ils sont raisonnablement capables de « mener la danse » en termes de « compréhension » d'une question ou tâche qui leur est présentée, et ce qu'il faudrait pour répondre à cette question ou faire cette tâche.

Ensuite, une grande part de l'enthousiasme concernant les systèmes d'IA modernes est due à leur généralité ; mais certains des systèmes d'IA les plus capables – comme ceux qui génèrent ou reconnaissent la parole ou les images, font de la prédiction et modélisation scientifique, jouent à des jeux, etc. – sont beaucoup plus étroits et bien « à l'intérieur des Portes » en termes de calcul.[^132] Ces systèmes sont surhumains aux tâches particulières qu'ils font. Ils peuvent avoir des faiblesses de cas limites[^133] (ou [exploitables](https://arxiv.org/abs/2211.00241)) dues à leur étroitesse ; cependant *totalement* étroit ou *pleinement* général ne sont pas les seules options disponibles : il y a de nombreuses architectures entre les deux.[^134]

Ces outils d'IA peuvent grandement accélérer l'avancement dans d'autres technologies positives, sans IAG. Pour mieux faire de la physique nucléaire, nous n'avons pas besoin que l'IA soit un physicien nucléaire – nous en avons ! Si nous voulons accélérer la médecine, donnons aux biologistes, chercheurs médicaux, et chimistes des outils puissants. Ils les veulent et les utiliseront à un bénéfice énorme. Nous n'avons pas besoin d'une ferme de serveurs pleine d'un million de génies numériques ; nous avons des millions d'humains dont l'IA peut aider à faire ressortir le génie. Oui, il faudra plus de temps pour obtenir l'immortalité et le remède à toutes les maladies. C'est un coût réel. Mais même les innovations sanitaires les plus prometteuses seraient de peu d'utilité si l'instabilité menée par l'IA conduit à un conflit global ou un effondrement sociétal. Nous nous devons de donner d'abord une chance aux humains assistés par l'IA de s'attaquer au problème.

Et supposons qu'il y ait, en fait, quelque énorme avantage à l'IAG qui ne puisse être obtenu par l'humanité utilisant des outils intra-Portes. Perdons-nous ceux-là en ne construisant *jamais* l'IAG et la superintelligence ? En pesant les risques et récompenses ici, il y a un bénéfice asymétrique énorme à attendre plutôt qu'à se précipiter : nous pouvons attendre jusqu'à ce que cela puisse être fait de manière garantie sûre et bénéfique, et presque tout le monde pourra encore récolter les récompenses ; si nous nous précipitons, cela pourrait être – dans les mots du PDG d'OpenAI Sam Altman – [extinction pour *tous* d'entre nous.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

Mais si les outils non-IAG sont potentiellement si puissants, pouvons-nous les gérer ? La réponse est un clair... peut-être.

### Les systèmes d'IA-outil sont (probablement, en principe) gérables

Mais ce ne sera pas facile. Les systèmes d'IA de pointe actuels peuvent grandement renforcer les personnes et institutions dans l'atteinte de leurs objectifs. C'est, en général, une bonne chose ! Cependant, il y a des dynamiques naturelles d'avoir de tels systèmes à notre disposition – soudainement et sans beaucoup de temps pour que la société s'adapte – qui offrent des risques sérieux qui doivent être gérés. Il vaut la peine de discuter quelques classes majeures de tels risques, et comment ils peuvent être diminués, en supposant une fermeture de Porte.

Une classe de risques est celle d'IA-outil de haute puissance permettant l'accès à la connaissance ou capacité qui avait précédemment été liée à une personne ou organisation, rendant une combinaison de haute capacité plus haute loyauté disponible à un éventail très large d'acteurs. Aujourd'hui, avec assez d'argent une personne de mauvaise intention pourrait embaucher une équipe de chimistes pour concevoir et produire de nouvelles armes chimiques – mais il n'est pas si facile d'avoir cet argent ou de trouver/assembler l'équipe et la convaincre de faire quelque chose d'assez clairement illégal, non éthique, et dangereux. Pour empêcher les systèmes d'IA de jouer un tel rôle, des améliorations sur les méthodes actuelles pourraient bien suffire,[^135] tant que tous ces systèmes et l'accès à eux sont gérés de manière responsable. D'autre part, si des systèmes puissants sont libérés pour usage et modification généraux, toute mesure de sécurité intégrée est probablement supprimable. Donc pour éviter les risques dans cette classe, de fortes restrictions quant à ce qui peut être publiquement libéré – analogues aux restrictions sur les détails des technologies nucléaires, explosives, et autres dangereuses – seront requises.[^136]

Une seconde classe de risques découle de la montée en échelle de machines qui agissent comme ou se font passer pour des personnes. Au niveau des dommages aux personnes individuelles, ces risques incluent des arnaques, spam, et phishing beaucoup plus efficaces, et la prolifération de deepfakes non consensuels.[^137] À un niveau collectif, ils incluent la perturbation de processus sociaux centraux comme la discussion et débat publics, nos systèmes sociétaux de collecte, traitement, et dissémination d'information et de connaissance, et nos systèmes de choix politiques. Atténuer ce risque impliquera probablement (a) des lois restreignant l'imitation de personnes par les systèmes d'IA, et tenant responsables les développeurs d'IA qui créent des systèmes qui génèrent de telles imitations, (b) des systèmes de filigrane et provenance qui identifient et classifient (de manière responsable) le contenu d'IA généré, et (c) de nouveaux systèmes épistémiques socio-techniques qui peuvent créer une chaîne de confiance des données (par exemple caméras et enregistrements) à travers les faits, compréhension, et bons modèles du monde.[^138] Tout ceci est possible, et l'IA peut aider avec certaines parties.

Un troisième risque général est que dans la mesure où certaines tâches sont automatisées, les humains faisant présentement ces tâches peuvent avoir moins de valeur financière en tant que travail. Historiquement, automatiser les tâches a rendu les choses rendues possibles par ces tâches moins chères et plus abondantes, tout en triant les personnes faisant précédemment ces tâches entre celles encore impliquées dans la version automatisée (généralement à compétence/salaire plus élevés), et celles dont le travail vaut moins ou peu. Au net, il est difficile de prédire dans quels secteurs plus versus moins de travail humain sera requis dans le secteur résultant plus large mais plus efficace. En parallèle, la dynamique d'automatisation tend à augmenter l'inégalité et la productivité générale, diminuer le coût de certains biens et services (via les augmentations d'efficacité), et augmenter le coût d'autres (via [la maladie des coûts](https://en.wikipedia.org/wiki/Baumol_effect)). Pour ceux du côté défavorisé de l'augmentation d'inégalité, il est profondément incertain si la diminution de coût dans ces certains biens et services l'emporte sur l'augmentation dans d'autres, et mène à un bien-être global plus grand. Alors comment cela ira-t-il pour l'IA ? À cause de la facilité relative avec laquelle le travail intellectuel humain peut être remplacé par l'IA générale, nous pouvons nous attendre à une version rapide de ceci avec l'IA généraliste compétitive humaine.[^139] Si nous fermons la Porte à l'IAG, beaucoup moins d'emplois seront remplacés en gros par des agents IA ; mais un déplacement massif de main-d'œuvre reste probable sur une période d'années.[^140] Pour éviter une souffrance économique généralisée, il sera probablement nécessaire d'implémenter à la fois une forme d'actifs de base universels ou revenu, et aussi d'ingénier un changement culturel vers la valorisation et récompense du travail centré humain qui est plus difficile à automatiser (plutôt que de voir les prix du travail chuter due à l'augmentation dans la main-d'œuvre disponible poussée hors d'autres parties de l'économie.) D'autres construits, tels que celui de [« dignité des données »](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society) (où les producteurs humains de données d'entraînement se voient automatiquement accordés des royalties pour la valeur créée par ces données dans l'IA) peuvent aider. L'automatisation par l'IA a aussi un second effet adverse potentiel, qui est celui d'automatisation *inappropriée*. Avec les applications où l'IA fait simplement un travail pire, cela inclurait celles où les systèmes d'IA sont susceptibles de violer des préceptes moraux, éthiques, ou légaux – par exemple dans les décisions de vie et de mort, et en matières judiciaires. Celles-ci doivent être traitées en appliquant et étendant nos cadres légaux actuels.

Finalement, une menace significative de l'IA intra-portes est son usage dans la persuasion personnalisée, capture d'attention, et manipulation. Nous avons vu dans les médias sociaux et autres plateformes en ligne la croissance d'une économie de l'attention profondément enracinée (où les services en ligne se battent férocement pour l'attention des utilisateurs) et des systèmes de [« capitalisme de surveillance »](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism) (dans lesquels l'information et profilage des utilisateurs est ajouté à la marchandisation de l'attention.) Il est presque certain que plus d'IA sera mise au service des deux. L'IA est déjà massivement utilisée dans les algorithmes de flux addictifs, mais ceci évoluera vers du contenu généré par IA addictif, personnalisé pour être consommé compulsivement par une seule personne. Et les entrées, réponses, et données de cette personne, seront alimentées dans la machine attention/publicité pour continuer le cercle vicieux. De même, à mesure que les assistants IA fournis par les entreprises technologiques deviennent l'interface pour plus de vie en ligne, ils remplaceront probablement les moteurs de recherche et flux comme mécanisme par lequel la persuasion et monétisation des clients se produit. L'échec de notre société à contrôler ces dynamiques jusqu'ici n'augure rien de bon. Une partie de cette dynamique peut être réduite via des régulations concernant la vie privée, droits des données, et manipulation. Arriver plus à la racine du problème peut nécessiter différentes perspectives, telles que celle d'assistants IA loyaux (discutée ci-dessous.)

Le résultat de cette discussion est celui d'espoir : les systèmes basés sur des outils intra-Portes – au moins tant qu'ils restent comparables en puissance et capacité aux systèmes de pointe d'aujourd'hui – sont probablement gérables s'il y a volonté et coordination pour le faire. Des institutions humaines décentes, renforcées par des outils d'IA,[^141] peuvent le faire. Nous pourrions aussi échouer à le faire. Mais il est difficile de voir comment permettre des systèmes plus puissants aiderait – autre qu'en les mettant aux commandes et espérer le mieux.

### Sécurité nationale

Les courses vers la suprématie IA – menées par la sécurité nationale ou d'autres motivations – nous poussent vers des systèmes d'IA puissants incontrôlés qui tendraient à absorber, plutôt qu'octroyer, le pouvoir. Une course IAG entre les États-Unis et la Chine est une course pour déterminer quelle nation obtient la superintelligence en premier.

Alors que devraient faire ceux responsables de la sécurité nationale à la place ? Les gouvernements ont une forte expérience dans la construction de systèmes contrôlables et sécurisés, et ils devraient redoubler d'efforts pour le faire dans l'IA, soutenant le genre de projets d'infrastructure qui réussissent le mieux quand fait à l'échelle et avec l'approbation gouvernementale.

Au lieu d'un « projet Manhattan » téméraire vers l'IAG,[^142] le gouvernement américain pourrait lancer un projet Apollo pour des systèmes contrôlables, sécurisés, fiables. Ceci pourrait inclure par exemple :

- Un programme majeur pour (a) développer les mécanismes de sécurité matérielle sur puce et (b) l'infrastructure, pour gérer le côté calcul de l'IA puissante. Ceux-ci pourraient s'appuyer sur l'[acte CHIPS](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) américain et le [régime de contrôle d'exportation](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion).
- Une initiative à grande échelle pour développer des techniques de vérification formelle afin que des caractéristiques particulières des systèmes d'IA (comme un interrupteur d'arrêt) puissent être *prouvées* présentes ou absentes. Ceci peut exploiter l'IA elle-même pour développer des preuves de propriétés.
- Un effort à l'échelle nationale pour créer des logiciels vérifiablement sécurisés, propulsé par des outils d'IA qui peuvent recoder les logiciels existants dans des cadres vérifiablement sécurisés.
- Un projet d'investissement national dans l'avancement scientifique utilisant l'IA,[^143] fonctionnant comme un partenariat entre le DOE, NSF, et NIH.

En général, il y a une énorme surface d'attaque sur notre société qui nous rend vulnérables aux risques de l'IA et de sa mauvaise utilisation. Se protéger de certains de ces risques nécessitera un investissement et une standardisation de taille gouvernementale. Ceux-ci fourniraient vastement plus de sécurité que jeter de l'essence sur le feu des courses vers l'IAG. Et si l'IA va être intégrée dans l'armement et les systèmes de commandement et contrôle, il est crucial que l'IA soit fiable et sécurisée, ce que l'IA actuelle n'est simplement pas.

### Concentration du pouvoir et ses atténuations

Cet essai s'est concentré sur l'idée du contrôle humain de l'IA et de son échec potentiel. Mais une autre perspective valide à travers laquelle voir la situation de l'IA est à travers la *concentration du pouvoir.* Le développement d'IA très puissante menace de concentrer le pouvoir soit dans les mains corporatives très peu nombreuses et très larges qui l'ont développée et la contrôleront, soit dans les gouvernements utilisant l'IA comme nouveau moyen de maintenir leur propre pouvoir et contrôle, soit dans les systèmes d'IA eux-mêmes. Ou quelque mélange impie de ce qui précède. Dans tous ces cas, la plupart de l'humanité perd pouvoir, contrôle, et agency. Comment pourrions-nous combattre cela ?

La toute première et plus importante étape, bien sûr, est une fermeture de Porte à l'IAG et superintelligence plus intelligente que l'humain. Celles-ci peuvent explicitement remplacer directement les humains et groupes d'humains. Si elles sont sous contrôle corporatif ou gouvernemental elles concentreront le pouvoir dans ces corporations ou gouvernements ; si elles sont « libres » elles concentreront le pouvoir en elles-mêmes. Supposons donc que les Portes sont fermées. Et alors ?

Une solution proposée à la concentration de pouvoir est l'IA « open-source », où les poids des modèles sont disponibles gratuitement ou largement. Mais comme mentionné précédemment, une fois qu'un modèle est ouvert, la plupart des mesures de sécurité ou garde-fous peuvent être (et sont généralement) supprimés. Il y a donc une tension aiguë entre d'une part la décentralisation, et d'autre part la sécurité, sûreté, et contrôle humain des systèmes d'IA. Il y a aussi des raisons d'être sceptique que les modèles ouverts combattront par eux-mêmes significativement la concentration de pouvoir dans l'IA plus qu'ils l'ont fait dans les systèmes d'exploitation (encore dominés par Microsoft, Apple, et Google malgré les alternatives ouvertes).[^144]

Pourtant il peut y avoir des moyens de résoudre ce cercle – de centraliser et atténuer les risques tout en décentralisant la capacité et récompense économique. Ceci nécessite de repenser à la fois comment l'IA est développée et comment ses bénéfices sont distribués.

De nouveaux modèles de développement et propriété d'IA publique aideraient. Ceci pourrait prendre plusieurs formes : IA développée par le gouvernement (sujette à supervision démocratique),[^145] organisations de développement d'IA à but non lucratif (comme Mozilla pour les navigateurs), ou structures permettant une propriété et gouvernance très largement répandues. La clé est que ces institutions seraient explicitement mandatées pour servir l'intérêt public tout en opérant sous de fortes contraintes de sécurité.[^146] Des régimes réglementaires et de standards/certifications bien conçus seront aussi vitaux, afin que les produits d'IA offerts par un marché vibrant restent genuinement utiles plutôt qu'exploitants envers leurs utilisateurs.

En termes de concentration du pouvoir économique, nous pouvons utiliser le suivi de provenance et la « dignité des données » pour assurer que les bénéfices économiques coulent plus largement. En particulier, la plupart du pouvoir d'IA maintenant (et dans le futur si nous gardons les Portes fermées) découle de données générées par l'humain, que ce soient des données d'entraînement directes ou des retours humains. Si les entreprises d'IA étaient requises de compenser équitablement les fournisseurs de données,[^147] ceci pourrait au moins aider à distribuer les récompenses économiques plus largement. Au-delà de ceci, un autre modèle pourrait être la propriété publique de fractions significatives de grandes entreprises d'IA. Par exemple, les gouvernements capables de taxer les entreprises d'IA pourraient investir une fraction des recettes dans un fonds souverain qui détient des actions dans les entreprises, et paie des dividendes à la population.[^148]

Crucial dans ces mécanismes est d'utiliser le pouvoir de l'IA elle-même pour aider à mieux distribuer le pouvoir, plutôt que de simplement combattre la concentration de pouvoir menée par l'IA en utilisant des moyens non-IA. Une approche puissante serait à travers des assistants IA bien conçus qui opèrent avec un véritable devoir fiduciaire envers leurs utilisateurs – mettant les intérêts des utilisateurs en premier, surtout au-dessus de ceux des fournisseurs corporatifs.[^149] Ces assistants doivent être vraiment dignes de confiance, techniquement compétents mais appropriément limités basé sur le cas d'usage et niveau de risque, et largement disponibles à tous à travers des canaux publics, à but non lucratif, ou à but lucratif certifiés. Tout comme nous n'accepterions jamais un assistant humain qui travaille secrètement contre nos intérêts pour une autre partie, nous ne devrions pas accepter des assistants IA qui surveillent, manipulent, ou extraient de la valeur de leurs utilisateurs pour le bénéfice corporatif.

Une telle transformation changerait fondamentalement la dynamique actuelle où les individus sont laissés à négocier seuls avec de vastes machines corporatives et bureaucratiques (propulsées par l'IA) qui priorisent l'extraction de valeur sur le bien-être humain. Bien qu'il y ait de nombreuses approches possibles pour redistribuer plus largement le pouvoir mené par l'IA, aucune n'émergera par défaut : elles doivent être délibérément conçues et gouvernées avec des mécanismes comme les exigences fiduciaires, provision publique, et accès à niveaux basé sur le risque.

Les approches pour atténuer la concentration de pouvoir peuvent faire face à des vents contraires significatifs des pouvoirs en place.[^150] Mais il y a des chemins vers le développement d'IA qui ne nécessitent pas de choisir entre sécurité et pouvoir concentré. En construisant les bonnes institutions maintenant, nous pourrions assurer que les bénéfices de l'IA soient largement partagés tandis que ses risques sont soigneusement gérés.

### Nouvelles structures de gouvernance et sociales

Nos structures de gouvernance actuelles peinent : elles sont lentes à répondre, souvent capturées par des intérêts spéciaux, et [de plus en plus méfiées par le public.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) Pourtant ce n'est pas une raison de les abandonner – tout à fait le contraire. Certaines institutions peuvent avoir besoin d'être remplacées, mais plus largement nous avons besoin de nouveaux mécanismes qui peuvent améliorer et supplémenter nos structures existantes, les aidant à mieux fonctionner dans notre monde en évolution rapide.

Beaucoup de notre faiblesse institutionnelle découle non des structures gouvernementales formelles, mais d'institutions sociales dégradées : nos systèmes pour développer une compréhension partagée, coordonner l'action, et conduire un discours significatif. Jusqu'ici, l'IA a accéléré cette dégradation, inondant nos canaux d'information avec du contenu généré, nous pointant vers le contenu le plus polarisant et diviseur, et rendant plus difficile de distinguer la vérité de la fiction.

Mais l'IA pourrait en fait aider à reconstruire et renforcer ces institutions sociales. Considérons trois domaines cruciaux :

Premièrement, l'IA pourrait aider à restaurer la confiance dans nos systèmes épistémiques – nos façons de savoir ce qui est vrai. Nous pourrions développer des systèmes propulsés par IA qui suivent et vérifient la provenance de l'information, des données brutes à travers l'analyse aux conclusions. Ces systèmes pourraient combiner la vérification cryptographique avec une analyse sophistiquée pour aider les gens à comprendre non seulement si quelque chose est vrai, mais comment nous savons que c'est vrai.[^151] Des assistants IA loyaux pourraient être chargés de suivre les détails pour s'assurer qu'ils se vérifient.

Deuxièmement, l'IA pourrait permettre de nouvelles formes de coordination à grande échelle. Beaucoup de nos problèmes les plus pressants – du changement climatique à la résistance aux antibiotiques – sont fondamentalement des problèmes de coordination. Nous sommes [coincés dans des situations qui sont pires qu'elles ne pourraient être pour presque tout le monde](https://equilibriabook.com/), parce qu'aucun individu ou groupe ne peut se permettre de faire le premier pas. Les systèmes d'IA pourraient aider en modélisant des structures d'incitations complexes, identifiant des chemins viables vers de meilleurs résultats, et facilitant les mécanismes de construction de confiance et d'engagement nécessaires pour y arriver.

Peut-être plus intriguant, l'IA pourrait permettre des formes entièrement nouvelles de discours social. Imaginez pouvoir « parler à une ville »[^152] – pas seulement voir des statistiques, mais avoir un dialogue significatif avec un système d'IA qui traite et synthétise les vues, expériences, besoins, et aspirations de millions de résidents. Ou considérez comment l'IA pourrait faciliter un dialogue genuine entre des groupes qui actuellement se parlent sans s'entendre, en aidant chaque côté à mieux comprendre les préoccupations et valeurs actuelles de l'autre plutôt que leurs caricatures l'un de l'autre.[^153] Ou l'IA pourrait offrir une intermédiation compétente et crédiblement neutre des disputes entre personnes ou même de grands groupes de personnes (qui pourraient tous interagir avec elle directement et individuellement !) L'IA actuelle est totalement capable de faire ce travail, mais les outils pour le faire ne viendront pas à l'existence par eux-mêmes, ou via des incitations de marché.

Ces possibilités pourraient sembler utopiques, surtout donné le rôle actuel de l'IA dans la dégradation du discours et de la confiance. Mais c'est précisément pourquoi nous devons activement développer ces applications positives. En fermant les Portes à l'IAG incontrôlable et priorisant l'IA qui améliore l'agency humaine, nous pouvons orienter le progrès technologique vers un futur où l'IA sert comme force pour l'autonomisation, la résilience, et l'avancement collectif.

[^124]: Cela dit, rester loin de la triple-intersection n'est malheureusement pas aussi facile qu'on pourrait l'aimer. Pousser très fort la capacité dans n'importe lequel des trois aspects tend à l'augmenter dans les autres. En particulier, il peut être difficile de créer une intelligence extrêmement générale et capable qui ne peut pas être facilement rendue autonome. Une approche est d'entraîner des modèles [« myopes »](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) avec une capacité de planification entravée. Une autre serait de se concentrer sur l'ingénierie de purs systèmes [« oracle »](https://arxiv.org/abs/1711.05541) qui s'écarteraient de répondre aux questions orientées action.

[^125]: Beaucoup d'entreprises ne réalisent pas qu'elles aussi seraient éventuellement déplacées par l'IAG, même si cela prend plus de temps – si elles le faisaient, elles pousseraient peut-être un peu moins sur ces Portes !

[^126]: Les systèmes d'IA pourraient communiquer de manières plus efficaces mais moins intelligibles, mais maintenir la compréhension humaine devrait prendre priorité.

[^127]: Cette idée d'IA modulaire, interprétable a été développée en détail par plusieurs chercheurs ; voir par exemple le modèle [« Comprehensive AI Services »](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) de Drexler, l'[« Open Agency Architecture »](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai) de Dalrymple et autres. Bien que de tels systèmes puissent nécessiter plus d'effort d'ingénierie que des réseaux de neurones monolithiques entraînés avec calcul massif, c'est précisément où les limites de calcul aident – en rendant le chemin plus sûr, plus transparent aussi le plus pratique.

[^128]: Sur les dossiers de sécurité en général voir [ce manuel](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). Concernant l'IA en particulier, voir [Wasil et al.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [Clymer et al.](https://arxiv.org/abs/2403.10462), [Buhl et al.](https://arxiv.org/abs/2410.21572), et [Balesni et al.](https://arxiv.org/abs/2411.03336)

[^129]: Nous voyons en fait déjà cette tendance menée juste par le coût élevé de l'inférence : des modèles plus petits et plus spécialisés « distillés » des plus grands et capables de fonctionner sur du matériel moins cher.

[^130]: Je comprends pourquoi ceux enthousiasmés par l'écosystème technologique de l'IA peuvent s'opposer à ce qu'ils voient comme une régulation onéreuse sur leur industrie. Mais c'est franchement déconcertant pour moi pourquoi, disons, un capital-risqueur voudrait permettre une fuite vers l'IAG et la superintelligence. Ces systèmes (et entreprises, tant qu'elles restent sous contrôle d'entreprise) *dévoreront toutes les startups comme collation*. Probablement même *plus tôt* que dévorer d'autres industries. Quiconque investi dans un écosystème d'IA prospère devrait prioriser assurer que le développement d'IAG ne mène pas à une monopolisation par quelques acteurs dominants.

[^131]: Comme l'économiste et ancien chercheur Deepmind Michael Webb [l'a dit](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/), « Je pense que si nous arrêtions tout développement de plus grands modèles de langage aujourd'hui, donc GPT-4 et Claude et peu importe, et ils sont les dernières choses que nous entraînons de cette taille – donc nous permettons beaucoup plus d'itération sur des choses de cette taille et toutes sortes de réglage fin, mais rien de plus grand que ça, pas d'avancements plus grands – juste ce que nous avons aujourd'hui je pense est assez pour propulser 20 ou 30 ans de croissance économique incroyable. »

[^132]: Par exemple, le système alphafold de DeepMind a utilisé seulement 100 000e du nombre de FLOP de GPT-4.

[^133]: La difficulté des voitures autonomes est importante à noter ici : bien que nominalement une tâche étroite, et réalisable avec fiabilité raisonnable avec des systèmes d'IA relativement petits, une connaissance et compréhension étendues du monde réel sont nécessaires pour obtenir la fiabilité au niveau nécessaire dans une tâche si critique pour la sécurité.

[^134]: Par exemple, donné un budget de calcul, nous verrions probablement des modèles GPAI pré-entraînés à (disons) la moitié de ce budget, et l'autre moitié utilisée pour entraîner une très haute capacité dans une gamme plus étroite de tâches. Ceci donnerait une capacité étroite surhumaine soutenue par une intelligence générale quasi-humaine.

[^135]: La technique d'alignement dominante actuelle est « apprentissage par renforcement par retour humain » [(RLHF)](https://arxiv.org/abs/1706.03741) et utilise le retour humain pour créer un signal de récompense/punition pour l'apprentissage par renforcement du modèle d'IA. Cette technique et les techniques reliées comme [IA constitutionnelle](https://arxiv.org/abs/2212.08073) marchent étonnamment bien (bien qu'elles manquent de robustesse et puissent être contournées avec un effort modeste.) De plus, les modèles de langage actuels sont généralement assez compétents au raisonnement de sens commun qu'ils ne feront pas d'erreurs morales stupides. C'est quelque chose d'un point optimal : assez intelligents pour comprendre ce que les gens veulent (dans la mesure où cela peut être défini), mais pas assez intelligents pour planifier des déceptions élaborées ou causer d'énormes dommages quand ils se trompent.

[^136]: À long terme, tout niveau de capacité d'IA qui est développé est susceptible de proliférer, puisqu'ultimement c'est du logiciel, et utile. Nous devrons avoir des mécanismes robustes pour nous défendre contre les risques que de tels systèmes posent. Mais nous *n'avons pas cela maintenant* donc nous devons être très mesurés dans combien de modèles d'IA puissants sont autorisés à proliférer.

[^137]: La vaste majorité de ceux-ci sont des deepfakes pornographiques non consensuels, incluant de mineurs.

[^138]: Beaucoup d'ingrédients pour de telles solutions existent, sous la forme de lois « bot-ou-pas » (dans l'acte d'IA de l'UE entre autres endroits), [technologies de suivi de provenance industrielles](https://c2pa.org/), [agrégateurs de nouvelles innovants](https://www.improvethenews.org/), [agrégateurs](https://metaculus.com/) de prédiction et marchés, etc.

[^139]: La vague d'automatisation peut ne pas suivre les patterns précédents, en ce que les tâches relativement de *haute* compétence telles que l'écriture de qualité, interpréter la loi, ou donner des conseils médicaux, peuvent être autant ou même plus vulnérables à l'automatisation que les tâches de compétence inférieure.

[^140]: Pour une modélisation soigneuse de l'effet de l'IAG sur les salaires, voir le rapport [ici](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek), et les détails sanglants [ici](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0), d'Anton Korinek et collaborateurs. Ils trouvent qu'à mesure que plus de pièces d'emplois sont automatisées, productivité et salaires montent – jusqu'à un point. Une fois que *trop* est automatisé, la productivité continue d'augmenter, mais les salaires s'effondrent parce que les gens sont remplacés en gros par l'IA efficace. C'est pourquoi fermer les Portes est si utile : nous obtenons la productivité sans les salaires humains évanouis.

[^141]: Il y a de nombreuses façons dont l'IA peut être utilisée comme, et pour aider à construire, des technologies « défensives » pour rendre les protections et gestion plus robustes. Voir [ce](https://vitalik.eth.limo/general/2025/01/05/dacc2.html) poste influent décrivant cet agenda « D/acc ».

[^142]: Quelque peu ironiquement, un projet Manhattan américain ferait probablement peu pour accélérer les calendriers vers l'IAG – le cadran d'investissement humain et fiscal dans le progrès de l'IA est déjà épinglé à 11. Les résultats primaires seraient d'inspirer un projet similaire en Chine (qui excelle aux projets d'infrastructure au niveau national), de rendre les accords internationaux limitant le risque de l'IA beaucoup plus difficiles, et d'alarmer d'autres adversaires géopolitiques des États-Unis tels que la Russie.

[^143]: Le programme [« National AI Research Resource »](https://nairrpilot.org/) est un bon pas actuel dans cette direction et devrait être étendu.

[^144]: Voir [cette analyse](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807) des diverses significations et implications d'« ouvert » dans les produits technologiques et comment certains ont mené à plus, plutôt que moins, d'enracinement de dominance.

[^145]: Les plans aux États-Unis pour une [National AI Research Resource](https://nairratdoe.ornl.gov/) et le lancement récent d'une [Fondation AI Européenne](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/) sont des pas intéressants dans cette direction.

[^146]: Le défi ici n'est pas technique mais institutionnel – nous avons un besoin urgent d'exemples et expériences du monde réel de ce à quoi le développement d'IA d'intérêt public pourrait ressembler.

[^147]: Ceci va à l'encontre des modèles d'affaires actuels des grandes technologies et nécessiterait à la fois action légale et nouvelles normes.

[^148]: Seuls certains gouvernements seront capables de le faire. Une idée plus radicale est [un fonds universel de ce type, sous propriété conjointe de tous les humains.](https://futureoflife.org/project/the-windfall-trust/)

[^149]: Pour une exposition longue de ce cas voir [ce papier](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338) sur la loyauté de l'IA. Malheureusement la trajectoire par défaut des assistants IA est probablement d'être de plus en plus déloyaux.

[^150]: Quelque peu ironiquement, beaucoup de pouvoirs en place sont aussi à risque de désempowerment soutenu par l'IA ; mais il peut être difficile pour eux de percevoir ceci jusqu'à moins que le processus n'aille assez loin.

[^151]: Quelques efforts intéressants dans cette direction sont représentés par [la coalition c2pa](https://c2pa.org/) sur la vérification cryptographique ; [Verity](https://www.improvethenews.org/) et [Ground news](https://ground.news/) sur de meilleures épistémologies de nouvelles ; et [Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com) et marchés de prédiction sur l'ancrage du discours dans des prédictions falsifiables.

[^152]: Voir [ce](https://talktothecity.org/) projet pilote fascinant.

[^153]: Voir [Kialo](https://www.kialo-edu.com/), et efforts du [Collective Intelligence Project](https://www.cip.org/) pour quelques exemples.

## Chapitre 10 - Le choix qui s'offre à nous

Pour préserver notre avenir humain, nous devons choisir de fermer les Portes à l'IAG et à la superintelligence.

La dernière fois que l'humanité a partagé la Terre avec d'autres esprits qui parlaient, pensaient, construisaient des technologies et résolvaient des problèmes de manière polyvalente, c'était il y a 40 000 ans dans l'Europe de l'ère glaciaire. Ces autres esprits se sont éteints, en partie ou entièrement à cause des efforts des nôtres.

Nous entrons à nouveau dans une telle époque. Les produits les plus avancés de notre culture et de notre technologie – des jeux de données construits à partir de l'ensemble de notre patrimoine informationnel d'internet, et des puces de 100 milliards d'éléments qui sont les technologies les plus complexes que nous ayons jamais créées – sont combinés pour donner naissance à des systèmes d'IA polyvalents avancés.

Les développeurs de ces systèmes sont soucieux de les présenter comme des outils d'autonomisation humaine. Et en effet, ils pourraient l'être. Mais ne nous y trompons pas : notre trajectoire actuelle consiste à construire des agents numériques de plus en plus puissants, orientés vers des objectifs, capables de prendre des décisions et généralement compétents. Ils performent déjà aussi bien que de nombreux humains dans un large éventail de tâches intellectuelles, s'améliorent rapidement, et contribuent à leur propre amélioration.

À moins que cette trajectoire ne change ou ne rencontre un obstacle imprévu, nous aurons bientôt – en années, pas en décennies – des intelligences numériques dangereusement puissantes. Même dans le *meilleur* des scénarios, celles-ci apporteraient de grands bénéfices économiques (du moins à certains d'entre nous) mais seulement au prix d'une profonde perturbation de notre société, et du remplacement des humains dans la plupart des choses les plus importantes que nous faisons : ces machines penseraient pour nous, planifieraient pour nous, décideraient pour nous, et créeraient pour nous. Nous serions gâtés, mais comme des enfants gâtés. Il est bien plus probable que ces systèmes remplacent les humains aussi bien dans les choses positives *que* négatives que nous faisons, notamment l'exploitation, la manipulation, la violence et la guerre. Pouvons-nous survivre à des versions suralimentées par l'IA de ces fléaux ? Enfin, il est plus que plausible que les choses ne se passent pas bien du tout : que relativement vite nous soyons remplacés non seulement dans ce que nous faisons, mais dans ce que nous *sommes*, en tant qu'architectes de la civilisation et de l'avenir. Demandez aux néandertaliens comment cela se passe. Peut-être leur avons-nous fourni des babioles supplémentaires pendant un moment également.

*Nous n'avons pas à faire cela.* Nous avons une IA compétitive par rapport aux humains, et il n'y a aucune nécessité de construire une IA avec laquelle nous *ne pouvons pas* rivaliser. Nous pouvons construire des outils d'IA extraordinaires sans construire une espèce successeur. L'idée que l'IAG et la superintelligence sont inévitables est un *choix qui se déguise en fatalité*.

En imposant certaines limites strictes et mondiales, nous pouvons maintenir la capacité générale de l'IA à approximativement le niveau humain tout en récoltant les bénéfices de la capacité des ordinateurs à traiter les données de manières que nous ne pouvons pas, et à automatiser des tâches qu'aucun de nous ne veut faire. Ceux-ci présenteraient encore de nombreux risques, mais s'ils sont bien conçus et bien gérés, seraient un énorme bienfait pour l'humanité, de la médecine à la recherche en passant par les produits de consommation.

Imposer des limites nécessiterait une coopération internationale, mais moins qu'on pourrait le penser, et ces limites laisseraient encore largement place à une énorme industrie de l'IA et du matériel IA axée sur des applications qui améliorent le bien-être humain, plutôt que sur la poursuite pure du pouvoir. Et si, avec de solides garanties de sécurité et après un dialogue mondial significatif, nous décidons d'aller plus loin, cette option continue d'être nôtre à poursuivre.

L'humanité doit *choisir* de fermer les Portes à l'IAG et à la superintelligence.

Pour garder l'avenir humain.

### Note de l'auteur

Merci d'avoir pris le temps d'explorer ce sujet avec nous.

J'ai écrit cet essai parce qu'en tant que scientifique, je pense qu'il est important de dire la vérité sans fard, et parce qu'en tant que personne, je pense qu'il est crucial pour nous d'agir rapidement et de manière décisive face à un enjeu qui change le monde : le développement de systèmes d'IA plus intelligents que les humains.

Si nous devons répondre à cet état de fait remarquable avec sagesse, nous devons être prêts à examiner de manière critique le narratif dominant selon lequel l'IAG et la superintelligence « doivent » être construites pour sécuriser nos intérêts, ou sont « inévitables » et ne peuvent être arrêtées. Ces narratifs nous privent de notre pouvoir, nous empêchent de voir les voies alternatives qui s'ouvrent devant nous.

J'espère que vous vous joindrez à moi pour appeler à la prudence face à l'imprudence, et au courage face à la cupidité.

J'espère que vous vous joindrez à moi pour appeler à un avenir humain.

*– Anthony*

![Anthony Aguirre signature](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAnthony-Aguirre-signature-300x84.png&w=3840&q=75)

## Annexes

Informations complémentaires, notamment : détails techniques sur la comptabilisation du calcul, exemple de mise en œuvre d'une « fermeture des portes », détails d'un régime de responsabilité stricte pour l'IAG, et approche par niveaux pour les normes de sécurité et de sûreté de l'IAG.

### Annexe A : Détails techniques de la comptabilisation du calcul

Une méthode détaillée pour établir à la fois la « vérité de terrain » ainsi que de bonnes approximations du calcul total utilisé pour l'entraînement et l'inférence est nécessaire pour des contrôles significatifs basés sur le calcul. Voici un exemple de la façon dont la « vérité de terrain » pourrait être comptabilisée au niveau technique.

**Définitions :**

*Graphe causal de calcul :* Pour une sortie O donnée d'un modèle d'IA, il existe un ensemble de calculs numériques pour lesquels modifier le résultat de ce calcul pourrait potentiellement changer O. (Ceci devrait être présumé de manière prudente, c'est-à-dire qu'il devrait y avoir une raison claire de croire qu'un calcul est indépendant d'un précurseur qui se produit à la fois plus tôt dans le temps et a un chemin causal physique potentiel d'effet.) Cela inclut les calculs effectués par le modèle d'IA pendant l'inférence, ainsi que les calculs qui ont contribué à l'entrée, la préparation des données et l'entraînement du modèle. Comme chacun de ceux-ci peut lui-même être la sortie d'un modèle d'IA, ceci est calculé récursivement, interrompu là où un humain a apporté un changement significatif à l'entrée.

*Calcul d'entraînement :* Le calcul total, en FLOP ou autres unités, impliqué par le graphe causal de calcul d'un réseau de neurones (incluant la préparation des données, l'entraînement et l'ajustement fin, et tous autres calculs.)

*Calcul de sortie :* Le calcul total dans le graphe causal de calcul d'une sortie d'IA donnée, incluant tous les réseaux de neurones (et incluant leur calcul d'entraînement) et autres calculs contribuant à cette sortie.

*Taux de calcul d'inférence :* Dans une série de sorties, le taux de changement (en FLOP/s ou autres unités) du calcul de sortie entre les sorties, c'est-à-dire le calcul utilisé pour produire la sortie suivante, divisé par l'intervalle de temps entre les sorties.

**Exemples et approximations :**

- Pour un réseau de neurones unique entraîné sur des données créées par des humains, le calcul d'entraînement est simplement le calcul d'entraînement total tel que rapporté habituellement.
- Pour un tel réseau de neurones effectuant l'inférence à un rythme régulier, le taux de calcul d'inférence est approximativement la vitesse totale du cluster de calcul effectuant l'inférence en FLOP/s.
- Pour l'ajustement fin de modèle, le calcul d'entraînement du modèle complet est donné par le calcul d'entraînement du modèle non ajusté plus le calcul effectué pendant l'ajustement fin et pour préparer toute donnée utilisée dans l'ajustement fin.
- Pour un modèle distillé, le calcul d'entraînement du modèle complet inclut l'entraînement à la fois du modèle distillé et du modèle plus large utilisé pour fournir des données synthétiques ou autre entrée d'entraînement.
- Si plusieurs modèles sont entraînés, mais que de nombreux « essais » sont écartés sur la base du jugement humain, ceux-ci ne comptent pas dans le calcul d'entraînement ou de sortie du modèle retenu.

### Annexe B : Exemple de mise en œuvre d'une fermeture des portes

**Exemple de mise en œuvre :** Voici un exemple de la façon dont une fermeture des portes pourrait fonctionner, avec une limite de 10<sup>27</sup> FLOP pour l'entraînement et 10<sup>20</sup> FLOP/s pour l'inférence (faire fonctionner l'IA) :

**1. Pause :** Pour des raisons de sécurité nationale, l'exécutif américain demande à toutes les entreprises basées aux États-Unis, faisant des affaires aux États-Unis, ou utilisant des puces fabriquées aux États-Unis, de cesser tout nouvel entraînement d'IA qui pourrait dépasser la limite de 10<sup>27</sup> FLOP de calcul d'entraînement. Les États-Unis devraient entamer des discussions avec d'autres pays hébergeant le développement d'IA, les encourageant fortement à prendre des mesures similaires et indiquant que la pause américaine pourrait être levée s'ils choisissent de ne pas se conformer.

**2. Supervision et licences américaines :** Par décret exécutif ou action d'une agence réglementaire existante, les États-Unis exigent que dans un délai d'(disons) un an :

- Tous les entraînements d'IA estimés au-dessus de 10<sup>25</sup> FLOP effectués par des entreprises opérant aux États-Unis soient enregistrés dans une base de données maintenue par une agence réglementaire américaine. (Note : Une version légèrement plus faible de ceci avait déjà été incluse dans le décret exécutif américain de 2023 sur l'IA, maintenant abrogé, exigeant l'enregistrement pour les modèles au-dessus de 10<sup>26</sup> FLOP.)
- Tous les fabricants de matériel pertinent pour l'IA opérant aux États-Unis ou faisant affaire avec le gouvernement américain adhèrent à un ensemble d'exigences sur leur matériel spécialisé et le logiciel qui le pilote. (Beaucoup de ces exigences pourraient être intégrées dans des mises à jour logicielles et de micrologiciel pour le matériel existant, mais des solutions à long terme et robustes nécessiteraient des changements aux générations ultérieures de matériel.) Parmi celles-ci figure une exigence que si le matériel fait partie d'un cluster interconnecté haute vitesse capable d'exécuter 10<sup>18</sup> FLOP/s de calcul, un niveau de vérification plus élevé est requis, qui inclut une permission régulière par un « gouverneur » distant qui reçoit à la fois la télémétrie et les demandes d'effectuer du calcul supplémentaire.
- Le dépositaire rapporte le calcul total effectué sur son matériel à l'agence maintenant la base de données américaine.
- Des exigences plus fortes sont progressivement mises en place pour permettre une supervision et un système de permissions à la fois plus sûrs et plus flexibles.

**3. Supervision internationale :**

- Les États-Unis, la Chine, et tout autre pays hébergeant une capacité avancée de fabrication de puces négocient un accord international.
- Cet accord crée une nouvelle agence internationale, analogue à l'Agence internationale de l'énergie atomique, chargée de superviser l'entraînement et l'exécution d'IA.
- Les pays signataires doivent exiger que leurs fabricants nationaux de matériel d'IA se conforment à un ensemble d'exigences au moins aussi strictes que celles imposées aux États-Unis.
- Les dépositaires sont maintenant tenus de rapporter les chiffres de calcul d'IA à la fois aux agences dans leurs pays d'origine ainsi qu'à un nouveau bureau au sein de l'agence internationale.
- Les pays supplémentaires sont fortement encouragés à rejoindre l'accord international existant : les contrôles à l'exportation par les pays signataires restreignent l'accès au matériel haut de gamme par les non-signataires tandis que les signataires peuvent recevoir un soutien technique dans la gestion de leurs systèmes d'IA.

**4. Vérification et application internationales :**

- Le système de vérification matérielle est mis à jour de sorte qu'il rapporte l'utilisation du calcul à la fois au dépositaire original et aussi directement au bureau de l'agence internationale.
- L'agence, via la discussion avec les signataires de l'accord international, s'accorde sur des limitations de calcul qui prennent alors force de loi dans les pays signataires.
- En parallèle, un ensemble de normes internationales peut être développé de sorte que l'entraînement et le fonctionnement d'IA au-dessus d'un seuil de calcul (mais en dessous de la limite) soient tenus d'adhérer à ces normes.
- L'agence peut, si nécessaire pour compenser de meilleurs algorithmes etc., abaisser la limite de calcul. Ou, si c'est jugé sûr et souhaitable (au niveau par exemple de garanties de sécurité prouvables), relever la limite de calcul.

### Annexe C : Détails d'un régime de responsabilité stricte pour l'IAG

**Détails d'un régime de responsabilité stricte pour l'IAG**

- La création et l'exploitation d'un système d'IA avancé qui est hautement général, capable et autonome, est considérée comme une activité « anormalement dangereuse ».
- En tant que telle, la responsabilité par défaut pour l'entraînement et l'exploitation de tels systèmes est une responsabilité objective, conjointe et solidaire (ou son équivalent hors États-Unis) pour tous dommages causés par le modèle ou ses sorties/actions.
- Une responsabilité personnelle sera imposée aux dirigeants et membres du conseil d'administration en cas de négligence grave ou de faute intentionnelle. Ceci devrait inclure des sanctions pénales pour les cas les plus flagrants.
- Il existe de nombreux régimes d'exonération sous lesquels la responsabilité revient à la responsabilité par défaut (basée sur la faute, aux États-Unis) à laquelle les personnes et entreprises seraient normalement soumises.
	- Modèles entraînés et exploités en dessous d'un certain seuil de calcul (qui serait au moins 10 fois inférieur aux plafonds décrits ci-dessus.)
	- IA qui est « faible » (grossièrement, en dessous du niveau d'expert humain aux tâches pour lesquelles elle est destinée) et/ou
	- IA qui est « étroite » (ayant une portée fixe et assez limitée de tâches et d'opérations pour lesquelles elle est spécifiquement conçue et entraînée) et/ou
	- IA qui est « passive » (très limitée dans sa capacité – même sous modification modeste – à entreprendre des actions ou effectuer des tâches complexes multi-étapes sans implication et contrôle humains directs.)
	- Une IA qui est garantie d'être sûre, sécurisée et contrôlable (prouvablement sûre, ou une analyse de risque indique un niveau négligeable de dommage attendu.)
- Les régimes d'exonération peuvent être revendiqués sur la base d'un [dossier de sécurité](https://arxiv.org/abs/2410.21572) préparé par le développeur d'IA et approuvé par une agence ou un auditeur accrédité par une agence. Pour revendiquer un régime d'exonération basé sur le calcul, le développeur doit simplement fournir des estimations crédibles du calcul d'entraînement total et du taux d'inférence maximal
- La législation définirait explicitement les situations dans lesquelles une réparation par injonction du développement de systèmes d'IA avec un risque élevé de préjudice public serait appropriée.
- Les consortiums d'entreprises, travaillant avec les ONG et agences gouvernementales, devraient développer des normes et standards définissant ces termes, comment les régulateurs devraient accorder les régimes d'exonération, comment les développeurs d'IA devraient développer les dossiers de sécurité, et comment les tribunaux devraient interpréter la responsabilité là où les régimes d'exonération ne sont pas proactivement revendiqués.

### Annexe D : Une approche par niveaux pour les normes de sécurité et de sûreté de l'IAG

**Une approche par niveaux pour les normes de sécurité et de sûreté de l'IAG**

| Niveau de risque | Déclencheur(s) | Exigences pour l'entraînement | Exigences pour le déploiement |
| --- | --- | --- | --- |
| NR-0 | IA faible en autonomie, généralité et intelligence | aucune | aucune |
| NR-1 | IA forte dans l'un des domaines : autonomie, généralité et intelligence | aucune | Basé sur le risque et l'utilisation, potentiellement dossiers de sécurité approuvés par les autorités nationales partout où le modèle peut être utilisé |
| NR-2 | IA forte dans deux des domaines : autonomie, généralité et intelligence | Enregistrement auprès de l'autorité nationale ayant juridiction sur le développeur | Dossier de sécurité bornant le risque de dommage majeur en dessous des niveaux autorisés plus audits de sécurité indépendants (incluant tests d'intrusion boîte noire et boîte blanche) approuvés par les autorités nationales partout où le modèle peut être utilisé |
| NR-3 | IAG forte en autonomie, généralité et intelligence | Pré-approbation du plan de sécurité et de sûreté par l'autorité nationale ayant juridiction sur le développeur | Dossier de sécurité garantissant un risque borné de dommage majeur en dessous des niveaux autorisés ainsi que des spécifications requises, incluant cybersécurité, contrôlabilité, un interrupteur d'urgence non-amovible, alignement avec les valeurs humaines, et robustesse à l'usage malveillant. |
| NR-4 | Tout modèle qui dépasse également soit 10<sup>27</sup> FLOP d'entraînement soit 10<sup>20</sup> FLOP/s d'inférence | Interdit en attendant la levée convenue internationalement du plafond de calcul | Interdit en attendant la levée convenue internationalement du plafond de calcul |

Classifications de risque et normes de sécurité/sûreté, avec niveaux basés sur les seuils de calcul ainsi que les combinaisons d'autonomie, généralité et intelligence élevées :

- *Autonomie forte* s'applique si le système est capable d'effectuer, ou peut facilement être amené à effectuer, des tâches multi-étapes et/ou entreprendre des actions complexes qui sont pertinentes pour le monde réel, sans supervision ou intervention humaine significative. Exemples : véhicules autonomes et robots ; bots de trading financier. Contre-exemples : GPT-4 ; classificateurs d'images
- *Généralité forte* indique une large portée d'application, performance de tâches pour lesquelles le modèle n'a pas été délibérément et spécifiquement entraîné, et capacité significative à apprendre de nouvelles tâches. Exemples : GPT-4 ; mu-zero. Contre-exemples : AlphaFold ; véhicules autonomes ; générateurs d'images
- *Intelligence forte* correspond à égaler la performance de niveau expert humain sur les tâches pour lesquelles le modèle performe le mieux (et pour un modèle général, à travers une large gamme de tâches.) Exemples : AlphaFold ; mu-zero ; o3. Contre-exemples : GPT-4 ; Siri

### Remerciements

Quelques remerciements aux personnes qui ont contribué à Keep The Future Human.

Ce travail reflète les opinions de l'auteur et ne doit pas être considéré comme la position officielle du Future of Life Institute (bien qu'elles soient compatibles ; pour sa position officielle, voir [cette page](https://futureoflife.org/our-position-on-ai/)), ni d'aucune autre organisation à laquelle l'auteur est affilié.

Je suis reconnaissant envers Mark Brakel, Ben Eisenpress, Anna Hehir, Carlos Gutierrez, Emilia Javorsky, Richard Mallah, Jordan Scharnhorst, Elyse Fulcher, Max Tegmark, et Jaan Tallinn pour leurs commentaires sur le manuscrit ; à Tim Schrier pour son aide avec certaines références ; à Taylor Jones et Elyse Fulcher pour l'amélioration esthétique des diagrammes.

Ce travail a fait un usage limité de modèles d'IA générative (Claude et ChatGPT) dans sa création, pour certaines corrections et tests contradictoires. Selon l'échelle bien établie des niveaux d'implication de l'IA dans les œuvres créatives, ce travail mériterait probablement une note de 3/10. (Il n'existe en fait aucune échelle de ce type ! Mais il devrait y en avoir une.)

Nous sommes très reconnaissants envers [Julius Odai](https://www.linkedin.com/in/julius-odai/) pour avoir produit cette version web de l'essai, qui rend la lecture et la navigation dans l'essai très agréables. Julius est un technologue et récent participant du cours AI Governance de BlueDot Impact.