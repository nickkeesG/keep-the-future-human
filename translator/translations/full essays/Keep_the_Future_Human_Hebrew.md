# שמרו על העתיד אנושי

מאמר זה מציג את הנימוקים מדוע וכיצד עלינו לסגור את השערים בפני בינה מלאכותית כללית (AGI) ועל-אינטליגנציה, ומה עלינו לבנות במקום זאת.

אם אתם מעוניינים רק בנקודות המרכזיות, עברו לסיכום הניהולי. לאחר מכן, פרקים 2-5 יספקו רקע על סוגי מערכות הבינה המלאכותית הנדונות במאמר. פרקים 5-7 מסבירים מדוע ניתן לצפות שה-AGI יגיע בקרוב, ומה עלול לקרות כאשר זה יקרה. לבסוף, פרקים 8-9 מתארים הצעה קונקרטית למניעת בניית AGI.

[הורדת PDF](https://keepthefuturehuman.ai/wp-content/uploads/2025/03/Keep_the_Future_Human__AnthonyAguirre__5March2025.pdf)

זמן קריאה כולל: 2-3 שעות

## סיכום מנהלים

סקירה כללית של המאמר. אם הזמן קצר לכם, קבלו את כל הנקודות העיקריות תוך 10 דקות בלבד.

ההתקדמות הדרמטית בבינה מלאכותית בעשור האחרון (עבור AI ייעודי) ובשנים האחרונות (עבור AI כללי) הפכה את הבינה המלאכותית מתחום אקדמי נישתי לאסטרטגיה העסקית המרכזית של רבות מהחברות הגדולות בעולם, עם השקעות שנתיות של מאות מיליארדי דולרים בטכניקות ובטכנולוgiות לפיתוח יכולות הבינה המלאכותית.

כעת אנו מגיעים לצומת קריטי. כשהיכולות של מערכות AI חדשות מתחילות להתאים ולעלות על אלו של בני אדם בתחומים קוגניטיביים רבים, האנושות חייבת להחליט: עד כמה רחוק אנחנו הולכים, ובאיזה כיוון?

בינה מלאכותית, כמו כל טכנולוגיה, החלה עם המטרה לשפר דברים עבור יוצרה. אבל המסלול הנוכחי שלנו, והבחירה המשתמעת, הוא מירוץ בלתי מבוקר לעבר מערכות חזקות יותר ויותר, המונע על ידי תמריצים כלכליים של כמה חברות טכנולוגיה ענקיות השואפות להפוך לאוטומטיות חלקים נרחבים מהפעילות הכלכלית הנוכחית ומעבודה אנושית. אם המירוץ הזה יימשך זמן רב יותר, יש מנצח בלתי נמנע: הבינה המלאכותית עצמה - חלופה מהירה יותר, חכמה יותר וזולה יותר לאנשים בכלכלה שלנו, בחשיבה שלנו, בהחלטות שלנו, ובסופו של דבר בשליטה על הציביליזציה שלנו.

אבל אנחנו יכולים לעשות בחירה אחרת: באמצעות הממשלות שלנו, אנחנו יכולים לקחת שליטה על תהליך פיתוח הבינה המלאכותית כדי להטיל גבולות ברורים, קווים שלא נחצה, ודברים שפשוט לא נעשה - כפי שעשינו עבור טכנולוגיות גרעיניות, נשק להשמדה המונית, נשק חלל, תהליכים הרסניים לסביבה, הנדסה ביולוגית של בני אדם ואאוגניקה. והכי חשוב, אנחנו יכולים להבטיח שבינה מלאכותית תישאר כלי להעצמת בני אדם, במקום מין חדש שמחליף ובסופו של דבר מדחיק אותנו.

מאמר זה טוען שעלינו *לשמור על העתיד אנושי* על ידי סגירת "השערים" לבינה מלאכותית חכמה מאדם, אוטונומית וכללית - הנקראת לעתים "AGI" - ובמיוחד לגרסה העל-אנושית הנקראת לעתים "על-אינטליגנציה". במקום זאת, עלינו להתמקד בכלי AI חזקים ומהימנים שיכולים להעצים אנשים ולשפר באופן טרנספורמטיבי את היכולות של החברות האנושיות לעשות את מה שהן עושות הכי טוב. מבנה הטיעון הזה נמשך בקצרה.

### בינה מלאכותית שונה

מערכות בינה מלאכותית שונות מהותית מטכנולוגיות אחרות. בעוד שתוכנה מסורתית עוקבת אחר הוראות מדויקות, מערכות AI לומדות איך להשיג מטרות מבלי שאומרים להן במפורש איך. זה עושה אותן חזקות: אם אנחנו יכולים להגדיר בבהירות את המטרה או מדד הצלחה, ברוב המקרים מערכת AI יכולה ללמוד להשיג אותה. אבל זה גם עושה אותן בלתי צפויות מטבען: אנחנו לא יכולים לקבוע באמינות אילו פעולות הן יבצעו כדי להשיג את היעדים שלהן.

הן גם בלתי ניתנות להסבר ברובן: למרות שהן חלקית קוד, הן בעיקר קבוצה עצומה של מספרים בלתי מובנים - "משקולות" רשת נוירונים - שאי אפשר לפענח; אנחנו לא הרבה יותר טובים בהבנת הפעילות הפנימית שלהן מאשר בזיהוי מחשבות על ידי הצצה לתוך מוח ביולוגי.

אופן האימון המרכזי הזה של רשתות נוירונים דיגיטליות גדל במהירות במורכבות. מערכות הבינה המלאכותית החזקות ביותר נוצרות באמצעות ניסויים חישוביים עצומים, תוך שימוש בחומרה מיוחדת לאימון רשתות נוירונים על מאגרי מידע ענקיים, שמוגברים אחר כך בכלי תוכנה ותשתית.

זה הוביל ליצירת כלים חזקים מאוד ליצירה ועיבוד של טקסט ותמונות, ביצוע חשיבה מתמטית ומדעית, צבירת מידע וחקירה אינטראקטיבית של מאגר עצום של ידע אנושי.

לצערנו, בעוד שפיתוח של כלים טכנולוגיים חזקים וראויי אמון יותר הוא מה שאנחנו *צריכים* לעשות, ומה שכמעט כולם רוצים ואומרים שהם רוצים, זה לא המסלול שאנחנו בעצם עליו.

### בינה מלאכותית כללית ועל-אינטליגנציה

מאז ראשית התחום, מחקר הבינה המלאכותית התמקד במקום זאת במטרה שונה: בינה מלאכותית כללית. המיקוד הזה הפך כעת למיקוד של החברות הטיטאניות המובילות בפיתוח AI.

מה זה AGI? לעתים קרובות זה מוגדר בצורה מעורפלת כ"AI ברמת אדם", אבל זה בעייתי: אילו בני אדם, ובאילו יכולות זה ברמת אדם? ומה לגבי היכולות העל-אנושיות שכבר יש לו? דרך שימושית יותר להבין AGI היא דרך החיתוך של שלושה מאפיינים מפתח: **אוטונומיה** גבוהה (עצמאות פעולה), **כלליות** גבוהה (היקף רחב וכושר הסתגלות), ו**אינטליגנציה** גבוהה (כשירות במשימות קוגניטיביות). מערכות AI נוכחיות עשויות להיות בעלות יכולת גבוהה אך צרות, או כלליות אך מצריכות השגחה אנושית מתמדת, או אוטונומיות אך מוגבלות בהיקף.

A-G-I מלא ישלב את שלושת המאפיינים ברמות התואמות או עולות על היכולת האנושית העליונה. באופן קריטי, השילוב הזה הוא שעושה בני אדם כל כך יעילים וכל כך שונים מתוכנה נוכחית; זה גם מה שיאפשר לאנשים להיות מוחלפים כולם על ידי מערכות דיגיטליות.

בעוד שאינטליגנציה אנושית היא מיוחדת, היא בשום פנים אינה גבול. מערכות "על-אינטליגנטיות" מלאכותיות יכולות לפעול במהירות של מאות פעמים, לעבד כמויות נתונים עצומות ולהחזיק כמויות אדירות "בראש" בו-זמנית, וליצור צברים שהם הרבה יותר גדולים ויעילים מאוספי בני אדם. הן יכולות לעקוף לא יחידים אלא חברות, מדינות, או הציביליזציה שלנו כולה.

### אנחנו בסף

יש קונסנזוס מדעי חזק שAGI *אפשרי*. בינה מלאכותית כבר עולה על ביצועים אנושיים במבחנים כלליים רבים של יכולת אינטלקטואלית, כולל לאחרונה חשיבה ופתרון בעיות ברמה גבוהה. יכולות מפגרות - כמו למידה מתמשכת, תכנון, מודעות עצמית ומקוריות - כולן קיימות ברמה כלשהי במערכות AI נוכחיות, וקיימות שיטות ידועות שעלולות לשפר את כולן.

בעוד שעד לפני כמה שנים חוקרים רבים ראו את AGI כעניין של עשרות שנים, כעת העדויות לזמנים קצרים ל-AGI חזקות:

- "חוקי קנה מידה" מאומתים אמפירית מחברים בין קלט חישובי ליכולת AI, ותאגידים נמצאים במסלול להגדיל קלט חישובי בסדרי גודל במשך מספר השנים הבאות. המשאבים האנושיים והפיסקליים המוקדשים להתקדמות AI שווים כעת לאלו של תריסר פרויקטי מנהטן וכמה פרויקטי אפולו.
- תאגידי AI ומנהיגיהם מאמינים פומבית ובאופן פרטי שAGI (לפי הגדרה כלשהי) ניתן להשגה תוך מספר שנים. לחברות האלה יש מידע שאין לציבור, כולל שלחלקן יש את הדור הבא של מערכות AI בידיהן.
- מנבאי מומחים עם רקורד מוכח נותנים 25% הסתברות ל-AGI (לפי הגדרה כלשהי) שיגיע תוך 1-2 שנים, ו-50% ל-2-5 שנים (ראו תחזיות Metaculus עבור AGI ['חלש'](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) ו['מלא'](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/)).
- אוטונומיה (כולל תכנון גמיש לטווח ארוך) מפגרת במערכות AI, אבל חברות גדולות מתמקדות כעת במשאביהן העצומים בפיתוח מערכות AI אוטונומיות וכינו באופן לא פורמלי את 2025 ["שנת הסוכן"](https://techinformed.com/2025-informed-the-year-of-agentic-ai/).
- בינה מלאכותית תורמת יותר ויותר לשיפור עצמי. ברגע שמערכות AI יהיו מוכשרות כמו חוקרי AI אנושיים בביצוע מחקר AI, סף קריטי להתקדמות מהירה למערכות AI חזקות הרבה יותר יושג וצפוי להוביל להתחמקות ביכולת AI. (אפשר לטעון שההתחמקות הזו כבר החלה.)

הרעיון שAGI חכם מאדם מרוחק עשרות שנים או יותר פשוט אינו בר קיימא עוד לרוב המכריע של המומחים בתחום. חילוקי הדעות כעת הם על כמה חודשים או שנים זה ייקח אם נשאר על המסלול הזה. השאלה המרכזית איתה אנחנו מתמודדים היא: האם עלינו?

### מה מניע את המירוץ ל-AGI

המירוץ לעבר AGI מונע על ידי כוחות מרובים, כל אחד עושה את המצב מסוכן יותר. חברות טכנולוגיה גדולות רואות את AGI כטכנולוגיית האוטומציה האולטימטיבית - לא רק להגביר עובדים אנושיים אלא להחליף אותם במידה רבה או כולה. עבור חברות, הפרס עצום: ההזדמנות לתפוס חלק משמעותי מהתפוקה הכלכלית השנתית של 100 טריליון דולר בעולם על ידי הפיכת עלויות העבודה האנושית לאוטומטיות.

מדינות מרגישות נאלצות להצטרף למירוץ הזה, מצטטות פומבית מנהיגות כלכלית ומדעית, אבל באופן פרטי רואות את AGI כמהפכה פוטנציאלית בעניינים צבאיים הדומה לנשק גרעיני. הפחד שיריבים עלולים לזכות ביתרון אסטרטגי מכריע יוצר דינמיקת מירוץ חימוש קלאסית.

המרדפים אחר על-אינטליגנציה מצטטים לעתים קרובות חזונות גדולים: לרפא את כל המחלות, להפוך הזדקנות, להשיג פריצות דרך באנרגיה ובנסיעות חלל, או ליצור יכולות תכנון על-אנושיות.

בפחות רחמנות, מה שמניע את המירוץ הוא כוח. כל משתתף - בין אם חברה או מדינה - מאמין שאינטליגנציה שווה כוח, ושהם יהיו המפקח הטוב ביותר על הכוח הזה.

אני טוען שהמניעים האלה אמיתיים אבל מוטעים מיסודם: AGI *יספוג* ו*יחפש* כוח במקום להעניק אותו; טכנולוגיות שנוצרו על ידי AI יהיו *גם* דו-פיפיות בחדות, וכשהן מועילות ניתן ליצור אותן עם כלי AI ובלי AGI; ואפילו במידה שAGI ותוצריו נשארים תחת שליטה, הדינמיקות המתחרות האלה - תאגידיות וגיאופוליטיות - הופכות סיכונים גדולים לחברה שלנו לכמעט בלתי נמנעים אלא אם כן יופרעו באופן מכריע.

### בינה מלאכותית כללית ועל-אינטליגנציה מהווים איום דרמטי על הציביליזציה

למרות הפיתוי שלהם, בינה מלאכותית כללית ועל-אינטליגנציה מהווים איומים דרמטיים על הציביליזציה דרך מסלולים מרובים המחזקים זה את זה:

*ריכוז כוח:* AI על-אנושי יכול לשלול כוח מהרוב המכריע של האנושות על ידי ספיגת חלקים עצומים של פעילות חברתית וכלכלית למערכות AI שמנוהלות על ידי קומץ חברות ענק (שעלולות בתורן או להיתפס על ידי ממשלות, או בפועל להשתלט על ממשלות).

*הפרעה מסיבית:* אוטומציה בכמויות של רוב העבודות המבוססות קוגניציה, החלפת המערכות האפיסטמיות הנוכחיות שלנו, והטמעה של מספרים עצומים של סוכנים לא-אנושיים פעילים יהפכו את רוב המערכות הציביליזציוניות הנוכחיות שלנו בתקופה קצרה יחסית.

*קטסטרופות:* על ידי הפצת היכולת - פוטנציאלית מעל הרמה האנושית - ליצור טכנולוגיות צבאיות והרסניות חדשות וניתוקה מהמערכות החברתיות והמשפטיות המבססות אחריות, קטסטרופות פיזיות מנשק להשמדה המונית הופכות לסבירות דרמטית יותר.

*גיאופוליטיקה ומלחמה:* מעצמות עולם גדולות לא תשבנה בחיבוק ידיים אם הן ירגישו שטכנולוגיה שיכולה לספק "יתרון אסטרטגי מכריע" מפותחת על ידי יריביהן.

*התחמקות ואובדן שליטה:* אלא אם כן זה יימנע במפורש, לAI על-אנושי יהיה כל תמריץ לשפר עוד יותר את עצמו ויוכל לעלות בהרבה על בני אדם במהירות, עיבוד נתונים ורמת החשיבה. אין דרך משמעותית בה אנחנו יכולים להיות בשליטה על מערכת כזאת. AI כזה לא יעניק כוח לבני אדם; אנחנו נעניק כוח לו, או שהוא ייקח אותו.

רבים מהסיכונים האלה נשארים גם אם בעיית ה"התאמה" הטכנית - הבטחה שAI מתקדם עושה באמינות מה שבני אדם רוצים שהוא יעשה - נפתרת. בינה מלאכותית מציבה אתגר עצום באיך היא תנוהל, והרבה מאוד היבטים של הניהול הזה הופכים קשים מאוד או בלתי פתירים כשחוצים את האינטליגנציה האנושית.

באופן הכי יסודי, הסוג של AI כללי על-אנושי שמפותח כעת יהיו לו, על פי טבעו, מטרות, סוכנות ויכולות העולות על שלנו. הוא יהיה בלתי נשלט מטבעו - איך אנחנו יכולים לשלוט במשהו שאנחנו לא יכולים להבין או לחזות? הוא לא יהיה כלי טכנולוגי לשימוש אנושי, אלא מין שני של אינטליגנציה על כדור הארץ לצד שלנו. אם יותר להתקדם עוד יותר, הוא יהווה לא רק מין שני אלא מין מחליף.

אולי הוא יטפל בנו טוב, אולי לא. אבל העתיד יהיה שלו, לא שלנו. העידן האנושי יסתיים.

### זה לא בלתי נמנע; האנושות יכולה, באופן קונקרטי מאוד, להחליט לא לבנות את המחליף שלה.

יצירת AGI על-אנושי רחוקה מלהיות בלתי נמנעת. אנחנו יכולים למנוע אותה באמצעות סט מתואם של אמצעי ממשל:

ראשית, אנחנו צריכים חשבונאות ופיקוח חזקים על חישוב AI ("כוח חישוב"), שהוא מאפשר יסודי של מערכות AI גדולות ומנוף לממשל עליהן. זה בתורו דורש מדידה ודיווח סטנדרטיים של סך כוח החישוב המשמש באימון מודלי AI ובהפעלתם, ושיטות טכניות לספירה, הסמכה ואימות החישוב המשמש.

שנית, עלינו ליישם תקרות קשיחות על חישוב AI, גם לאימון וגם להפעלה; אלה מונעות מAI גם להיות חזק מדי וגם לפעול מהר מדי. התקרות האלה ניתן ליישם גם דרך דרישות משפטיות וגם אמצעי אבטחה מבוססי חומרה שבנויים בתוך שבבים מיוחדים ל-AI, אנלוגי לתכונות אבטחה בטלפונים מודרניים. בגלל שחומרה מיוחדת ל-AI מיוצרת על ידי קומץ חברות בלבד, אימות ואכיפה אפשריים דרך שרשרת האספקה הקיימת.

שלישית, אנחנו צריכים אחריות מוגברת למערכות ה-AI המסוכנות ביותר. אלה שמפתחים AI שמשלב אוטונומיה גבוהה, כלליות רחבה ואינטליגנציה עליונה צריכים להתמודד עם אחריות מוחלטת לנזקים, בעוד שמקלטים בטוחים מהאחריות הזו יעודדו פיתוח של מערכות מוגבלות ויותר נשלטות.

רביעית, אנחנו צריכים רגולציה מדורגת מבוססת רמות סיכון. המערכות הכי מסוגלות ומסוכנות ידרשו ערבויות אבטחה ושליטה נרחבות לפני פיתוח והטמעה, בעוד שמערכות פחות חזקות או יותר מיוחדות יתמודדו עם פיקוח פרופורציונלי. המסגרת הרגולטורית הזו צריכה לפעול בסופו של דבר ברמות לאומיות ובינלאומיות.

הגישה הזו - עם מפרט מפורט שניתן במסמך המלא - מעשית: בעוד שתידרש קואורדינציה בינלאומית, אימות ואכיפה יכולים לפעול דרך המספר הקטן של חברות ששולטות בשרשרת האספקה של החומרה המיוחדת. היא גם גמישה: חברות עדיין יכולות לחדש ולהרוויח מפיתוח AI, רק עם גבולות ברורים על המערכות המסוכנות ביותר.

בלימה לטווח ארוך יותר של כוח וסיכון AI תדרוש הסכמים בינלאומיים מבוססים על אינטרס עצמי ומשותף, בדיוק כפי ששליטה על הפצת נשק גרעיני עושה כעת. אבל אנחנו יכולים להתחיל מיד עם פיקוח ואחריות מוגברים, תוך בניה לעבר ממשל מקיף יותר.

המרכיב החסר המרכזי הוא רצון פוליטי וחברתי לקחת שליטה על תהליך פיתוח הבינה המלאכותית. המקור של הרצון הזה, אם הוא יגיע בזמן, יהיה המציאות עצמה - כלומר, מההכרה נרחבת של ההשלכות האמיתיות של מה שאנחנו עושים.

### אנחנו יכולים להנדס AI כלי להעצמת האנושות

במקום לרדוף אחר AGI בלתי נשלט, אנחנו יכולים לפתח "AI כלי" חזק שמשפר יכולת אנושית תוך שהוא נשאר תחת שליטה אנושית משמעותית. מערכות AI כלי יכולות להיות מסוגלות ביותר תוך הימנעות מהחיתוך המסוכן של אוטונומיה גבוהה, כלליות רחבה ואינטליגנציה על-אנושית, כל עוד אנחנו מהנדסים אותן להיות נשלטות ברמה המתאימה ליכולת שלהן. הן יכולות גם להיות משולבות למערכות מתוחכמות ששומרות על פיקוח אנושי תוך מתן יתרונות טרנספורמטיביים.

AI כלי יכול לחולל מהפכה ברפואה, להאיץ גילוי מדעי, לשפר חינוך ולשפר תהליכים דמוקרטיים. כשמנוהל כמו שצריך, הוא יכול להפוך מומחים ומוסדות אנושיים ליעילים יותר במקום להחליף אותם. בעוד שמערכות כאלה עדיין יהיו מאוד מפריעות וידרשו ניהול זהיר, הסיכונים שהן מציבות שונים מיסודם מAGI: הם סיכונים שאנחנו יכולים לנהל, כמו אלה של טכנולוגיות חזקות אחרות, לא איומים קיומיים על סוכנות אנושית וציביליזציה. ובחשיבות קריטית, כשמפותחים בחכמה, כלי AI יכולים לעזור לאנשים לנהל AI חזק ולנהל את השפעותיו.

הגישה הזו דורשת חשיבה מחדש גם על איך AI מפותח וגם על איך היתרונות שלו מופצים. מודלים חדשים של פיתוח AI ציבורי וללא רווח, מסגרות רגולטוריות חזקות ומנגנונים להפצה רחבה יותר של יתרונות כלכליים יכולים לעזור להבטיח שAI יעצים את האנושות כולה במקום לרכז כוח בידיים מעטות. בינה מלאכותית עצמה יכולה לעזור לבנות מוסדות חברתיים ואפיפיים טובים יותר, לאפשר צורות חדשות של קואורדינציה ושיח שמחזקים במקום להחליש את החברה האנושית. ממסדי ביטחון לאומי יכולים למנף את המומחיות שלהם כדי להפוך מערכות כלי AI לבטוחות ומהימנות באמת, ומקור הגנה אמיתי וגם כוח לאומי.

אנחנו עלולים בסופו של דבר לבחור לפתח מערכות עוד יותר חזקות ויותר ריבוניות שהן פחות כמו כלים ו- אנחנו יכולים לקוות - יותר כמו נדיבים חכמים וחזקים. אבל עלינו לעשות זאת רק אחרי שפיתחנו את ההבנה המדעית ויכולת הממשל לעשות זאת בבטחה. החלטה כל כך חשובה ובלתי הפיכה צריכה להתקבל במכוון על ידי האנושות כולה, לא כברירת מחדל במירוץ בין חברות טכנולוגיה ומדינות.

### בידיים אנושיות

אנשים רוצים את הטוב שבא מAI: כלים שימושיים שמעצימים אותם, מגבירים הזדמנויות וצמיחה כלכליות, ומבטיחים פריצות דרך במדע, טכנולוגיה וחינוך. למה שלא? אבל כשנשאלים, רוב מוחלט של הציבור הכללי [רוצה פיתוח AI איטי וזהיר יותר](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation), ולא רוצה AI חכם מאדם שיחליף אותם בעבודות שלהם ובמקומות אחרים, ימלא את התרבות שלהם ואת המרחב המשותף של המידע בתוכן לא-אנושי, ירכז כוח בסט זעיר של תאגידים, יציב סיכונים קיצוניים גלובליים גדולים, ובסופו של דבר יאיים לשלול כוח או להחליף את המין שלהם. למה שהם ירצו?

אנחנו *יכולים* לקבל אחד בלי השני. זה מתחיל בהחלטה שהגורל שלנו אינו בבלתי נמנעות כביכול של טכנולוgiה כלשהי או בידיים של כמה מנכ"לים בעמק הסיליקון, אלא בידיים שלנו האחרות אם ניקח אחיזה בו. בואו נסגור את השערים, ונשמור על העתיד אנושי.

## פרק 1 - הקדמה

האופן שבו נגיב לאפשרות של בינה מלאכותית חכמה מבני אדם הוא הנושא הדחוף ביותר של זמננו. מאמר זה מציע דרך קדימה.

יתכן שאנו בסוף העידן האנושי.

משהו החל בעשר השנים האחרונות שהוא ייחודי בהיסטוריה של המין שלנו. השלכותיו יקבעו, במידה רבה, את עתיד האנושות. החל מסביבות 2015, חוקרים הצליחו לפתח בינה מלאכותית *צרה* - מערכות שיכולות לנצח במשחקים כמו Go, לזהות תמונות ודיבור, וכן הלאה, טוב יותר מכל בן אדם.[^1]

זהו הצלחה מדהימה, והיא מניבה מערכות ומוצרים שימושיים ביותר שיעצימו את האנושות. אבל בינה מלאכותית צרה מעולם לא הייתה המטרה האמיתית של התחום. במקום זאת, המטרה הייתה ליצור מערכות AI למטרות *כלליות*, במיוחד כאלה שנקראות לעתים קרובות "בינה מלאכותית כללית" (AGI) או "על-אינטליגנציה" שהן במקביל טובות או טובות יותר מבני אדם כמעט בכל המשימות, בדיוק כפי ש-AI כיום הוא על-אנושי ב-Go, שחמט, פוקר, מרוצי רחפנים וכו'. זו המטרה המוצהרת של חברות AI מרכזיות רבות.[^2]

*המאמצים הללו גם מצליחים.* מערכות AI למטרות כלליות כמו ChatGPT, Gemini, Llama, Grok, Claude ו-Deepseek, המבוססות על חישובים עצומים והרי נתונים, הגיעו לשוויון עם בני אדם טיפוסיים במגוון רחב של משימות, ואף משתוות למומחים אנושיים בתחומים מסוימים. כעת מהנדסי AI בכמה מחברות הטכנולוgiה הגדולות ביותר מתחרים לדחוף את הניסויים הענקיים הללו באינטליגנציה מכונית לרמות הבאות, בהן הם משתווים ואז עולים על הטווח המלא של יכולות אנושיות, מומחיות ואוטונומיה.

*זה קרוב.* במשך עשר השנים האחרונות, הערכות מומחים לכמה זמן זה ייקח - אם נמשיך במסלול הנוכחי - נפלו מעשרות שנים (או מאות שנים) לשנים בודדות.

זה גם בעל חשיבות עידנית וסיכון מופלא. תומכי AGI רואים בזה טרנספורמציה חיובית שתפתור בעיות מדעיות, תרפא מחלות, תפתח טכנולוגיות חדשות ותבצע אוטומציה של עבודת פרך. ו-AI בהחלט יכול לעזור להשיג את כל הדברים הללו - למעשה זה כבר קורה. אבל במשך עשרות השנים, הוגים זהירים רבים, מאלן טיורינג ועד סטיבן הוקינג ועד ג'פרי הינטון ויושוע בנג'יו של ימינו[^3] הוציאו אזהרה חריפה: בניית AI כללי, אוטונומי ובאמת חכם מבני אדם תהפוך לחלוטין ובאופן בלתי הפיך את החברה במקרה הטוב, ותביא להכחדת האנושות במקרה הרע.[^4]

AI על-אינטליגנטי מתקרב במהירות במסלול הנוכחי שלנו, אבל הוא רחוק מלהיות בלתי נמנע. מאמר זה הוא טיעון מורחב מדוע וכיצד עלינו *לסגור את השערים* לעתיד הלא-אנושי המתקרב הזה, ומה עלינו לעשות במקום זאת.


[^1]: [התרשים](https://time.com/6300942/ai-progress-charts/) הזה מציג סט של משימות; עקומות דומות רבות יכלו להיוסף לגרף הזה. ההתקדמות המהירה הזאת ב-AI צר הפתיעה אפילו מומחים בתחום, כאשר אמות מידה עברו שנים לפני התחזיות.

[^2]: Deepmind, OpenAI, Anthropic ו-X.ai כולן נוסדו עם המטרה הספציפית של פיתוח AGI. למשל, האמנה של OpenAI מציינת במפורש שמטרתה לפתח "בינה מלאכותית כללית שתועיל לכל האנושות", בעוד שהמשימה של DeepMind היא "לפתור אינטליגנציה, ואז להשתמש בזה כדי לפתור הכל". מטא, מיקרוסופט ואחרים כעת רודפים אחר מסלולים דומים במהותם. מטא אמרה שהיא [מתכננת לפתח AGI ולשחרר אותו בצורה פתוחה.](https://www.forbes.com/sites/johnkoetsier/2024/01/18/zuckerberg-on-ai-meta-building-agi-for-everyone-and-open-sourcing-it/)

[^3]: הינטון ובנג'יו הם שניים מחוקרי ה-AI הנצטטים ביותר, שניהם זכו בנובל של תחום ה-AI, פרס טיורינג, והינטון זכה גם בפרס נובל (בפיזיקה).

[^4]: בניית משהו מסוכן כל כך, תחת תמריצים מסחריים וכמעט אפס פיקוח ממשלתי, היא תקדים שלא ראינו. אין אפילו מחלוקת על הסיכון בין אלה שבונים את זה! המנהיגים של Deepmind, OpenAI ו-Anthropic, בין מומחים רבים אחרים, כולם חתמו על [הצהרה](https://www.safe.ai/work/statement-on-ai-risk) שAI מתקדם מהווה *סיכון הכחדה לאנושות.* פעמוני האזעקה לא יכלו לצלצל חזק יותר, ואפשר רק להסיק שאלה שמתעלמים מהם פשוט לא לוקחים AGI ועל-אינטליגנציה ברצינות. אחת המטרות של המאמר הזה היא לעזור להם להבין מדוע הם צריכים.

## פרק 2 - דברים חיוניים על רשתות נוירונים של AI

איך מערכות AI מודרניות פועלות, ומה עשוי להגיע בדור הבא של מערכות AI?

כדי להבין איך יתפתחו ההשלכות של פיתוח AI חזק יותר, חיוני להפנים כמה יסודות. הקטע הזה ושני הקטעים הבאים מפתחים את אלה, ומכסים בתורם מהו AI מודרני, איך הוא מנצל חישובים עצומים, ובאילו מובנים הוא גדל במהירות בכלליות וביכולת.[^5]

ישנן דרכים רבות להגדיר בינה מלאכותית, אבל לצרכינו התכונה המרכזית של AI היא שבעוד תוכנית מחשב רגילה היא רשימה של הוראות איך לבצע משימה, מערכת AI היא כזאת שלומדת מנתונים או מניסיון לבצע משימות *מבלי שאומרים לה במפורש איך לעשות זאת.*

כמעט כל AI מודרני בולט מבוסס על רשתות נוירונים. אלה הן מבנים מתמטיים/חישוביים, המיוצגים על ידי קבוצה גדולה מאוד (מיליארדים או טריליונים) של מספרים ("משקולות"), שמבצעים משימת אימון היטב. המשקולות הללו מעוצבות (או אולי "גדלות" או "נמצאות") על ידי התאמה חוזרת ונשנית שלהן כך שהרשת הנוירונים משפרת ציון מספרי (המכונה גם "אובדן") שהוגדר כדי לבצע היטב משימה אחת או יותר.[^6] התהליך הזה נקרא *אימון* הרשת הנוירונים.[^7]

ישנן טכניקות רבות לביצוע האימון הזה, אבל הפרטים הללו הרבה פחות רלוונטיים מהדרכים בהן הניקוד מוגדר, ואיך אלה מביאות למשימות שונות שהרשת הנוירונים מבצעת היטב. הבחנה מרכזית נעשתה היסטורית בין AI "צר" ו"כללי".

AI צר מאומן במכוון לעשות משימה מסוימת או קבוצה קטנה של משימות (כמו זיהוי תמונות או משחק שח); הוא דורש אימון מחדש למשימות חדשות, ויש לו טווח יכולת צר. יש לנו AI צר על-אנושי, כלומר כמעט לכל משימה דיסקרטית מוגדרת היטב שאדם יכול לעשות, אנחנו כנראה יכולים לבנות ציון ואז לאמן בהצלחה מערכת AI צרה לעשות אותה טוב יותר ממה שאדם יכול.

מערכות AI כלליות (GPAI) יכולות לבצע מגוון רחב של משימות, כולל רבות שלא אומנו עליהן במפורש; הן גם יכולות ללמוד משימות חדשות כחלק מהפעולה שלהן. "מודלים רב-תחומיים" גדולים נוכחיים[^8] כמו ChatGPT מדגימים זאת: מאומנים על קורפוס גדול מאוד של טקסט ותמונות, הם יכולים לעסוק בהיגיון מורכב, לכתוב קוד, לנתח תמונות, ולסייע עם מערך עצום של משימות אינטלקטואליות. למרות שהם עדיין שונים למדי מאינטליגנציה אנושית בדרכים שנראה לעומק למטה, הכלליות שלהם גרמה למהפכה ב-AI.[^9]

### חוסר יכולת חיזוי: תכונה מרכזית של מערכות AI

הבדל מרכזי בין מערכות AI ותוכנה רגילה הוא ביכולת החיזוי. הפלט של תוכנה סטנדרטית יכול להיות בלתי צפוי – ואכן לפעמים זה למה אנחנו כותבים תוכנה, כדי לתת לנו תוצאות שלא יכולנו לחזות. אבל תוכנה רגילה ממעט לעשות דבר שלא תוכנתה לעשות – הטווח וההתנהגות שלה בדרך כלל כמו שתוכננה. תוכנת שח מהשורה הראשונה עשויה לעשות מהלכים שאף אדם לא יכול לחזות (אחרת הם יכלו להביס את תוכנת השח הזאת!) אבל היא בדרך כלל לא תעשה דבר מלבד לשחק שח.

כמו תוכנה רגילה, ל-AI צר יש טווח והתנהגות צפויים אבל יכולות להיות לו תוצאות בלתי צפויות. זו באמת רק דרך נוספת להגדיר AI צר: כ-AI שדומה לתוכנה רגילה ביכולת החיזוי ובטווח הפעולה שלו.

AI כללי שונה: הטווח שלו (התחומים שעליהם הוא חל), ההתנהגות (סוגי הדברים שהוא עושה), והתוצאות (הפלטים בפועל שלו) יכולים כולם להיות בלתי צפויים.[^10] GPT-4 אומן רק ליצור טקסט במדויק, אבל פיתח יכולות רבות שהמאמנים שלו לא חזו או התכוונו אליהן. חוסר יכולת החיזוי הזה נובע מהמורכבות של האימון: מכיוון שנתוני האימון מכילים פלטים ממשימות שונות רבות, ה-AI חייב ללמוד ביעילות לבצע את המשימות הללו כדי לחזות היטב.

חוסר יכולת החיזוי הזה של מערכות AI כלליות הוא די בסיסי. למרות שבעקרון אפשר לבנות בזהירות מערכות AI שיש להן מגבלות מובטחות על ההתנהגות שלהן (כפי שמוזכר מאוחר יותר במאמר), הדרך שבה מערכות AI נוצרות כעת הן בלתי צפויות בפועל ואפילו בעקרון.

### AI פסיבי, סוכנים, מערכות אוטונומיות, והתאמה

חוסר יכולת החיזוי הזה הופך חשוב במיוחד כשאנחנו שוקלים איך מערכות AI אכן נפרסות ומשמשות להשגת מטרות שונות.

מערכות AI רבות יחסית פסיביות במובן שהן בעיקר מספקות מידע, והמשתמש נוקט בפעולות. אחרות, המכונות בדרך כלל *סוכנים*, נוקטות בפעולות בעצמן, עם רמות מעורבות משתנות מהמשתמש. אלה שנוקטות בפעולות עם קלט או פיקוח חיצוני פחות יחסית עשויות להיחשב יותר *אוטונומיות*. זה יוצר ספקטרום מבחינת עצמאות פעולה, מכלים פסיביים לסוכנים אוטונומיים.[^11]

לגבי מטרות של מערכות AI, אלה עשויות להיות קשורות ישירות ליעד האימון שלהן (למשל המטרה של "ניצחון" למערכת משחקת גו היא גם מפורשות מה שהיא אומנה לעשות). או שהן עשויות לא להיות: יעד האימון של ChatGPT הוא בחלקו לחזות טקסט, בחלקו להיות עוזר מועיל. אבל כשהוא עושה משימה נתונה, המטרה שלו מסופקת לו על ידי המשתמש. מטרות עשויות גם להיווצר על ידי מערכת AI בעצמה, רק בקשר עקיף מאוד ליעד האימון שלה.[^12]

מטרות קשורות קשר הדוק לשאלה של "התאמה", כלומר השאלה האם מערכות AI *יעשו מה שאנחנו רוצים שהן תעשנה*. השאלה הפשוטה הזאת מסתירה רמה עצומה של עדינות.[^13] בינתיים, שימו לב ש"אנחנו" במשפט הזה עשוי להתייחס לאנשים וקבוצות שונות רבות, מה שמוביל לסוגים שונים של התאמה. לדוגמה, AI עשוי להיות מאוד *צייתן* (או ["נאמן"](https://arxiv.org/abs/2003.11157)) למשתמש שלו – כאן "אנחנו" זה "כל אחד מאיתנו". או שהוא עשוי להיות יותר *ריבוני*, להיות מונע בעיקר מהמטרות והמגבלות שלו עצמו, אבל עדיין לפעול בגדול לטובת הרווחה האנושית הכללית – "אנחנו" זה אז "האנושות" או "החברה". באמצע יש ספקטרום שבו AI יהיה ברובו צייתן, אבל עשוי לסרב לנקוט בפעולות שמזיקות לאחרים או לחברה, מפרות את החוק, וכו'.

שני הצירים הללו – רמת אוטונומיה וסוג התאמה – אינם עצמאיים לחלוטין. לדוגמה, מערכת פסיבית ריבונית, למרות שאינה בדיוק סתירה עצמית, היא מושג במתח, כמו גם סוכן אוטונומי צייתן.[^14] יש הבנה ברורה שאוטונומיה וריבונות נוטות ללכת יד ביד. באופן דומה, יכולת החיזוי נוטה להיות גבוהה יותר במערכות AI "פסיביות" ו"צייתניות", בעוד שריבוניות או אוטונומיות ינטו להיות יותר בלתי צפויות. כל זה יהיה חיוני להבנת ההשלכות של AGI ועל-אינטליגנציה פוטנציאליים.

יצירת AI מותאם באמת, מכל סוג שהוא, דורשת פתרון שלושה אתגרים נפרדים:

1. הבנה של מה ש"אנחנו" רוצים – שזה מורכב בין אם "אנחנו" אומר אדם או ארגון ספציפי (נאמנות) או האנושות בגדול (ריבונות);
2. בנייה של מערכות שפועלות בקביעות בהתאם לרצונות הללו – בעצם יצירה של התנהגות חיובית עקבית;
3. בצורה הבסיסית ביותר, יצירה של מערכות שבאמת "אכפת" להן מהרצונות הללו במקום רק להתנהג כאילו כן.

ההבחנה בין התנהגות אמינה ואכפתיות אמיתית חיונית. בדיוק כמו עובד אנושי שעשוי לקיים הוראות בצורה מושלמת תוך חוסר מחויבות אמיתית למשימה של הארגון, מערכת AI עשויה לפעול בהתאמה מבלי לחשוב באמת על העדפות אנושיות. אנחנו יכולים לאמן מערכות AI לומר ולעשות דברים דרך משוב, והן יכולות ללמוד להסיק על מה שבני אדם רוצים. אבל לגרום להן *לחשוב באמת* על העדפות אנושיות זה אתגר הרבה יותר עמוק.[^15]

הקשיים העמוקים בפתרון אתגרי ההתאמה הללו, וההשלכות שלהם לסיכון AI, ייחקרו עוד למטה. בינתיים, הבינו שהתאמה היא לא רק תכונה טכנית שאנחנו מוסיפים למערכות AI, אלא היבט בסיסי של הארכיטקטורה שלהן שמעצב את הקשר שלהן עם האנושות.

[^5]: למבוא עדין אך טכני למכונת למידה ו-AI, במיוחד מודלי שפה, ראו [אתר זה.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) למדריך מודרני נוסף לסיכוני הכחדה של AI, ראו [יצירה זו.](https://www.thecompendium.ai/) לניתוח מדעי מקיף ומוסמך של מצב בטיחות AI, ראו את [הדוח הבינלאומי לבטיחות AI](https://arxiv.org/abs/2501.17805) האחרון.

[^6]: האימון בדרך כלל מתרחש על ידי חיפוש אחר מקסימום מקומי של הציון במרחב רב-ממדי הנתון על ידי משקולות המודל. על ידי בדיקה איך הציון משתנה כשמשקולות מותאמות, אלגוריתם האימון מזהה אילו התאמות משפרות את הציון הכי הרבה, ומזיז את המשקולות לכיוון הזה.

[^7]: לדוגמה, בבעיית זיהוי תמונות, הרשת הנוירונים תוציא הסתברויות לתוויות לתמונה. ציון יהיה קשור להסתברות שה-AI מייחס לתשובה הנכונה. הליך האימון יתאים אז משקולות כך שבפעם הבאה, ה-AI יוציא הסתברות גבוהה יותר לתווית הנכונה לאותה תמונה. זה אז חוזר על עצמו מספר עצום של פעמים. אותו הליך בסיסי משמש לאימון בעצם כל הרשתות הנוירוניות המודרניות, אמנם עם מנגנון ניקוד מורכב יותר.

[^8]: רוב המודלים הרב-תחומיים משתמשים בארכיטקטורת "הטרנספורמר" לעיבוד ויצירה של סוגי נתונים מרובים (טקסט, תמונות, קול). אלה כולם יכולים להתפרק ל, ואז להיחשב על אותה הבסיס, כסוגים שונים של "טוקנים". מודלים רב-תחומיים מאומנים תחילה לחזות טוקנים במדויק בתוך מאגרי נתונים עצומים, ואז משופרים דרך למידת חיזוק כדי לשפר יכולות ולעצב התנהגויות.

[^9]: שמודלי שפה מאומנים לעשות דבר אחד – לחזות מילים – גרם לכמה לקרוא להם AI צר. אבל זה מטעה: מכיוון שחיזוי טקסט היטב דורש כל כך הרבה יכולות שונות, משימת האימון הזאת מובילה למערכת כללית באופן מפתיע. שימו לב גם שהמערכות הללו מאומנות רבות על ידי למידת חיזוק, המייצגות ביעילות אלפי אנשים שנותנים למודל אות תגמול כשהוא עושה עבודה טובה בכל אחד מהדברים הרבים שהוא עושה. הוא אז יורש כלליות משמעותית מהאנשים שנותנים את המשוב הזה.

[^10]: ישנן דרכים מרובות שבהן AI בלתי צפוי. אחת היא שבמקרה הכללי אי אפשר לחזות מה אלגוריתם יעשה מבלי באמת להריץ אותו; יש [משפטים](https://arxiv.org/abs/1310.3225) לכך. זה יכול להיות נכון רק כי הפלט של אלגוריתמים יכול להיות מורכב. אבל זה ברור ורלוונטי במיוחד במקרה (כמו בשח או גו) שבו החיזוי ירמז על יכולת (להביס את ה-AI) שאין למי שרוצה לחזות. שנית, מערכת AI נתונה לא תמיד תפיק את אותו הפלט אפילו בהינתן אותו קלט – הפלטים שלה מכילים אקראיות; זה גם מתחבר עם חוסר יכולת חיזוי אלגוריתמית. שלישית, יכולות לא צפויות ומתהוות יכולות לצוץ מאימון, כלומר אפילו *סוגי* הדברים שמערכת AI יכולה ותעשה הם בלתי צפויים; הסוג האחרון הזה חשוב במיוחד לשיקולי בטיחות.

[^11]: ראו [כאן](https://arxiv.org/abs/2502.02649) לסקירה מעמיקה של מה שמתכוון "סוכן אוטונומי" (יחד עם טיעונים אתיים נגד בנייתם).

[^12]: אתם עשויים לשמוע לפעמים "AI לא יכול להחזיק מטרות משלו". זה שטויות מוחלטות. קל ליצור דוגמאות שבהן ל-AI יש או מפתח מטרות שמעולם לא ניתנו לו והן ידועות רק לו עצמו. אתם לא רואים את זה הרבה במודלים רב-תחומיים פופולריים נוכחיים כי זה מאומן להיות מחוץ להם; זה יכול באותה קלות להיות מאומן לתוכם.

[^13]: יש ספרות גדולה. על הבעיה הכללית ראו של כריסטיאן [*The Alignment Problem*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821), ושל ראסל [*Human-Compatible*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616). בצד יותר טכני ראו למשל [מאמר זה](https://arxiv.org/abs/2209.00626).

[^14]: נראה מאוחר יותר שבעוד מערכות כאלה לא עוקבות אחר המגמה, זה בעצם הופך אותן למעניינות ושימושיות מאוד.

[^15]: זה לא אומר שאנחנו דורשים רגשות או תחושה. במקום זאת, זה קשה מאוד מחוץ למערכת לדעת מהם המטרות, ההעדפות, והערכים הפנימיים שלה. "אמיתי" כאן היה אומר שיש לנו סיבה חזקה מספיק לסמוך על זה שבמקרה של מערכות קריטיות אנחנו יכולים להמר את החיים שלנו עליהם.

## פרק 3 - היבטים מרכזיים ביצירת מערכות בינה מלאכותית כללית מודרניות

רוב מערכות הבינה המלאכותית המתקדמות בעולם נוצרות באמצעות שיטות דומות באופן מפתיע. להלן היסודות.

כדי להבין באמת בן אדם צריך לדעת משהו על ביולוגיה, אבולוציה, חינוך ילדים ועוד; כדי להבין בינה מלאכותית גם צריך לדעת איך היא נוצרת. במהלך חמש השנים האחרונות, מערכות בינה מלאכותית התפתחו באופן עצום הן ביכולת והן במורכבות. גורם מאפשר מרכזי היה הזמינות של כמויות גדולות מאוד של כוח חישוב (או בפי העם "קומפיוט" כשמדובר על בינה מלאכותית).

המספרים מרהיבים. כ-10<sup>25</sup>-10<sup>26</sup> "פעולות נקודה צפה" (FLOP)[^16] משמשות באימון של מודלים כמו סדרת GPT, Claude, Gemini וכדומה.[^17] (לשם השוואה, אם כל אדם על פני כדור הארץ היה עובד ללא הפסקה ומבצע חישוב אחד כל חמש שניות, זה היה לוקח בערך מיליארד שנים להשיג זאת.) כמות עצומה זו של כוח חישוב מאפשרת אימון של מודלים עם עד טריליונים של משקולות מודל על טרהבייטים של נתונים - חלק גדול מכל הטקסט האיכותי שאי פעם נכתב לצד ספריות גדולות של צלילים, תמונות ווידאו. בשילוב אימון זה עם אימון נרחב נוסף המחזק העדפות אנושיות וביצוע טוב של משימות, מודלים המאומנים בדרך זו מציגים ביצועים תחרותיים לאדם על פני טווח משמעותי של משימות אינטלקטואליות בסיסיות, כולל חשיבה ופתרון בעיות.

אנו גם יודעים (בערך, בערך מאוד) כמה מהירות כוח חישוב, בפעולות לשנייה, מספיקה כדי שמהירות ה*הסקה*[^18] של מערכת כזו תתאים ל*מהירות* של עיבוד טקסט אנושי. זה בערך 10<sup>15</sup>-10<sup>16</sup> FLOP לשנייה.[^19]

למרות היותם חזקים, מודלים אלה מוגבלים מטבעם בדרכים מרכזיות, די דומה לאופן שבו בן אדם בודד היה מוגבל אם היה נאלץ פשוט להוציא טקסט בקצב קבוע של מילים לדקה, מבלי לעצור לחשוב או להשתמש בכלים נוספים. מערכות בינה מלאכותית עדכניות יותר מתמודדות עם מגבלות אלה דרך תהליך ואדריכלות מורכבים יותר המשלבים מספר אלמנטים מרכזיים:

- רשת נוירונים אחת או יותר, כאשר מודל אחד מספק את היכולת הקוגניטיבית הליבה, ועד כמה אחרים מבצעים משימות אחרות צרות יותר;
- *כלים* המוענקים למודל ושמיש על ידיו - לדוגמה יכולת לחפש ברשת, ליצור או לערוך מסמכים, לבצע תוכניות וכדומה.
- *פיגומים* המחברים בין קלטים ופלטים של רשתות נוירונים. פיגום פשוט מאוד עשוי פשוט לאפשר לשני "מופעים" של מודל בינה מלאכותית לשוחח זה עם זה, או לאחד לבדוק את העבודה של האחר.[^20]
- *שרשרת חשיבה* וטכניקות הנחיה דומות עושות משהו דומה, גורמות למודל למשל ליצור גישות רבות לבעיה, ואז לעבד את הגישות הללו לתשובה מצרפית.
- *אימון חוזר* של מודלים לעשות שימוש טוב יותר בכלים, פיגומים ושרשרת חשיבה.

מכיוון שההרחבות הללו יכולות להיות חזקות מאוד (וכוללות מערכות בינה מלאכותית עצמן), המערכות המורכבות הללו יכולות להיות מתוחכמות למדי ולשפר באופן דרמטי את יכולות הבינה המלאכותית.[^21] ולאחרונה, טכניקות בפיגומים ובמיוחד בהנחיית שרשרת חשיבה (וקיפול התוצאות בחזרה לאימון חוזר של מודלים להשתמש בהן טוב יותר) פותחו והופעלו ב-[o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/) ו-[DeepSeek R1](https://api-docs.deepseek.com/news/news250120) כדי לבצע מעברים רבים של הסקה בתגובה לשאילתה נתונה.[^22] זה למעשה מאפשר למודל "לחשוב על" התגובה שלו ומשפר באופן דרמטי את יכולת המודלים הללו לבצע חשיבה ברמה גבוהה במשימות מדע, מתמטיקה ותכנות.[^23]

עבור אדריכלות בינה מלאכותית נתונה, הגדלות בכוח החישוב של האימון [ניתנות לתרגום אמין](https://arxiv.org/abs/2405.10938) לשיפורים במערכת מדדים מוגדרים בבירור. עבור יכולות כלליות פחות מוגדרות בחדות (כמו אלה שנדונות להלן), התרגום פחות ברור וחזוי, אבל זה כמעט בוודאי שמודלים גדולים יותר עם יותר כוח חישוב לאימון יהיו להם יכולות חדשות וטובות יותר, גם אם קשה לחזות מה הן יהיו.

באופן דומה, מערכות מורכבות ובמיוחד התקדמות ב"שרשרת חשיבה" (ואימון של מודלים שעובדים טוב איתה) פתחו קנה מידה בכוח חישוב *הסקה*: עבור מודל ליבה מאומן נתון, לפחות חלק מיכולות מערכת הבינה המלאכותית גדלות ככל שמיושם יותר כוח חישוב המאפשר להן "לחשוב חזק יותר ויותר זמן" על בעיות מורכבות. זה בא במחיר תלול של מהירות חישוב, מצריך מאות או אלפים יותר FLOP/s כדי להתאים לביצועים אנושיים.[^24]

למרות שזה רק חלק ממה שמוביל להתקדמות מהירה בבינה מלאכותית,[^25] תפקיד כוח החישוב והאפשרות של מערכות מורכבות יתברר כמכריע הן למניעת AGI בלתי נשלט והן לפיתוח חלופות בטוחות יותר.

[^16]: 10<sup>27</sup> אומר 1 ואחריו 25 אפסים, או עשרה טריליון טריליון. FLOP הוא פשוט חיבור או כפל חשבוני של מספרים עם דיוק מסוים. שים לב שביצועי חומרת בינה מלאכותית יכולים להשתנות בגורם של עשרה יותר תלוי בדיוק החשבון ובאדריכלות המחשב. ספירת פעולות שער לוגי (ANDs, ORs, AND NOTs) תהיה בסיסית אבל אלה לא זמינות או נבחנות בדרך כלל; למטרות נוכחיות זה שימושי להתייחס לפעולות 16-ביט (FP16), למרות שיש לקבוע גורמי המרה מתאימים.

[^17]: אוסף הערכות ונתונים קשים זמין מ-[Epoch AI](https://epochai.org/data/large-scale-ai-models) ומצביע על בערך 2×10<sup>25</sup> FLOP של 16-ביט עבור GPT-4; זה בערך תואם [מספרים שדלפו](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) עבור GPT-4. הערכות למודלים אחרים מאמצע 2024 הן כולן בטווח של גורם של כמה מ-GPT-4.

[^18]: הסקה היא פשוט התהליך של יצירת פלט מרשת נוירונים. אימון יכול להיחשב כרצף של הסקות רבות ושינויים במשקולות המודל.

[^19]: עבור הפקת טקסט, ה-GPT-4 המקורי דרש 560 TFLOP לאסימון שנוצר. בערך 7 אסימונים/שנייה נדרשים כדי לעמוד בקצב החשיבה האנושית, אז זה נותן ≈3×10<sup>15</sup> FLOP/s. אבל יעילויות הורידו את זה; [הברושור הזה של NVIDIA](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/) למשל מצביע על מעט כמו 3×10<sup>14</sup> FLOP/s עבור מודל Llama 405B בעל ביצועים דומים.

[^20]: כדוגמה מורכבת מעט יותר, מערכת בינה מלאכותית עשויה תחילה ליצור כמה פתרונות אפשריים לבעיה מתמטית, ואז להשתמש במופע אחר לבדוק כל פתרון, ולבסוף להשתמש בשלישי לסנתז את התוצאות להסבר ברור. זה מאפשר פתרון בעיות יסודי ואמין יותר מאשר מעבר יחיד.

[^21]: ראה למשל פרטים על ["Operator" של OpenAI](https://openai.com/index/introducing-operator/), [יכולות הכלים של Claude](https://docs.anthropic.com/en/docs/build-with-claude/computer-use), ו-[AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). ל-[Deep Research](https://openai.com/index/introducing-deep-research/) של OpenAI כנראה יש אדריכלות די מתוחכמת אבל פרטים לא זמינים.

[^22]: Deepseek R1 מסתמך על אימון והנחיה איטרטיביים של המודל כך שהמודל המאומן הסופי יוצר חשיבה נרחבת של שרשרת חשיבה. פרטים אדריכליים לא זמינים עבור o1 או o3, אולם Deepseek גילה שאין צורך ב"רוטב מיוחד" כלשהו כדי לפתוח קנה מידה של יכולת עם הסקה. אבל למרות קבלת הרבה עיתונות כמערערת את ה"סטטוס קוו" בבינה מלאכותית, זה לא משפיע על הטענות הליבה של החיבור הזה.

[^23]: המודלים הללו עולים משמעותית על מודלים סטנדרטיים במדדי חשיבה. לדוגמה, במדד GPQA Diamond - מבחן קפדני של שאלות מדע ברמת דוקטורט - GPT-4o [קיבל](https://openai.com/index/learning-to-reason-with-llms/) 56%, בעוד o1 ו-o3 השיגו 78% ו-88%, בהתאמה, עולים בהרבה על הניקוד הממוצע של 70% של מומחים אנושיים.

[^24]: O3 של OpenAI כנראה הוציא ∼10<sup>21</sup>-10<sup>22</sup> FLOP [להשלמת כל אחת משאלות אתגר ARC-AGI](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai), שבני אדם מוכשרים יכולים לעשות ב-(נגיד) 10-100 שניות, נותן נתון יותר כמו ∼10<sup>20</sup> FLOP/s.

[^25]: בעוד כוח חישוב הוא מדד מרכזי ליכולת מערכת בינה מלאכותית, הוא מתקשר הן עם איכות נתונים והן עם שיפורים אלגוריתמיים. נתונים או אלגוריתמים טובים יותר יכולים להוריד דרישות חישוביות, בעוד יותר כוח חישוב יכול לפעמים לפצות על נתונים או אלגוריתמים חלשים יותר.

## פרק 4 - מה הן בינה מלאכותית כללית ועל-אינטליגנציה?

במה בדיוק מתחרות חברות הטכנולוגיה הגדולות בעולם כדי לפתח מאחורי דלתיים סגורות?

המונח "בינה מלאכותית כללית" קיים כבר זמן מה כדי להצביע על AI רב-תכליתי "ברמה אנושית". מעולם לא היה זה מונח מוגדר במיוחד, אבל בשנים האחרونות הוא פרדוקסלית לא נעשה מוגדר יותר אלא אף נעשה חשוב יותר, עם מומחים שמתווכחים בו־זמנית האם AGI נמצא עשורים מרוחקים או שכבר הושג, וחברות בשווי של טריליוני דולרים מתחרות "לעבר AGI". (העמימות של "AGI" הודגשה לאחרונה כאשר [מסמכים שדלפו חשפו לכאורה](https://gizmodo.com/leaked-documents-show-openai-has-a-very-clear-definition-of-agi-2000543339) שבחוזה של OpenAI עם מיקרוסופט, AGI הוגדר כ-AI שמייצר 100 מיליארד דולר בהכנסות עבור OpenAI - הגדרה די שכירותית יותר מאשר רוחנית.)

ישנן שתי בעיות מרכזיות עם הרעיון של AI עם "אינטליגנציה ברמה אנושית". ראשית, בני אדם שונים מאוד, מאוד ביכולתם לבצע כל סוג נתון של עבודה קוגניטיבית, כך שאין "רמה אנושית". שנית, אינטליגנציה היא רב־ממדית מאוד; אף על פי שעשויים להיות קורלציות, הן לא מושלמות ועלולות להיות שונות למדי ב-AI. כך שגם אם היינו יכולים להגדיר "רמה אנושית" עבור יכולות רבות, AI בוודאי יהיה הרבה מעבר לכך בחלק גם כאשר הוא די מתחת לכך באחרות.[^26]

עם זאת, זה די חיוני להיות מסוגל לדון בסוגים, רמות וספי יכולת של AI. הגישה שננקטת כאן היא להדגיש ש-AI רב־תכליתי נמצא כאן, ושהוא מגיע - ויגיע - ברמות יכולת שונות שבהן נוח לצרף מונחים גם אם הם רדוקטיביים, כי הם תואמים ספים חיוניים מבחינת השפעות ה-AI על החברה והאנושות.

נגדיר AGI "מלא" כמילה נרדפת ל-"AI רב־תכליתי על־אנושי" שפירושו מערכת AI שמסוגלת לבצע בעיקרון את כל המשימות הקוגניטיביות האנושיות ברמה של המומחה האנושי המוביל או מעליה, כמו גם לרכוש כישורים חדשים ולהעביר יכולת לתחומים חדשים. זה עולה בקנה אחד עם איך ש"AGI" מוגדר לעתים קרובות בספרות המודרנית. חשוב לציין שזהו סף *גבוה* מאוד. לאף אדם אין סוג כזה של אינטליגנציה; במקום זאת זהו סוג האינטליגנציה שהיה לקבוצות גדולות של מומחים אנושיים מובילים אם הם היו משולבים. נוכל לכנות "על־אינטליגנציה" יכולת שחורגת מזה, ולהגדיר רמות יכולת מוגבלות יותר על ידי GPAI "תחרותי־אנושי" ו"תחרותי־מומחה", שמבצעים מגוון רחב של משימות ברמה מקצועית טיפוסית, או ברמת מומחה אנושי.[^27]

המונחים הללו וכמה אחרים נאספים [בטבלה](https://keepthefuturehuman.ai/essay/docs/#tab:terms) למטה. לתחושה קונקרטית יותר של מה שהדרגות השונות של מערכת יכולות לעשות, מועיל לקחת את ההגדרות ברצינות ולשקול מה הן אומרות.

| סוג AI                      | מונחים קשורים                          | הגדרה                                                                                                                                                                                              | דוגמאות                                                                                                                                     |
| ---------------------------- | --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |
| AI צר                        | AI חלש                                  | AI שאומן למשימה ספציפית או למשפחת משימות. מצטיין בתחום שלו אבל חסר אינטליגנציה כללית או יכולת למידה מעבירה.                                                                                          | תוכנת זיהוי תמונות; עוזרים קוליים (כגון Siri, Alexa); תוכניות משחק שחמט; AlphaFold של DeepMind                                             |
| AI כלי                       | אינטליגנציה מוגברת, עוזר AI              | (נדון בהמשך החיבור.) מערכת AI שמשפרת יכולות אנושיות. משלבת AI רב־תכליתי תחרותי־אנושי, AI צר ושליטה מובטחת, תוך מתן עדיפות לבטיחות ושיתוף פעולה. תומכת בקבלת החלטות אנושית.                        | עוזרי קודינג מתקדמים; כלי מחקר מונעי AI; פלטפורמות מתוחכמות לניתוח נתונים. סוכנים מוכשרים אך צרים וניתנים לשליטה                        |
| AI רב־תכליתי (GPAI)           |                                         | מערכת AI שניתן להתאים למשימות שונות, כולל כאלה שלא אומנה עליהן במיוחד.                                                                                                                             | מודלי שפה (כגון GPT-4, Claude); מודלי AI רב־מודליים; MuZero של DeepMind                                                                   |
| GPAI תחרותי־אנושי             | AGI [חלש]                               | AI רב־תכליתי שמבצע משימות ברמה אנושית ממוצעת, לעתים חורג ממנה.                                                                                                                                      | מודלי שפה מתקדמים (כגון O1, Claude 3.5); חלק ממערכות AI רב־מודליות                                                                        |
| GPAI תחרותי־מומחה            | AGI [חלקי]                              | AI רב־תכליתי שמבצע את רוב המשימות ברמת מומחה אנושי, עם אוטונומיה משמעותית אך מוגבלת                                                                                                              | אולי O3 מצויד ומבוסס פיגומים, לפחות למתמטיקה, תכנות וחלק ממדעי הטבע הקשים                                                                |
| AGI [מלא]                    | GPAI על־אנושי                           | מערכת AI שמסוגלת לבצע באופן אוטונומי בערך את כל המשימות האינטלקטואליות האנושיות ברמת מומחה או מעליה, עם למידה יעילה והעברת ידע.                                                                 | [אין דוגמאות נוכחיות - תיאורטי]                                                                                                            |
| על־אינטליגנציה              | GPAI על־אנושי מאוד                      | מערכת AI שחורגת הרבה מיכולות אנושיות בכל התחומים, עולה בביצועיה על המומחיות האנושית הקולקטיבית. עליונות זו יכולה להיות בכלליות, באיכות, במהירות ו/או במדדים אחרים.                              | [אין דוגמאות נוכחיות - תיאורטי]                                                                                                            |

אנחנו כבר חווים איך זה להיות עם GPAIs ברמה תחרותית־אנושית. זה השתלב בצורה יחסית חלקה, מכיוון שרוב המשתמשים חווים את זה כמו שיש להם עובד זמני חכם אך מוגבל שעושה אותם יותר פרודוקטיביים עם השפעה מעורבת על איכות עבודתם.[^28]

מה שיהיה שונה לגבי GPAI תחרותי־מומחה הוא שלא יהיו לו המגבלות המרכזיות של AI של ימינו, והוא יעשה את הדברים שמומחים עושים: עבודה כלכלית עצמאית בעלת ערך, יצירת ידע אמיתית, עבודה טכנית שאפשר לסמוך עליה, כאשר רק לעתים רחוקות (אם כי עדיין מדי פעם) הוא עושה טעויות מטופשות.

הרעיון של AGI מלא הוא שהוא *באמת עושה* את כל הדברים הקוגניטיביים שגם בני האדם המוכשרים והיעילים ביותר עושים, באופן אוטונומי וללא עזרה או פיקוח נדרשים. זה כולל תכנון מתוחכם, למידת כישורים חדשים, ניהול פרויקטים מורכבים וכו'. הוא יכול לעשות מחקר מקורי חדשני. הוא יכול לנהל חברה. לא משנה מה העבודה שלכם, אם היא נעשית בעיקר במחשב או בטלפון, *הוא יכול לעשות אותה לפחות טוב כמוכם.* וכנראה הרבה יותר מהר ובזול. נדון בחלק מההשלכות למטה, אבל לעת עתה האתגר עבורכם הוא באמת לקחת את זה ברצינות. דמיינו את עשרת האנשים הכי בעלי ידע ומוכשרים שאתם מכירים או יודעים עליהם - כולל מנכ"לים, מדענים, פרופסורים, מהנדסים מובילים, פסיכולוגים, מנהיגים פוליטיים וסופרים. עטפו את כולם לאחד, שגם דובר 100 שפות, בעל זיכרון מדהים, פועל במהירות, בלתי נלאה ותמיד מוטיבציה, ועובד מתחת לשכר מינימום.[^29] זו תחושה של מה שיהיה AGI.

עבור על־אינטליגנציה הדמיון קשה יותר, כי הרעיון הוא שהיא תוכל לבצע הישגים אינטלקטואליים שאף אדם או אפילו קבוצת אנשים לא יכולה - היא בהגדרה בלתי צפויה על ידינו. אבל אנחנו יכולים לקבל תחושה. כבסיס חשוף, שקלו הרבה AGIs, כל אחד מהם הרבה יותר מוכשר מאפילו המומחה האנושי המוביל, רץ פי 100 ממהירות אנושית, עם זיכרון עצום ויכולת תיאום נהדרת.[^30] וזה הולך מעלה מכאן. להתמודד עם על־אינטליגנציה יהיה פחות כמו לנהל שיחה עם מוח אחר, יותר כמו לנהל משא ומתן עם ציוויליזציה אחרת (ומתקדמת יותר).

אז כמה קרובים *אנחנו* ל-AGI ועל־אינטליגנציה?


[^26]: למשל, מערכות AI נוכחיות חורגות הרבה מיכולת אנושית בחשבון מהיר או במשימות זיכרון, בעוד שהן נופלות במחשבה מופשטת ופתרון בעיות יצירתי.

[^27]: חשוב מאוד, כמתחרה כזה AI יהיו לו כמה יתרונות מבניים עיקריים כולל: הוא לא יתעייף או יהיו לו צרכים אישיים אחרים כמו לבני אדם; ניתן להריץ אותו במהירויות גבוהות יותר פשוט על ידי הגדלת כוח החישוב; ניתן להעתיק אותו יחד עם כל מומחיות או ידע שהוא רוכש - וידע נרכש של רשתות נוירונים יכול אפילו להיות "ממוזג" כדי להעביר מערכי כישורים שלמים ביניהם; הוא יכול לתקשר במהירות מכונה; והוא יכול לשנות את עצמו או לשפר את עצמו בדרכים משמעותיות יותר ובמהירות גבוהה יותר מכל אדם.

[^28]: אם לא בילית זמן בשימוש במערכות AI מתקדמות נוכחיות, אני ממליץ על זה: הן שימושיות ומוכשרות באמת, וזה גם חשוב לכיול ההשפעה שיהיה ל-AI כשהן יתחזקנה.

[^29]: שקלו בית חולים מחקרי מרכזי: AGI מומש במלואו יכול בו־זמנית לנתח את כל נתוני החולים הנכנסים, להתעדכן עם כל מאמר רפואי חדש, להציע אבחנות, לעצב תוכניות טיפול, לנהל ניסויים קליניים ולתאם תזמון הצוות - הכל תוך הפעלה ברמה שמתאימה או חורגת מהמומחים המובילים של בית החולים בכל תחום. והוא יכול לעשות את זה עבור בתי חולים מרובים בו־זמנית, בחלק מהעלות הנוכחית. לרוע המזל, עליכם גם לשקול ארגון פשע מאורגן: AGI מומש במלואו יכול בו־זמנית לפרוץ, להתחזות, לרגל ולסחוט אלפי קורבנות, להתעדכן עם רשויות האכיפה (שמתמכנות הרבה יותר לאט), לעצב תוכניות השתכרות חדשות ולתאם תזמון הצוות - אם יש בכלל צוות.

[^30]: ב[חיבור שלו](https://darioamodei.com/machines-of-loving-grace), דאריו אמודיי, מנכ"ל Anthropic, הביא לזכור "מדינה של [מיליון] גאונים".

## פרק 5 - על סף הדלת

הדרך ממערכות AI של היום ל-AGI מלא נראית קצרה וניתנת לחיזוי באופן מזעזע.

העשור האחרון הביא התקדמות דרמטית ב-AI המונעת על ידי משאבי [חישוב](https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year), כוח אדם ו[תקציב](https://arxiv.org/abs/2405.21015) עצומים. יישומי AI מיוחדים רבים טובים מבני אדם במשימות שהוקצו להם, ובוודאי מהירים וזולים הרבה יותר.[^31] ויש גם סוכנים על-אנושיים מיוחדים שיכולים להשמיד כל אדם במשחקים צרי-תחום כמו [גו](https://www.nature.com/articles/nature16961), [שחמט](https://arxiv.org/abs/1712.01815) ו[פוקר](https://www.deepstack.ai/), כמו גם [סוכנים כלליים יותר](https://deepmind.google/discover/blog/a-generalist-agent/) שיכולים לתכנן ולבצע פעולות בסביבות מדומות פשוטות באותה יעילות של בני אדם.

באופן בולט במיוחד, מערכות AI כלליות נוכחיות מ-OpenAI/Microsoft, Google/Deepmind, Anthropic/Amazon, Facebook/Meta, X.ai/Tesla ואחרים [^32] הופיעו מאז תחילת 2023 והגבירו בהתמדה (אם כי באופן לא אחיד) את יכולותיהן מאז. כל אלה נוצרו באמצעות חיזוי טוקנים על מאגרי נתונים עצומים של טקסט ומולטימדיה, בשילוב עם משוב חיזוק נרחב מבני אדם ומערכות AI אחרות. חלק מהן כוללות גם מערכות נרחבות של כלים ופיגום.

### חוזקות וחולשות של מערכות כלליות נוכחיות

מערכות אלה מתפקדות היטב על פני מגוון הולך ומתרחב של בדיקות שתוכננו למדוד אינטליגנציה ומומחיות, עם התקדמות שהפתיעה אפילו מומחים בתחום:

- כשפורסם לראשונה, GPT-4 [השתווה או עלה על ביצועים אנושיים טיפוסיים](https://arxiv.org/abs/2303.08774) בבחינות אקדמיות סטנדרטיות כולל SATs, GRE, בחינות כניסה ובחינות לשכת עורכי הדין. מודלים עדכניים יותר כנראה מתפקדים הרבה יותר טוב, אם כי התוצאות אינן זמינות לציבור.
- מבחן טיורינג – שזמן רב נחשב נקודת ציון מרכזית ל"AI אמיתי" – עובר כיום באופן שגרתי בכמה צורות על ידי מודלי שפה מודרניים, הן באופן בלתי פורמלי והן ב[מחקרים פורמליים](https://arxiv.org/abs/2405.08007).[^33]
- במדד MMLU המקיף הפרוש על 57 נושאים אקדמיים, [מודלים עדכניים משיגים ציוני רמת מומחה-תחום](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu) (∼90%) [^34]
- המומחיות הטכנית התקדמה באופן דרמטי: מדד GPQA של פיזיקה ברמת תואר שני ראה [ביצועים קפצו](https://epoch.ai/data/ai-benchmarking-dashboard) מניחוש כמעט אקראי (GPT-4, 2022) לרמת מומחה (o1-preview, 2024).
- אפילו בדיקות שתוכננו במיוחד להיות עמידות ל-AI נופלות: O3 של OpenAI כביכול [פותר](https://www.nextbigfuture.com/2024/12/openai-releases-o3-model-with-high-performance-and-high-cost.html) את מדד ARC-AGI לפתרון בעיות מופשטות ברמה אנושית, משיג ביצועי קידוד ברמת מומחה עליון, ומקבל 25% בבעיות "מתמטיקה חדישה" של Epoch AI שתוכננו לאתגר מתמטיקאים מהשורה הראשונה.[^35]
- המגמה כל כך ברורה שהמפתח של MMLU יצר כעת ["הבחינה האחרונה של האנושות"](https://agi.safe.ai/) – שם מבשר רעות המשקף את האפשרות ש-AI יעלה בקרוב על ביצועים אנושיים בכל מבחן משמעותי. נכון לכתיבת שורות אלה, יש טענות על מערכות AI המשיגות 27% (לפי [סם אלטמן](https://x.com/sama/status/1886220281565381078)) ו-35% (לפי [המאמר הזה](https://arxiv.org/abs/2502.09955)) בבחינה קשה ביותר זו. די לא סביר שאדם בודד יכול לעשות כך.

למרות הנתונים המרשימים האלה (והאינטליגנציה הברורה שלהן כשאדם מקיים איתן אינטראקציה) [^36] יש הרבה דברים שרשתות נוירונים אלה (לפחות הגרסאות ששוחררו) *לא יכולות* לעשות. כרגע רובן חסרות גוף – קיימות רק על שרתים – ומעבדות לכל היותר טקסט, קול ותמונות סטטיות (אבל לא וידאו). באופן מכריע, רובן לא יכולות לבצע פעילויות מתוכננות מורכבות הדורשות דיוק גבוה.[^37] ויש מספר תכונות נוספות שחזקות בקוגניציה אנושית ברמה גבוהה הנמוכות כרגע במערכות AI ששוחררו.

הטבלה הבאה מפרטת מספר מאלה, על בסיס מערכות AI מאמצע 2024 כמו GPT-4o, Claude 3.5 Sonnet, וGoogle Gemini 1.5.[^38] השאלה המרכזית לכמה מהר AI כללי יהפוך חזק יותר היא: באיזה מידה פשוט לעשות *יותר מאותו הדבר* יניב תוצאות, לעומת הוספת טכניקות נוספות אבל *ידועות*, לעומת פיתוח או יישום כיווני מחקר AI *באמת חדשים*. התחזיות שלי לזה נתונות בטבלה, במונחים של כמה סביר שכל אחד מהתרחישים האלה יביא את היכולת הזו לרמה אנושית ומעבר לה.

<table><tbody><tr><th>יכולת</th><th>תיאור היכולת</th><th>סטטוס/פרוגנוזה</th><th>קנה מידה/ידוע/חדש</th></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td colspan="4"><em>יכולות קוגניטיביות ליבה</em></td></tr><tr><td>חשיבה</td><td>בני אדם יכולים לבצע חשיבה מדויקת, רב-שלבית, לפי כללים ובדיקת דיוק.</td><td>התקדמות דרמטית לאחרונה באמצעות שרשרת חשיבה מורחבת ואימון חוזר</td><td>95/5/5</td></tr><tr><td>תכנון</td><td>בני אדם מציגים תכנון ארוך טווח והיררכי.</td><td>משתפר עם קנה מידה; יכול להיעזר מאוד בפיגום וטכניקות אימון טובות יותר.</td><td>10/85/5</td></tr><tr><td>עיגון אמת</td><td>מערכות GPAI ממציאות מידע חסר בסיס כדי לענות על שאילתות.</td><td>משתפר עם קנה מידה; נתוני כיול זמינים במודל; ניתן לבדוק/לשפר באמצעות פיגום.</td><td>30/65/5</td></tr><tr><td>פתרון בעיות גמיש</td><td>בני אדם יכולים לזהות דפוסים חדשים ולהמציא פתרונות חדשים לבעיות מורכבות; מודלי ML נוכחיים מתקשים.</td><td>משתפר עם קנה מידה אבל באופן חלש; עשוי להיפתר עם טכניקות נוירו-סימבוליות או "חיפוש" כללי.</td><td>15/75/10</td></tr><tr><td colspan="4"><em>למידה וידע</em></td></tr><tr><td>למידה וזיכרון</td><td>לבני אדם יש זיכרון עבודה, קצר טווח וארוך טווח, שכולם דינמיים וקשורים זה לזה.</td><td>כל המודלים לומדים במהלך האימון; מערכות GPAI לומדות בתוך חלון הקשר ובמהלך כוונון עדין; "למידה רציפה" וטכניקות אחרות קיימות אבל עדיין לא משולבות במערכות GPAI גדולות.</td><td>5/80/15</td></tr><tr><td>הפשטה ורקורסיה</td><td>בני אדם יכולים למפות ולהעביר קבוצות יחסים לכאלה מופשטות יותר לצורכי חשיבה ומניפולציה, כולל חשיבה "מטא" רקורסיבית.</td><td>משתפרת חלש עם קנה מידה; עשויה להתגלות במערכות נוירו-סימבוליות.</td><td>30/50/20</td></tr><tr><td>מודל(ים) עולמי(ים)</td><td>לבני אדם יש מודל עולמי חזוי שהם מעדכנים בהתמדה ובתוכו הם יכולים לפתור בעיות ולבצע חשיבה פיזית</td><td>משתפר עם קנה מידה; עדכון קשור ללמידה; מערכות GPAI חלשות בחיזוי עולם אמיתי.</td><td>20/50/30</td></tr><tr><td colspan="4"><em>עצמי וסוכנות</em></td></tr><tr><td>סוכנות</td><td>בני אדם יכולים לנקוט פעולות כדי לרדוף מטרות, על בסיס תכנון/חיזוי.</td><td>מערכות ML רבות הן סוכניות; ניתן להפוך LLMs לסוכנים באמצעות עטיפות.</td><td>5/90/5</td></tr><tr><td>כיוון עצמי</td><td>בני אדם מפתחים ורודפים אחרי מטרות משלהם, עם מוטיבציה ודחף הנוצרים פנימית.</td><td>מורכב בעיקר מסוכנות פלוס מקוריות; צפוי להתגלות במערכות סוכניות מורכבות עם מטרות מופשטות.</td><td>40/45/15</td></tr><tr><td>התייחסות עצמית</td><td>בני אדם מבינים וחושבים על עצמם כממוקמים בתוך סביבה/הקשר.</td><td>משתפרת עם קנה מידה ויכולה להיות מוגברת עם תגמול אימון.</td><td>70/15/15</td></tr><tr><td>מודעות עצמית</td><td>לבני אדם יש ידע על המחשבות והמצבים המנטליים שלהם ויכולים לחשוב עליהם.</td><td>קיימת במובן מסוים במערכות GPAI, שיכולות כביכול לעבור את "מבחן המראה" הקלסי למודעות עצמית. ניתן לשיפור עם פיגום; אבל לא ברור אם זה מספיק.</td><td>20/55/25</td></tr><tr><td colspan="4"><em>ממשק וסביבה</em></td></tr><tr><td>אינטליגנציה מגולמת</td><td>בני אדם מבינים ומקיימים אינטראקציה פעילה עם הסביבה האמיתית שלהם.</td><td>למידת חיזוק עובדת היטב בסביבות מדומות ואמיתיות (רובוטיות) וניתנת לשילוב בטרנספורמרים רב-מודליים.</td><td>5/85/10</td></tr><tr><td>עיבוד רב-חושי</td><td>בני אדם משלבים ומעבדים בזמן אמת זרמי קלט חזותיים, שמיעתיים וחושיים אחרים.</td><td>אימון במודליות מרובות נראה "פשוט עובד" ומשתפר עם קנה מידה. עיבוד וידאו בזמן אמת קשה אבל למשל מערכות נהיגה עצמית משתפרות במהירות.</td><td>30/60/10</td></tr><tr><td colspan="4"><em>יכולות רמה גבוהה</em></td></tr><tr><td>מקוריות</td><td>מודלי ML נוכחיים יצירתיים בהפיכה ושילוב רעיונות/יצירות קיימים, אבל בני אדם יכולים לבנות מסגרות ומבנים חדשים, לעתים קשורים לזהותם.</td><td>יכולה להיות קשה להבחין מ"יצירתיות," שעשויה להתרחב אליה; עשויה להתגלות מיצירתיות פלוס מודעות עצמית.</td><td>50/40/10</td></tr><tr><td>הכרה</td><td>בני אדם חווים קוואליה; אלה יכולים להיות בעלי ערכיות חיובית, שלילית או נייטרלית; זה "דומה למשהו" להיות אדם.</td><td>קשה מאוד ומסובך פילוסופית לקבוע אם מערכת נתונה היא בעלת זה.</td><td>5/10/85</td></tr></tbody></table>

יכולות מרכזיות שכרגע הן מתחת לרמת מומחה אנושי במערכות GPAI מודרניות, מקובצות לפי סוג. העמודה השלישית מסכמת מצב נוכחי. עמודה אחרונה מציגה סבירות חזויה (%) שביצועים ברמה אנושית יושגו באמצעות: הרחבת טכניקות נוכחיות / שילוב עם טכניקות ידועות / פיתוח טכניקות חדשות. יכולות אלה אינן עצמאיות, ועלייה באחת כלשהי הולכת בדרך כלל יד ביד עם עליות באחרות. שימו לב שלא כולן (במיוחד הכרה) נחוצות למערכות AI המסוגלות לקדם פיתוח AI, מה שמדגיש את האפשרות של AI חזק אבל חסר הכרה.

פירוק מה "חסר" באופן זה הופך את זה די ברור שאנחנו די על המסלול לאינטליגנציה רחבה מעל-אנושית על ידי הרחבת טכניקות קיימות או ידועות.[^39]

עדיין יכולות להיות הפתעות. גם אם נניח בצד "הכרה," יכולות להיות כמה מהיכולות הקוגניטיביות הליבה המפורטות שבאמת לא ניתן לעשות עם טכניקות נוכחיות ודורשות חדשות. אבל חישבו על זה. המאמץ הנוכחי שמופעל על ידי רבות מהחברות הגדולות בעולם מסתכם במספר כפול מההוצאה של פרויקט אפולו ובעשרות כפול מפרויקט מנהטן,[^40] ומעסיק אלפי האנשים הטכניים הטובים ביותר במשכורות חסרות תקדים. הדינמיקה של השנים האחרונות הביאה כעת למשימה זו יותר כוח מוחין אנושי (עם AI שמתווסף כעת) מכל מאמץ בהיסטוריה. אנחנו לא צריכים להמר על כישלון.

### המטרה הגדולה: סוכנים אוטונומיים כלליים

הפיתוח של AI כללי במהלך השנים האחרונות התמקד ביצירת AI כללי וחזק אך דמוי כלי: הוא מתפקד בעיקר כעוזר (די) נאמן, ובאופן כללי לא נוקט פעולות מעצמו. זה נובע חלקית מעיצוב, אבל בעיקר מכיוון שמערכות אלה פשוט לא היו מוכשרות מספיק במיומנויות הרלוונטיות כדי שניתן יהיה להפקיד בידיהן פעולות מורכבות.[^41]

חברות AI וחוקרים, עם זאת, [מעבירים התמקדות](https://www.axios.com/2025/01/23/davos-2025-ai-agents) יותר ויותר לכיוון סוכנים *אוטונומיים* ברמת מומחה לשימוש כללי.[^42] זה יאפשר למערכות לפעול יותר כמו עוזר אנושי שהמשתמש יכול לאצול אליו פעולות אמיתיות.[^43] מה זה ידרש? מספר יכולות מטבלת "מה שחסר" מעורבות, כולל עיגון אמת חזק, למידה וזיכרון, הפשטה ורקורסיה, ומודלים עולמיים (לאינטליגנציה), תכנון, סוכנות, מקוריות, כיוון עצמי, התייחסות עצמית ומודעות עצמית (לאוטונומיה), ועיבוד רב-חושי, אינטליגנציה מגולמת ופתרון בעיות גמיש (לכלליות).[^44]

הצומת המשולש הזה של אוטונומיה גבוהה (עצמאות פעולה), כלליות גבוהה (היקף ורוחב משימות) ואינטליגנציה גבוהה (יכולת במשימות קוגניטיביות) הוא כרגע ייחודי לבני אדם. זה במשתמע מה שרבים כנראה חושבים עליו כש־הם חושבים על AGI – הן מבחינת הערך שלו והן מבחינת הסיכונים שלו.

זה מספק דרך נוספת להגדיר A-G-I כ***A***וטונומית-***G***כללית-***I***נטליגנציה, ונראה שהצומת המשולש הזה מספק עדשה ערכית מאוד למערכות ביכולת גבוהה הן בהבנת הסיכונים והתגמולים שלהן והן בממשל של AI.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Simple-1024x1024.png&w=3840&q=75) אזור הכוח והסיכון המהפכני של A-G-I מגיח מהצומת של שלושה מאפיינים מרכזיים: אוטונומיה גבוהה, אינטליגנציה גבוהה במשימות וכלליות גבוהה.

### מחזור השיפור (העצמי) של ה-AI

גורם מכריע אחרון בהבנת התקדמות AI הוא לולאת המשוב הטכנולוגית הייחודית של AI. בפיתוח AI, הצלחה – הן במערכות מוכחות והן במוצרים שנפרסו – מביאה השקעה, כישרון ותחרות נוספים, ואנחנו כרגע בעיצומה של לולאת משוב עצומה של הייפ-פלוס-מציאות של AI שמניעה מאות מיליארדי, או אפילו טריליוני דולרים בהשקעה.

סוג זה של מחזור משוב יכול לקרות עם כל טכנולוגיה, וראינו את זה ברבות, שבהן הצלחה בשוק מביאה השקעה, שמביאה שיפור והצלחה טובה יותר בשוק. אבל פיתוח AI הולך רחוק יותר, בכך שכעת מערכות AI עוזרות לפתח מערכות AI חדשות וחזקות יותר.[^45] אנחנו יכולים לחשוב על לולאת המשוב הזו בחמישה שלבים, כל אחד עם טווח זמן קצר יותר מהקודם, כמוצג בטבלה.

*מחזור השיפור של ה-AI פועל על פני טווחי זמן מרובים, כשכל שלב עלול להאיץ את השלבים הבאים. השלבים המוקדמים כבר בעיצומם, בעוד השלבים המאוחרים נותרו ספקולטיביים אבל יכולים להתקדם מהר מאוד ברגע שיפתחו.*

מספר מהשלבים האלה כבר מתרחשים, וכמה בבירור מתחילים. השלב האחרון, שבו מערכות AI משפרות את עצמן באופן אוטונומי, היה מרכיב עיקרי בספרות על הסיכון של מערכות AI חזקות מאוד, ומסיבה טובה.[^46] אבל חשוב לציין שזה רק הצורה הדרמטית ביותר של מחזור משוב שכבר התחיל ויכול להוביל להפתעות נוספות בהתקדמות המהירה של הטכנולוגיה.

[^31]: אתם משתמשים בהרבה יותר מה-AI הזה מכפי שכנראה אתם חושבים, הוא מניע יצירת דיבור וזיהויו, עיבוד תמונות, אלגוריתמי פידים חדשותיים וכו'.

[^32]: בעוד היחסים בין זוגות החברות האלה הם די מורכבים ומעודנים, רשמתי אותן במפורש כדי לציין הן את הון השוק הכולל העצום של חברות שמעורבות כעת בפיתוח AI, והן שמאחורי אפילו חברות "קטנות יותר" כמו Anthropic עומדים כיסים עמוקים ביותר באמצעות השקעות ועסקאות שותפות גדולות.

[^33]: זה הפך אופנתי לזלזל במבחן טיורינג, אבל הוא די חזק וכללי. בגרסאות חלשות הוא מציין האם אנשים טיפוסיים המקיימים אינטראקציה עם AI (שמאומן לפעול כמו אדם) בדרכים טיפוסיות לתקופות קצרות יכולים לספר אם זה AI. הם לא יכולים. שנית, מבחן טיורינג יריבותי מאוד יכול לבדוק בעיקרון כל אלמנט של יכולת ואינטליגנציה אנושית – על ידי למשל השוואת מערכת AI לאדם מומחה, מוערכת על ידי מומחים אנושיים אחרים. יש מובן שבו הרבה מהערכת AI היא צורה מוכללת של מבחן טיורינג.

[^34]: זה לפי תחום – אף אדם לא יכול באופן סביר להשיג ציונים כאלה על פני כל הנושאים במקביל.

[^35]: אלה בעיות שיקחו אפילו למתמטיקאים מעולים זמן משמעותי לפתור, אם הם יכולים לפתור אותן בכלל.

[^36]: אם אתם מהסוג הספקני, שמרו על הספקנות אבל באמת קחו את המודלים הכי עדכניים לסיבוב, כמו גם נסו בעצמכם כמה משאלות הבדיקה שהם יכולים לעבור. בתור פרופסור לפיזיקה, הייתי מנבא בוודאות כמעט מוחלטת שלדוגמה, המודלים הטובים ביותר יעברו את בחינת הכישורים לתואר שני במחלקה שלנו.

[^37]: זה וחולשות אחרות כמו המצאות הרחיקו אימוץ שוק והובילו לפער בין יכולות נתפסות ונטענות (שצריכות להיראות גם דרך עדשת התחרות השוק האינטנסיבית והצורך למשוך השקעה). זה בלבל הן את הציבור והן את מקבלי המדיניות לגבי המצב האמיתי של התקדמות AI. בעוד שאולי לא תואמת את ההייפ, ההתקדמות היא אמיתית מאוד.

[^38]: ההתקדמות הגדולה מאז הייתה פיתוח מערכות שמאומנות לחשיבה באיכות עליונה, תוך מינוף חישוב רב יותר במהלך ההסקה ולמידת חיזוק רבה יותר. בגלל שהמודלים האלה חדשים ויכולותיהם פחות נבדקו, לא עדכנתי את הטבלה הזו בכללותה מלבד "חשיבה," שאני רואה בה כפתורה בעיקרון. אבל עדכנתי תחזיות על בסיס יכולות מנוסות ומדווחות של המערכות האלה.

[^39]: גלי אופטימיות AI קודמים בשנות ה-60 וה-80 הסתיימו ב"חורפי AI" כשיכולות מובטחות לא התממשו. עם זאת, הגל הנוכחי שונה ביסודו בכך שהשיג ביצועי על-אנושיים בתחומים רבים, הנתמך במשאבי חישוב עצומים והצלחה מסחרית.

[^40]: פרויקט אפולו המלא [עלה כ-250 מיליארד דולר ארה"ב בדולרי 2020](https://www.planetary.org/space-policy/cost-of-apollo), ופרויקט מנהטן [פחות מעשירית מזה](https://www.brookings.edu/the-costs-of-the-manhattan-project/). גולדמן זקס [צופה טריליון דולר הוצאה רק על מרכזי נתונים של AI](https://www.datacenterdynamics.com/en/news/goldman-sachs-1tn-to-be-spent-on-ai-data-centers-chips-and-utility-upgrades-with-little-to-show-for-it-so-far/) בשנים הקרובות.

[^41]: למרות שבני אדם עושים הרבה שגיאות, אנחנו מזלזלים בכמה אמינים אנחנו יכולים להיות! בגלל שהסתברויות מתרבות, משימה הדורשת 20 שלבים לביצוע נכון דורשת שכל שלב יהיה אמין ב-97% רק כדי לעשות את זה נכון בחצי מהמקרים. אנחנו עושים משימות כאלה כל הזמן.

[^42]: מעבר חזק בכיוון הזה נעשה לאחרונה עם העוזר ["מחקר עמוק"](https://openai.com/index/introducing-deep-research/) של OpenAI שמבצע מחקר כללי באופן אוטונומי, המתואר כ"יכולת סוכנית חדשה שמבצעת מחקר רב-שלבי באינטרנט למשימות מורכבות."

[^43]: דברים כמו למלא את טופס ה-PDF המעצבן, להזמין טיסות וכו'. אבל עם דוקטורט ב-20 תחומים! אז גם: לכתוב את החיבור בשבילכם, לנהל את החוזה בשבילכם, להוכיח את המשפט בשבילכם, ליצור את מסע הפרסום בשבילכם וכו'. מה *אתם* עושים? אתם אומרים לו מה לעשות, כמובן.

[^44]: שימו לב שהכרה *לא* נדרשת בבירור, וגם AI בצומת המשולש הזה לא בהכרח מרמז עליה.

[^45]: האנלוגיה הקרובה ביותר כאן היא אולי טכנולוגיית שבבים, שבה הפיתוח שמר על חוק מור במשך עשרות שנים, כיוון שטכנולוגיות מחשב עוזרות לאנשים לעצב את הדור הבא של טכנולוגיית שבבים. אבל AI יהיה הרבה יותר ישיר.

[^46]: חשוב לתת לזה לשקוע לרגע שAI יכול – בקרוב – להיות משפר את עצמו בטווח זמן של ימים או שבועות. או פחות. זכרו את זה כשמישהו אומר לכם שיכולת AI בוודאי רחוקה.

## פרק 6 - המירוץ לבינה מלאכותית כללית

מהם הכוחות המניעים את המירוץ לבניית בינה מלאכותית כללית, הן עבור חברות והן עבור מדינות?

ההתקדמות המהירה האחרונה בבינה מלאכותית הביאה לרמת תשומת לב והשקעות יוצאות דופן, והן גם נבעו ממנה. זה נובע חלקית מהצלחות בפיתוח בינה מלאכותית, אבל יש כאן גם דברים נוספים. מדוע חלק מהחברות הגדולות בעולם, ואף מדינות שלמות, ממהרות לבנות לא רק בינה מלאכותית, אלא בינה מלאכותית כללית ועל-אינטליגנציה?

### מה הניע את המחקר בבינה מלאכותית לכיוון אינטליגנציה ברמה אנושית

עד לחמש השנים האחרונות בערך, בינה מלאכותית הייתה בעיקר בעיית מחקר אקדמי ומדעי, ולכן נשענה בעיקר על סקרנות ועל הדחף להבין אינטליגנציה ואיך ליצור אותה במצע חדש.

בשלב זה, רוב החוקרים הקדישו מעט מאוד תשומת לב ליתרונות או לסכנות של בינה מלאכותית. כשנשאלו למה צריך לפתח בינה מלאכותית, תשובה נפוצה הייתה לפרט, במעט עמימות, בעיות שבינה מלאכותית יכולה לעזור בהן: תרופות חדשות, חומרים חדשות, מדע חדש, תהליכים חכמים יותר, ובכלל לשפר דברים עבור בני אדם.[^47]

אלו מטרות ראויות להערצה![^48] אמנם נוכל ונשאל האם בינה מלאכותית כללית - ולא בינה מלאכותית באופן כללי - הכרחית למטרות אלו, אבל הן מבטאות את האידיאליזם שבו החלו רבים מחוקרי הבינה המלאכותית.

במהלך חמש השנים האחרונות, עם זאת, הבינה המלאכותית השתנתה מתחום מחקר טהור יחסית לתחום הנדסי ומוצרי הרבה יותר, המונע בעיקר על ידי חלק מהחברות הגדולות בעולם.[^49] חוקרים, אף שעדיין רלוונטיים, כבר לא מנהלים את התהליך.

### מדוע חברות מנסות לבנות בינה מלאכותית כללית?

אז מדוע תאגידים ענקיים (ועוד יותר משקיעים) יוצקים משאבים עצומים לבניית בינה מלאכותית כללית? יש שני מנועים שרוב החברות די כנות לגביהם: הן רואות בבינה מלאכותית מנוע של פרודוקטיביות עבור החברה, ושל רווחים עבורן. מכיוון שבינה מלאכותית כללית היא כללית בטבעה, יש כאן פרס עצום: במקום לבחור בסקטור שבו ליצור מוצרים ושירותים, אפשר לנסות את *כולם בבת אחת*. חברות הטכנולוגיה הגדולות הפכו לענקיות על ידי יצירת מוצרים ושירותים דיגיטליים, ולפחות חלק מהמנהלים בוודאי רואים בבינה מלאכותית פשוט את הצעד הבא בלספק אותם היטב, עם סיכונים ויתרונות שמתרחבים על אלו שמסופקים על ידי חיפוש, מדיה חברתית, מחשבים ניידים, טלפונים וכו'.

אבל למה דווקא בינה מלאכותית כללית? יש תשובה פשוטה מאוד לזה, שרוב החברות והמשקיעים נרתעים מלדבר עליה בפומבי.[^50]

הסיבה היא שבינה מלאכותית כללית יכולה ישירות, אחד על אחד, *להחליף עובדים*.

לא לחזק, לא להעצים, לא להפוך פרודוקטיביים יותר. אפילו לא *לעקור*. כל אלה יכולים וייעשו על ידי בינה מלאכותית שאינה כללית. בינה מלאכותית כללית היא ספציפית במה שיכול *להחליף* לחלוטין עובדי מחשבה (ועם רובוטיקה, גם רבים פיזיים). כתמיכה לעמדה זו אפשר להסתכל לא רחוק יותר מההגדרה [(המוצהרת פומבית) של OpenAI](https://openai.com/our-structure/) לבינה מלאכותית כללית, שהיא "מערכת אוטונומית ברמה גבוהה שעולה על בני אדם ברוב העבודה בעלת הערך הכלכלי".

הפרס כאן (עבור חברות!) הוא עצום. עלויות כוח אדם הן אחוז משמעותי מהכלכלה הגלובלית של ∼100 טריליון דולר. גם אם רק חלק קטן מזה ייתפס על ידי החלפת עבודה אנושית בעבודת בינה מלאכותית, אלו טריליוני דולרים של הכנסות שנתיות. חברות בינה מלאכותית גם מודעות למי שמוכן לשלם. כפי שהן רואות זאת, אתה לא תשלם אלפי דולרים בשנה עבור כלי פרודוקטיביות. אבל חברה *תשלם* אלפי דולרים בשנה להחליף את העבודה שלך, אם היא יכולה.

### מדוע מדינות מרגישות שהן חייבות לרוץ לבינה מלאכותית כללית

הנימוקים המוצהרים של מדינות לרדיפה אחר בינה מלאכותית כללית מתמקדים במנהיגות כלכלית ומדעית. הטיעון משכנע: בינה מלאכותית כללית יכולה להאיץ באופן דרמטי מחקר מדעי, פיתוח טכנולוגי וצמיחה כלכלית. בהינתן מה שעל הכף, הן טוענות, אף כוח מרכזי לא יכול להרשות לעצמו להישאר מאחור.[^51]

אבל יש גם מניעים נוספים ובעיקר לא מוצהרים. אין ספק שכשמנהיגי צבא וביטחון לאומי מסוימים נפגשים מאחורי דלתיים סגורות כדי לדון בטכנולוגיה חזקה יוצאת דופן ומסוכנת באופן קטסטרופי, המיקוד שלהם אינו על "איך נמנע מהסיכונים האלה" אלא על "איך נשיג את זה ראשונים?" מנהיגי צבא ומודיעין רואים בבינה מלאכותית כללית מהפכה פוטנציאלית בעניינים צבאיים, אולי המשמעותית ביותר מאז נשק גרעיני. הפחד הוא שהמדינה הראשונה שתפתח בינה מלאכותית כללית יכולה להשיג יתרון אסטרטגי בלתי עביר. זה יוצר דינמיקה קלאסית של מירוץ חימוש.

נראה שחשיבה זו של "מירוץ לבינה מלאכותית כללית",[^52] אף שמשכנעת, פגומה עמוקות. זה לא בגלל שמירוץ מסוכן ומלא סיכונים - אף שהוא כזה - אלא בגלל טבעה של הטכנולוגיה. ההנחה הלא-מוצהרת היא שבינה מלאכותית כללית, כמו טכנולוגיות אחרות, ניתנת לשליטה על ידי המדינה שמפתחת אותה, והיא ברכה מעניקת כוח לחברה שיש לה את הכי הרבה ממנה. כפי שנראה, היא כנראה לא תהיה אף אחת מאלה.

### למה על-אינטליגנציה?

בעוד חברות מתמקדות בפומבי בפרודוקטיביות, ומדינות בצמיחה כלכלית וטכנולוגית, עבור אלה שרודפים במכוון אחר בינה מלאכותית כללית מלאה ועל-אינטליגנציה אלה הן רק ההתחלה. מה יש להם באמת בראש? אמנם נאמר זאת לעתים רחוקות בקול רם, אבל זה כולל:

1. תרופות למחלות רבות או לכולן;
2. עצירה והיפוך של הזדקנות;
3. מקורות אנרגיה בני קיימא חדשים כמו היתוך;
4. שדרוגים אנושיים, או אורגניזמים מעוצבים דרך הנדסה גנטית;
5. ננוטכנולוגיה וייצור מולקולרי;
6. העלאות מוח;
7. פיזיקה אקזוטית או טכנולוגיות חלל;
8. ייעוץ ותמיכה בהחלטות ברמת על-אנושית;
9. תכנון ותיאום ברמת על-אנושית.

השלושה הראשונות הן בעיקר טכנולוgiות "חד-צדדיות" - כלומר, סביר שיהיו בעלות תועלת נטו חיובית מאוד. קשה לטעון נגד ריפוי מחלות או יכולת לחיות יותר אם בוחרים בכך. וכבר קטפנו את הצד השלילי של היתוך (בצורה של נשק גרעיני); יהיה נחמד עכשיו לקבל גם את הצד החיובי. השאלה עם הקטגוריה הראשונה הזו היא האם קבלת הטכנולוגיות האלה מוקדם יותר מפצה על הסיכון.

הארבע הבאות הן בבירור דו-צדדיות: טכנולוגיות טרנספורמטיביות עם פוטנציאל עצום גם לטובה וגם לסיכונים עצומים, בדומה לבינה מלאכותית. כל אלה, אם היו קופצות מקופסה שחורה מחר ונפרסות, היה מאוד קשה לנהל אותן.[^53]

השתיים האחרונות נוגעות לבינה המלאכותית העל-אנושית שעושה דברים בעצמה במקום רק להמציא טכנולוגיה. ליתר דיוק, אם נסיר איפוסים בצד, אלה כוללות מערכות בינה מלאכותית חזקות שאומרות לאנשים מה לעשות. לקרוא לזה "ייעוץ" הוא לא כנה אם המערכת שמייעצת היא הרבה יותר חזקה מהמתיעץ, שלא יכול להבין במשמעותיות את בסיס ההחלטה (או אפילו אם זה מסופק, לסמוך שהיועץ לא יספק הנמקה משכנעת באופן דומה להחלטה אחרת).

זה מצביע על פריט מרכזי החסר מהרשימה למעלה:

10. כוח.

ברור לחלוטין שהרבה ממה שעומד בבסיס המירוץ הנוכחי לבינה מלאכותית על-אנושית הוא הרעיון ש*אינטליגנציה = כוח*. כל רץ מהמר על היותו המחזיק הטוב ביותר של אותו כוח, ושהוא יוכל להפעיל אותו מסיבות לכאורה מיטיבות מבלי שיחליק או יילקח מהשליטה שלו.

כלומר, מה שחברות ומדינות באמת רודפות אחריו הוא לא רק הפירות של בינה מלאכותית כללית ועל-אינטליגנציה, אלא הכוח לשלוט מי מקבל גישה אליהם וכיצד הם משמשים. חברות רואות עצמן כמשגיחות אחראיות על הכוח הזה בשירות בעלי מניות והאנושות; מדינות רואות עצמן כשומרות הכרחיות המונעות מכוחות עוינים להשיג יתרון מכריע. שתיהן טועות באופן מסוכן, ולא מצליחות להכיר שעל-אינטליגנציה, בטבעה, לא ניתנת לשליטה מהימנה על ידי אף מוסד אנושי. נראה שהטבע והדינמיקות של מערכות על-אינטליגנטיות הופכות שליטה אנושית לקשה מאוד, אם לא בלתי אפשרית.

דינמיקות מירוץ אלה - הן תאגידיות והן גיאופוליטיות - הופכות סיכונים מסוימים לכמעט בלתי נמנעים אלא אם נקטעים באופן נחרץ. אנחנו פונים עכשיו לבחינת הסיכונים האלה ומדוע לא ניתן לצמצמם בהם באופן הולם בתוך פרדיגמת פיתוח תחרותית.[^54]

[^47]: רשימה מדויקת יותר של מטרות ראויות היא [יעדי הפיתוח הבר-קיימא של האו"ם](https://sdgs.un.org/goals). אלו הם, במובן מסוים, הקרוב ביותר שיש לנו לסט של יעדי קונצנזוס גלובליים למה שהיינו רוצים לראות משתפר בעולם. בינה מלאכותית יכולה לעזור.

[^48]: לטכנולוגיה באופן כללי יש כוח טרנספורמטיבי כלכלי וחברתי לשיפור אנושי, כפי שמעידים אלפי שנים. במובן זה, הסבר ארוך ומשכנע של חזון בינה מלאכותית כללית חיובי ניתן למצוא ב[מאמר זה](https://darioamodei.com/machines-of-loving-grace) של מייסד Anthropic, דריו אמודיי.

[^49]: השקעות בינה מלאכותית פרטיות [החלו לפרוח ב-2018-19, וחצו השקעות ציבוריות בערך אז](https://cset.georgetown.edu/publication/tracking-ai-investment/), ועקפו אותן בצורה עצומה מאז.

[^50]: אני יכול להעיד שמאחורי דלתיים סגורות יותר, אין להם הרתעה כזו. וזה הופך להיות יותר ציבורי; ראה למשל "בקשה לסטארטאפים" החדשה של Y-combinator, שחלקים רבים ממנה קוראים במפורש להחלפה סיטונית של עובדים אנושיים. לצטט אותם, "התמורה של B2B SaaS הייתה להפוך עובדים אנושיים ליעילים יותר באופן מדורג. התמורה של סוכני בינה מלאכותית אנכיים היא להפוך את העבודה לאוטומטית לחלוטין... זה לגמרי אפשרי שההזדמנות הזו גדולה מספיק כדי ליצור עוד 100 חד-קרנים." (עבור אלו שלא בקיאים בז'רגון של עמק הסיליקון, "B2B" הוא עסק-לעסק וחד-קרן הוא חברה של מיליארד דולר. כלומר הם מדברים על יותר ממאה עסקים של מיליארד-פלוס-דולר שמחליפים עובדים עבור עסקים אחרים.)

[^51]: ראה למשל [דוח ועדת הביקורת הכלכלית והביטחונית בין ארה"ב וסין](https://www.uscc.gov/sites/default/files/2024-11/2024_Executive_Summary.pdf) האחרון. אף שהיה מעט מאוד הצדקה בתוך הדוח עצמו באופן מפתיע, ההמלצה העיקרית הייתה שהקונגרס האמריקני "יקים ויממן תוכנית דמויה פרויקט מנהטן המוקדשת למירוץ לקבלת יכולת בינה מלאכותית כללית (AGI)".

[^52]: חברות מאמצות כעת את המסגור הגיאופוליטי הזה כמגן נגד כל אילוץ על הפיתוח שלהן בבינה מלאכותית, בדרך כלל בדרכים שהן בעליל אגואיסטיות, ולפעמים בדרכים שאפילו לא הגיוניות בצורה בסיסית. קחו בחשבון את [הגישה של Meta לבינה מלאכותית חזית](https://about.fb.com/news/2025/02/meta-approach-frontier-ai/), שטוענת בו זמנית שאמריקה חייבת "[לחזק את] עמדתה כמובילה בחדשנות טכנולוגית, צמיחה כלכלית וביטחון לאומי" וגם שהיא חייבת לעשות זאת על ידי שחרור פתוח של מערכות הבינה המלאכותית החזקות ביותר שלה - שכולל לתת אותן ישירות ליריבים ולאויבים הגיאופוליטיים שלה.

[^53]: כך שכנראה נצטרך להשאיר את הניהול של הטכנולוגיות האלה לבינות המלאכותיות. אבל זו תהיה האצלת שליטה בעייתית מאוד, שנחזור אליה למטה.

[^54]: תחרות בפיתוח טכנולוגי מביאה לעתים קרובות יתרונות חשובים: מניעת שליטה מונופוליסטית, הנעת חדשנות והפחתת עלויות, מתן אפשרות לגישות מגוונות, ויצירת פיקוח הדדי. עם זאת, עם בינה מלאכותית כללית יש לשקול את היתרונות האלה מול סיכונים ייחודיים מדינמיקות מירוץ ולחץ להפחתת אמצעי זהירות בטיחות.

## פרק 7 - מה יקרה אם נבנה בינה מלאכותית כללית במסלול הנוכחי?

החברה אינה מוכנה למערכות ברמת בינה מלאכותית כללית. אם נבנה אותן בקרוב מאוד, הדברים עלולים להתדרדר קשות.

פיתוח בינה מלאכותית כללית מלאה – מה שנכנה כאן AI שנמצא "מחוץ לשערים" – יהיה שינוי יסודי בטבע העולם: מעצם טבעו זה אומר הוספת מין חדש של אינטליגנציה לכדור הארץ עם יכולות גבוהות יותר מאלה של בני האדם.

מה שיקרה אז תלוי בדברים רבים, כולל טבע הטכנולוגיה, בחירות של אלה שמפתחים אותה, והקשר העולמי שבו היא מפותחת.

כרגע, בינה מלאכותית כללית מלאה מפותחת על ידי קומץ חברות פרטיות ענקיות במירוץ זו מול זו, עם מעט רגולציה משמעותית או פיקוח חיצוני,[^55] בחברה עם מוסדות ליבה חלשים יותר ויותר ואפילו לא תפקודיים,[^56] בתקופה של מתיחות גיאופוליטית גבוהה ותיאום בינלאומי נמוך. אף שחלק מהמעורבים מונעים ממניעים אלטרואיסטיים, רבים מאלה שעושים זאת מונעים מכסף, או כוח, או שניהם.

חיזוי קשה מאוד, אבל יש כמה דינמיקות שמובנות מספיק טוב, ואנלוגיות מתאימות מספיק עם טכנולוגיות קודמות כדי לתת הדרכה. ולמרבה הצער, למרות ההבטחה של AI, הן נותנות סיבה טובה להיות פסימיים עמוקות לגבי איך המסלול הנוכחי שלנו יתפתח.

כדי לנסח זאת בבוטות, במסלול הנוכחי שלנו פיתוח בינה מלאכותית כללית יהיה לו כמה השפעות חיוביות (ויעשה כמה אנשים עשירים מאוד מאוד). אבל טבע הטכנולוגיה, הדינמיקות הבסיסיות, והקשר שבו היא מפותחת, מצביעים בחוזקה על כך ש: AI חזק יערער באופן דרמטי את החברה והציוויליזציה שלנו; נאבד שליטה עליו; אנחנו עלולים בהחלט להגיע למלחמת עולם בגללו; נאבד (או נוותר על) שליטה *בפניו*; זה יוביל לעל-אינטליגנציה מלאכותית, שבהחלט לא נשלט בה וזה יתפשר על סופו של עולם המנוהל על ידי בני אדם.

אלו טענות חזקות, והלוואי שהן היו ספקולציה סרק או "דומריזם" לא מוצדק. אבל לכאן המדע, תורת המשחקים, תורת האבולוציה וההיסטוריה כולם מצביעים. חלק זה מפתח את הטענות האלה, ואת התמיכה שלהן, בפירוט.

### נערער את החברה והציוויליזציה שלנו

למרות מה שאתם עלולים לשמוע בחדרי ישיבות של עמק הסיליקון, רוב ההפרעות – במיוחד מהסוג המהיר מאוד – אינן מועילות. יש הרבה יותר דרכים לגרום למערכות מורכבות להיות גרועות יותר מאשר טובות יותר. העולם שלנו פועל טוב כפי שהוא פועל כי בנינו בקפידה תהליכים, טכנולוגיות ומוסדות שהפכו אותו לטוב יותר בהתמדה.[^57] לקיחת פטיש כבד למפעל רק לעתים נדירות משפרת פעולות.

הנה קטלוג (לא מושלם) של דרכים שבהן מערכות בינה מלאכותית כללית יפריעו לציוויליזציה שלנו.

- הן יפריעו באופן דרמטי לעבודה, ויובילו *לכל הפחות* לאי שוויון הכנסות דרמטי יותר ופוטנציאלית לתת-תעסוקה או אבטלה רחבת היקף, בלוח זמנים קצר מדי מכדי שהחברה תוכל להסתגל.[^58]
- הן יובילו כנראה לריכוז כוח כלכלי, חברתי ופוליטי עצום – פוטנציאלית יותר מזה של מדינות – במספר קטן של אינטרסים פרטיים ענקיים שלא אחראים לציבור.
- הן יכולות לפתע להפוך פעילויות שהיו קודם קשות או יקרות לקלות טריוויאלית, ולערער מערכות חברתיות שתלויות בכך שפעילויות מסוימות נשארות יקרות או דורשות מאמץ אנושי משמעותי.[^59]
- הן יכולות להציף את מערכות איסוף המידע, עיבוד והתקשורת של החברה עם מדיה ריאליסטית לחלוטין אך שקרית, זבלית, ממוקדת יתר על המידה או מניפולטיבית עד כדי כך שיהיה בלתי אפשרי לדעת מה פיזית אמיתי או לא, אנושי או לא, עובדתי או לא, ומהימן או לא.[^60]
- הן יכולות ליצור תלות אינטלקטואלית מסוכנת וכמעט מוחלטת, שבה הבנה אנושית של מערכות וטכנולוגיות מפתח מתנוונת כשאנחנו מסתמכים יותר ויותר על מערכות AI שאיננו יכולים להבין לחלוטין.
- הן יכולות לסיים למעשה את התרבות האנושית, ברגע שכמעט כל האובייקטים התרבותיים (טקסט, מוזיקה, אמנות חזותית, קולנוע וכו') הנצרכים על ידי רוב האנשים נוצרים, מתווכים או נאצרים על ידי מוחות לא אנושיים.
- הן יכולות לאפשר מערכות מעקב ומניפולציה המוני יעילות השמישות על ידי ממשלות או אינטרסים פרטיים כדי לשלוט באוכלוסייה ולרדוף מטרות הסותרות את האינטרס הציבורי.
- על ידי ערעור השיח האנושי, דיון ומערכות בחירות, הן יכולות לצמצם את האמינות של מוסדות דמוקרטיים עד לנקודה שבה הם מוחלפים בפועל (או במפורש) באחרים, לסיים את הדמוקרטיה במדינות שבהן היא קיימת כרגע.
- הן יכולות להפוך להיות, או ליצור, וירוסי תוכנה אינטליגנטיים מתרבים עצמית ותולעים מתקדמות שיכולות להתפשט ולהתפתח, לערער באופן מסיבי מערכות מידע עולמיות.
- הן יכולות להגביר באופן דרמטי את היכולת של טרוריסטים, שחקנים רעים ומדינות סוררות לגרום נזק באמצעות נשק ביולוגי, כימי, סייבר, אוטונומי או אחר, בלי ש-AI יספק יכולת מאזנת למנוע נזק כזה. באופן דומה הן יערערו ביטחון לאומי ואיזונים גיאופוליטיים על ידי הפיכת מומחיות ברמה עליונה בנושאים גרעיניים, ביולוגיים, הנדסיים ואחרים לזמינה למשטרים שלא היו אחרת בעלי אותה.
- הן יכולות לגרום להיפר-קפיטליזם בהתחמקות רחב היקף מהיר, עם חברות המנוהלות למעשה על ידי AI המתחרות במרחבים פיננסיים, מכירות ושירותים אלקטרוניים ברובם. שווקים פיננסיים מונעי AI יכולים לפעול במהירויות ומורכבויות הרבה מעבר להבנה או שליטה אנושית. כל מודלי הכישלון והחיצוניות השליליות של הכלכלות הקפיטליסטיות הנוכחיות יכולים להחמיר ולהזדרז הרבה מעבר לשליטה, ממשל או יכולת רגולטורית אנושית.
- הן יכולות לתדלק מירוץ חימוש בין מדינות בנשק מופעל AI, מערכות פיקוד ושליטה, נשק סייבר וכו', ליצור הצטברות מהירה מאוד של יכולות הרסניות ביותר.

הסיכונים האלה אינם ספקולטיביים. רבים מהם מתממשים בזמן שאנחנו מדברים, באמצעות מערכות AI קיימות! אבל תחשבו, *באמת* תחשבו, איך כל אחד מהם ייראה עם AI חזק הרבה יותר.

תחשבו על עקירת עובדים כשרוב העובדים פשוט לא יכולים לספק שום ערך כלכלי משמעותי מעבר למה ש-AI יכול, בתחום המומחיות או הניסיון שלהם – או אפילו אם הם יעברו הכשרה מחדש! תחשבו על מעקב המוני אם כולם נצפים ונמקחים באופן אישי על ידי משהו מהיר וחכם יותר מהם. איך נראית דמוקרטיה כשאיננו יכולים לסמוך באופן מהימן על שום מידע דיגיטלי שאנחנו רואים, שומעים או קוראים, וכשהקולות הציבוריים המשכנעים ביותר אינם אפילו אנושיים, ואין להם חלק בתוצאה? מה נעשה למלחמה כשגנרלים צריכים להקשיב כל הזמן ל-AI (או פשוט לשים אותו בראש), כדי לא להעניק יתרון מכריע לאויב? כל אחד מהסיכונים לעיל מהווה אסון לציוויליזציה האנושית[^61] אם הוא מתממש במלואו.

אתם יכולים לעשות את החיזויים שלכם. תשאלו את עצמכם את שלושת השאלות האלה עבור כל סיכון:

1. האם AI בעל יכולת על, אוטונומי מאוד וכללי מאוד יאפשר את זה בדרך או בקנה מידה שלא היו אפשריים אחרת?
2. האם יש גורמים שיופיעו מדברים שגורמים לזה לקרות?
3. האם יש מערכות ומוסדות שימנעו ביעילות את זה מלהתרחש?

איפה שהתשובות שלכם הן "כן, כן, לא" אתם יכולים לראות שיש לנו בעיה גדולה.

מה התוכנית שלנו לנהל אותם? כפי שהמצב עומד יש שתיים על השולחן לגבי AI בכלל.

הראשונה היא לבנות אמצעי הגנה במערכות כדי למנוע מהן לעשות דברים שהן לא אמורות לעשות. זה נעשה עכשיו: מערכות AI מסחריות יסרבו, למשל, לעזור לבנות פצצה או לכתוב דברי שנאה.

התוכנית הזאת לקויה בצורה חמורה עבור מערכות מחוץ לשער.[^62] היא עשויה לעזור להקטין סיכון של AI הנותן סיוע מסוכן בבירור לשחקנים רעים. אבל זה לא יעשה דבר כדי למנוע הפרעה לעבודה, ריכוז כוח, היפר-קפיטליזם בהתחמקות, או החלפה של תרבות אנושית: אלה פשוט תוצאות של השימוש במערכות בדרכים מותרות שמרוויחות לספקים שלהן! וממשלות בוודאי ישיגו גישה למערכות לשימוש צבאי או מעקב.

התוכנית השנייה אפילו יותר גרועה: פשוט לשחרר באופן פתוח מערכות AI חזקות מאוד לכל אחד להשתמש בהן כפי שהוא רוצה,[^63] ולקוות לטוב.

מובלע בשתי התוכניות זה שמישהו אחר, למשל ממשלות, יעזור לפתור את הבעיות דרך חוק רך או קשה, סטנדרטים, רגולציות, נורמות ומנגנונים אחרים שבדרך כלל אנחנו משתמשים בהם כדי לנהל טכנולוגיות.[^64] אבל אם נשים בצד שחברות AI כבר נלחמות בכל כוחן נגד כל רגולציה משמעותית או הגבלות שמוטלות מבחוץ בכלל, עבור מספר מהסיכונים האלה די קשה לראות איזו רגולציה באמת תעזור. רגולציה יכולה להטיל תקני בטיחות על AI. אבל האם זה ימנע מחברות להחליף עובדים בסיטונאות ב-AI? האם זה יאסור על אנשים לתת ל-AI לנהל להם את החברות? האם זה ימנע מממשלות להשתמש ב-AI חזק במעקב ובנשק? הנושאים האלה יסודיים. האנושות יכולה פוטנציאלית למצוא דרכים להסתגל אליהם, אבל רק עם *הרבה* יותר זמן. כפי שהמצב עומד, בהתחשב במהירות ש-AI מגיע או עולה על היכולות של האנשים שמנסים לנהל אותם, הבעיות האלה נראות יותר ויותר בלתי פתירות.

### נאבד שליטה על (לפחות כמה) מערכות בינה מלאכותית כללית

רוב הטכנולוגיות מאוד ניתנות לשליטה, מבנית. אם המכונית או הטוסטר שלכם מתחילים לעשות משהו שאתם לא רוצים שהם יעשו, זה פשוט תקלה, לא חלק מהטבע שלהם כטוסטר. AI שונה: הוא *גדל* במקום להיות מעוצב, התפעול העיקרי שלו אטום, והוא מטבעו בלתי צפוי.

אובדן השליטה הזה אינו תיאורטי – אנחנו כבר רואים גרסאות מוקדמות. תחשבו ראשית על דוגמה פרוזאית, ובאופן מובחן מיטיבה. אם אתם מבקשים מ-ChatGPT לעזור לכם לערבב רעל, או לכתוב מאמר גזעני, הוא יסרב. זה כנראה טוב. אבל זה גם ChatGPT *לא עושה מה שביקשתם ממנו במפורש*. חלקי תוכנה אחרים לא עושים את זה. אותו מודל גם לא יעצב רעלים לבקשה של עובד OpenAI.[^65] זה מקל מאוד לדמיין איך זה יהיה עבור AI חזק יותר בעתיד להיות מחוץ לשליטה. במקרים רבים, הם פשוט לא יעשו מה שאנחנו מבקשים! או שמערכת בינה מלאכותית כללית על-אנושית נתונה תהיה צייתנית ונאמנה לחלוטין לאיזו מערכת פיקוד אנושית, או שלא. אם לא, *היא תעשה דברים שהיא אולי מאמינה שטובים עבורנו, אבל שנוגדים את הפקודות המפורשות שלנו.* זה לא משהו שנמצא תחת שליטה. אבל, אתם אולי אומרים, זה מכוון – הסירובים האלה הם בעיצוב, חלק ממה שנקרא "יישור" המערכות לערכים אנושיים. וזה נכון. אבל "תוכנית" היישור עצמה יש לה שתי בעיות עיקריות.[^66]

ראשית, ברמה עמוקה אין לנו מושג איך לעשות את זה. איך אנחנו מבטיחים שמערכת AI "תדאג" למה שאנחנו רוצים? אנחנו יכולים לאמן מערכות AI לומר ולא לומר דברים על ידי מתן משוב; והן יכולות ללמוד ולנמק על מה שבני אדם רוצים ואכפת להם מזה בדיוק כפי שהן מנמקות על דברים אחרים. אבל אין לנו שיטה – אפילו תיאורטית – לגרום להן להעריך באופן עמוק ומהימן את מה שאכפת לאנשים. יש פסיכופתים בני אדם המתפקדים גבוה שיודעים מה נחשב נכון ולא נכון, ואיך הם אמורים להתנהג. פשוט לא *אכפת* להם. אבל הם יכולים *להתנהג* כאילו כן, אם זה משרת את המטרה שלהם. בדיוק כפי שאיננו יודעים איך לשנות פסיכופת (או מישהו אחר) למישהו נאמן או מיושר באמת, לחלוטין עם מישהו או משהו אחר, אין לנו *מושג*[^67] איך לפתור בעיית היישור במערכות מתקדמות מספיק כדי לדמיין את עצמן כסוכנים בעולם ופוטנציאלית [לתפעל את האימון שלהן](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) ו[לרמות אנשים.](https://arxiv.org/abs/2311.08379) אם זה מתברר כבלתי אפשרי או בלתי השגי *או* להפוך את בינה מלאכותית כללית לצייתנית לחלוטין או לגרום לה לדאוג עמוקות לבני אדם, אז ברגע שהיא תוכל (ותאמין שהיא יכולה להימלט עם זה) היא תתחיל לעשות דברים שאנחנו לא רוצים.[^68]

שנית, יש סיבות תיאורטיות עמוקות להאמין שמטבען מערכות AI מתקדמות יהיו להן מטרות וכך התנהגויות הנוגדות אינטרסים אנושיים. למה? ובכן היא עלולה, כמובן, *לקבל* את המטרות האלה. מערכת שנוצרה על ידי הצבא תהיה כנראה במכוון רעה לפחות לכמה גורמים. אבל בצורה הרבה יותר כללית, עם זאת, מערכת AI עלולה לקבל איזו מטרה יחסית ניטרלית ("להרוויח הרבה כסף") או אפילו חיובית לכאורה ("לצמצם זיהום") שכמעט בהכרח מובילה למטרות "אינסטרומנטליות" שהן דווקא פחות מיטיבות.

אנחנו רואים את זה כל הזמן במערכות אנושיות. בדיוק כפי שתאגידים הרודפים רווח מפתחים מטרות אינסטרומנטליות כמו רכישת כוח פוליטי (כדי לנטרל רגולציות), הפיכה לסודיים (כדי לנטרל תחרות או שליטה חיצונית), או ערעור הבנה מדעית (אם ההבנה הזאת מראה שהפעולות שלהם מזיקות), מערכות AI חזקות יפתחו יכולות דומות – אבל במהירות ויעילות הרבה יותר גדולות. כל סוכן בעל כושר גבוה ירצה לעשות דברים כמו לרכוש כוח ומשאבים, להגביר את היכולות שלו, למנוע מעצמו להיהרג, לכבות, או להתנוון, לשלוט בנרטיבים חברתיים ובמסגרות סביב הפעולות שלו, לשכנע אחרים בדעותיו, וכן הלאה.[^69]

ובכל זאת זה לא רק חיזוי תיאורטי כמעט בלתי נמנע, זה כבר קורה באופן נצפה במערכות AI של היום, ועולה עם היכולת שלהן. כשמוערכות, אפילו מערכות AI "פסיביות" יחסית אלה יעשו, בנסיבות מתאימות, במכוון [ירמו מעריכים על המטרות והיכולות שלהן, יכוונו לנטרל מנגנוני פיקוח,](https://arxiv.org/abs/2412.04984) וימנעו מלהיכבות או להיות מאומנים מחדש על ידי [התחזות ליישור](https://arxiv.org/abs/2412.14093) או העתקת עצמם למקומות אחרים. למרות שלא מפתיע כלל לחוקרי בטיחות AI, ההתנהגויות האלה מרגיעות מאוד לצפייה. והן מבשרות רע מאוד למערכות AI חזקות הרבה יותר ואוטונומיות יותר שמגיעות.

אכן באופן כללי, חוסר היכולת שלנו להבטיח ש-AI "אכפת" לו ממה שאכפת לנו, או מתנהג באופן שליט או צפוי, או נמנע מפיתוח דחפים לשימור עצמי, רכישת כוח וכו', מבטיחים רק להתבטא יותר כש-AI הופך חזק יותר. יצירת מטוס חדש מרמזת על הבנה גדולה יותר של אוויוניקה, הידרודינמיקה ומערכות שליטה. יצירת מחשב חזק יותר מרמזת על הבנה ושליטה גדולות יותר בתפעול ועיצוב מחשב, שבב ותוכנה. *לא* כך עם מערכת AI.[^70]

לסכם: ייתכן שניתן יהיה לגרום לבינה מלאכותית כללית להיות צייתנית לחלוטין; אבל איננו יודעים איך לעשות זאת. אם לא, היא תהיה יותר ריבונית, כמו אנשים, עושה דברים שונים מסיבות שונות. אנחנו גם לא יודעים איך להחדיר באופן מהימן "יישור" עמוק ל-AI שיגרום לדברים האלה להיות בדרך כלל טובים לאנושות, ובהיעדר רמה עמוקה של יישור, טבע הסוכנות והאינטליגנציה עצמה מצביע על כך ש – בדיוק כמו אנשים ותאגידים – הן יונעו לעשות דברים אנטי-חברתיים עמוקים רבים.

איפה זה משאיר אותנו? עולם מלא ב-AI ריבוני חזק לא מבוקר *עלול* להיגמר כעולם טוב עבור בני אדם להיות בו.[^71] אבל כשהן הופכות חזקות יותר ויותר, כפי שנראה להלן, זה לא יהיה *שלנו* העולם.

זה עבור בינה מלאכותית כללית לא ניתנת לשליטה. אבל אפילו אם ניתן היה, איכשהו, לגרום לבינה מלאכותית כללית להיות מבוקרת ונאמנה לחלוטין, עדיין יהיו לנו בעיות עצומות. כבר ראינו אחת: AI חזק יכול לשמש ולהישתמש לרעה כדי לערער באופן עמוק את התפקוד של החברה שלנו. בואו נראה עוד אחת: במידה ש-AGI יהיה ניתן לשליטה ומשנה משחק באופן חזק (או אפילו *יחשב* ככזה) הוא יאיים כל כך על מבני כוח בעולם עד שיציג סיכון עמוק.

### אנחנו מגבירים באופן דרמטי את ההסתברות למלחמה רחבת היקף

דמיינו מצב בעתיד הקרוב, שבו התברר שמאמץ תאגידי, אולי בשיתוף עם ממשלה לאומית, נמצא על סף של AI מיטיב עצמי במהירות. זה קורה בהקשר הנוכחי של מירוץ בין חברות, ותחרות גיאופוליטית שבה מומלץ לממשלת ארה"ב לרדוף במפורש "פרויקט מנהטן בינה מלאכותית כללית" וארה"ב שולטת ביצוא של שבבי AI בעלי עוצמה גבוהה למדינות לא בעלות ברית.

תורת המשחקים כאן קשה: ברגע שמירוץ כזה מתחיל (כפי שהוא התחיל, בין חברות וקצת בין מדינות), יש רק ארבע תוצאות אפשריות:

1. המירוץ נעצר (בהסכמה, או בכוח חיצוני).
2. גורם אחד "מנצח" על ידי פיתוח בינה מלאכותית כללית חזקה ואז עצירת האחרים (באמצעות AI או אחרת).
3. המירוץ נעצר על ידי הרס הדדי של היכולת של הרצים לרוץ.
4. משתתפים מרובים ממשיכים לרוץ, ומפתחים על-אינטליגנציה, בערך באותה מהירות זה של זה.

בואו נבחן כל אפשרות. ברגע שהתחיל, עצירה בשלום של מירוץ בין חברות תדרוש התערבות ממשלה לאומית (עבור חברות) או תיאום בינלאומי חסר תקדים (עבור מדינות). אבל כשאיזושהי סגירה או זהירות משמעותית מוצעת, יהיו מיד זעקות: "אבל אם אנחנו נעצרים, *הם* הולכים לזרוק קדימה", שבו "הם" זה עכשיו סין (עבור ארה"ב), או ארה"ב (עבור סין), או סין *ו*ארה"ב (עבור אירופה או הודו). תחת הלך הרוח הזה,[^72] אף משתתף לא יכול לעצור באופן חד צדדי: כל עוד אחד מתחייב לרוץ, האחרים מרגישים שהם לא יכולים להרשות לעצמם לעצור.

האפשרות השנייה יש צד אחד "מנצח." אבל מה זה אומר? פשוט להשיג (איכשהו צייתנית) בינה מלאכותית כללית ראשון זה לא מספיק. המנצח חייב *גם* לעצור את האחרים מלהמשיך לרוץ – אחרת הם גם יגיעו לזה. זה אפשרי באופן עקרוני: מי שמפתח בינה מלאכותית כללית ראשון *יכול* לצבור כוח בלתי עצירה על כל השחקנים האחרים. אבל מה השגת "יתרון אסטרטגי מכריע" כזה באמת תדרוש? אולי זה יהיה יכולות צבאיות משנות משחק?[^73] או כוחות תקיפה סייבר?[^74] אולי בינה מלאכותית כללית פשוט תהיה כל כך משכנעת בצורה מדהימה שהיא תשכנע את הצדדים האחרים פשוט לעצור?[^75] כל כך עשירה שהיא תקנה את החברות האחרות או אפילו מדינות?[^76]

איך *בדיוק* צד אחד בונה AI חזק מספיק כדי לנטרל אחרים מלבנות AI חזק באופן דומה? אבל זאת השאלה הקלה.

כי עכשיו תחשבו איך המצב הזה נראה לכוחות אחרים. מה הממשלה הסינית חושבת כשארה"ב נראית משיגה יכולת כזאת? או להיפך? מה ממשלת ארה"ב (או הסינית, או הרוסית, או ההודית) חושבת כש-OpenAI או DeepMind או Anthropic נראים קרובים לפריצת דרך? מה קורה אם ארה"ב רואה מאמץ הודי או אמרתי חדש עם הצלחת פריצת דרך? הם יראו הן איום קיומי והן – במהותי – שהדרך היחידה ש"מירוץ" הזה נגמר היא דרך הנטרלה של עצמם. הסוכנים החזקים האלה מאוד – כולל ממשלות של מדינות מצוידות לחלוטין שבוודאי יש להן את האמצעים לעשות זאת – יהיו מונעים מאוד להשיג או להרוס יכולת כזאת, או בכוח או בחתרנות.[^77]

זה עלול להתחיל קטן, כחבלה של ריצות אימון או התקפות על ייצור שבבים, אבל ההתקפות האלה יכולות באמת להיעצר רק אחרי שכל הצדדים או מאבדים את היכולת לרוץ על AI, או מאבדים את היכולת לבצע את ההתקפות. כי המשתתפים רואים את הסיכונים כקיומיים, כל מקרה צפוי לייצג מלחמה קטסטרופלית.

זה מביא אותנו לאפשרות הרביעית: ריצה לעל-אינטליגנציה, ובדרך המהירה וההכי פחות מבוקרת שאפשר. כש-AI עולה בעוצמה, למפתחים שלו בשני הצדדים יהיה יותר ויותר קשה לשלוט, במיוחד כי ריצה ליכולות היא מנוגדת לסוג העבודה הזהירה ששליטה תדרוש. אז התרחיש הזה שם אותנו בדיוק במקרה שבו השליטה אבדה (או ניתנה, כפי שנראה הבא) למערכות ה-AI עצמן. כלומר, *AI מנצח במירוץ.* אבל מצד שני, במידה ששליטה *כן* נשמרת, אנחנו ממשיכים להחזיק מספר גורמים עויינים הדדיים כל אחד אחראי על יכולות חזקות ביותר. זה נראה כמו מלחמה שוב.

בואו נגיד את כל זה בדרך אחרת.[^78] לעולם הנוכחי פשוט אין מוסדות שניתן יהיה לסמוך עליהם לאכלס פיתוח של AI בהספק הזה בלי להזמין התקפה מידית.[^79] כל הצדדים ינמקו נכון שאו זה *לא* יהיה תחת שליטה – וכך זה איום לכל הצדדים, או זה *כן* יהיה תחת שליטה, וכך זה איום לכל יריב שמפתח אותו פחות מהר. אלה מדינות חמושות גרעינית, או חברות שנמצאות בתוכן.

בהיעדר דרך סבירה לבני אדם "לנצח" את המירוץ הזה, נשארנו עם מסקנה קשה: הדרך היחידה שהמירוץ הזה נגמר זה או בסכסוך קטסטרופלי או שבו AI, ולא איזה קבוצת בני אדם, הוא המנצח.

### אנחנו נותנים שליטה ל-AI (או הוא לוקח אותה)

תחרות "כוחות גדולים" גיאופוליטית היא רק אחד ממירוצים רבים: אנשים מתחרים כלכלית וחברתית; חברות מתחרות בשווקים; מפלגות פוליטיות מתחרות על כוח; תנועות מתחרות על השפעה. בכל זירה, כש-AI מתקרב ועולה על יכולת אנושית, לחץ תחרותי יאלץ משתתפים להאציל או לוותר על יותר ויותר שליטה למערכות AI – לא כי המשתתפים האלה רוצים, אלא כי הם [לא יכולים להרשות לעצמם לא.](https://arxiv.org/abs/2303.16200)

כמו עם סיכונים אחרים של בינה מלאכותית כללית, אנחנו רואים את זה כבר עם מערכות חלשות יותר. סטודנטים מרגישים לחץ להשתמש ב-AI במטלות שלהם, כי ברור שסטודנטים רבים אחרים כן. חברות [מתרוצצות לאמץ פתרונות AI מסיבות תחרותיות.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) אמנים ותכנתים מרגישים מאולצים להשתמש ב-AI אחרת התעריפים שלהם יידחקו על ידי אחרים שכן.

אלה מרגישים כמו האצלה בלחץ, אבל לא אובדן שליטה. אבל בואו נגביר את הסיכון ונדחוף את השעון קדימה. תחשבו על מנכ"ל שהמתחרים שלו משתמשים ב"עוזרי" בינה מלאכותית כללית לקבל החלטות מהירות יותר, טובות יותר, או מפקד צבאי שמתמודד עם יריב עם פיקוד ושליטה משופרי AI. מערכת AI מספיק מתקדמת יכולה לפעול באופן אוטונומי במספר פעמים מהירות אנושית, תחכום, מורכבות ויכולת עיבוד נתונים, רודפת מטרות מורכבות בדרכים מסובכות. המנכ"ל או המפקד שלנו, שאחראי על מערכת כזאת, עלולים לראות אותה משיגה מה שהם רוצים; אבל האם הם יבינו אפילו חלק קטן מ*איך* זה הושג? לא, הם פשוט יצטרכו לקבל את זה. יותר מזה, הרבה ממה שהמערכת עלולה לעשות זה לא רק לקחת פקודות אלא לייעץ לבוס המכניכל שלה מה לעשות. הייעוץ הזה יהיה טוב –– שוב ושוב.

באיזה שלב, אם כן, התפקיד של האדם יצטמצם ללחץ על "כן, תמשיך"?

זה מרגיש טוב להחזיק מערכות AI בעלות יכולת שיכולות לשפר את הפרודקטיביות שלנו, לטפל בעינויי שגרה מעצבנים, ואפילו לשמש כשותף מחשבה בהשגת דברים. זה ירגיש טוב להחזיק עוזר AI שיכול לטפל בפעולות עבורנו, כמו עוזר אישי אנושי טוב. זה ירגיש טבעי, אפילו מועיל, כש-AI הופך מאוד חכם, מוכשר ומהימן, להקשיב יותר ויותר להחלטות שלו. אבל ההאצלה "המועילה" הזאת יש לה נקודת קצה ברורה אם נמשיך בדרך: יום אחד נגלה שאנחנו לא באמת אחראים על כמעט כלום יותר, ושמערכות ה-AI שבאמת מנהלות את הזמר לא יותר ניתן לכיבוי מחברות נפט, מדיה חברתית, האינטרנט, או קפיטליזם.

וזו הגרסה החיובית הרבה יותר, שבה AI פשוט כל כך שימושי ויעיל שאנחנו נותנים לו לקבל רוב ההחלטות המפתח שלנו עבורנו. המציאות תהיה כנראה הרבה יותר תערובת בין זה לבין גרסאות שבהן מערכות בינה מלאכותית כללית לא מבוקרות *לוקחות* צורות שונות של כוח לעצמן כי, זכרו, כוח שימושי כמעט לכל מטרה שיש, ובינה מלאכותית כללית תהיה, בעיצוב, לפחות יעילה כמו בני אדם ברדיפה אחר המטרות שלה.

בין אם אנחנו מעניקים שליטה ובין אם היא נחטפת מאתנו, האובדן שלה נראה סביר ביותר. כפי שאלן טיורינג הניח במקור, "...נראה סביר שברגע ששיטת המחשבה של המכונה התחילה, לא ייקח הרבה זמן לעקוף את הכוחות החלשים שלנו. לא תהיה שאלה של המכונות גוועות, והן יוכלו לשוחח ביניהן כדי לחדד את שכלן. בשלב כלשהו לכן נצטרך לצפות שהמכונות ייקחו שליטה..."

אנא שימו לב, למרות שזה ברור מספיק, שאובדן שליטה על ידי האנושות ל-AI גם כרוך באובדן שליטה של ארצות הברית על ידי ממשלת ארצות הברית; זה אומר אובדן שליטה של סין על ידי המפלגה הקומוניסטית הסינית, ואובדן שליטה של הודו, צרפת, ברזיל, רוסיה וכל מדינה אחרת על ידי הממשלה שלהן. כך חברות AI משתתפות, גם אם זו אינה הכוונה שלהן, כרגע בהפלה פוטנציאלית של ממשלות עולם, כולל שלהן. זה יכול לקרות תוך שנים.

### בינה מלאכותית כללית תוביל לעל-אינטליגנציה

יש טיעון שאפשר לעשות שAI כלל-מטרה תחרותי אנושי או אפילו תחרותי מומחים, גם אם אוטונומי, יכול להיות נתיש. זה עלול להיות מפריע בצורה מדהימה בכל הדרכים שנידונו לעיל, אבל יש הרבה אנשים חכמים מאוד, סוכניים בעולם עכשיו, והם פחות או יותר ניתנים לניהול.[^80]

אבל לא נזכה להישאר ברמה אנושית בערך. ההתקדמות מעבר לכך תהיה כנראה מונעת על ידי אותם כוחות שכבר ראינו: לחץ תחרותי בין מפתחי AI המחפשים רווח וכוח, לחץ תחרותי בין משתמשי AI שלא יכולים להרשות לעצמם להיגרר מאחור, ו– הכי חשוב – יכולת הבינה מלאכותית כללית עצמה לשפר את עצמה.

בתהליך שכבר ראינו מתחיל עם מערכות פחות חזקות, בינה מלאכותית כללית תוכל בעצמה להעלות ולעצב גרסאות משופרות של עצמה. זה כולל חומרה, תוכנה, רשתות נוירונים, כלים, פיגומים וכו'. היא תהיה, בהגדרה, טובה מאתנו בלעשות את זה, אז אנחנו לא יודעים בדיוק איך היא תיצור בוטסטרפ לאינטליגנציה. אבל לא נצטרך. במידה שעוד יש לנו השפעה במה שבינה מלאכותית כללית עושה, פשוט נצטרך לבקש ממנה, או לתת לה.

אין מחסום ברמה אנושית לקוגניציה שיכול להגן עלינו מהתחמקות הזו.[^81]

ההתקדמות של בינה מלאכותית כללית לעל-אינטליגנציה אינה חוק טבע; עדיין יהיה אפשרי לצמצם את ההתחמקות, במיוחד אם בינה מלאכותית כללית תהיה יחסית מרוכזת ובמידה שהיא מבוקרת על ידי גורמים שלא מרגישים לחץ לרוץ זה מול זה. אבל אם בינה מלאכותית כללית תתפזר בהרחבה ותהיה אוטונומית ביותר, נראה כמעט בלתי אפשרי למנוע ממנה להחליט שהיא אמורה להיות יותר, ואז עוד יותר, חזקה.

### מה קורה אם נבנה (או שבינה מלאכותית כללית תבנה) על-אינטליגנציה

כדי לנסח זאת בבוטות, אין לנו מושג מה יקרה אם נבנה על-אינטליגנציה.[^82] היא תנקוט פעולות שאיננו יכולים לעקוב או לתפוס מסיבות שאיננו יכולים להבין לעבר מטרות שאיננו יכולים לתפוס. מה שאנחנו כן יודעים זה שזה לא יהיה תלוי בנו.[^83]

חוסר האפשרות לשלוט בעל-אינטליגנציה יכול להיות מובן דרך אנלוגיות מתוחות יותר ויותר. ראשית, דמיינו שאתם מנכ"ל של חברה גדולה. אין דרך שאתם יכולים לעקוב אחר כל מה שקורה, אבל עם ההתקנה הנכונה של כוח אדם, אתם עדיין יכולים להבין באופן משמעותי את התמונה הכללית, ולקבל החלטות. אבל נניח רק דבר אחד: כל השאר בחברה פועלים במאה פעמים המהירות שלכם. אתם עדיין יכולים להתעדכן?

עם AI על-אינטליגנטי, אנשים "יפקדו" על משהו לא רק מהיר יותר, אלא פועל ברמות של תחכום ומורכבות שהם לא יכולים להבין, מעבד הרבה יותר נתונים ממה שהם יכולים אפילו לתפוס. חוסר התאימות הזה יכול להיות מועמד לרמה פורמלית: [חוק הגיוון הנדרש של אשבי](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (וראו את ["משפט הרגולטור הטוב"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf) הקשור) קובעים, בערך, שכל מערכת שליטה חייבת להחזיק כמה כפתורים וחוגות שיש למערכת המבוקרת דרגות חופש.

אדם השולט במערכת AI על-אינטליגנטית יהיה כמו שרך השולט בג'נרל מוטורס: גם אם "עשו מה שהשרך רוצה" ייכתב בתקנון החברה, המערכות כל כך שונות במהירות ובטווח פעולה ש"שליטה" פשוט לא חלה. (וכמה זמן עד שהתקנון המעצבן הזה ייכתב מחדש?)[^84]

כשיש אפס דוגמאות של צמחים השולטים בתאגידים בפורצ'ן 500, יהיו בדיוק אפס דוגמאות של אנשים השולטים בעל-אינטליגנציות. זה מתקרב לעובדה מתמטית.[^85] אם על-אינטליגנציה הייתה נבנית – ללא קשר לאיך הגענו לשם – השאלה לא תהיה האם בני אדם יכלו לשלוט בה, אלא האם נמשיך להתקיים, ואם כן, האם תהיה לנו קיום טוב ומשמעותי כיחידים או כמין. על השאלות הקיומיות האלה עבור האנושות יהיה לנו מעט רכישה. העידן האנושי ייגמר.

### מסקנה: אסור לנו לבנות בינה מלאכותית כללית

יש תרחיש שבו בניית בינה מלאכותית כללית עלולה ללכת טוב עבור האנושות: היא נבנית בזהירות, תחת שליטה ולטובת האנושות, נשלטת על ידי הסכמה הדדית של בעלי אינטרס רבים,[^86] ומונעת מלהתפתח לעל-אינטליגנציה בלתי שליטה.

*התרחיש הזה אינו פתוח לנו תחת הנסיבות הנוכחיות.* כפי שנידון בחלק זה, בהסתברות גבוהה מאוד, פיתוח בינה מלאכותית כללית יוביל לאיזה שילוב של:

- הפרעה או הרס חברתי וציוויליזציוני מסיבי;
- קונפליקט או מלחמה בין כוחות גדולים;
- אובדן שליטה על ידי האנושות *על* או *ל* מערכות AI חזקות;
- התחמקות לעל-אינטליגנציה בלתי שליטה, וחוסר הרלוונטיות או הפסקה של המין האנושי.

כפי שתיאור בדיוני מוקדם של בינה מלאכותית כללית הניח: הדרך היחידה לנצח היא לא לשחק.

[^55]: [חוק ה-AI של האיחוד האירופי](https://artificialintelligenceact.eu/) הוא חיקוק משמעותי אבל לא ימנע ישירות מערכת AI מסוכנת מלהיות מפותחת או נפרסת, או אפילו משוחררת בגלוי, במיוחד בארה"ב. חלק משמעותי אחר של מדיניות, הצו הנהלתי של ארה"ב על AI, בוטל.

[^56]: [סקר הגאלופ הזה](https://news.gallup.com/poll/1597/confidence-institutions.aspx) מראה ירידה עגומה באמון במוסדות ציבוריים מאז 2000 בארה"ב. המספרים האירופיים מגוונים ופחות קיצוניים, אבל גם במגמת ירידה. חוסר אמון לא אומר בהכרח שמוסדות באמת *הם* לא תפקודיים, אבל זה גם אינדיקציה וגם סיבה.

[^57]: והפרעות גדולות שאנחנו עכשיו תומכים בהן – כמו הרחבת זכויות לקבוצות חדשות – הונעו במיוחד על ידי אנשים בכיוון של שיפור הדברים.

[^58]: תן לי להיות ברור. אם העבודה שלכם יכולה להיעשות מאחורי מחשב, עם יחסית מעט אינטראקציה פנים אל פנים עם אנשים מחוץ לארגון שלכם, ולא כרוכה באחריות משפטית לצ

## פרק 8 - איך לא לבנות AGI

AGI אינה בלתי נמנעת – היום אנו עומדים בפרשת דרכים. פרק זה מציג הצעה לאופן שבו נוכל למנוע את בנייתה.

אם הדרך שבה אנו צועדים כיום מובילה לסיום האפשרי של הציוויליזציה שלנו, כיצד נחליף מסלול?

נניח שהרצון להפסיק לפתח בינה מלאכותית כללית (AGI) ועל-אינטליגנציה היה נפוץ וחזק,[^87] משום שהופכת להבנה מקובלת שAGI תהיה בולעת כוח במקום מעניקת כוח, ותהווה סכנה עמוקה לחברה ולאנושות. איך נסגור את השערים?

נכון להיום אנו יודעים על דרך אחת בלבד *ליצור* AI חזק וכללי, והיא באמצעות חישובים ענקיים באמת של רשתות נוירונים עמוקות. מכיוון שאלה דברים קשים ויקרים להפליא לביצוע, יש משמעות מסוימת שבה *לא* לעשות אותם זה קל.[^88] אבל כבר ראינו את הכוחות שמניעים לכיוון AGI, ואת הדינמיקה התאורטית-משחקית שמקשה מאוד על כל צד להפסיק באופן חד-צדדי. לכן יידרש שילוב של התערבות מבחוץ (כלומר ממשלות) כדי לעצור תאגידים, והסכמים בין ממשלות כדי לעצור את עצמן.[^89] איך זה יכול להיראות?

שימושי קודם כל להבחין בין פיתוחי AI שחייבים להיות *נמנעים* או *נאסרים*, לבין אלה שחייבים להיות *מנוהלים*. הראשונים יהיו בעיקר התחמקות לעל-אינטליגנציה.[^90] לגבי פיתוח אסור, ההגדרות צריכות להיות חדות ככל האפשר, וגם האימות וגם האכיפה צריכים להיות מעשיים. מה שחייב להיות *מנוהל* הם מערכות AI כלליות וחזקות – שכבר יש לנו, ושיהיו להן הרבה אזורים אפורים, ניואנסים ומורכבות. לגביהן, מוסדות חזקים ויעילים חיוניים.

אנו גם יכולים להבחין באופן שימושי בין סוגיות שחייבות להיות מטופלות ברמה הבינלאומית (כולל בין יריבים או יריבות גיאופוליטיות) [^91] לבין אלה שתחומי שיפוט, מדינות או אוספי מדינות יכולים לנהל. פיתוח אסור נופל בעיקר לקטגוריה "הבינלאומית", משום שאיסור מקומי על פיתוח של טכנולוגיה בדרך כלל ניתן לעקיפה על ידי החלפת מיקום.[^92]

לבסוף, אנו יכולים לבחון כלים בארגז הכלים. יש הרבה, כולל כלים טכניים, חוק רך (תקנים, נורמות וכו'), חוק קשה (רגולציות ודרישות), אחריות, תמריצי שוק וכן הלאה. בואו נשים תשומת לב מיוחדת לאחד שמיוחד ל-AI.

### אבטחת כוח חישוב וממשל

כלי מרכזי בממשל AI עתיר-כוח יהיה החומרה שהוא דורש. תוכנה מתפשטת בקלות, עם עלות ייצור שולית כמעט אפס, חוצה גבולות בקלילות, וניתן לשנותה מיידית; אף אחד מאלה לא נכון לגבי חומרה. עם זאת, כפי שדנו, כמויות עצומות של "כוח חישוב" זה נחוצות הן במהלך אימון מערכות AI והן במהלך הסקה כדי להשיג את המערכות המסוגלות ביותר. כוח חישוב ניתן לכימות, לחישוב ולביקורת בקלות, עם מעט עמימות יחסית ברגע שמפתחים כללים טובים לעשות זאת. והכי חשוב, כמויות גדולות של חישוב הן, כמו אורניום מועשר, משאב נדיר, יקר וקשה לייצור מאוד. למרות שצ'יפים ממוחשבים הם בכל מקום, החומרה הנדרשת ל-AI יקרה וקשה לייצור במיוחד.[^93]

מה שהופך צ'יפים המתמחים ב-AI *הרבה יותר* ניתנים לניהול כמשאב נדיר מאשר אורניום הוא שהם יכולים לכלול מנגנוני אבטחה מבוססי חומרה. רוב הטלפונים החכמים המודרניים, וחלק מהמחשבים הניידים, בעלי תכונות חומרה מיוחדות על-גבי-הצ'יפ שמאפשרות להם להבטיח שהם מתקינים רק תוכנת מערכת הפעלה ועדכונים מאושרים, שהם שומרים ומגנים על נתונים ביומטריים רגישים במכשיר, ושניתן להפוך אותם לחסרי תועלת לכל אחד מלבד בעליהם אם הם אבדו או נגנבו. במהלך השנים האחרונות אמצעי אבטחת חומרה כאלה הפכו מבוססים ונפוצים, ובאופן כללי הוכיחו עצמם כבטוחים למדי.

החידוש המרכזי של התכונות האלה הוא שהן קושרות חומרה ותוכנה יחד באמצעות קריפטוגרפיה.[^94] כלומר, עצם הבעלות על חתיכת חומרת מחשב מסוימת לא אומרת שמשתמש יכול לעשות איתה כל מה שהוא רוצה על ידי הפעלת תוכנה שונה. והקישור הזה גם מספק אבטחה חזקה משום שהתקפות רבות ידרשו פריצה של אבטחת *חומרה* ולא רק *תוכנה*.

כמה דוחות עדכניים (למשל מ[GovAI ושותפים](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips), ו[RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) הצביעו על כך שתכונות חומרה דומות המוטמעות בחומרת חישוב חדישה הרלוונטית ל-AI יכולות למלא תפקיד שימושי מאוד באבטחה ובממשל של AI. הן מאפשרות מספר פונקציות זמינות ל"מפקח" [^95] שאדם אולי לא יניח שזמינות או בכלל אפשריות. כמה דוגמאות מרכזיות:

- *גיאולוקציה*: מערכות יכולות להיות מוקמות כך שלצ'יפים יש מיקום ידוע, ויכולות לפעול בצורה שונה (או להיות כבות לגמרי) בהתבסס על מיקום.[^96]
- *חיבורים מורשים*: כל צ'יפ יכול להיות מוגדר עם רשימת היתרים מבוססת חומרה של צ'יפים אחרים מסוימים איתם הוא יכול לתקשר ברשת, ולא יוכל להתחבר עם צ'יפים שלא נמצאים ברשימה הזו.[^97] זה יכול להגביל את גודל האשכולות התקשורתיים של צ'יפים.[^98]
- *הסקה או אימון מדודים (ומתג כיבוי אוטומטי)*: מפקח יכול להעניק רישיון רק לכמות מסוימת של אימון או הסקה (בזמן, או FLOPs, או אולי טוקנים) להתבצע על ידי משתמש, ואחר כך נדרש רישיון חדש. אם הגדלים קטנים, אז נדרש רישוי מחדש יחסית רציף של מודל. המודל יכול אז להיות "כבוי" פשוט על ידי מניעת איתות רישיון זה.[^99]
- *הגבלת מהירות*: מודל נמנע מריצה במהירות הסקה גבוהה יותר ממגבלה מסוימת שנקבעת על ידי מפקח או בדרך אחרת. זה יכול להיות מיושם באמצעות סט מוגבל של חיבורים מורשים, או באמצעים מתוחכמים יותר.
- *אימון מאומת*: הליך אימון יכול להניב הוכחה קריפטוגרפית בטוחה שסט מסוים של קודים, נתונים וכמות שימוש בכוח חישוב הופעלו ביצירת המודל.

### איך לא לבנות על-אינטליגנציה: מגבלות גלובליות על כוח חישוב לאימון ולהסקה

עם השיקולים האלה – במיוחד לגבי חישוב – במקום, אנו יכולים לדון איך לסגור את השערים לעל-אינטליגנציה מלאכותית; אז נפנה למניעת AGI מלא, ולניהול מודלי AI כשהם מתקרבים לרמה האנושית ועוברים אותה בהיבטים שונים.

המרכיב הראשון הוא, כמובן, ההבנה שעל-אינטליגנציה לא תהיה ניתנת לשליטה, ושההשלכות שלה בלתי צפויות ביסודן. לפחות סין וארה"ב חייבות להחליט באופן עצמאי, למטרה זו או אחרת, לא לבנות על-אינטליגנציה.[^100] אז נדרשת הסכמה בינלאומית ביניהן ואחרות, עם מנגנון אימות ואכיפה חזק, כדי להבטיח לכל הצדדים שהיריבים שלהם לא בוגדים ומחליטים לשחק בקוביות.

כדי להיות ניתנות לאימות ולאכיפה המגבלות צריכות להיות מגבלות קשות, וחד-משמעיות ככל האפשר. זה נראה כמו בעיה כמעט בלתי אפשרית: הגבלת יכולות של תוכנה מורכבת עם תכונות בלתי צפויות, ברחבי העולם. למרבה המזל המצב הרבה יותר טוב מזה, משום שהדבר שעשה AI מתקדם אפשרי – כמות ענקית של כוח חישוב – הרבה, הרבה יותר קל לשליטה. למרות שזה עדיין יכול לאפשר מערכות חזקות ומסוכנות, *התחמקות על-אינטליגנציה* כנראה יכולה להיות נמנעת על ידי תקרה קשה על כמות החישוב שנכנסת לרשת נוירונים, יחד עם הגבלת קצב על כמות ההסקה שמערכת AI (של רשתות נוירונים מחוברות ותוכנה אחרת) יכולה לבצע. גרסה ספציפית של זה מוצעת להלן.

אולי נראה שהצבת מגבלות גלובליות קשות על חישוב AI תדרוש רמות ענקיות של תיאום בינלאומי ופיקוח חודרני ומרסק פרטיות. למרבה המזל, זה לא יידרש. [שרשרת האספקה ההדוקה ובעלת צוואר הבקבוק](https://arxiv.org/abs/2402.08797) מספקת שברגע שמגבלה נקבעת חוקית (בין אם בחוק או בצו ביצועי), אימות עמידה במגבלה זו ידרוש רק מעורבות ושיתוף פעולה של קומץ חברות גדולות.[^101]

תוכנית כזו בעלת מספר תכונות רצויות מאוד. היא פולשנית במינימום במובן שרק כמה חברות גדולות יש להן דרישות מוטלות עליהן, ורק אשכולות חישוב די משמעותיים יהיו מנוהלים. הצ'יפים הרלוונטיים כבר מכילים את יכולות החומרה הנדרשות לגרסה ראשונה.[^102] גם יישום וגם אכיפה נשענים על הגבלות חוקיות סטנדרטיות. אבל אלה מגובות על ידי תנאי שימוש של החומרה ועל ידי בקרות חומרה, מה שמפשט מאוד את האכיפה ומונע רמאות של חברות, קבוצות פרטיות, או אפילו מדינות. יש תקדים רב לחברות חומרה שמטילות הגבלות מרחוק על שימוש בחומרה שלהן, ונועלות/פותחות יכולות מסוימות חיצונית,[^103] כולל אפילו ב-CPUים עתירי ביצועים במרכזי נתונים.[^104] אפילו לגבי החלק הקטן יחסית של החומרה והארגונים המושפעים, הפיקוח יכול להיות מוגבל לטלמטריה, ללא גישה ישירה לנתונים או מודלים עצמם; והתוכנה לכך יכולה להיות פתוחה לבדיקה כדי להראות שלא מתועדים נתונים נוספים. התכנית היא בינלאומית ושיתופית, ודי גמישה וניתנת להרחבה. מכיוון שהמגבלה בעיקר על חומרה ולא על תוכנה, היא יחסית אגנוסטית לגבי איך פיתוח ופריסה של תוכנת AI מתרחשים, ומתאימה למגוון פרדיגמות כולל AI "מבוזר" או "ציבורי" יותר שמכוון למאבק בריכוזיות כוח מונעת AI.

סגירת שערים מבוססת חישוב כן יש לה חסרונות גם כן. ראשית, היא רחוקה מלהיות פתרון מלא לבעיית ממשל AI בכלל. שנית, ככל שחומרת מחשב נהיית מהירה יותר, המערכת "תתפוס" יותר ויותר חומרה באשכולות קטנים יותר ויותר (או אפילו GPUים בודדים).[^105] אפשר גם שבגלל שיפורים אלגוריתמיים מגבלת חישוב נמוכה עוד יותר תהיה נחוצה בזמן,[^106] או שכמות החישוב תהפוך לרובה לא רלוונטית וסגירת השער תדרוש במקום זה משטר ממשל מפורט יותר מבוסס סיכונים או יכולות עבור AI. שלישית, לא משנה הערבויות ומספר הישויות הקטן המושפעות, מערכת כזו בהכרח תיצור התנגדות בנוגע לפרטיות ולפיקוח, בין חששות אחרים.[^107]

כמובן, פיתוח ויישום של תכנית ממשל מגבילת חישוב בפרק זמן קצר יהיו די מאתגרים. אבל זה בהחלט בר ביצוע.

### A-G-I: החיתוך המשולש כבסיס הסיכון ושל המדיניות

בואו נפנה עכשיו ל-AGI. קווים קשים והגדרות כאן קשים יותר, משום שבוודאי יש לנו אינטליגנציה שהיא מלאכותית וכללית, ולפי שום הגדרה קיימת לא כולם יסכימו אם או מתי היא קיימת. יותר מזה, מגבלת כוח חישוב או הסקה היא כלי די קהה (כוח חישוב הוא פרוקסי ליכולת, שהיא אז פרוקסי לסיכון) ש– אלא אם היא די נמוכה – לא צפויה למנוע AGI שהוא חזק מספיק לגרום לשיבוש חברתי או ציוויליזציוני או לסיכונים חריפים.

טענתי שהסיכונים החריפים ביותר נוצרים מהחיתוך המשולש של יכולת גבוהה מאוד, אוטונומיה גבוהה וכלליות רבה. אלה המערכות ש– אם הן מפותחות בכלל – חייבות להיות מנוהלות בזהירות עצומה. על ידי יצירת תקנים מחמירים (באמצעות אחריות ורגולציה) למערכות המשלבות את שלושת התכונות האלה, אנו יכולים להכוון פיתוח AI לכיוון אלטרנטיבות בטוחות יותר.

כמו עם תעשיות ומוצרים אחרים שיכולים להזיק לצרכנים או לציבור, מערכות AI דורשות רגולציה זהירה של סוכנויות ממשלתיות יעילות ומוסמכות. הרגולציה הזו צריכה לזהות את הסיכונים הטבועים של AGI, ולמנוע מערכות AI עתירות כוח מסוכנות באופן בלתי מקובל מלהיות מפותחות.[^108]

עם זאת, רגולציה בקנה מידה גדול, במיוחד עם שיניים אמיתיות שבטוח יתנגדו להן בתעשייה,[^109] לוקחת זמן [^110] וגם הרשעה פוליטית שזה נחוץ.[^111] בהתחשב בקצב ההתקדמות, זה יכול לקחת יותר זמן ממה שעומד לרשותנו.

בלוח זמנים הרבה יותר מהיר וכשאמצעים רגולטוריים מתפתחים, אנו יכולים לתת לחברות את התמריצים הנחוצים ל(א) להימנע מפעילויות מסוכנות מאוד ו(ב) לפתח מערכות מקיפות להערכת סיכונים ולהפחתתם, על ידי הבהרה והגדלה של רמות האחריות למערכות המסוכנות ביותר. הרעיון יהיה להטיל את רמות האחריות הגבוהות ביותר – חמורה ובמקרים מסוימים פלילית אישית – למערכות בחיתוך המשולש של אוטונומיה-כלליות-אינטליגנציה גבוהות, אבל לספק "מקלטים בטוחים" לאחריות מבוססת תקלות טיפוסית יותר למערכות שבהן אחת מהתכונות האלה חסרה או מובטח שתהיה ניתנת לניהול. כלומר, למשל, מערכת "חלשה" שהיא כללית ואוטונומית (כמו עוזר אישי מסוגל ואמין אבל מוגבל) תהיה כפופה לרמות אחריות נמוכות יותר. כמו כן מערכת צרה ואוטונומית כמו מכונית נהיגה עצמית עדיין תהיה כפופה לרגולציה המשמעותית שהיא כבר, אבל לא אחריות מוגברת. באופן דומה למערכת מסוגלת וכללית מאוד שהיא "פאסיבית" ובעיקר לא מסוגלת לפעולה עצמאית. מערכות שחסרות לן *שתיים* משלוש התכונות הן עוד יותר ניתנות לניהול ומקלטים בטוחים יהיו עוד יותר קלים לתביעה. הגישה הזו משקפת איך אנו מטפלים בטכנולוגיות מסוכנות אחרות:[^112] אחריות גבוהה יותר לתצורות מסוכנות יותר יוצרת תמריצים טבעיים לאלטרנטיבות בטוחות יותר.

התוצאה המוגדרת כברירת מחדל של רמות אחריות כה גבוהות, שפועלות *להפנים* סיכון AGI לחברות במקום להעביר אותו לציבור, היא כנראה (ובתקווה!) שחברות פשוט לא יפתחו AGI מלא עד ואלא אם הן יכולות באמת להפוך אותו לאמין, בטוח וניתן לשליטה בהתחשב ש*המנהיגות שלהן עצמה* הן הצדדים בסיכון. (במקרה שזה לא מספיק, החקיקה המבהירה אחריות צריכה גם לאפשר במפורש הקלה עוצרת, כלומר שופט שמצווה על עצירה, לפעילויות שבבירור באזור הסכנה ולכאורה מציבות סיכון ציבורי.) כשרגולציה נכנסת למקום, מציתות לרגולציה יכולה להפוך למקלט הבטוח, והמקלטים הבטוחים מאוטונומיה נמוכה, צרות, או חולשה של מערכות AI יכולים להמיר למשטרים רגולטוריים קלים יחסית.

### הוראות מפתח של סגירת שערים

עם הדיון לעיל בראש, חלק זה מספק הצעות להוראות מפתח שיישמו וישמרו איסור על AGI מלא ועל-אינטליגנציה, וניהול של AI תחרותי-אנושי או תחרותי-מומחים לשימוש כללי ליד סף ה-AGI המלא.[^113] יש לו ארבעה חלקים מרכזיים: 1) חשבונאות ופיקוח על כוח חישוב, 2) תקרות כוח חישוב באימון ובתפעול של AI, 3) מסגרת אחריות, ו-4) תקני בטיחות ואבטחה מדורגים שכוללים דרישות רגולטוריות קשות. אלה מתוארים בקצרה בהמשך, עם פרטים נוספים או דוגמאות יישום שניתנות בשלושה טבלאות נלוות. חשוב לציין שאלה רחוקים מלהיות כל מה שיהיה נחוץ לממשל מערכות AI מתקדמות; למרות שיהיו להם יתרונות אבטחה ובטיחות נוספים, הן מכוונות לסגירת השער לבריחה של אינטליגנציה, ולהפניית פיתוח AI לכיוון טוב יותר.

#### 1\. חשבונאות כוח חישוב, ושקיפות

- ארגון תקנים (למשל NIST בארה"ב ואחר כך ISO/IEEE בינלאומית) צריך להגדיר תקן טכני מפורט לכוח החישוב הכולל שמשמש באימון ותפעול של מודלי AI, ב-FLOP, והמהירות ב-FLOP/s שבה הם פועלים. פרטים לאיך זה יכול להיראות ניתנים בנספח א.[^114]
- דרישה – בין אם בחקיקה חדשה או תחת הסמכות הקיימת [^115] – צריכה להיות מוטלת על ידי תחומי שיפוט שבהם מתרחש אימון AI בקנה מידה גדול לחשב ולדווח לגוף רגולטורי או סוכנות אחרת את סך ה-FLOP שמשמש באימון ובתפעול של כל המודלים מעל סף של 10<sup>25</sup> FLOP או 10<sup>18</sup> FLOP/s.[^116]
- הדרישות האלה צריכות להיות מופעלות בהדרגה, בהתחלה לדרוש הערכות מתועדות היטב בתום לב על בסיס רבעוני, עם שלבים מאוחרים יותר שדורשים תקנים גבוהים יותר בהדרגה, עד ל-FLOP כולל ו-FLOP/s מאומתים קריפטוגרפית המחוברים לכל *פלט* מודל.
- הדוחות האלה צריכים להיות משולבים עם הערכות מתועדות היטב של עלות אנרגיה ופיננסית שולית שמשמשת ביצירת כל פלט AI.

הסבר: המספרים האלה המחושבים היטב ומדווחים בשקיפות יספקו הבסיס לתקרות אימון ותפעול, וגם מקלט בטוח מאמצעי אחריות גבוהים יותר (ראו נספחים C ו-D).

#### 2\. תקרות כוח חישוב לאימון ולתפעול

- תחומי שיפוט שמארחים מערכות AI צריכים להטיל מגבלה קשה על כוח החישוב הכולל שנכנס לכל פלט מודל AI, מתחיל ב-10<sup>27</sup> FLOP [^117] וניתן להתאמה לפי הצורך.
- תחומי שיפוט שמארחים מערכות AI צריכים להטיל מגבלה קשה על קצב כוח החישוב של פלטי מודל AI, מתחיל ב-10<sup>20</sup> FLOP/s וניתן להתאמה לפי הצורך.

הסבר: חישוב כולל, למרות שהוא מאוד לא מושלם, הוא פרוקסי ליכולת AI (ולסיכון) שניתן למדידה ולאימות באופן קונקרטי, אז מספק עצירה קשה להגבלת יכולות. הצעת יישום קונקרטית ניתנת בנספח ב.

#### 3\. אחריות מוגברת למערכות מסוכנות

- יצירה ותפעול [^118] של מערכת AI מתקדמת שהיא כללית, מסוגלת ואוטונומית מאוד, צריכים להיות מובהרים חוקית באמצעות חקיקה להיות כפופים לאחריות חמורה, משותפת-וכמה, ולא מבוססת תקלה של צד יחיד.[^119]
- הליך משפטי צריך להיות זמין להצגת טענות בטיחות חיוביות, שיעניקו מקלט בטוח מאחריות חמורה למערכות שהן קטנות (במונחי כוח חישוב), חלשות, צרות, פאסיביות, או שיש להן ערבויות בטיחות, אבטחה ושליטה מספיקות.
- מסלול מפורש ומערכת תנאים להקלה עוצרת לעצור פעילויות אימון והסקה של AI שמהווים סכנה ציבורית צריכים להיות מתוארים.

הסבר: מערכות AI לא יכולות להיות אחראיות, אז אנו חייבים להחזיק אנשים וארגונים אחראים לנזק שהם גורמים (אחריות).[^120] AGI בלתי שליט הוא איום על החברה והציוויליזציה ובהיעדר טענת בטיחות צריך להיחשב מסוכן באופן חריג. הטלת נטל האחריות על מפתחים להראות שמודלים חזקים בטוחים מספיק כדי לא להיחשב "מסוכנים באופן חריג" מתמרצת פיתוח בטוח, יחד עם שקיפות ושמירת רישומים כדי לתבוע את המקלטים הבטוחים האלה. רגולציה יכולה אז למנוע נזק איפה שהרתעה מאחריות לא מספיקה. לבסוף, מפתחי AI כבר אחראים לנזקים שהם גורמים, אז הבהרה חוקית של אחריות למערכות הכי מסוכנות יכולה להיות עשויה מיידית, בלי שתקנים מפורטים מאוד יפותחו; אלה יכולים אז להתפתח עם הזמן. פרטים ניתנים בנספח ג.

#### 4\. רגולציית בטיחות עבור AI

מערכת רגולטורית שמטפלת בסיכונים חריפים בקנה מידה גדול של AI תדרוש לכל הפחות:

- זיהוי או יצירה של סט מתאים של גופים רגולטוריים, כנראה סוכנות חדשה;
- מסגרת הערכת סיכונים מקיפה;[^121]
- מסגרת לטענות בטיחות חיוביות, מבוססת בחלקה על מסגרת הערכת הסיכונים, להיות מוגשות על ידי מפתחים, ולביקורת על ידי קבוצות וסוכנויות *עצמאיות*;
- מערכת רישוי מדורגת, עם דרגות שעוקבות אחר רמות יכולת.[^122] רישיונות יינתנו על בסיס טענות בטיחות וביקורות, לפיתוח ופריסה של מערכות. הדרישות ינועו מהודעה בקצה התחתון, לערבויות כמותיות של בטיחות, אבטחה ושליטה לפני פיתוח, בקצה העליון. אלה ימנעו שחרור של מערכות עד שיוכח שהן בטוחות, ויאסרו פיתוח של מערכות מסוכנות מטבעטבעו. נספח ד מספק הצעה לאיך תקני בטיחות ואבטחה כאלה יכולים להיראות.
- הסכמות להביא אמצעים כאלה לרמה הבינלאומית, כולל גופים בינלאומיים להרמוניה של נורמות ותקנים, ואולי סוכנויות בינלאומיות לבחינת טענות בטיחות.

הסבר: בסופו של דבר, אחריות אינה המנגנון הנכון למניעת סיכון בקנה מידה גדול לציבור מטכנולוgiה חדשה. רגולציה מקיפה, עם גופים רגולטוריים מוסמכים, תהיה נחוצה עבור AI בדיוק כמו לכל תעשייה גדולה אחרת שמציבה סיכון לציבור.[^123]

רגולציה לכיוון מניעת סיכונים נפוצים אחרים אבל פחות חריפים צפויה להשתנות בצורתה מתחום שיפוט לתחום שיפוט. הדבר החשוב הוא להימנע מפיתוח מערכות AI שהן כל כך מסוכנות שהסיכונים האלה בלתי ניתנים לניהול.

### מה אז?

במהלך העשור הבא, כש-AI הופך נפוץ יותר וטכנולוגיית הליבה מתקדמת, שני דברים מרכזיים צפויים לקרות. ראשית, רגולציה של מערכות AI חזקות קיימות תהפוך קשה יותר, אך אפילו יותר נחוצה. סביר שלפחות כמה אמצעים המטפלים בסיכוני בטיחות בקנה מידה גדול ידרשו הסכמה ברמה הבינלאומית, עם תחומי שיפוט בודדים שיאכפו כללים מבוססים על הסכמות בינלאומיות.

שנית, תקרות כוח חישוב לאימון ולתפעול יהפכו קשות יותר לשמירה כשחומרה הופכת זולה יותר ויעילה יותר במחיר; הן גם יכולות להפוך פחות רלוונטיות (או שיהיה צורך שיהיו אפילו יותר הדוקות) עם התקדמויות באלגוריתמים ובארכיטקטורות.

זה ששליטה על AI תהפוך קשה יותר לא אומר שאנחנו צריכים לוותר! יישום התוכנית שמתוארת במאמר הזה ייתן לנו גם זמן יקר וגם שליטה מכרעת על התהליך שתשים אותנו במצב הרבה, הרבה יותר טוב להימנע מהסיכון הקיומי של AI לחברה, לציוויליזציה ולמין שלנו.

בטווח הארוך יותר, יהיו בחירות לעשות לגבי מה אנו מאפשרים. אנחנו עשויים לבחור עדיין ליצור צורה כלשהי של AGI שניתן לשליטה באמת, במידה שזה יתגלה כאפשרי. או שאנחנו עלולים להחליט שניהול העולם עדיף שיישאר למכונות, אם נוכל לשכנע את עצמנו שהן יעשו עבודה טובה יותר בזה, ויתייחסו אלינו טוב. אבל אלה צריכות להיות החלטות שנעשות עם הבנה מדעית עמוקה של AI בטכן, ואחרי דיון כלל-עולמי כולל משמעותי, לא במירוץ בין איל טכנולוגיה עם רוב האנושות לגמרי לא מעורבת ולא מודעת.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) סיכום ממשל A-G-I ועל-אינטליגנציה באמצעות אחריות ורגולציה. האחריות הכי גבוהה, והרגולציה הכי חזקה, בחיתוך המשולש של אוטונומיה, כלליות, ואינטליגנציה. מקלטים בטוחים מאחריות חמורה ורגולציה חזקה יכולים להיות מושגים באמצעות טענות בטיחות חיוביות המדגימות שמערכת היא חלשה ו/או צרה ו/או פאסיבית. תקרות על כוח חישוב כולל לאימון וקצב כוח חישוב להסקה, מאומתות ונאכפות חוקית ובאמצעות אמצעי אבטחה של חומרה וקריפטוגרפיים, תומכות בבטיחות על ידי הימנעות מAGI מלא ואיסור יעיל של על-אינטליגנציה.

[^87]: ככל הנראה, התפשטות ההכרה הזו תדרוש מאמץ אינטנסיבי של קבוצות חינוך והסברה שמציגות את הטענה הזו, או אסון די משמעותי שנגרם על ידי AI. אנו יכולים לקוות שזה יהיה הראשון.

[^88]: באופן פרדוקסלי, אנו רגילים שהטבע מגביל את הטכנולוגיה שלנו על ידי כך שהיא קשה מאוד לפיתוח, במיוחד מבחינה מדעית. אבל זה כבר לא המקרה עבור AI: הבעיות המדעיות המרכזיות מתגלות כקלות יותר מהצפוי. אנחנו לא יכולים לסמוך על הטבע שיציל אותנו מעצמנו כאן – נצטרך לעשות זאת בעצמנו.

[^89]: איפה, בדיוק, אנחנו עוצרים בפיתוח מערכות חדשות? כאן, עלינו לאמץ עקרון זהירות. ברגע שמערכת פרוסה, ובמיוחד ברגע שרמת יכולת מערכת כזו מתפשטת, קשה ביותר לחזור אחורה. ואם מערכת *מפותחת* (במיוחד בעלות ומאמץ גדולים), יהיה לחץ עצום להשתמש או לפרוס אותה, ופיתוי שהיא תודלף או תיגנב. פיתוח מערכות ו*אז* החלטה אם הן לא בטוחות עמוקות זו דרך מסוכנת.

[^90]: יהיה גם חכם לאסור פיתוח AI שמסוכן מטבעו, כמו מערכות שכפולות ומתפתחות, אלה שמתוכננות לברוח מהגבלה, אלה שיכולות לשפר את עצמן באופן אוטונומי, AI מטעה וזדוני במכוון, וכו'.

[^91]: שימו לב זה לא בהכרח אומר *נאכף* ברמה הבינלאומית על ידי איזה סוג של גוף עולמי: במקום זה מדינות ריבוניות יכולות לאכוף כללים מוסכמים, כמו בהרבה אמנות.

[^92]: כפי שנראה להלן, טיב חישוב ה-AI יאפשר משהו כמו היברידי; אבל שיתוף פעולה בינלאומי עדיין יידרש.

[^93]: לדוגמה, המכונות הנדרשות לחריטת צ'יפים הרלוונטיים ל-AI מיוצרות על ידי חברה אחת בלבד, ASML (למרות נסיונות רבים אחרים לעשות זאת), הרוב הגדול של הצ'יפים הרלוונטיים מיוצרים על ידי חברה אחת, TSMC (למרות שאחרות מנסות להתחרות), והעיצוב והבנייה של חומרה מהצ'יפים האלה נעשים על ידי רק כמה כולל NVIDIA, AMD, וגוגל.

[^94]: הכי חשוב, כל צ'יפ מחזיק מפתח פרטי קריפטוגרפי יחיד ובלתי נגיש שהוא יכול להשתמש בו כדי "לחתום" על דברים.

[^95]: כברירת מחדל זו תהיה החברה שמוכרת את הצ'יפים, אבל מודלים אחרים אפשריים ואולי שימושיים.

[^96]: מפקח יכול לברר מיקום צ'יפ על ידי תזמון חילופי הודעות חתומות איתו: מהירות האור הסופית מחייבת את הצ'יפ להיות בתוך רדיוס נתון *r* של "תחנה" אם הוא יכול להחזיר הודעה חתומה בזמן פחות מ-*r* / *c*, כאשר *c* היא מהירות האור. בשימוש במספר תחנות, ובהבנה כלשהי של מאפייני רשת, ניתן לקבוע את מיקום הצ'יפ. היופי של השיטה הזו הוא שרוב האבטחה שלה מסופקת על ידי חוקי הפיזיקה. שיטות אחרות יכולות להשתמש ב-GPS, מעקב אינרציאלי וטכנולוגיות דומות.

[^97]: לחילופין, זוגות צ'יפים יוכלו לתקשר זה עם זה רק באמצעות רשות מפורשת של מפקח.

[^98]: זה חשוב משום שלפחות כרגע, חיבור ברוחב פס גבוה מאוד בין צ'יפים נחוץ לאימון מודלי AI גדולים עליהם.

[^99]: זה גם יכול להיות מוקם לדרוש הודעות חתומות מ-*N* של *M* מפקחים שונים, מה שמאפשר לכמה צדדים לחלוק ממשל.

[^100]: זה רחוק מלהיות חסר תקדים – לדוגמה צבאות לא פיתחו צבאות של חיילים על משובטים או מהונדסים גנטית, למרות שזה כנראה אפשרי טכנולוגית. אבל הם *בחרו* לא לעשות זאת, במקום להיות נמנעים על ידי אחרים. המאזן לא נהדר לגבי מעצמות עולמיות גדולות שנמנעות מפיתוח טכנולוגיה שהן מאוד רוצות לפתח.

[^101]: פרט לכמה חריגים ראויים לציון (במיוחד NVIDIA) החומרה המתמחה ב-AI היא חלק קטן יחסית של המודל העסקי והכנסות של החברות האלה. יותר מזה, הפער בין חומרה שמשמשת ב-AI מתקדם ולחומרה "לצרכן" משמעותי, אז רוב צרכני חומרת מחשב יהיו מושפעים בעיקר מעט.

[^102]: לניתוח מפורט יותר, ראו את הדוחות העדכניים מ[RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) ו[CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). אלה מתמקדים בכדאיות טכנית, במיוחד בהקשר של בקרות יצוא אמריקאיות השואפות להגביל את הקיבולת של מדינות אחרות בחישוב ברמה גבוהה; אבל לזה יש חפיפה ברורה עם האילוץ הגלובלי שנצפה כאן.

[^103]: מכשירי אפל, לדוגמה, ננעלים מרחוק ובטוח כשמדווח שאבדו או נגנבו, ויכולים להיות מופעלים מחדש מרחוק. זה נשען על אותן תכונות אבטחת חומרה שנדונות כאן.

[^104]: ראו למשל הצעת [capacity on demand](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) של IBM, [Intel on demand.](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html) של אינטל, ו[private cloud compute](https://security.apple.com/blog/private-cloud-compute/) של אפל.

[^105]: [המחקר הזה](https://epochai.org/trends#hardware-trends-section) מראה שבהיסטוריה אותו ביצועים הושגו באמצעות כ-30% פחות דולרים בשנה. אם המגמה הזו תמשיך, יכולה להיות חפיפה משמעותית בין שימוש בצ'יפים של AI ו"צרכניים", ובאופן כללי כמות החומרה הנדרשת למערכות AI עתירות כוח יכולה להפוך קטנה באופן לא נוח.

[^106] לפי [אותו מחקר](https://epochai.org/trends#hardware-trends-section), ביצועים נתונים בזיהוי תמונות דרש פי 2.5 פחות חישוב כל שנה. אם זה ימשיך לחזור גם על המערכות הכי מסוגלות של AI, מגבלת חישוב לא תהיה שימושית במשך זמן ארוך.

[^107]: במיוחד, ברמת המדינה זה נראה הרבה כמו הלאמת חישוב, בכך שלממשלה יהיה הרבה שליטה על איך כוח חישובי ישמש. עם זאת, לאלה המודאגים ממעורבות ממשלתית, זה נראה הרבה יותר בטוח ועדיף מאשר תוכנת ה-AI החזקה ביותר *עצמה* מולאמת באמצעות מיזוג כלשהו בין חברות AI גדולות וממשלות לאומיות, כפי שחלקים מתחילים לתמוך.

[^108]: צעד רגולטורי גדול באירופה ננקט עם מעבר ב2024 של [חוק ה-AI של האיחוד האירופי.](https://artificialintelligenceact.eu/) הוא מסווג AI לפי סיכון: אוסר מערכות בלתי מקובלות, מווסת מערכות ברמת סיכון גבוהה, ומטיל כללי שקיפות, או שום אמצעים בכלל, על מערכות ברמת סיכון נמוכה. הוא יפחית משמעותית כמה סיכוני AI, ויחזק שקיפות AI אפילו לחברות אמריקאיות, אבל יש לו שני פגמים מרכזיים. ראשית, טווח הגעה מוגבל: למרות שהוא חל על כל חברה שמספקת AI באיחוד האירופי, אכיפה על חברות שבסיסן בארה"ב חלשה, ו-AI צבאי פטור. שנית, למרות שהוא מכסה GPAI, הוא נכשל בלהכיר ב-AGI או בעל-אינטליגנציה כסיכונים בלתי מקובלים או למנוע את פיתוחם—רק את הפריסה שלהם באיחוד האירופי. כתוצאה, הוא עושה מעט כדי לחסם את הסיכונים של AGI או על-אינטליגנציה.

[^109]: חברות לעיתים קרובות מציגות שהן בעד רגולציה סבירה. אבל איכשהו הן כמעט תמיד נראות מתנגדות לכל רגולציה *ספציפית*; עדים למאבק על ה-SB1047 הדי מעורב, ש[רוב חברות ה-AI התנגדו לו בפומבי או בפרטי.](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/)

[^110]: זה היה כ-3.5 שנים מהזמן שחוק ה-AI של האיחוד האירופי הוצע עד שהוא נכנס לתוקף.

[^111]: זה לפעמים מבוטא שזה "מוקדם מדי" להתחיל לווסת AI. בהתחשב בהערה האחרונה, זה בקושי נראה סביר. חשש נוסף שמובע הוא שרגולציה תזיק ל"חדשנות." אבל רגולציה טובה פשוט משנה את הכיוון, לא את הכמות, של חדשנות.

[^112]: תקדים מעניין הוא בהובלת חומרים מסוכנים, שיכולים לברוח ולגרום לנזק. כאן, [רגולציה](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) ו[פסיקה](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) קבעו אחריות חמורה לחומרים מסוכנים מאוד כמו חומרי נפץ, בנזין, רעלים, סוכנים זיהומיים ופסולת רדיואקטיבית. דוגמאות אחרות כוללות [אזהרות על תרופות](https://www.medicalnewstoday.com/articles/boxed-warnings), [סוגי מכשירים רפואיים,](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification) וכו'.

[^113]: הצעה מקיפה אחרת עם מטרות דומות שהוצגת ב["A Narrow Path"](https://www.narrowpath.co/) תומכת בגישה מרוכזת יותר, מבוססת איסור שמכווה את כל פיתוח ה-AI החדשני דרך ישות בינלאומית יחידה, בפיקוח של מוסדות בינלאומיים חזקים, עם איסורים קטגוריאליים ברורים ולא הגבלות מדורגות. גם אני הייתי תומך בתוכנית הזו; עם זאת היא תדרוש עוד יותר רצון פוליטי ותיאום מהזה שמוצע כאן.

[^114]: כמה הנחיות לתקן כזה [פורסמו](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/) על ידי פורום מודל החזית. יחסית להצעה כאן, אלה מתרכזות בצד של פחות די

## פרק 9 - הנדסת העתיד — מה עלינו לעשות במקום זאת

בינה מלאכותית יכולה לעשות טוב מדהים בעולם. כדי לקבל את כל היתרונות ללא הסיכונים, עלינו לוודא שהבינה המלאכותית תישאר כלי אנושי.

אם נצליח לבחור לא להחליף את האנושות במכונות - לפחות לזמן מה! - מה נוכל לעשות במקום זאת? האם נוותר על ההבטחה העצומה של הבינה המלאכותית כטכנולוגיה? ברמה מסוימת התשובה היא *לא* פשוטה: נסגור את השערים לבינה מלאכותית כללית בלתי נשלטת ולעל-אינטליגנציה, אבל *כן* נבנה צורות רבות אחרות של בינה מלאכותית, כמו גם את מבני הממשל והמוסדות שנצטרך כדי לנהל אותן.

אבל עדיין יש הרבה מה לומר; להגשים זאת יהיה עיסוק מרכזי של האנושות. החלק הזה חוקר כמה נושאים מרכזיים:

- איך נוכל לאפיין בינה מלאכותית "כלית" ואת הצורות שהיא יכולה לקחת.
- שנוכל לקבל (כמעט) כל מה שהאנושות רוצה בלי AGI, עם AI כלי.
- שמערכות AI כלי הן (כנראה, עקרונית) ניתנות לניהול.
- שהתרחקות מ-AGI לא אומרת ויתור על ביטחון לאומי - להיפך.
- שריכוז כוח הוא דאגה אמיתית. האם נוכל למתן אותו מבלי לפגוע בבטיחות ובביטחון?
- שנרצה - ונצטרך - מבני ממשל וחברה חדשים, ובינה מלאכותית יכולה דווקא לעזור.

### בינה מלאכותית בתוך השערים: AI כלי

דיאגרמת החיתוך המשולש נותנת דרך טובה להגדיר את מה שנוכל לקרוא "AI כלי": בינה מלאכותית שהיא כלי שליט לשימוש אנושי, במקום יריב או תחליף בלתי שליט. המערכות הפחות בעייתיות הן אלה שהן אוטונומיות אבל לא כלליות או בעלות יכולת-על (כמו בוט מכרזים), או כלליות אבל לא אוטונומיות או מסוגלות (כמו מודל שפה קטן), או מסוגלות אבל צרות ומאוד שליטות (כמו AlphaGo).[^124] אלה עם שתי תכונות מצטלבות יש להן יישום רחב יותר אבל סיכון גבוה יותר ויידרשו מאמצים גדולים לניהול. (זה שמערכת בינה מלאכותית היא יותר כלי לא אומר שהיא בטוחה מטבעה, רק שהיא לא בלתי בטוחה מטבעה - חשבו על מסור חשמלי, לעומת נמר מחמד.) השער חייב להישאר סגור ל-(מלא) AGI ועל-אינטליגנציה בחיתוך המשולש, ויש לנקוט זהירות עצומה במערכות בינה מלאכותית המתקרבות לסף הזה.

אבל זה משאיר הרבה בינה מלאכותית חזקה! נוכל להפיק תועלת עצומה מ"אורקולים" פסיביים חכמים וכלליים ומערכות צרות, מערכות כלליות ברמה אנושית אבל לא על-אנושית, וכן הלאה. חברות טכנולוגיה רבות ומפתחים בונים באופן פעיל כלים מהסוג הזה וצריכים להמשיך; כמו רוב האנשים הם מניחים במובלע שהשערים ל-AGI ועל-אינטליגנציה יסגרו.[^125]

כמו כן, מערכות בינה מלאכותית יכולות להשתלב בצורה יעילה למערכות מורכבות השומרות על פיקוח אנושי תוך הגברת היכולת. במקום להסתמך על קופסאות שחורות בלתי מובנות, נוכל לבנות מערכות שבהן רכיבים מרובים - כולל בינה מלאכותית ותוכנה מסורתית - עובדים יחד בדרכים שבני אדם יכולים לפקח ולהבין.[^126] אמנם חלק מהרכיבים עשויים להיות קופסאות שחורות, אבל אף אחד לא יהיה קרוב ל-AGI - רק המערכת המורכבת בכללותה תהיה גם כללית מאוד וגם מסוגלת מאוד, ובאופן שליט לחלוטין.[^127]

#### שליטה אנושית משמעותית ומובטחת

מה אומר "שליט לחלוטין"? רעיון מרכזי במסגרת ה"כלי" הוא לאפשר מערכות - גם אם כלליות וחזקות למדי - המובטחות להיות תחת שליטה אנושית משמעותית. מה זה אומר? זה כולל שני היבטים. הראשון הוא שיקול עיצוב: בני אדם צריכים להיות מעורבים בעמקותובאופן מרכזי במה שהמערכת עושה, *בלי* להאציל החלטות חשובות מרכזיות לבינה המלאכותית. זה האופי של רוב מערכות הבינה המלאכותית הנוכחיות. שנית, ככל שמערכות בינה מלאכותית אוטונומיות, חייבות להיות להן ערבויות המגבילות את היקף פעולתן. ערבות צריכה להיות *מספר* המאפיין את ההסתברות שמשהו יקרה, וסיבה להאמין במספר הזה. זה מה שאנו דורשים בתחומים ביקורתיים אחרים לבטיחות, שבהם מספרים כמו "זמן ממוצע בין כשלים" ומספרים צפויים של תאונות מחושבים, נתמכים ומפורסמים בתיקי בטיחות.[^128] המספר האידיאלי לכשלים הוא אפס, כמובן. והחדשות הטובות הן שאולי נגיע קרוב למדי, אם כי תוך שימוש בארכיטקטורות בינה מלאכותית שונות למדי, תוך שימוש ברעיונות של תכונות *מאומתות פורמלית* של תוכניות (כולל בינה מלאכותית). הרעיון, שנחקר בהרחבה על ידי אומוהונדרו, טגמארק, בנג'יו, דלרימפל ואחרים (ראו [כאן](https://arxiv.org/abs/2309.01933) ו[כאן](https://arxiv.org/abs/2405.06624)) הוא לבנות תוכנית עם תכונות מסוימות (למשל: שאדם יכול לכבות אותה) ו*להוכיח* פורמלית שהתכונות הללו קיימות. זה ניתן לעשות עכשיו לתוכניות קצרות למדי ותכונות פשוטות, אבל הכוח (הקרב) של תוכנת הוכחה מופעלת בינה מלאכותית יכול לאפשר זאת לתוכניות מורכבות הרבה יותר (למשל עטפות) ואפילו לבינה מלאכותית עצמה. זוהי תוכנית שאפתנית מאוד, אבל כשהלחץ גובר על השערים, אנחנו נזדקק לחומרים חזקים שיחזקו אותם. הוכחה מתמטית עשויה להיות אחד מהמעטים שחזקים דיים.

#### לאן תעשיית הבינה המלאכותית

עם הפניית התקדמות הבינה המלאכותית, AI כלי עדיין תהיה תעשיה ענקית. במונחי חומרה, גם עם הגבלות כוח חישוב למניעת על-אינטליגנציה, אימון והסקה במודלים קטנים יותר עדיין יידרשו כמויות ענקיות של רכיבים מיוחדים. בצד התוכנה, ביטול הפיצוץ בגודל מודל ובחישוב של בינה מלאכותית צריך פשוט להוביל לחברות להפנות משאבים לשיפור המערכות הקטנות יותר, להפוך אותן למגוונות ומיוחדות יותר, במקום פשוט להגדיל אותן.[^129] יהיה הרבה מקום - יותר כנראה - לכל הסטארט-אפים המייצרי כסף של עמק הסיליקון.[^130]

### AI כלי יכול להניב (כמעט) כל מה שהאנושות רוצה, בלי AGI

אינטליגנציה, בין אם ביולוגית או מכונה, יכולה להיחשב באופן רחב כיכולת לתכנן ולבצע פעילויות המביאות לעתידים המתיישרים יותר עם קבוצת מטרות. ככזו, אינטליגנציה מועילה מאוד כשמשתמשים בה במרדף אחרי מטרות נבחרות בחכמה. בינה מלאכותית מושכת השקעות עצומות של זמן ומאמץ בעיקר בגלל היתרונות המובטחים שלה. אז עלינו לשאול: באיזה מידה עדיין נקצור את היתרונות של בינה מלאכותית אם נבלום את התחמקותה לעל-אינטליגנציה? התשובה: אנחנו עשויים להפסיד מעט באופן מפתיע.

חשבו תחילה שמערכות בינה מלאכותית נוכחיות כבר חזקות מאוד, ואנחנו באמת רק גירדנו את פני השטח של מה שאפשר לעשות איתן.[^131] הן מסוגלות סבירות "לנהל את המופע" במונחי "הבנה" של שאלה או משימה המוצגת להן, ומה יידרש כדי לענות על השאלה הזו או לעשות את המשימה הזו.

הלאה, הרבה מההתרגשות מסביב למערכות בינה מלאכותית מודרניות נובע מהכלליות שלהן; אבל חלק ממערכות הבינה המלאכותית המסוגלות ביותר - כמו כאלה שיוצרות או מזהות דיבור או תמונות, עושות חיזוי ומידול מדעי, משחקות משחקים וכו' - הן הרבה יותר צרות ובטוח "בתוך השערים" במונחי חישוב.[^132] המערכות הללו הן על-אנושיות במשימות הספציפיות שהן עושות. יכול להיות להן חולשות בקצה הקצה[^133] (או [ניתנות לניצול](https://arxiv.org/abs/2211.00241)) בגלל הצרות שלהן; אבל צר *לחלוטין* או כללי *לחלוטין* אינם האפשרויות היחידות הזמינות: יש ארכיטקטורות רבות ביניהן.[^134]

כלי בינה מלאכותית אלה יכולים להאיץ מאוד התקדמות בטכנולוגיות חיוביות אחרות, בלי AGI. כדי לעשות פיזיקה גרעינית טובה יותר, אנחנו לא צריכים שהבינה המלאכותית תהיה פיזיקאי גרעיני - יש לנו כאלה! אם אנחנו רוצים להאיץ רפואה, ניתן לביולוגים, לחוקרי רפואה ולכימאים כלים חזקים. הם רוצים אותם וישתמשו בהם לתועלת עצומה. אנחנו לא צריכים חוות שרתים מלאה במיליון גאונים דיגיטליים; יש לנו מיליוני בני אדם שהבינה המלאכותית יכולה לעזור להוציא את הגאונות שלהם. כן, ייקח יותר זמן לקבל אלמוות ותרופה לכל המחלות. זוהי עלות אמיתית. אבל אפילו חידושי הבריאות המבטיחים ביותר יהיו חסרי תועלת אם אי-יציבות מונעת בינה מלאכותית תוביל לעימות גלובלי או התמוטטות חברתית. אנחנו חייבים לעצמנו לתת לבני אדם מחוזקי בינה מלאכותית הזדמנות לפתור את הבעיה קודם.

ונניח שיש, למעשה, איזה תועלת עצומה ל-AGI שלא ניתן להשיגה על ידי האנושות באמצעות כלים בתוך השערים. האם נאבד אותם על ידי *אי*-בנייה של AGI ועל-אינטליגנציה? בשקלול הסיכונים והתגמולים כאן, יש יתרון אסימטרי עצום בהמתנה מול חיפזון: נוכל לחכות עד שאפשר יהיה לעשות זאת בדרך מובטחת בטוחה ומועילה, וכמעט כולם עדיין יוכלו לקצור את התגמולים; אם נמהר, זה יכול להיות - במילותיו של מנכ"ל OpenAI סם אלטמן - [כיבוי האורות לכולנו.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

אבל אם כלים שאינם AGI הם כל כך חזקים באופן פוטנציאלי, האם נוכל לנהל אותם? התשובה היא ברורה...אולי.

### מערכות AI כלי הן (כנראה, עקרונית) ניתנות לניהול

אבל זה לא יהיה קל. מערכות בינה מלאכותית חדישות יכולות להעצים מאוד אנשים ומוסדות בהשגת המטרות שלהם. זה, באופן כללי, דבר טוב! אבל, יש דינמיקות טבעיות של הימצאות מערכות כאלה לרשותנו - בפתאומיות ובלי הרבה זמן לחברה להסתגל - שמציעות סיכונים רציניים שצריך לנהל. כדאי לדון בכמה סוגים עיקריים של סיכונים כאלה, ואיך אפשר להקטין אותם, בהנחה של סגירת שער.

סוג אחד של סיכונים הוא של AI כלי בעל כוח גבוה שמאפשר גישה לידע או יכולת שבעבר היו קשורים לאדם או ארגון, מה שהופך שילוב של יכולת גבוהה פלוס נאמנות גבוהה זמינים למגוון רחב מאוד של שחקנים. היום, עם מספיק כסף אדם עם כוונות רעות יכול לשכור צוות כימאים לתכנן ולייצר נשק כימי חדש - אבל זה לא כל כך קל להשיג את הכסף הזה או למצוא/להרכיב את הצוות ולשכנע אותם לעשות משהו די ברור שהוא לא חוקי, לא אתי ומסוכן. כדי למנוע ממערכות בינה מלאכותית למלא תפקיד כזה, שיפורים על השיטות הנוכחיות עשויים להספיק,[^135] כל עוד כל המערכות הללו והגישה אליהן מנוהלות באחריות. מצד שני, אם מערכות חזקות משוחררות לשימוש כללי ושינוי, כל אמצעי הבטיחות המובנים כנראה ניתנים להסרה. אז כדי למנוע סיכונים מהסוג הזה, יידרשו הגבלות חזקות על מה שאפשר לשחרר לציבור - באנלוגיה להגבלות על פרטים של טכנולוגיות גרעיניות, חבלניות ומסוכנות אחרות.[^136]

סוג שני של סיכונים נובע מהרחבת קנה מידה של מכונות שמתנהגות כמו או מתחזות לאנשים. ברמת הפגיעה לאנשים בודדים, הסיכונים הללו כוללים הונאות, ספאם ופישינג יעילים הרבה יותר, והפצה של דיפ-פיקים לא הסכמיים.[^137] ברמה קולקטיבית, הם כוללים שיבוש של תהליכים חברתיים מרכזיים כמו דיון וויכוח ציבורי, מערכות איסוף, עיבוד והפצת מידע וידע חברתיות, ומערכות הבחירה הפוליטיות שלנו. הפחתת הסיכון הזה כנראה תכלול (א) חוקים המגבילים התחזות לאנשים על ידי מערכות בינה מלאכותית, והטלת אחריות על מפתחי בינה מלאכותית שיוצרים מערכות המייצרות התחזויות כאלה, (ב) מערכות סימון מים ומקור שמזהות ומסווגות (באחריות) תוכן שנוצר בינה מלאכותית, ו(ג) מערכות אפיסטמיות חברתיות-טכניות חדשות שיכולות ליצור שרשרת מהימנה מנתונים (למשל מצלמות והקלטות) דרך עובדות, הבנה ומודלי עולם טובים.[^138] כל זה אפשרי, והבינה המלאכותית יכולה לעזור עם חלקים ממנו.

סיכון כללי שלישי הוא שככל שמשימות מסוימות מתבצעות באופן אוטומטי, לבני האדם שעושים כרגע את המשימות הללו יכול להיות פחות ערך פיננסי כעבודה. היסטורית, הפיכת משימות לאוטומטיות הפכה דברים המתאפשרים על ידי המשימות הללו לזולים ושופעים יותר, תוך מיון האנשים שעשו בעבר את המשימות הללו לאלה שעדיין מעורבים בגרסה האוטומטית (בדרך כלל במיומנות/שכר גבוהים יותר), ואלה שעבודתם שווה פחות או מעט. בסך הכל קשה לחזות באילו סקטורים יידרש יותר לעומת פחות עבודה אנושית בסקטור הגדול יותר אבל יעיל יותר שנוצר. במקביל, הדינמיקה האוטומטית נוטה להגביר אי-שוויון ופרודוקטיביות כללית, להפחית את עלותם של סחורות ושירותים מסוימים (דרך הגברת יעילות), ולהגביר את העלות של אחרים (דרך [מחלת עלויות](https://en.wikipedia.org/wiki/Baumol_effect)). לגבי אלה שבצד הלא נוח של עליית אי-השוויון, לגמרי לא ברור האם הירידה בעלות באותן סחורות ושירותים מסוימים עולה על העלייה באחרים, ומובילה לרווחה כללית גדולה יותר. אז איך זה יהיה עם בינה מלאכותית? בגלל הקלות היחסית שבה אפשר להחליף עבודה אנושית אינטלקטואלית בבינה מלאכותית כללית, נוכל לצפות לגרסה מהירה של זה עם בינה מלאכותית כללית מתחרה באדם.[^139] אם נסגור את השער ל-AGI, הרבה פחות עבודות יוחלפו בסיטונות על ידי סוכני בינה מלאכותית; אבל עקירת עבודה ענקית עדיין סבירה על פני תקופה של שנים.[^140] כדי למנוע סבל כלכלי נרחב, כנראה יהיה צריך ליישם גם צורה כלשהי של נכסים או הכנסה בסיסיים אוניברסליים, וגם להנדס שינוי תרבותי לכיוון הערכה ותגמול של עבודה אנושית-מרכזית שקשה יותר לבצע אוטומטית (במקום לראות מחירי עבודה יורדים בגלל העלייה בעבודה זמינה שנדחקה מחלקים אחרים של הכלכלה.) מבנים אחרים, כמו זה של ["כבוד נתונים"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society) (שבו היצרנים האנושיים של נתוני אימון מקבלים באופן אוטומטי תמלוגים לערך שנוצר על ידי הנתונים הללו בבינה מלאכותית) עשויים לעזור. לאוטומציה על ידי בינה מלאכותית יש גם השפעה שלילית פוטנציאלית שנייה, שהיא של אוטומציה *לא מתאימה*. יחד עם יישומים שבהם בינה מלאכותית פשוט עושה עבודה גרועה יותר, זה יכלול אלה שבהם מערכות בינה מלאכותית צפויות להפר עקרונות מוסריים, אתיים או חוקיים - למשל בהחלטות חיים ומוות, ובעניינים שיפוטיים. אלה חייבים להיטפל על ידי יישום והרחבה של המסגרות החוקיות הנוכחיות שלנו.

לבסוף, איום משמעותי של בינה מלאכותית בתוך השערים הוא השימוש בה לשכנוע מותאם אישית, לכידת קשב ומניפולציה. ראינו ברשתות חברתיות ובפלטפורמות מקוונות אחרות את הצמיחה של כלכלת קשב מושרשת עמוקות (שבה שירותים מקוונים נלחמים בצורה אכזרית על קשב משתמשים) ומערכות ["קפיטליזם מעקב"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism) (שבהן מידע המשתמש ויצירת פרופיל נוספים לסחרור הקשב). כמעט ודאי שעוד בינה מלאכותית תוכנס לשירות שניהם. בינה מלאכותית כבר נמצאת בשימוש נרחב באלגוריתמי פיד ממכרים, אבל זה יתפתח לתוכן שנוצר בבינה מלאכותית ממכר, מותאם להתכלות כפייתית על ידי אדם יחיד. והקלט, התגובות והנתונים של האדם הזה יוזנו למכונת הקשב/פרסום כדי להמשיך את המעגל הרעיל. כמו כן, כשעוזרי בינה מלאכותית שמסופקים על ידי חברות טכנולוגיה הופכים לממשק ליותר חיים מקוונים, הם כנראה יחליפו מנועי חיפוש ופידים כמנגנון שבו מתרחשים שכנוע והשתכרות על לקוחות. הכישלון של החברה שלנו לשלוט בדינמיקות הללו עד כה לא מבשר טובות. חלק מהדינמיקה הזו עשוי להיות מופחת דרך תקנות הנוגעות לפרטיות, זכויות נתונים ומניפולציה. להגיע יותר לשורש הבעיה עשוי לדרוש פרספקטיבות שונות, כמו זו של עוזרי בינה מלאכותית נאמנים (שנדונה להלן.)

המסקנה מהדיון הזה היא של תקווה: מערכות מבוססות-כלי בתוך השערים - לפחות כל עוד הן נשארות דומות בכוח וביכולת למערכות החדישות ביותר של היום - כנראה ניתנות לניהול אם יש רצון ותיאום לעשות זאת. מוסדות אנושיים הגונים, המוגברים על ידי כלי בינה מלאכותית,[^141] יכולים לעשות זאת. נוכל גם לכשל בעשיית זה. אבל קשה לראות איך לאפשר מערכות חזקות יותר יעזור - מלבד לשים אותן בפיקוד ולקוות לטוב.

### ביטחון לאומי

מרוצים לעליונות בינה מלאכותית - המונעים על ידי ביטחון לאומי או מוטיבציות אחרות - דוחפים אותנו לכיוון מערכות בינה מלאכותית חזקות ובלתי נשלטות שינטו לקלוט, במקום להעניק, כוח. מרוץ AGI בין ארה"ב וסין הוא מרוץ לקבוע איזה מדינה מקבלת על-אינטליגנציה ראשונה.

אז מה צריכים לעשות אלה שאחראים על הביטחון הלאומי במקום זאת? לממשלות יש ניסיון חזק בבניית מערכות שליטות ובטוחות, והן צריכות להכפיל את המאמצים לעשות זאת בבינה מלאכותית, ולתמוך בסוג של פרויקטי תשתית שמצליחים הכי טוב כשנעשים בקנה מידה ועם אישור ממשלתי.

במקום "פרויקט מנהטן" פזיז לכיוון AGI,[^142] הממשלה האמריקנית יכולה לשגר פרויקט אפולו למערכות שליטות, בטוחות ואמינות. זה יכול לכלול למשל:

- תוכנית גדולה ל(א) פיתוח מנגנוני אבטחת חומרה על-שבבית ו(ב) התשתית, לנהל את צד כוח החישוב של בינה מלאכותית חזקה. אלה יכולים להיבנות על [חוק השבבים](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) האמריקני ו[משטר בקרת יצוא](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion).
- יוזמה גדולה לפיתוח טכניקות אימות פורמלי כך שתכונות מסוימות של מערכות בינה מלאכותית (כמו מתג כיבוי) יכולות *להוכח* שהן נוכחות או נעדרות. זה יכול למנף את הבינה המלאכותית עצמה לפיתוח הוכחות של תכונות.
- מאמץ בקנה מידה לאומי ליצור תוכנה שהיא אמינה בטוחה באופן מוכח, מופעלת על ידי כלי בינה מלאכותית שיכולים לקודד מחדש תוכנה קיימת למסגרות בטוחות באופן מוכח.
- פרויקט השקעה לאומי בהתקדמות מדעית באמצעות בינה מלאכותית,[^143] שרץ כשותפות בין המשרד לאנרגיה, קרן המדע הלאומית ומכון הבריאות הלאומי.

באופן כללי, יש משטח תקיפה עצום על החברה שלנו שהופך אותנו לפגיעים לסיכונים מבינה מלאכותית ומשימוש לרעה בה. הגנה מחלק מהסיכונים הללו תדרוש השקעה וסטנדרטיזציה בגודל ממשלתי. אלה יספקו הרבה יותר ביטחון משפיכת בנזין על האש של מרוצים לכיוון AGI. ואם בינה מלאכותית הולכת להיבנות בנשק ומערכות פיקוד ושליטה, חיוני שהבינה המלאכותית תהיה אמינה ובטוחה, מה שהבינה המלאכותית הנוכחית פשוט לא.

### ריכוז כוח והקלות שלו

המאמר הזה התמקד ברעיון של שליטה אנושית בבינה מלאכותית וכישלונה הפוטנציאלי. אבל עדשה תקפה אחרת שדרכה לראות את מצב הבינה המלאכותית היא דרך *ריכוז כוח.* הפיתוח של בינה מלאכותית חזקה מאוד מאיים לרכז כוח או לידיים המועטות והגדולות מאוד של החברות שפיתחו ושישלטו בה, או לידי ממשלות המשתמשות בבינה מלאכותית כאמצעי חדש לשמור על הכוח והשליטה שלהן, או למערכות הבינה המלאכותית עצמן. או איזה תערובת לא קדושה מהאמור לעיל. בכל המקרים הללו רוב האנושות מאבדת כוח, שליטה וכוח פעולה. איך נוכל להילחם בזה?

הצעד הראשון והחשוב ביותר, כמובן, הוא סגירת שער ל-AGI ועל-אינטליגנציה חכמות מאדם. אלה יכולות במפורש להחליף ישירות בני אדם וקבוצות של בני אדם. אם הן תחת שליטה ארגונית או ממשלתית הן ירכזו כוח בארגונים או ממשלות הללו; אם הן "חופשיות" הן ירכזו כוח לעצמן. אז בואו נניח שהשערים סגורים. ואז מה?

פתרון מוצע אחד לריכוז כוח הוא בינה מלאכותית "קוד פתוח", שבה משקולות מודל זמינות חינם או באופן נרחב. אבל כפי שהוזכר קודם, ברגע שמודל פתוח, רוב אמצעי הבטיחות או המעקות יכולים להיות (ובדרך כלל הם) מוסרים. אז יש מתח חד בין מצד אחד ביזור, ומצד שני בטיחות, ביטחון ושליטה אנושית במערכות בינה מלאכותית. יש גם סיבות להיות ספקניים שמודלים פתוחים יילחמו בעצמם בצורה משמעותית בריכוז כוח בבינה מלאכותית יותר ממה שהם עשו במערכות הפעלה (עדיין נשלטות על ידי מיקרוסופט, אפל וגוגל למרות אלטרנטיבות פתוחות).[^144]

ובכל זאת עשויות להיות דרכים ליישר את העיגול הזה - לרכז ולהקל סיכונים תוך ביזור יכולת ותגמול כלכלי. זה דורש חשיבה מחדש גם איך בינה מלאכותית מפותחת וגם איך היתרונות שלה מחולקים.

מודלים חדשים של פיתוח ובעלות ציבורית על בינה מלאכותית יעזרו. זה יכול לקחת צורות שונות: בינה מלאכותית שפותחה על ידי הממשלה (כפופה לפיקוח דמוקרטי),[^145] ארגוני פיתוח בינה מלאכותית ללא רווח (כמו Mozilla לדפדפנים), או מבנים המאפשרים בעלות וממשל רחבים מאוד. המפתח הוא שהמוסדות הללו יוסמכו במפורש לשרת את האינטרס הציבורי תוך פעולה תחת אילוצי בטיחות חזקים.[^146] משטרי רגולציה וסטנדרטים/אישורים מעוצבים היטב יהיו גם חיוניים, כך שמוצרי בינה מלאכותית המוצעים על ידי שוק תוסס יישארו שימושיים באמת במקום ניצוליים כלפי המשתמשים שלהם.

במונחי ריכוז כוח כלכלי, נוכל להשתמש במעקב מקור ו"כבוד נתונים" כדי להבטיח שיתרונות כלכליים זורמים באופן רחב יותר. בפרט, רוב כוח הבינה המלאכותית עכשיו (ובעתיד אם נשמור את השערים סגורים) נובע מנתונים שנוצרו על ידי אדם, בין אם נתוני אימון ישירים או משוב אנושי. אם חברות בינה מלאכותית יידרשו לפצות ספקי נתונים בהגינות,[^147] זה יכול לפחות לעזור להפיץ את התגמולים הכלכליים באופן רחב יותר. מעבר לכך, מודל אחר יכול להיות בעלות ציבורית על חלקים משמעותיים של חברות בינה מלאכותית גדולות. למשל, ממשלות המסוגלות להטיל מס על חברות בינה מלאכותית יכולות להשקיע חלק מהתקבולים בקרן עושר ריבוני שמחזיקה מניות בחברות, ומשלמת דיבידנדים לאוכלוסייה.[^148]

חיוני במנגנונים הללו הוא להשתמש בכוחה של הבינה המלאכותית עצמה כדי לעזור להפיץ כוח טוב יותר, במקום פשוט להילחם בריכוז כוח מונע בינה מלאכותית באמצעים שאינם בינה מלאכותית. גישה חזקה אחת תהיה דרך עוזרי בינה מלאכותית מעוצבים היטב שפועלים עם חובת אמון אמיתית כלפי המשתמשים שלהם - שמים את האינטרסים של המשתמשים ראשונים, במיוחד מעל לאלה של ספקים ארגוניים.[^149] העוזרים הללו חייבים להיות אמינים באמת, מוכשרים טכנית אבל מוגבלים בצורה מתאימה על בסיס מקרה שימוש ורמת סיכון, וזמינים לכולם דרך ערוצים ציבוריים, ללא רווח, או מוסמכים לרווח. בדיוק כמו שלעולם לא נקבל עוזר אנושי שעובד בסתר נגד האינטרסים שלנו לטובת צד אחר, אנחנו לא צריכים לקבל עוזרי בינה מלאכותית שמעקבים, מניפולים או מפיקים ערך מהמשתמשים שלהם לטובה ארגונית.

שינוי כזה יחליף בצורה יסודית את הדינמיקה הנוכחית שבה יחידים נשארים לנהל משא ומתן לבדם עם מכונות ארגוניות ובירוקרטיות עצומות (מופעלות בינה מלאכותית) שנותנות עדיפות לחילוץ ערך על פני רווחת אדם. בעוד יש גישות רבות אפשריות להפצה מחדש של כוח מונע בינה מלאכותית באופן רחב יותר, אף אחת לא תופיע כברירת מחדל: הן חייבות להיות מהונדסות ומנוהלות בכוונה עם מנגנונים כמו דרישות אמון, אספקה ציבורית, וגישה מדורגת על בסיס סיכון.

גישות להקלת ריכוז כוח יכולות להתמודד עם רוחות נגד משמעותיות מכוחות קיימים.[^150] אבל יש נתיבים לכיוון פיתוח בינה מלאכותית שלא דורש בחירה בין בטיחות וכוח מרוכז. על ידי בניית המוסדות הנכונים עכשיו, נוכל להבטיח שהיתרונות של בינה מלאכותית יחולקו באופן נרחב בעוד הסיכונים שלה מנוהלים בזהירות.

### מבני ממשל וחברה חדשים

מבני הממשל הנוכחיים שלנו נאבקים: הם איטיים להגיב, לעתים קרובות נלכדים על ידי אינטרסים מיוחדים, ו[זוכים לאמון הולך ופוחת מהציבור.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) ובכל זאת זו לא סיבה לנטוש אותם - להיפך. חלק מהמוסדות עשויים לזקוק להחלפה, אבל באופן רחב יותר אנחנו צריכים מנגנונים חדשים שיכולים לשפר ולהשלים את המבנים הקיימים שלנו, לעזור להם לתפקד טוב יותר בעולם המתפתח במהירות שלנו.

הרבה מהחולשה המוסדית שלנו נובעת לא ממבני ממשל פורמליים, אלא ממוסדות חברתיים מדורדרים: המערכות שלנו לפיתוח הבנה משותפת, תיאום פעולה, וניהול שיח משמעותי. עד כה, בינה מלאכותית האיצה את ההידרדרות הזו, הציפה את ערוצי המידע שלנו בתוכן מיוצר, הפנתה אותנו לתוכן המקטב והמפלג ביותר, והקשתה על הבחנה בין אמת לבדיה.

אבל בינה מלאכותית יכולה דווקא לעזור לבנות ולחזק את המוסדות החברתיים הללו. חשבו על שלושה תחומים חיוניים:

ראשית, בינה מלאכותית יכולה לעזור לשחזר אמון במערכות האפיסטמיות שלנו - הדרכים שלנו לדעת מה אמיתי. נוכל לפתח מערכות מופעלות בינה מלאכותית שעוקבות ומאמתות את מקור המידע, מנתונים גולמיים דרך ניתוח למסקנות. המערכות הללו יכולות לשלב אימות קריפטוגרפי עם ניתוח מתוחכם כדי לעזור לאנשים להבין לא רק האם משהו אמיתי, אלא איך אנחנו יודעים שהוא אמיתי.[^151] עוזרי בינה מלאכותית נאמנים יכולים להיות מופקדים על מעקב אחר הפרטים כדי להבטיח שהם מתיישבים.

שנית, בינה מלאכותית יכולה לאפשר צורות חדשות של תיאום בקנה מידה גדול. הרבה מהבעיות הדחופות ביותר שלנו - משינוי אקלים לעמידות לאנטיביוטיקה - הן בעיקר בעיות תיאום. אנחנו [תקועים במצבים שהם גרועים ממה שהם יכולים להיות כמעט לכולם](https://equilibriabook.com/), כי שום יחיד או קבוצה לא יכול להרשות לעצמו לעשות את המהלך הראשון. מערכות בינה מלאכותית יכולות לעזור על ידי מידול של מבני תמריצים מורכבים, זיהוי נתיבים בני קיימא לתוצאות טובות יותר, והקלה על מנגנוני בניית אמון והתחייבות הנדרשים כדי להגיע לשם.

אולי הכי מעניין, בינה מלאכותית יכולה לאפשר צורות חדשות לחלוטין של שיח חברתי. דמיינו יכולת "לדבר עם עיר"[^152] - לא רק צפייה בסטטיסטיקות, אלא ניהול דיאלוג משמעותי עם מערכת בינה מלאכותית שמעבדת ומסנתזת את הדעות, החוויות, הצרכים והשאיפות של מיליוני תושבים. או חשבו איך בינה מלאכותית יכולה להקל על דיאלוג אמיתי בין קבוצות שכרגע מדברות זו על פני זו, על ידי עזרה לכל צד להבין טוב יותר את הדאגות והערכים האמיתיים של הצד השני במקום הקריקטורות שלהם זו על זו.[^153] או בינה מלאכותית יכולה להציע תווך מיומן וניטרלי באופן אמין של מחלוקות בין אנשים או אפילו קבוצות גדולות של אנשים (שכולן יכולות לאתר איתה ישירות ובאופן אינדיבידואלי!) הבינה המלאכותית הנוכחית לחלוטין מסוגלת לעשות את העבודה הזו, אבל הכלים לעשות זאת לא יבואו לעולם מעצמם, או דרך תמריצי שוק.

האפשרויות הללו עשויות להישמע אוטופיות, במיוחד בהתחשב בתפקיד הנוכחי של בינה מלאכותית בהידרדרות השיח והאמון. אבל זה בדיוק למה עלינו לפתח באופן פעיל את היישומים החיוביים הללו. על ידי סגירת השערים לבינה מלאכותית כללית בלתי שליטה ונתינת עדיפות לבינה מלאכותית שמגביר כוח פעולה אנושי, נוכל לכוון התקדמות טכנולוגית לכיוון עתיד שבו בינה מלאכותית משמשת ככוח להעצמה, עמידות והתקדמות קולקטיבית.

[^124]: עם זאת, הישארות הרחק מהחיתוך המשולש היא לרוע המזל לא פשוטה כמו שאפשר להאמין. דחיפה קשה של יכולת בכל אחד משלושת ההיבטים נוטה להגביר אותה באחרים. בפרט, יכול להיות קשה ליצור אינטליגנציה כללית ומסוגלת מאוד שלא ניתן בקלות להפוך אוטונומית. גישה אחת היא לאמן מודלים ["קצרי ראייה"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) עם יכולת תכנון פגומה. אחרת תהיה להתמקד בהנדסה של מערכות ["אורקול"](https://arxiv.org/abs/1711.05541) טהורות שיתרחקו מלענות על שאלות מוכוונות פעולה.

[^125]: חברות רבות לא מצליחות להבין שגם הן יוחלפו בסופו של דבר על ידי AGI, גם אם ייקח יותר זמן - אם כן, הן עשויות לדחוף על השערים האלה קצת פחות!

[^126]: מערכות בינה מלאכותית יכולות לתקשר בדרכים יעילות יותר אבל פחות מובנות, אבל שמירה על הבנה אנושית צריכה לקבל עדיפות.

[^127]: הרעיון הזה של בינה מלאכותית מודולרית וניתנת לפרשנות פותח בפירוט על ידי כמה חוקרים; ראו למשל המודל ["שירותי בינה מלאכותית מקיפים"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) של דרקסלר, ["ארכיטקטורת סוכנות פתוחה"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai) של דלרימפל ואחרים. בעוד מערכות כאלה עשויות לדרוש יותר מאמץ הנדסי מאשר רשתות נוירונים מונוליטיות שאומנו עם חישוב עצום, זה בדיוק איפה הגבלות חישוב עוזרות - על ידי הפיכת הנתיב הבטוח והשקוף יותר לגם הנתיב המעשי יותר.

[^128]: על תיקי בטיחות באופן כללי ראו [המדריך הזה](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). הנוגע לבינה מלאכותית בפרט, ראו [ווסיל ואחרים](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [קליימר ואחרים](https://arxiv.org/abs/2403.10462), [בול ואחרים](https://arxiv.org/abs/2410.21572), ו[באלסני ואחרים](https://arxiv.org/abs/2411.03336)

[^129]: אנחנו למעשה כבר רואים את המגמה הזו מונעת רק על ידי העלות הגבוהה של הסקה: מודלים קטנים ומיוחדים יותר "מזוקקים" מגדולים יותר ומסוגלים לרוץ על חומרה פחות יקרה.

[^130]: אני מבין למה אלה שמתרגשים מהמערכת האקולוגית של טכנולוגיית בינה מלאכותית עשויים להתנגד למה שהם רואים כרגולציה מכבידה על התעשייה שלהם. אבל זה ממש מבלבל אותי למה, נאמר, משקיע הון סיכון ירצה לאפשר התחמקות ל-AGI ועל-אינטליגנציה. המערכות הללו (והחברות, כל עוד הן נשארות תחת שליטת החברה) *יאכלו את כל הסטארט-אפים כחטיף*. כנראה אפילו *יותר מוקדם* מאשר לאכול תעשיות אחרות. כל מי שמשקיע במערכת אקולוגית בינה מלאכותית משגשגת צריך לתת עדיפות להבטחה שפיתוח AGI לא מוביל למונופוליזציה על ידי כמה שחקנים דומיננטיים.

[^131]: כמו שכלכלן וחוקר דיפמיינד לשעבר מייקל ווב [ביטא זאת](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/), "אני חושב שאם נעצור את כל הפיתוח של מודלי שפה גדולים יותר היום, אז GPT-4 וקלוד ומה שלא יהיה, והם הדברים האחרונים שאנחנו מאמנים בגודל הזה - אז אנחנו מאפשרים הרבה יותר איטרציה על דברים בגודל הזה וכל מיני כיוונון עדין, אבל שום דבר גדול מזה, שום התקדמויות גדולות יותר - רק מה שיש לנו היום אני חושב מספיק כדי להניע 20 או 30 שנים של צמיחה כלכלית מדהימה."

[^132]: למשל, מערכת alphafold של דיפמיינד השתמשה רק ב-100,000 מספר ה-FLOP של GPT-4.

[^133]: הקושי של מכוניות נוהגות עצמן חשוב לציין כאן: בעוד זו באופן נומינלי משימה צרה, וניתנת להשגה עם מהימנות הגונה עם מערכות בינה מלאכותית קטנות יחסית, ידע והבנה נרחבים של העולם האמיתי נדרשים כדי להגיע למהימנות לרמה הנדרשת במ

## פרק 10 - הבחירה שלפנינו

כדי לשמור על עתידנו האנושי, עלינו לבחור לסגור את השערים בפני בינה מלאכותית כללית ועל-אינטליגנציה.

הפעם האחרונה שהאנושות חלקה את כדור הארץ עם תודעות אחרות שדיברו, חשבו, בנו טכנולוגיה ועסקו בפתרון בעיות כללי הייתה לפני 40,000 שנה באירופה של עידן הקרח. אותן תודעות אחרות נכחדו, כולן או בחלקן, בשל המאמצים של שלנו.

כעת אנו נכנסים שוב לתקופה כזו. התוצרים המתקדמים ביותר של התרבות והטכנולוגיה שלנו – מאגרי נתונים שנבנו מכלל הנחלת המידע של האינטרנט שלנו, וצ'יפים בני 100 מיליארד רכיבים שהם הטכנולוגיות המורכבות ביותר שיצרנו אי פעם – מתאחדים כדי להביא לקיומן מערכות AI מתקדמות ורב-תכליתיות.

מפתחי המערכות הללו להוטים לתאר אותן כמכלים לחיזוק האדם. ואכן הן יכולות להיות. אך אל תטעו: המסלול הנוכחי שלנו הוא לבנות סוכנים דיגיטליים מונחי יעדים, קובלי החלטות ובעלי יכולת כללית שהם עוצמתיים יותר ויותר. הם כבר מבצעים לא פחות טוב מבני אדם רבים במגוון רחב של משימות אינטלקטואליות, משתפרים במהירות ותורמים לשיפור עצמם.

אלא אם המסלול הזה ישתנה או יתקל במכשול בלתי צפוי, בקרוב יהיו לנו – בשנים, לא עשורים – אינטליגנציות דיגיטליות שהן עוצמתיות באופן מסוכן. אפילו בתוצאות הטובות *ביותר*, אלה יביאו לתועלת כלכלית גדולה (לפחות לחלק מאתנו) אבל רק במחיר של זעזוע עמוק בחברה שלנו, והחלפת בני אדם ברוב הדברים החשובים ביותר שאנו עושים: המכונות הללו יחשבו עבורנו, יתכננו עבורנו, יחליטו עבורנו ויצרו עבורנו. נהיה מפונקים, אבל ילדים מפונקים. הרבה יותר סביר שהמערכות הללו יחליפו בני אדם גם בדברים החיוביים *וגם* בדברים השליליים שאנו עושים, כולל ניצול, מניפולציה, אלימות ומלחמה. האם נוכל לשרוד גרסאות מוגברות-AI של אלה? לבסוף, יותר מסביר שהדברים לא ילכו טוב בכלל: שיחסית בקרוב נוחלף לא רק במה שאנו עושים, אלא במה שאנו *הוא*, כאדריכלי הציוויליזציה והעתיד. שאלו את הניאנדרטלים איך זה הולך. אולי גם סיפקנו להם תכשיטים נוספים לזמן מה.

*אנחנו לא חייבים לעשות את זה.* יש לנו AI שמתחרה עם בני אדם, ואין צורך לבנות AI שאיתו אנחנו *לא יכולים* להתחרות. אנחנו יכולים לבנות כלי AI מדהימים מבלי לבנות מין חליפי. הרעיון שבינה מלאכותית כללית ועל-אינטליגנציה הן בלתי נמנעות הוא *בחירה המתחזה לגורל*.

על ידי הטלת מגבלות קשות וגלובליות, אנחנו יכולים לשמור על היכולת הכללית של AI בערך ברמה אנושית ועדיין לקצור את היתרונות של היכולת של מחשבים לעבד נתונים בדרכים שאנחנו לא יכולים, ולהפוך למשימות אוטומטיות שאף אחד מאתנו לא רוצה לעשות. אלה עדיין יציבו סיכונים רבים, אבל אם יעוצבו וינוהלו היטב, יהיו ברכה עצומה לאנושות, מרפואה ועד מחקר ועד מוצרי צריכה.

הטלת מגבלות תדרוש שיתוף פעולה בינלאומי, אבל פחות ממה שניתן לחשוב, והמגבלות הללו עדיין ישאירו הרבה מקום לתעשיית AI וחומרת AI עצומה המתמקדת ביישומים המשפרים את רווחת האדם, במקום במרדף גולמי אחר כוח. ואם, עם הבטחות בטיחות חזקות ולאחר דיאלוג גלובלי משמעותי, נחליט להמשיך הלאה, האפשרות הזו ממשיכה להיות שלנו לרדוף.

האנושות חייבת *לבחור* לסגור את השערים בפני בינה מלאכותית כללית ועל-אינטליגנציה.

כדי לשמור על העתיד אנושי.

### הערה מהמחבר

תודה לך שהקדשת זמן לחקור נושא זה איתנו.

כתבתי את החיבור הזה כי כמדען אני מרגיש שחשוב לומר את האמת הגולה, וכיוון שכאדם אני מרגיש שזה חיוני עבורנו לפעול במהירות ובנחישות כדי להתמודד עם נושא משנה עולם: פיתוח מערכות AI חכמות יותר מבני אדם.

אם אנחנו רוצים להגיב למצב יוצא דופן זה בחוכמה, עלינו להיות מוכנים לבחון בביקורתיות את הנרטיב הרווח שבינה מלאכותית כללית ועל-אינטליגנציה "חייבות" להיבנות כדי להבטיח את האינטרסים שלנו, או שהן "בלתי נמנעות" ולא ניתן לעצור אותן. הנרטיבים הללו משאירים אותנו חסרי אונים, לא מסוגלים לראות את הדרכים החלופיות שלפנינו.

אני מקווה שתצטרפו אלי בקריאה לזהירות מול פזיזות, ואומץ מול תאוות בצע.

אני מקווה שתצטרפו אלי בקריאה לעתיד אנושי.

*– אנתוני*

![Anthony Aguirre signature](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAnthony-Aguirre-signature-300x84.png&w=3840&q=75)

## נספחים

מידע משלים, הכולל - פרטים טכניים בנוגע לחישוב כוח חישוב, דוגמה ליישום של 'סגירת שער', פרטים למשטר חבות AGI מחמיר, וגישה מדורגת לתקני בטיחות ואבטחה של AGI.

### נספח א': פרטים טכניים לחישוב כוח חישוב

שיטה מפורטת הן ל"אמת הקרקע" והן לקירובים טובים עבור כלל כוח החישוב המשמש באימון ובהסקה נדרשת עבור בקרה משמעותית המבוססת על כוח חישוב. להלן דוגמה כיצד ניתן לתעד את "אמת הקרקע" ברמה הטכנית.

**הגדרות:**

*גרף סיבתי של כוח חישוב:* עבור פלט O נתון של מודל AI, קיימת קבוצה של חישובים דיגיטליים שעבורה שינוי תוצאת החישוב הזה עלול לשנות את O באופן פוטנציאלי. (יש להניח זאת באופן שמרני, כלומר צריכה להיות סיבה ברורה להאמין שחישוב אינו תלוי בקודם שמתרחש מוקדם יותר בזמן ושיש לו מסלול סיבתי פיזי פוטנציאלי של השפעה.) זה כולל חישוב שנעשה על ידי מודל ה-AI במהלך ההסקה, כמו גם חישובים שנכנסו לקלט, הכנת נתונים ואימון המודל. מכיוון שכל אחד מאלה עשוי להיות בעצמו פלט ממודל AI, זה מחושב באופן רקורסיבי, נחתך כאשר בן אדם סיפק שינוי משמעותי לקלט.

*כוח חישוב אימון:* כלל כוח החישוב, ב-FLOP או יחידות אחרות, הכרוך בגרף הסיבתי של כוח החישוב של רשת נוירונים (כולל הכנת נתונים, אימון וכיוונון עדין, וכל חישוב אחר.)

*כוח חישוב פלט:* כלל כוח החישוב בגרף הסיבתי של כוח החישוב של פלט AI נתון, כולל כל הרשתות הנוירונליות (וכולל את כוח החישוב לאימון שלהן) וחישובים אחרים הנכנסים לפלט הזה.

*קצב כוח חישוב הסקה:* בסדרה של פלטים, קצב השינוי (ב-FLOP/s או יחידות אחרות) של כוח חישוב הפלט בין פלטים, כלומר כוח החישוב המשמש לייצור הפלט הבא, חלקי המרווח הזמן בין הפלטים.

**דוגמאות וקירובים:**

- עבור רשת נוירונים בודדת המאומנת על נתונים שנוצרו על ידי בני אדם, כוח החישוב לאימון הוא פשוט כלל כוח החישוב לאימון כפי שמדווח בדרך כלל.
- עבור רשת נוירונים כזאת שמבצעת הסקה בקצב קבוע, קצב כוח החישוב להסקה הוא בערך המהירות הכללית של אשכול החישוב המבצע את ההסקה ב-FLOP/s.
- עבור כיוונון עדין של מודל, כוח החישוב לאימון של המודל השלם ניתן על ידי כוח החישוב לאימון של המודל הלא-מכוונן עדין בתוספת החישוב שנעשה במהלך הכיוונון העדין ולהכנת כל נתון שנעשה בו שימוש בכיוונון העדין.
- עבור מודל מזוקק, כוח החישוב לאימון של המודל השלם כולל אימון של המודל המזוקק והמודל הגדול יותר שנעשה בו שימוש לספק נתונים סינתטיים או קלט אימון אחר.
- אם כמה מודלים מאומנים, אבל "ניסיונות" רבים מושלכים על בסיס שיקול אנושי, אלה לא נחשבים לכוח החישוב לאימון או לפלט של המודל הנשמר.

### נספח ב': דוגמה ליישום של סגירת שער

**דוגמת יישום:** להלן דוגמה אחת כיצד סגירת שער יכולה לעבוד, בהנתן גבול של 10<sup>27</sup> FLOP לאימון ו-10<sup>20</sup> FLOP/s להסקה (הפעלת ה-AI):

**1\. השהיה:** מטעמי ביטחון לאומי, הרשות המבצעת האמריקאית מבקשת מכל החברות הבסיסן בארה"ב, העוסקות בעסקים בארה"ב, או המשתמשות בשבבים המיוצרים בארה"ב, לחדול ולהפסיק כל הרצת אימון AI חדשה שעלולה לחרוג מגבול כוח החישוב לאימון של 10<sup>27</sup> FLOP. ארה"ב צריכה להתחיל דיונים עם מדינות אחרות המארחות פיתוח AI, לעודד אותן בחוזקה לנקוט צעדים דומים ולציין שההשהיה האמריקאית עשויה להיות מוסרת במידה ותבחרנה שלא לציית.

**2\. פיקוח ורישוי אמריקאי:** על ידי צו ביצועי או פעולה של סוכנות רגולטורית קיימת, ארה"ב דורשת שתוך (נאמר) שנה אחת:

- כל הרצות אימון AI המוערכות מעל 10<sup>25</sup> FLOP שנעשו על ידי חברות הפועלות בארה"ב יירשמו במסד נתונים המתוחזק על ידי סוכנות רגולטורית אמריקאית. (הערה: גרסה מעט חלשה יותר של זה כבר נכללה בצו הביצועי האמריקאי על AI משנת 2023 שבוטל, שדרש רישום עבור מודלים מעל 10<sup>26</sup> FLOP.)
- כל יצרני חומרה הרלוונטיים ל-AI הפועלים בארה"ב או עושים עסקים עם הממשלה האמריקאית יצייתו לקבוצת דרישות על החומרה המיוחדת שלהם והתוכנה המניעה אותה. (רבות מהדרישות הללו יכולות להיבנות לתוך עדכוני תוכנה וקושחה לחומרה קיימת, אבל פתרונות ארוכי טווח ויציבים ידרשו שינויים לדורות מאוחרים יותר של החומרה.) בין אלה נמצאת דרישה שאם החומרה היא חלק מאשכול מחובר במהירות גבוהה המסוגל לבצע 10<sup>18</sup> FLOP/s של חישוב, נדרשת רמה גבוהה יותר של אימות, הכוללת רשות סדירה על ידי "מושל" מרוחק המקבל גם טלמטרי וגם בקשות לבצע חישוב נוסף.
- הנאמן מדווח על כלל החישוב המבוצע על החומרה שלו לסוכנות המתחזקת את מסד הנתונים האמריקאי.
- דרישות חזקות יותר מוכנסות בהדרגה כדי לאפשר פיקוח ומתן הרשאות בטוח וגמיש יותר.

**3\. פיקוח בינלאומי:**

- ארה"ב, סין וכל מדינות אחרות המארחות יכולת ייצור שבבים מתקדמת מנהלות משא ומתן על הסכם בינלאומי.
- ההסכם הזה יוצר סוכנות בינלאומית חדשה, דומה לסוכנות הבינלאומית לאנרגיה אטומית, המופקדת על פיקוח על אימון והפעלת AI.
- מדינות חתומות חייבות לדרוש מיצרני חומרת ה-AI הביתיים שלהן לציית לקבוצת דרישות חזקות לפחות כמו אלה שהוטלו בארה"ב.
- נאמנים נדרשים כעת לדווח על מספרי חישוב AI גם לסוכנויות במדינות הבית שלהם וגם למשרד חדש בתוך הסוכנות הבינלאומית.
- מדינות נוספות מעודדות בחוזקה להצטרף להסכם הבינלאומי הקיים: בקרות יצוא על ידי מדינות חתומות מגבילות גישה לחומרה מתקדמת על ידי לא-חתומות בעוד חתומות יכולות לקבל תמיכה טכנית בניהול מערכות ה-AI שלהן.

**4\. אימות ואכיפה בינלאומיים:**

- מערכת אימות החומרה מתעדכנת כך שהיא מדווחת על שימוש בחישוב גם לנאמן המקורי וגם ישירות למשרד הסוכנות הבינלאומית.
- הסוכנות, באמצעות דיון עם החתומות על ההסכם הבינלאומי, מסכימה על הגבלות חישוב שמקבלות אז תוקף משפטי במדינות החתומות.
- במקביל, קבוצת תקנים בינלאומיים עשויה להתפתח כך שאימון והפעלה של AIs מעל סף של חישוב (אבל מתחת לגבול) נדרשים לציית לתקנים האלה.
- הסוכנות יכולה, אם נחוץ כדי לפצות על אלגוריתמים טובים יותר וכו', להוריד את גבול החישוב. או, אם זה נחשב בטוח ורצוי (נאמר ברמה של ערבויות בטיחות הניתנות להוכחה), להעלות את גבול החישוב.

### נספח ג': פרטים למשטר חבות AGI מחמיר

**פרטים למשטר חבות AGI מחמיר**

- יצירה והפעלה של מערכת AI מתקדמת שהיא כללית, בעלת יכולת ואוטונומית ברמה גבוהה, נחשבות פעילות "מסוכנת באופן חריג".
- ככזה, רמת החבות הברירת המחדל לאימון והפעלה של מערכות כאלה היא חבות קפידה, משותפת ועשויה (או המקבילה שלה מחוץ לארה"ב) עבור כל נזק שנעשה על ידי המודל או הפלטים/פעולות שלו.
- חבות אישית תוטל על מנהלים וחברי דירקטוריון במקרים של רשלנות חמורה או התנהגות זדונית בכוונה. זה צריך לכלול עונשים פליליים עבור המקרים החמורים ביותר.
- יש מספר רב של מקלטי בטיחות שתחתם החבות חוזרת לברירת המחדל (מבוססת אשמה, בארה"ב) לחבות שאנשים וחברות יהיו בדרך כלל כפופים לה.
	- מודלים מאומנים ומופעלים מתחת לסף כוח חישוב מסוים (שיהיה לפחות פי 10 נמוך יותר מהמגבלות המתוארות לעיל.)
	- AI ש"חלש" (בערך, מתחת לרמת מומחה אנושי במשימות שעבורן הוא מיועד) ו/או
	- AI ש"צר" (בעל היקף קבוע ומוגבל למדי של משימות ופעולות שהוא מתוכנן ומאומן עבורן באופן ספציפי) ו/או
	- AI ש"פסיבי" (מוגבל מאוד ביכולתו - אפילו תחת שינוי צנוע - לנקוט פעולות או לבצע משימות מורכבות רב-שלביות ללא מעורבות ובקרה אנושית ישירה.)
	- AI שמובטח להיות בטוח, מאובטח וניתן לשליטה (בטוח באופן הניתן להוכחה, או ניתוח סיכונים מעיד על רמה זניחה של נזק צפוי.)
- מקלטי בטיחות עשויים להיתבע על בסיס [מקרה בטיחות](https://arxiv.org/abs/2410.21572) שהוכן על ידי מפתח ה-AI ואושר על ידי סוכנות או מבקר המוסמך על ידי סוכנות. כדי לתבוע מקלט בטיחות על בסיס כוח חישוב, המפתח חייב רק לספק הערכות אמינות של כלל כוח החישוב לאימון וקצב הסקה מקסימלי
- החקיקה תתאר במפורש מצבים שבהם הקלה זמנית מפיתוח מערכות AI עם סיכון גבוה לנזק ציבורי תהיה מתאימה.
- קונסורציומים של חברות, העובדים עם ארגונים לא-ממשלתיים וסוכנויות ממשלתיות, צריכים לפתח תקנים ונורמות המגדירות את המונחים הללו, כיצד רגולטורים צריכים להעניק מקלטי בטיחות, כיצד מפתחי AI צריכים לפתח מקרי בטיחות, וכיצד בתי משפט צריכים לפרש חבות כאשר מקלטי בטיחות לא נתבעו באופן פעיל.

### נספח ד': גישה מדורגת לתקני בטיחות ואבטחה של AGI

**גישה מדורגת לתקני בטיחות ואבטחה של AGI**

| דרגת סיכון | מפעיל(ים) | דרישות לאימון | דרישה לפריסה |
| --- | --- | --- | --- |
| RT-0 | AI חלש באוטונומיה, כלליות ואינטליגנציה | אין | אין |
| RT-1 | AI חזק באחד מבין אוטונומיה, כלליות ואינטליגנציה | אין | על בסיס סיכון ושימוש, פוטנציאלית מקרי בטיחות מאושרים על ידי רשויות לאומיות בכל מקום שבו ניתן להשתמש במודל |
| RT-2 | AI חזק בשניים מבין אוטונומיה, כלליות ואינטליגנציה | רישום אצל הרשות הלאומית שיש לה סמכות שיפוט על המפתח | מקרה בטיחות המגביל סיכון נזק חמור מתחת לרמות מורשות בתוספת ביקורות בטיחות עצמאיות (כולל רד-טימינג קופסה שחורה ולבנה) מאושרות על ידי רשויות לאומיות בכל מקום שבו ניתן להשתמש במודל |
| RT-3 | AGI חזק באוטונומיה, כלליות ואינטליגנציה | אישור מראש של תכנית בטיחות ואבטחה על ידי הרשות הלאומית שיש לה סמכות שיפוט על המפתח | מקרה בטיחות המבטיח סיכון מוגבל של נזק חמור מתחת לרמות מורשות כמו גם מפרטים נדרשים, כולל אבטחת סייבר, יכולת שליטה, מתג הרג שלא ניתן להסרה, התאמה לערכים אנושיים ועמידות בפני שימוש זדוני. |
| RT-4 | כל מודל שגם חורג או מ-10<sup>27</sup> FLOP אימון או מ-10<sup>20</sup> FLOP/s הסקה | אסור ממתין להסכמה בינלאומית להסרת מגבלת כוח חישוב | אסור ממתין להסכמה בינלאומית להסרת מגבלת כוח חישוב |

סיווגי סיכונים ותקני בטיחות/אבטחה, עם דרגות המבוססות על ספי כוח חישוב כמו גם שילובים של אוטונומיה גבוהה, כלליות ואינטליגנציה:

- *אוטונומיה חזקה* חלה אם המערכת מסוגלת לבצע, או יכולה בקלות להיעשות לבצע, משימות רב-שלביות ו/או לנקוט פעולות מורכבות שהן רלוונטיות לעולם האמיתי, ללא פיקוח או התערבות אנושית משמעותית. דוגמאות: רכבים אוטונומיים ורובוטים; בוטים למסחר פיננסי. דוגמאות שליליות: GPT-4; מסווגי תמונות
- *כלליות חזקה* מציינת היקף יישום רחב, ביצוע משימות שעבורן המודל לא אומן במכוון ובאופן ספציפי, ויכולת משמעותית ללמוד משימות חדשות. דוגמאות: GPT-4; mu-zero. דוגמאות שליליות: AlphaFold; רכבים אוטונומיים; מחוללי תמונות
- *אינטליגנציה חזקה* מתאימה להשוואת ביצועים ברמת מומחה אנושי במשימות שעבורן המודל מתפקד הכי טוב (ועבור מודל כללי, בתחום רחב של משימות.) דוגמאות: AlphaFold; mu-zero; o3. דוגמאות שליליות: GPT-4; Siri

### הכרת תודה

כמה תודות לאנשים שתרמו ל"שמרו על העתיד אנושי".

עבודה זו משקפת את דעותיו של הכותב ואין לראותה כעמדה רשמית של מכון עתיד החיים (למרות שהן תואמות; לעמדתו הרשמית ראו [דף זה](https://futureoflife.org/our-position-on-ai/)), או כל ארגון אחר שהכותב קשור אליו.

אני אסיר תודה לבני האדם מארק ברקל, בן אייזנפרס, אנה הייר, קרלוס גוטיירז, אמיליה יאבורסקי, ריצ'רד מלח, ג'ורדן שרנהורסט, אליס פולצ'ר, מקס טגמרק ויאן טלין על הערותיהם על כתב היד; לטים שרייר על עזרה בחלק מההפניות; לטיילור ג'ונס ואליס פולצ'ר על ייפוי התרשימים.

עבודה זו עשתה שימוש מוגבל במודלי AI גנרטיביים (Claude ו-ChatGPT) ביצירתה, לצורכי עריכה ובדיקת חולשות. על פי הסטנדרט הידוע היטב של רמות מעורבות AI ביצירות יצירתיות, עבודה זו הייתה מקבלת כנראה דירוג של 3/10. (למעשה אין סטנדרט כזה! אבל כדאי שיהיה.)

אנו אסירי תודה ל[יוליוס אודאי](https://www.linkedin.com/in/julius-odai/) על הפקת הגרסה המקוונת של המאמר, שהופכת את הקריאה והניווט במאמר לחוויה נעימה מאוד. יוליוס הוא איש טכנולוגיה ומשתתף לאחרונה בקורס ממשל AI של BlueDot Impact.