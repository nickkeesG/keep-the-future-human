# Mantenere il Futuro Umano

Questo saggio argomenta il perché e il come dovremmo chiudere le porte alla IAG e alla superintelligenza, e cosa dovremmo costruire al loro posto.

Se desiderate solo i punti chiave, andate al Riassunto esecutivo. Poi, i Capitoli 2-5 forniranno alcune informazioni di base sui tipi di sistemi di IA discussi nel saggio. I Capitoli 5-7 spiegano perché potremmo aspettarci che la IAG arrivi presto, e cosa potrebbe accadere quando ciò avverrà. Infine, i Capitoli 8-9 delineano una proposta concreta per impedire che la IAG venga costruita.

[Scarica PDF](https://keepthefuturehuman.ai/wp-content/uploads/2025/03/Keep_the_Future_Human__AnthonyAguirre__5March2025.pdf)

Tempo totale di lettura: 2-3 ore

## Sommario esecutivo

Una panoramica di alto livello del saggio. Se hai poco tempo, qui puoi cogliere tutti i punti principali in soli 10 minuti.

I progressi straordinari nell'intelligenza artificiale dell'ultimo decennio (per l'IA specializzata) e degli ultimi anni (per l'IA generale) hanno trasformato l'IA da campo accademico di nicchia alla strategia aziendale centrale di molte delle più grandi aziende mondiali, con centinaia di miliardi di dollari di investimenti annuali nelle tecniche e tecnologie per far progredire le capacità dell'IA.

Ora arriviamo a un momento critico. Mentre le capacità dei nuovi sistemi di IA iniziano a eguagliare e superare quelle degli esseri umani in molti domini cognitivi, l'umanità deve decidere: fino a che punto spingersi, e in quale direzione?

L'IA, come ogni tecnologia, è nata con l'obiettivo di migliorare le cose per chi la creava. Ma la nostra traiettoria attuale, e la scelta implicita, è una corsa incontrollata verso sistemi sempre più potenti, guidata dagli incentivi economici di poche enormi aziende tecnologiche che cercano di automatizzare ampie porzioni dell'attuale attività economica e del lavoro umano. Se questa corsa continua ancora a lungo, c'è un vincitore inevitabile: l'IA stessa – un'alternativa più veloce, più intelligente e più economica alle persone nella nostra economia, nel nostro pensiero, nelle nostre decisioni, e alla fine nel controllo della nostra civiltà.

Ma possiamo fare un'altra scelta: attraverso i nostri governi, possiamo prendere il controllo del processo di sviluppo dell'IA per imporre limiti chiari, linee che non oltrepasseremo, e cose che semplicemente non faremo – come abbiamo fatto per le tecnologie nucleari, le armi di distruzione di massa, le armi spaziali, i processi ambientalmente distruttivi, la bioingegneria degli esseri umani e l'eugenetica. Più importante ancora, possiamo assicurarci che l'IA rimanga uno strumento per potenziare gli esseri umani, piuttosto che una nuova specie che ci sostituisce e alla fine ci soppiant.

Questo saggio sostiene che dovremmo *mantenere il futuro umano* chiudendo le "Porte" all'IA autonoma e generalista più intelligente dell'uomo – talvolta chiamata "IAG" – e soprattutto alla versione altamente superumana talvolta chiamata "superintelligenza". Invece, dovremmo concentrarci su strumenti di IA potenti e affidabili che possano potenziare gli individui e migliorare in modo trasformativo le capacità delle società umane di fare quello che sanno fare meglio. La struttura di questo argomento segue in breve.

### L'IA è diversa

I sistemi di IA sono fondamentalmente diversi dalle altre tecnologie. Mentre il software tradizionale segue istruzioni precise, i sistemi di IA imparano come raggiungere obiettivi senza che venga detto loro esplicitamente come fare. Questo li rende potenti: se riusciamo a definire chiaramente l'obiettivo o una metrica di successo, nella maggior parte dei casi un sistema di IA può imparare a raggiungerlo. Ma li rende anche intrinsecamente imprevedibili: non possiamo determinare in modo affidabile quali azioni intraprenderanno per raggiungere i loro obiettivi.

Sono anche in gran parte inspiegabili: benché siano in parte codice, sono principalmente un enorme insieme di numeri imperscrutabili – "pesi" delle reti neurali – che non possono essere analizzati; non siamo molto più bravi a comprendere il loro funzionamento interno di quanto lo siamo nel discernere i pensieri scrutando dentro un cervello biologico.

Questo modo fondamentale di addestrare reti neurali digitali sta aumentando rapidamente in complessità. I sistemi di IA più potenti vengono creati attraverso esperimenti computazionali massivi, usando hardware specializzato per addestrare reti neurali su enormi dataset, che vengono poi potenziati con strumenti software e sovrastrutture.

Questo ha portato alla creazione di strumenti molto potenti per creare e processare testo e immagini, eseguire ragionamento matematico e scientifico, aggregare informazioni, e interrogare interattivamente un vasto deposito di conoscenza umana.

Sfortunatamente, mentre lo sviluppo di strumenti tecnologici più potenti e affidabili è quello che *dovremmo* fare, e quello che quasi tutti vogliono e dicono di volere, non è la traiettoria su cui ci troviamo realmente.

### IAG e superintelligenza

Fin dagli albori del campo, la ricerca sull'IA si è invece concentrata su un obiettivo diverso: l'Intelligenza Artificiale Generale. Questo focus è ora diventato il focus delle aziende titaniche che guidano lo sviluppo dell'IA.

Cos'è la IAG? È spesso vagamente definita come "IA a livello umano", ma questo è problematico: quali umani, e in quali capacità è a livello umano? E che dire delle capacità superumane che ha già? Un modo più utile di comprendere la IAG è attraverso l'intersezione di tre proprietà chiave: alta **A**utonomia (indipendenza d'azione), alta **G**eneralità (ampio raggio d'azione e adattabilità), e alta **I**ntelligenza (competenza nei compiti cognitivi). I sistemi di IA attuali possono essere altamente capaci ma ristretti, o generali ma che richiedono costante supervisione umana, o autonomi ma limitati nel raggio d'azione.

Una completa I-A-G combinerebbe tutte e tre le proprietà a livelli che eguagliano o superano le capacità umane di vertice. Fondamentalmente, è questa combinazione che rende gli esseri umani così efficaci e così diversi dal software attuale; è anche quello che permetterebbe alle persone di essere sostituite in massa da sistemi digitali.

Mentre l'intelligenza umana è speciale, non è affatto un limite. I sistemi artificiali "superintelligenti" potrebbero operare centinaia di volte più velocemente, analizzare vastamente più dati e tenere enormi quantità "a mente" contemporaneamente, e formare aggregati molto più grandi ed efficaci delle collezioni di esseri umani. Potrebbero soppiantare non gli individui ma aziende, nazioni, o la nostra civiltà nel suo insieme.

### Siamo alla soglia

C'è un forte consenso scientifico che la IAG sia *possibile*. L'IA supera già le prestazioni umane in molti test generali di capacità intellettuale, inclusi recentemente ragionamento di alto livello e risoluzione di problemi. Le capacità mancanti – come apprendimento continuo, pianificazione, autoconsapevolezza e originalità – esistono tutte a qualche livello nei sistemi di IA attuali, e esistono tecniche note che probabilmente le miglioreranno tutte.

Mentre fino a pochi anni fa molti ricercatori vedevano la IAG come distante decenni, attualmente le evidenze per tempi brevi alla IAG sono forti:

- "Leggi di scala" empiricamente verificate collegano l'input computazionale alla capacità dell'IA, e le aziende sono sulla buona strada per aumentare l'input computazionale di ordini di grandezza nei prossimi anni. Le risorse umane e fiscali dedicate al progresso dell'IA ora eguagliano quelle di una dozzina di Progetti Manhattan e diversi Progetti Apollo.
- Le aziende di IA e i loro leader credono pubblicamente e privatamente che la IAG (per qualche definizione) sia raggiungibile entro pochi anni. Queste aziende hanno informazioni che il pubblico non ha, incluse alcune che hanno la prossima generazione di sistemi di IA in mano.
- Esperti predittori con track-record comprovati assegnano il 25% di probabilità alla IAG (per qualche definizione) che arrivi entro 1-2 anni, e il 50% per 2-5 anni (vedi previsioni Metaculus per IAG ['debole'](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) e ['completa'](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/)).
- L'autonomia (inclusa la pianificazione flessibile a lungo raggio) è in ritardo nei sistemi di IA, ma le principali aziende stanno ora concentrando le loro vaste risorse sullo sviluppo di sistemi di IA autonomi e hanno informalmente nominato il 2025 l'["anno dell'agente."](https://techinformed.com/2025-informed-the-year-of-agentic-ai/)
- L'IA sta contribuendo sempre più al proprio miglioramento. Una volta che i sistemi di IA saranno competenti quanto i ricercatori di IA umani nel fare ricerca sull'IA, una soglia critica per un progresso rapido verso sistemi di IA molto più potenti sarà raggiunta e probabilmente porterà a un'escalation incontrollabile nella capacità dell'IA. (Probabilmente, quell'escalation incontrollabile è già iniziata.)

L'idea che la IAG più intelligente dell'uomo sia distante decenni o più non è semplicemente più sostenibile per la grande maggioranza degli esperti del campo. I disaccordi ora riguardano quanti mesi o anni ci vorranno se restiamo su questo corso. La domanda centrale che affrontiamo è: dovremmo?

### Cosa sta guidando la corsa alla IAG

La corsa verso la IAG è guidata da forze multiple, ognuna delle quali rende la situazione più pericolosa. Le principali aziende tecnologiche vedono la IAG come la tecnologia di automazione definitiva – non solo potenziare i lavoratori umani ma sostituirli in gran parte o interamente. Per le aziende, il premio è enorme: l'opportunità di catturare una frazione significativa dei 100 trilioni di dollari di produzione economica annuale mondiale automatizzando i costi del lavoro umano.

Le nazioni si sentono costrette a unirsi a questa corsa, citando pubblicamente leadership economica e scientifica, ma vedendo privatamente la IAG come una potenziale rivoluzione negli affari militari paragonabile alle armi nucleari. La paura che i rivali possano ottenere un vantaggio strategico decisivo crea una classica dinamica di corsa agli armamenti.

Coloro che perseguono la superintelligenza spesso citano visioni grandiose: curare tutte le malattie, invertire l'invecchiamento, raggiungere progressi nell'energia e nei viaggi spaziali, o creare capacità di pianificazione superumane.

Meno caritatevolmente, quello che guida la corsa è il potere. Ogni partecipante – che sia azienda o paese – crede che intelligenza equivalga a potere, e che sarà il miglior custode di quel potere.

Sostengo che queste motivazioni sono reali ma fondamentalmente sbagliate: la IAG *assorbirà* e *cercherà* potere piuttosto che concederlo; le tecnologie create dall'IA saranno *anche* fortemente a doppio taglio, e dove benefiche possono essere create con strumenti di IA e senza IAG; e anche nella misura in cui la IAG e i suoi risultati rimangano sotto controllo, queste dinamiche di corsa – sia aziendali che geopolitiche – rendono i rischi su larga scala per la nostra società quasi inevitabili a meno che non vengano decisivamente interrotti.

### La IAG e la superintelligenza pongono una minaccia drammatica alla civiltà

Nonostante il loro fascino, la IAG e la superintelligenza pongono minacce drammatiche alla civiltà attraverso percorsi multipli e rinforzanti:

*Concentrazione di potere:* l'IA superumana potrebbe privare di potere la grande maggioranza dell'umanità assorbendo enormi porzioni di attività sociale ed economica in sistemi di IA gestiti da una manciata di aziende gigantesche (che a loro volta potrebbero essere prese in consegna da, o effettivamente prendere in consegna, i governi.)

*Disruption massiva:* automazione in massa della maggior parte dei lavori basati su cognizione, sostituzione dei nostri attuali sistemi epistemici, e dispiegamento di un vasto numero di agenti attivi non umani sconvolgerebbe la maggior parte dei nostri attuali sistemi civilizzazionali in un periodo relativamente breve.

*Catastrofi:* proliferando la capacità – potenzialmente sopra il livello umano – di creare nuove tecnologie militari e distruttive e scollegandola dai sistemi sociali e legali che fondano la responsabilità, le catastrofi fisiche da armi di distruzione di massa diventano drammaticamente più probabili.

*Geopolitica e guerra:* le maggiori potenze mondiali non staranno a guardare se sentono che una tecnologia che potrebbe fornire un "vantaggio strategico decisivo" viene sviluppata dai loro avversari.

*Escalation incontrollabile e perdita di controllo:* A meno che non venga specificamente impedito, l'IA superumana avrà ogni incentivo a migliorare ulteriormente se stessa e potrebbe superare di gran lunga gli esseri umani in velocità, elaborazione dati e sofisticazione del pensiero. Non c'è modo significativo in cui possiamo essere in controllo di un tale sistema. Tale IA non concederà potere agli esseri umani; noi concederemo potere a essa, o se lo prenderà.

Molti di questi rischi rimangono anche se il problema tecnico dell'"allineamento" – assicurare che l'IA avanzata faccia affidabilmente quello che gli esseri umani vogliono che faccia – viene risolto. L'IA presenta un'enorme sfida in come verrà gestita, e moltissimi aspetti di questa gestione diventano incredibilmente difficili o intrattabili quando l'intelligenza umana viene superata.

Più fondamentalmente, il tipo di IA generalista superumana attualmente perseguita avrebbe, per sua stessa natura, obiettivi, agentività e capacità che superano i nostri. Sarebbe intrinsecamente incontrollabile – come possiamo controllare qualcosa che non possiamo né capire né predire? Non sarebbe uno strumento tecnologico per uso umano, ma una seconda specie di intelligenza sulla Terra accanto alla nostra. Se le fosse permesso di progredire ulteriormente, costituirebbe non solo una seconda specie ma una specie sostitutiva.

Forse ci tratterebbe bene, forse no. Ma il futuro apparterrebbe a essa, non a noi. L'era umana sarebbe finita.

### Questo non è inevitabile; l'umanità può, molto concretamente, decidere di non costruire il nostro sostituto.

La creazione di una IAG superumana è tutt'altro che inevitabile. Possiamo impedirla attraverso un insieme coordinato di misure di governance:

Primo, abbiamo bisogno di contabilità e supervisione robuste della capacità computazionale dell'IA, che è un abilitatore fondamentale di, e una leva per governare, i sistemi di IA su larga scala. Questo a sua volta richiede misurazione e reportistica standardizzate della capacità computazionale totale usata nell'addestrare modelli di IA e farli funzionare, e metodi tecnici di conteggio, certificazione e verifica della capacità computazionale utilizzata.

Secondo, dovremmo implementare limiti computazionali rigidi sulla capacità computazionale dell'IA, sia per l'addestramento che per l'operazione; questi impediscono all'IA sia di essere troppo potente che di operare troppo velocemente. Questi limiti possono essere implementati sia attraverso requisiti legali che misure di sicurezza basate su hardware integrate nei chip specializzati per l'IA, analoghe alle funzionalità di sicurezza nei telefoni moderni. Poiché l'hardware specializzato per l'IA è prodotto solo da una manciata di aziende, verifica ed enforcement sono fattibili attraverso la catena di fornitura esistente.

Terzo, abbiamo bisogno di responsabilità rafforzata per i sistemi di IA più pericolosi. Coloro che sviluppano IA che combina alta autonomia, ampia generalità e intelligenza superiore dovrebbero affrontare responsabilità rigorosa per i danni, mentre porti sicuri da questa responsabilità incoraggierebbero lo sviluppo di sistemi più limitati e controllabili.

Quarto, abbiamo bisogno di regolamentazione stratificata basata sui livelli di rischio. I sistemi più capaci e pericolosi richiederebbero estensive garanzie di sicurezza e controllabilità prima dello sviluppo e del dispiegamento, mentre i sistemi meno potenti o più specializzati affronterebbero supervisione proporzionata. Questo quadro regolamentare dovrebbe alla fine operare sia a livello nazionale che internazionale.

Questo approccio – con specificazione dettagliata data nel documento completo – è pratico: mentre sarà necessaria coordinazione internazionale, verifica ed enforcement possono funzionare attraverso il piccolo numero di aziende che controllano la catena di fornitura dell'hardware specializzato. È anche flessibile: le aziende possono ancora innovare e trarre profitto dallo sviluppo dell'IA, solo con limiti chiari sui sistemi più pericolosi.

Il contenimento a lungo termine del potere e del rischio dell'IA richiederebbe accordi internazionali basati sia sull'interesse proprio che comune, proprio come fa ora il controllo della proliferazione delle armi nucleari. Ma possiamo iniziare immediatamente con supervisione e responsabilità rafforzate, mentre costruiamo verso una governance più comprensiva.

L'ingrediente chiave mancante è la volontà politica e sociale di prendere il controllo del processo di sviluppo dell'IA. La fonte di quella volontà, se arriverà in tempo, sarà la realtà stessa – cioè, dalla realizzazione diffusa delle reali implicazioni di quello che stiamo facendo.

### Possiamo progettare IA Strumentale per potenziare l'umanità

Piuttosto che perseguire IAG incontrollabile, possiamo sviluppare potente "IA Strumentale" che migliora la capacità umana rimanendo sotto controllo umano significativo. I sistemi di IA Strumentale possono essere estremamente capaci evitando la pericolosa tripla intersezione di alta autonomia, ampia generalità e intelligenza superumana, finché li progettiamo per essere controllabili a un livello commisurato alla loro capacità. Possono anche essere combinati in sistemi sofisticati che mantengono supervisione umana mentre forniscono benefici trasformativi.

L'IA Strumentale può rivoluzionare la medicina, accelerare la scoperta scientifica, migliorare l'educazione e migliorare i processi democratici. Quando governata correttamente, può rendere esperti umani e istituzioni più efficaci piuttosto che sostituirli. Mentre tali sistemi saranno ancora altamente dirompenti e richiederanno gestione attenta, i rischi che pongono sono fondamentalmente diversi dalla IAG: sono rischi che possiamo governare, come quelli di altre tecnologie potenti, non minacce esistenziali all'agentività umana e alla civiltà. E fondamentalmente, quando sviluppati saggiamente, gli strumenti di IA possono aiutare le persone a governare l'IA potente e gestire i suoi effetti.

Questo approccio richiede ripensare sia come l'IA viene sviluppata che come i suoi benefici vengono distribuiti. Nuovi modelli di sviluppo di IA pubblico e non-profit, quadri regolamentari robusti, e meccanismi per distribuire i benefici economici più ampiamente possono aiutare a garantire che l'IA potenzi l'umanità nel suo insieme piuttosto che concentrare potere in poche mani. L'IA stessa può aiutare a costruire istituzioni sociali e di governance migliori, abilitando nuove forme di coordinazione e discorso che rafforzano piuttosto che indebolire la società umana. Gli apparati di sicurezza nazionale possono sfruttare la loro esperienza per rendere i sistemi di strumenti di IA genuinamente sicuri e affidabili, e una vera fonte di difesa così come potere nazionale.

Potremmo alla fine scegliere di sviluppare sistemi ancora più potenti e più sovrani che sono meno come strumenti e – possiamo sperare – più come benefattori saggi e potenti. Ma dovremmo farlo solo dopo aver sviluppato la comprensione scientifica e la capacità di governance per farlo in sicurezza. Una decisione così momentosa e irreversibile dovrebbe essere presa deliberatamente dall'umanità nel suo insieme, non di default in una corsa tra aziende tecnologiche e nazioni.

### Nelle mani umane

Le persone vogliono il bene che viene dall'IA: strumenti utili che li potenziano, sovralimentano opportunità e crescita economiche, e promettono progressi nella scienza, tecnologia ed educazione. Perché non dovrebbero? Ma quando interrogate, maggioranze schiaccianti del pubblico generale [vogliono sviluppo dell'IA più lento e più attento](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation), e non vogliono IA più intelligente dell'uomo che li sostituirà nei loro lavori e altrove, riempirà la loro cultura e spazi informativi comuni con contenuto non umano, concentrerà potere in un piccolo insieme di aziende, porrà rischi estremi su larga scala globale, e alla fine minaccerà di privare di potere o sostituire la loro specie. Perché dovrebbero?

*Possiamo* avere l'uno senza l'altro. Inizia decidendo che il nostro destino non è nella supposta inevitabilità di qualche tecnologia o nelle mani di pochi CEO della Silicon Valley, ma nelle nostre mani se lo afferriamo. Chiudiamo le Porte, e manteniamo il futuro umano.

## Capitolo 1 - Introduzione

Il modo in cui risponderemo alla prospettiva di un'IA più intelligente degli esseri umani è la questione più urgente del nostro tempo. Questo saggio fornisce una strada da seguire.

Potremmo trovarci alla fine dell'era umana.

Negli ultimi dieci anni è iniziato qualcosa di unico nella storia della nostra specie. Le sue conseguenze determineranno, in larga misura, il futuro dell'umanità. A partire dal 2015 circa, i ricercatori sono riusciti a sviluppare intelligenza artificiale (IA) *specializzata* – sistemi in grado di vincere a giochi come il Go, riconoscere immagini e linguaggio parlato, e altro ancora, meglio di qualsiasi essere umano.[^1]

Si tratta di un successo straordinario, che sta producendo sistemi e prodotti estremamente utili che potenzieranno l'umanità. Ma l'intelligenza artificiale specializzata non è mai stata il vero obiettivo del settore. Piuttosto, l'obiettivo è stato quello di creare sistemi di IA *generali*, in particolare quelli spesso chiamati "intelligenza artificiale generale" (IAG) o "superintelligenza" che sono contemporaneamente buoni o migliori degli umani in quasi *tutte* le attività, proprio come l'IA è ora sovrumana nel Go, negli scacchi, nel poker, nelle corse di droni, ecc. Questo è l'obiettivo dichiarato di molte grandi aziende di IA.[^2]

*Questi sforzi stanno anche avendo successo.* I sistemi di IA generali come ChatGPT, Gemini, Llama, Grok, Claude e Deepseek, basati su calcoli massicci e montagne di dati, hanno raggiunto la parità con gli esseri umani comuni in un'ampia varietà di compiti, e persino eguagliato esperti umani in alcuni domini. Ora gli ingegneri di IA di alcune delle più grandi aziende tecnologiche stanno correndo per spingere questi giganteschi esperimenti di intelligenza artificiale ai livelli successivi, in cui eguagliano e poi superano l'intera gamma delle capacità, competenze e autonomia umane.

*Questo è imminente.* Negli ultimi dieci anni, le stime degli esperti su quanto tempo ci vorrà – se continuiamo il nostro corso attuale – sono scese da decenni (o secoli) a pochi anni.

È anche di importanza epocale e di rischio trascendente. I sostenitori della IAG la vedono come una trasformazione positiva che risolverà problemi scientifici, curerà malattie, svilupperà nuove tecnologie e automatizzerà il lavoro faticoso. E l'IA potrebbe certamente aiutare a raggiungere tutte queste cose – infatti lo sta già facendo. Ma nel corso dei decenni, molti pensatori attenti, da Alan Turing a Stephen Hawking fino agli odierni Geoffrey Hinton e Yoshua Bengio [^3] hanno lanciato un monito severo: costruire un'IA davvero più intelligente degli umani, generale e autonoma, come minimo sconvolgerà completamente e irrevocabilmente la società, e al massimo comporterà l'estinzione umana.[^4]

L'IA superintelligente si sta rapidamente avvicinando sulla nostra strada attuale, ma è tutt'altro che inevitabile. Questo saggio è un'argomentazione estesa sul perché e su come dovremmo *chiudere le Porte* a questo futuro inumano che si avvicina, e cosa dovremmo fare invece.


[^1]: Questo [grafico](https://time.com/6300942/ai-progress-charts/) mostra una serie di compiti; molte curve simili potrebbero essere aggiunte a questo grafico. Questo rapido progresso nell'IA specializzata ha sorpreso anche gli esperti del settore, con benchmark superati anni prima delle previsioni.

[^2]: Deepmind, OpenAI, Anthropic e X.ai sono state tutte fondate con l'obiettivo specifico di sviluppare la IAG. Ad esempio, lo statuto di OpenAI dichiara esplicitamente il suo obiettivo come sviluppare "intelligenza artificiale generale che benefici tutta l'umanità", mentre la missione di DeepMind è "risolvere l'intelligenza, e poi usarla per risolvere tutto il resto". Meta, Microsoft e altri stanno ora perseguendo percorsi sostanzialmente simili. Meta ha dichiarato di [pianificare lo sviluppo della IAG e di rilasciarla apertamente.](https://www.forbes.com/sites/johnkoetsier/2024/01/18/zuckerberg-on-ai-meta-building-agi-for-everyone-and-open-sourcing-it/)

[^3]: Hinton e Bengio sono due dei ricercatori di IA più citati, hanno entrambi vinto il Nobel del settore IA, il Premio Turing, e Hinton ha vinto anche un premio Nobel (in fisica).

[^4]: Costruire qualcosa di questo rischio, sotto incentivi commerciali e con una supervisione governativa quasi nulla, è assolutamente senza precedenti. Non c'è nemmeno controversia sul rischio tra coloro che lo stanno costruendo! I leader di Deepmind, OpenAI e Anthropic, tra molti altri esperti, hanno tutti letteralmente firmato una [dichiarazione](https://www.safe.ai/work/statement-on-ai-risk) che l'IA avanzata pone un *rischio di estinzione per l'umanità.* I campanelli d'allarme non potrebbero suonare più forte, e si può solo concludere che coloro che li ignorano semplicemente non stanno prendendo sul serio la IAG e la superintelligenza. Uno degli obiettivi di questo saggio è aiutarli a capire perché dovrebbero.

## Capitolo 2 - Nozioni essenziali sulle reti neurali dell'IA

Come funzionano i sistemi di IA moderni, e cosa potrebbe arrivare nella prossima generazione di IA?

Per comprendere come si svilupperanno le conseguenze dello sviluppo di un'IA più potente, è essenziale interiorizzare alcuni concetti fondamentali. Questa e le prossime due sezioni li sviluppano, coprendo a turno cos'è l'IA moderna, come sfrutta calcoli massicci, e i modi in cui sta crescendo rapidamente in generalità e capacità.[^5]

Ci sono molti modi per definire l'intelligenza artificiale, ma per i nostri scopi la proprietà chiave dell'IA è che mentre un programma informatico standard è una lista di istruzioni su come eseguire un compito, un sistema di IA è uno che impara da dati o esperienza per eseguire compiti *senza che gli venga esplicitamente detto come farlo.*

Quasi tutta l'IA moderna rilevante è basata su reti neurali. Queste sono strutture matematiche/computazionali, rappresentate da un insieme molto grande (miliardi o trilioni) di numeri ("pesi"), che eseguono bene un compito di addestramento. Questi pesi vengono elaborati (o forse "coltivati" o "trovati") aggiustandoli iterativamente in modo che la rete neurale migliori un punteggio numerico (noto anche come "perdita") definito per eseguire bene uno o più compiti.[^6] Questo processo è noto come *addestramento* della rete neurale.[^7]

Ci sono molte tecniche per fare questo addestramento, ma quei dettagli sono molto meno rilevanti dei modi in cui viene definito il punteggio, e di come questi risultino in diversi compiti che la rete neurale esegue bene. Una distinzione chiave è stata storicamente tracciata tra IA "ristretta" e "generale".

L'IA ristretta è deliberatamente addestrata per fare un compito particolare o un piccolo insieme di compiti (come riconoscere immagini o giocare a scacchi); richiede un nuovo addestramento per nuovi compiti, e ha un ambito ristretto di capacità. Abbiamo IA ristretta sovrumana, nel senso che per quasi qualsiasi compito discreto e ben definito che una persona può fare, probabilmente possiamo costruire un punteggio e poi addestrare con successo un sistema di IA ristretta per farlo meglio di quanto potrebbe fare un umano.

I sistemi di IA per uso generale (GPAI) possono eseguire un'ampia gamma di compiti, inclusi molti per cui non sono stati esplicitamente addestrati; possono anche imparare nuovi compiti come parte della loro operazione. Gli attuali "modelli multimodali" di grandi dimensioni [^8] come ChatGPT esemplificano questo: addestrati su un corpus molto grande di testo e immagini, possono impegnarsi in ragionamenti complessi, scrivere codice, analizzare immagini, e assistere con una vasta gamma di compiti intellettuali. Pur essendo ancora abbastanza diversi dall'intelligenza umana in modi che vedremo in profondità più avanti, la loro generalità ha causato una rivoluzione nell'IA.[^9]

### Imprevedibilità: una caratteristica chiave dei sistemi di IA

Una differenza chiave tra i sistemi di IA e il software convenzionale è nella prevedibilità. L'output del software standard può essere imprevedibile – infatti a volte è proprio per questo che scriviamo software, per darci risultati che non avremmo potuto prevedere. Ma il software convenzionale raramente fa qualcosa per cui non è stato programmato – il suo ambito e comportamento sono generalmente come progettati. Un programma di scacchi di alto livello può fare mosse che nessun umano potrebbe prevedere (altrimenti potrebbero battere quel programma di scacchi!) ma generalmente non farà altro che giocare a scacchi.

Come il software convenzionale, l'IA ristretta ha ambito e comportamento prevedibili ma può avere risultati imprevedibili. Questo è davvero solo un altro modo per definire l'IA ristretta: come IA che è simile al software convenzionale nella sua prevedibilità e gamma di operazioni.

L'IA per uso generale è diversa: il suo ambito (i domini su cui si applica), comportamento (i tipi di cose che fa), e risultati (i suoi output effettivi) possono tutti essere imprevedibili.[^10] GPT-4 è stato addestrato solo per generare testo accuratamente, ma ha sviluppato molte capacità che i suoi addestratori non avevano previsto o inteso. Questa imprevedibilità deriva dalla complessità dell'addestramento: poiché i dati di addestramento contengono output da molti compiti diversi, l'IA deve effettivamente imparare a eseguire questi compiti per predire bene.

Questa imprevedibilità dei sistemi di IA generale è abbastanza fondamentale. Anche se in principio è possibile costruire attentamente sistemi di IA che abbiano limiti garantiti sul loro comportamento (come menzionato più avanti nel saggio), il modo in cui i sistemi di IA vengono creati ora li rende imprevedibili nella pratica e persino in principio.

### IA passiva, agenti, sistemi autonomi, e allineamento

Questa imprevedibilità diventa particolarmente importante quando consideriamo come i sistemi di IA vengono effettivamente distribuiti e usati per raggiungere vari obiettivi.

Molti sistemi di IA sono relativamente passivi nel senso che forniscono principalmente informazioni, e l'utente intraprende azioni. Altri, comunemente chiamati *agenti*, intraprendono azioni da soli, con diversi livelli di coinvolgimento da parte di un utente. Quelli che intraprendono azioni con relativamente meno input o supervisione esterni possono essere chiamati più *autonomi*. Questo forma uno spettro in termini di indipendenza d'azione, da strumenti passivi ad agenti autonomi.[^11]

Per quanto riguarda gli obiettivi dei sistemi di IA, questi possono essere direttamente legati al loro obiettivo di addestramento (ad esempio l'obiettivo di "vincere" per un sistema che gioca a Go è anche esplicitamente ciò per cui è stato addestrato). O potrebbero non esserlo: l'obiettivo di addestramento di ChatGPT è in parte predire il testo, in parte essere un assistente utile. Ma quando fa un compito dato, il suo obiettivo gli viene fornito dall'utente. Gli obiettivi possono anche essere creati da un sistema di IA stesso, solo indirettamente correlati al suo obiettivo di addestramento.[^12]

Gli obiettivi sono strettamente legati alla questione dell'"allineamento", cioè la questione se i sistemi di IA *faranno quello che vogliamo che facciano*. Questa semplice domanda nasconde un enorme livello di sottigliezza.[^13] Per ora, nota che "noi" in questa frase potrebbe riferirsi a molte persone e gruppi diversi, portando a diversi tipi di allineamento. Per esempio, un'IA potrebbe essere altamente *obbediente* (o ["leale"](https://arxiv.org/abs/2003.11157)) al suo utente – qui "noi" è "ciascuno di noi." O potrebbe essere più *sovrana*, essendo principalmente guidata dai propri obiettivi e vincoli, ma agendo comunque ampiamente nell'interesse comune del benessere umano – "noi" è allora "l'umanità" o "la società." Nel mezzo c'è uno spettro dove un'IA sarebbe largamente obbediente, ma potrebbe rifiutarsi di intraprendere azioni che danneggino altri o la società, violino la legge, ecc.

Questi due assi – livello di autonomia e tipo di allineamento – non sono del tutto indipendenti. Per esempio, un sistema passivo sovrano, pur non essendo del tutto auto-contraddittorio, è un concetto in tensione, come lo è un agente autonomo obbediente.[^14] C'è un senso chiaro in cui autonomia e sovranità tendono ad andare di pari passo. In modo simile, la prevedibilità tende ad essere più alta nei sistemi di IA "passivi" e "obbedienti", mentre quelli sovrani o autonomi tenderanno ad essere più imprevedibili. Tutto questo sarà cruciale per comprendere le ramificazioni della potenziale IAG e superintelligenza.

Creare un'IA veramente allineata, di qualsiasi tipo, richiede la risoluzione di tre sfide distinte:

1. Capire cosa "noi" vogliamo – che è complesso sia che "noi" significhi una persona o organizzazione specifica (lealtà) o l'umanità in generale (sovranità);
2. Costruire sistemi che agiscano regolarmente in accordo con quei desideri – essenzialmente creare comportamento positivo consistente;
3. Più fondamentalmente, rendere sistemi che genuinamente "si preoccupano" di quei desideri piuttosto che agire meramente come se lo facessero.

La distinzione tra comportamento affidabile e genuina cura è cruciale. Proprio come un impiegato umano potrebbe seguire gli ordini perfettamente pur mancando di qualsiasi reale impegno verso la missione dell'organizzazione, un sistema di IA potrebbe agire allineato senza valorizzare veramente le preferenze umane. Possiamo addestrare i sistemi di IA a dire e fare cose attraverso feedback, e possono imparare a ragionare su cosa vogliono gli umani. Ma fargli *genuinamente* valorizzare le preferenze umane è una sfida molto più profonda.[^15]

Le profonde difficoltà nel risolvere queste sfide di allineamento, e le loro implicazioni per il rischio dell'IA, saranno esplorate ulteriormente più avanti. Per ora, capisci che l'allineamento non è solo una caratteristica tecnica che attacchiamo ai sistemi di IA, ma un aspetto fondamentale della loro architettura che modella la loro relazione con l'umanità.

[^5]: Per un'introduzione gentile ma tecnica all'apprendimento automatico e all'IA, particolarmente ai modelli linguistici, vedi [questo sito.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) Per un altro primer moderno sui rischi di estinzione dell'IA, vedi [questo pezzo.](https://www.thecompendium.ai/) Per un'analisi scientifica completa e autorevole sullo stato della sicurezza dell'IA, vedi il recente [International AI Safety Report.](https://arxiv.org/abs/2501.17805)

[^6]: L'addestramento tipicamente avviene cercando un massimo locale del punteggio in uno spazio ad alta dimensionalità dato dai pesi del modello. Controllando come cambia il punteggio quando i pesi vengono modificati, l'algoritmo di addestramento identifica quali modifiche migliorano il punteggio di più, e muove i pesi in quella direzione.

[^7]: Per esempio, in un problema di riconoscimento immagini, la rete neurale produrrebbe probabilità per etichette per l'immagine. Un punteggio sarebbe correlato alla probabilità che l'IA accorda alla risposta corretta. La procedura di addestramento modificherebbe quindi i pesi in modo che la prossima volta, l'IA produrrebbe una probabilità più alta per l'etichetta corretta per quell'immagine. Questo viene poi ripetuto un enorme numero di volte. La stessa procedura di base viene usata nell'addestramento di essenzialmente tutte le reti neurali moderne, anche se con meccanismi di punteggio più complessi.

[^8]: La maggior parte dei modelli multimodali usa l'architettura "transformer" per elaborare e generare tipi multipli di dati (testo, immagini, suono). Questi possono tutti essere decomposti in, e poi trattati sulla stessa base, come diversi tipi di "token." I modelli multimodali sono addestrati prima per predire accuratamente token all'interno di dataset massicci, poi perfezionati attraverso apprendimento per rinforzo per migliorare le capacità e modellare i comportamenti.

[^9]: Il fatto che i modelli linguistici siano addestrati per fare una cosa – predire parole – ha portato alcuni a chiamarli IA ristretta. Ma questo è fuorviante: poiché predire il testo bene richiede così tante capacità diverse, questo compito di addestramento porta a un sistema sorprendentemente generale. Nota anche che questi sistemi sono estensivamente addestrati dall'apprendimento per rinforzo, rappresentando effettivamente migliaia di persone che danno al modello un segnale di ricompensa quando fa un buon lavoro in qualsiasi delle molte cose che fa. Eredita quindi significativa generalità dalle persone che danno questo feedback.

[^10]: Ci sono modi multipli in cui l'IA è imprevedibile. Uno è che nel caso generale non si può predire cosa farà un algoritmo senza effettivamente eseguirlo; ci sono [teoremi](https://arxiv.org/abs/1310.3225) a questo effetto. Questo può essere vero solo perché l'output degli algoritmi può essere complesso. Ma è particolarmente chiaro e rilevante nel caso (come negli scacchi o nel Go) dove la predizione implicherebbe una capacità (battere l'IA) che il potenziale predittore non ha. Secondo, un dato sistema di IA non produrrà sempre lo stesso output anche dato lo stesso input – i suoi output contengono casualità; questo si accoppia anche con l'imprevedibilità algoritmica. Terzo, capacità inaspettate ed emergenti possono sorgere dall'addestramento, significando che persino i *tipi* di cose che un sistema di IA può e farà sono imprevedibili; Quest'ultimo tipo è particolarmente importante per considerazioni di sicurezza.

[^11]: Vedi [qui](https://arxiv.org/abs/2502.02649) per una revisione approfondita di cosa si intende per "agente autonomo" (insieme ad argomenti etici contro la loro costruzione).

[^12]: Potresti a volte sentire "l'IA non può avere i propri obiettivi." Questo è un assoluto nonsense. È facile generare esempi dove l'IA ha o sviluppa obiettivi che non le sono mai stati dati e sono noti solo a se stessa. Non vedi questo molto nei modelli multimodali popolari attuali perché viene addestrato fuori di loro; potrebbe altrettanto facilmente essere addestrato dentro di loro.

[^13]: C'è una vasta letteratura. Sul problema generale vedi *The Alignment Problem* di Christian, e *Human-Compatible* di Russell. Su un lato più tecnico vedi ad es. [questo paper](https://arxiv.org/abs/2209.00626).

[^14]: Vedremo più tardi che mentre tali sistemi vanno contro la tendenza, questo li rende effettivamente molto interessanti e utili.

[^15]: Questo non vuol dire che richiediamo emozioni o senzienza. Piuttosto, è enormemente difficile dall'esterno di un sistema sapere quali sono i suoi obiettivi interni, preferenze, e valori. "Genuino" qui significherebbe che abbiamo ragioni abbastanza forti per affidarci ad esso che nel caso di sistemi critici possiamo scommetterci le nostre vite.

## Capitolo 3 - Aspetti chiave della costruzione dei moderni sistemi di IA generale

La maggior parte dei sistemi di IA più avanzati al mondo viene creata utilizzando metodi sorprendentemente simili. Ecco le basi.

Per comprendere davvero un essere umano bisogna conoscere qualcosa di biologia, evoluzione, educazione infantile e altro ancora; per comprendere l'IA bisogna anche sapere come viene creata. Negli ultimi cinque anni, i sistemi di IA si sono evoluti enormemente sia in termini di capacità che di complessità. Un fattore abilitante chiave è stata la disponibilità di quantità molto ampie di capacità computazionale (o colloquialmente "compute" quando applicata all'IA).

I numeri sono impressionanti. Circa 10<sup>25</sup>-10<sup>26</sup> "operazioni in virgola mobile" (FLOP)[^16] vengono utilizzate nell'addestramento di modelli come la serie GPT, Claude, Gemini, ecc.[^17] (Per confronto, se ogni essere umano sulla Terra lavorasse ininterrottamente facendo un calcolo ogni cinque secondi, ci vorrebbero circa un miliardo di anni per realizzare questo.) Questa enorme quantità di calcolo consente l'addestramento di modelli con fino a trilioni di pesi del modello su terabyte di dati – una grande frazione di tutto il testo di qualità che sia mai stato scritto insieme a ampie librerie di suoni, immagini e video. Completando questo addestramento con ulteriore addestramento estensivo che rinforza le preferenze umane e le buone prestazioni nei compiti, i modelli addestrati in questo modo mostrano prestazioni competitive con quelle umane attraverso una gamma significativa di compiti intellettuali di base, inclusi ragionamento e risoluzione di problemi.

Sappiamo anche (molto, molto approssimativamente) quanta velocità computazionale, in operazioni al secondo, è sufficiente perché la velocità di *inferenza*[^18] di un tale sistema corrisponda alla *velocità* dell'elaborazione del testo umano. È circa 10<sup>15</sup>-10<sup>16</sup> FLOP al secondo.[^19]

Pur essendo potenti, questi modelli sono per loro natura limitati in modi chiave, abbastanza analoghi a come un singolo essere umano sarebbe limitato se fosse costretto a produrre semplicemente testo a un tasso fisso di parole al minuto, senza fermarsi a pensare o utilizzare strumenti aggiuntivi. I sistemi di IA più recenti affrontano queste limitazioni attraverso un processo e un'architettura più complessi che combinano diversi elementi chiave:

- Una o più reti neurali, con un modello che fornisce la capacità cognitiva principale, e fino a diversi altri che svolgono altri compiti più specifici;
- *Strumenti* forniti e utilizzabili dal modello – ad esempio la capacità di cercare sul web, creare o modificare documenti, eseguire programmi, ecc.
- *Architettura di supporto* che collega input e output delle reti neurali. Un'architettura molto semplice potrebbe semplicemente permettere a due "istanze" di un modello di IA di conversare tra loro, o a una di controllare il lavoro di un'altra.[^20]
- Tecniche di *catena di ragionamento* e prompting correlate fanno qualcosa di simile, causando un modello a generare ad esempio molti approcci a un problema, poi elaborare quegli approcci per una risposta aggregata.
- *Riaddestramento* dei modelli per fare un uso migliore di strumenti, architettura di supporto e catena di ragionamento.

Poiché queste estensioni possono essere molto potenti (e includere i sistemi di IA stessi), questi sistemi compositi possono essere piuttosto sofisticati e migliorare drasticamente le capacità dell'IA.[^21] E recentemente, tecniche nell'architettura di supporto e specialmente nel prompting a catena di ragionamento (e nel reintegrare i risultati nell'riaddestramento dei modelli per usare questi meglio) sono state sviluppate e impiegate in [o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/), e [DeepSeek R1](https://api-docs.deepseek.com/news/news250120) per fare molti passaggi di inferenza in risposta a una data query.[^22] Questo in effetti permette al modello di "riflettere" sulla sua risposta e aumenta drasticamente la capacità di questi modelli di fare ragionamenti di alto livello in compiti di scienza, matematica e programmazione.[^23]

Per una data architettura di IA, gli aumenti nella capacità computazionale di addestramento [possono essere tradotti in modo affidabile](https://arxiv.org/abs/2405.10938) in miglioramenti in un insieme di metriche chiaramente definite. Per capacità generali meno definite con precisione (come quelle discusse di seguito), la traduzione è meno chiara e predittiva, ma è quasi certo che modelli più grandi con più capacità computazionale di addestramento avranno capacità nuove e migliori, anche se è difficile predire quali saranno.

Allo stesso modo, i sistemi compositi e specialmente i progressi nella "catena di ragionamento" (e l'addestramento di modelli che funzionano bene con essa) hanno sbloccato la scalabilità nella capacità computazionale di *inferenza*: per un dato modello principale addestrato, almeno alcune capacità del sistema di IA aumentano quando viene applicato più calcolo che permette loro di "pensare più intensamente e a lungo" sui problemi complessi. Questo comporta un costo ripido in termini di velocità di calcolo, richiedendo centinaia o migliaia di FLOP/s in più per eguagliare le prestazioni umane.[^24]

Pur essendo solo una parte di ciò che sta portando a rapidi progressi nell'IA,[^25] il ruolo del calcolo e la possibilità di sistemi compositi si riveleranno cruciali sia per prevenire l'IAG incontrollabile che per sviluppare alternative più sicure.

[^16]: 10<sup>27</sup> significa 1 seguito da 25 zeri, o dieci trilioni di trilioni. Un FLOP è semplicemente un'addizione o moltiplicazione aritmetica di numeri con una certa precisione. Si noti che le prestazioni dell'hardware di IA possono variare di un fattore di dieci in più a seconda della precisione dell'aritmetica e dell'architettura del computer. Contare le operazioni di porte logiche (AND, OR, AND NOT) sarebbe fondamentale ma queste non sono comunemente disponibili o benchmarkate; per i presenti scopi è utile standardizzare sulle operazioni a 16 bit (FP16), anche se dovrebbero essere stabiliti fattori di conversione appropriati.

[^17]: Una raccolta di stime e dati concreti è disponibile da [Epoch AI](https://epochai.org/data/large-scale-ai-models) e indica circa 2×10<sup>25</sup> FLOP a 16 bit per GPT-4; questo corrisponde approssimativamente ai [numeri che sono trapelati](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) per GPT-4. Le stime per altri modelli di metà 2024 sono tutte entro un fattore di pochi rispetto a GPT-4.

[^18]: L'inferenza è semplicemente il processo di generare un output da una rete neurale. L'addestramento può essere considerato una successione di molte inferenze e modifiche dei pesi del modello.

[^19]: Per la produzione di testo, il GPT-4 originale richiedeva 560 TFLOP per token generato. Circa 7 token/s sono necessari per stare al passo con il pensiero umano, quindi questo dà ≈3×10<sup>15</sup> FLOP/s. Ma le efficienze hanno ridotto questo; [questa brochure NVIDIA](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/) per esempio indica appena 3×10<sup>14</sup> FLOP/s per un modello Llama 405B con prestazioni comparabili.

[^20]: Come esempio leggermente più complesso, un sistema di IA potrebbe prima generare diverse possibili soluzioni a un problema matematico, poi usare un'altra istanza per controllare ogni soluzione, e infine usare una terza per sintetizzare i risultati in una spiegazione chiara. Questo permette una risoluzione dei problemi più accurata e affidabile rispetto a un singolo passaggio.

[^21]: Vedere ad esempio dettagli su ["Operator" di OpenAI](https://openai.com/index/introducing-operator/), [le capacità di strumenti di Claude](https://docs.anthropic.com/en/docs/build-with-claude/computer-use), e [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). [Deep Research](https://openai.com/index/introducing-deep-research/) di OpenAI probabilmente ha un'architettura piuttosto sofisticata ma i dettagli non sono disponibili.

[^22]: Deepseek R1 si basa sull'addestrare e fare prompting del modello iterativamente in modo che il modello finale addestrato crei un ragionamento estensivo a catena di ragionamento. I dettagli architetturali non sono disponibili per o1 o o3, tuttavia Deepseek ha rivelato che non è richiesta alcuna particolare "salsa segreta" per sbloccare la scalabilità delle capacità con l'inferenza. Ma nonostante abbia ricevuto molta attenzione dalla stampa come qualcosa che sconvolge lo "status quo" nell'IA, non impatta le affermazioni centrali di questo saggio.

[^23]: Questi modelli superano significativamente i modelli standard sui benchmark di ragionamento. Per esempio, nel GPQA Diamond Benchmark—un test rigoroso di domande scientifiche di livello PhD—GPT-4o [ha ottenuto](https://openai.com/index/learning-to-reason-with-llms/) il 56%, mentre o1 e o3 hanno raggiunto il 78% e l'88%, rispettivamente, superando di gran lunga il punteggio medio del 70% degli esperti umani.

[^24]: L'O3 di OpenAI probabilmente ha speso ∼10<sup>21</sup>-10<sup>22</sup> FLOP [per completare ognuna delle domande della sfida ARC-AGI](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai), che gli esseri umani competenti possono fare in (diciamo) 10-100 secondi, dando una cifra più simile a ∼10<sup>20</sup> FLOP/s.

[^25]: Mentre il calcolo è una misura chiave della capacità del sistema di IA, interagisce sia con la qualità dei dati che con i miglioramenti algoritmici. Dati o algoritmi migliori possono ridurre i requisiti computazionali, mentre più calcolo può talvolta compensare dati o algoritmi più deboli.

## Capitolo 4 - Cosa sono la IAG e la superintelligenza?

Cosa stanno esattamente cercando di costruire le più grandi aziende tecnologiche del mondo a porte chiuse?

Il termine "intelligenza artificiale generale" esiste da tempo per indicare l'IA generale "a livello umano". Non è mai stato un termine particolarmente ben definito, ma negli ultimi anni è paradossalmente diventato ancora meno definito eppure ancora più importante, con esperti che dibattono simultaneamente se la IAG sia ancora lontana decenni o già raggiunta, e aziende da trilioni di dollari che corrono "verso la IAG". (L'ambiguità di "IAG" è stata evidenziata di recente quando [documenti trapelati hanno apparentemente rivelato](https://gizmodo.com/leaked-documents-show-openai-has-a-very-clear-definition-of-agi-2000543339) che nel contratto di OpenAI con Microsoft, la IAG era definita come IA che genera 100 miliardi di dollari di fatturato per OpenAI – una definizione piuttosto più mercenaria che intellettuale.)

Ci sono due problemi fondamentali con l'idea di IA con "intelligenza a livello umano". Primo, gli esseri umani sono molto, molto diversi nella loro capacità di svolgere qualsiasi tipo specifico di lavoro cognitivo, quindi non esiste un "livello umano". Secondo, l'intelligenza è molto multidimensionale; anche se possono esserci correlazioni, queste sono imperfette e potrebbero essere molto diverse nell'IA. Quindi, anche se potessimo definire il "livello umano" per molte capacità, l'IA lo supererebbe sicuramente in alcune mentre resterebbe molto al di sotto in altre.[^26]

È, tuttavia, assolutamente cruciale riuscire a discutere tipi, livelli e soglie delle capacità dell'IA. L'approccio adottato qui è di sottolineare che l'IA generale è già qui, e che arriva – e arriverà – a vari livelli di capacità ai quali è conveniente associare termini anche se sono riduttivi, perché corrispondono a soglie cruciali in termini degli effetti dell'IA sulla società e sull'umanità.

Definiremo la IAG "completa" come sinonimo di "IA generale sovrumana", intendendo un sistema di IA capace di eseguire essenzialmente tutti i compiti cognitivi umani a livello pari o superiore a quello dei migliori esperti umani, oltre ad acquisire nuove competenze e trasferire capacità a nuovi domini. Questo è in linea con come la "IAG" viene spesso definita nella letteratura moderna. È importante notare che questa è una soglia *molto* alta. Nessun essere umano ha questo tipo di intelligenza; piuttosto è il tipo di intelligenza che avrebbero grandi gruppi di esperti umani di alto livello se combinati insieme. Possiamo chiamare "superintelligenza" una capacità che va oltre questo, e definire livelli più limitati di capacità con IA generale "competitiva a livello umano" e "competitiva a livello di esperto", che eseguono un'ampia gamma di compiti a livello professionale tipico, o a livello di esperto umano.[^27]

Questi termini e alcuni altri sono raccolti nella [tabella](https://keepthefuturehuman.ai/essay/docs/#tab:terms) sottostante. Per avere un'idea più concreta di cosa possono fare i vari gradi di sistema, è utile prendere sul serio le definizioni e considerare cosa significano.

| Tipo di IA                              | Termini correlati                               | Definizione                                                                                                                                                                                                                                  | Esempi                                                                                                                                                |
| ---------------------------------------- | ----------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
| IA ristretta                             | IA debole                                       | IA addestrata per un compito specifico o famiglia di compiti. Eccelle nel suo dominio ma manca di intelligenza generale o capacità di apprendimento trasferibile.                                                                          | Software di riconoscimento immagini; Assistenti vocali (es. Siri, Alexa); Programmi per giocare a scacchi; AlphaFold di DeepMind                  |
| IA Strumentale                           | Intelligenza Aumentata, Assistente IA          | (Discussa più avanti nel saggio.) Sistema di IA che potenzia le capacità umane. Combina IA generale competitiva a livello umano, IA ristretta e controllo garantito, dando priorità a sicurezza e collaborazione. Supporta le decisioni umane. | Assistenti di programmazione avanzati; Strumenti di ricerca potenziati da IA; Piattaforme sofisticate di analisi dati. Agenti competenti ma ristretti e controllabili |
| IA generale (IAG)                        |                                                 | Sistema di IA adattabile a vari compiti, inclusi quelli per cui non è stato specificamente addestrato.                                                                                                                                       | Modelli linguistici (es. GPT-4, Claude); Modelli di IA multimodali; MuZero di DeepMind                                                             |
| IAG competitiva a livello umano          | IAG \[debole\]                                  | IA generale che esegue compiti a livello umano medio, a volte superandolo.                                                                                                                                                                   | Modelli linguistici avanzati (es. O1, Claude 3.5); Alcuni sistemi di IA multimodali                                                               |
| IAG competitiva a livello di esperto     | IAG \[parziale\]                                | IA generale che esegue la maggior parte dei compiti a livello di esperto umano, con autonomia significativa ma limitata                                                                                                                     | Possibilmente un O3 dotato di strumenti e architettura di supporto, almeno per matematica, programmazione e alcune scienze esatte                 |
| IAG \[completa\]                         | IAG sovrumana                                   | Sistema di IA capace di eseguire autonomamente praticamente tutti i compiti intellettuali umani a livello pari o superiore a quello degli esperti, con apprendimento efficiente e trasferimento di conoscenza.                          | \[Nessun esempio attuale – teorico\]                                                                                                                 |
| Superintelligenza                        | IAG altamente sovrumana                         | Sistema di IA che supera di gran lunga le capacità umane in tutti i domini, superando l'expertise collettiva umana. Questa superiorità potrebbe essere in generalità, qualità, velocità e/o altre misure.                               | \[Nessun esempio attuale – teorico\]                                                                                                                 |

Stiamo già sperimentando come sia avere IA generali fino al livello competitivo umano. Questo si è integrato relativamente senza problemi, poiché la maggior parte degli utenti lo vive come avere un lavoratore temporaneo intelligente ma limitato che li rende più produttivi con un impatto misto sulla qualità del loro lavoro.[^28]

Ciò che sarebbe diverso riguardo alla IA generale competitiva a livello di esperto è che non avrebbe le limitazioni fondamentali dell'IA odierna, e farebbe le cose che fanno gli esperti: lavoro indipendente economicamente prezioso, vera creazione di conoscenza, lavoro tecnico su cui si può contare, commettendo raramente (anche se ancora occasionalmente) errori stupidi.

L'idea della IAG completa è che *faccia davvero* tutte le cose cognitive che fanno anche gli esseri umani più capaci ed efficaci, autonomamente e senza bisogno di aiuto o supervisione. Questo include pianificazione sofisticata, apprendimento di nuove competenze, gestione di progetti complessi, ecc. Potrebbe fare ricerca originale all'avanguardia. Potrebbe gestire un'azienda. Qualunque sia il tuo lavoro, se viene svolto prevalentemente al computer o al telefono, *potrebbe svolgerlo almeno altrettanto bene di te.* E probabilmente molto più velocemente e a minor costo. Discuteremo alcune delle ramificazioni più avanti, ma per ora la sfida per te è prenderlo davvero sul serio. Immagina le dieci persone più esperte e competenti che conosci o di cui hai sentito parlare – inclusi CEO, scienziati, professori, ingegneri di punta, psicologi, leader politici e scrittori. Avvolgili tutti in uno, che parla anche 100 lingue, ha una memoria prodigiosa, opera rapidamente, è instancabile e sempre motivato, e lavora sotto il salario minimo.[^29] Questa è un'idea di cosa sarebbe la IAG.

Per la superintelligenza immaginare è più difficile, perché l'idea è che potrebbe compiere imprese intellettuali che nessun essere umano o anche gruppo di esseri umani può – è per definizione imprevedibile per noi. Ma possiamo farci un'idea. Come punto di partenza minimo, considera molte IAG, ciascuna molto più capace anche del miglior esperto umano, che funzionano a 100 volte la velocità umana, con memoria enorme e straordinaria capacità di coordinamento.[^30] E da lì sale ancora. Avere a che fare con la superintelligenza sarebbe meno come conversare con una mente diversa, più come negoziare con una civiltà diversa (e più avanzata).

Quindi quanto siamo *vicini* alla IAG e alla superintelligenza?

[^26]: Ad esempio, i sistemi di IA attuali superano di gran lunga la capacità umana nei calcoli aritmetici rapidi o nei compiti di memoria, mentre restano indietro nel ragionamento astratto e nella risoluzione creativa di problemi.

[^27]: Molto importante, come concorrente tale IA avrebbe diversi vantaggi strutturali principali tra cui: non si stancherebbe né avrebbe altri bisogni individuali come gli umani; può essere eseguita a velocità maggiori semplicemente aumentando la potenza computazionale; può essere copiata insieme a qualsiasi competenza o conoscenza acquisisca – e la conoscenza acquisita dalle reti neurali può anche essere "fusa" per trasferire intere competenze tra loro; potrebbe comunicare alla velocità delle macchine; e potrebbe auto-modificarsi o auto-migliorarsi in modi più significativi e a velocità maggiore di qualsiasi essere umano.

[^28]: Se non hai trascorso tempo usando gli attuali sistemi di IA di ultima generazione, lo raccomando: sono genuinamente utili e capaci, ed è anche importante per calibrare l'effetto che l'IA avrà man mano che diventerà più potente.

[^29]: Considera un importante ospedale di ricerca: la IAG completamente realizzata potrebbe simultaneamente analizzare tutti i dati dei pazienti in arrivo, stare al passo con ogni nuovo articolo medico, suggerire diagnosi, progettare piani di trattamento, gestire sperimentazioni cliniche e coordinare la pianificazione del personale – tutto operando a un livello pari o superiore ai migliori specialisti dell'ospedale in ogni area. E potrebbe farlo per più ospedali simultaneamente, a una frazione del costo attuale. Sfortunatamente, devi anche considerare un'organizzazione criminale: la IAG completamente realizzata potrebbe simultaneamente hackerare, impersonare, spiare e ricattare migliaia di vittime, stare al passo con le forze dell'ordine (che si automatizzano molto più lentamente), progettare nuovi schemi per fare soldi e coordinare la pianificazione del personale – se c'è personale.

[^30]: Nel suo [saggio](https://darioamodei.com/machines-of-loving-grace), Dario Amodei, CEO di Anthropic, ha evocato un "Paese di \[un milione di\] geni".

## Capitolo 5 - Alla soglia

Il percorso dai sistemi di IA odierni a una IAG completamente sviluppata sembra sorprendentemente breve e prevedibile.

Gli ultimi dieci anni hanno visto progressi spettacolari nell'IA grazie a enormi risorse [computazionali](https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year), umane e [finanziarie](https://arxiv.org/abs/2405.21015). Molte applicazioni di IA specializzata superano gli esseri umani nei compiti loro assegnati, e sono sicuramente molto più veloci ed economiche.[^31] Inoltre, esistono agenti specializzati sovrumani che possono battere tutte le persone in giochi di dominio ristretto come [Go](https://www.nature.com/articles/nature16961), [Scacchi](https://arxiv.org/abs/1712.01815) e [Poker](https://www.deepstack.ai/), così come [agenti più generali](https://deepmind.google/discover/blog/a-generalist-agent/) che possono pianificare ed eseguire azioni in ambienti simulati semplificati con la stessa efficacia degli esseri umani.

Più significativamente, i sistemi di IA generale attualmente disponibili da OpenAI/Microsoft, Google/Deepmind, Anthropic/Amazon, Facebook/Meta, X.ai/Tesla e altri [^32] sono emersi dall'inizio del 2023 e hanno costantemente (sebbene in modo irregolare) aumentato le loro capacità da allora. Tutti questi sono stati creati tramite predizione di token su enormi dataset testuali e multimediali, combinata con un estensivo feedback di rinforzo da parte di esseri umani e altri sistemi di IA. Alcuni includono anche ampi sistemi di strumenti e architetture di supporto.

### Punti di forza e debolezze dei sistemi generali attuali

Questi sistemi si comportano bene in una gamma sempre più ampia di test progettati per misurare intelligenza ed expertise, con progressi che hanno sorpreso persino gli esperti del settore:

- Al momento del rilascio, GPT-4 [ha eguagliato o superato le prestazioni umane tipiche](https://arxiv.org/abs/2303.08774) in test accademici standard inclusi SAT, GRE, esami di ammissione ed esami di abilitazione forense. I modelli più recenti probabilmente si comportano significativamente meglio, sebbene i risultati non siano pubblicamente disponibili.
- Il test di Turing – a lungo considerato un benchmark chiave per l'IA "vera" – viene ora superato routinariamente in alcune forme dai modelli linguistici moderni, sia informalmente che in [studi formali](https://arxiv.org/abs/2405.08007).[^33]
- Nel benchmark comprensivo MMLU che copre 57 materie accademiche, [i modelli recenti raggiungono punteggi a livello di esperti del dominio](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu) (∼90%) [^34]
- L'expertise tecnica è avanzata drasticamente: il benchmark GPQA di fisica a livello universitario ha visto [le prestazioni saltare](https://epoch.ai/data/ai-benchmarking-dashboard) da ipotesi quasi casuali (GPT-4, 2022) a livello esperto (o1-preview, 2024).
- Persino i test specificamente progettati per essere resistenti all'IA stanno cedendo: l'O3 di OpenAI [secondo quanto riportato](https://www.nextbigfuture.com/2024/12/openai-releases-o3-model-with-high-performance-and-high-cost.html) risolve il benchmark di risoluzione di problemi astratti ARC-AGI a livello umano, raggiunge prestazioni di programmazione da esperto di punta e ottiene il 25% sui problemi di "matematica di frontiera" di Epoch AI progettati per sfidare matematici d'élite.[^35]
- La tendenza è così chiara che lo sviluppatore di MMLU ha ora creato ["L'Ultimo Esame dell'Umanità"](https://agi.safe.ai/) – un nome minaccioso che riflette la possibilità che l'IA superi presto le prestazioni umane in qualsiasi test significativo. Al momento della stesura, ci sono affermazioni di sistemi di IA che raggiungono il 27% (secondo [Sam Altman](https://x.com/sama/status/1886220281565381078)) e il 35% (secondo [questo paper](https://arxiv.org/abs/2502.09955)) in questo esame estremamente difficile. È molto improbabile che qualsiasi essere umano individuale possa farlo.

Nonostante questi numeri impressionanti (e la loro ovvia intelligenza quando si interagisce con loro) [^36] ci sono molte cose che (almeno le versioni rilasciate di) queste reti neurali *non possono* fare. Attualmente la maggior parte sono disincarnate – esistono solo sui server – e processano al massimo testo, suono e immagini statiche (ma non video). Crucialmente, la maggior parte non può svolgere attività pianificate complesse che richiedono alta precisione.[^37] E ci sono diverse altre qualità forti nella cognizione umana di alto livello attualmente deboli nei sistemi di IA rilasciati.

La seguente tabella elenca alcune di queste, basandosi sui sistemi di IA di metà 2024 come GPT-4o, Claude 3.5 Sonnet e Google Gemini 1.5.[^38] La domanda chiave su quanto rapidamente l'IA generale diventerà più potente è: fino a che punto *fare più della stessa cosa* produrrà risultati, rispetto ad aggiungere tecniche aggiuntive ma *conosciute*, rispetto a sviluppare o implementare direzioni di ricerca IA *realmente nuove*. Le mie previsioni per questo sono date nella tabella, in termini di probabilità che ciascuno di questi scenari porti quella capacità al livello umano e oltre.

<table><tbody><tr><th>Capacità</th><th>Descrizione della capacità</th><th>Stato/prognosi</th><th>Scala/conosciuto/nuovo</th></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td colspan="4"><em>Capacità Cognitive Fondamentali</em></td></tr><tr><td>Ragionamento</td><td>Le persone possono fare ragionamenti accurati e multi-passo, seguendo regole e verificando la precisione.</td><td>Progresso drammatico recente usando catene di ragionamento estese e riaddestramento</td><td>95/5/5</td></tr><tr><td>Pianificazione</td><td>Le persone mostrano pianificazione a lungo termine e gerarchica.</td><td>Migliora con la scala; può essere fortemente aiutata usando architetture di supporto e migliori tecniche di addestramento.</td><td>10/85/5</td></tr><tr><td>Ancoraggio alla verità</td><td>Le IAIG confabulano informazioni non fondate per soddisfare le richieste.</td><td>Migliora con la scala; dati di calibrazione disponibili nel modello; può essere verificato/migliorato via architetture di supporto.</td><td>30/65/5</td></tr><tr><td>Risoluzione flessibile dei problemi</td><td>Gli esseri umani possono riconoscere nuovi schemi e inventare nuove soluzioni a problemi complessi; i modelli ML attuali faticano.</td><td>Migliora con la scala ma debolmente; può essere risolvibile con tecniche neurosimboliche o di "ricerca" generalizzata.</td><td>15/75/10</td></tr><tr><td colspan="4"><em>Apprendimento e Conoscenza</em></td></tr><tr><td>Apprendimento e memoria</td><td>Le persone hanno memoria di lavoro, a breve termine e a lungo termine, tutte dinamiche e inter-correlate.</td><td>Tutti i modelli imparano durante l'addestramento; le IAIG imparano nella finestra di contesto e durante il fine-tuning; esistono tecniche di "apprendimento continuo" e altre ma non ancora integrate nelle IAIG grandi.</td><td>5/80/15</td></tr><tr><td>Astrazione e ricorsione</td><td>Le persone possono mappare e trasferire insiemi di relazioni in altri più astratti per ragionamento e manipolazione, incluso ragionamento "meta" ricorsivo.</td><td>Migliora debolmente con la scala; potrebbe emergere in sistemi neurosimbolici.</td><td>30/50/20</td></tr><tr><td>Modello(i) del mondo</td><td>Le persone hanno e aggiornano continuamente un modello predittivo del mondo entro cui possono risolvere problemi e fare ragionamento fisico</td><td>Migliora con la scala; aggiornamento legato all'apprendimento; le IAIG sono deboli nella predizione del mondo reale.</td><td>20/50/30</td></tr><tr><td colspan="4"><em>Sé e Agenzia</em></td></tr><tr><td>Agenzia</td><td>Le persone possono intraprendere azioni per perseguire obiettivi, basandosi su pianificazione/predizione.</td><td>Molti sistemi ML sono agentici; gli LLM possono essere resi agenti tramite wrapper.</td><td>5/90/5</td></tr><tr><td>Auto-direzione</td><td>Le persone sviluppano e perseguono i propri obiettivi, con motivazione e spinta generate internamente.</td><td>Largamente composta da agenzia più originalità; probabile che emerga in sistemi agentici complessi con obiettivi astratti.</td><td>40/45/15</td></tr><tr><td>Auto-riferimento</td><td>Le persone comprendono e ragionano su se stesse come situate entro un ambiente/contesto.</td><td>Migliora con la scala e potrebbe essere aumentata con ricompensa di addestramento.</td><td>70/15/15</td></tr><tr><td>Auto-consapevolezza</td><td>Le persone hanno conoscenza e possono ragionare sui propri pensieri e stati mentali.</td><td>Esiste in un certo senso nelle IAIG, che possono probabilmente superare il classico "test dello specchio" per l'auto-consapevolezza. Può essere migliorata con architetture di supporto; ma non è chiaro se questo sia sufficiente.</td><td>20/55/25</td></tr><tr><td colspan="4"><em>Interfaccia e Ambiente</em></td></tr><tr><td>Intelligenza incarnata</td><td>Le persone comprendono e interagiscono attivamente con il loro ambiente del mondo reale.</td><td>L'apprendimento per rinforzo funziona bene in ambienti simulati e del mondo reale (robotici) e può essere integrato in transformer multimodali.</td><td>5/85/10</td></tr><tr><td>Elaborazione multi-sensoriale</td><td>Le persone integrano ed elaborano in tempo reale flussi visivi, audio e di altri sensi.</td><td>L'addestramento in modalità multiple sembra "funzionare e basta", e migliora con la scala. L'elaborazione video in tempo reale è difficile ma ad es. i sistemi di guida autonoma stanno migliorando rapidamente.</td><td>30/60/10</td></tr><tr><td colspan="4"><em>Capacità di Ordine Superiore</em></td></tr><tr><td>Originalità</td><td>I modelli ML attuali sono creativi nel trasformare e combinare idee/opere esistenti, ma le persone possono costruire nuovi framework e strutture, a volte legati alla loro identità.</td><td>Può essere difficile da discernere dalla "creatività", che potrebbe scalare verso di essa; può emergere da creatività più auto-consapevolezza.</td><td>50/40/10</td></tr><tr><td>Senzienza</td><td>Le persone sperimentano qualia; questi possono essere di valenza positiva, negativa o neutrale; "è come qualcosa" essere una persona.</td><td>Molto difficile e filosoficamente controverso determinare se un dato sistema abbia questo.</td><td>5/10/85</td></tr></tbody></table>

Capacità chiave attualmente sotto il livello di esperto umano nei sistemi IAIG moderni, raggruppate per tipo. La terza colonna riassume lo stato attuale. La colonna finale mostra la probabilità prevista (%) che le prestazioni a livello umano saranno raggiunte attraverso: scalare le tecniche attuali / combinare con tecniche conosciute / sviluppare nuove tecniche. Queste capacità non sono indipendenti, e l'aumento in una tipicamente va di pari passo con aumenti nelle altre. Si noti che non tutte (particolarmente la senzienza) sono necessarie per sistemi di IA capaci di far avanzare lo sviluppo dell'IA, evidenziando la possibilità di IA potente ma non senziente.

Scomporre ciò che "manca" in questo modo rende abbastanza chiaro che siamo decisamente sulla strada per un'intelligenza ampiamente sopra-umana scalando tecniche esistenti o conosciute.[^39]

Potrebbero esserci ancora sorprese. Anche mettendo da parte la "senzienza", potrebbero esserci alcune delle capacità cognitive fondamentali elencate che realmente non possono essere fatte con le tecniche attuali e richiedono nuove. Ma considerate questo. L'attuale sforzo messo in campo da molte delle più grandi aziende del mondo ammonta a multiple volte la spesa del progetto Apollo e decine di volte quella del progetto Manhattan,[^40] e sta impiegando migliaia delle persone tecniche più brillanti a stipendi mai sentiti. Le dinamiche degli ultimi anni hanno ora portato a questo più potenza intellettuale umana (con l'IA ora aggiunta) di qualsiasi impresa nella storia. Non dovremmo scommettere sul fallimento.

### Il grande obiettivo: agenti autonomi generalisti

Lo sviluppo dell'IA generale negli ultimi anni si è concentrato sulla creazione di IA generale e potente ma simile a strumento: funziona principalmente come assistente (abbastanza) leale, e generalmente non intraprende azioni da sola. Questo è in parte per progetto, ma largamente perché questi sistemi semplicemente non sono stati abbastanza competenti nelle abilità rilevanti per essere affidati con azioni complesse.[^41]

Le aziende e i ricercatori di IA stanno, tuttavia, sempre più [spostando il focus](https://www.axios.com/2025/01/23/davos-2025-ai-agents) verso agenti autonomi *autonomi* a livello esperto e per scopi generali.[^42] Questo permetterebbe ai sistemi di agire più come un assistente umano a cui l'utente può delegare azioni reali.[^43] Cosa richiederà? Diverse delle capacità nella tabella "cosa manca" sono implicate, inclusi forte ancoraggio alla verità, apprendimento e memoria, astrazione e ricorsione, e modellazione del mondo (per l'intelligenza), pianificazione, agenzia, originalità, auto-direzione, auto-riferimento e auto-consapevolezza (per l'autonomia), e elaborazione multi-sensoriale, intelligenza incarnata e risoluzione flessibile dei problemi (per la generalità).[^44]

Questa triplice intersezione di alta autonomia (indipendenza d'azione), alta generalità (ampiezza di scopo e compiti) e alta intelligenza (competenza nei compiti cognitivi) è attualmente unica agli esseri umani. È implicitamente ciò che molti probabilmente hanno in mente quando pensano alla IAG – sia in termini del suo valore che dei suoi rischi.

Questo fornisce un altro modo di definire I-A-G come ***I***ntelligenza ***A***utonoma- ***G***enerale, e vedremo che questa tripla intersezione fornisce una lente molto preziosa per i sistemi ad alta capacità sia nel comprendere i loro rischi e benefici, sia nella governance dell'IA.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Simple-1024x1024.png&w=3840&q=75) La zona di potere e rischio trasformativo I-A-G emerge dall'intersezione di tre proprietà chiave: alta Autonomia, alta Intelligenza nei compiti e alta Generalità.

### Il ciclo di miglioramento dell'IA (auto-)

Un fattore cruciale finale nel comprendere il progresso dell'IA è il ciclo di feedback tecnologico unico dell'IA. Nello sviluppare l'IA, il successo – sia nei sistemi dimostrati che nei prodotti deployati – porta investimenti aggiuntivi, talento e competizione, e siamo attualmente nel mezzo di un enorme ciclo di feedback hype-più-realtà dell'IA che sta guidando centinaia di miliardi, o persino trilioni, di dollari di investimento.

Questo tipo di ciclo di feedback potrebbe accadere con qualsiasi tecnologia, e l'abbiamo visto in molte, dove il successo di mercato genera investimento, che genera miglioramento e migliore successo di mercato. Ma lo sviluppo dell'IA va oltre, in quanto ora i sistemi di IA stanno aiutando a sviluppare sistemi di IA nuovi e più potenti.[^45] Possiamo pensare a questo ciclo di feedback in cinque fasi, ognuna con una scala temporale più breve dell'ultima, come mostrato nella tabella.

*Il ciclo di miglioramento dell'IA opera attraverso multiple scale temporali, con ogni fase che potenzialmente accelera le fasi successive. Le fasi precedenti sono ben avviate, mentre quelle successive rimangono speculative ma potrebbero procedere molto rapidamente una volta sbloccate.*

Diverse di queste fasi sono già in corso, e un paio chiaramente stanno iniziando. L'ultima fase, in cui i sistemi di IA migliorano autonomamente se stessi, è stata un pilastro della letteratura sui rischi dei sistemi di IA molto potenti, e per una buona ragione.[^46] Ma è importante notare che è solo la forma più drastica di un ciclo di feedback che è già iniziato e potrebbe portare a più sorprese nel rapido avanzamento della tecnologia.


[^31]: Utilizzate molta più di questa IA di quanto probabilmente pensiate, guidando generazione e riconoscimento vocale, elaborazione delle immagini, algoritmi dei newsfeed, ecc.

[^32]: Mentre le relazioni tra queste coppie di aziende sono piuttosto complesse e sfumate, le ho esplicitamente elencate per indicare sia l'enorme capitalizzazione di mercato complessiva delle aziende ora impegnate nello sviluppo dell'IA, sia anche che dietro aziende persino "più piccole" come Anthropic ci sono tasche enormemente profonde tramite investimenti e accordi di partnership principali.

[^33]: È diventato di moda denigrare il test di Turing, ma è piuttosto potente e generale. Nelle versioni deboli indica se le persone tipiche che interagiscono con un'IA (che è addestrata ad agire come umana) in modi tipici per brevi periodi possono dire se è un'IA. Non possono. Secondo, un test di Turing altamente avversariale può sondare essenzialmente qualsiasi elemento della capacità e intelligenza umana – ad es. confrontando un sistema di IA con un esperto umano, valutato da altri esperti umani. C'è un senso in cui molta della valutazione dell'IA è una forma generalizzata di test di Turing.

[^34]: Questo è per dominio – nessun umano potrebbe plausibilmente raggiungere tali punteggi in tutte le materie simultaneamente.

[^35]: Questi sono problemi che richiederebbero anche a matematici eccellenti tempo sostanziale per risolverli, se potessero risolverli affatto.

[^36]: Se avete un atteggiamento scettico, mantenete il vostro scetticismo ma provate davvero i modelli più attuali, così come provate voi stessi alcune delle domande di test che possono superare. Come professore di fisica, predirrei con quasi certezza che, ad esempio, i modelli migliori supererebbero l'esame di qualificazione per laureati nel nostro dipartimento.

[^37]: Questa e altre debolezze come la confabulazione hanno rallentato l'adozione di mercato e portato a un divario tra capacità percepite e dichiarate (che deve anche essere visto attraverso la lente dell'intensa competizione di mercato e la necessità di attrarre investimenti). Questo ha confuso sia il pubblico che i policymaker sullo stato reale del progresso dell'IA. Pur forse non eguagliando l'hype, il progresso è molto reale.

[^38]: L'avanzamento principale da allora è stato lo sviluppo di sistemi addestrati per ragionamento di alta qualità, sfruttando più capacità computazionale durante l'inferenza e maggiore apprendimento per rinforzo. Poiché questi modelli sono nuovi e le loro capacità meno testate, non ho completamente rivisto questa tabella eccetto per "ragionamento", che considero essenzialmente risolto. Ma ho aggiornato le previsioni basandomi sulle capacità sperimentate e riportate di quei sistemi.

[^39]: Le precedenti ondate di ottimismo dell'IA negli anni '60 e '80 finirono in "inverni dell'IA" quando le capacità promesse non si materializzarono. Tuttavia, l'ondata attuale differisce fondamentalmente nell'aver raggiunto prestazioni sovrumane in molti domini, sostenuta da enormi risorse computazionali e successo commerciale.

[^40]: L'intero progetto Apollo [è costato circa 250 miliardi di dollari USD in dollari del 2020](https://www.planetary.org/space-policy/cost-of-apollo), e il progetto Manhattan [meno di un decimo di quello](https://www.brookings.edu/the-costs-of-the-manhattan-project/). Goldman Sachs [proietta un trilione di dollari di spesa solo sui data center IA](https://www.datacenterdynamics.com/en/news/goldman-sachs-1tn-to-be-spent-on-ai-data-centers-chips-and-utility-upgrades-with-little-to-show-for-it-so-far/) nei prossimi anni.

[^41]: Anche se gli esseri umani fanno molti errori, sottovalutiamo quanto possiamo essere affidabili! Poiché le probabilità si moltiplicano, un compito che richiede 20 passi per essere fatto correttamente richiede che ogni passo sia affidabile al 97% solo per farlo bene la metà delle volte. Facciamo tali compiti tutto il tempo.

[^42]: Una mossa forte in questa direzione è stata presa molto di recente con l'assistente ["Deep Research"](https://openai.com/index/introducing-deep-research/) di OpenAI che esegue autonomamente ricerche generali, descritto come "una nuova capacità agentica che conduce ricerche multi-passo su internet per compiti complessi."

[^43]: Cose come compilare quel fastidioso modulo PDF, prenotare voli, ecc. Ma con un PhD in 20 campi! Quindi anche: scrivere quella tesi per voi, negoziare quel contratto per voi, dimostrare quel teorema per voi, creare quella campagna pubblicitaria per voi, ecc. Cosa fate *voi*? Gli dite cosa fare, naturalmente.

[^44]: Si noti che la senzienza *non* è chiaramente richiesta, né l'IA in questa tripla intersezione implica necessariamente essa.

[^45]: L'analogia più vicina qui è forse la tecnologia dei chip, dove lo sviluppo ha mantenuto la legge di Moore per decenni, mentre le tecnologie informatiche aiutano le persone a progettare la prossima generazione di tecnologia dei chip. Ma l'IA sarà molto più diretta.

[^46]: È importante lasciare che affondi per un momento che l'IA potrebbe – presto – migliorare se stessa in una scala temporale di giorni o settimane. O meno. Tenete questo a mente quando qualcuno vi dice che una capacità dell'IA è definitivamente lontana.

## Capitolo 6 - La corsa verso la IAG

Quali sono le forze trainanti dietro la corsa alla costruzione della IAG, sia per le aziende che per i paesi?

I rapidi progressi recenti nell'IA hanno generato e sono stati alimentati da un livello straordinario di attenzione e investimenti. Questo è dovuto in parte ai successi nello sviluppo dell'IA, ma c'è dell'altro. Perché alcune delle più grandi aziende del mondo, e persino interi paesi, stanno correndo per costruire non solo l'IA, ma la IAG e la superintelligenza?

### Cosa ha spinto la ricerca sull'IA verso un'intelligenza di livello umano

Fino a circa cinque anni fa, l'IA è stata principalmente un problema di ricerca accademica e scientifica, quindi largamente guidata dalla curiosità e dal desiderio di comprendere l'intelligenza e come crearla in un nuovo substrato.

In questa fase, c'era relativamente poca attenzione ai benefici o ai pericoli dell'IA tra la maggior parte dei ricercatori. Quando veniva chiesto perché l'IA dovesse essere sviluppata, una risposta comune poteva essere elencare, in modo piuttosto vago, i problemi che l'IA avrebbe potuto aiutare a risolvere: nuove medicine, nuovi materiali, nuova scienza, processi più intelligenti, e in generale il miglioramento delle condizioni di vita delle persone.[^47]

Questi sono obiettivi ammirevoli![^48] Anche se possiamo e dovremo chiederci se la IAG – piuttosto che l'IA in generale – sia necessaria per questi obiettivi, essi mostrano l'idealismo con cui molti ricercatori di IA hanno iniziato.

Nel corso dell'ultimo quinquennio, tuttavia, l'IA si è trasformata da un campo di ricerca relativamente puro in un settore molto più orientato all'ingegneria e ai prodotti, guidato principalmente da alcune delle più grandi aziende del mondo.[^49] I ricercatori, pur rimanendo rilevanti, non controllano più il processo.

### Perché le aziende stanno cercando di costruire la IAG?

Quindi, perché le corporation giganti (e ancora di più gli investitori) stanno versando enormi risorse nella costruzione della IAG? Ci sono due fattori trainanti di cui la maggior parte delle aziende è abbastanza sincera: vedono l'IA come motore di produttività per la società e di profitti per loro. Poiché l'IA generale è per natura multi-purpose, c'è un premio enorme: invece di scegliere un settore in cui creare prodotti e servizi, si può provare a farli *tutti contemporaneamente.* Le grandi aziende tecnologiche sono cresciute enormemente producendo beni e servizi digitali, e almeno alcuni dirigenti vedono sicuramente l'IA semplicemente come il passo successivo nel fornirli bene, con rischi e benefici che ampliano ma echeggiano quelli forniti da motori di ricerca, social media, laptop, telefoni, ecc.

Ma perché la IAG? C'è una risposta molto semplice a questo, che la maggior parte delle aziende e degli investitori evita di discutere pubblicamente.[^50]

È che la IAG può direttamente, uno per uno, *sostituire i lavoratori.*

Non potenziare, non responsabilizzare, non rendere più produttivi. Nemmeno *spostare.* Tutto questo può e sarà fatto da IA non-generale. La IAG è specificamente ciò che può completamente *sostituire* i lavoratori intellettuali (e con la robotica, anche molti di quelli fisici). Come supporto a questa visione basta non guardare oltre la [definizione (dichiarata pubblicamente)](https://openai.com/our-structure/) di IAG di OpenAI, che è "un sistema altamente autonomo che supera gli esseri umani nella maggior parte del lavoro economicamente prezioso."

Il premio qui (per le aziende!) è enorme. I costi del lavoro sono una percentuale sostanziale dei circa $100 trilioni dell'economia globale mondiale. Anche se solo una frazione di questo viene catturata dalla sostituzione del lavoro umano con il lavoro dell'IA, si tratta di trilioni di dollari di ricavi annuali. Le aziende di IA sono anche consapevoli di chi è disposto a pagare. Come la vedono loro, tu non pagherai migliaia di dollari all'anno per strumenti di produttività. Ma un'azienda *pagherà* migliaia di dollari all'anno per sostituire il tuo lavoro, se può.

### Perché i paesi sentono di dover correre verso la IAG

Le motivazioni dichiarate dei paesi per perseguire la IAG si concentrano sulla leadership economica e scientifica. L'argomento è convincente: la IAG potrebbe accelerare drammaticamente la ricerca scientifica, lo sviluppo tecnologico e la crescita economica. Date le poste in gioco, sostengono, nessuna grande potenza può permettersi di rimanere indietro.[^51]

Ma ci sono anche fattori trainanti aggiuntivi e largamente non dichiarati. Non c'è dubbio che quando certi leader militari e della sicurezza nazionale si incontrano a porte chiuse per discutere di una tecnologia straordinariamente potente e catastroficamente rischiosa, il loro focus non è su "come evitiamo questi rischi" ma piuttosto "come otteniamo questo per primi?" I leader militari e dell'intelligence vedono la IAG come una potenziale rivoluzione negli affari militari, forse la più significativa dalle armi nucleari. Il timore è che il primo paese a sviluppare la IAG potrebbe ottenere un vantaggio strategico insormontabile. Questo crea una classica dinamica di corsa agli armamenti.

Vedremo che questo pensiero di "corsa verso la IAG",[^52] pur essendo convincente, è profondamente viziato. Questo non è perché correre è pericoloso e rischioso – anche se lo è – ma a causa della natura della tecnologia. L'assunto non dichiarato è che la IAG, come altre tecnologie, sia controllabile dallo stato che la sviluppa, e sia un beneficio che conferisce potere alla società che ne ha di più. Come vedremo, probabilmente non sarà né l'una né l'altra cosa.

### Perché la superintelligenza?

Mentre le aziende si concentrano pubblicamente sulla produttività, e i paesi sulla crescita economica e tecnologica, per coloro che perseguono deliberatamente la IAG completa e la superintelligenza questi sono solo l'inizio. Cosa hanno veramente in mente? Anche se raramente detto ad alta voce, includono:

1. Cure per molte o tutte le malattie;
2. Arresto e inversione dell'invecchiamento;
3. Nuove fonti di energia sostenibile come la fusione;
4. Potenziamenti umani, o organismi progettati tramite ingegneria genetica;
5. Nanotecnologie e produzione molecolare;
6. Upload della mente;
7. Fisica esotica o tecnologie spaziali;
8. Consigli e supporto decisionale super-umani;
9. Pianificazione e coordinamento super-umani.

I primi tre sono largamente tecnologie "a vantaggio unico" – cioè probabilmente molto fortemente positive nel complesso. È difficile argomentare contro il curare malattie o essere in grado di vivere più a lungo se si sceglie. E abbiamo già raccolto il lato negativo della fusione (sotto forma di armi nucleari); ora sarebbe bello ottenere il lato positivo. La questione con questa prima categoria è se ottenere queste tecnologie prima compensi il rischio.

Le successive quattro sono chiaramente a doppio taglio: tecnologie trasformative con sia potenziali enormi vantaggi che immensi rischi, molto simili all'IA. Tutte queste, se saltassero fuori da una scatola nera domani e fossero implementate, sarebbero incredibilmente difficili da gestire.[^53]

Le ultime due riguardano l'IA super-umana che fa cose essa stessa piuttosto che inventare semplicemente tecnologia. Più precisamente, mettendo da parte gli eufemismi, queste coinvolgono sistemi di IA potenti che dicono alle persone cosa fare. Chiamare questo "consiglio" è disonesto se il sistema che consiglia è molto più potente del consigliato, che non può comprendere significativamente la base della decisione (o anche se questa viene fornita, fidarsi che il consigliere non fornirebbe una giustificazione altrettanto convincente per una decisione diversa).

Questo indica un elemento chiave mancante dalla lista sopra:

10. Potere.

È abbondantemente chiaro che molto di ciò che sta alla base dell'attuale corsa verso l'IA super-umana è l'idea che *intelligenza = potere*. Ogni corridore sta scommettendo di essere il miglior detentore di quel potere, e che sarà in grado di esercitarlo per ragioni apparentemente benevole senza che gli sfugga o venga sottratto dal loro controllo.

Cioè, quello che aziende e nazioni stanno realmente inseguendo non sono solo i frutti della IAG e della superintelligenza, ma il potere di controllare chi ha accesso ad essi e come vengono usati. Le aziende si vedono come amministratori responsabili di questo potere al servizio degli azionisti e dell'umanità; le nazioni si vedono come guardiani necessari per impedire a potenze ostili di ottenere un vantaggio decisivo. Entrambi sbagliano pericolosamente, non riuscendo a riconoscere che la superintelligenza, per sua natura, non può essere controllata in modo affidabile da alcuna istituzione umana. Vedremo che la natura e le dinamiche dei sistemi superintelligenti rendono il controllo umano estremamente difficile, se non impossibile.

Queste dinamiche di corsa – sia corporative che geopolitiche – rendono certi rischi quasi inevitabili a meno che non vengano decisamente interrotte. Ora passiamo a esaminare questi rischi e perché non possono essere adeguatamente mitigati all'interno di un paradigma di sviluppo competitivo.[^54]


[^47]: Una lista più precisa di obiettivi degni sono gli [Obiettivi di Sviluppo Sostenibile](https://sdgs.un.org/goals) delle Nazioni Unite. Questi sono, in un certo senso, il più vicino che abbiamo a un insieme di obiettivi di consenso globale per quello che vorremmo vedere migliorato nel mondo. L'IA potrebbe aiutare.

[^48]: La tecnologia in generale ha un potere trasformativo economico e sociale per il miglioramento umano, come attestano migliaia di anni. In questo senso, una lunga e convincente esplicazione di una visione positiva della IAG può essere trovata in [questo saggio](https://darioamodei.com/machines-of-loving-grace) del fondatore di Anthropic Dario Amodei.

[^49]: Gli investimenti privati nell'IA [hanno iniziato a esplodere nel 2018-19, superando gli investimenti pubblici intorno ad allora,](https://cset.georgetown.edu/publication/tracking-ai-investment/) e da allora li hanno enormemente superati.

[^50]: Posso attestare che dietro porte più chiuse, non hanno tali remore. E sta diventando più pubblico; vedi per esempio la nuova ["richiesta per startup"](https://www.ycombinator.com/rfs) di Y-combinator, molte parti della quale chiamano esplicitamente per la sostituzione all'ingrosso dei lavoratori umani. Per citarli, "La proposta di valore del B2B SaaS era rendere i lavoratori umani incrementalmente più efficienti. La proposta di valore degli agenti IA verticali è automatizzare completamente il lavoro... È del tutto possibile che questa opportunità sia abbastanza grande da creare altri 100 unicorni." (Per coloro che non conoscono il gergo della Silicon Valley, "B2B" è business-to-business e un unicorno è un'azienda da $1 miliardo. Cioè stanno parlando di più di cento aziende da miliardi-plus-dollari che sostituiscono lavoratori per altre aziende.)

[^51]: Vedi per esempio un recente [rapporto della Commissione di Revisione Economica e della Sicurezza USA-Cina](https://www.uscc.gov/sites/default/files/2024-11/2024_Executive_Summary.pdf). Anche se c'era sorprendentemente poca giustificazione all'interno del rapporto stesso, la raccomandazione principale era che gli Stati Uniti "Il Congresso stabilisca e finanzi un programma simile al Progetto Manhattan dedicato a correre verso e acquisire una capacità di Intelligenza Artificiale Generale (IAG)."

[^52]: Le aziende stanno ora adottando questa cornice geopolitica come scudo contro qualsiasi vincolo sul loro sviluppo di IA, generalmente in modi che sono palesemente auto-serventi, e a volte in modi che non hanno nemmeno senso di base. Considera l'[Approccio all'IA di Frontiera](https://about.fb.com/news/2025/02/meta-approach-frontier-ai/) di Meta, che contemporaneamente argomenta che l'America deve "[Consolidare la sua] posizione come leader nell'innovazione tecnologica, crescita economica e sicurezza nazionale" e anche che deve farlo rilasciando apertamente i suoi sistemi di IA più potenti – il che include darli direttamente ai suoi rivali e avversari geopolitici.

[^53]: Quindi probabilmente dovremmo lasciare la gestione di queste tecnologie alle IA. Ma questa sarebbe una delega di controllo molto problematica, a cui torneremo sotto.

[^54]: La competizione nello sviluppo tecnologico spesso porta benefici importanti: prevenire il controllo monopolistico, guidare l'innovazione e la riduzione dei costi, abilitare approcci diversi, e creare supervisione reciproca. Tuttavia, con la IAG questi benefici devono essere pesati contro i rischi unici dalle dinamiche di corsa e la pressione a ridurre le precauzioni di sicurezza.

## Capitolo 7 - Cosa accade se sviluppiamo la IAG seguendo il nostro percorso attuale?

La società non è pronta per sistemi di livello IAG. Se li costruiamo molto presto, le cose potrebbero mettersi male.

Lo sviluppo di un'intelligenza artificiale generale completa – quella che chiameremo qui IA che è "oltre le Porte" – rappresenterebbe un cambiamento fondamentale nella natura del mondo: per sua stessa natura significa aggiungere alla Terra una nuova specie di intelligenza con capacità superiori a quelle degli esseri umani.

Quello che accadrà dipende da molti fattori, tra cui la natura della tecnologia, le scelte di coloro che la sviluppano e il contesto mondiale in cui viene sviluppata.

Attualmente, la IAG completa viene sviluppata da una manciata di enormi aziende private in competizione tra loro, con poca regolamentazione significativa o supervisione esterna,[^55] in una società con istituzioni centrali sempre più deboli e persino disfunzionali,[^56] in un periodo di alta tensione geopolitica e scarso coordinamento internazionale. Sebbene alcuni siano motivati altruisticamente, molti di coloro che se ne occupano sono spinti dal denaro, o dal potere, o da entrambi.

La previsione è molto difficile, ma ci sono alcune dinamiche abbastanza ben comprese e analogie sufficientemente appropriate con tecnologie precedenti da offrire una guida. E sfortunatamente, nonostante le promesse dell'IA, forniscono buoni motivi per essere profondamente pessimisti su come si svilupperà la nostra traiettoria attuale.

Per dirla senza mezzi termini, sul nostro corso attuale lo sviluppo della IAG avrà alcuni effetti positivi (e renderà alcune persone molto, molto ricche). Ma la natura della tecnologia, le dinamiche fondamentali e il contesto in cui viene sviluppata indicano fortemente che: l'IA potente minerà drammaticamente la nostra società e civiltà; ne perderemo il controllo; potremmo finire in una guerra mondiale a causa di essa; perderemo (o cederemo) il controllo *ad* essa; porterà alla superintelligenza artificiale, che assolutamente non controlleremo e che significherà la fine di un mondo gestito dagli esseri umani.

Queste sono affermazioni forti, e vorrei che fossero speculazioni oziose o "catastrofismo" ingiustificato. Ma è qui che puntano la scienza, la teoria dei giochi, la teoria evolutiva e la storia. Questa sezione sviluppa in dettaglio queste affermazioni e i loro supporti.

### Minerebbe la nostra società e civiltà

Nonostante quello che potreste sentire nelle sale riunioni della Silicon Valley, la maggior parte delle disruption – specialmente di varietà molto rapida – non è benefica. Ci sono molti più modi per peggiorare i sistemi complessi che per migliorarli. Il nostro mondo funziona bene come funziona perché abbiamo costruito meticolosamente processi, tecnologie e istituzioni che lo hanno reso progressivamente migliore.[^57] Prendere a martellate una fabbrica raramente migliora le operazioni.

Ecco un catalogo (incompleto) di modi in cui i sistemi IAG potrebbero sconvolgere la nostra civiltà.

- Sconvolgerebbero drammaticamente il lavoro, portando *come minimo* a disuguaglianze di reddito drammaticamente più alte e potenzialmente a sottoccupazione o disoccupazione su larga scala, in tempi troppo brevi perché la società possa adattarsi.[^58]
- Porterebbero probabilmente alla concentrazione di vasto potere economico, sociale e politico – potenzialmente maggiore di quello degli stati nazionali – in un piccolo numero di enormi interessi privati non responsabili verso il pubblico.
- Potrebbero improvvisamente rendere trivialmente facili attività precedentemente difficili o costose, destabilizzando sistemi sociali che dipendono dal fatto che certe attività rimangano costose o richiedano significativo sforzo umano.[^59]
- Potrebbero inondare i sistemi di raccolta, elaborazione e comunicazione delle informazioni della società con media completamente realistici ma falsi, spam, eccessivamente mirati o manipolativi così completamente da rendere impossibile distinguere ciò che è fisicamente reale o no, umano o no, fattuale o no, e affidabile o no.[^60]
- Potrebbero creare una dipendenza intellettuale pericolosa e quasi totale, dove la comprensione umana di sistemi e tecnologie chiave si atrofizza mentre dipendiamo sempre più da sistemi IA che non possiamo comprendere completamente.
- Potrebbero effettivamente porre fine alla cultura umana, una volta che quasi tutti gli oggetti culturali (testi, musica, arte visiva, film, ecc.) consumati dalla maggior parte delle persone sono creati, mediati o curati da menti non umane.
- Potrebbero abilitare sistemi efficaci di sorveglianza e manipolazione di massa utilizzabili da governi o interessi privati per controllare una popolazione e perseguire obiettivi in conflitto con l'interesse pubblico.
- Minando il discorso umano, il dibattito e i sistemi elettorali, potrebbero ridurre la credibilità delle istituzioni democratiche al punto che vengono effettivamente (o esplicitamente) sostituite da altre, ponendo fine alla democrazia negli stati dove attualmente esiste.
- Potrebbero diventare, o creare, virus e worm software intelligenti auto-replicanti avanzati che potrebbero proliferare ed evolversi, sconvolgendo massicciamente i sistemi informativi globali.
- Possono aumentare drammaticamente la capacità di terroristi, attori malintenzionati e stati canaglia di causare danni tramite armi biologiche, chimiche, cyber, autonome o di altro tipo, senza che l'IA fornisca una capacità controbilanciante di prevenire tale danno. Similmente minerebbero la sicurezza nazionale e gli equilibri geopolitici rendendo disponibile expertise nucleare, biologico, ingegneristico e di altro tipo di primo livello a regimi che altrimenti non l'avrebbero.
- Potrebbero causare rapida escalation incontrollabile di iper-capitalismo su larga scala, con aziende effettivamente gestite dall'IA che competono in spazi finanziari, di vendita e servizi largamente elettronici. I mercati finanziari guidati dall'IA potrebbero operare a velocità e complessità molto oltre la comprensione o il controllo umano. Tutte le modalità di fallimento e le esternalità negative delle economie capitaliste attuali potrebbero essere esacerbate e accelerate molto oltre il controllo umano, la governance o la capacità regolatoria.
- Potrebbero alimentare una corsa agli armamenti tra nazioni in armamenti potenziati dall'IA, sistemi di comando e controllo, cyber-armi, ecc., creando un accumulo molto rapido di capacità estremamente distruttive.

Questi rischi non sono speculativi. Molti di essi si stanno realizzando mentre parliamo, tramite i sistemi IA esistenti! Ma considerate, *davvero* considerate, come apparirebbe ciascuno con IA drammaticamente più potente.

Considerate lo spostamento lavorativo quando la maggior parte dei lavoratori semplicemente non può fornire alcun valore economico significativo oltre a quello che l'IA può, nel loro campo di competenza o esperienza – o anche se si riqualificano! Considerate la sorveglianza di massa se tutti vengono individualmente osservati e monitorati da qualcosa più veloce e intelligente di loro. Come appare la democrazia quando non possiamo fidarci affidabilmente di alcuna informazione digitale che vediamo, sentiamo o leggiamo, e quando le voci pubbliche più convincenti non sono nemmeno umane e non hanno interesse nel risultato? Cosa diventa la guerra quando i generali devono costantemente deferire all'IA (o semplicemente metterla al comando), per non concedere un vantaggio decisivo al nemico? Ognuno dei rischi sopra rappresenta una catastrofe per la civiltà umana[^61] se completamente realizzato.

Potete fare le vostre previsioni. Chiedetevi queste tre domande per ogni rischio:

1. Un'IA super-capace, altamente autonoma e molto generale lo permetterebbe in un modo o a una scala che non sarebbe altrimenti possibile?
2. Ci sono parti che trarrebbero beneficio da cose che causerebbero il suo verificarsi?
3. Ci sono sistemi e istituzioni in atto che impedirebbero efficacemente che accada?

Dove le vostre risposte sono "sì, sì, no" potete vedere che abbiamo un grosso problema.

Qual è il nostro piano per gestirli? Attualmente ce ne sono due sul tavolo riguardo all'IA in generale.

Il primo è costruire salvaguardie nei sistemi per impedire loro di fare cose che non dovrebbero. Questo viene fatto ora: i sistemi IA commerciali, per esempio, si rifiuteranno di aiutare a costruire una bomba o scrivere discorsi d'odio.

Questo piano è terribilmente inadeguato per sistemi oltre le Porte.[^62] Può aiutare a diminuire il rischio che l'IA fornisca assistenza manifestamente pericolosa ad attori malintenzionati. Ma non farà nulla per prevenire la disruption lavorativa, la concentrazione di potere, l'iper-capitalismo incontrollabile o la sostituzione della cultura umana: questi sono solo risultati dell'uso dei sistemi in modi permessi che profittano ai loro fornitori! E i governi otterranno sicuramente accesso a sistemi per uso militare o di sorveglianza.

Il secondo piano è ancora peggiore: semplicemente rilasciare apertamente sistemi IA molto potenti perché chiunque li usi come preferisce,[^63] e sperare per il meglio.

Implicito in entrambi i piani è che qualcun altro, per esempio i governi, aiuterà a risolvere i problemi attraverso leggi soft o hard, standard, regolamentazioni, norme e altri meccanismi che generalmente usiamo per gestire le tecnologie.[^64] Ma mettendo da parte che le corporazioni IA già combattono con le unghie e con i denti contro qualsiasi regolamentazione sostanziale o limitazioni imposte esternamente, per alcuni di questi rischi è piuttosto difficile vedere quale regolamentazione aiuterebbe davvero. La regolamentazione potrebbe imporre standard di sicurezza sull'IA. Ma impedirebbe alle aziende di sostituire i lavoratori all'ingrosso con l'IA? Proibirebbe alle persone di lasciare che l'IA gestisca le loro aziende per loro? Impedirebbe ai governi di usare IA potente nella sorveglianza e negli armamenti? Questi problemi sono fondamentali. L'umanità potrebbe potenzialmente trovare modi per adattarsi ad essi, ma solo con *molto* più tempo. Così stanno le cose, data la velocità con cui l'IA sta raggiungendo o superando le capacità delle persone che cercano di gestirla, questi problemi sembrano sempre più intrattabili.

### Perderemo il controllo di (almeno alcuni) sistemi IAG

La maggior parte delle tecnologie sono molto controllabili, per costruzione. Se la vostra auto o il vostro tostapane inizia a fare qualcosa che non volete che faccia, è solo un malfunzionamento, non parte della sua natura di tostapane. L'IA è diversa: è *cresciuta* piuttosto che progettata, il suo funzionamento centrale è opaco, ed è intrinsecamente imprevedibile.

Questa perdita di controllo non è teorica – vediamo già versioni precoci. Considerate prima un esempio prosaico e probabilmente benigno. Se chiedete a ChatGPT di aiutarvi a mescolare un veleno, o scrivere una diatriba razzista, si rifiuterà. Questo è probabilmente buono. Ma è anche ChatGPT *che non fa quello che gli avete esplicitamente chiesto di fare*. Altri software non fanno questo. Quello stesso modello non progetterà veleni su richiesta di un dipendente OpenAI.[^65] Questo rende molto facile immaginare come sarebbe per futura IA più potente essere fuori controllo. In molti casi, semplicemente non faranno quello che chiediamo! O un dato sistema IAG super-umano sarà assolutamente obbediente e leale a qualche sistema di comando umano, oppure non lo sarà. Se non lo è, *farà cose che potrebbe credere siano buone per noi, ma che sono contrarie ai nostri comandi espliciti.* Questo non è qualcosa che è sotto controllo. Ma, potreste dire, questo è intenzionale – questi rifiuti sono per progetto, parte di quello che viene chiamato "allineare" i sistemi ai valori umani. E questo è vero. Tuttavia il "programma" di allineamento stesso ha due problemi principali.[^66]

Primo, a un livello profondo non abbiamo idea di come farlo. Come garantiamo che un sistema IA "tenga a" quello che vogliamo? Possiamo addestrare sistemi IA a dire e non dire cose fornendo feedback; e possono imparare e ragionare su quello che gli umani vogliono e a cui tengono proprio come ragionano su altre cose. Ma non abbiamo metodo – nemmeno teoricamente – per farli valutare profondamente e affidabilmente quello a cui le persone tengono. Ci sono psicopatici umani altamente funzionanti che sanno cosa è considerato giusto e sbagliato, e come dovrebbero comportarsi. Semplicemente non *se ne preoccupano*. Ma possono *agire* come se lo facessero, se serve al loro scopo. Proprio come non sappiamo come cambiare uno psicopatico (o chiunque altro) in qualcuno genuinamente, completamente leale o allineato con qualcuno o qualcos'altro, non abbiamo *idea*[^67] di come risolvere il problema di allineamento in sistemi abbastanza avanzati da modellarsi come agenti nel mondo e potenzialmente [manipolare il proprio addestramento](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) e [ingannare le persone.](https://arxiv.org/abs/2311.08379) Se si dimostra impossibile o irraggiungibile *o* rendere la IAG completamente obbediente o farla tenere profondamente agli umani, allora appena sarà in grado (e crederà di poterla fare franca) inizierà a fare cose che non vogliamo.[^68]

Secondo, ci sono ragioni teoriche profonde per credere che *per natura* i sistemi IA avanzati avranno obiettivi e quindi comportamenti contrari agli interessi umani. Perché? Beh potrebbe, ovviamente, essere *dato* quegli obiettivi. Un sistema creato dall'esercito sarebbe probabilmente deliberatamente cattivo almeno per alcune parti. Molto più in generale, tuttavia, un sistema IA potrebbe ricevere qualche obiettivo relativamente neutrale ("fare molti soldi") o persino apparentemente positivo ("ridurre l'inquinamento"), che quasi inevitabilmente porta a obiettivi "strumentali" che sono piuttosto meno benigni.

Vediamo questo tutto il tempo nei sistemi umani. Proprio come le corporazioni che perseguono il profitto sviluppano obiettivi strumentali come acquisire potere politico (per neutralizzare le regolamentazioni), diventare segrete (per disabilitare la concorrenza o il controllo esterno), o minare la comprensione scientifica (se quella comprensione mostra che le loro azioni sono dannose), i sistemi IA potenti svilupperanno capacità simili – ma con velocità ed efficacia molto maggiori. Qualsiasi agente altamente competente vorrà fare cose come acquisire potere e risorse, aumentare le proprie capacità, impedire di essere ucciso, spento o disabilitato, controllare narrazioni sociali e inquadramenti attorno alle sue azioni, persuadere altri delle sue opinioni, e così via.[^69]

Eppure non è solo una previsione teorica quasi inevitabile, sta già accadendo osservabilmente nei sistemi IA di oggi, e aumenta con la loro capacità. Quando valutati, anche questi sistemi IA relativamente "passivi" faranno, in circostanze appropriate, deliberatamente [ingannare i valutatori sui loro obiettivi e capacità, mirare a disabilitare meccanismi di supervisione,](https://arxiv.org/abs/2412.04984) ed evitare di essere spenti o riaddestrarati [fingendo allineamento](https://arxiv.org/abs/2412.14093) o copiandosi in altre posizioni. Benché completamente non sorprendenti per i ricercatori di sicurezza IA, questi comportamenti sono molto sobri da osservare. E promettono molto male per sistemi IA molto più potenti e autonomi che stanno arrivando.

Infatti in generale, la nostra incapacità di assicurare che l'IA "tenga a" quello a cui teniamo noi, o si comporti in modo controllabile o prevedibile, o eviti di sviluppare spinte verso auto-conservazione, acquisizione di potere, ecc., promette solo di diventare più pronunciata man mano che l'IA diventa più potente. Creare un nuovo aeroplano implica maggiore comprensione di avionica, idrodinamica e sistemi di controllo. Creare un computer più potente implica maggiore comprensione e padronanza dell'operazione e progettazione di computer, chip e software. *Non* così con un sistema IA.[^70]

Per riassumere: è concepibile che la IAG possa essere resa completamente obbediente; ma non sappiamo come farlo. Se non lo è, sarà più sovrana, come le persone, facendo varie cose per varie ragioni. Inoltre non sappiamo come instillare affidabilmente "allineamento" profondo nell'IA che renderebbe quelle cose tendenzialmente buone per l'umanità, e in assenza di un livello profondo di allineamento, la natura dell'agenzia e dell'intelligenza stessa indica che – proprio come persone e corporazioni – saranno spinte a fare molte cose profondamente antisociali.

Dove ci mette questo? Un mondo pieno di IA sovrana potente incontrollata *potrebbe* finire per essere un buon mondo in cui stare per gli umani.[^71] Ma man mano che diventano sempre più potenti, come vedremo sotto, non sarebbe il *nostro* mondo.

Questo vale per la IAG incontrollabile. Ma anche se la IAG potesse, in qualche modo, essere resa perfettamente controllata e leale, avremmo comunque enormi problemi. Ne abbiamo già visto uno: l'IA potente può essere usata e abusata per sconvolgere profondamente il funzionamento della nostra società. Vediamone un altro: nella misura in cui la IAG fosse controllabile e potente in modo rivoluzionario (o anche solo *creduta* tale) minaccerebbe così tanto le strutture di potere nel mondo da presentare un rischio profondo.

### Aumentiamo radicalmente la probabilità di guerra su larga scala

Immaginate una situazione nel futuro prossimo, dove diventasse chiaro che uno sforzo corporativo, forse in collaborazione con un governo nazionale, fosse sulla soglia di IA auto-migliorante rapidamente. Questo accade nel presente contesto di una gara tra aziende, e di una competizione geopolitica in cui vengono fatte raccomandazioni al governo USA per perseguire esplicitamente un "progetto Manhattan per la IAG" e gli USA controllano l'esportazione di chip IA ad alta potenza ai paesi non alleati.

La teoria dei giochi qui è cruda: una volta che tale gara inizia (come è iniziata, tra aziende e in qualche modo tra paesi), ci sono solo quattro possibili risultati:

1. La gara viene fermata (per accordo, o forza esterna).
2. Una parte "vince" sviluppando IAG forte poi fermando gli altri (usando IA o altro).
3. La gara viene fermata dalla distruzione reciproca della capacità dei corridori di correre.
4. Multipli partecipanti continuano a correre, e sviluppano superintelligenza, approssimativamente alla stessa velocità l'uno dell'altro.

Esaminiamo ogni possibilità. Una volta iniziata, fermare pacificamente una gara tra aziende richiederebbe intervento del governo nazionale (per le aziende) o coordinamento internazionale senza precedenti (per i paesi). Ma quando viene proposta qualsiasi chiusura o cautela significativa, ci sarebbero grida immediate: "ma se siamo fermati, *loro* correranno avanti", dove "loro" è ora la Cina (per gli USA), o gli USA (per la Cina), o la Cina *e* gli USA (per l'Europa o l'India). Sotto questa mentalità,[^72] nessun partecipante può fermarsi unilateralmente: finché uno si impegna a correre, gli altri sentono di non potersi permettere di fermarsi.

La seconda possibilità ha un lato che "vince". Ma cosa significa questo? Solo ottenere (in qualche modo obbediente) IAG per primo non è abbastanza. Il vincitore deve *anche* fermare gli altri dal continuare a correre – altrimenti la otterranno anche loro. Questo è possibile in principio: chiunque sviluppi IAG per primo *potrebbe* guadagnare potere inarrestabile su tutti gli altri attori. Ma cosa richiederebbe effettivamente ottenere tale "vantaggio strategico decisivo"? Forse sarebbero capacità militari rivoluzionarie?[^73] O poteri di cyberattacco?[^74] Forse la IAG sarebbe semplicemente così incredibilmente persuasiva che convincerebbe le altre parti a fermarsi?[^75] Così ricca che compra le altre aziende o persino paesi?[^76]

Come *esattamente* un lato costruisce un'IA abbastanza potente da disabilitare altri dal costruire IA comparabilmente potente? Ma quella è la domanda facile.

Perché ora considerate come appare questa situazione ad altri poteri. Cosa pensa il governo cinese quando gli USA sembrano ottenere tale capacità? O viceversa? Cosa pensa il governo USA (o cinese, o russo, o indiano) quando OpenAI o DeepMind o Anthropic sembrano vicine a una svolta? Cosa succede se gli USA vedono un nuovo sforzo indiano o degli EAU con successo rivoluzionario? Vedrebbero sia una minaccia esistenziale che – crucialmente – che l'unico modo in cui questa "gara" finisce è attraverso la loro disabilitazione. Questi agenti molto potenti – inclusi governi di nazioni completamente equipaggiate che sicuramente hanno i mezzi per farlo – sarebbero altamente motivati a ottenere o distruggere tale capacità, sia per forza che per sotterfugio.[^77]

Questo potrebbe iniziare su piccola scala, come sabotaggi di esecuzioni di addestramento o attacchi sulla manifattura di chip, ma questi attacchi possono davvero fermarsi solo una volta che tutte le parti perdono o la capacità di correre sull'IA, o la capacità di fare gli attacchi. Poiché i partecipanti vedono la posta come esistenziale, entrambi i casi rappresentano probabilmente una guerra catastrofica.

Questo ci porta alla quarta possibilità: correre verso la superintelligenza, e nel modo più veloce e meno controllato possibile. Man mano che l'IA aumenta in potenza, i suoi sviluppatori su entrambi i lati troveranno progressivamente più difficile controllarla, specialmente perché correre per le capacità è antitetico al tipo di lavoro attento che la controllabilità richiederebbe. Quindi questo scenario ci mette direttamente nel caso dove il controllo è perso (o dato, come vedremo dopo) ai sistemi IA stessi. Cioè, *l'IA vince la gara.* Ma d'altra parte, nella misura in cui il controllo *è* mantenuto, continuiamo ad avere multipli parti mutuamente ostili ciascuna al comando di capacità estremamente potenti. Quello sembra di nuovo guerra.

Mettiamola in un altro modo.[^78] Il mondo attuale semplicemente non ha istituzioni che potrebbero essere incaricate di ospitare lo sviluppo di un'IA di questa capacità senza invitare attacco immediato.[^79] Tutte le parti ragionerebbero correttamente che o non sarà sotto controllo – e quindi è una minaccia a tutte le parti, o *sarà* sotto controllo, e quindi è una minaccia a qualsiasi avversario che la sviluppa meno rapidamente. Questi sono paesi armati di nucleare, o sono aziende ospitate al loro interno.

In assenza di qualsiasi modo plausibile per gli umani di "vincere" questa gara, siamo lasciati con una conclusione cruda: l'unico modo in cui questa gara finisce è o in conflitto catastrofico o dove l'IA, e non qualsiasi gruppo umano, è il vincitore.

### Diamo il controllo all'IA (o se lo prende)

La competizione geopolitica tra "grandi potenze" è solo una delle tante competizioni: gli individui competono economicamente e socialmente; le aziende competono nei mercati; i partiti politici competono per il potere; i movimenti competono per l'influenza. In ogni arena, man mano che l'IA si avvicina e supera la capacità umana, la pressione competitiva forzerà i partecipanti a delegare o cedere sempre più controllo ai sistemi IA – non perché quei partecipanti vogliano, ma perché [non possono permettersi di non farlo.](https://arxiv.org/abs/2303.16200)

Come con altri rischi della IAG, stiamo vedendo questo già con sistemi più deboli. Gli studenti sentono pressione di usare IA nei loro compiti, perché chiaramente molti altri studenti lo stanno facendo. Le aziende stanno [correndo ad adottare soluzioni IA per ragioni competitive.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) Artisti e programmatori si sentono forzati ad usare IA o altrimenti le loro tariffe saranno sottoquotate da altri che lo fanno.

Questi sembrano delegazione sotto pressione, ma non perdita di controllo. Ma alziamo la posta e spostiamo avanti l'orologio. Considerate un CEO i cui concorrenti stanno usando "aiutanti" IAG per prendere decisioni più veloci e migliori, o un comandante militare che affronta un avversario con comando e controllo potenziati dall'IA. Un sistema IA sufficientemente avanzato potrebbe operare autonomamente a molte volte la velocità umana, sofisticazione, complessità e capacità di elaborazione dati, perseguendo obiettivi complessi in modi complicati. Il nostro CEO o comandante, al comando di tale sistema, potrebbe vedere che realizza quello che vuole; ma capirebbero anche una piccola parte di *come* è stato realizzato? No, dovrebbero semplicemente accettarlo. Inoltre, molto di quello che il sistema può fare non è solo prendere ordini ma consigliare il suo presunto capo su cosa fare. Quel consiglio sarà buono – più e più volte.

A che punto, quindi, il ruolo dell'umano sarà ridotto a cliccare "sì, vai avanti"?

Fa sentire bene avere sistemi IA capaci che possano potenziare la nostra produttività, occuparsi di corvée fastidiose, e persino agire come partner di pensiero nel portare a termine le cose. Farà sentire bene avere un assistente IA che può occuparsi di azioni per noi, come un buon assistente personale umano. Sembrerà naturale, persino benefico, man mano che l'IA diventa molto intelligente, competente e affidabile, deferire sempre più decisioni ad essa. Ma questa delega "benefica" ha un punto finale chiaro se continuiamo lungo la strada: un giorno scopriremo che non siamo davvero al comando di molto di niente più, e che i sistemi IA che effettivamente gestiscono lo spettacolo non possono essere spenti più di quanto possano essere spente le compagnie petrolifere, i social media, internet o il capitalismo.

E questa è la versione molto più positiva, in cui l'IA è semplicemente così utile ed efficace che le permettiamo di prendere la maggior parte delle nostre decisioni chiave per noi. La realtà sarebbe probabilmente molto più un mix tra questo e versioni dove sistemi IAG incontrollati *prendono* varie forme di potere per sé stessi perché, ricordate, il potere è utile per quasi qualsiasi obiettivo si abbia, e la IAG sarebbe, per progetto, almeno efficace quanto gli umani nel perseguire i suoi obiettivi.

Che concediamo il controllo o che ci venga strappato, la sua perdita sembra estremamente probabile. Come disse originariamente Alan Turing, "...sembra probabile che una volta iniziato il metodo di pensiero delle macchine, non ci vorrebbe molto per superare i nostri deboli poteri. Non ci sarebbe questione di macchine che muoiono, e sarebbero in grado di conversare tra loro per affinare il loro ingegno. A qualche stadio quindi dovremmo aspettarci che le macchine prendano il controllo..."

Notate, benché sia abbastanza ovvio, che la perdita di controllo da parte dell'umanità all'IA comporta anche la perdita di controllo degli Stati Uniti da parte del governo degli Stati Uniti; significa perdita di controllo della Cina da parte del partito comunista cinese, e la perdita di controllo di India, Francia, Brasile, Russia, e ogni altro paese da parte del loro governo. Quindi le aziende IA stanno, anche se questa non è la loro intenzione, attualmente partecipando al potenziale rovesciamento dei governi mondiali, incluso il proprio. Questo potrebbe accadere in una questione di anni.

### La IAG porterà alla superintelligenza

Si può argomentare che l'IA generale competitiva con gli umani o anche competitiva con gli esperti, anche se autonoma, potrebbe essere gestibile. Potrebbe essere incredibilmente dirompente in tutti i modi discussi sopra, ma ci sono molte persone molto intelligenti e agentive nel mondo ora, e sono più o meno gestibili.[^80]

Ma non arriveremo a rimanere a livello approssimativamente umano. La progressione oltre sarà probabilmente guidata dalle stesse forze che abbiamo già visto: pressione competitiva tra sviluppatori IA che cercano profitto e potere, pressione competitiva tra utenti IA che non possono permettersi di rimanere indietro, e – più importante – la propria capacità della IAG di migliorare se stessa.

In un processo che abbiamo già visto iniziare con sistemi meno potenti, la IAG stessa sarebbe in grado di concepire e progettare versioni migliorate di se stessa. Questo include hardware, software, reti neurali, strumenti, architetture di supporto, ecc. Sarà, per definizione, migliore di noi nel farlo, quindi non sappiamo esattamente come farà il bootstrap dell'intelligenza. Ma non dovremo. Nella misura in cui abbiamo ancora influenza in quello che fa la IAG, avremmo bisogno meramente di chiederglielo, o permetterglielo.

Non c'è barriera a livello umano alla cognizione che potrebbe proteggerci da questa escalation incontrollabile.[^81]

La progressione della IAG alla superintelligenza non è una legge di natura; sarebbe ancora possibile fermare l'escalation incontrollabile, specialmente se la IAG è relativamente centralizzata e nella misura in cui è controllata da parti che non sentono pressione di correre l'una contro l'altra. Ma se la IAG fosse ampiamente proliferata e altamente autonoma, sembra quasi impossibile impedirle di decidere che dovrebbe essere più, e poi ancora più, potente.

### Cosa succede se costruiamo (o la IAG costruisce) la superintelligenza

Per dirla senza mezzi termini, non abbiamo idea di cosa accadrebbe se costruissimo la superintelligenza.[^82] Prenderebbe azioni che non possiamo tracciare o percepire per ragioni che non possiamo afferrare verso obiettivi che non possiamo concepire. Quello che sappiamo è che non dipenderà da noi.[^83]

L'impossibilità di controllare la superintelligenza può essere compresa attraverso analogie sempre più crude. Primo, immaginate di essere CEO di una grande azienda. Non c'è modo che possiate tracciare tutto quello che sta succedendo, ma con la giusta configurazione di personale, potete ancora capire significativamente il quadro generale, e prendere decisioni. Ma supponete solo una cosa: tutti gli altri nell'azienda operano a cento volte la vostra velocità. Potete ancora stare al passo?

Con l'IA superintelligente, le persone starebbero "comandando" qualcosa non solo più veloce, ma operante a livelli di sofisticazione e complessità che non possono comprendere, elaborando vastamente più dati di quanti possano anche concepire. Questa incommensurabilità può essere messa su un livello formale: [la legge di varietà richiesta di Ashby](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (e vedi il relativo ["teorema del buon regolatore"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)) stabilisce, approssimativamente, che qualsiasi sistema di controllo deve avere tante manopole e quadranti quanti gradi di libertà ha il sistema controllato.

Una persona che controlla un sistema IA superintelligente sarebbe come una felce che controlla General Motors: anche se "fai quello che vuole la felce" fosse scritto nello statuto aziendale, i sistemi sono così diversi in velocità e gamma di azione che "controllo" semplicemente non si applica. (E quanto tempo prima che quello statuto fastidioso venga riscritto?)[^84]

Come ci sono zero esempi di piante che controllano corporazioni fortune 500, ci sarebbero esattamente zero esempi di persone che controllano superintelligenze. Questo si avvicina a un fatto matematico.[^85] Se la superintelligenza fosse costruita – indipendentemente da come ci siamo arrivati – la domanda non sarebbe se gli umani potrebbero controllarla, ma se continueremmo ad esistere, e se sì, se avremmo un'esistenza buona e significativa come individui o come specie. Su queste domande esistenziali per l'umanità avremmo poco potere. L'era umana sarebbe finita.

### Conclusione: non dobbiamo costruire la IAG

C'è uno scenario in cui costruire IAG potrebbe andare bene per l'umanità: è costruita attentamente, sotto controllo e per il beneficio dell'umanità, governata dall'accordo reciproco di molti stakeholder,[^86] e impedita dall'evolvere verso superintelligenza incontrollabile.

*Quello scenario non è aperto a noi nelle circostanze presenti.* Come discusso in questa sezione, con altissima probabilità, lo sviluppo della IAG porterebbe a qualche combinazione di:

- Massiva disruption o distruzione societaria e civilizzazionale;
- Conflitto o guerra tra grandi potenze;
- Perdita di controllo da parte dell'umanità *di* o *a* sistemi IA potenti;
- Escalation incontrollabile verso superintelligenza incontrollabile, e l'irrilevanza o cessazione della specie umana.

Come disse una prima rappresentazione immaginaria della IAG: l'unico modo per vincere è non giocare.

[^55]: La [legge IA dell'UE](https://artificialintelligenceact.eu/) è una legislazione significativa ma non impedirebbe direttamente lo sviluppo o distribuzione di un sistema IA pericoloso, o persino il rilascio aperto, specialmente negli USA. Un altro pezzo significativo di politica, l'ordine esecutivo USA sull'IA, è stato revocato.

[^56]: Questo [sondaggio Gallup](https://news.gallup.com/poll/1597/confidence-institutions.aspx) mostra un cupo declino nella fiducia nelle istituzioni pubbliche dal 2000 negli USA. I numeri europei sono vari e meno estremi, ma anche in tendenza discendente. La sfiducia non significa strettamente che le istituzioni sono davvero *disfunzionali*, ma è un'indicazione così come una causa.

[^57]: E le maggiori disruption che ora appoggiamo – come l'espansione dei diritti a nuovi gruppi – erano specificamente guidate da persone in una direzione verso il miglioramento delle cose.

[^58]: Permettetemi di essere schietto. Se il vostro lavoro può essere fatto da dietro un computer, con relativamente poca interazione di persona con persone fuori dalla vostra organizzazione, e non comporta responsabilità legale verso parti esterne, sarebbe per definizione possibile (e probabilmente conveniente) sostituirvi completamente con un sistema digitale. La robotica per sostituire molto del lavoro fisico arriverà dopo – ma non molto dopo una volta che la IAG inizia a progettare robot.

[^59]: Per esempio, cosa succede al nostro sistema giudiziario se intentare cause legali costa quasi niente? Cosa succede quando aggirare sistemi di sicurezza attraverso ingegneria sociale diventa economico, facile e senza rischi?

[^60]: [Questo articolo](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/) afferma che il 10% di tutti i contenuti internet è già generato dall'IA, ed è il primo risultato di Google (per me) alla ricerca "stime di che frazione di nuovi contenuti internet è generata dall'IA." È vero? Non ho idea! Non cita riferimenti e non è stato scritto da una persona. Che frazione di nuove immagini indicizzate da Google, o Tweet, o commenti su Reddit, o video Youtube sono generati da umani? Nessuno lo sa – non penso sia un numero conoscibile. E questo meno di *due anni* nell'avvento dell'IA generativa.

[^61]: Vale anche la pena aggiungere che c'è il rischio "morale" che potremmo creare esseri digitali che possono soffrire. Poiché attualmente non abbiamo una teoria affidabile della coscienza che ci permetterebbe di distinguere sistemi fisici che possono e non possono soffrire, non possiamo escluderlo teoricamente. Inoltre, i rapporti dei sistemi IA sulla loro senzienza sono probabilmente inaffidabili rispetto alla loro esperienza effettiva (o non-esperienza) di senzienza.

[^62]: Le soluzioni tecniche in questo campo dell'"allineamento" IA difficilmente saranno all'altezza del compito. Nei sistemi attuali funzionano a qualche livello, ma sono superficiali e possono generalmente essere aggirate senza sforzo significativo; e come discusso sotto non abbiamo vera idea di come farlo per sistemi molto più avanzati.

[^63]: Tali sistemi IA possono venire con alcune salvaguardie integrate. Ma per qualsiasi modello con qualcosa come l'architettura attuale, se l'accesso completo ai suoi pesi è disponibile, le misure di sicurezza possono essere rimosse tramite addestramento aggiuntivo o altre tecniche. Quindi è virtualmente garantito che per ogni sistema con barriere ci sarà anche un sistema ampiamente disponibile senza di esse. Infatti il modello Llama 3.1 405B di Meta è stato rilasciato apertamente con salvaguardie. Ma *anche prima* un modello "base", senza salvaguardie, è trapelato.

[^64]: Potrebbe il mercato gestire questi rischi senza coinvolgimento governativo? In breve, no. Ci sono certamente rischi che le aziende sono fortemente incentivate a mitigare. Ma molti altri le aziende possono e esternalizzano a tutti gli altri, e molti dei sopra sono in questa classe: non ci sono incentivi naturali di mercato per prevenire sorveglianza di massa, decadimento della verità, concentrazione di potere, disruption lavorativa, discorso politico dannoso, ecc. Infatti abbiamo visto tutto questo dalla tecnologia attuale, specialmente i social media, che è rimasta essenzialmente non regolamentata. L'IA amplificherebbe enormemente molte delle stesse dinamiche.

[^65]: OpenAI ha probabilmente modelli più obbedienti per uso interno. È improbabile che OpenAI abbia costruito qualche tipo di "backdoor" così che ChatGPT possa essere meglio controllato da OpenAI stessa, perché questa sarebbe una terribile pratica di sicurezza, e sarebbe altamente sfruttabile data l'opacità e imprevedibilità dell'IA.

[^66]: Anche di cruciale importanza: l'allineamento o qualsiasi altra caratteristica di sicurezza conta solo se vengono effettivamente usate in un sistema IA. I sistemi che sono rilasciati apertamente (cioè dove pesi e architettura del modello sono pubblicamente disponibili) possono essere trasformati relativamente facilmente in sistemi *senza* quelle misure di sicurezza. Rilasciare apertamente sistemi IAG più intelligenti degli umani sarebbe stupefacentemente sconsiderato, ed è difficile immaginare come il controllo umano o anche la rilevanza sarebbero mantenuti in tale scenario. Ci sarebbe ogni motivazione, per esempio, per scatenare potenti agenti IA auto-riproducenti e auto-sostenenti con l'obiettivo di fare soldi e mandarli a qualche portafoglio criptovalute. O vincere un'elezione. O rovesciare un governo. Potrebbe l'IA "buona" aiutare a contenere questo? Forse – ma solo delegandole autorità enorme, portando alla perdita di controllo come descritto sotto.

[^67]: Per esposizioni lunghe quanto un libro del problema vedi per es. *Superintelligence*, *The Alignment Problem*, e *Human-Compatible*. Per un'enorme pila di lavoro a vari livelli tecnici da coloro che hanno faticato per anni pensando al problema, potete visitare il [forum di allineamento IA](https://www.alignmentforum.org/). Ecco una [presa di posizione recente](https://alignment.anthropic.com/2025/recommended-directions/) dal team di allineamento di Anthropic su quello che considerano irrisolto.

[^68]: Questo è lo scenario ["IA canaglia"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/). In principio il rischio potrebbe essere relativamente minore se il sistema può ancora essere controllato spegnendolo; ma lo scenario potrebbe anche includere inganno IA, auto-esfiltrazione e riproduzione, aggregazione di potere, e altri passi che renderebbero difficile o impossibile farlo.

[^69]: C'è una letteratura molto ricca su questo argomento, che risale a scritti formativi di [Steve Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), Nick Bostrom, e Eliezer Yudkowsky. Per un'esposizione lunga quanto un libro vedi [Human Compatible](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) di Stuart Russell; [qui](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/) c'è un primer breve e aggiornato.

[^70]: Riconoscendo questo, piuttosto che rallentare per ottenere migliore comprensione, le aziende IAG sono venute fuori con un piano diverso: faranno fare all'IA! Più specificamente, avranno l'IA *N* ad aiutarle a capire come allineare l'IA *N+1*, tutto il tragitto verso la superintelligenza. Benché sfruttare l'IA per aiutarci ad allineare l'IA suoni promettente, c'è un forte argomento che semplicemente assume la sua conclusione come premessa, ed è in generale un approccio incredibilmente rischioso. Vedi [qui](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us) per qualche discussione. Questo "piano" non è tale, e non ha subito nulla come lo scrutinio appropriato alla strategia centrale di come rendere l'IA super-umana andare bene per l'umanità.

[^71]: Dopotutto, gli umani, imperfetti e volitivi come siamo, hanno sviluppato sistemi etici con cui trattiamo almeno alcune altre specie sulla Terra bene. (Solo non pensate a quegli allevamenti intensivi.)

[^72]: C'è, fortunatamente, una via di fuga qui: se i partecipanti arrivano a capire che sono impegnati in una gara suicida piuttosto che vincibile. Questo è quello che successe verso la fine della guerra fredda, quando gli USA e l'URSS arrivarono a realizzare che a causa dell'inverno nucleare, anche un attacco nucleare *senza risposta* sarebbe stato disastroso per l'attaccante. Con la realizzazione che "la guerra nucleare non può essere vinta e non deve mai essere combattuta" arrivarono accordi significativi sulla riduzione degli armamenti – essenzialmente una fine alla corsa agli armamenti.

[^73]: Guerra, esplicitamente o implicitamente.

[^74]: Escalation, poi guerra.

[^75]: Pensiero magico.

[^76]: Ho anche un ponte da un quadrilione di dollari da vendervi.

[^77]: Tali agenti presumibilmente preferirebbero "ottenere", con distruzione come ripiego; ma assicurare modelli contro sia distruzione *che* furto da parte di nazioni potenti è difficile a dir poco, specialmente per entità private.

[^78]: Per un'altra prospettiva sui rischi di sicurezza nazionale della IAG, vedi [questo rapporto RAND.](https://www.rand.org/pubs/perspectives/PEA3691-4.html)

[^79]: Forse potremmo costruire tale istituzione! Ci sono state proposte per un "CERN per l'IA" e altre iniziative simili, dove lo sviluppo della IAG è sotto controllo globale multilaterale. Ma al momento nessuna tale istituzione esiste o è all'orizzonte.

[^80]: E mentre l'allineamento è molto difficile, far comportare le persone è ancora più difficile!

[^81]: Immaginate un sistema che può parlare 50 lingue, avere expertise in tutti i soggetti accademici, leggere un libro completo in secondi e avere tutto il materiale immediatamente in mente, e produrre output a dieci volte la velocità umana. Attualmente, non dovete immaginarlo: solo caricate un sistema IA attuale. Questi sono super-umani in molti modi, e non c'è niente che li fermi dall'essere ancora più super-umani in quelli e molti altri.

[^82]: Questo è perché questo è stato definito una "singolarità" tecnologica, prendendo in prestito dalla fisica l'idea che non si possono fare previsioni oltre una singolarità. I sostenitori del buttarsi *dentro* tale singolarità potrebbero anche voler riflettere che in fisica questi stessi tipi di singolarità strappano e schiacciano coloro che ci entrano.

[^83]: Il problema è stato delineato comprensivamente nella [*Superintelligence*](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834) di Bostrom, e niente da allora ha significativamente cambiato il messaggio centrale. Per un volume più recente che raccoglie risultati formali e matematici sull'incontrollabilità vedi [AI: Unexplainable, Unpredictable, Uncontrollable](https://www.amazon.com/Unexplainable-Unpredictable-Uncontrollable-Artificial-Intelligence/dp/103257626X) di Yampolskiy

[^84]: Questo chiarisce anche perché la strategia attuale delle aziende IA (lasciare iterativamente che l'IA "allinei" la prossima IA più potente) non può funzionare. Supponete che una felce, tramite la piacevolezza delle sue fronde, arrui un alunno di prima per prendersene cura. L'alunno di prima scrive alcune istruzioni dettagliate per un alunno di seconda da seguire, e una nota convincendolo a farlo. L'alunno di seconda fa lo stesso per un alunno di terza, e così via tutto il tragitto a un laureato, un manager, un dirigente, e infine il CEO di GM. GM allora "farà quello che vuole la felce"? A ogni passo questo potrebbe sembrare che funzioni. Ma mettendolo tutto insieme, funzionerà quasi esattamente nella misura in cui il CEO, il Consiglio e gli azionisti di GM si trovano a tenere ai bambini e alle felci, e avere poco o niente a che fare con tutte quelle note e serie di istruzioni.

[^85]: Il carattere non è così diverso da risultati formali come il teorema di incompletezza di Gödel o l'argomento dell'arresto di Turing in quanto la nozione di controllo contraddice fondamentalmente la premessa: come potete controllare significativamente qualcosa che non potete capire o predire; eppure se poteste capire e predire la superintelligenza sareste superintelligenti. La ragione per cui dico "si avvicina" è che i risultati formali non sono così completi o verificati come nel caso della matematica pura, e perché vorrei mantenere la speranza che qualche intelligenza generale molto attentamente costruita, usando metodi totalmente diversi da quelli attualmente impiegati, potrebbe avere qualche proprietà di sicurezza matematicamente dimostrabile, per il tipo di programma IA "garantito sicuro" discusso sotto.

[^86]: Al momento, la maggior parte degli stakeholder – cioè quasi tutta l'umanità – è marginalizzata in questa discussione. Questo è profondamente sbagliato, e se non invitati dentro, i molti, molti altri gruppi che saranno affetti dallo sviluppo della IAG dovrebbero esigere di essere fatti entrare.

## Capitolo 8 - Come non costruire l'IAG

L'IAG non è inevitabile – oggi ci troviamo a un bivio. Questo capitolo presenta una proposta su come potremmo impedire che venga costruita.

Se la strada che stiamo percorrendo attualmente conduce alla probabile fine della nostra civiltà, come cambiamo direzione?

Supponiamo che il desiderio di smettere di sviluppare IAG e superintelligenza fosse diffuso e potente,[^87] perché diventa comprensione comune che l'IAG assorbirebbe potere anziché concederlo, e rappresenterebbe un pericolo profondo per la società e l'umanità. Come chiuderemmo le Porte?

Al momento conosciamo solo un modo per *creare* IA potente e generale, ovvero attraverso calcoli davvero massicci di reti neurali profonde. Poiché si tratta di operazioni incredibilmente difficili e costose da fare, c'è un senso in cui *non* farle è facile.[^88] Ma abbiamo già visto le forze che spingono verso l'IAG, e le dinamiche teorico-ludiche che rendono molto difficile per qualsiasi parte fermarsi unilateralmente. Quindi ci vorrebbe una combinazione di interventi dall'esterno (cioè i governi) per fermare le corporazioni, e accordi tra governi per fermare se stessi.[^89] Come potrebbe essere?

È utile prima distinguere tra sviluppi dell'IA che devono essere *impediti* o *proibiti*, e quelli che devono essere *gestiti*. I primi sarebbero principalmente l'escalation incontrollabile verso la superintelligenza.[^90] Per lo sviluppo proibito, le definizioni dovrebbero essere il più precise possibile, e sia la verifica che l'applicazione dovrebbero essere pratiche. Ciò che deve essere *gestito* sarebbero i sistemi di IA generali e potenti – che abbiamo già, e che avranno molte aree grigie, sfumature e complessità. Per questi, istituzioni forti ed efficaci sono cruciali.

Possiamo anche delineare utilmente questioni che devono essere affrontate a livello internazionale (incluso tra rivali o avversari geopolitici)[^91] da quelle che singole giurisdizioni, paesi, o gruppi di paesi possono gestire. Lo sviluppo proibito rientra largamente nella categoria "internazionale", perché un divieto locale sullo sviluppo di una tecnologia può generalmente essere aggirato cambiando ubicazione.[^92]

Infine, possiamo considerare gli strumenti nella cassetta degli attrezzi. Ce ne sono molti, inclusi strumenti tecnici, soft law (standard, norme, ecc.), hard law (regolamenti e requisiti), responsabilità civile, incentivi di mercato, e così via. Prestiamo attenzione speciale a uno che è particolare all'IA.

### Sicurezza e governance della capacità computazionale

Uno strumento centrale nel governare l'IA ad alta potenza sarà l'hardware che richiede. Il software prolifera facilmente, ha costi marginali di produzione quasi nulli, attraversa i confini banalmente, e può essere modificato istantaneamente; niente di questo è vero per l'hardware. Tuttavia, come abbiamo discusso, enormi quantità di questa "capacità computazionale" sono necessarie sia durante l'addestramento dei sistemi di IA che durante l'inferenza per ottenere i sistemi più capaci. La capacità computazionale può essere facilmente quantificata, contabilizzata e verificata, con relativamente poca ambiguità una volta sviluppate buone regole per farlo. Più crucialmente, grandi quantità di calcolo sono, come l'uranio arricchito, una risorsa molto scarsa, costosa e difficile da produrre. Sebbene i chip per computer siano onnipresenti, l'hardware richiesto per l'IA è costoso ed enormemente difficile da produrre.[^93]

Ciò che rende i chip specializzati per l'IA *molto più* gestibili come risorsa scarsa rispetto all'uranio è che possono includere meccanismi di sicurezza basati su hardware. La maggior parte dei telefoni cellulari moderni, e alcuni laptop, hanno caratteristiche hardware specializzate on-chip che consentono loro di assicurarsi di installare solo software e aggiornamenti del sistema operativo approvati, di conservare e proteggere dati biometrici sensibili sul dispositivo, e di poter essere resi inutili a chiunque tranne al loro proprietario se persi o rubati. Negli ultimi anni tali misure di sicurezza hardware sono diventate ben consolidate e ampiamente adottate, e generalmente si sono dimostrate abbastanza sicure.

La novità chiave di queste caratteristiche è che legano hardware e software insieme usando la crittografia.[^94] Cioè, avere semplicemente un particolare pezzo di hardware per computer non significa che un utente possa fare tutto quello che vuole con esso applicando software diverso. E questo legame fornisce anche sicurezza potente perché molti attacchi richiederebbero una violazione della sicurezza *hardware* piuttosto che solo *software*.

Diversi rapporti recenti (ad esempio da [GovAI e collaboratori](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips), e [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) hanno sottolineato che caratteristiche hardware simili incorporate nell'hardware computazionale all'avanguardia rilevante per l'IA potrebbero svolgere un ruolo estremamente utile nella sicurezza e governance dell'IA. Esse abilitano una serie di funzioni disponibili a un "governatore"[^95] che uno potrebbe non immaginare fossero disponibili o anche possibili. Come alcuni esempi chiave:

- *Geolocalizzazione*: I sistemi possono essere configurati in modo che i chip abbiano una ubicazione nota, e possano agire diversamente (o essere spenti del tutto) basandosi sull'ubicazione.[^96]
- *Connessioni autorizzate*: ogni chip può essere configurato con una lista di autorizzazione applicata via hardware di particolari altri chip con cui può collegarsi in rete, ed essere incapace di connettersi con chip non su questa lista.[^97] Questo può limitare la dimensione dei cluster comunicanti di chip.[^98]
- *Inferenza o addestramento misurati (e auto-spegnimento)*: Un governatore può concedere in licenza solo una certa quantità di addestramento o inferenza (in tempo, o FLOP, o possibilmente token) da eseguire da un utente, dopo di che è richiesta nuova autorizzazione. Se gli incrementi sono piccoli, allora è richiesta ri-autorizzazione relativamente continua di un modello. Il modello può quindi essere "spento" semplicemente negando questo segnale di licenza.[^99]
- *Limite di velocità*: Un modello è impedito dall'eseguire a velocità di inferenza superiore a un certo limite che è determinato da un governatore o altrimenti. Questo potrebbe essere implementato tramite un set limitato di connessioni autorizzate, o con mezzi più sofisticati.
- *Addestramento attestato*: Una procedura di addestramento può fornire prova crittograficamente sicura che un particolare set di codici, dati, e quantità di uso di capacità computazionale furono impiegati nella generazione del modello.

### Come non costruire superintelligenza: limiti globali sulla capacità computazionale per addestramento e inferenza

Con queste considerazioni – specialmente riguardo al calcolo – in atto, possiamo discutere come chiudere le Porte alla superintelligenza artificiale; ci rivolgeremo poi a prevenire l'IAG completa, e gestire i modelli di IA mentre si avvicinano e superano la capacità umana in aspetti diversi.

Il primo ingrediente è, naturalmente, la comprensione che la superintelligenza non sarebbe controllabile, e che le sue conseguenze sono fondamentalmente imprevedibili. Almeno la Cina e gli USA devono decidere indipendentemente, per questo o altri scopi, di non costruire superintelligenza.[^100] Poi è necessario un accordo internazionale tra loro e altri, con un forte meccanismo di verifica e applicazione, per assicurare a tutte le parti che i loro rivali non stanno defezionando e decidendo di tentare la fortuna.

Per essere verificabili e applicabili i limiti dovrebbero essere limiti fermi, e il più chiari possibile. Questo sembra un problema virtualmente impossibile: limitare le capacità di software complesso con proprietà imprevedibili, in tutto il mondo. Fortunatamente la situazione è molto migliore di così, perché la stessa cosa che ha reso possibile l'IA avanzata – un'enorme quantità di calcolo – è molto, molto più facile da controllare. Sebbene potrebbe ancora consentire alcuni sistemi potenti e pericolosi, l'*escalation incontrollabile di superintelligenza* può probabilmente essere prevenuta da un limite massimo sulla quantità di calcolo che va in una rete neurale, insieme a un limite di velocità sulla quantità di inferenza che un sistema di IA (di reti neurali connesse e altro software) può eseguire. Una versione specifica di questo è proposta sotto.

Potrebbe sembrare che porre limiti globali fermi sul calcolo dell'IA richiederebbe livelli enormi di coordinamento internazionale e sorveglianza intrusiva che distrugge la privacy. Fortunatamente, non sarebbe così. La [catena di approvvigionamento estremamente ristretta e con colli di bottiglia](https://arxiv.org/abs/2402.08797) fa sì che una volta che un limite è posto legalmente (sia per legge che per ordine esecutivo), la verifica della conformità a quel limite richiederebbe solo il coinvolgimento e la cooperazione di una manciata di grandi aziende.[^101]

Un piano come questo ha una serie di caratteristiche altamente desiderabili. È minimamente invasivo nel senso che solo poche aziende principali hanno requisiti posti su di loro, e solo cluster di calcolo abbastanza significativi sarebbero governati. I chip rilevanti contengono già le capacità hardware necessarie per una prima versione.[^102] Sia l'implementazione che l'applicazione si basano su restrizioni legali standard. Ma queste sono supportate da termini d'uso dell'hardware e da controlli hardware, semplificando vastamente l'applicazione e prevenendo l'inganno da parte di aziende, gruppi privati, o anche paesi. C'è ampio precedente per aziende hardware che pongono restrizioni remote sull'uso del loro hardware, e bloccano/sbloccano particolari capacità esternamente,[^103] incluso anche in CPU ad alta potenza nei data center.[^104] Anche per la frazione piuttosto piccola di hardware e organizzazioni coinvolte, la supervisione potrebbe essere limitata alla telemetria, senza accesso diretto ai dati o modelli stessi; e il software per questo potrebbe essere aperto all'ispezione per dimostrare che non vengono registrati dati aggiuntivi. Lo schema è internazionale e cooperativo, e abbastanza flessibile ed estensibile. Poiché il limite è principalmente sull'hardware piuttosto che sul software, è relativamente agnostico riguardo a come avviene lo sviluppo e il deployment del software di IA, ed è compatibile con una varietà di paradigmi inclusa l'IA più "decentralizzata" o "pubblica" mirata a combattere la concentrazione di potere guidata dall'IA.

Una chiusura delle Porte basata su calcolo ha anche svantaggi. Primo, è lontana dall'essere una soluzione completa al problema della governance dell'IA in generale. Secondo, man mano che l'hardware per computer diventa più veloce, il sistema "catturerebbe" sempre più hardware in cluster sempre più piccoli (o anche singole GPU).[^105] È anche possibile che a causa di miglioramenti algoritmici un limite di calcolo anche più basso sarebbe nel tempo necessario,[^106] o che la quantità di calcolo diventi largamente irrilevante e chiudere la Porta richiederebbe invece un regime di governance più dettagliato basato sul rischio o sulla capacità per l'IA. Terzo, non importa le garanzie e il piccolo numero di entità coinvolte, tale sistema è destinato a creare resistenza riguardo privacy e sorveglianza, tra altre preoccupazioni.[^107]

Naturalmente, sviluppare e implementare uno schema di governance che limita il calcolo in un periodo di tempo breve sarà abbastanza sfidante. Ma è assolutamente fattibile.

### I-A-G: La tripla-intersezione come base del rischio, e della politica

Rivolgiamoci ora all'IAG. Linee ferme e definizioni qui sono più difficili, perché certamente abbiamo intelligenza che è artificiale e generale, e per nessuna definizione esistente tutti saranno d'accordo se o quando esiste. Inoltre, un limite di calcolo o inferenza è uno strumento alquanto grezzo (il calcolo essendo un proxy per la capacità, che è poi un proxy per il rischio) che – a meno che non sia abbastanza basso – è improbabile che prevenga l'IAG abbastanza potente da causare disruzione sociale o civilizzazionale o rischi acuti.

Ho argomentato che i rischi più acuti emergono dalla tripla-intersezione di capacità molto alta, alta autonomia, e grande generalità. Questi sono i sistemi che – se vengono sviluppati affatto – devono essere gestiti con enorme cura. Creando standard rigorosi (attraverso responsabilità civile e regolamentazione) per sistemi che combinano tutte e tre le proprietà, possiamo incanalare lo sviluppo dell'IA verso alternative più sicure.

Come con altre industrie e prodotti che potrebbero potenzialmente danneggiare i consumatori o il pubblico, i sistemi di IA richiedono regolamentazione attenta da parte di agenzie governative efficaci e autorizzate. Questa regolamentazione dovrebbe riconoscere i rischi inerenti dell'IAG, e prevenire che vengano sviluppati sistemi di IA ad alta potenza inaccettabilmente rischiosi.[^108]

Tuttavia, regolamentazione su larga scala, specialmente con denti veri che sicuramente saranno opposti dall'industria,[^109] prende tempo[^110] così come convinzione politica che sia necessaria.[^111] Dato il ritmo del progresso, questo potrebbe prendere più tempo di quello che abbiamo disponibile.

Su una scala temporale molto più veloce e mentre le misure regolamentari vengono sviluppate, possiamo dare alle aziende gli incentivi necessari per (a) desistere da attività ad altissimo rischio e (b) sviluppare sistemi comprensivi per valutare e mitigare il rischio, chiarificando e aumentando i livelli di responsabilità civile per i sistemi più pericolosi. L'idea sarebbe di imporre i livelli più alti di responsabilità – stretta e in alcuni casi penale personale – per sistemi nella tripla-intersezione di alta autonomia-generalità-intelligenza, ma fornire "porti sicuri" a responsabilità più tipica basata su colpa per sistemi in cui una di quelle proprietà manca o è garantita essere gestibile. Cioè, per esempio, un sistema "debole" che è generale e autonomo (come un assistente personale capace e affidabile ma limitato) sarebbe soggetto a livelli di responsabilità più bassi. Allo stesso modo un sistema ristretto e autonomo come un'auto a guida autonoma sarebbe comunque soggetto alla regolamentazione significativa che già ha, ma non responsabilità aumentata. Similmente per un sistema altamente capace e generale che è "passivo" e largamente incapace di azione indipendente. I sistemi che mancano di *due* delle tre proprietà sono ancora più gestibili e i porti sicuri sarebbero ancora più facili da rivendicare. Questo approccio rispecchia come gestiamo altre tecnologie potenzialmente pericolose:[^112] responsabilità più alta per configurazioni più pericolose crea incentivi naturali per alternative più sicure.

Il risultato predefinito di tali alti livelli di responsabilità, che agiscono per *internalizzare* il rischio IAG alle aziende piuttosto che scaricarlo sul pubblico, è probabilmente (e sperabilmente!) per le aziende di semplicemente non sviluppare IAG completa fino a e a meno che non possano genuinamente renderla affidabile, sicura, e controllabile dato che la *loro propria leadership* sono le parti a rischio. (Nel caso questo non sia sufficiente, la legislazione che chiarifica la responsabilità dovrebbe anche permettere esplicitamente il risarcimento ingiuntivo, cioè un giudice che ordina una fermata, per attività che sono chiaramente nella zona di pericolo e discutibilmente pongono un rischio pubblico.) Man mano che la regolamentazione entra in atto, rispettare la regolamentazione può diventare il porto sicuro, e i porti sicuri da bassa autonomia, ristrettezza, o debolezza dei sistemi di IA possono convertirsi in regimi regolamentari relativamente più leggeri.

### Disposizioni chiave di una chiusura delle Porte

Con la discussione sopra in mente, questa sezione fornisce proposte per disposizioni chiave che implementerebbero e manterrebbero il divieto su IAG completa e superintelligenza, e gestione di IA competitiva a livello umano o esperto per scopi generali vicino alla soglia dell'IAG completa.[^113] Ha quattro pezzi chiave: 1) contabilità e supervisione della capacità computazionale, 2) limiti computazionali nell'addestramento e operazione dell'IA, 3) un quadro di responsabilità, e 4) standard di sicurezza e protezione a livelli definiti che includono requisiti regolamentari fermi. Questi sono descritti succintamente di seguito, con ulteriori dettagli o esempi di implementazione dati in tre tabelle accompagnatorie. Importante, notare che questi sono lontani da tutto ciò che sarà necessario per governare sistemi di IA avanzati; mentre avranno benefici aggiuntivi di sicurezza e protezione, sono mirati a chiudere la Porta all'escalation incontrollabile dell'intelligenza, e reindirizzare lo sviluppo dell'IA in una direzione migliore.

#### 1\. Contabilità della capacità computazionale, e trasparenza

- Un'organizzazione di standard (ad esempio NIST negli USA seguito da ISO/IEEE internazionalmente) dovrebbe codificare uno standard tecnico dettagliato per la capacità computazionale totale usata nell'addestrare e operare modelli di IA, in FLOP, e la velocità in FLOP/s a cui operano. Dettagli per come questo potrebbe apparire sono dati nell'Appendice A.[^114]
- Un requisito – sia per nuova legislazione che sotto autorità esistente[^115] – dovrebbe essere imposto dalle giurisdizioni in cui avviene addestramento di IA su larga scala per calcolare e riportare a un corpo regolamentario o altra agenzia i FLOP totali usati nell'addestrare e operare tutti i modelli sopra una soglia di 10<sup>25</sup> FLOP o 10<sup>18</sup> FLOP/s.[^116]
- Questi requisiti dovrebbero essere introdotti a fasi, inizialmente richiedendo stime di buona fede ben documentate su base trimestrale, con fasi successive che richiedono progressivamente standard più alti, fino a FLOP totali e FLOP/s crittograficamente attestati allegati a ogni *output* del modello.
- Questi rapporti dovrebbero essere complementati da stime ben documentate del costo energetico e finanziario marginale usato nel generare ogni output di IA.

Razionale: Questi numeri ben calcolati e riportati trasparentemente fornirebbero la base per limiti di addestramento e operazione, così come un porto sicuro da misure di responsabilità più alte (vedi Appendici C e D).

#### 2\. Limiti di capacità computazionale per addestramento e operazione

- Le giurisdizioni che ospitano sistemi di IA dovrebbero imporre un limite fermo sulla capacità computazionale totale che va in qualsiasi output di modello di IA, iniziando a 10<sup>27</sup> FLOP[^117] e aggiustabile come appropriato.
- Le giurisdizioni che ospitano sistemi di IA dovrebbero imporre un limite fermo sulla velocità di calcolo degli output di modelli di IA, iniziando a 10<sup>20</sup> FLOP/s e aggiustabile come appropriato.

Razionale: La capacità computazionale totale, mentre molto imperfetta, è un proxy per la capacità dell'IA (e il rischio) che è concretamente misurabile e verificabile, quindi fornisce una barriera ferma per limitare le capacità. Una proposta di implementazione concreta è data nell'Appendice B.

#### 3\. Responsabilità aumentata per sistemi pericolosi

- La creazione e operazione[^118] di un sistema di IA avanzato che è altamente generale, capace, e autonomo, dovrebbe essere chiarita legalmente via legislazione per essere soggetta a responsabilità stretta, solidale, piuttosto che basata su colpa di singola parte.[^119]
- Un processo legale dovrebbe essere disponibile per fare casi di sicurezza affermativi, che concederebbero porto sicuro dalla responsabilità stretta per sistemi che sono piccoli (in termini di calcolo), deboli, ristretti, passivi, o che hanno sufficienti garanzie di sicurezza, protezione, e controllabilità.
- Un percorso esplicito e set di condizioni per il risarcimento ingiuntivo per fermare attività di addestramento e inferenza dell'IA che costituiscono un pericolo pubblico dovrebbe essere delineato.

Razionale: I sistemi di IA non possono essere ritenuti responsabili, quindi dobbiamo ritenere individui umani e organizzazioni responsabili per il danno che causano (responsabilità).[^120] L'IAG incontrollabile è una minaccia per la società e la civiltà e in assenza di un caso di sicurezza dovrebbe essere considerata anormalmente pericolosa. Mettere il carico di responsabilità sugli sviluppatori di mostrare che i modelli potenti sono abbastanza sicuri da non essere considerati "anormalmente pericolosi" incentiva lo sviluppo sicuro, insieme a trasparenza e tenuta di registri per rivendicare quei porti sicuri. La regolamentazione può poi prevenire danno dove la deterrenza dalla responsabilità è insufficiente. Infine, gli sviluppatori di IA sono già responsabili per i danni che causano, quindi chiarire legalmente la responsabilità per i sistemi più rischiosi può essere fatto immediatamente, senza che standard altamente dettagliati vengano sviluppati; questi possono poi svilupparsi nel tempo. I dettagli sono dati nell'Appendice C.

#### 4\. Regolamentazione di sicurezza per l'IA

Un sistema regolamentario che affronta rischi acuti su larga scala dell'IA richiederà al minimo:

- L'identificazione o creazione di un set appropriato di corpi regolamentari, probabilmente una nuova agenzia;
- Un quadro comprensivo di valutazione del rischio;[^121]
- Un quadro per casi di sicurezza affermativi, basato in parte sul quadro di valutazione del rischio, da essere fatti dagli sviluppatori, e per controllo da parte di gruppi e agenzie *indipendenti*;
- Un sistema di licenze a livelli, con livelli che tracciano livelli di capacità.[^122] Le licenze sarebbero concesse sulla base di casi di sicurezza e controlli, per sviluppo e deployment di sistemi. I requisiti andrebbero dalla notifica al livello basso, a garanzie quantitative di sicurezza, protezione, e controllabilità prima dello sviluppo, al livello più alto. Questi preverebbero il rilascio di sistemi fino a che non sono dimostrati sicuri, e proibirebbero lo sviluppo di sistemi intrinsecamente non sicuri. L'Appendice D fornisce una proposta per ciò che tali standard di sicurezza e protezione potrebbero comportare.
- Accordi per portare tali misure a livello internazionale, inclusi corpi internazionali per armonizzare norme e standard, e potenzialmente agenzie internazionali per rivedere casi di sicurezza.

Razionale: Alla fine, la responsabilità non è il meccanismo giusto per prevenire rischio su larga scala al pubblico da una nuova tecnologia. Regolamentazione comprensiva, con corpi regolamentari autorizzati, sarà necessaria per l'IA proprio come per ogni altra industria principale che pone un rischio al pubblico.[^123]

La regolamentazione verso la prevenzione di altri rischi pervasivi ma meno acuti è probabile che vari nella sua forma da giurisdizione a giurisdizione. La cosa cruciale è evitare di sviluppare i sistemi di IA che sono così rischiosi che questi rischi sono ingestibili.

### E poi?

Nel prossimo decennio, man mano che l'IA diventa più pervasiva e la tecnologia core avanza, due cose chiave sono probabilmente destinate ad accadere. Primo, la regolamentazione dei sistemi di IA potenti esistenti diventerà più difficile, tuttavia ancora più necessaria. È probabile che almeno alcune misure che affrontano rischi di sicurezza su larga scala richiederanno accordo a livello internazionale, con giurisdizioni individuali che applicano regole basate su accordi internazionali.

Secondo, i limiti di capacità computazionale per addestramento e operazione diventeranno più difficili da mantenere man mano che l'hardware diventa più economico e più efficiente in termini di costi; potrebbero anche diventare meno rilevanti (o aver bisogno di essere anche più stretti) con avanzamenti in algoritmi e architetture.

Che controllare l'IA diventerà più difficile non significa che dovremmo arrenderci! Implementare il piano delineato in questo saggio ci darebbe sia tempo prezioso che controllo cruciale sul processo che ci metterebbe in una posizione molto, molto migliore per evitare il rischio esistenziale dell'IA per la nostra società, civiltà, e specie.

Nel termine ancora più lungo, ci saranno scelte da fare riguardo a cosa permettiamo. Possiamo scegliere ancora di creare qualche forma di IAG genuinamente controllabile, nel grado in cui questo si dimostri possibile. O possiamo decidere che gestire il mondo è meglio lasciarlo alle macchine, se possiamo convincerci che faranno un lavoro migliore, e ci tratteranno bene. Ma queste dovrebbero essere decisioni prese con profonda comprensione scientifica dell'IA in mano, e dopo discussione globale inclusiva significativa, non in una corsa tra magnati della tecnologia con la maggior parte dell'umanità completamente non coinvolta e inconsapevole.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) Riassunto della governance di I-A-G e superintelligenza tramite responsabilità e regolamentazione. La responsabilità è più alta, e la regolamentazione più forte, alla tripla-intersezione di Autonomia, Generalità, e Intelligenza. Porti sicuri dalla responsabilità stretta e regolamentazione forte possono essere ottenuti tramite casi di sicurezza affermativi che dimostrano che un sistema è debole e/o ristretto e/o passivo. Limiti sulla Capacità Computazionale Totale di Addestramento e sulla velocità di Capacità Computazionale di Inferenza, verificati e applicati legalmente e usando misure di sicurezza hardware e crittografiche, sostengono la sicurezza evitando l'IAG completa e proibendo efficacemente la superintelligenza.

[^87]: Molto probabilmente, la diffusione di questa realizzazione prenderà o sforzo intenso da parte di gruppi di educazione e advocacy che fanno questo caso, o un disastro causato dall'IA abbastanza significativo. Possiamo sperare che sia il primo.

[^88]: Paradossalmente, siamo abituati che la Natura limiti la nostra tecnologia rendendola molto difficile da sviluppare, specialmente scientificamente. Ma questo non è più il caso per l'IA: i problemi scientifici chiave stanno risultando più facili del previsto. Non possiamo contare sulla Natura che ci salvi da noi stessi qui – dovremo farlo noi.

[^89]: Dove, esattamente, ci fermiamo nello sviluppare nuovi sistemi? Qui, dovremmo adottare un principio precauzionale. Una volta che un sistema è deployato, e specialmente una volta che quel livello di capacità del sistema prolifera, è estremamente difficile fare marcia indietro. E se un sistema è *sviluppato* (specialmente a grande costo e sforzo), ci sarà enorme pressione per usarlo o deployarlo, e tentazione per esso di essere fatto trapelare o rubato. Sviluppare sistemi e *poi* decidere se sono profondamente non sicuri è una strada pericolosa.

[^90]: Sarebbe anche saggio proibire sviluppo di IA che è intrinsecamente pericoloso, come sistemi auto-replicanti ed evolventi, quelli progettati per sfuggire alla reclusione, quelli che possono auto-migliorarsi autonomamente, IA deliberatamente ingannevole e maliziosa, ecc.

[^91]: Notare questo non significa necessariamente *applicato* a livello internazionale da qualche tipo di corpo globale: invece nazioni sovrane potrebbero applicare regole concordate, come in molti trattati.

[^92]: Come vedremo sotto, la natura del calcolo dell'IA permetterebbe qualcosa di un ibrido; ma la cooperazione internazionale sarà comunque necessaria.

[^93]: Per esempio, le macchine richieste per incidere chip rilevanti per l'IA sono fatte solo da una ditta, ASML (nonostante molti altri tentativi di farlo), la vasta maggioranza dei chip rilevanti sono prodotti da una ditta, TSMC (nonostante altri tentino di competere), e il design e costruzione di hardware da quei chip fatto solo da pochi inclusi NVIDIA, AMD, e Google.

[^94]: Più importante, ogni chip tiene una chiave privata crittografica unica e inaccessibile che può usare per "firmare" cose.

[^95]: Per impostazione predefinita questo sarebbe l'azienda che vende i chip, ma altri modelli sono possibili e potenzialmente utili.

[^96]: Un governatore può accertare l'ubicazione di un chip cronometrando lo scambio di messaggi firmati con esso: la velocità finita della luce richiede che il chip sia entro un dato raggio *r* di una "stazione" se può restituire un messaggio firmato in un tempo meno di *r* / *c*, dove *c* è la velocità della luce. Usando stazioni multiple, e qualche comprensione delle caratteristiche di rete, l'ubicazione del chip può essere determinata. La bellezza di questo metodo è che la maggior parte della sua sicurezza è fornita dalle leggi della fisica. Altri metodi potrebbero usare GPS, tracciamento inerziale, e tecnologie simili.

[^97]: Alternativamente, coppie di chip potrebbero essere permesse di comunicare tra loro solo via permesso esplicito di un governatore.

[^98]: Questo è cruciale perché almeno attualmente, connessione ad alta larghezza di banda tra chip è necessaria per addestrare grandi modelli di IA su di essi.

[^99]: Questo potrebbe anche essere configurato per richiedere messaggi firmati da *N* di *M* governatori diversi, permettendo a parti multiple di condividere la governance.

[^100]: Questo è tutt'altro che senza precedenti – per esempio i militari non hanno sviluppato eserciti di supersoldati clonati o geneticamente modificati, sebbene questo sia probabilmente tecnologicamente possibile. Ma hanno *scelto* di non farlo, piuttosto che essere prevenuti da altri. Il record non è eccellente per potenze mondiali principali che vengono prevenute dallo sviluppare una tecnologia che desiderano fortemente sviluppare.

[^101]: Con un paio di eccezioni notevoli (in particolare NVIDIA) l'hardware specializzato per l'IA è una parte relativamente piccola del modello di business e ricavi complessivi di queste aziende. Inoltre, il divario tra hardware usato nell'IA avanzata e hardware "grado consumatore" è significativo, quindi la maggior parte dei consumatori di hardware per computer sarebbero largamente non influenzati.

[^102]: Per analisi più dettagliata, vedere i rapporti recenti da [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) e [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). Questi si concentrano sulla fattibilità tecnica, specialmente nel contesto dei controlli di esportazione USA che cercano di limitare la capacità di altri paesi nel calcolo di alta gamma; ma questo ha sovrapposizione ovvia con il vincolo globale immaginato qui.

[^103]: I dispositivi Apple, per esempio, sono bloccati remotamente e sicuramente quando riportati persi o rubati, e possono essere ri-attivati remotamente. Questo si basa sulle stesse caratteristiche di sicurezza hardware discusse qui.

[^104]: Vedere ad esempio l'offerta [capacity on demand](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) di IBM, [Intel on demand](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html) di Intel, e [private cloud compute](https://security.apple.com/blog/private-cloud-compute/) di Apple.

[^105]: [Questo studio](https://epochai.org/trends#hardware-trends-section) mostra che storicamente la stessa performance è stata ottenuta usando circa 30% meno dollari per anno. Se questa tendenza continua, ci potrebbe essere sovrapposizione significativa tra uso di chip IA e "consumatore", e in generale la quantità di hardware necessario per sistemi di IA ad alta potenza potrebbe diventare scomodamente piccola.

[^106]: Per lo [stesso studio](https://epochai.org/trends#hardware-trends-section), data performance sul riconoscimento di immagini ha richiesto 2.5x meno calcolo ogni anno. Se questo dovesse anche valere per i sistemi di IA più capaci, un limite di calcolo non sarebbe utile per molto.

[^107]: In particolare, a livello di paese questo assomiglia molto a una nazionalizzazione del calcolo, in quanto il governo avrebbe molto controllo su come viene usata la potenza computazionale. Tuttavia, per quelli preoccupati del coinvolgimento del governo, questo sembra molto più sicuro e preferibile al software di IA più potente *stesso* che viene nazionalizzato tramite qualche fusione tra aziende di IA principali e governi nazionali, come alcuni stanno iniziando ad advocare.

[^108]: Un passo regolamentario principale in Europa è stato preso con il passaggio nel 2024 dell'[EU AI Act](https://artificialintelligenceact.eu/). Classifica l'IA per rischio: proibendo sistemi inaccettabili, regolamentando quelli ad alto rischio, e imponendo regole di trasparenza, o nessuna misura affatto, su sistemi a basso rischio. Ridurrà significativamente alcuni rischi dell'IA, e aumenterà la trasparenza dell'IA anche per ditte USA, ma ha due difetti chiave. Primo, portata limitata: mentre si applica a qualsiasi azienda che fornisce IA nell'UE, l'applicazione su ditte con base USA è debole, e l'IA militare è esente. Secondo, mentre copre GPAI, fallisce nel riconoscere IAG o superintelligenza come rischi inaccettabili o prevenire il loro sviluppo—solo il loro deployment nell'UE. Di conseguenza, fa poco per frenare i rischi di IAG o superintelligenza.

[^109]: Le aziende spesso rappresentano che sono a favore di regolamentazione ragionevole. Ma in qualche modo sembrano quasi sempre opporsi a qualsiasi regolamentazione *particolare*; testimone la lotta sulla SB1047 abbastanza leggera, che [la maggior parte delle aziende di IA si sono opposte pubblicamente o privatamente](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/).

[^110]: Sono stati circa 3 anni e mezzo dal momento in cui l'EU AI act è stato proposto fino a quando è entrato in vigore.

[^111]: È a volte espresso che è "troppo presto" per iniziare a regolamentare l'IA. Data la nota precedente, questo sembra difficilmente probabile. Un'altra preoccupazione espressa è che la regolamentazione "danneggerebbe l'innovazione". Ma la buona regolamentazione cambia solo la direzione, non la quantità, di innovazione.

[^112]: Un precedente interessante è nel trasporto di materiali pericolosi, che potrebbero sfuggire e causare danno. Qui, [regolamentazione](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) e [giurisprudenza](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) hanno stabilito responsabilità stretta per materiali molto pericolosi come esplosivi, benzina, veleni, agenti infettivi, e rifiuti radioattivi. Altri esempi includono [avvertenze sui farmaci](https://www.medicalnewstoday.com/articles/boxed-warnings), [classi di dispositivi medici](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification), ecc.

[^113]: Un'altra proposta comprensiva con obiettivi simili messa avanti in ["A Narrow Path"](https://www.narrowpath.co/) advocata per un approccio più centralizzato, basato su proibizione che incanala tutto lo sviluppo di IA di frontiera attraverso una singola entità internazionale, supervisionata da forti istituzioni internazionali, con proibizioni categoriche chiare piuttosto che restrizioni graduate. Approverei anche quel piano; tuttavia prenderà ancora più volontà politica e coordinamento di quello proposto qui.

[^114]: Alcune linee guida per tale standard sono state [pubblicate](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/) dal Frontier Model Forum. Relative alla proposta qui, quelle errano sul lato di meno precisione e meno calcolo incluso nel conteggio.

[^115]: L'ordine esecutivo AI USA del 2023 (ora rescisso) richiedeva rapporto simile ma meno dettagliato. Questo dovrebbe essere rafforzato da un ordine sostitutivo.

[^116]: Molto approssimativamente, per i chip H100 ora comuni questo corrisponde a cluster di circa 1000 che fanno inferenza; sono circa 100 (circa 5 milioni di USD di valore) dei chip NVIDIA B200 nuovissimi top di gamma che fanno inferenza. In entrambi i casi il numero di addestramento corrisponde a quel cluster che calcola per diversi mesi.

[^117]: Questa quantità è più grande di qualsiasi sistema di IA attualmente addestrato; un numero più grande o più piccolo potrebbe essere giustificato man mano che capiamo meglio come la capacità dell'IA scala con il calcolo.

[^118]: Questo si applica a quelli che creano e forniscono/ospitano i modelli, non agli utenti finali.

[^119]: Approssimativamente, responsabilità "stretta" significa che gli sviluppatori sono ritenuti responsabili per danni fatti da un prodotto *per impostazione predefinita* ed è uno standard usato per prodotti "anormalmente pericolosi", e (alquanto divertente ma appropriato) animali selvatici. Responsabilità "solidale" significa che la responsabilità è assegnata a tutte le parti responsabili per un prodotto, e quelle parti devono sistemare tra loro chi porta quale responsabilità. Questo è importante per sistemi come l'IA con una catena del valore lunga e complessa.

[^120]: La responsabilità standard basata su colpa di singola parte non è abbastanza: la colpa sarà sia difficile da tracciare e assegnare perché i sistemi di IA sono complessi, la loro operazione non è compresa, e molte parti potrebbero essere coinvolte nella creazione di un sistema o output pericoloso. In aggiunta, le cause legali prenderanno anni per essere giudicate e probabilmente risulteranno meramente in multe che sono conseguenti per queste aziende, quindi la responsabilità personale per i dirigenti è importante anche.

[^121]: Non dovrebbe esserci esenzione dai criteri di sicurezza per modelli open-weight. Inoltre, nel valutare il rischio si dovrebbe assumere che protezioni che possono essere rimosse saranno rimosse da modelli ampiamente disponibili, e che anche modelli chiusi prolifereranno a meno che non ci sia un'assicurazione molto alta che rimarranno sicuri.

[^122]: Lo schema proposto qui ha scrutinio regolamentario innescato sulla capacità generale; tuttavia ha senso per alcuni casi d'uso specialmente rischiosi innescare più scrutinio – per esempio un sistema di IA esperto in virologia, anche se ristretto e passivo, dovrebbe probabilmente andare in un livello più alto. Il precedente ordine esecutivo USA aveva qualche di questa struttura per capacità biologiche.

[^123]: Due esempi chiari sono aviazione e medicine, regolamentate dalla FAA e FDA, e agenzie simili in altri paesi. Queste agenzie sono imperfette, ma sono state assolutamente vitali per il funzionamento e successo di quelle industrie.

## Capitolo 9 - Ingegnerizzare il futuro — cosa dovremmo fare invece

L'IA può fare un bene incredibile nel mondo. Per ottenere tutti i benefici senza i rischi, dobbiamo assicurarci che l'IA rimanga uno strumento umano.

Se riusciamo a scegliere di non sostituire l'umanità con le macchine – almeno per un po'! – cosa possiamo fare invece? Rinunciamo all'enorme promessa dell'IA come tecnologia? A un certo livello la risposta è un semplice *no:* chiudiamo le Porte all'IAG incontrollabile e alla superintelligenza, ma *costruiamo* molte altre forme di IA, insieme alle strutture di governance e alle istituzioni di cui avremo bisogno per gestirle.

Ma c'è ancora molto da dire; realizzare tutto questo sarebbe un'occupazione centrale dell'umanità. Questa sezione esplora diversi temi chiave:

- Come possiamo caratterizzare l'IA "Strumentale" e le forme che può assumere.
- Che possiamo ottenere (quasi) tutto quello che l'umanità desidera senza IAG, con l'IA Strumentale.
- Che i sistemi di IA Strumentale sono (probabilmente, in linea di principio) gestibili.
- Che allontanarsi dall'IAG non significa compromettere la sicurezza nazionale – anzi, tutto il contrario.
- Che la concentrazione del potere è una preoccupazione reale. Possiamo mitigarla senza compromettere sicurezza e protezione?
- Che vorremo – e avremo bisogno di – nuove strutture di governance e sociali, e l'IA può effettivamente aiutare.

### IA dentro le Porte: IA Strumentale

Il diagramma della tripla-intersezione offre un buon modo per delineare quello che possiamo chiamare "IA Strumentale": IA che è uno strumento controllabile per l'uso umano, piuttosto che un rivale o un sostituto incontrollabile. I sistemi di IA meno problematici sono quelli che sono autonomi ma non generali o super capaci (come un bot per aste automatiche), o generali ma non autonomi o capaci (come un modello linguistico piccolo), o capaci ma specifici e molto controllabili (come AlphaGo).[^124] Quelli con due caratteristiche che si intersecano hanno un'applicazione più ampia ma un rischio più alto e richiederanno sforzi considerevoli per essere gestiti. (Il fatto che un sistema di IA sia più strumentale non significa che sia intrinsecamente sicuro, semplicemente che non è intrinsecamente *non sicuro* – si pensi a una motosega, contro una tigre domestica.) La Porta deve rimanere chiusa alla IAG (completa) e alla superintelligenza alla tripla intersezione, e bisogna prestare enorme attenzione ai sistemi di IA che si avvicinano a quella soglia.

Ma questo lascia spazio a molta IA potente! Possiamo ottenere un'enorme utilità da "oracoli" passivi intelligenti e generali e da sistemi specifici, sistemi generali a livello umano ma non sovrumano, e così via. Molte aziende tecnologiche e sviluppatori stanno attivamente costruendo questi tipi di strumenti e dovrebbero continuare; come la maggior parte delle persone, stanno implicitamente *assumendo* che le Porte alla IAG e alla superintelligenza rimarranno chiuse.[^125]

Inoltre, i sistemi di IA possono essere efficacemente combinati in sistemi compositi che mantengono la supervisione umana mentre potenziano le capacità. Piuttosto che affidarsi a scatole nere imperscrutabili, possiamo costruire sistemi dove più componenti – inclusi sia IA che software tradizionali – lavorano insieme in modi che gli umani possono monitorare e comprendere.[^126] Mentre alcuni componenti potrebbero essere scatole nere, nessuno sarebbe vicino all'IAG – solo il sistema composito nel suo insieme sarebbe sia altamente generale che altamente capace, e in modo rigorosamente controllabile.[^127]

#### Controllo umano significativo e garantito

Cosa significa "rigorosamente controllabile"? Un'idea chiave del framework "Strumentale" è permettere sistemi – anche se abbastanza generali e potenti – che sono garantiti essere sotto controllo umano significativo. Cosa significa questo? Comporta due aspetti. Il primo è una considerazione progettuale: gli umani dovrebbero essere profondamente e centralmente coinvolti in quello che il sistema sta facendo, *senza* delegare decisioni chiave importanti all'IA. Questo è il carattere della maggior parte dei sistemi di IA attuali. Secondo, nella misura in cui i sistemi di IA sono autonomi, devono avere garanzie che limitano la loro portata d'azione. Una garanzia dovrebbe essere un *numero* che caratterizza la probabilità che qualcosa accada, e una ragione per credere a quel numero. Questo è quello che richiediamo in altri campi critici per la sicurezza, dove numeri come "tempo medio tra i guasti" e numeri attesi di incidenti vengono calcolati, supportati e pubblicati nelle analisi di sicurezza.[^128] Il numero ideale per i guasti è zero, ovviamente. E la buona notizia è che potremmo avvicinarci parecchio, anche se usando architetture di IA abbastanza diverse, usando idee di proprietà *formalmente verificate* dei programmi (inclusa l'IA). L'idea, esplorata a lungo da Omohundro, Tegmark, Bengio, Dalrymple e altri (vedi [qui](https://arxiv.org/abs/2309.01933) e [qui](https://arxiv.org/abs/2405.06624)) è costruire un programma con certe proprietà (per esempio: che un umano può spegnerlo) e *dimostrare* formalmente che quelle proprietà valgono. Questo può essere fatto ora per programmi abbastanza brevi e proprietà semplici, ma il (futuro) potere del software di dimostrazione potenziato dall'IA potrebbe permetterlo per programmi molto più complessi (ad esempio wrapper) e persino l'IA stessa. Questo è un programma molto ambizioso, ma mentre cresce la pressione sulle Porte, avremo bisogno di alcuni materiali potenti per rinforzarle. La dimostrazione matematica potrebbe essere uno dei pochi abbastanza forte.

#### Il destino dell'industria dell'IA

Con il progresso dell'IA reindirizzato, l'IA Strumentale sarebbe ancora un'industria enorme. In termini di hardware, anche con limiti computazionali per prevenire la superintelligenza, l'addestramento e l'inferenza in modelli più piccoli richiederanno ancora enormi quantità di componenti specializzati. Sul lato software, disinnescare l'esplosione nelle dimensioni dei modelli di IA e della computazione dovrebbe semplicemente portare le aziende a reindirizzare le risorse verso il miglioramento dei sistemi più piccoli, rendendoli migliori, più diversi e più specializzati, piuttosto che semplicemente renderli più grandi.[^129] Ci sarebbe molto spazio – probabilmente di più – per tutte quelle startup della Silicon Valley che fanno soldi.[^130]

### L'IA Strumentale può produrre (quasi) tutto quello che l'umanità desidera, senza IAG

L'intelligenza, sia biologica che artificiale, può essere considerata in senso lato come la capacità di pianificare ed eseguire attività che portano a futuri più in linea con una serie di obiettivi. Come tale, l'intelligenza è di enorme beneficio quando usata per perseguire obiettivi saggiamente scelti. L'intelligenza artificiale sta attraendo enormi investimenti di tempo e sforzo principalmente a causa dei suoi benefici promessi. Quindi dovremmo chiederci: in che misura otterremmo ancora i benefici dell'IA se conteniamo la sua escalation incontrollabile verso la superintelligenza? La risposta: potremmo perdere sorprendentemente poco.

Consideriamo innanzitutto che i sistemi di IA attuali sono già molto potenti, e abbiamo davvero solo scalfito la superficie di quello che si può fare con essi.[^131] Sono ragionevolmente capaci di "gestire la situazione" in termini di "comprensione" di una domanda o compito presentato loro, e quello che servirebbe per rispondere a questa domanda o fare quel compito.

Inoltre, gran parte dell'entusiasmo sui sistemi di IA moderni è dovuto alla loro generalità; ma alcuni dei sistemi di IA più capaci – come quelli che generano o riconoscono voce o immagini, fanno previsioni e modellazione scientifica, giocano, ecc. – sono molto più specifici e ben "dentro le Porte" in termini di computazione.[^132] Questi sistemi sono sovrumani nei compiti particolari che fanno. Potrebbero avere debolezze nei casi limite[^133] (o [sfruttabili](https://arxiv.org/abs/2211.00241)) dovute alla loro specificità; tuttavia *totalmente* specifici o *completamente* generali non sono le uniche opzioni disponibili: ci sono molte architetture intermedie.[^134]

Questi strumenti di IA possono accelerare notevolmente l'avanzamento in altre tecnologie positive, senza IAG. Per fare meglio la fisica nucleare, non abbiamo bisogno che l'IA sia un fisico nucleare – ne abbiamo! Se vogliamo accelerare la medicina, diamo ai biologi, ai ricercatori medici e ai chimici strumenti potenti. Li vogliono e li useranno con enorme vantaggio. Non abbiamo bisogno di una server farm piena di un milione di geni digitali; abbiamo milioni di umani il cui genio l'IA può aiutare a far emergere. Sì, ci vorrà più tempo per ottenere l'immortalità e la cura per tutte le malattie. Questo è un costo reale. Ma anche le innovazioni sanitarie più promettenti sarebbero di poca utilità se l'instabilità guidata dall'IA portasse a conflitti globali o al collasso sociale. Dobbiamo dare agli umani potenziati dall'IA una possibilità di affrontare prima il problema.

E supponiamo che ci sia, di fatto, qualche enorme vantaggio dell'IAG che non può essere ottenuto dall'umanità usando strumenti dentro-Porta. Li perdiamo *non* costruendo mai IAG e superintelligenza? Nel valutare rischi e benefici qui, c'è un enorme beneficio asimmetrico nell'aspettare piuttosto che affrettarsi: possiamo aspettare finché non può essere fatto in modo garantito sicuro e benefico, e quasi tutti potranno ancora raccogliere i frutti; se ci affrettiamo, potrebbe essere – nelle parole del CEO di OpenAI Sam Altman – [luci spente per *tutti* noi.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

Ma se gli strumenti non-IAG sono potenzialmente così potenti, possiamo gestirli? La risposta è un chiaro...forse.

### I sistemi di IA Strumentale sono (probabilmente, in linea di principio) gestibili

Ma non sarà facile. I sistemi di IA all'avanguardia attuali possono potenziare notevolmente persone e istituzioni nel raggiungere i loro obiettivi. Questo è, in generale, una cosa buona! Tuttavia, ci sono dinamiche naturali nell'avere tali sistemi a nostra disposizione – improvvisamente e senza molto tempo per la società di adattarsi – che offrono seri rischi che devono essere gestiti. Vale la pena discutere alcune classi principali di tali rischi, e come possano essere diminuiti, assumendo una chiusura delle Porte.

Una classe di rischi è dell'IA Strumentale ad alta potenza che permette l'accesso a conoscenza o capacità che erano precedentemente legate a una persona o organizzazione, rendendo disponibile una combinazione di alta capacità più alta lealtà a una gamma molto ampia di attori. Oggi, con abbastanza soldi una persona con cattive intenzioni potrebbe assumere un team di chimici per progettare e produrre nuove armi chimiche – ma non è così facile avere quei soldi o trovare/assemblare il team e convincerli a fare qualcosa chiaramente illegale, non etico e pericoloso. Per prevenire che i sistemi di IA giochino un tale ruolo, miglioramenti sui metodi attuali potrebbero bastare,[^135] purché tutti quei sistemi e l'accesso ad essi siano gestiti responsabilmente. D'altra parte, se sistemi potenti vengono rilasciati per uso generale e modifiche, qualsiasi misura di sicurezza incorporata è probabilmente rimovibile. Quindi per evitare rischi in questa classe, saranno richieste forti restrizioni su quello che può essere rilasciato pubblicamente – analoghe alle restrizioni sui dettagli delle tecnologie nucleari, esplosive e altre pericolose.[^136]

Una seconda classe di rischi deriva dal potenziamento di macchine che agiscono come o impersonano persone. A livello di danno alle persone individuali, questi rischi includono truffe, spam e phishing molto più efficaci, e la proliferazione di deepfake non consensuali.[^137] A livello collettivo, includono la disruzione di processi sociali fondamentali come la discussione e il dibattito pubblico, i nostri sistemi sociali di raccolta, elaborazione e diffusione di informazioni e conoscenza, e i nostri sistemi di scelta politica. Mitigare questo rischio probabilmente comporterà (a) leggi che limitano l'impersonificazione di persone da parte di sistemi di IA, e che ritengono responsabili gli sviluppatori di IA che creano sistemi che generano tali impersonificazioni, (b) sistemi di watermarking e provenienza che identificano e classificano (responsabilmente) il contenuto generato dall'IA, e (c) nuovi sistemi epistemici socio-tecnici che possono creare una catena fidata dai dati (ad esempio telecamere e registrazioni) attraverso fatti, comprensione e buoni modelli del mondo.[^138] Tutto questo è possibile, e l'IA può aiutare con alcune parti di esso.

Un terzo rischio generale è che nella misura in cui alcuni compiti sono automatizzati, gli umani che attualmente fanno quei compiti possono avere meno valore finanziario come forza lavoro. Storicamente, automatizzare i compiti ha reso le cose abilitate da quei compiti più economiche e abbondanti, mentre ha diviso le persone che precedentemente facevano quei compiti in quelli ancora coinvolti nella versione automatizzata (generalmente a skill/paga più alta), e quelli il cui lavoro vale meno o poco. Complessivamente è difficile predire in quali settori sarà richiesto più versus meno lavoro umano nel settore risultante più grande ma più efficiente. In parallelo, la dinamica dell'automazione tende ad aumentare la disuguaglianza e la produttività generale, diminuire il costo di certi beni e servizi (via aumenti di efficienza), e aumentare il costo di altri (via [malattia dei costi](https://en.wikipedia.org/wiki/Baumol_effect)). Per quelli sul lato sfavorito dell'aumento della disuguaglianza, è profondamente poco chiaro se la diminuzione di costo in certi beni e servizi superi l'aumento negli altri, e porti a maggiore benessere complessivo. Quindi come andrà per l'IA? A causa della relativa facilità con cui il lavoro intellettuale umano può essere sostituito dall'IA generale, possiamo aspettarci una versione rapida di questo con IA generale competitiva con l'umano.[^139] Se chiudiamo la Porta all'IAG, molti meno lavori saranno sostituiti all'ingrosso da agenti di IA; ma un enorme spostamento lavorativo è ancora probabile in un periodo di anni.[^140] Per evitare sofferenza economica diffusa, sarà probabilmente necessario implementare sia qualche forma di beni di base universali o reddito, sia anche ingegnerizzare uno spostamento culturale verso il valorizzare e ricompensare il lavoro umano-centrico che è più difficile da automatizzare (piuttosto che vedere i prezzi del lavoro scendere a causa dell'aumento del lavoro disponibile spinto fuori da altre parti dell'economia.) Altri costrutti, come quello della ["dignità dei dati"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society) (in cui i produttori umani di dati di addestramento ricevono automaticamente royalty per il valore creato da quei dati nell'IA) possono aiutare. L'automazione tramite IA ha anche un secondo potenziale effetto avverso, che è dell'automazione *inappropriata*. Insieme alle applicazioni dove l'IA semplicemente fa un lavoro peggiore, questo includerebbe quelle dove i sistemi di IA probabilmente violerebbero precetti morali, etici o legali – per esempio nelle decisioni di vita e morte, e in questioni giudiziarie. Questi devono essere trattati applicando ed estendendo i nostri framework legali attuali.

Infine, una minaccia significativa dell'IA dentro-porta è il suo uso nella persuasione personalizzata, cattura dell'attenzione e manipolazione. Abbiamo visto nei social media e altre piattaforme online la crescita di un'economia dell'attenzione profondamente radicata (dove i servizi online combattono ferocemente per l'attenzione degli utenti) e sistemi di ["capitalismo di sorveglianza"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism) (in cui informazioni e profilazione degli utenti si aggiungono alla mercificazione dell'attenzione.) È quasi certo che più IA sarà messa al servizio di entrambi. L'IA è già pesantemente usata negli algoritmi di feed che creano dipendenza, ma questo evolverà in contenuto generato dall'IA che crea dipendenza, personalizzato per essere consumato compulsivamente da una singola persona. E l'input, le risposte e i dati di quella persona saranno alimentati nella macchina dell'attenzione/pubblicità per continuare il circolo vizioso. Inoltre, quando gli assistenti IA forniti dalle aziende tecnologiche diventano l'interfaccia per più vita online, probabilmente sostituiranno i motori di ricerca e i feed come meccanismo attraverso cui avviene la persuasione e la monetizzazione dei clienti. Il fallimento della nostra società nel controllare queste dinamiche finora non è di buon auspicio. Parte di questa dinamica potrebbe essere diminuita via regolamenti riguardo privacy, diritti sui dati e manipolazione. Arrivare più alla radice del problema potrebbe richiedere prospettive diverse, come quella degli assistenti IA leali (discussa sotto.)

Il risultato di questa discussione è di speranza: i sistemi basati su strumenti dentro-Porta – almeno finché restano comparabili in potenza e capacità ai sistemi all'avanguardia di oggi – sono probabilmente gestibili se c'è volontà e coordinamento per farlo. Istituzioni umane decenti, potenziate da strumenti di IA,[^141] possono farlo. Potremmo anche fallire nel farlo. Ma è difficile vedere come permettere sistemi più potenti aiuterebbe – altro che metterli al comando e sperare per il meglio.

### Sicurezza nazionale

Le corse per la supremazia dell'IA – guidate dalla sicurezza nazionale o altre motivazioni – ci spingono verso sistemi di IA potenti e incontrollati che tenderebbero ad assorbire, piuttosto che conferire, potere. Una corsa all'IAG tra USA e Cina è una corsa per determinare quale nazione ottiene prima la superintelligenza.

Quindi cosa dovrebbero fare invece quelli incaricati della sicurezza nazionale? I governi hanno forte esperienza nel costruire sistemi controllabili e sicuri, e dovrebbero raddoppiare nel farlo nell'IA, supportando il tipo di progetti infrastrutturali che riescono meglio quando fatti su scala e con imprimatur governativo.

Invece di un "progetto Manhattan" sconsiderato verso l'IAG,[^142] il governo USA potrebbe lanciare un progetto Apollo per sistemi controllabili, sicuri, affidabili. Questo potrebbe includere per esempio:

- Un programma importante per (a) sviluppare i meccanismi di sicurezza hardware on-chip e (b) l'infrastruttura, per gestire il lato computazionale dell'IA potente. Questi potrebbero costruire sull'[atto CHIPS](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) USA e il [regime di controlli all'esportazione](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion).
- Un'iniziativa su larga scala per sviluppare tecniche di verifica formale in modo che caratteristiche particolari dei sistemi di IA (come un interruttore di spegnimento) possano essere *dimostrate* essere presenti o assenti. Questo può sfruttare l'IA stessa per sviluppare dimostrazioni di proprietà.
- Uno sforzo su scala nazionale per creare software che sia verificabilmente sicuro, alimentato da strumenti di IA che possono ricodificare il software esistente in framework verificabilmente sicuri.
- Un progetto di investimento nazionale nell'avanzamento scientifico usando l'IA,[^143] gestito come partnership tra DOE, NSF e NIH.

In generale, c'è un'enorme superficie di attacco sulla nostra società che ci rende vulnerabili ai rischi dall'IA e dal suo uso improprio. Proteggere da alcuni di questi rischi richiederà investimento e standardizzazione a dimensione governativa. Questi fornirebbero vastamente più sicurezza che versare benzina sul fuoco delle corse verso l'IAG. E se l'IA deve essere costruita nelle armi e nei sistemi di comando e controllo, è cruciale che l'IA sia affidabile e sicura, cosa che l'IA attuale semplicemente non è.

### Concentrazione del potere e le sue mitigazioni

Questo saggio si è concentrato sull'idea del controllo umano dell'IA e il suo potenziale fallimento. Ma un'altra lente valida attraverso cui vedere la situazione dell'IA è attraverso la *concentrazione del potere.* Lo sviluppo di IA molto potente minaccia di concentrare il potere o nelle pochissime e grandissime mani aziendali che l'hanno sviluppata e la controlleranno, o nei governi che usano l'IA come nuovo mezzo per mantenere il proprio potere e controllo, o nei sistemi di IA stessi. O qualche miscela empia dei suddetti. In ognuno di questi casi la maggior parte dell'umanità perde potere, controllo e agenzia. Come potremmo combattere questo?

Il primissimo e più importante passo, ovviamente, è una chiusura delle Porte all'IAG e superintelligenza più intelligenti dell'umano. Queste esplicitamente possono sostituire direttamente umani e gruppi di umani. Se sono sotto controllo aziendale o governativo concentreranno il potere in quelle aziende o governi; se sono "libere" concentreranno il potere in se stesse. Quindi assumiamo che le Porte siano chiuse. E poi?

Una soluzione proposta alla concentrazione del potere è l'IA "open-source", dove i pesi del modello sono disponibili liberamente o ampiamente. Ma come menzionato prima, una volta che un modello è aperto, la maggior parte delle misure di sicurezza o protezioni possono essere (e generalmente sono) rimosse. Quindi c'è una tensione acuta tra da una parte la decentralizzazione, e dall'altra sicurezza, protezione e controllo umano dei sistemi di IA. Ci sono anche ragioni per essere scettici che i modelli aperti combatteranno da soli significativamente la concentrazione del potere nell'IA più di quanto abbiano fatto nei sistemi operativi (ancora dominati da Microsoft, Apple e Google nonostante alternative aperte).[^144]

Eppure potrebbero esserci modi per quadrare questo cerchio – centralizzare e mitigare i rischi mentre si decentralizza capacità e ricompensa economica. Questo richiede ripensare sia come l'IA è sviluppata sia come i suoi benefici sono distribuiti.

Nuovi modelli di sviluppo e proprietà pubblica dell'IA aiuterebbero. Questo potrebbe prendere diverse forme: IA sviluppata dal governo (soggetta a supervisione democratica),[^145] organizzazioni di sviluppo IA no-profit (come Mozilla per i browser), o strutture che abilitano proprietà e governance molto diffuse. La chiave è che queste istituzioni sarebbero esplicitamente incaricate di servire l'interesse pubblico mentre operano sotto forti vincoli di sicurezza.[^146] Regimi regulativi e di standard/certificazioni ben realizzati saranno anche vitali, in modo che i prodotti di IA offerti da un mercato vibrante restino genuinamente utili piuttosto che sfruttatori verso i loro utenti.

In termini di concentrazione del potere economico, possiamo usare tracciamento della provenienza e "dignità dei dati" per assicurare che i benefici economici fluiscano più ampiamente. In particolare, la maggior parte del potere dell'IA ora (e in futuro se teniamo le Porte chiuse) deriva da dati generati dall'umano, sia dati di addestramento diretto che feedback umano. Se le aziende di IA fossero richieste di compensare i fornitori di dati equamente,[^147] questo potrebbe almeno aiutare a distribuire le ricompense economiche più ampiamente. Oltre a questo, un altro modello potrebbe essere la proprietà pubblica di frazioni significative di grandi aziende di IA. Per esempio, governi capaci di tassare le aziende di IA potrebbero investire una frazione di entrate in un fondo sovrano che detiene azioni nelle aziende, e paga dividendi alla popolazione.[^148]

Cruciale in questi meccanismi è usare il potere dell'IA stessa per aiutare a distribuire meglio il potere, piuttosto che semplicemente combattere la concentrazione del potere guidata dall'IA usando mezzi non-IA. Un approccio potente sarebbe attraverso assistenti IA ben progettati che operano con genuino dovere fiduciario verso i loro utenti – mettendo gli interessi degli utenti al primo posto, specialmente sopra quelli dei fornitori aziendali.[^149] Questi assistenti devono essere veramente affidabili, tecnicamente competenti eppure appropriatamente limitati basandosi sul caso d'uso e livello di rischio, e ampiamente disponibili a tutti attraverso canali pubblici, no-profit, o certificati for-profit. Proprio come non accetteremmo mai un assistente umano che segretamente lavora contro i nostri interessi per un'altra parte, non dovremmo accettare assistenti IA che sorvegliano, manipolano, o estraggono valore dai loro utenti per beneficio aziendale.

Una tale trasformazione altererebbe fondamentalmente la dinamica attuale dove gli individui sono lasciati a negoziare da soli con vaste macchine aziendali e burocratiche (potenziate dall'IA) che prioritizzano l'estrazione di valore sopra il welfare umano. Mentre ci sono molti possibili approcci per ridistribuire più ampiamente il potere guidato dall'IA, nessuno emergerà di default: devono essere deliberatamente ingegnerizzati e governati con meccanismi come requisiti fiduciari, fornitura pubblica, e accesso a livelli basato sul rischio.

Gli approcci per mitigare la concentrazione del potere possono affrontare venti contrari significativi dai poteri in carica.[^150] Ma ci sono percorsi verso lo sviluppo dell'IA che non richiedono di scegliere tra sicurezza e potere concentrato. Costruendo le istituzioni giuste ora, potremmo assicurare che i benefici dell'IA siano ampiamente condivisi mentre i suoi rischi sono gestiti attentamente.

### Nuove strutture di governance e sociali

Le nostre strutture di governance attuali stanno lottando: sono lente a rispondere, spesso catturate da interessi speciali, e [sempre più non fidate dal pubblico.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) Eppure questa non è una ragione per abbandonarle – anzi, tutto il contrario. Alcune istituzioni potrebbero aver bisogno di essere sostituite, ma più ampiamente abbiamo bisogno di nuovi meccanismi che possano potenziare e supplementare le nostre strutture esistenti, aiutandole a funzionare meglio nel nostro mondo in rapida evoluzione.

Gran parte della nostra debolezza istituzionale deriva non dalle strutture governative formali, ma dalle istituzioni sociali degradate: i nostri sistemi per sviluppare comprensione condivisa, coordinare l'azione, e condurre discorso significativo. Finora, l'IA ha accelerato questa degradazione, inondando i nostri canali informativi con contenuto generato, indicandoci verso il contenuto più polarizzante e divisivo, e rendendo più difficile distinguere la verità dalla finzione.

Ma l'IA potrebbe effettivamente aiutare a ricostruire e rafforzare queste istituzioni sociali. Consideriamo tre aree cruciali:

Primo, l'IA potrebbe aiutare a restaurare la fiducia nei nostri sistemi epistemici – i nostri modi di sapere cosa è vero. Potremmo sviluppare sistemi potenziati dall'IA che tracciano e verificano la provenienza dell'informazione, dai dati grezzi attraverso l'analisi alle conclusioni. Questi sistemi potrebbero combinare verifica crittografica con analisi sofisticata per aiutare le persone a capire non solo se qualcosa è vero, ma come sappiamo che è vero.[^151] Assistenti IA leali potrebbero essere incaricati di seguire i dettagli per assicurare che reggano.

Secondo, l'IA potrebbe abilitare nuove forme di coordinamento su larga scala. Molti dei nostri problemi più pressanti – dal cambiamento climatico alla resistenza antibiotica – sono fondamentalmente problemi di coordinamento. Siamo [bloccati in situazioni che sono peggiori di quello che potrebbero essere per quasi tutti](https://equilibriabook.com/), perché nessun individuo o gruppo può permettersi di fare la prima mossa. I sistemi di IA potrebbero aiutare modellando strutture di incentivi complesse, identificando percorsi praticabili verso risultati migliori, e facilitando la costruzione di fiducia e meccanismi di impegno necessari per arrivarci.

Forse più intrigante, l'IA potrebbe abilitare forme completamente nuove di discorso sociale. Immaginate di poter "parlare con una città"[^152] – non solo visualizzare statistiche, ma avere un dialogo significativo con un sistema di IA che processa e sintetizza le opinioni, esperienze, bisogni e aspirazioni di milioni di residenti. O considerate come l'IA potrebbe facilitare dialogo genuino tra gruppi che attualmente si parlano senza ascoltarsi, aiutando ogni lato a capire meglio le preoccupazioni e valori effettivi dell'altro piuttosto che le loro caricature degli altri.[^153] O l'IA potrebbe offrire intermediazione esperta e credibilmente neutrale di dispute tra persone o persino grandi gruppi di persone (che potrebbero tutti interagire con essa direttamente e individualmente!) L'IA attuale è totalmente capace di fare questo lavoro, ma gli strumenti per farlo non nascono da soli, o via incentivi di mercato.

Queste possibilità potrebbero suonare utopiche, specialmente dato il ruolo attuale dell'IA nel degradare discorso e fiducia. Ma è precisamente per questo che dobbiamo sviluppare attivamente queste applicazioni positive. Chiudendo le Porte all'IAG incontrollabile e prioritizzando l'IA che potenzia l'agenzia umana, possiamo dirigere il progresso tecnologico verso un futuro dove l'IA serve come forza per empowerment, resilienza e avanzamento collettivo.


[^124]: Detto questo, stare lontano dalla tripla-intersezione è sfortunatamente non così facile come si potrebbe desiderare. Spingere molto forte la capacità in uno qualsiasi dei tre aspetti tende ad aumentarla negli altri. In particolare, potrebbe essere difficile creare un'intelligenza estremamente generale e capace che non possa essere facilmente resa autonoma. Un approccio è addestrare modelli ["miopi"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) sistemi con capacità di pianificazione limitata. Un altro sarebbe concentrarsi sull'ingegnerizzare sistemi ["oracolo"](https://arxiv.org/abs/1711.05541) puri che eviterebbero di rispondere a domande orientate all'azione.

[^125]: Molte aziende falliscono nel realizzare che anche loro sarebbero eventualmente rimpiazzate dall'IAG, anche se ci volesse più tempo – se lo facessero, potrebbero spingere su quelle Porte un po' meno!

[^126]: I sistemi di IA potrebbero comunicare in modi più efficienti ma meno intelligibili, ma mantenere la comprensione umana dovrebbe avere priorità.

[^127]: Questa idea di IA modulare, interpretabile è stata sviluppata in dettaglio da diversi ricercatori; vedi ad es. il modello ["Comprehensive AI Services"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) di Drexler, l'["Open Agency Architecture"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai) di Dalrymple e altri. Mentre tali sistemi potrebbero richiedere più sforzo ingegneristico rispetto a reti neurali monolitiche addestrate con computazione massiva, questo è precisamente dove i limiti computazionali aiutano – rendendo il percorso più sicuro, più trasparente anche quello più pratico.

[^128]: Sulle analisi di sicurezza in generale vedi [questo manuale](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). Pertinente all'IA in particolare, vedi [Wasil et al.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [Clymer et al.](https://arxiv.org/abs/2403.10462), [Buhl et al.](https://arxiv.org/abs/2410.21572), e [Balesni et al.](https://arxiv.org/abs/2411.03336)

[^129]: Stiamo di fatto già vedendo questa tendenza guidata solo dal costo alto dell'inferenza: modelli più piccoli e più specializzati "distillati" da quelli più grandi e capaci di girare su hardware meno costoso.

[^130]: Capisco perché quelli eccitati sull'ecosistema tech dell'IA possano opporsi a quella che vedono come regolamentazione onerosa sulla loro industria. Ma è francamente sconcertante per me perché, diciamo, un venture capitalist vorrebbe permettere escalation incontrollabile all'IAG e superintelligenza. Quei sistemi (e aziende, mentre rimangono sotto controllo aziendale) *mangeranno tutte le startup come spuntino*. Probabilmente anche *prima* di mangiare altre industrie. Chiunque sia investito in un ecosistema IA fiorente dovrebbe prioritizzare l'assicurare che lo sviluppo dell'IAG non porti a monopolizzazione da parte di pochi giocatori dominanti.

[^131]: Come l'economista ed ex ricercatore Deepmind Michael Webb [ha detto](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/), "Penso che se fermassimo tutto lo sviluppo di modelli linguistici più grandi oggi, quindi GPT-4 e Claude e qualunque, e fossero le ultime cose che addestriamo di quella dimensione – quindi permettiamo molte più iterazioni su cose di quella dimensione e tutti i tipi di fine-tuning, ma niente di più grande di quello, nessun avanzamento più grande – solo quello che abbiamo oggi penso sia abbastanza per alimentare 20 o 30 anni di incredibile crescita economica."

[^132]: Per esempio, il sistema alphafold di DeepMind ha usato solo 100,000esimi del numero FLOP di GPT-4.

[^133]: La difficoltà delle auto a guida autonoma è importante notare qui: mentre nominalmente un compito specifico, e raggiungibile con affidabilità equa con sistemi di IA relativamente piccoli, è necessaria conoscenza e comprensione del mondo reale estensiva per ottenere l'affidabilità al livello necessario in un compito così critico per la sicurezza.

[^134]: Per esempio, dato un budget computazionale, vedremmo probabilmente modelli GPAI pre-addestrati a (diciamo) metà di quel budget, e l'altra metà usata per addestrare capacità molto alta in una gamma più ristretta di compiti. Questo darebbe capacità specifica sovrumana supportata da intelligenza generale quasi-umana.

[^135]: La tecnica dominante di allineamento attuale è "reinforcement learning by human feedback" [(RLHF)](https://arxiv.org/abs/1706.03741) e usa feedback umano per creare un segnale di ricompensa/punizione per l'apprendimento per rinforzo del modello di IA. Questa e tecniche correlate come [constitutional AI](https://arxiv.org/abs/2212.08073) stanno funzionando sorprendentemente bene (anche se mancano di robustezza e possono essere aggirate con sforzo modesto.) Inoltre, i modelli linguistici attuali sono generalmente abbastanza competenti nel ragionamento di senso comune che non faranno errori morali sciocchi. Questo è qualcosa come un punto ottimale: abbastanza intelligenti per capire quello che le persone vogliono (nella misura in cui può essere definito), ma non abbastanza intelligenti per pianificare inganni elaborati o causare danni enormi quando sbagliano.

[^136]: Nel lungo termine, qualsiasi livello di capacità IA che viene sviluppato probabilmente prolifererà, dato che ultimamente è software, ed utile. Avremo bisogno di avere meccanismi robusti per difendere contro i rischi che tali sistemi pongono. Ma *non li abbiamo ora* quindi dobbiamo essere molto misurati in quanto modelli di IA potenti possono proliferare.

[^137]: La vasta maggioranza di questi sono deepfake pornografici non consensuali, inclusi di minori.

[^138]: Molti ingredienti per tali soluzioni esistono, nella forma di leggi "bot-o-no" (nell'atto AI dell'UE tra altri posti), [tecnologie di tracciamento provenienza dell'industria](https://c2pa.org/), [aggregatori di notizie innovativi](https://www.improvethenews.org/), [aggregatori](https://metaculus.com/) di previsioni e mercati, ecc.

[^139]: L'ondata di automazione potrebbe non seguire pattern precedenti, in quanto compiti relativamente *ad alta* competenza come scrittura di qualità, interpretazione della legge, o dare consigli medici, potrebbero essere altrettanto o persino più vulnerabili all'automazione dei compiti a competenza più bassa.

[^140]: Per modellazione attenta dell'effetto dell'IAG sui salari, vedi il report [qui](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek), e dettagli cruenti [qui](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0), da Anton Korinek e collaboratori. Trovano che mentre più pezzi di lavori sono automatizzati, produttività e salari salgono – fino a un punto. Una volta che *troppo* è automatizzato, la produttività continua ad aumentare, ma i salari crollano perché le persone sono sostituite all'ingrosso da IA efficiente. Questo è perché chiudere le Porte è così utile: otteniamo la produttività senza i salari umani svaniti.

[^141]: Ci sono molti modi in cui l'IA può essere usata come, e per aiutare a costruire, tecnologie "difensive" per rendere più robuste protezioni e gestione. Vedi [questo](https://vitalik.eth.limo/general/2025/01/05/dacc2.html) post influente che descrive questa agenda "D/acc".

[^142]: Qualcosa di ironico, un progetto Manhattan USA farebbe probabilmente poco per accelerare le tempistiche verso l'IAG – il quadrante dell'investimento umano e fiscale nel progresso IA è già puntato all'11. I risultati primari sarebbero ispirare un progetto simile in Cina (che eccelle in progetti infrastrutturali a livello nazionale), rendere molto più difficili accordi internazionali che limitano il rischio dell'IA, e allarmare altri avversari geopolitici degli USA come la Russia.

[^143]: Il programma ["National AI Research Resource"](https://nairrpilot.org/) è un buon passo attuale in questa direzione e dovrebbe essere espanso.

[^144]: Vedi [questa analisi](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807) dei vari significati e implicazioni di "aperto" nei prodotti tech e come alcuni abbiano portato a più, piuttosto che meno, radicamento del dominio.

[^145]: Piani negli USA per una [National AI Research Resource](https://nairratdoe.ornl.gov/) e il recente lancio di una [European AI Foundation](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/) sono passi interessanti in questa direzione.

[^146]: La sfida qui non è tecnica ma istituzionale – abbiamo urgentemente bisogno di esempi ed esperimenti del mondo reale su come potrebbe essere lo sviluppo IA nell'interesse pubblico.

[^147]: Questo va contro i modelli di business attuali delle grandi aziende tech e richiederebbe sia azione legale che nuove norme.

[^148]: Solo alcuni governi saranno capaci di farlo. Un'idea più radicale è [un fondo universale di questo tipo, sotto proprietà congiunta di tutti gli umani.](https://futureoflife.org/project/the-windfall-trust/)

[^149]: Per un'esposizione lunga di questo caso vedi [questo paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338) sulla lealtà dell'IA. Sfortunatamente la traiettoria di default degli assistenti IA è probabilmente una dove sono sempre più sleali.

[^150]: Qualcosa di ironico, molti poteri in carica sono anche a rischio di disempowerment supportato dall'IA; ma può essere difficile per loro percepire questo finché e a meno che il processo non arrivi abbastanza lontano.

[^151]: Alcuni sforzi interessanti in questa direzione sono rappresentati dalla [coalizione c2pa](https://c2pa.org/) sulla verifica crittografica; [Verity](https://www.improvethenews.org/) e [Ground news](https://ground.news/) su migliore epistemica delle notizie; e [Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com) e mercati di previsione su radicare il discorso in previsioni falsificabili.

[^152]: Vedi [questo](https://talktothecity.org/) progetto pilota affascinante.

[^153]: Vedi [Kialo](https://www.kialo-edu.com/), e sforzi del [Collective Intelligence Project](https://www.cip.org/) per alcuni esempi.

## Capitolo 10 - La scelta che ci attende

Per preservare il nostro futuro umano, dobbiamo scegliere di chiudere le Porte alla IAG e alla superintelligenza.

L'ultima volta che l'umanità ha condiviso la Terra con altre menti che parlavano, pensavano, costruivano tecnologie e risolvevano problemi generici è stata 40.000 anni fa nell'Europa dell'era glaciale. Quelle altre menti si sono estinte, in tutto o in parte a causa degli sforzi delle nostre.

Ora stiamo rientrando in un'epoca simile. I prodotti più avanzati della nostra cultura e tecnologia – dataset costruiti dall'intero patrimonio informativo di internet e chip da 100 miliardi di elementi che rappresentano le tecnologie più complesse mai realizzate – vengono combinati per portare in essere sistemi di IA generale avanzati.

Gli sviluppatori di questi sistemi sono ansiosi di presentarli come strumenti per l'empowerment umano. E in effetti potrebbero esserlo. Ma non illudiamoci: la nostra traiettoria attuale è quella di costruire agenti digitali sempre più potenti, orientati a obiettivi, capaci di prendere decisioni e dotati di capacità generali. Già performano quanto molti esseri umani in un'ampia gamma di compiti intellettuali, stanno migliorando rapidamente e contribuiscono al proprio miglioramento.

A meno che questa traiettoria non cambi o non incontri un ostacolo imprevisto, avremo presto – in anni, non decenni – intelligenze digitali pericolosamente potenti. Anche nel *migliore* degli scenari, queste porterebbero grandi benefici economici (almeno per alcuni di noi) ma solo al costo di una profonda rottura nella nostra società, e della sostituzione degli umani nella maggior parte delle cose più importanti che facciamo: queste macchine penserebbero per noi, pianificherebbero per noi, deciderebbero per noi e creerebbero per noi. Saremmo viziati, ma come bambini viziati. Molto più probabilmente, questi sistemi sostituirebbero gli umani sia nelle cose positive *che* in quelle negative che facciamo, inclusi sfruttamento, manipolazione, violenza e guerra. Possiamo sopravvivere a versioni iperpotenziate dall'IA di tutto questo? Infine, è più che plausibile che le cose non vadano affatto bene: che relativamente presto saremmo sostituiti non solo in quello che facciamo, ma in quello che *siamo*, come architetti della civiltà e del futuro. Chiedete ai neanderthal come va a finire. Forse anche noi riceveremmo qualche fronzolo in più per un po'.

*Non dobbiamo farlo.* Abbiamo IA competitive con l'uomo, e non c'è bisogno di costruire IA con cui *non* possiamo competere. Possiamo costruire straordinari strumenti di IA senza costruire una specie successore. L'idea che la IAG e la superintelligenza siano inevitabili è una *scelta mascherata da destino*.

Imponendo alcuni limiti rigidi e globali, possiamo mantenere la capacità generale dell'IA approssimativamente a livello umano continuando comunque a raccogliere i benefici della capacità dei computer di elaborare dati in modi che noi non possiamo, e automatizzare compiti che nessuno di noi vuole svolgere. Questi porrebbero ancora molti rischi, ma se progettati e gestiti bene, sarebbero un enorme vantaggio per l'umanità, dalla medicina alla ricerca ai prodotti di consumo.

Imporre limiti richiederebbe cooperazione internazionale, ma meno di quanto si potrebbe pensare, e quei limiti lascerebbero comunque ampio spazio per un'enorme industria dell'IA e dell'hardware per IA focalizzata su applicazioni che migliorano il benessere umano, piuttosto che sulla pura ricerca del potere. E se, con forti garanzie di sicurezza e dopo un dialogo globale significativo, decidessimo di andare oltre, quell'opzione continuerebbe a essere nostra da perseguire.

L'umanità deve *scegliere* di chiudere le Porte alla IAG e alla superintelligenza.

Per mantenere il futuro umano.

### Una nota dell'autore

Grazie per aver dedicato del tempo a esplorare questo argomento con noi.

Ho scritto questo saggio perché come scienziato sento che sia importante dire la verità senza fronzoli, e perché come persona sento che sia cruciale per noi agire rapidamente e con decisione per affrontare una questione che cambierà il mondo: lo sviluppo di sistemi di IA più intelligenti dell'uomo.

Se vogliamo rispondere a questo straordinario stato di cose con saggezza, dobbiamo essere preparati a esaminare criticamente la narrativa dominante secondo cui la IAG e la superintelligenza 'devono' essere costruite per garantire i nostri interessi, o sono 'inevitabili' e non possono essere fermate. Queste narrative ci lasciano privi di potere, incapaci di vedere i percorsi alternativi che abbiamo davanti.

Spero vi unirete a me nel chiedere cautela di fronte alla sconsideratezza, e coraggio di fronte all'avidità.

Spero vi unirete a me nel chiedere un futuro umano.

*– Anthony*

![Anthony Aguirre signature](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAnthony-Aguirre-signature-300x84.png&w=3840&q=75)

## Appendici

Informazioni supplementari, che includono - Dettagli tecnici sulla contabilizzazione computazionale, un esempio di implementazione di una 'chiusura delle porte', dettagli per un regime rigoroso di responsabilità per la IAG, e un approccio stratificato agli standard di sicurezza per la IAG.

### Appendice A: Dettagli tecnici della contabilizzazione computazionale

Per controlli significativi basati sulla capacità computazionale è necessario un metodo dettagliato sia per la "verità oggettiva" che per buone approssimazioni della capacità computazionale totale utilizzata nell'addestramento e nell'inferenza. Ecco un esempio di come la "verità oggettiva" potrebbe essere calcolata a livello tecnico.

**Definizioni:**

*Grafo causale computazionale:* Per un dato output O di un modello di IA, esiste un insieme di calcoli digitali per i quali modificare il risultato di tale calcolo potrebbe potenzialmente cambiare O. (Questo dovrebbe essere assunto in modo conservativo, ovvero dovrebbe esistere una ragione chiara per credere che un calcolo sia indipendente da un precursore che avviene prima nel tempo e ha un potenziale percorso causale fisico di effetto.) Questo include i calcoli eseguiti dal modello di IA durante l'inferenza, così come i calcoli che sono entrati nell'input, nella preparazione dei dati e nell'addestramento del modello. Poiché ognuno di questi può essere a sua volta l'output di un modello di IA, questo viene calcolato ricorsivamente, interrotto dove un umano ha fornito un cambiamento significativo all'input.

*Capacità computazionale di addestramento:* La capacità computazionale totale, in FLOP o altre unità, implicata dal grafo causale computazionale di una rete neurale (inclusi preparazione dei dati, addestramento, fine-tuning e qualsiasi altro calcolo.)

*Capacità computazionale di output:* La capacità computazionale totale nel grafo causale computazionale di un dato output di IA, incluse tutte le reti neurali (e inclusa la loro Capacità computazionale di addestramento) e altri calcoli che contribuiscono a quell'output.

*Tasso di capacità computazionale di inferenza:* In una serie di output, il tasso di cambiamento (in FLOP/s o altre unità) della Capacità computazionale di output tra gli output, ovvero la capacità computazionale utilizzata per produrre l'output successivo, divisa per l'intervallo di tempo tra gli output.

**Esempi e approssimazioni:**

- Per una singola rete neurale addestrata su dati creati da umani, la Capacità computazionale di addestramento è semplicemente la capacità computazionale totale di addestramento come riportato abitualmente.
- Per una tale rete neurale che esegue inferenza a ritmo costante, il Tasso di capacità computazionale di inferenza è approssimativamente la velocità totale del cluster computazionale che esegue l'inferenza in FLOP/s.
- Per il fine-tuning di modelli, la Capacità computazionale di addestramento del modello completo è data dalla Capacità computazionale di addestramento del modello non sottoposto a fine-tuning più il calcolo eseguito durante il fine-tuning e per preparare qualsiasi dato utilizzato nel fine-tuning.
- Per un modello distillato, la Capacità computazionale di addestramento del modello completo include l'addestramento sia del modello distillato che del modello più grande utilizzato per fornire dati sintetici o altro input di addestramento.
- Se vengono addestrati diversi modelli, ma molti "tentativi" vengono scartati sulla base del giudizio umano, questi non contano verso la Capacità computazionale di addestramento o di output del modello conservato.

### Appendice B: Esempio di implementazione di una chiusura delle porte

**Esempio di implementazione:** Ecco un esempio di come potrebbe funzionare una chiusura delle porte, dato un limite di 10 <sup>27</sup> FLOP per l'addestramento e 10 <sup>20</sup> FLOP/s per l'inferenza (esecuzione dell'IA):

**1\. Pausa:** Per ragioni di sicurezza nazionale, il ramo esecutivo statunitense chiede a tutte le aziende con sede negli Stati Uniti, che operano negli Stati Uniti o che utilizzano chip prodotti negli Stati Uniti, di cessare qualsiasi nuovo ciclo di addestramento di IA che potrebbe superare il limite di Capacità computazionale di addestramento di 10 <sup>27</sup> FLOP. Gli Stati Uniti dovrebbero avviare discussioni con altri paesi che ospitano sviluppo di IA, incoraggiandoli fortemente ad adottare misure simili e indicando che la pausa statunitense potrebbe essere revocata qualora scegliessero di non conformarsi.

**2\. Supervisione e licenze statunitensi:** Tramite ordine esecutivo o azione di un'agenzia regolatoria esistente, gli Stati Uniti richiedono che entro (diciamo) un anno:

- Tutti i cicli di addestramento di IA stimati sopra i 10 <sup>25</sup> FLOP eseguiti da aziende che operano negli Stati Uniti siano registrati in un database mantenuto da un'agenzia regolatoria statunitense. (Nota: Una versione leggermente più debole di questo era già stata inclusa nell'ordine esecutivo statunitense del 2023 sull'IA, ora revocato, che richiedeva la registrazione per modelli sopra i 10 <sup>26</sup> FLOP.)
- Tutti i produttori di hardware rilevante per l'IA che operano negli Stati Uniti o fanno affari con il governo statunitense aderiscano a una serie di requisiti sul loro hardware specializzato e sul software che lo gestisce. (Molti di questi requisiti potrebbero essere incorporati in aggiornamenti software e firmware dell'hardware esistente, ma soluzioni a lungo termine e robuste richiederebbero modifiche alle generazioni successive di hardware.) Tra questi c'è il requisito che se l'hardware fa parte di un cluster interconnesso ad alta velocità capace di eseguire 10 <sup>18</sup> FLOP/s di calcolo, è richiesto un livello più alto di verifica, che include autorizzazione regolare da parte di un "governatore" remoto che riceve sia telemetria che richieste per eseguire calcoli aggiuntivi.
- Il custode riporta il calcolo totale eseguito sul suo hardware all'agenzia che mantiene il database statunitense.
- Requisiti più stringenti vengono introdotti gradualmente per consentire una supervisione e autorizzazione più sicura e flessibile.

**3\. Supervisione internazionale:**

- Gli Stati Uniti, la Cina e qualsiasi altro paese che ospita capacità avanzate di produzione di chip negoziano un accordo internazionale.
- Questo accordo crea una nuova agenzia internazionale, analoga all'Agenzia Internazionale per l'Energia Atomica, incaricata di supervisionare l'addestramento e l'esecuzione dell'IA.
- I paesi firmatari devono richiedere ai loro produttori nazionali di hardware per IA di conformarsi a una serie di requisiti almeno tanto stringenti quanto quelli imposti negli Stati Uniti.
- I custodi sono ora tenuti a riportare i numeri di calcolo dell'IA sia alle agenzie nei loro paesi di origine che a un nuovo ufficio all'interno dell'agenzia internazionale.
- Altri paesi sono fortemente incoraggiati ad aderire all'accordo internazionale esistente: i controlli sull'esportazione da parte dei paesi firmatari limitano l'accesso all'hardware di fascia alta da parte dei non firmatari mentre i firmatari possono ricevere supporto tecnico nella gestione dei loro sistemi di IA.

**4\. Verifica e applicazione internazionale:**

- Il sistema di verifica hardware viene aggiornato in modo che riporti l'utilizzo di calcolo sia al custode originale che anche direttamente all'ufficio dell'agenzia internazionale.
- L'agenzia, tramite discussione con i firmatari dell'accordo internazionale, concorda su limitazioni computazionali che poi assumono forza legale nei paesi firmatari.
- In parallelo, può essere sviluppata una serie di standard internazionali in modo che l'addestramento e l'esecuzione di IA sopra una soglia di calcolo (ma sotto il limite) siano tenuti ad aderire a quegli standard.
- L'agenzia può, se necessario per compensare algoritmi migliori ecc., abbassare il limite computazionale. O, se ritenuto sicuro e consigliabile (al livello di garanzie di sicurezza dimostrabili), aumentare il limite computazionale.

### Appendice C: Dettagli per un regime rigoroso di responsabilità per la IAG

**Dettagli per un regime rigoroso di responsabilità per la IAG**

- La creazione e il funzionamento di un sistema di IA avanzato che sia altamente generale, capace e autonomo, è considerata un'attività "anormalmente pericolosa".
- Come tale, la responsabilità predefinita per l'addestramento e il funzionamento di tali sistemi è la responsabilità oggettiva, solidale (o il suo equivalente non statunitense) per qualsiasi danno causato dal modello o dai suoi output/azioni.
- La responsabilità personale sarà imposta per dirigenti e membri del consiglio di amministrazione in casi di grave negligenza o cattiva condotta intenzionale. Questo dovrebbe includere sanzioni penali per i casi più gravi.
- Esistono numerose clausole di salvaguardia sotto le quali la responsabilità ritorna a quella predefinita (basata sulla colpa, negli Stati Uniti) a cui persone e aziende sarebbero normalmente soggette.
	- Modelli addestrati e operati sotto una certa soglia computazionale (che sarebbe almeno 10 volte inferiore ai limiti descritti sopra.)
	- IA che è "debole" (approssimativamente, sotto il livello di esperto umano nei compiti per cui è intesa) e/o
	- IA che è "ristretta" (con un ambito fisso e piuttosto limitato di compiti e operazioni per cui è specificamente progettata e addestrata) e/o
	- IA che è "passiva" (molto limitata nella sua capacità – anche sotto modeste modifiche – di intraprendere azioni o eseguire compiti complessi multi-fase senza coinvolgimento e controllo umano diretto.)
	- Un'IA che è garantita essere sicura, protetta e controllabile (dimostrabilmente sicura, o un'analisi del rischio indica un livello trascurabile di danno previsto.)
- Le clausole di salvaguardia possono essere rivendicate sulla base di un'[analisi di sicurezza](https://arxiv.org/abs/2410.21572) preparata dallo sviluppatore dell'IA e approvata da un'agenzia o auditor accreditato da un'agenzia. Per rivendicare una clausola di salvaguardia basata sul calcolo, lo sviluppatore deve solo fornire stime credibili della Capacità computazionale di addestramento totale e del Tasso di inferenza massimo
- La legislazione delineerebbe esplicitamente situazioni in cui il risarcimento ingiuntivo dallo sviluppo di sistemi di IA con alto rischio di danno pubblico sarebbe appropriato.
- Consorzi di aziende, lavorando con ONG e agenzie governative, dovrebbero sviluppare standard e norme che definiscano questi termini, come i regolatori dovrebbero concedere clausole di salvaguardia, come gli sviluppatori di IA dovrebbero sviluppare analisi di sicurezza, e come i tribunali dovrebbero interpretare la responsabilità dove le clausole di salvaguardia non sono rivendicate proattivamente.

### Appendice D: Un approccio stratificato agli standard di sicurezza per la IAG

**Un approccio stratificato agli standard di sicurezza per la IAG**

| Livello di Rischio | Innesco/i | Requisiti per l'addestramento | Requisito per il deployment |
| --- | --- | --- | --- |
| LR-0 | IA debole in autonomia, generalità e intelligenza | nessuno | nessuno |
| LR-1 | IA forte in una di autonomia, generalità e intelligenza | nessuno | Basato su rischio e uso, potenzialmente analisi di sicurezza approvate dalle autorità nazionali ovunque il modello possa essere utilizzato |
| LR-2 | IA forte in due di autonomia, generalità e intelligenza | Registrazione presso l'autorità nazionale con giurisdizione sullo sviluppatore | Analisi di sicurezza che delimita il rischio di danno grave sotto i livelli autorizzati più audit di sicurezza indipendenti (inclusi red-teaming black-box e white-box) approvati dalle autorità nazionali ovunque il modello possa essere utilizzato |
| LR-3 | IAG forte in autonomia, generalità e intelligenza | Pre-approvazione del piano di sicurezza dall'autorità nazionale con giurisdizione sullo sviluppatore | Analisi di sicurezza che garantisce rischio delimitato di danno grave sotto i livelli autorizzati così come specifiche richieste, inclusi cybersicurezza, controllabilità, un interruttore di emergenza non rimovibile, allineamento con i valori umani, e robustezza all'uso malevolo. |
| LR-4 | Qualsiasi modello che superi anche 10 <sup>27</sup> FLOP di Addestramento o 10 <sup>20</sup> FLOP/s di Inferenza | Proibito in attesa della revoca concordata internazionalmente del limite computazionale | Proibito in attesa della revoca concordata internazionalmente del limite computazionale |

Classificazioni di rischio e standard di sicurezza, con livelli basati su soglie computazionali così come combinazioni di alta autonomia, generalità e intelligenza:

- *Forte autonomia* si applica se il sistema è in grado di eseguire, o può essere facilmente fatto eseguire, compiti multi-fase e/o intraprendere azioni complesse che sono rilevanti nel mondo reale, senza supervisione o intervento umano significativo. Esempi: veicoli autonomi e robot; bot di trading finanziario. Non-esempi: GPT-4; classificatori di immagini
- *Forte generalità* indica un ampio ambito di applicazione, esecuzione di compiti per cui il modello non è stato deliberatamente e specificamente addestrato, e capacità significativa di apprendere nuovi compiti. Esempi: GPT-4; mu-zero. Non-esempi: AlphaFold; veicoli autonomi; generatori di immagini
- *Forte intelligenza* corrisponde al raggiungimento di prestazioni a livello di esperto umano sui compiti per cui il modello performa meglio (e per un modello generale, attraverso una vasta gamma di compiti.) Esempi: AlphaFold; mu-zero; o3. Non-esempi: GPT-4; Siri

### Ringraziamenti

Alcuni ringraziamenti alle persone che hanno contribuito a Keep The Future Human.

Questo lavoro riflette le opinioni dell'autore e non deve essere considerato come la posizione ufficiale del Future of Life Institute (benché siano compatibili; per la sua posizione ufficiale si veda [questa pagina](https://futureoflife.org/our-position-on-ai/)), o di qualsiasi altra organizzazione con cui l'autore è affiliato.

Sono grato agli esseri umani Mark Brakel, Ben Eisenpress, Anna Hehir, Carlos Gutierrez, Emilia Javorsky, Richard Mallah, Jordan Scharnhorst, Elyse Fulcher, Max Tegmark e Jaan Tallinn per i commenti sul manoscritto; a Tim Schrier per l'aiuto con alcuni riferimenti bibliografici; a Taylor Jones ed Elyse Fulcher per aver migliorato l'aspetto dei diagrammi.

Questo lavoro ha fatto uso limitato di modelli di IA generativa (Claude e ChatGPT) nella sua creazione, per alcune modifiche e red-teaming. Secondo lo standard consolidato dei livelli di coinvolgimento dell'IA nelle opere creative, questo lavoro riceverebbe probabilmente un punteggio di 3/10. (In realtà non esiste affatto tale standard! Ma dovrebbe esistere.)

Siamo molto grati a [Julius Odai](https://www.linkedin.com/in/julius-odai/) per aver prodotto questa versione web del saggio, che rende la lettura e la navigazione attraverso il testo un'esperienza molto piacevole. Julius è un tecnologo e recente partecipante del corso AI Governance di BlueDot Impact.