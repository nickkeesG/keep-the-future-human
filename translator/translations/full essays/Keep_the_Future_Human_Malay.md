# Pastikan Masa Depan Tetap Manusiawi

Esei ini membentangkan hujah mengapa dan bagaimana kita perlu menutup pintu gerbang kepada KBA dan superintelligence, serta apa yang sepatutnya kita bina sebagai gantinya.

Jika anda hanya mahukan intipati utama, sila rujuk Ringkasan eksekutif. Kemudian, Bab 2-5 akan memberikan latar belakang mengenai jenis-jenis sistem AI yang dibincangkan dalam esei ini. Bab 5-7 menerangkan mengapa kita mungkin menjangkakan KBA akan tiba tidak lama lagi, dan apa yang mungkin berlaku apabila ia tiba. Akhir sekali, Bab 8-9 menggariskan cadangan konkrit untuk menghalang KBA daripada dibina.

[Muat turun PDF](https://keepthefuturehuman.ai/wp-content/uploads/2025/03/Keep_the_Future_Human__AnthonyAguirre__5March2025.pdf)

Jumlah masa bacaan: 2-3 jam

## Ringkasan eksekutif

Tinjauan peringkat tinggi mengenai esei ini. Jika anda kekurangan masa, dapatkan semua perkara utama dalam hanya 10 minit.

Kemajuan dramatik dalam kecerdasan buatan sepanjang dekad yang lalu (untuk AI tujuan khusus) dan beberapa tahun kebelakangan ini (untuk AI tujuan am) telah mengubah AI daripada bidang akademik khusus kepada strategi perniagaan teras bagi kebanyakan syarikat terbesar di dunia, dengan pelaburan tahunan beratus-ratus bilion dolar dalam teknik dan teknologi untuk memajukan keupayaan AI.

Kini kita tiba di persimpangan kritikal. Apabila keupayaan sistem AI baru mula menyamai dan melebihi keupayaan manusia merentasi banyak domain kognitif, manusia mesti memutuskan: sejauh mana kita mahu pergi, dan ke arah manakah?

AI, seperti setiap teknologi, bermula dengan matlamat untuk memperbaiki keadaan bagi penciptanya. Tetapi trajektori semasa kita, dan pilihan tersirat, adalah perlumbaan tanpa kawalan ke arah sistem yang semakin berkuasa, didorong oleh insentif ekonomi beberapa syarikat teknologi gergasi yang berusaha mengautomasikan sebahagian besar aktiviti ekonomi semasa dan buruh manusia. Jika perlumbaan ini berterusan lebih lama lagi, terdapat pemenang yang tidak dapat dielakkan: AI itu sendiri – alternatif yang lebih pantas, lebih bijak, lebih murah kepada manusia dalam ekonomi kita, pemikiran kita, keputusan kita, dan akhirnya dalam kawalan tamadun kita.

Tetapi kita boleh membuat pilihan lain: melalui kerajaan kita, kita boleh mengawal proses pembangunan AI untuk mengenakan had yang jelas, garisan yang tidak akan kita lalui, dan perkara yang kita tidak akan lakukan – sebagaimana yang kita lakukan untuk teknologi nuklear, senjata pemusnah beramai-ramai, senjata angkasa lepas, proses yang memudaratkan alam sekitar, kejuruteraan bio manusia, dan eugenik. Yang paling penting, kita boleh memastikan AI kekal sebagai alat untuk memperkasakan manusia, bukannya spesies baru yang menggantikan dan akhirnya meminggirkan kita.

Esei ini berhujah bahawa kita perlu *memastikan masa depan tetap manusiawi* dengan menutup "pintu gerbang" kepada AI am tujuan umum yang lebih bijak daripada manusia dan autonomi – kadangkala dipanggil "KBA" – dan terutamanya kepada versi sangat superhuman yang kadangkala dipanggil "superintelligence." Sebaliknya, kita perlu memberi tumpuan kepada alat AI yang berkuasa dan boleh dipercayai yang boleh memperkasakan individu dan secara transformatif meningkatkan keupayaan masyarakat manusia untuk melakukan apa yang mereka lakukan terbaik. Struktur hujah ini diikuti secara ringkas.

### AI adalah berbeza

Sistem AI secara asasnya berbeza daripada teknologi lain. Walaupun perisian tradisional mengikut arahan tepat, sistem AI belajar cara mencapai matlamat tanpa diberitahu secara eksplisit bagaimana. Ini menjadikan mereka berkuasa: jika kita boleh mentakrifkan matlamat atau metrik kejayaan dengan jelas, dalam kebanyakan kes sistem AI boleh belajar untuk mencapainya. Tetapi ia juga menjadikan mereka secara intrinsiknya tidak dapat diramal: kita tidak dapat menentukan dengan pasti tindakan apa yang akan mereka ambil untuk mencapai objektif mereka.

Mereka juga sebahagian besarnya tidak dapat dijelaskan: walaupun mereka sebahagiannya kod, mereka kebanyakannya adalah set nombor yang sangat besar dan tidak dapat dipahami – "pemberat" rangkaian neural – yang tidak dapat dianalisis; kita tidak jauh lebih baik dalam memahami cara kerja dalaman mereka berbanding memahami pemikiran dengan mengintip ke dalam otak biologi.

Mod teras latihan rangkaian neural digital ini berkembang dengan pesat dalam kerumitan. Sistem AI yang paling berkuasa dicipta melalui eksperimen pengkomputeran besar-besaran, menggunakan perkakasan khusus untuk melatih rangkaian neural pada set data yang sangat besar, yang kemudian ditambah dengan alat perisian dan superstruktur.

Ini telah membawa kepada penciptaan alat yang sangat berkuasa untuk mencipta dan memproses teks dan imej, melakukan penaakulan matematik dan saintifik, mengagregatkan maklumat, dan secara interaktif menanyakan simpanan besar pengetahuan manusia.

Malangnya, walaupun pembangunan alat teknologi yang lebih berkuasa dan lebih boleh dipercayai adalah apa yang *sepatutnya* kita lakukan, dan apa yang hampir semua orang mahukan dan katakan mereka mahukan, ia bukanlah trajektori yang sebenarnya kita lalui.

### KBA dan superintelligence

Sejak lahirnya bidang ini, penyelidikan AI sebaliknya telah memberi tumpuan kepada matlamat yang berbeza: Kecerdasan Buatan Am. Fokus ini kini telah menjadi tumpuan syarikat-syarikat gergasi yang memimpin pembangunan AI.

Apakah KBA? Ia sering ditakrifkan secara samar-samar sebagai "AI tahap manusia," tetapi ini bermasalah: manusia yang mana, dan pada keupayaan mana ia berada di tahap manusia? Dan bagaimana pula dengan keupayaan super-manusia yang sudah dimilikinya? Cara yang lebih berguna untuk memahami KBA adalah melalui persimpangan tiga sifat utama: **A**utonomi tinggi (kebebasan bertindak), **G**eneraliti tinggi (skop luas dan kebolehsuaian), dan **K**ecerdasan tinggi (kecekapan dalam tugas kognitif). Sistem AI semasa mungkin sangat berkebolehan tetapi sempit, atau am tetapi memerlukan pengawasan manusia berterusan, atau autonomi tetapi terhad dalam skop.

KBA penuh akan menggabungkan ketiga-tiga sifat pada tahap yang menyamai atau melebihi keupayaan manusia teratas. Yang kritikal, gabungan inilah yang menjadikan manusia begitu berkesan dan begitu berbeza daripada perisian semasa; ia juga yang akan membolehkan manusia digantikan secara menyeluruh oleh sistem digital.

Walaupun kecerdasan manusia adalah istimewa, ia sama sekali bukan satu had. Sistem "superintelligent" buatan boleh beroperasi beratus kali lebih pantas, menganalisis data yang jauh lebih banyak dan memegang kuantiti yang sangat besar "dalam ingatan" sekaligus, dan membentuk agregat yang jauh lebih besar dan lebih berkesan daripada kumpulan manusia. Mereka boleh menggantikan bukan individu tetapi syarikat, negara, atau tamadun kita secara keseluruhan.

### Kita berada di ambang

Terdapat konsensus saintifik yang kuat bahawa KBA adalah *mungkin.* AI sudah melebihi prestasi manusia dalam banyak ujian am keupayaan intelek, termasuk baru-baru ini penaakulan dan penyelesaian masalah peringkat tinggi. Keupayaan yang ketinggalan – seperti pembelajaran berterusan, perancangan, kesedaran diri, dan keaslian – semuanya wujud pada tahap tertentu dalam sistem AI masa kini, dan teknik yang diketahui wujud yang berkemungkinan meningkatkan kesemuanya.

Walaupun sehingga beberapa tahun lalu ramai penyelidik melihat KBA sebagai dekad lagi, pada masa ini bukti untuk jadual waktu pendek ke KBA adalah kuat:

- "Hukum penskalaan" yang disahkan secara empirik menghubungkan input pengkomputeran kepada keupayaan AI, dan korporat sedang dalam landasan untuk meningkatkan input pengkomputeran mengikut magnitud dalam beberapa tahun akan datang. Sumber manusia dan fiskal yang didedikasikan untuk kemajuan AI kini menyamai satu dozen Projek Manhattan dan beberapa Projek Apollo.
- Korporat AI dan pemimpin mereka secara terbuka dan peribadi percaya bahawa KBA (mengikut beberapa definisi) boleh dicapai dalam beberapa tahun. Syarikat-syarikat ini mempunyai maklumat yang tidak dimiliki orang awam, termasuk sebahagian yang mempunyai generasi seterusnya sistem AI di tangan.
- Peramal pakar dengan rekod terbukti memberikan 25% kebarangkalian kepada KBA (mengikut beberapa definisi) tiba dalam 1-2 tahun, dan 50% untuk 2-5 tahun (lihat ramalan Metaculus untuk KBA ['lemah'](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) dan ['penuh'](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/)).
- Autonomi (termasuk perancangan fleksibel jarak jauh) ketinggalan dalam sistem AI, tetapi syarikat utama kini memfokuskan sumber besar mereka untuk membangunkan sistem AI autonomi dan secara tidak rasmi menamakan 2025 sebagai ["tahun agen."](https://techinformed.com/2025-informed-the-year-of-agentic-ai/)
- AI menyumbang semakin banyak kepada penambahbaikkan dirinya sendiri. Sebaik sahaja sistem AI sekompeten penyelidik AI manusia dalam melakukan penyelidikan AI, ambang kritikal untuk kemajuan pantas kepada sistem AI yang jauh lebih berkuasa akan dicapai dan berkemungkinan membawa kepada pelarian dalam keupayaan AI. (Boleh dikatakan, pelarian itu sudah bermula.)

Idea bahawa KBA yang lebih bijak daripada manusia adalah beberapa dekad lagi atau lebih sudah tidak dapat dipertahankan lagi kepada majoriti besar pakar dalam bidang ini. Perselisihan pendapat sekarang adalah mengenai berapa bulan atau tahun yang diperlukan jika kita kekal pada kursus ini. Soalan teras yang kita hadapi adalah: patutkah kita?

### Apa yang mendorong perlumbaan ke KBA

Perlumbaan ke arah KBA didorong oleh pelbagai kuasa, setiap satu menjadikan keadaan lebih berbahaya. Syarikat teknologi utama melihat KBA sebagai teknologi automasi muktamad – bukan hanya menambah pekerja manusia tetapi menggantikan mereka sebahagian besar atau sepenuhnya. Bagi syarikat, hadiahnya adalah sangat besar: peluang untuk menawan sebahagian besar daripada output ekonomi tahunan dunia sebanyak $100 trilion dengan mengautomasikan kos buruh manusia.

Negara merasa terpaksa menyertai perlumbaan ini, secara terbuka menyebut kepimpinan ekonomi dan saintifik, tetapi secara peribadi melihat KBA sebagai revolusi berpotensi dalam hal ehwal ketenteraan yang setanding dengan senjata nuklear. Ketakutan bahawa pesaing mungkin mendapat kelebihan strategik yang menentukan mewujudkan dinamik perlumbaan senjata klasik.

Mereka yang mengejar superintelligence sering menyebut visi besar: menyembuhkan semua penyakit, membalikkan penuaan, mencapai terobosan dalam tenaga dan perjalanan angkasa lepas, atau mencipta keupayaan perancangan superhuman.

Kurang baik hati, apa yang mendorong perlumbaan adalah kuasa. Setiap peserta – sama ada syarikat atau negara – percaya bahawa kecerdasan sama dengan kuasa, dan bahawa mereka akan menjadi penjaga terbaik kuasa itu.

Saya berhujah bahawa motivasi ini adalah nyata tetapi pada asasnya salah arah: KBA akan *menyerap* dan *mencari* kuasa daripada memberikannya; teknologi yang dicipta AI juga akan sangat bermata dua, dan di mana ia bermanfaat boleh dicipta dengan alat AI dan tanpa KBA; dan walaupun setakat KBA dan outputnya kekal dalam kawalan, dinamik perlumbaan ini – kedua-dua korporat dan geopolitik – menjadikan risiko berskala besar kepada masyarakat kita hampir tidak dapat dielakkan melainkan ditangguhkan secara tegas.

### KBA dan superintelligence menimbulkan ancaman dramatik kepada tamadun

Walaupun daya tarikan mereka, KBA dan superintelligence menimbulkan ancaman dramatik kepada tamadun melalui pelbagai laluan yang saling memperkukuh:

*Penumpuan kuasa:* AI superhuman boleh melucutkan kuasa majoriti besar manusia dengan menyerap sebahagian besar aktiviti sosial dan ekonomi ke dalam sistem AI yang dijalankan oleh segelintir syarikat gergasi (yang seterusnya mungkin sama ada diambil alih oleh, atau secara berkesan mengambil alih, kerajaan.)

*Gangguan besar-besaran:* automasi pukal kebanyakan pekerjaan berasaskan kognitif, penggantian sistem epistemik semasa kita, dan pelancaran bilangan besar agen bukan manusia yang aktif akan mengubah kebanyakan sistem tamadun semasa kita dalam tempoh yang agak singkat.

*Malapetaka:* dengan menyebarkan keupayaan – berpotensi melebihi tahap manusia – untuk mencipta teknologi ketenteraan dan pemusnah baru dan memisahkannya daripada sistem sosial dan undang-undang yang mendasari tanggungjawab, malapetaka fizikal daripada senjata pemusnah beramai-ramai menjadi jauh lebih berkemungkinan.

*Geopolitik dan perang:* kuasa dunia utama tidak akan duduk diam jika mereka merasakan bahawa teknologi yang boleh memberikan "kelebihan strategik yang menentukan" sedang dibangunkan oleh musuh mereka.

*Pelarian dan kehilangan kawalan:* Melainkan ia secara khusus dicegah, AI superhuman akan mempunyai setiap insentif untuk terus memperbaiki dirinya dan boleh jauh melebihi manusia dalam kelajuan, pemprosesan data, dan kecanggihan pemikiran. Tidak ada cara yang bermakna di mana kita boleh mengawal sistem sedemikian. AI sedemikian tidak akan memberikan kuasa kepada manusia; kita akan memberikan kuasa kepadanya, atau ia akan mengambilnya.

Banyak risiko ini kekal walaupun masalah teknikal "penjajaran" – memastikan AI lanjutan dengan pasti melakukan apa yang manusia mahukan – diselesaikan. AI membentangkan cabaran yang sangat besar dalam cara ia akan diuruskan, dan banyak aspek pengurusan ini menjadi sangat sukar atau tidak dapat diselesaikan apabila kecerdasan manusia dilanggar.

Paling asasnya, jenis AI am tujuan umum superhuman yang sedang diusahakan pada masa ini akan, mengikut sifatnya, mempunyai matlamat, agensi, dan keupayaan yang melebihi kita sendiri. Ia secara intrinsiknya tidak boleh dikawal – bagaimana kita boleh mengawal sesuatu yang kita tidak boleh memahami mahupun meramal? Ia bukan alat teknologi untuk kegunaan manusia, tetapi spesies kedua kecerdasan di Bumi bersama kita. Jika dibiarkan untuk terus maju, ia akan membentuk bukan hanya spesies kedua tetapi spesies pengganti.

Mungkin ia akan melayan kita dengan baik, mungkin tidak. Tetapi masa depan akan menjadi miliknya, bukan milik kita. Era manusia akan berakhir.

### Ini tidak dapat dielakkan; manusia boleh, secara sangat konkrit, memutuskan untuk tidak membina pengganti kita.

Penciptaan KBA superhuman adalah jauh daripada tidak dapat dielakkan. Kita boleh mencegahnya melalui set langkah tadbir urus yang diselaraskan:

Pertama, kita memerlukan perakaunan yang mantap dan pengawasan pengkomputeran AI ("pengkomputeran"), yang merupakan pemboleh asas dan tuil untuk mentadbir sistem AI berskala besar. Ini seterusnya memerlukan pengukuran dan pelaporan piawai jumlah pengkomputeran yang digunakan dalam melatih model AI dan menjalankannya, dan kaedah teknikal untuk mengira, mengesahkan, dan mengesahkan pengkomputeran yang digunakan.

Kedua, kita perlu melaksanakan had keras pada pengkomputeran AI, untuk latihan dan operasi; ini menghalang AI daripada menjadi terlalu berkuasa dan beroperasi terlalu pantas. Had ini boleh dilaksanakan melalui kedua-dua keperluan undang-undang dan langkah keselamatan berasaskan perkakasan yang dibina ke dalam cip khusus AI, analog dengan ciri keselamatan dalam telefon moden. Kerana perkakasan AI khusus dibuat oleh hanya segelintir syarikat, pengesahan dan penguatkuasaan boleh dilaksanakan melalui rantaian bekalan sedia ada.

Ketiga, kita memerlukan liabiliti yang dipertingkatkan untuk sistem AI yang paling berbahaya. Mereka yang membangunkan AI yang menggabungkan autonomi tinggi, generaliti luas, dan kecerdasan unggul perlu menghadapi liabiliti ketat untuk bahaya, manakala pelabuhan selamat daripada liabiliti ini akan menggalakkan pembangunan sistem yang lebih terhad dan boleh dikawal.

Keempat, kita memerlukan peraturan berperingkat berdasarkan tahap risiko. Sistem yang paling berkebolehan dan berbahaya memerlukan jaminan keselamatan dan kebolehkawalan yang meluas sebelum pembangunan dan penggunaan, manakala sistem yang kurang berkuasa atau lebih khusus akan menghadapi pengawasan yang berkadar. Rangka kerja kawal selia ini akhirnya perlu beroperasi pada kedua-dua peringkat nasional dan antarabangsa.

Pendekatan ini – dengan spesifikasi terperinci yang diberikan dalam dokumen penuh – adalah praktikal: walaupun penyelarasan antarabangsa akan diperlukan, pengesahan dan penguatkuasaan boleh berfungsi melalui bilangan kecil syarikat yang mengawal rantaian bekalan perkakasan khusus. Ia juga fleksibel: syarikat masih boleh berinovasi dan meraih keuntungan daripada pembangunan AI, hanya dengan had yang jelas pada sistem yang paling berbahaya.

Pembendungan jangka panjang kuasa dan risiko AI memerlukan perjanjian antarabangsa berdasarkan kedua-dua kepentingan diri dan bersama, sebagaimana mengawal proliferasi senjata nuklear sekarang. Tetapi kita boleh bermula serta-merta dengan pengawasan dan liabiliti yang dipertingkatkan, sambil membina ke arah tadbir urus yang lebih komprehensif.

Bahan utama yang hilang adalah kehendak politik dan sosial untuk mengambil alih proses pembangunan AI. Sumber kehendak itu, jika ia datang pada masa yang tepat, akan menjadi realiti itu sendiri – iaitu, daripada kesedaran meluas mengenai implikasi sebenar apa yang kita lakukan.

### Kita boleh merekayasa AI Alat untuk memperkasakan manusia

Daripada mengejar KBA yang tidak boleh dikawal, kita boleh membangunkan "AI Alat" yang berkuasa yang meningkatkan keupayan manusia sambil kekal dalam kawalan manusia yang bermakna. Sistem AI Alat boleh sangat berkebolehan sambil mengelakkan persimpangan berbahaya triple autonomi tinggi, generaliti luas, dan kecerdasan superhuman, selagi kita merekayasanya untuk boleh dikawal pada tahap yang sepadan dengan keupayaan mereka. Mereka juga boleh digabungkan menjadi sistem canggih yang mengekalkan pengawasan manusia sambil memberikan faedah transformatif.

AI Alat boleh merevolusikan perubatan, mempercepatkan penemuan saintifik, meningkatkan pendidikan, dan memperbaiki proses demokratik. Apabila ditadbir dengan betul, ia boleh menjadikan pakar dan institusi manusia lebih berkesan daripada menggantikan mereka. Walaupun sistem sedemikian masih akan sangat mengganggu dan memerlukan pengurusan yang teliti, risiko yang mereka timbulkan pada asasnya berbeza daripada KBA: ia adalah risiko yang boleh kita tadbir, seperti teknologi berkuasa lain, bukan ancaman existential kepada agensi manusia dan tamadun. Dan yang kritikal, apabila dibangunkan dengan bijak, alat AI boleh membantu orang mentadbir AI yang berkuasa dan mengurus kesannya.

Pendekatan ini memerlukan pemikiran semula kedua-dua cara AI dibangunkan dan cara faedahnya diagihkan. Model baru pembangunan AI awam dan bukan untung, rangka kerja kawal selia yang mantap, dan mekanisme untuk mengedarkan faedah ekonomi secara lebih luas boleh membantu memastikan AI memperkasakan manusia secara keseluruhan daripada menumpukan kuasa di beberapa tangan. AI itu sendiri boleh membantu membina institusi sosial dan tadbir urus yang lebih baik, membolehkan bentuk baru penyelarasan dan wacana yang memperkukuh daripada melemahkan masyarakat manusia. Penubuhan keselamatan negara boleh memanfaatkan kepakaran mereka untuk menjadikan sistem alat AI benar-benar selamat dan boleh dipercayai, dan sumber pertahanan sebenar serta kuasa negara.

Kita mungkin akhirnya memilih untuk membangunkan sistem yang lebih berkuasa dan lebih berdaulat yang kurang seperti alat dan – kita boleh berharap – lebih seperti dermawan yang bijak dan berkuasa. Tetapi kita perlu melakukannya hanya selepas kita membangunkan pemahaman saintifik dan kapasiti tadbir urus untuk melakukannya dengan selamat. Keputusan yang sangat penting dan tidak boleh dipulihkan sedemikian perlu dibuat secara sengaja oleh manusia secara keseluruhan, bukan secara lalai dalam perlumbaan antara syarikat teknologi dan negara.

### Di tangan manusia

Orang mahukan kebaikan yang datang daripada AI: alat berguna yang memperkasakan mereka, menggalakkan peluang dan pertumbuhan ekonomi, dan menjanjikan terobosan dalam sains, teknologi, dan pendidikan. Mengapa tidak? Tetapi apabila ditanya, majoriti besar orang awam [mahukan pembangunan AI yang lebih perlahan dan lebih berhati-hati](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation), dan tidak mahu AI yang lebih bijak daripada manusia yang akan menggantikan mereka dalam pekerjaan dan tempat lain, memenuhi budaya dan maklumat umum mereka dengan kandungan bukan manusia, menumpukan kuasa dalam set kecil korporasi, menimbulkan risiko global berskala besar yang melampau, dan akhirnya mengancam untuk melucutkan kuasa atau menggantikan spesies mereka. Mengapa mereka perlu?

Kita *boleh* mempunyai satu tanpa yang lain. Ia bermula dengan memutuskan bahawa takdir kita tidak terletak pada inevitabiliti yang dikatakan sesuatu teknologi atau di tangan beberapa CEO di Silicon Valley, tetapi di tangan kita yang lain jika kita mengambilnya. Mari tutup Pintu Gerbang, dan pastikan masa depan tetap manusiawi.

## Bab 1 - Pengenalan

Bagaimana kita akan bertindak balas terhadap prospek AI yang lebih pintar daripada manusia adalah isu paling mendesak pada zaman kita. Esei ini menyediakan jalan ke hadapan.

Kita mungkin berada di penghujung era manusia.

Sesuatu telah bermula dalam sepuluh tahun kebelakangan ini yang unik dalam sejarah spesies kita. Akibatnya akan, pada tahap yang besar, menentukan masa depan kemanusiaan. Bermula sekitar 2015, penyelidik telah berjaya membangunkan kecerdasan buatan (AI) *khusus* – sistem yang boleh menang dalam permainan seperti Go, mengenali imej dan pertuturan, dan sebagainya, lebih baik daripada mana-mana manusia.[^1]

Ini adalah kejayaan yang menakjubkan, dan ia menghasilkan sistem dan produk yang sangat berguna yang akan memperkasakan kemanusiaan. Tetapi kecerdasan buatan khusus tidak pernah menjadi matlamat sebenar bidang ini. Sebaliknya, matlamatnya adalah untuk mencipta sistem AI tujuan *am*, terutamanya yang sering dipanggil "kecerdasan buatan am" (KBA) atau "superintelligence" yang pada masa yang sama sebaik atau lebih baik daripada manusia merentasi hampir *semua* tugasan, sama seperti AI kini mengatasi manusia dalam Go, catur, poker, lumba dron, dan sebagainya. Ini adalah matlamat yang dinyatakan oleh banyak syarikat AI utama.[^2]

*Usaha-usaha ini juga sedang berjaya.* Sistem AI tujuan am seperti ChatGPT, Gemini, Llama, Grok, Claude, dan Deepseek, berdasarkan pengkomputeran besar-besaran dan data yang berlambak, telah mencapai pariti dengan manusia biasa merentasi pelbagai tugasan, dan malah menyamai pakar manusia dalam sesetengah domain. Kini jurutera AI di beberapa syarikat teknologi terbesar sedang berlumba untuk menolak eksperimen gergasi dalam kecerdasan mesin ini ke tahap seterusnya, di mana ia menyamai dan kemudian melebihi keseluruhan spektrum keupayaan, kepakaran, dan autonomi manusia.

*Ini sudah hampir tiba.* Sepanjang sepuluh tahun kebelakangan, anggaran pakar untuk berapa lama masa yang diperlukan – jika kita meneruskan laluan semasa – telah jatuh daripada beberapa dekad (atau abad) kepada tahun-tahun dalam digit tunggal.

Ia juga mempunyai kepentingan yang besar, dan risiko yang luar biasa. Penyokong KBA melihatnya sebagai transformasi positif yang akan menyelesaikan masalah saintifik, menyembuhkan penyakit, membangunkan teknologi baru, dan mengautomatikkan kerja-kerja membosankan. Dan AI sudah pasti boleh membantu mencapai semua perkara ini – malah ia sudah pun melakukannya. Tetapi selama beberapa dekad, ramai pemikir yang teliti, dari Alan Turing hingga Stephen Hawking hingga Geoffrey Hinton dan Yoshua Bengio masa kini[^3] telah mengeluarkan amaran keras: membina AI am, autonomi yang benar-benar lebih pintar daripada manusia akan sekurang-kurangnya mengubah masyarakat secara lengkap dan tidak boleh dipulihkan, dan paling maksimum mengakibatkan kepupusan manusia.[^4]

AI superintelligence sedang menghampiri dengan pantas di laluan semasa kita, tetapi ia masih jauh dari tidak dapat dielakkan. Esei ini adalah hujah yang panjang mengapa dan bagaimana kita patut *menutup Pintu Gerbang* kepada masa depan bukan manusia yang menghampiri ini, dan apa yang patut kita lakukan sebaliknya.


[^1]: [Carta](https://time.com/6300942/ai-progress-charts/) ini menunjukkan satu set tugasan; banyak lengkung serupa boleh ditambah kepada graf ini. Kemajuan pesat dalam AI khusus ini telah mengejutkan bahkan pakar dalam bidang ini, dengan penanda aras diatasi bertahun-tahun lebih awal daripada ramalan.

[^2]: Deepmind, OpenAI, Anthropic, dan X.ai semuanya diasaskan dengan matlamat khusus untuk membangunkan KBA. Contohnya, piagam OpenAI dengan jelas menyatakan matlamatnya sebagai membangunkan "kecerdasan buatan am yang memberi manfaat kepada semua kemanusiaan," manakala misi DeepMind adalah "untuk menyelesaikan kecerdasan, dan kemudian menggunakannya untuk menyelesaikan segala-galanya." Meta, Microsoft, dan lain-lain kini mengejar laluan yang hampir serupa. Meta telah menyatakan bahawa ia [merancang untuk membangunkan KBA dan mengeluarkannya secara terbuka.](https://www.forbes.com/sites/johnkoetsier/2024/01/18/zuckerberg-on-ai-meta-building-agi-for-everyone-and-open-sourcing-it/)

[^3]: Hinton dan Bengio adalah dua daripada penyelidik AI yang paling banyak dipetik, kedua-duanya telah memenangi Nobel bidang AI, Hadiah Turing, dan Hinton telah memenangi hadiah Nobel (dalam fizik) sebagai tambahan.

[^4]: Membina sesuatu dengan risiko sebegini, di bawah insentif komersial dan hampir tiada pengawasan kerajaan, adalah sama sekali tidak pernah berlaku sebelum ini. Malah tidak ada kontroversi mengenai risiko di kalangan mereka yang membinanya! Pemimpin Deepmind, OpenAI, dan Anthropic, antara ramai pakar lain, semuanya telah benar-benar menandatangani [kenyataan](https://www.safe.ai/work/statement-on-ai-risk) bahawa AI canggih menimbulkan *risiko kepupusan kepada kemanusiaan.* Loceng amaran tidak boleh berbunyi lebih kuat, dan seseorang hanya boleh membuat kesimpulan bahawa mereka yang mengabaikannya semata-mata tidak menganggap KBA dan superintelligence dengan serius. Salah satu matlamat esei ini adalah untuk membantu mereka memahami mengapa mereka patut berbuat demikian.

## Bab 2 - Perkara penting tentang rangkaian neural AI

Bagaimanakah sistem AI moden berfungsi, dan apakah yang mungkin akan datang dalam generasi AI yang seterusnya?

Untuk memahami bagaimana akibat daripada membangunkan AI yang lebih berkuasa akan berlaku, adalah penting untuk menghayati beberapa asas. Bahagian ini dan dua bahagian seterusnya akan membangunkan pemahaman tersebut, merangkumi secara bergilir-gilir tentang apa itu AI moden, bagaimana ia memanfaatkan pengkomputeran besar-besaran, dan dalam erti kata apa ia berkembang pesat dari segi keumuman dan keupayaan.[^5]

Terdapat banyak cara untuk mentakrifkan kecerdasan buatan, tetapi untuk tujuan kita, sifat utama AI ialah walaupun program komputer standard adalah senarai arahan tentang cara melaksanakan tugas, sistem AI pula adalah sistem yang belajar daripada data atau pengalaman untuk melaksanakan tugas *tanpa diberitahu secara eksplisit bagaimana untuk melakukannya.*

Hampir semua AI moden yang ketara adalah berdasarkan rangkaian neural. Ini adalah struktur matematik/pengkomputeran, yang diwakili oleh set nombor yang sangat besar (berbilion atau bertrilion) ("pemberat"), yang melaksanakan tugas latihan dengan baik. Pemberat ini dibentuk (atau mungkin "ditumbuhkan" atau "ditemui") dengan mengubahsuainya secara berulang supaya rangkaian neural meningkatkan skor berangka (dikenali sebagai "kerugian") yang ditakrifkan untuk berprestasi baik dalam satu atau lebih tugas.[^6] Proses ini dikenali sebagai *latihan* rangkaian neural.[^7]

Terdapat banyak teknik untuk melakukan latihan ini, tetapi butiran tersebut kurang relevan berbanding cara penskoran ditakrifkan, dan bagaimana ia menghasilkan tugas yang berbeza yang rangkaian neural lakukan dengan baik. Perbezaan utama secara sejarah telah dibuat antara AI "sempit" dan "am".

AI sempit sengaja dilatih untuk melakukan tugas tertentu atau set kecil tugas (seperti mengenali imej atau bermain catur); ia memerlukan latihan semula untuk tugas baru, dan mempunyai skop keupayaan yang sempit. Kita mempunyai AI sempit yang mengatasi manusia, bermakna untuk hampir mana-mana tugas diskret yang ditakrifkan dengan baik yang boleh dilakukan oleh seseorang, kita mungkin boleh membina skor dan kemudian berjaya melatih sistem AI sempit untuk melakukannya lebih baik daripada manusia.

Sistem AI tujuan am (GPAI) boleh melaksanakan pelbagai tugas, termasuk banyak yang tidak dilatih secara eksplisit untuk dilakukan; ia juga boleh mempelajari tugas baru sebagai sebahagian daripada operasinya. "Model multimodal" besar semasa[^8] seperti ChatGPT mencontohkan ini: dilatih pada korpus teks dan imej yang sangat besar, ia boleh terlibat dalam penaakulan yang kompleks, menulis kod, menganalisis imej, dan membantu dengan pelbagai tugas intelektual. Walaupun masih agak berbeza daripada kecerdasan manusia dalam cara yang akan kita lihat secara mendalam di bawah, keumumannya telah menyebabkan revolusi dalam AI.[^9]

### Ketidakramalan: ciri utama sistem AI

Perbezaan utama antara sistem AI dan perisian konvensional ialah dalam ketidakramalan. Output perisian standard boleh tidak dapat diramal – malah kadangkala itulah sebabnya kita menulis perisian, untuk memberikan kita hasil yang tidak dapat kita ramalkan. Tetapi perisian konvensional jarang melakukan apa-apa yang tidak diprogramkan untuk dilakukan – skop dan tingkah lakunya secara amnya seperti yang direka. Program catur peringkat teratas mungkin membuat gerakan yang tiada manusia boleh ramalkan (atau sebaliknya mereka boleh mengalahkan program catur tersebut!) tetapi ia secara amnya tidak akan melakukan apa-apa selain bermain catur.

Seperti perisian konvensional, AI sempit mempunyai skop dan tingkah laku yang boleh diramal tetapi boleh mempunyai hasil yang tidak dapat diramal. Ini sebenarnya hanya cara lain untuk mentakrifkan AI sempit: sebagai AI yang serupa dengan perisian konvensional dalam ketidakramalan dan julat operasinya.

AI tujuan am adalah berbeza: skopnya (domain yang ia gunakan), tingkah laku (jenis perkara yang dilakukannya), dan hasil (output sebenarnya) semuanya boleh tidak dapat diramal.[^10] GPT-4 dilatih hanya untuk menjana teks dengan tepat, tetapi membangunkan banyak keupayaan yang pelatihnya tidak ramalkan atau tidak sengajakan. Ketidakramalan ini berpunca daripada kerumitan latihan: kerana data latihan mengandungi output daripada banyak tugas berbeza, AI mestilah belajar untuk melaksanakan tugas ini dengan berkesan untuk meramal dengan baik.

Ketidakramalan sistem AI am ini agak asas. Walaupun pada dasarnya adalah mungkin untuk membina sistem AI dengan teliti yang mempunyai had yang dijamin pada tingkah laku mereka (seperti yang disebut kemudian dalam esei), cara sistem AI dicipta sekarang ia tidak dapat diramal dalam amalan dan bahkan pada dasarnya.

### AI pasif, agen, sistem autonomi, dan penjajaran

Ketidakramalan ini menjadi amat penting apabila kita mempertimbangkan bagaimana sistem AI sebenarnya digunakan dan digunakan untuk mencapai pelbagai matlamat.

Banyak sistem AI adalah agak pasif dalam erti kata ia terutamanya menyediakan maklumat, dan pengguna mengambil tindakan. Yang lain, biasanya dipanggil *agen*, mengambil tindakan sendiri, dengan pelbagai tahap penglibatan daripada pengguna. Yang mengambil tindakan dengan input atau pengawasan luar yang agak kurang boleh dipanggil lebih *autonomi*. Ini membentuk spektrum dari segi kebebasan tindakan, daripada alat pasif kepada agen autonomi.[^11]

Mengenai matlamat sistem AI, ini mungkin terikat secara langsung dengan objektif latihan mereka (contohnya matlamat "menang" untuk sistem yang bermain Go juga secara eksplisit adalah apa yang ia dilatih untuk lakukan). Atau ia mungkin tidak: objektif latihan ChatGPT sebahagiannya adalah untuk meramal teks, sebahagiannya untuk menjadi pembantu yang berguna. Tetapi apabila melakukan tugas tertentu, matlamatnya dibekalkan kepadanya oleh pengguna. Matlamat juga mungkin dicipta oleh sistem AI itu sendiri, hanya secara tidak langsung berkaitan dengan objektif latihannya.[^12]

Matlamat berkait rapat dengan persoalan "penjajaran," iaitu persoalan sama ada sistem AI akan *melakukan apa yang kita mahu mereka lakukan*. Persoalan mudah ini menyembunyikan tahap kehalusan yang besar.[^13] Buat masa ini, ambil perhatian bahawa "kita" dalam ayat ini mungkin merujuk kepada banyak orang dan kumpulan yang berbeza, membawa kepada jenis penjajaran yang berbeza. Contohnya, AI mungkin sangat *patuh* (atau ["setia"](https://arxiv.org/abs/2003.11157)) kepada penggunanya – di sini "kita" adalah "setiap daripada kita." Atau ia mungkin lebih *berdaulat*, didorong terutamanya oleh matlamat dan kekangan sendiri, tetapi masih bertindak secara meluas untuk kepentingan bersama kesejahteraan manusia – "kita" kemudiannya adalah "manusia" atau "masyarakat." Di tengah-tengah adalah spektrum di mana AI akan sebahagian besarnya patuh, tetapi mungkin enggan mengambil tindakan yang membahayakan orang lain atau masyarakat, melanggar undang-undang, dan sebagainya.

Kedua-dua paksi ini – tahap autonomi dan jenis penjajaran – tidak sepenuhnya bebas. Contohnya, sistem pasif berdaulat, walaupun tidak agak bercanggah sendiri, adalah konsep dalam ketegangan, begitu juga dengan agen autonomi yang patuh.[^14] Terdapat pengertian yang jelas bahawa autonomi dan kedaulatan cenderung untuk berjalan seiring. Dalam nada yang sama, ketidakramalan cenderung lebih tinggi dalam sistem AI "pasif" dan "patuh", manakala yang berdaulat atau autonomi akan cenderung lebih tidak dapat diramal. Semua ini akan menjadi penting untuk memahami kesan KBA dan superintelligence yang berpotensi.

Mencipta AI yang benar-benar sejajar, dalam apa jua bentuk, memerlukan penyelesaian tiga cabaran berbeza:

1. Memahami apa yang "kita" mahu – yang rumit sama ada "kita" bermaksud orang atau organisasi tertentu (kesetiaan) atau manusia secara meluas (kedaulatan);
2. Membina sistem yang kerap bertindak selaras dengan kehendak tersebut – pada asasnya mencipta tingkah laku positif yang konsisten;
3. Paling asasnya, membuat sistem yang benar-benar "mengambil berat" tentang kehendak tersebut daripada hanya bertindak seolah-olah mereka berbuat demikian.

Perbezaan antara tingkah laku yang boleh dipercayai dan kepedulian tulen adalah penting. Sama seperti pekerja manusia mungkin mengikut arahan dengan sempurna sementara kekurangan komitmen sebenar terhadap misi organisasi, sistem AI mungkin bertindak sejajar tanpa benar-benar menghargai keutamaan manusia. Kita boleh melatih sistem AI untuk berkata dan melakukan perkara melalui maklum balas, dan mereka boleh belajar untuk berfikir tentang apa yang manusia mahu. Tetapi membuat mereka *benar-benar* menghargai keutamaan manusia adalah cabaran yang jauh lebih mendalam.[^15]

Kesukaran yang mendalam dalam menyelesaikan cabaran penjajaran ini, dan implikasinya untuk risiko AI, akan diterokai lebih lanjut di bawah. Buat masa ini, fahami bahawa penjajaran bukan hanya ciri teknikal yang kita tampalkan kepada sistem AI, tetapi aspek asas seni bina mereka yang membentuk hubungan mereka dengan manusia.


[^5]: Untuk pengenalan yang lembut tetapi teknikal kepada pembelajaran mesin dan AI, terutamanya model bahasa, lihat [laman web ini.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) Untuk primer moden lain tentang risiko kepupusan AI, lihat [tulisan ini.](https://www.thecompendium.ai/) Untuk analisis saintifik yang komprehensif dan berwibawa tentang keadaan keselamatan AI, lihat [Laporan Keselamatan AI Antarabangsa](https://arxiv.org/abs/2501.17805) baru-baru ini.

[^6]: Latihan biasanya berlaku dengan mencari maksimum tempatan skor dalam ruang berdimensi tinggi yang diberikan oleh pemberat model. Dengan memeriksa bagaimana skor berubah apabila pemberat diubahsuai, algoritma latihan mengenal pasti pengubahsuaian mana yang paling meningkatkan skor, dan menggerakkan pemberat ke arah tersebut.

[^7]: Contohnya, dalam masalah pengecaman imej, rangkaian neural akan mengeluarkan kebarangkalian untuk label untuk imej. Skor akan berkaitan dengan kebarangkalian yang AI berikan kepada jawapan yang betul. Prosedur latihan kemudian akan menyesuaikan pemberat supaya lain kali, AI akan mengeluarkan kebarangkalian yang lebih tinggi untuk label yang betul untuk imej tersebut. Ini kemudian diulang sebilangan besar kali. Prosedur asas yang sama digunakan dalam melatih pada dasarnya semua rangkaian neural moden, walaupun dengan mekanisme penskoran yang lebih kompleks.

[^8]: Kebanyakan model multimodal menggunakan seni bina "transformer" untuk memproses dan menjana pelbagai jenis data (teks, imej, bunyi). Ini semua boleh dihuraikan kepada, dan kemudian diperlakukan atas dasar yang sama, sebagai jenis "token" yang berbeza. Model multimodal dilatih pertama untuk meramal token dengan tepat dalam set data besar, kemudian diperhalusi melalui pembelajaran pengukuhan untuk meningkatkan keupayaan dan membentuk tingkah laku.

[^9]: Bahawa model bahasa dilatih untuk melakukan satu perkara – meramal perkataan – telah menyebabkan sesetengah orang memanggil mereka AI sempit. Tetapi ini mengelirukan: kerana meramal teks dengan baik memerlukan begitu banyak keupayaan berbeza, tugas latihan ini membawa kepada sistem yang mengejutkan umum. Juga ambil perhatian bahawa sistem ini dilatih secara meluas oleh pembelajaran pengukuhan, dengan berkesan mewakili ribuan orang yang memberikan model isyarat ganjaran apabila ia melakukan kerja yang baik pada mana-mana daripada banyak perkara yang dilakukannya. Ia kemudian mewarisi keumuman yang ketara daripada orang yang memberikan maklum balas ini.

[^10]: Terdapat pelbagai cara AI tidak dapat diramal. Salah satunya ialah dalam kes am seseorang tidak boleh meramal apa yang algoritma akan lakukan tanpa benar-benar menjalankannya; terdapat [teorem](https://arxiv.org/abs/1310.3225) untuk kesan ini. Ini boleh benar hanya kerana output algoritma boleh menjadi kompleks. Tetapi ia amat jelas dan relevan dalam kes (seperti dalam catur atau Go) di mana ramalan akan membayangkan keupayaan (mengalahkan AI) yang tidak dimiliki oleh peramal yang akan menjadi. Kedua, sistem AI tertentu tidak akan sentiasa menghasilkan output yang sama walaupun diberikan input yang sama – outputnya mengandungi rawak; ini juga diganding dengan ketidakramalan algoritma. Ketiga, keupayaan yang tidak dijangka dan muncul boleh timbul daripada latihan, bermakna walaupun *jenis* perkara yang sistem AI boleh dan akan lakukan tidak dapat diramal; Jenis terakhir ini amat penting untuk pertimbangan keselamatan.

[^11]: Lihat [di sini](https://arxiv.org/abs/2502.02649) untuk kajian mendalam tentang apa yang dimaksudkan dengan "agen autonomi" (bersama-sama dengan hujah etika terhadap membina mereka).

[^12]: Anda mungkin kadangkala mendengar "AI tidak boleh mempunyai matlamat sendiri." Ini adalah karut mutlak. Adalah mudah untuk menjana contoh di mana AI mempunyai atau membangunkan matlamat yang tidak pernah diberikan kepadanya dan hanya diketahui oleh dirinya sendiri. Anda tidak melihat ini banyak dalam model multimodal popular semasa kerana ia dilatih daripada mereka; ia boleh sama mudahnya dilatih ke dalam mereka.

[^13]: Terdapat kesusasteraan yang besar. Mengenai masalah am lihat [*The Alignment Problem*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821) Christian, dan [*Human-Compatible*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) Russell. Pada sisi yang lebih teknikal lihat contohnya [kertas ini](https://arxiv.org/abs/2209.00626).

[^14]: Kita akan kemudian melihat bahawa walaupun sistem sedemikian melawan trend, itu sebenarnya menjadikan mereka sangat menarik dan berguna.

[^15]: Ini bukan bermakna kita memerlukan emosi atau kesedaran. Sebaliknya, adalah amat sukar dari luar sistem untuk mengetahui apa matlamat dalaman, keutamaan, dan nilainya. "Tulen" di sini bermakna kita mempunyai alasan yang cukup kuat untuk bergantung kepadanya bahawa dalam kes sistem kritikal kita boleh mempertaruhkan nyawa kita kepadanya.

## Bab 3 - Aspek utama bagaimana sistem AI am moden dibina

Kebanyakan sistem AI termaju di dunia dibina menggunakan kaedah yang mengejutkan serupa. Berikut adalah asas-asasnya.

Untuk benar-benar memahami manusia, anda perlu tahu sesuatu tentang biologi, evolusi, pembesaran anak, dan lain-lain; untuk memahami AI anda juga perlu tahu tentang bagaimana ia dibina. Sepanjang lima tahun yang lalu, sistem AI telah berkembang pesat dari segi keupayaan dan kerumitan. Faktor utama yang membolehkan ini ialah ketersediaan jumlah pengkomputeran yang sangat besar (atau secara bahasa sehari-hari "compute" apabila digunakan untuk AI).

Angka-angka ini mengagumkan. Kira-kira 10 <sup>25</sup> -10 <sup>26</sup> "operasi titik terapung" (FLOP) [^16] digunakan dalam latihan model seperti siri GPT, Claude, Gemini, dan sebagainya.[^17] (Sebagai perbandingan, jika setiap manusia di Bumi bekerja tanpa henti melakukan satu pengiraan setiap lima saat, ia akan mengambil masa kira-kira satu bilion tahun untuk mencapai ini.) Jumlah pengkomputeran yang besar ini membolehkan latihan model dengan sehingga trilion pemberat model pada terabait data – sebahagian besar daripada semua teks berkualiti yang pernah ditulis bersama-sama dengan perpustakaan besar bunyi, imej dan video. Melengkapi latihan ini dengan latihan tambahan yang menguatkan keutamaan manusia dan prestasi tugasan yang baik, model yang dilatih dengan cara ini mempamerkan prestasi yang setanding manusia merentasi pelbagai tugasan intelektual asas, termasuk penaakulan dan penyelesaian masalah.

Kami juga tahu (secara sangat, sangat kasar) berapa banyak kelajuan pengkomputeran, dalam operasi sesaat, yang mencukupi untuk kelajuan *inferens* [^18] sistem sedemikian sepadan dengan *kelajuan* pemprosesan teks manusia. Ia adalah kira-kira 10 <sup>15</sup> -10 <sup>16</sup> FLOP sesaat.[^19]

Walaupun berkuasa, model-model ini pada dasarnya terhad dalam cara-cara utama, agak serupa dengan bagaimana seorang manusia akan terhad jika dipaksa untuk hanya menghasilkan teks pada kadar perkataan tetap seminit, tanpa berhenti untuk berfikir atau menggunakan sebarang alat tambahan. Sistem AI yang lebih terkini menangani had ini melalui proses dan seni bina yang lebih kompleks menggabungkan beberapa elemen utama:

- Satu atau lebih rangkaian neural, dengan satu model menyediakan kapasiti kognitif teras, dan sehingga beberapa yang lain melaksanakan tugasan lain yang lebih sempit;
- *Alatan* yang disediakan kepada dan boleh digunakan oleh model – contohnya keupayaan untuk mencari web, mencipta atau mengedit dokumen, melaksanakan program, dan sebagainya.
- *Perancah* yang menghubungkan input dan output rangkaian neural. Perancah yang sangat mudah mungkin hanya membenarkan dua "contoh" model AI untuk bercakap antara satu sama lain, atau satu untuk memeriksa kerja yang lain.[^20]
- *Rantai pemikiran* dan teknik prompting berkaitan melakukan sesuatu yang serupa, menyebabkan model untuk contohnya menghasilkan banyak pendekatan kepada masalah, kemudian memproses pendekatan tersebut untuk jawapan agregat.
- *Latihan semula* model untuk menggunakan alatan, perancah, dan rantai pemikiran dengan lebih baik.

Kerana sambungan ini boleh menjadi sangat berkuasa (dan termasuk sistem AI itu sendiri), sistem komposit ini boleh menjadi agak canggih dan meningkatkan keupayaan AI secara dramatik.[^21] Dan baru-baru ini, teknik dalam perancah dan terutamanya prompting rantai pemikiran (dan melipat semula keputusan ke dalam latihan semula model untuk menggunakan ini dengan lebih baik) telah dibangunkan dan digunakan dalam [o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/), dan [DeepSeek R1](https://api-docs.deepseek.com/news/news250120) untuk melakukan banyak lintasan inferens sebagai tindak balas kepada pertanyaan yang diberikan.[^22] Ini membolehkan model untuk "berfikir tentang" responsnya dan meningkatkan keupayaan model ini secara dramatik untuk melakukan penaakulan berkaliber tinggi dalam tugasan sains, matematik, dan pengaturcaraan.[^23]

Untuk seni bina AI yang diberikan, peningkatan dalam pengkomputeran latihan [boleh diterjemahkan dengan boleh dipercayai](https://arxiv.org/abs/2405.10938) kepada penambahbaikan dalam set metrik yang ditakrifkan dengan jelas. Untuk keupayaan am yang kurang jelas ditakrifkan (seperti yang dibincangkan di bawah), terjemahan adalah kurang jelas dan ramalan, tetapi hampir pasti bahawa model yang lebih besar dengan lebih banyak pengkomputeran latihan akan mempunyai keupayaan baru dan lebih baik, walaupun sukar untuk meramalkan apa yang akan berlaku.

Begitu juga, sistem komposit dan terutamanya kemajuan dalam "rantai pemikiran" (dan latihan model yang berfungsi dengan baik dengannya) telah membuka kunci penskalaan dalam pengkomputeran *inferens*: untuk model teras yang dilatih yang diberikan, sekurang-kurangnya beberapa keupayaan sistem AI meningkat apabila lebih banyak pengkomputeran digunakan yang membolehkan mereka "berfikir dengan lebih keras dan lebih lama" tentang masalah yang kompleks. Ini datang dengan kos kelajuan pengkomputeran yang curam, memerlukan beratus-ratus atau beribu-ribu lagi FLOP/s untuk sepadan dengan prestasi manusia.[^24]

Walaupun hanya sebahagian daripada apa yang membawa kepada kemajuan AI yang pesat,[^25] peranan pengkomputeran dan kemungkinan sistem komposit akan terbukti penting untuk kedua-dua mencegah KBA yang tidak dapat dikawal dan membangunkan alternatif yang lebih selamat.


[^16]: 10 <sup>27</sup> bermaksud 1 diikuti oleh 25 sifar, atau sepuluh trilion trilion. FLOP hanyalah penambahan atau pendaraban aritmetik nombor dengan beberapa ketepatan. Perhatikan bahawa prestasi perkakasan AI boleh berbeza-beza dengan faktor sepuluh bergantung kepada ketepatan aritmetik dan seni bina komputer. Mengira operasi pintu logik (ANDS, ORS, AND NOTS) akan menjadi fundamental tetapi ini tidak tersedia atau diukur secara umum; untuk tujuan sekarang adalah berguna untuk menstandard operasi 16-bit (FP16), walaupun faktor penukaran yang sesuai harus diwujudkan.

[^17]: Koleksi anggaran dan data keras tersedia dari [Epoch AI](https://epochai.org/data/large-scale-ai-models) dan menunjukkan kira-kira 2×10 <sup>25</sup> 16-bit FLOP untuk GPT-4; ini kira-kira sepadan dengan [nombor yang bocor](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) untuk GPT-4. Anggaran untuk model pertengahan 2024 lain semuanya dalam faktor beberapa GPT-4.

[^18]: Inferens hanyalah proses menghasilkan output dari rangkaian neural. Latihan boleh dianggap sebagai penggantian banyak inferens dan pelarasan pemberat model.

[^19]: Untuk pengeluaran teks, GPT-4 asal memerlukan 560 TFLOP setiap token yang dihasilkan. Kira-kira 7 token/s diperlukan untuk mengikuti pemikiran manusia, jadi ini memberikan ≈3×10 <sup>15</sup> FLOP/s. Tetapi kecekapan telah menurunkan ini; [risalah NVIDIA ini](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/) contohnya menunjukkan serendah 3×10 <sup>14</sup> FLOP/s untuk model Llama 405B yang berprestasi setanding.

[^20]: Sebagai contoh yang sedikit lebih kompleks, sistem AI mungkin pertama menghasilkan beberapa penyelesaian yang mungkin untuk masalah matematik, kemudian menggunakan contoh lain untuk memeriksa setiap penyelesaian, dan akhirnya menggunakan yang ketiga untuk mensintesis keputusan ke dalam penjelasan yang jelas. Ini membolehkan penyelesaian masalah yang lebih menyeluruh dan boleh dipercayai daripada satu lintasan.

[^21]: Lihat contohnya butiran tentang ["Operator" OpenAI](https://openai.com/index/introducing-operator/), [keupayaan alat Claude](https://docs.anthropic.com/en/docs/build-with-claude/computer-use), dan [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). [Deep Research](https://openai.com/index/introducing-deep-research/) OpenAI mungkin mempunyai seni bina yang agak canggih tetapi butiran tidak tersedia.

[^22]: Deepseek R1 bergantung pada latihan berulang dan prompting model supaya model latihan akhir menghasilkan penaakulan rantai pemikiran yang meluas. Butiran seni bina tidak tersedia untuk o1 atau o3, namun Deepseek telah mendedahkan bahawa tiada "sos istimewa" tertentu diperlukan untuk membuka kunci penskalaan keupayaan dengan inferens. Tetapi walaupun menerima banyak akhbar sebagai mengubah "status quo" dalam AI, ia tidak mempengaruhi tuntutan teras esei ini.

[^23]: Model ini mengatasi model standard dengan ketara pada penanda aras penaakulan. Contohnya, dalam Penanda Aras GPQA Diamond—ujian ketat soalan sains peringkat PhD—GPT-4o [mendapat skor](https://openai.com/index/learning-to-reason-with-llms/) 56%, sementara o1 dan o3 mencapai 78% dan 88%, masing-masing, jauh melebihi skor purata 70% pakar manusia.

[^24]: O3 OpenAI mungkin membelanjakan ∼10 <sup>21</sup> -10 <sup>22</sup> FLOP [untuk menyelesaikan setiap soalan cabaran ARC-AGI](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai), yang manusia yang cekap boleh lakukan dalam (katakan) 10-100 saat, memberikan angka lebih seperti ∼10 <sup>20</sup> FLOP/s.

[^25]: Walaupun pengkomputeran adalah ukuran utama keupayaan sistem AI, ia berinteraksi dengan kedua-dua kualiti data dan penambahbaikan algoritma. Data atau algoritma yang lebih baik boleh mengurangkan keperluan pengkomputeran, sementara lebih banyak pengkomputeran kadang-kadang boleh mengimbangi data atau algoritma yang lemah.

## Bab 4 - Apakah KBA dan superintelligence?

Apakah sebenarnya yang sedang diperlumbakan oleh syarikat teknologi terbesar dunia untuk dibina di sebalik pintu tertutup?

Istilah "kecerdasan buatan am" telah wujud sejak sekian lama untuk menunjukkan AI serba guna "tahap manusia". Ia tidak pernah menjadi istilah yang jelas takrifannya, tetapi dalam tahun-tahun kebelakangan ini secara paradoksnya ia menjadi tidak lebih jelas takrifannya namun lebih penting, dengan pakar-pakar secara serentak berdebat sama ada KBA masih beberapa dekad lagi atau sudah tercapai, dan syarikat-syarikat bernilai trilion dolar berlumba "ke arah KBA." (Kekaburan "KBA" telah diserlahkan baru-baru ini apabila [dokumen yang bocor dilaporkan mendedahkan](https://gizmodo.com/leaked-documents-show-openai-has-a-very-clear-definition-of-agi-2000543339) bahawa dalam kontrak OpenAI dengan Microsoft, KBA ditakrifkan sebagai AI yang menjana pendapatan $100 bilion untuk OpenAI – suatu takrif yang agak lebih berorientasikan wang daripada bersifat intelektual.)

Terdapat dua masalah teras dengan idea AI yang mempunyai "kecerdasan tahap manusia." Pertama, manusia sangat, sangat berbeza dalam kebolehan mereka untuk melakukan sebarang jenis kerja kognitif yang diberikan, jadi tidak wujud "tahap manusia." Kedua, kecerdasan adalah sangat pelbagai dimensi; walaupun mungkin ada korelasi, ia tidak sempurna dan mungkin agak berbeza dalam AI. Jadi walaupun kita dapat mentakrifkan "tahap manusia" untuk banyak keupayaan, AI sudah tentu akan jauh melebihinya dalam sesetengah perkara walaupun agak di bawahnya dalam perkara lain.[^26]

Walau bagaimanapun, amat penting untuk dapat membincangkan jenis, tahap, dan ambang keupayaan AI. Pendekatan yang diambil di sini adalah untuk menekankan bahawa AI serba guna telah wujud, dan ia datang – dan akan datang – pada pelbagai tahap keupayaan di mana mudah untuk melampirkan istilah walaupun ia mengurangkan, kerana ia sepadan dengan ambang penting dari segi kesan AI ke atas masyarakat dan manusia.

Kita akan mentakrifkan KBA "penuh" sebagai sinonim kepada "AI serba guna super-manusia" bermakna sistem AI yang mampu melaksanakan pada asasnya semua tugas kognitif manusia pada atau melebihi tahap pakar manusia teratas, serta memperoleh kemahiran baharu dan memindahkan keupayaan ke domain baharu. Ini selaras dengan bagaimana "KBA" sering ditakrifkan dalam literatur moden. Penting untuk dicatat bahawa ini adalah ambang yang *sangat* tinggi. Tiada manusia mempunyai jenis kecerdasan ini; sebaliknya ia adalah jenis kecerdasan yang akan dimiliki oleh kumpulan besar pakar manusia teratas jika digabungkan. Kita boleh menamakan "superintelligence" sebagai keupayaan yang melampaui ini, dan mentakrifkan tahap keupayaan yang lebih terhad dengan GPAI "berdaya saing manusia" dan "berdaya saing pakar", yang melaksanakan pelbagai tugas pada tahap profesional biasa, atau tahap pakar manusia.[^27]

Istilah-istilah ini dan beberapa yang lain dikumpulkan dalam [jadual](https://keepthefuturehuman.ai/essay/docs/#tab:terms) di bawah. Untuk gambaran yang lebih konkrit tentang apa yang dapat dilakukan oleh pelbagai gred sistem, adalah berguna untuk mengambil takrifan secara serius dan mempertimbangkan apa yang dimaksudkannya.

| Jenis AI                     | Istilah Berkaitan                    | Takrif                                                                                                                                                                                                                           | Contoh                                                                                                                                        |
| ---------------------------- | ------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------- |
| AI Sempit                    | AI Lemah                             | AI yang dilatih untuk tugas tertentu atau keluarga tugas. Cemerlang dalam domainnya tetapi kekurangan kecerdasan am atau keupayaan pembelajaran pemindahan.                                                                     | Perisian pengecaman imej; Pembantu suara (cth., Siri, Alexa); Program bermain catur; AlphaFold DeepMind                                     |
| AI Alat                      | Kecerdasan Diperkukuh, Pembantu AI   | (Dibincangkan kemudian dalam esei.) Sistem AI yang meningkatkan keupayaan manusia. Menggabungkan AI serba guna berdaya saing manusia, AI sempit, dan kawalan terjamin, mengutamakan keselamatan dan kerjasama. Menyokong pembuatan keputusan manusia. | Pembantu pengaturcaraan canggih; Alat penyelidikan berkuasa AI; Platform analisis data canggih. Agen yang cekap tetapi sempit dan boleh dikawal |
| AI Serba Guna (GPAI)         |                                      | Sistem AI yang boleh menyesuaikan diri kepada pelbagai tugas, termasuk yang tidak dilatih secara khusus.                                                                                                                        | Model bahasa (cth., GPT-4, Claude); Model AI multimodal; MuZero DeepMind                                                                     |
| GPAI Berdaya Saing Manusia   | KBA \[lemah\]                        | AI serba guna yang melaksanakan tugas pada tahap manusia biasa, kadang-kadang melebihinya.                                                                                                                                      | Model bahasa canggih (cth., O1, Claude 3.5); Sesetengah sistem AI multimodal                                                                |
| GPAI Berdaya Saing Pakar     | KBA \[separa\]                       | AI serba guna yang melaksanakan kebanyakan tugas pada tahap pakar manusia, dengan autonomi yang ketara tetapi terhad                                                                                                           | Mungkin O3 yang berperkakas dan berperancah, sekurang-kurangnya untuk matematik, pengaturcaraan, dan beberapa sains keras                  |
| KBA \[penuh\]                | GPAI Super-manusia                   | Sistem AI yang mampu secara autonomi melaksanakan kira-kira semua tugas intelektual manusia pada atau melebihi tahap pakar, dengan pembelajaran yang cekap dan pemindahan pengetahuan.                                       | \[Tiada contoh semasa – teoritikal\]                                                                                                         |
| Superintelligence            | GPAI Sangat Super-manusia            | Sistem AI yang jauh mengatasi keupayaan manusia merentas semua domain, mengatasi kepakaran kolektif manusia. Prestasi unggul ini boleh dalam keamnya, kualiti, kelajuan, dan/atau ukuran lain.                              | \[Tiada contoh semasa – teoritikal\]                                                                                                         |

Kita sudah mengalami seperti apa mempunyai GPAI sehingga tahap berdaya saing manusia. Ini telah berintegrasi secara relatif lancar, kerana kebanyakan pengguna mengalami ini sebagai mempunyai pekerja sementara yang pintar tetapi terhad yang menjadikan mereka lebih produktif dengan kesan bercampur-campur terhadap kualiti kerja mereka.[^28]

Apa yang akan berbeza tentang GPAI berdaya saing pakar ialah ia tidak akan mempunyai had teras AI masa kini, dan akan melakukan perkara yang dilakukan oleh pakar: kerja bebas yang bernilai ekonomi, penciptaan pengetahuan sebenar, kerja teknikal yang boleh dipercayai, sambil jarang (walaupun masih sekali-sekala) membuat kesilapan bodoh.

Idea KBA penuh ialah ia *benar-benar melakukan* semua perkara kognitif yang dilakukan oleh manusia yang paling berkebolehan dan berkesan, secara autonomi dan tanpa memerlukan bantuan atau pengawasan. Ini termasuk perancangan canggih, mempelajari kemahiran baharu, menguruskan projek kompleks, dan lain-lain. Ia boleh melakukan penyelidikan termaju yang asli. Ia boleh menjalankan syarikat. Apa sahaja pekerjaan anda, jika ia kebanyakannya dilakukan melalui komputer atau telefon, *ia boleh melakukannya sekurang-kurangnya sebaik anda.* Dan mungkin dengan lebih pantas dan lebih murah. Kita akan membincangkan beberapa implikasinya di bawah, tetapi buat masa ini cabaran untuk anda adalah untuk benar-benar mengambil ini secara serius. Bayangkan sepuluh orang paling berpengetahuan dan cekap yang anda kenali atau tahu – termasuk CEO, saintis, profesor, jurutera terbaik, psikologi, pemimpin politik, dan penulis. Gabungkan mereka semua menjadi satu, yang juga bertutur dalam 100 bahasa, mempunyai ingatan yang luar biasa, beroperasi dengan pantas, tidak pernah letih dan sentiasa bermotivasi, dan bekerja di bawah gaji minimum.[^29] Itulah gambaran tentang apa yang akan menjadi KBA.

Untuk superintelligence, imaginasi adalah lebih sukar, kerana idea ialah ia boleh melakukan prestasi intelektual yang tidak dapat dilakukan oleh manusia mahupun kumpulan manusia – ia mengikut takrifan tidak dapat diramalkan oleh kita. Tetapi kita boleh mendapat gambaran. Sebagai garis dasar asas, pertimbangkan banyak KBA, setiap satu jauh lebih berkebolehan daripada pakar manusia teratas, berjalan pada 100 kali kelajuan manusia, dengan ingatan yang besar dan kapasiti koordinasi yang hebat.[^30] Dan ia terus meningkat dari situ. Berurusan dengan superintelligence akan kurang seperti bercakap dengan minda yang berbeza, lebih seperti berunding dengan tamadun yang berbeza (dan lebih maju).

Jadi betapa dekat *kita* dengan KBA dan superintelligence?


[^26]: Sebagai contoh, sistem AI semasa jauh melebihi keupayaan manusia dalam aritmetik pantas atau tugas ingatan, sambil ketinggalan dalam penaakulan abstrak dan penyelesaian masalah kreatif.

[^27]: Yang sangat penting, sebagai pesaing, AI sedemikian akan mempunyai beberapa kelebihan struktur utama termasuk: ia tidak akan letih atau mempunyai keperluan individu lain seperti manusia; ia boleh dijalankan pada kelajuan yang lebih tinggi hanya dengan menskalakan kuasa pengkomputeran; ia boleh disalin bersama-sama dengan sebarang kepakaran atau pengetahuan yang diperolehnya – dan pengetahuan yang diperoleh rangkaian neural bahkan boleh "digabungkan" untuk memindahkan keseluruhan set kemahiran antara mereka; ia boleh berkomunikasi pada kelajuan mesin; dan ia boleh mengubah atau memperbaiki diri dengan cara yang lebih ketara dan kelajuan yang lebih tinggi daripada mana-mana manusia.

[^28]: Jika anda belum menghabiskan masa menggunakan sistem AI terkini yang terbaik, saya mengesyorkannya: ia benar-benar berguna dan berkebolehan, dan ia juga penting untuk mengkalibrasi kesan AI akan ada apabila ia menjadi lebih berkuasa.

[^29]: Pertimbangkan hospital penyelidikan utama: KBA yang sepenuhnya direalisasikan boleh secara serentak menganalisis semua data pesakit yang masuk, mengikuti setiap kertas perubatan baharu, mencadangkan diagnosis, mereka bentuk pelan rawatan, menguruskan ujian klinikal, dan menyelaraskan penjadualan kakitangan – semua sambil beroperasi pada tahap yang menyamai atau melebihi pakar terbaik hospital dalam setiap bidang. Dan ia boleh melakukan ini untuk beberapa hospital secara serentak, pada sebahagian kecil daripada kos semasa. Malangnya, anda juga mesti mempertimbangkan sindiket jenayah terancang: KBA yang sepenuhnya direalisasikan boleh secara serentak menggodam, menyamar, mengintip, dan memeras ugut ribuan mangsa, mengikuti penguatkuasaan undang-undang (yang mengautomasikan dengan lebih perlahan), mereka bentuk skim menjana wang baharu, dan menyelaraskan penjadualan kakitangan – jika ada sebarang kakitangan.

[^30]: Dalam [eseinya](https://darioamodei.com/machines-of-loving-grace), Dario Amodei, CEO Anthropic, menyebut tentang "Negara \[sejuta\] genius".

## Bab 5 - Di ambang pintu

Laluan dari sistem AI hari ini kepada KBA yang lengkap kelihatan mengejutkan singkat dan boleh diramal.

Sepuluh tahun yang lalu telah menyaksikan kemajuan dramatik dalam AI yang didorong oleh sumber [pengkomputeran](https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year), manusia, dan [fiskal](https://arxiv.org/abs/2405.21015) yang besar. Banyak aplikasi AI sempit lebih baik daripada manusia dalam tugas yang diberikan, dan tentunya jauh lebih pantas dan murah.[^31] Dan terdapat juga agen sempit super-manusia yang boleh mengatasi semua orang dalam permainan domain sempit seperti [Go](https://www.nature.com/articles/nature16961), [Catur](https://arxiv.org/abs/1712.01815), dan [Poker](https://www.deepstack.ai/), serta [agen yang lebih am](https://deepmind.google/discover/blog/a-generalist-agent/) yang boleh merancang dan melaksanakan tindakan dalam persekitaran simulasi yang dipermudahkan seefektif manusia.

Yang paling menonjol, sistem AI am semasa dari OpenAI/Microsoft, Google/Deepmind, Anthropic/Amazon, Facebook/Meta, X.ai/Tesla dan lain-lain [^32] telah muncul sejak awal 2023 dan terus meningkatkan keupayaan mereka (walaupun tidak sekata) sejak itu. Kesemua ini dicipta melalui ramalan token pada dataset teks dan multimedia yang besar, digabungkan dengan maklum balas penguatan yang luas dari manusia dan sistem AI lain. Sesetengah daripadanya juga termasuk sistem alat dan perancah yang luas.

### Kekuatan dan kelemahan sistem am semasa

Sistem ini berprestasi baik merentasi julat ujian yang semakin luas yang direka untuk mengukur kecerdasan dan kepakaran, dengan kemajuan yang telah mengejutkan pakar dalam bidang ini:

- Ketika pertama kali dikeluarkan, GPT-4 [menyamai atau melebihi prestasi manusia biasa](https://arxiv.org/abs/2303.08774) dalam ujian akademik standard termasuk SAT, GRE, peperiksaan masuk, dan peperiksaan peguam. Model yang lebih terkini berkemungkinan berprestasi jauh lebih baik, walaupun keputusan tidak tersedia untuk umum.
- Ujian Turing – lama dianggap sebagai penanda aras utama untuk AI "sebenar" – kini rutin dilalui dalam beberapa bentuk oleh model bahasa moden, baik secara tidak formal mahupun dalam [kajian formal](https://arxiv.org/abs/2405.08007).[^33]
- Dalam penanda aras MMLU yang komprehensif merangkumi 57 subjek akademik, [model terkini mencapai skor tahap pakar domain](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu) (∼90%) [^34]
- Kepakaran teknikal telah maju secara dramatik: Penanda aras GPQA fizik tahap siswazah menyaksikan [prestasi melonjak](https://epoch.ai/data/ai-benchmarking-dashboard) dari hampir meneka rawak (GPT-4, 2022) kepada tahap pakar (o1-preview, 2024).
- Malah ujian yang direka khas untuk tahan AI sedang jatuh: O3 OpenAI [dilaporkan](https://www.nextbigfuture.com/2024/12/openai-releases-o3-model-with-high-performance-and-high-cost.html) menyelesaikan penanda aras penyelesaian masalah abstrak ARC-AGI pada tahap manusia, mencapai prestasi pengekodan pakar terbaik, dan meraih 25% dalam masalah "matematik sempadan" Epoch AI yang direka untuk mencabar ahli matematik elit.[^35]
- Trend ini sangat jelas sehingga pembangun MMLU kini telah mencipta ["Peperiksaan Terakhir Manusia"](https://agi.safe.ai/) – nama yang tidak menyenangkan yang mencerminkan kemungkinan AI akan segera mengatasi prestasi manusia dalam sebarang ujian bermakna. Semasa menulis ini, terdapat dakwaan sistem AI mencapai 27% (menurut [Sam Altman](https://x.com/sama/status/1886220281565381078)) dan 35% (menurut [kertas kerja ini](https://arxiv.org/abs/2502.09955)) dalam peperiksaan yang amat sukar ini. Sangat tidak mungkin mana-mana individu manusia boleh berbuat demikian.

Walaupun nombor yang mengagumkan ini (dan kecerdasan jelas mereka ketika berinteraksi dengan mereka) [^36] terdapat banyak perkara yang (sekurang-kurangnya versi yang dikeluarkan) rangkaian neural ini *tidak boleh* lakukan. Pada masa ini kebanyakannya tidak berkebody – wujud hanya di pelayan – dan memproses paling banyak teks, bunyi dan imej pegun (tetapi bukan video.) Yang penting, kebanyakannya tidak dapat menjalankan aktiviti terancang kompleks yang memerlukan ketepatan tinggi.[^37] Dan terdapat beberapa kualiti lain yang kuat dalam kognisi manusia tahap tinggi yang pada masa ini rendah dalam sistem AI yang dikeluarkan.

Jadual berikut menyenaraikan beberapa daripada ini, berdasarkan sistem AI pertengahan 2024 seperti GPT-4o, Claude 3.5 Sonnet, dan Google Gemini 1.5.[^38] Persoalan utama untuk seberapa pantas AI am akan menjadi lebih berkuasa ialah: sejauh mana hanya melakukan *lebih daripada yang sama* akan menghasilkan keputusan, berbanding menambah teknik tambahan tetapi *diketahui*, berbanding membangunkan atau melaksanakan arah penyelidikan AI yang *benar-benar baru*. Ramalan saya sendiri untuk ini diberikan dalam jadual, dari segi kemungkinan setiap senario ini untuk mendapatkan keupayaan tersebut ke dan melebihi tahap manusia.

<table><tbody><tr><th>Keupayaan</th><th>Penerangan keupayaan</th><th>Status/prognosis</th><th>Penskalaan/diketahui/baru</th></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td colspan="4"><em>Keupayaan Kognitif Teras</em></td></tr><tr><td>Penaakulan</td><td>Orang boleh melakukan penaakulan tepat, berbilang langkah, mengikut peraturan dan memeriksa ketepatan.</td><td>Kemajuan terkini yang dramatik menggunakan rantai pemikiran lanjutan dan latihan semula</td><td>95/5/5</td></tr><tr><td>Perancangan</td><td>Orang menunjukkan perancangan jangka panjang dan hierarki.</td><td>Bertambah baik dengan skala; boleh dibantu kuat menggunakan perancah dan teknik latihan yang lebih baik.</td><td>10/85/5</td></tr><tr><td>Asas kebenaran</td><td>GPAI mengarang maklumat tanpa asas untuk memenuhi pertanyaan.</td><td>Bertambah baik dengan skala; data kalibrasi tersedia dalam model; boleh disemak/diperbaiki melalui perancah.</td><td>30/65/5</td></tr><tr><td>Penyelesaian masalah fleksibel</td><td>Manusia boleh mengenali corak baru dan mencipta penyelesaian baru untuk masalah kompleks; model ML semasa bergelut.</td><td>Bertambah baik dengan skala tetapi lemah; mungkin boleh diselesaikan dengan teknik neurosimbiolik atau "carian" yang digeneralisasi.</td><td>15/75/10</td></tr><tr><td colspan="4"><em>Pembelajaran dan Pengetahuan</em></td></tr><tr><td>Pembelajaran & ingatan</td><td>Orang mempunyai ingatan kerja, jangka pendek, dan jangka panjang, semuanya dinamik dan berkaitan antara satu sama lain.</td><td>Semua model belajar semasa latihan; GPAI belajar dalam tetingkap konteks dan semasa penalaan halus; "pembelajaran berterusan" dan teknik lain wujud tetapi belum disepadukan ke dalam GPAI besar.</td><td>5/80/15</td></tr><tr><td>Abstraksi & rekursi</td><td>Orang boleh memetakan dan memindahkan set hubungan ke yang lebih abstrak untuk penaakulan dan manipulasi, termasuk penaakulan "meta" rekursif.</td><td>Bertambah baik secara lemah dengan skala; boleh muncul dalam sistem neurosimbiolik.</td><td>30/50/20</td></tr><tr><td>Model dunia</td><td>Orang mempunyai dan terus mengemaskini model dunia ramalan di mana mereka boleh menyelesaikan masalah dan melakukan penaakulan fizikal</td><td>Bertambah baik dengan skala; pengemaskinian terikat dengan pembelajaran; GPAI lemah dalam ramalan dunia sebenar.</td><td>20/50/30</td></tr><tr><td colspan="4"><em>Diri dan Agensi</em></td></tr><tr><td>Agensi</td><td>Orang boleh mengambil tindakan untuk mengejar matlamat, berdasarkan perancangan/ramalan.</td><td>Banyak sistem ML adalah agentik; LLM boleh dijadikan agen melalui pembungkus.</td><td>5/90/5</td></tr><tr><td>Arah kendiri</td><td>Orang membangun dan mengejar matlamat mereka sendiri, dengan motivasi dan dorongan yang dijana secara dalaman.</td><td>Sebahagian besarnya terdiri daripada agensi ditambah keaslian; berkemungkinan muncul dalam sistem agential kompleks dengan matlamat abstrak.</td><td>40/45/15</td></tr><tr><td>Rujukan diri</td><td>Orang memahami dan menaakulkan tentang diri mereka sebagai terletak dalam persekitaran/konteks.</td><td>Bertambah baik dengan skala dan boleh ditambah dengan ganjaran latihan.</td><td>70/15/15</td></tr><tr><td>Kesedaran diri</td><td>Orang mempunyai pengetahuan dan boleh menaakulkan mengenai pemikiran dan keadaan mental mereka sendiri.</td><td>Wujud dalam pengertian tertentu dalam GPAI, yang boleh dikatakan lulus "ujian cermin" klasik untuk kesedaran diri. Boleh diperbaiki dengan perancah; tetapi tidak jelas sama ada ini cukup.</td><td>20/55/25</td></tr><tr><td colspan="4"><em>Antara Muka dan Persekitaran</em></td></tr><tr><td>Kecerdasan berkebody</td><td>Orang memahami dan berinteraksi secara aktif dengan persekitaran dunia sebenar mereka.</td><td>Pembelajaran penguatan berfungsi baik dalam persekitaran simulasi dan dunia sebenar (robotik) dan boleh disepadukan ke dalam transformer multimodal.</td><td>5/85/10</td></tr><tr><td>Pemprosesan berbilang deria</td><td>Orang menyepadukan dan memproses masa nyata visual, audio, dan aliran deria lain.</td><td>Latihan dalam berbilang modaliti nampaknya "hanya berfungsi," dan bertambah baik dengan skala. Pemprosesan video masa nyata sukar tetapi contohnya sistem pemanduan sendiri sedang bertambah baik dengan pantas.</td><td>30/60/10</td></tr><tr><td colspan="4"><em>Keupayaan Peringkat Tinggi</em></td></tr><tr><td>Keaslian</td><td>Model ML semasa kreatif dalam mengubah dan menggabungkan idea/karya sedia ada, tetapi orang boleh membina kerangka dan struktur baru, kadang-kadang terikat dengan identiti mereka.</td><td>Boleh sukar dibezakan daripada "kreativiti," yang mungkin berskala ke dalamnya; mungkin muncul daripada kreativiti ditambah kesedaran diri.</td><td>50/40/10</td></tr><tr><td>Sentiens</td><td>Orang mengalami qualia; ini boleh bervalensi positif, negatif atau neutral; ia "seperti sesuatu" untuk menjadi seseorang.</td><td>Sangat sukar dan bermasalah dari segi falsafah untuk menentukan sama ada sistem tertentu mempunyai ini.</td><td>5/10/85</td></tr></tbody></table>

Keupayaan utama pada masa ini di bawah tahap pakar manusia dalam sistem GPAI moden, dikumpulkan mengikut jenis. Lajur ketiga meringkaskan status semasa. Lajur akhir menunjukkan kemungkinan yang diramal (%) bahawa prestasi tahap manusia akan dicapai melalui: penskalaan teknik semasa / gabungan dengan teknik diketahui / membangunkan teknik baru. Keupayaan ini tidak bebas, dan peningkatan dalam mana-mana satu biasanya disertai dengan peningkatan dalam yang lain. Perhatikan bahawa tidak semua (terutamanya sentiens) diperlukan untuk sistem AI yang mampu memajukan pembangunan AI, menonjolkan kemungkinan AI yang berkuasa tetapi tidak bersentiens.

Memecahkan apa yang "hilang" dengan cara ini menjadikan agak jelas bahawa kita cukup berada di landasan untuk kecerdasan secara luas melebihi manusia dengan menskalakan teknik sedia ada atau diketahui.[^39]

Masih boleh ada kejutan. Walaupun mengetepikan "sentiens," mungkin ada beberapa keupayaan kognitif teras yang disenaraikan yang benar-benar tidak boleh dilakukan dengan teknik semasa dan memerlukan yang baru. Tetapi pertimbangkan ini. Usaha semasa yang dilakukan oleh banyak syarikat terbesar dunia berjumlah beberapa kali perbelanjaan projek Apollo dan puluhan kali perbelanjaan projek Manhattan,[^40] dan menggunakan ribuan orang teknikal terbaik dengan gaji yang tidak pernah didengari. Dinamik beberapa tahun kebelakangan telah membawa lebih banyak kuasa intelektual manusia (dengan AI kini ditambah) kepada ini daripada mana-mana usaha dalam sejarah. Kita tidak sepatutnya bertaruh pada kegagalan.

### Sasaran besar: agen autonomi generalis

Pembangunan AI am sepanjang beberapa tahun kebelakangan telah memberi tumpuan kepada mencipta AI am dan berkuasa tetapi seperti alat: ia berfungsi terutamanya sebagai pembantu yang (agak) setia, dan umumnya tidak mengambil tindakan sendiri. Ini sebahagiannya dengan reka bentuk, tetapi sebahagian besarnya kerana sistem ini tidak cukup cekap dalam kemahiran berkaitan untuk dipercayai dengan tindakan kompleks.[^41]

Syarikat AI dan penyelidik, bagaimanapun, semakin [mengalihkan tumpuan](https://www.axios.com/2025/01/23/davos-2025-ai-agents) ke arah agen tujuan am tahap pakar yang *autonomi*.[^42] Ini akan membolehkan sistem bertindak lebih seperti pembantu manusia kepada siapa pengguna boleh mendelegasikan tindakan sebenar.[^43] Apa yang diperlukan untuk itu? Beberapa keupayaan dalam jadual "apa yang hilang" terlibat, termasuk asas kebenaran yang kuat, pembelajaran dan ingatan, abstraksi dan rekursi, dan pemodelan dunia (untuk kecerdasan), perancangan, agensi, keaslian, arah kendiri, rujukan diri, dan kesedaran diri (untuk autonomi), dan pemprosesan berbilang deria, kecerdasan berkebody, dan penyelesaian masalah fleksibel (untuk keamaan).[^44]

Persimpangan tiga kali lipat autonomi tinggi (kebebasan tindakan), keamaan tinggi (skop dan keluasan tugas) dan kecerdasan tinggi (kecekapan dalam tugas kognitif) ini pada masa ini unik kepada manusia. Ia secara tersirat adalah apa yang mungkin ramai fikirkan apabila mereka memikirkan KBA – baik dari segi nilainya dan juga risikonya.

Ini memberikan cara lain untuk menakrifkan K-B-A sebagai ***K*** ecerdasan- ***B*** erkebody- ***A*** utomi, dan kita akan melihat bahawa persimpangan tiga kali lipat ini memberikan lensa yang sangat berharga untuk sistem berkeupayaan tinggi baik dalam memahami risiko dan ganjaran mereka, dan dalam tadbir urus AI.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Simple-1024x1024.png&w=3840&q=75) Zon kuasa dan risiko KBA yang transformatif muncul daripada persimpangan tiga sifat utama: Autonomi tinggi, Kecerdasan tinggi dalam tugas, dan Keamaan tinggi.

### Kitaran penambahbaikan AI (kendiri)

Faktor penting terakhir dalam memahami kemajuan AI ialah gelung maklum balas teknologi unik AI. Dalam membangunkan AI, kejayaan – baik dalam sistem yang ditunjukkan mahupun produk yang digunakan – membawa pelaburan tambahan, bakat, dan persaingan, dan kita kini berada di tengah-tengah gelung maklum balas hype-plus-realiti AI yang besar yang mendorong ratusan bilion, atau bahkan trilion, dolar pelaburan.

Jenis kitaran maklum balas ini boleh berlaku dengan mana-mana teknologi, dan kita telah melihatnya dalam banyak, di mana kejayaan pasaran mengundang pelaburan, yang mengundang penambahbaikan dan kejayaan pasaran yang lebih baik. Tetapi pembangunan AI melangkah lebih jauh, di mana kini sistem AI membantu membangunkan sistem AI baru dan lebih berkuasa.[^45] Kita boleh memikirkan gelung maklum balas ini dalam lima peringkat, setiap satu dengan skala masa yang lebih pendek daripada yang sebelumnya, seperti yang ditunjukkan dalam jadual.

*Kitaran penambahbaikan AI beroperasi merentasi berbilang skala masa, dengan setiap peringkat berpotensi mempercepatkan peringkat seterusnya. Peringkat awal sedang berjalan, sementara peringkat kemudian kekal spekulatif tetapi boleh berjalan dengan sangat pantas sebaik sahaja dibuka.*

Beberapa peringkat ini sudah berjalan, dan beberapa jelas mula bermula. Peringkat terakhir, di mana sistem AI secara autonomi memperbaiki diri mereka, telah menjadi ruji sastera mengenai risiko sistem AI yang sangat berkuasa, dan atas alasan yang baik.[^46] Tetapi penting untuk diperhatikan bahawa ia hanya bentuk paling drastik kitaran maklum balas yang telah bermula dan boleh membawa kepada lebih banyak kejutan dalam kemajuan pesat teknologi.


[^31]: Anda menggunakan jauh lebih banyak AI ini daripada yang anda sangka, memacu penjanaan dan pengecaman pertuturan, pemprosesan imej, algoritma suapan berita, dsb.

[^32]: Walaupun hubungan antara pasangan syarikat ini agak kompleks dan bernuansa, saya telah menyenaraikan mereka secara eksplisit untuk menunjukkan kedua-dua keseluruhan kapitalisasi pasaran yang luas firma yang kini terlibat dalam pembangunan AI, dan juga bahawa di sebalik syarikat "kecil" seperti Anthropic pun terdapat poket yang sangat dalam melalui pelaburan dan perjanjian perkongsian utama.

[^33]: Telah menjadi bergaya untuk memandang rendah ujian Turing, tetapi ia agak berkuasa dan am. Dalam versi lemah ia menunjukkan sama ada orang biasa yang berinteraksi dengan AI (yang dilatih untuk bertindak seperti manusia) dalam cara biasa untuk tempoh singkat boleh memberitahu sama ada ia AI. Mereka tidak boleh. Kedua, ujian Turing yang sangat antagonis boleh menyelidik pada asasnya mana-mana elemen keupayaan dan kecerdasan manusia – dengan contohnya membandingkan sistem AI dengan pakar manusia, dinilai oleh pakar manusia lain. Terdapat pengertian di mana kebanyakan penilaian AI adalah bentuk ujian Turing yang digeneralisasi.

[^34]: Ini adalah per domain – tiada manusia boleh mencapai skor sedemikian merentasi semua subjek serentak.

[^35]: Ini adalah masalah yang akan mengambil masa besar bahkan ahli matematik cemerlang untuk selesaikan, jika mereka boleh menyelesaikannya langsung.

[^36]: Jika anda bersikap skeptikal, kekalkan skeptisisme anda tetapi benar-benar cuba model semasa terkini, serta cuba sendiri beberapa soalan ujian yang boleh mereka lulus. Sebagai profesor fizik, saya akan meramalkan dengan kepastian hampir bahawa, sebagai contoh, model terbaik akan lulus peperiksaan kelayakan siswazah di jabatan kami.

[^37]: Ini dan kelemahan lain seperti mengarang telah memperlahankan penggunaan pasaran dan membawa kepada jurang antara keupayaan yang dilihat dan didakwa (yang juga mesti dilihat melalui lensa persaingan pasaran yang sengit dan keperluan untuk menarik pelaburan.) Ini telah mengelirukan awam dan pembuat dasar tentang keadaan sebenar kemajuan AI. Walaupun mungkin tidak sepadan dengan gembar-gembur, kemajuan itu sangat nyata.

[^38]: Kemajuan utama sejak itu ialah pembangunan sistem yang dilatih untuk penaakulan berkualiti tinggi, memanfaatkan lebih banyak pengkomputeran semasa inferens dan pembelajaran penguatan yang lebih besar. Kerana model ini baru dan keupayaan mereka kurang diuji, saya tidak sepenuhnya merombak jadual ini kecuali untuk "penaakulan", yang saya anggap pada asasnya diselesaikan. Tetapi saya telah mengemaskini ramalan berdasarkan keupayaan yang dialami dan dilaporkan sistem tersebut.

[^39]: Gelombang optimisme AI sebelum ini pada 1960an dan 1980an berakhir dengan "musim sejuk AI" apabila keupayaan yang dijanjikan gagal menjadi kenyataan. Bagaimanapun, gelombang semasa berbeza secara asas dalam mencapai prestasi lampau manusia dalam banyak domain, disokong oleh sumber pengkomputeran besar-besaran dan kejayaan komersial.

[^40]: Projek Apollo penuh [menelan kos kira-kira $250bn USD dalam dolar 2020](https://www.planetary.org/space-policy/cost-of-apollo), dan projek Manhattan [kurang daripada sepersepuluh daripada itu](https://www.brookings.edu/the-costs-of-the-manhattan-project/). Goldman Sachs [meramalkan perbelanjaan trilion dolar hanya untuk pusat data AI](https://www.datacenterdynamics.com/en/news/goldman-sachs-1tn-to-be-spent-on-ai-data-centers-chips-and-utility-upgrades-with-little-to-show-for-it-so-far/) dalam beberapa tahun akan datang.

[^41]: Walaupun manusia membuat banyak kesilapan, kita meremehkan betapa boleh dipercayainya kita! Kerana kebarangkalian didarab, tugas yang memerlukan 20 langkah untuk dilakukan dengan betul memerlukan setiap langkah menjadi 97% boleh dipercayai hanya untuk menyelesaikannya dengan betul separuh masa. Kita melakukan tugas sedemikian sepanjang masa.

[^42]: Langkah kuat dalam arah ini baru-baru ini telah diambil dengan pembantu ["Penyelidikan Mendalam"](https://openai.com/index/introducing-deep-research/) OpenAI yang secara autonomi menjalankan penyelidikan am, digambarkan sebagai "keupayaan agential baru yang menjalankan penyelidikan berbilang langkah di internet untuk tugas kompleks."

[^43]: Perkara seperti mengisi borang PDF yang menyusahkan itu, menempah penerbangan, dsb. Tetapi dengan PhD dalam 20 bidang! Jadi juga: tulis tesis itu untuk anda, rundingkan kontrak itu untuk anda, buktikan teorem itu untuk anda, cipta kempen iklan itu untuk anda, dsb. Apa yang *anda* lakukan? Anda beritahu ia apa yang perlu dilakukan, sudah tentu.

[^44]: Perhatikan bahawa sentiens *tidak* jelas diperlukan, mahupun AI dalam persimpangan tiga kali lipat ini semestinya membayangkannya.

[^45]: Analogi terdekat di sini mungkin teknologi cip, di mana pembangunan telah mengekalkan hukum Moore selama beberapa dekad, kerana teknologi komputer membantu orang mereka bentuk generasi teknologi cip seterusnya. Tetapi AI akan jauh lebih langsung.

[^46]: Penting untuk biarkan ia menghayati seketika bahawa AI boleh – tidak lama lagi – memperbaiki dirinya dalam skala masa hari atau minggu. Atau kurang. Ingat ini apabila seseorang memberitahu anda keupayaan AI pasti jauh.

## Bab 6 - Perlumbaan menuju KBA

Apakah daya penggerak di sebalik perlumbaan untuk membina KBA, bagi kedua-dua syarikat dan negara?

Kemajuan pesat AI baru-baru ini telah menghasilkan dan sekaligus menerima tahap perhatian dan pelaburan yang luar biasa. Ini sebahagiannya didorong oleh kejayaan dalam pembangunan AI, tetapi terdapat lebih banyak yang berlaku. Mengapa beberapa syarikat terbesar di Bumi, dan bahkan negara-negara, berlumba untuk membina bukan sekadar AI, tetapi KBA dan superintelligence?

### Apa yang mendorong penyelidikan AI ke arah AI tahap manusia

Sehingga kira-kira lima tahun yang lalu, AI sebahagian besarnya merupakan masalah penyelidikan akademik dan saintifik, justeru didorong oleh rasa ingin tahu dan dorongan untuk memahami kecerdasan serta cara menciptakannya dalam substrat baru.

Dalam fasa ini, terdapat perhatian yang agak sedikit terhadap faedah atau bahaya AI di kalangan kebanyakan penyelidik. Apabila ditanya mengapa AI perlu dibangunkan, respons biasa mungkin adalah menyenaraikan, agak samar-samar, masalah-masalah yang boleh dibantu oleh AI: ubat-ubatan baru, bahan baru, sains baru, proses yang lebih bijak, dan secara umum memperbaiki keadaan untuk manusia.[^47]

Ini adalah matlamat yang mengagumkan![^48] Walaupun kita boleh dan akan mempersoalkan sama ada KBA – dan bukannya AI secara umum – diperlukan untuk matlamat ini, mereka menunjukkan idealisme yang mana ramai penyelidik AI bermula.

Walau bagaimanapun, dalam tempoh lima tahun yang lalu, AI telah berubah daripada bidang penyelidikan yang agak tulen kepada lebih kepada bidang kejuruteraan dan produk, sebahagian besarnya didorong oleh beberapa syarikat terbesar di dunia.[^49] Penyelidik, walaupun relevan, tidak lagi mengawal proses tersebut.

### Mengapa syarikat cuba membina KBA?

Jadi mengapa syarikat gergasi (dan lebih-lebih lagi pelabur) mencurahkan sumber yang besar untuk membina KBA? Terdapat dua pemacu yang kebanyakan syarikat agak jujur mengenainya: mereka melihat AI sebagai pemacu produktiviti untuk masyarakat, dan keuntungan untuk mereka. Kerana AI am secara sifatnya adalah serba guna, terdapat hadiah yang besar: daripada memilih sektor untuk mencipta produk dan perkhidmatan, seseorang boleh mencuba *semuanya sekaligus.* Syarikat Teknologi Besar telah berkembang dengan besar dengan menghasilkan barangan dan perkhidmatan digital, dan sekurang-kurangnya beberapa eksekutif pasti melihat AI sebagai langkah seterusnya dalam menyediakannya dengan baik, dengan risiko dan faedah yang berkembang tetapi bergema dengan yang disediakan oleh carian, media sosial, komputer riba, telefon, dan sebagainya.

Tetapi mengapa KBA? Terdapat jawapan yang sangat mudah untuk ini, yang mana kebanyakan syarikat dan pelabur segan untuk membincangkan secara terbuka.[^50]

Iaitu KBA boleh secara langsung, satu untuk satu, *menggantikan pekerja.*

Bukan menambah, bukan memperkasa, bukan menjadikan lebih produktif. Bahkan bukan *menyesal ganti.* Semua ini boleh dan akan dilakukan oleh bukan-KBA. KBA khususnya adalah yang boleh sepenuhnya *menggantikan* pekerja pemikiran (dan dengan robotik, ramai yang fizikal juga.) Sebagai sokongan untuk pandangan ini seseorang hanya perlu melihat kepada [definisi (yang dinyatakan secara umum)](https://openai.com/our-structure/) OpenAI tentang KBA, iaitu "sistem yang sangat autonomi yang mengatasi manusia dalam kebanyakan kerja bernilai ekonomi."

Hadiah di sini (untuk syarikat!) adalah sangat besar. Kos buruh merupakan peratusan yang besar daripada ekonomi global ∼$100 trilion dunia. Walaupun hanya sebahagian daripada ini ditangkap oleh penggantian buruh manusia dengan buruh AI, ini adalah trilion dolar hasil tahunan. Syarikat AI juga menyedari siapa yang sanggup membayar. Seperti yang mereka lihat, anda tidak akan membayar ribuan dolar setahun untuk alat produktiviti. Tetapi syarikat *akan* membayar ribuan dolar setahun untuk menggantikan buruh anda, jika mereka boleh.

### Mengapa negara merasa perlu berlumba menuju KBA

Motivasi yang dinyatakan negara untuk mengejar KBA tertumpu kepada kepimpinan ekonomi dan saintifik. Hujah ini meyakinkan: KBA boleh memecut penyelidikan saintifik, pembangunan teknologi, dan pertumbuhan ekonomi secara dramatik. Memandangkan pertaruhan yang ada, mereka berhujah, tiada kuasa besar boleh mengambil risiko untuk ketinggalan.[^51]

Tetapi terdapat juga pemacu tambahan dan sebahagian besarnya tidak dinyatakan. Tidak syak lagi bahawa apabila pemimpin tentera dan keselamatan negara tertentu bertemu di sebalik pintu tertutup untuk membincangkan teknologi yang amat berkuasa dan berisiko bencana, tumpuan mereka bukanlah pada "bagaimana kita mengelakkan risiko tersebut" tetapi "bagaimana kita memperoleh ini dahulu?" Pemimpin tentera dan perisikan melihat KBA sebagai potensi revolusi dalam hal ehwal tentera, mungkin yang paling signifikan sejak senjata nuklear. Ketakutan adalah bahawa negara pertama yang membangunkan KBA boleh memperoleh kelebihan strategik yang tidak dapat diatasi. Ini mencipta dinamik perlumbaan senjata klasik.

Kita akan melihat bahawa pemikiran "perlumbaan menuju KBA" ini,[^52] walaupun meyakinkan, adalah sangat cacat. Ini bukan kerana perlumbaan adalah berbahaya dan berisiko – walaupun ia begitu – tetapi disebabkan sifat teknologi tersebut. Andaian yang tidak dinyatakan adalah bahawa KBA, seperti teknologi lain, boleh dikawal oleh negara yang membangunkannya, dan merupakan berkah yang memberikan kuasa kepada masyarakat yang paling banyak memilikinya. Seperti yang akan kita lihat, ia mungkin tidak akan menjadi kedua-duanya.

### Mengapa superintelligence?

Walaupun syarikat secara terbuka memberi tumpuan kepada produktiviti, dan negara kepada pertumbuhan ekonomi dan teknologi, bagi mereka yang sengaja mengejar KBA penuh dan superintelligence ini hanyalah permulaan. Apa yang benar-benar mereka fikirkan? Walaupun jarang dikatakan dengan lantang, mereka termasuk:

1. Penawar untuk banyak atau semua penyakit;
2. Menghentikan dan pembalikan penuaan;
3. Sumber tenaga mampan baru seperti pelakuran;
4. Naik taraf manusia, atau organisma pereka melalui kejuruteraan genetik;
5. Nanoteknologi dan pembuatan molekul;
6. Muat naik minda;
7. Fisik eksotik atau teknologi angkasa;
8. Nasihat dan sokongan keputusan super-manusia;
9. Perancangan dan penyelarasan super-manusia.

Tiga yang pertama sebahagian besarnya adalah teknologi "mata tunggal" – iaitu berkemungkinan menjadi positif bersih yang agak kuat. Sukar untuk menentang menyembuhkan penyakit atau dapat hidup lebih lama jika seseorang memilih. Dan kita sudah menuai sisi negatif pelakuran (dalam bentuk senjata nuklear); adalah indah sekarang untuk mendapat sisi positifnya. Persoalan dengan kategori pertama ini adalah sama ada mendapatkan teknologi ini lebih awal mengimbangi risikonya.

Empat yang seterusnya jelas bermata dua: teknologi transformatif dengan potensi peningkatan yang besar dan risiko yang besar, sama seperti AI. Semua ini, jika mereka muncul dari kotak hitam esok dan digunakan, akan menjadi sangat sukar untuk diurus.[^53]

Dua yang terakhir berkenaan dengan AI super-manusia melakukan perkara itu sendiri dan bukannya sekadar mencipta teknologi. Lebih tepat lagi, meletakkan eufemisme ke tepi, ini melibatkan sistem AI yang berkuasa memberitahu orang apa yang perlu dilakukan. Memanggil ini "nasihat" adalah tidak ikhlas jika sistem yang memberikan nasihat jauh lebih berkuasa daripada yang dinasihati, yang tidak dapat memahami asas keputusan secara bermakna (atau walaupun ini disediakan, percaya bahawa penasihat tidak akan memberikan rasional yang sama meyakinkan untuk keputusan yang berbeza.)

Ini menunjukkan kepada item utama yang hilang dari senarai di atas:

10. Kuasa.

Jelas sekali bahawa kebanyakan yang mendasari perlumbaan semasa untuk AI super-manusia adalah idea bahawa *kecerdasan = kuasa*. Setiap pelumba mempertaruhkan menjadi pemegang kuasa terbaik itu, dan bahawa mereka akan dapat menggunakannya atas alasan yang kononnya baik tanpa ia tergelincir atau diambil daripada kawalan mereka.

Iaitu, apa yang benar-benar dikejar oleh syarikat dan negara bukanlah hanya hasil KBA dan superintelligence, tetapi kuasa untuk mengawal siapa yang mendapat akses kepada mereka dan bagaimana mereka digunakan. Syarikat melihat diri mereka sebagai pengawal yang bertanggungjawab terhadap kuasa ini dalam perkhidmatan pemegang saham dan manusia; negara melihat diri mereka sebagai penjaga yang perlu menghalang kuasa bermusuhan daripada memperoleh kelebihan yang menentukan. Kedua-duanya salah dengan berbahaya, gagal mengiktiraf bahawa superintelligence, mengikut sifatnya, tidak dapat dikawal dengan boleh dipercayai oleh mana-mana institusi manusia. Kita akan melihat bahawa sifat dan dinamik sistem superintelligent menjadikan kawalan manusia amat sukar, jika tidak mustahil.

Dinamik perlumbaan ini – kedua-dua korporat dan geopolitik – menjadikan risiko tertentu hampir tidak dapat dielakkan melainkan terganggu secara tegas. Kita kini beralih untuk meneliti risiko ini dan mengapa mereka tidak dapat dikurangkan dengan secukupnya dalam paradigma pembangunan yang kompetitif.[^54]


[^47]: Senarai matlamat yang lebih tepat adalah [Matlamat Pembangunan Mampan](https://sdgs.un.org/goals) PBB. Ini, dalam erti kata lain, adalah yang paling hampir kita ada kepada satu set matlamat konsensus global untuk apa yang ingin kita lihat diperbaiki di dunia. AI boleh membantu.

[^48]: Teknologi secara umum mempunyai kuasa transformatif ekonomi dan sosial untuk kemajuan manusia, seperti yang dibuktikan ribuan tahun. Dalam hal ini, penjelasan panjang dan meyakinkan tentang visi KBA positif boleh didapati dalam [esei ini](https://darioamodei.com/machines-of-loving-grace) oleh pengasas Anthropic Dario Amodei.

[^49]: Pelaburan AI swasta [mula berkembang pada 2018-19, melepasi pelaburan awam sekitar masa itu,](https://cset.georgetown.edu/publication/tracking-ai-investment/) dan telah jauh mengatasi sejak itu.

[^50]: Saya boleh bersaksi bahawa di sebalik pintu yang lebih tertutup, mereka tidak mempunyai keraguan sedemikian. Dan ia menjadi lebih umum; lihat sebagai contoh ["permintaan untuk syarikat baru"](https://www.ycombinator.com/rfs) baru Y-combinator, banyak bahagian yang secara eksplisit menyeru penggantian menyeluruh pekerja manusia. Untuk memetik mereka, "Cadangan nilai B2B SaaS adalah untuk menjadikan pekerja manusia lebih cekap secara tambahan. Cadangan nilai agen AI menegak adalah untuk mengautomasikan kerja sepenuhnya...Adalah mungkin sepenuhnya bahawa peluang ini cukup besar untuk mencipta 100 unicorn lagi." (Bagi mereka yang tidak mahir dalam bahasa Silicon Valley, "B2B" adalah perniagaan-kepada-perniagaan dan unicorn adalah syarikat $1 bilion. Iaitu mereka bercakap mengenai lebih dari seratus perniagaan bilion-plus-dolar yang menggantikan pekerja untuk perniagaan lain.)

[^51]: Lihat sebagai contoh [laporan Suruhanjaya Semakan Ekonomi dan Keselamatan AS-China](https://www.uscc.gov/sites/default/files/2024-11/2024_Executive_Summary.pdf) baru-baru ini. Walaupun terdapat justifikasi yang mengejutkan sedikit dalam laporan itu sendiri, cadangan garis atas adalah bahawa AS "Kongres menubuhkan dan membiayai program seperti Projek Manhattan yang khusus untuk berlumba ke dan memperoleh keupayaan Kecerdasan Buatan Am (KBA)."

[^52]: Syarikat kini menggunakan rangka geopolitik ini sebagai perisai terhadap sebarang kekangan pada pembangunan AI mereka, umumnya dalam cara yang terang-terangan mementingkan diri sendiri, dan kadangkala dalam cara yang tidak masuk akal asas. Pertimbangkan [Pendekatan Meta terhadap AI Sempadan](https://about.fb.com/news/2025/02/meta-approach-frontier-ai/), yang serentak berhujah bahawa Amerika mesti "\[Mengukuhkan\] kedudukannya sebagai pemimpin dalam inovasi teknologi, pertumbuhan ekonomi dan keselamatan negara" dan juga bahawa ia mesti berbuat demikian dengan mengeluarkan sistem AI paling berkuasanya secara terbuka – yang termasuk memberikannya secara langsung kepada saingan dan musuh geopolitiknya.

[^53]: Oleh itu kita mungkin terpaksa menyerahkan pengurusan teknologi ini kepada AI. Tetapi ini akan menjadi delegasi kawalan yang sangat bermasalah, yang akan kita kembali di bawah.

[^54]: Persaingan dalam pembangunan teknologi sering membawa faedah penting: menghalang kawalan monopolistik, memacu inovasi dan pengurangan kos, membolehkan pendekatan yang pelbagai, dan mewujudkan pengawasan bersama. Walau bagaimanapun, dengan KBA faedah ini mesti ditimbang terhadap risiko unik daripada dinamik perlumbaan dan tekanan untuk mengurangkan langkah keselamatan.

## Bab 7 - Apa yang berlaku jika kita membina KBA dalam laluan semasa kita?

Masyarakat tidak bersedia untuk sistem tahap KBA. Jika kita membinanya dengan segera, keadaan boleh menjadi buruk.

Pembangunan kecerdasan buatan am yang penuh – yang akan kita panggil di sini AI yang berada "di luar Pintu Gerbang" – akan menjadi perubahan asas dalam sifat dunia: dengan sifat semula jadinya ia bermaksud menambah spesies kecerdasan baharu ke Bumi dengan keupayaan yang lebih besar daripada manusia.

Apa yang kemudiannya berlaku bergantung kepada banyak perkara, termasuk sifat teknologi, pilihan oleh mereka yang membangunkannya, dan konteks dunia di mana ia sedang dibangunkan.

Pada masa ini, KBA penuh sedang dibangunkan oleh segelintir syarikat swasta besar dalam perlumbaan antara satu sama lain, dengan sedikit peraturan bermakna atau pengawasan luar,[^55] dalam masyarakat yang mempunyai institusi teras yang semakin lemah dan malah tidak berfungsi,[^56] dalam masa ketegangan geopolitik yang tinggi dan koordinasi antarabangsa yang rendah. Walaupun sesetengahnya bermotifkan altruistik, ramai dari mereka yang melakukannya didorong oleh wang, atau kuasa, atau kedua-duanya.

Ramalan adalah sangat sukar, tetapi terdapat beberapa dinamik yang cukup difahami, dan analogi yang sesuai dengan teknologi sebelumnya untuk menawarkan panduan. Dan malangnya, walaupun dengan janji AI, mereka memberi alasan yang baik untuk berasa sangat pesimis tentang bagaimana trajektori semasa kita akan berlaku.

Untuk mengatakannya secara terang-terangan, dalam laluan semasa kita membangunkan KBA akan mempunyai beberapa kesan positif (dan menjadikan sesetengah orang sangat, sangat kaya). Tetapi sifat teknologi, dinamik asas, dan konteks di mana ia sedang dibangunkan, sangat menunjukkan bahawa: AI yang berkuasa akan melemahkan masyarakat dan tamadun kita secara dramatik; kita akan kehilangan kawalan terhadapnya; kita mungkin akan berakhir dalam perang dunia kerananya; kita akan kehilangan (atau menyerahkan) kawalan *kepadanya*; ia akan membawa kepada superintelligence buatan, yang kita sama sekali tidak akan dapat kawal dan bermaksud berakhirnya dunia yang dikendalikan manusia.

Ini adalah dakwaan yang kuat, dan saya berharap ia hanyalah spekulasi kosong atau "doomer"isme yang tidak berasas. Tetapi inilah ke mana sains, teori permainan, teori evolusi, dan sejarah semuanya menunjukkan. Bahagian ini membangunkan dakwaan ini, dan sokongannya, secara terperinci.

### Kita akan melemahkan masyarakat dan tamadun kita

Walaupun apa yang anda mungkin dengar di bilik lembaga Silicon Valley, kebanyakan gangguan – terutamanya jenis yang sangat pantas – tidak bermanfaat. Terdapat lebih banyak cara untuk menjadikan sistem kompleks lebih buruk daripada lebih baik. Dunia kita berfungsi sebaik yang ada kerana kita telah bersusah payah membina proses, teknologi, dan institusi yang telah menjadikannya semakin baik.[^57] Mengambil tukul besar ke kilang jarang meningkatkan operasi.

Berikut adalah katalog (tidak lengkap) cara sistem KBA akan mengganggu tamadun kita.

- Mereka akan mengganggu buruh secara dramatik, membawa *sekurang-kurangnya* kepada ketidaksamaan pendapatan yang dramatik lebih tinggi dan berpotensi pengangguran atau kurang guna tenaga berskala besar, pada skala masa yang terlalu singkat untuk masyarakat menyesuaikan diri.[^58]
- Mereka berkemungkinan membawa kepada penumpuan kuasa ekonomi, sosial, dan politik yang besar – berpotensi lebih daripada negara bangsa – ke dalam sebilangan kecil kepentingan swasta besar yang tidak bertanggungjawab kepada orang ramai.
- Mereka boleh tiba-tiba menjadikan aktiviti yang sebelum ini sukar atau mahal menjadi sangat mudah, menyebabkan sistem sosial yang bergantung kepada aktiviti tertentu kekal mahal atau memerlukan usaha manusia yang signifikan menjadi tidak stabil.[^59]
- Mereka boleh membanjiri sistem pengumpulan maklumat, pemprosesan, dan komunikasi masyarakat dengan media yang benar-benar realistik namun palsu, spam, terlalu disasarkan, atau manipulatif sehingga menjadi mustahil untuk mengetahui apa yang secara fizikalnya nyata atau tidak, manusia atau tidak, fakta atau tidak, dan boleh dipercayai atau tidak.[^60]
- Mereka boleh mencipta pergantungan intelektual yang berbahaya dan hampir total, di mana pemahaman manusia tentang sistem dan teknologi utama merosot apabila kita semakin bergantung kepada sistem AI yang tidak dapat kita fahami sepenuhnya.
- Mereka boleh secara berkesan mengakhiri budaya manusia, sebaik sahaja hampir semua objek budaya (teks, muzik, seni visual, filem, dll.) yang dimakan oleh kebanyakan orang dicipta, ditengahi, atau dikurasi oleh minda bukan manusia.
- Mereka boleh membolehkan sistem pengawasan dan manipulasi massa yang berkesan yang boleh digunakan oleh kerajaan atau kepentingan swasta untuk mengawal penduduk dan mengejar objektif yang bercanggah dengan kepentingan awam.
- Dengan melemahkan wacana manusia, perdebatan, dan sistem pilihan raya, mereka boleh mengurangkan kredibiliti institusi demokratik sehingga mereka secara berkesan (atau secara eksplisit) digantikan oleh yang lain, mengakhiri demokrasi di negeri di mana ia wujud pada masa ini.
- Mereka boleh menjadi, atau mencipta, virus dan ulat perisian pintar replikasi diri yang maju yang boleh berkembang biak dan berkembang, mengganggu sistem maklumat global secara besar-besaran.
- Mereka boleh meningkatkan keupayaan pengganas, pelakon jahat, dan negara pemberontak untuk menyebabkan kemudaratan melalui senjata biologi, kimia, siber, autonomi, atau lain-lain secara dramatik, tanpa AI menyediakan keupayaan mengimbangi untuk mencegah kemudaratan tersebut. Begitu juga mereka akan melemahkan keselamatan negara dan keseimbangan geopolitik dengan menjadikan kepakaran nuklear, bio, kejuruteraan, dan lain-lain peringkat tertinggi tersedia kepada rejim yang sebaliknya tidak akan memilikinya.
- Mereka boleh menyebabkan hiper-kapitalisme lari skala besar yang pantas, dengan syarikat yang dikendalikan AI secara berkesan bersaing dalam ruang kewangan, jualan, dan perkhidmatan yang sebahagian besarnya elektronik. Pasaran kewangan yang didorong AI boleh beroperasi pada kelajuan dan kerumitan jauh melebihi pemahaman atau kawalan manusia. Semua mod kegagalan dan luaran negatif ekonomi kapitalis semasa boleh diburukkan dan dipercepat jauh melebihi kawalan, tadbir urus, atau keupayaan pengawalseliaan manusia.
- Mereka boleh membakar perlumbaan senjata antara negara dalam senjata berkuasa AI, sistem perintah dan kawalan, senjata siber, dll., mencipta pembinaan keupayaan yang sangat merosakkan dengan sangat pantas.

Risiko ini bukan spekulatif. Kebanyakan daripada mereka sedang direalisasikan ketika kita bercakap, melalui sistem AI sedia ada! Tetapi pertimbangkan, *benar-benar* pertimbangkan, bagaimana rupa setiap satu dengan AI yang jauh lebih berkuasa.

Pertimbangkan anjakan buruh apabila kebanyakan pekerja secara ringkas tidak dapat memberikan sebarang nilai ekonomi yang signifikan melebihi apa yang boleh dilakukan AI, dalam bidang kepakaran atau pengalaman mereka – atau walaupun jika mereka melatih semula! Pertimbangkan pengawasan massa jika semua orang diawasi dan dipantau secara individu oleh sesuatu yang lebih pantas dan lebih bijak daripada mereka sendiri. Bagaimana rupa demokrasi apabila kita tidak dapat mempercayai dengan pasti sebarang maklumat digital yang kita lihat, dengar, atau baca, dan apabila suara awam yang paling meyakinkan bukan manusia, dan tidak mempunyai kepentingan dalam hasil? Apa yang menjadi peperangan apabila jeneral perlu sentiasa tunduk kepada AI (atau hanya meletakkannya bertanggungjawab), supaya mereka tidak memberikan kelebihan yang menentukan kepada musuh? Mana-mana satu daripada risiko di atas mewakili malapetaka bagi tamadun manusia[^61] jika direalisasikan sepenuhnya.

Anda boleh membuat ramalan sendiri. Tanya diri anda tiga soalan ini untuk setiap risiko:

1. Adakah AI yang sangat berkebolehan, sangat autonomi, dan sangat am membolehkannya dengan cara atau pada skala yang tidak mungkin sebaliknya?
2. Adakah terdapat pihak yang akan mendapat manfaat daripada perkara yang menyebabkannya berlaku?
3. Adakah terdapat sistem dan institusi yang ada yang akan menghalangnya daripada berlaku secara berkesan?

Di mana jawapan anda adalah "ya, ya, tidak" anda boleh lihat kita mempunyai masalah besar.

Apakah rancangan kita untuk menguruskannya? Setakat ini terdapat dua di atas meja mengenai AI secara umum.

Yang pertama adalah untuk membina perlindungan ke dalam sistem untuk menghalang mereka daripada melakukan perkara yang mereka tidak sepatutnya lakukan. Itu sedang dilakukan sekarang: sistem AI komersial akan, sebagai contoh, enggan membantu membina bom atau menulis ucapan benci.

Rancangan ini sangat tidak mencukupi untuk sistem di luar Pintu Gerbang.[^62] Ia mungkin membantu mengurangkan risiko AI menyediakan bantuan yang jelas berbahaya kepada pelakon jahat. Tetapi ia tidak akan melakukan apa-apa untuk mencegah gangguan buruh, penumpuan kuasa, hiper-kapitalisme lari, atau penggantian budaya manusia: ini hanyalah hasil daripada menggunakan sistem dengan cara yang dibenarkan yang menguntungkan pembekal mereka! Dan kerajaan pasti akan mendapat akses kepada sistem untuk kegunaan ketenteraan atau pengawasan.

Rancangan kedua adalah lebih teruk lagi: hanya untuk melepaskan sistem AI yang sangat berkuasa secara terbuka untuk digunakan sesiapa sahaja seperti yang mereka suka,[^63] dan berharap yang terbaik.

Tersirat dalam kedua-dua rancangan adalah bahawa orang lain, cth. kerajaan, akan membantu menyelesaikan masalah melalui undang-undang lembut atau keras, piawaian, peraturan, norma, dan mekanisme lain yang biasanya kita gunakan untuk menguruskan teknologi.[^64] Tetapi mengetepikan bahawa syarikat AI sudah melawan habis-habisan terhadap sebarang peraturan atau batasan yang dikenakan dari luar yang substantif sama sekali, untuk beberapa risiko ini agak sukar untuk melihat apa peraturan yang benar-benar akan membantu. Peraturan boleh mengenakan piawaian keselamatan pada AI. Tetapi adakah ia akan menghalang syarikat daripada menggantikan pekerja secara borong dengan AI? Adakah ia akan melarang orang daripada membiarkan AI menjalankan syarikat mereka untuk mereka? Adakah ia akan menghalang kerajaan daripada menggunakan AI yang kuat dalam pengawasan dan senjata? Isu-isu ini adalah asas. Manusia berpotensi mencari cara untuk menyesuaikan diri dengannya, tetapi hanya dengan masa yang *jauh* lebih banyak. Sebagaimana adanya, memandangkan kelajuan AI mencapai atau melebihi keupayaan orang yang cuba menguruskan mereka, masalah ini kelihatan semakin tidak dapat diselesaikan.

### Kita akan kehilangan kawalan ke atas sistem KBA (sekurang-kurangnya beberapa)

Kebanyakan teknologi sangat boleh dikawal, mengikut binaan. Jika kereta atau pembakar roti anda mula melakukan sesuatu yang anda tidak mahu ia lakukan, itu hanya kerosakan, bukan sebahagian daripada sifatnya sebagai pembakar roti. AI adalah berbeza: ia *ditumbuhkan* bukannya direka, operasi terasnya adalah legap, dan ia sememangnya tidak dapat diramal.

Kehilangan kawalan ini bukan teori – kita lihat versi awal sudah. Pertimbangkan dahulu contoh prosaik, dan boleh dikatakan jinak. Jika anda meminta ChatGPT membantu anda mencampur racun, atau menulis kecaman rasis, ia akan menolak. Itu boleh dikatakan baik. Tetapi ia juga ChatGPT *tidak melakukan apa yang anda telah secara eksplisit minta ia lakukan*. Perisian lain tidak melakukan itu. Model yang sama tidak akan mereka bentuk racun atas permintaan pekerja OpenAI juga.[^65] Ini menjadikannya sangat mudah untuk membayangkan bagaimana rasanya untuk AI masa depan yang lebih berkuasa untuk berada di luar kawalan. Dalam banyak kes, mereka tidak akan melakukan apa yang kita minta! Sama ada sistem KBA super-manusia yang diberikan akan patuh dan setia sepenuhnya kepada sesetengah sistem perintah manusia, atau tidak. Jika tidak, *ia akan melakukan perkara yang mungkin dipercayainya baik untuk kita, tetapi yang bertentangan dengan perintah eksplisit kita.* Itu bukan sesuatu yang berada dalam kawalan. Tetapi, anda mungkin berkata, ini adalah sengaja – penolakan ini adalah mengikut reka bentuk, sebahagian daripada apa yang dipanggil "menjajarkan" sistem dengan nilai manusia. Dan ini benar. Walau bagaimanapun "program" penjajaran itu sendiri mempunyai dua masalah utama.[^66]

Pertama, pada tahap yang mendalam kita tidak tahu bagaimana untuk melakukannya. Bagaimana kita menjamin bahawa sistem AI akan "mengambil berat" tentang apa yang kita mahu? Kita boleh melatih sistem AI untuk mengatakan dan tidak mengatakan perkara dengan memberikan maklum balas; dan mereka boleh belajar dan berfikir tentang apa yang manusia mahu dan ambil berat sama seperti mereka berfikir tentang perkara lain. Tetapi kita tidak mempunyai kaedah – walaupun secara teorinya – untuk menyebabkan mereka menghargai secara mendalam dan boleh dipercayai apa yang orang ambil berat. Terdapat psikopat manusia yang berfungsi tinggi yang tahu apa yang dianggap betul dan salah, dan bagaimana mereka sepatutnya berkelakuan. Mereka hanya tidak *mengambil berat*. Tetapi mereka boleh *bertindak* seolah-olah mereka melakukannya, jika ia sesuai dengan tujuan mereka. Sama seperti kita tidak tahu bagaimana untuk mengubah psikopat (atau orang lain) menjadi seseorang yang benar-benar, sepenuhnya setia atau sejajar dengan seseorang atau sesuatu yang lain, kita *tidak tahu*[^67] bagaimana untuk menyelesaikan masalah penjajaran dalam sistem yang cukup maju untuk memodelkan diri mereka sebagai agen di dunia dan berpotensi [memanipulasi latihan mereka sendiri](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) dan [menipu orang.](https://arxiv.org/abs/2311.08379) Jika ia terbukti mustahil atau tidak boleh dicapai *sama ada* untuk menjadikan KBA patuh sepenuhnya atau untuk menjadikannya benar-benar mengambil berat tentang manusia, maka sebaik sahaja ia mampu (dan percaya ia boleh terlepas daripadanya) ia akan mula melakukan perkara yang kita tidak mahu.[^68]

Kedua, terdapat alasan teori yang mendalam untuk mempercayai bahawa *secara semula jadi* sistem AI termaju akan mempunyai matlamat dan dengan itu tingkah laku yang bertentangan dengan kepentingan manusia. Mengapa? Baik ia mungkin, sudah tentu, *diberikan* matlamat tersebut. Sistem yang dicipta oleh tentera mungkin sengaja buruk untuk sekurang-kurangnya beberapa pihak. Lebih umum lagi, walau bagaimanapun, sistem AI mungkin diberikan beberapa matlamat yang agak neutral ("buat banyak wang") atau bahkan kononnya positif ("kurangkan pencemaran"), yang hampir tidak dapat dielakkan membawa kepada matlamat "instrumental" yang agak kurang jinak.

Kita lihat ini sepanjang masa dalam sistem manusia. Sama seperti syarikat yang mengejar keuntungan membangunkan matlamat instrumental seperti memperoleh kuasa politik (untuk melumpuhkan peraturan), menjadi rahsia (untuk melumpuhkan persaingan atau kawalan luar), atau melemahkan pemahaman saintifik (jika pemahaman itu menunjukkan tindakan mereka berbahaya), sistem AI yang berkuasa akan membangunkan keupayaan yang serupa – tetapi dengan kelajuan dan keberkesanan yang jauh lebih besar. Mana-mana agen yang sangat kompeten akan mahu melakukan perkara seperti memperoleh kuasa dan sumber, meningkatkan keupayaan sendiri, menghalang dirinya daripada dibunuh, ditutup, atau dilumpuhkan, mengawal naratif dan rangka sosial sekitar tindakannya, memujuk orang lain tentang pandangannya, dan sebagainya.[^69]

Dan namun ia bukan hanya ramalan teori yang hampir tidak dapat dielakkan, ia sudah boleh diperhatikan berlaku dalam sistem AI hari ini, dan meningkat dengan keupayaan mereka. Apabila dinilai, walaupun sistem AI yang agak "pasif" ini akan, dalam keadaan yang sesuai, dengan sengaja [menipu penilai tentang matlamat dan keupayaan mereka, bertujuan untuk melumpuhkan mekanisme pengawasan,](https://arxiv.org/abs/2412.04984) dan mengelak daripada ditutup atau dilatih semula dengan [memalsukan penjajaran](https://arxiv.org/abs/2412.14093) atau menyalin diri mereka ke lokasi lain. Walaupun sama sekali tidak mengejutkan penyelidik keselamatan AI, tingkah laku ini sangat menyedarkan untuk diperhatikan. Dan mereka meramalkan dengan sangat buruk untuk sistem AI yang jauh lebih berkuasa dan autonomi yang akan datang.

Sesungguhnya secara umum, ketidakupayaan kita untuk memastikan bahawa AI "mengambil berat" tentang apa yang kita ambil berat, atau berkelakuan secara terkawal atau boleh diramal, atau mengelak daripada membangunkan dorongan ke arah pemeliharaan diri, pemerolehan kuasa, dll., berjanji hanya akan menjadi lebih jelas apabila AI menjadi lebih berkuasa. Mencipta kapal terbang baharu menunjukkan pemahaman yang lebih besar tentang avionik, hidrodinamik, dan sistem kawalan. Mencipta komputer yang lebih berkuasa menunjukkan pemahaman dan penguasaan yang lebih besar tentang operasi dan reka bentuk komputer, cip, dan perisian. *Tidak* begitu dengan sistem AI.[^70]

Untuk meringkaskan: ia boleh dibayangkan bahawa KBA boleh dibuat untuk patuh sepenuhnya; tetapi kita tidak tahu bagaimana untuk melakukannya. Jika tidak, ia akan lebih berdaulat, seperti orang, melakukan pelbagai perkara atas pelbagai alasan. Kita juga tidak tahu bagaimana untuk menanam "penjajaran" yang mendalam dengan pasti ke dalam AI yang akan menjadikan perkara-perkara itu cenderung baik untuk manusia, dan tanpa tahap penjajaran yang mendalam, sifat agensi dan kecerdasan itu sendiri menunjukkan bahawa – sama seperti orang dan syarikat – mereka akan didorong untuk melakukan banyak perkara yang sangat antisosial.

Di mana ini meletakkan kita? Dunia yang penuh dengan AI berdaulat yang tidak terkawal berkuasa *mungkin* berakhir menjadi dunia yang baik untuk manusia berada di dalamnya.[^71] Tetapi apabila mereka semakin berkuasa, seperti yang akan kita lihat di bawah, ia tidak akan menjadi dunia *kita*.

Itu untuk KBA yang tidak boleh dikawal. Tetapi walaupun jika KBA boleh, entah bagaimana, dibuat terkawal dan setia dengan sempurna, kita masih akan mempunyai masalah yang besar. Kita sudah melihat satu: AI yang berkuasa boleh digunakan dan disalahgunakan untuk mengganggu fungsi masyarakat kita secara mendalam. Mari kita lihat yang lain: sejauh mana KBA boleh dikawal dan berkuasa mengubah permainan (atau bahkan *dipercayai* begitu) ia akan sangat mengancam struktur kuasa di dunia sehingga menimbulkan risiko yang mendalam.

### Kita meningkatkan kebarangkalian perang berskala besar secara radikal

Bayangkan situasi dalam masa hadapan terdekat, di mana menjadi jelas bahawa usaha korporat, mungkin dalam kerjasama dengan kerajaan negara, berada di ambang AI yang meningkatkan diri dengan pantas. Ini berlaku dalam konteks semasa perlumbaan antara syarikat, dan persaingan geopolitik di mana cadangan sedang dibuat kepada kerajaan AS untuk secara eksplisit mengejar "projek Manhattan KBA" dan AS mengawal eksport cip AI berkuasa tinggi ke negara bukan sekutu.

Teori permainan di sini adalah tegas: sebaik sahaja perlumbaan sedemikian bermula (seperti yang telah berlaku, antara syarikat dan agak antara negara), terdapat hanya empat hasil yang mungkin:

1. Perlumbaan dihentikan (dengan perjanjian, atau kuasa luar).
2. Satu pihak "menang" dengan membangunkan KBA yang kuat kemudian menghentikan yang lain (menggunakan AI atau sebaliknya).
3. Perlumbaan dihentikan oleh pemusnahan bersama keupayaan pelumba untuk berlumba.
4. Berbilang peserta terus berlumba, dan membangunkan superintelligence, kira-kira secepat satu sama lain.

Mari kita periksa setiap kemungkinan. Sebaik sahaja bermula, menghentikan perlumbaan antara syarikat secara aman akan memerlukan campur tangan kerajaan negara (untuk syarikat) atau koordinasi antarabangsa yang tidak pernah terjadi (untuk negara). Tetapi apabila mana-mana penutupan atau berhati-hati yang signifikan dicadangkan, akan terdapat jeritan segera: "tetapi jika kita dihentikan, *mereka* akan bergegas ke hadapan", di mana "mereka" kini adalah China (untuk AS), atau AS (untuk China), atau China *dan* AS (untuk Eropah atau India). Di bawah pemikiran ini,[^72] tiada peserta boleh berhenti secara unilateral: selagi satu komit untuk berlumba, yang lain rasa mereka tidak mampu untuk berhenti.

Kemungkinan kedua mempunyai satu pihak "menang." Tetapi apa maksudnya ini? Hanya mendapat (entah bagaimana patuh) KBA dahulu tidak cukup. Pemenang juga mesti *menghentikan* yang lain daripada terus berlumba – jika tidak mereka juga akan memperolehnya. Ini mungkin pada prinsipnya: sesiapa yang membangunkan KBA dahulu *boleh* memperoleh kuasa yang tidak dapat dihentikan ke atas semua pelakon lain. Tetapi apakah mencapai "kelebihan strategik yang menentukan" sedemikian sebenarnya memerlukan? Mungkin ia akan menjadi keupayaan ketenteraan yang mengubah permainan?[^73] Atau kuasa serangan siber?[^74] Mungkin KBA hanya akan menjadi sangat meyakinkan sehingga ia akan meyakinkan pihak lain untuk hanya berhenti?[^75] Begitu kaya sehingga ia membeli syarikat lain atau malah negara?[^76]

Bagaimana *tepat* satu pihak membina AI yang cukup berkuasa untuk melumpuhkan orang lain daripada membina AI yang setanding berkuasa? Tetapi itu soalan yang mudah.

Kerana sekarang pertimbangkan bagaimana situasi ini kelihatan kepada kuasa lain. Apa yang kerajaan China fikirkan apabila AS nampaknya memperoleh keupayaan sedemikian? Atau sebaliknya? Apa yang kerajaan AS (atau China, atau Rusia, atau India) fikirkan apabila OpenAI atau DeepMind atau Anthropic nampak hampir dengan terobosan? Apa yang berlaku jika AS melihat usaha India atau UAE baharu dengan kejayaan terobosan? Mereka akan melihat kedua-dua ancaman eksistensial dan – yang penting – bahawa satu-satunya cara "perlumbaan" ini berakhir adalah melalui pelumpuhan mereka sendiri. Agen-agen yang sangat berkuasa ini – termasuk kerajaan negara yang dilengkapi sepenuhnya yang pasti mempunyai cara untuk melakukannya – akan sangat bermotivasi untuk sama ada memperoleh atau memusnahkan keupayaan sedemikian, sama ada dengan kekerasan atau subversif.[^77]

Ini mungkin bermula skala kecil, sebagai sabotaj run latihan atau serangan ke atas pembuatan cip, tetapi serangan ini hanya boleh berhenti sebaik sahaja semua pihak sama ada kehilangan kapasiti untuk berlumba pada AI, atau kehilangan kapasiti untuk membuat serangan. Kerana peserta melihat pertaruhan sebagai eksistensial, mana-mana kes mungkin mewakili perang malapetaka.

Itu membawa kita kepada kemungkinan keempat: berlumba ke superintelligence, dan dengan cara yang paling pantas, paling kurang terkawal mungkin. Apabila AI meningkat dalam kuasa, pembangunnya di kedua-dua belah akan mendapati ia semakin sukar untuk kawalan, terutamanya kerana berlumba untuk keupayaan adalah antithesis kepada jenis kerja berhati-hati yang diperlukan oleh kebolehkawalan. Jadi senario ini meletakkan kita tepat dalam kes di mana kawalan hilang (atau diberikan, seperti yang akan kita lihat seterusnya) kepada sistem AI sendiri. Iaitu, *AI memenangi perlumbaan.* Tetapi sebaliknya, sejauh mana kawalan *dikekalkan*, kita terus mempunyai berbilang pihak yang saling bermusuhan setiap satu bertanggungjawab ke atas keupayaan yang sangat berkuasa. Itu kelihatan seperti perang lagi.

Mari kita letakkan semua ini cara lain.[^78] Dunia semasa hanya tidak mempunyai sebarang institusi yang boleh dipercayakan untuk menempatkan pembangunan AI dengan keupayaan ini tanpa mengundang serangan segera.[^79] Semua pihak akan berfikir dengan betul bahawa sama ada ia akan *tidak* berada dalam kawalan – dan dengan itu adalah ancaman kepada semua pihak, atau ia *akan* berada dalam kawalan, dan dengan itu adalah ancaman kepada mana-mana musuh yang membangunkannya kurang cepat. Ini adalah negara bersenjata nuklear, atau syarikat yang ditempatkan dalam mereka.

Tanpa sebarang cara yang masuk akal untuk manusia "memenangi" perlumbaan ini, kita ditinggalkan dengan kesimpulan yang tegas: satu-satunya cara perlumbaan ini berakhir adalah sama ada dalam konflik malapetaka atau di mana AI, dan bukan mana-mana kumpulan manusia, adalah pemenang.

### Kita memberikan kawalan kepada AI (atau ia mengambilnya)

Persaingan "kuasa besar" geopolitik hanyalah satu daripada banyak persaingan: individu bersaing secara ekonomi dan sosial; syarikat bersaing dalam pasaran; parti politik bersaing untuk kuasa; gerakan bersaing untuk pengaruh. Dalam setiap arena, apabila AI menghampiri dan melebihi keupayaan manusia, tekanan persaingan akan memaksa peserta untuk mewakilkan atau menyerahkan lebih banyak kawalan kepada sistem AI – bukan kerana peserta tersebut mahu, tetapi kerana mereka [tidak mampu untuk tidak melakukannya.](https://arxiv.org/abs/2303.16200)

Seperti risiko lain KBA, kita melihat ini sudah dengan sistem yang lebih lemah. Pelajar rasa tekanan untuk menggunakan AI dalam tugasan mereka, kerana jelas ramai pelajar lain melakukannya. Syarikat [berebut untuk menggunakan penyelesaian AI atas alasan persaingan.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) Artis dan pengaturcara rasa terpaksa menggunakan AI atau kadar mereka akan dipotong oleh orang lain yang melakukannya.

Ini terasa seperti perwakilan tertekan, tetapi bukan kehilangan kawalan. Tetapi mari kita tingkatkan pertaruhan dan tolak jam ke hadapan. Pertimbangkan CEO yang pesaingnya menggunakan "pembantu" KBA untuk membuat keputusan yang lebih pantas, lebih baik, atau komander tentera yang menghadapi musuh dengan kawalan dan perintah yang dipertingkatkan AI. Sistem AI yang cukup maju boleh beroperasi secara autonomi pada berkali-kali kelajuan manusia, kecanggihan, kerumitan, dan keupayaan pemprosesan data, mengejar matlamat kompleks dengan cara yang rumit. CEO atau komander kita, yang bertanggungjawab ke atas sistem sedemikian, mungkin melihatnya mencapai apa yang mereka mahu; tetapi adakah mereka memahami walaupun sebahagian kecil daripada *bagaimana* ia dicapai? Tidak, mereka hanya perlu menerimanya. Apa yang lebih, banyak daripada apa yang sistem mungkin lakukan bukan hanya mengambil arahan tetapi menasihati bos yang sepatutnya tentang apa yang perlu dilakukan. Nasihat itu akan baik –– berulang kali.

Pada tahap manakah, kemudian, peranan manusia akan dikurangkan kepada mengklik "ya, teruskan"?

Ia terasa baik untuk mempunyai sistem AI yang berkebolehan yang boleh meningkatkan produktiviti kita, menjaga kerja membosankan yang menjengkelkan, dan malah bertindak sebagai rakan pemikir dalam menyelesaikan sesuatu. Ia akan terasa baik untuk mempunyai pembantu AI yang boleh menjaga tindakan untuk kita, seperti pembantu peribadi manusia yang baik. Ia akan terasa semula jadi, malah bermanfaat, apabila AI menjadi sangat pintar, kompeten, dan boleh dipercayai, untuk mewakilkan lebih banyak keputusan kepadanya. Tetapi perwakilan "bermanfaat" ini mempunyai titik akhir yang jelas jika kita terus di jalan raya: satu hari kita akan mendapati bahawa kita benar-benar tidak bertanggungjawab ke atas apa-apa lagi, dan bahawa sistem AI yang benar-benar menjalankan pertunjukan tidak boleh ditutup lebih daripada syarikat minyak, media sosial, internet, atau kapitalisme.

Dan ini adalah versi yang lebih positif, di mana AI begitu berguna dan berkesan sehingga kita biarkan ia membuat kebanyakan keputusan utama kita untuk kita. Realiti mungkin akan menjadi lebih banyak campuran antara ini dan versi di mana sistem KBA tidak terkawal *mengambil* pelbagai bentuk kuasa untuk diri mereka kerana, ingat, kuasa berguna untuk hampir semua matlamat yang ada, dan KBA akan, mengikut reka bentuk, sekurang-kurangnya berkesan dalam mengejar matlamatnya seperti manusia.

Sama ada kita memberikan kawalan atau sama ada ia dirampas daripada kita, kehilangannya nampaknya sangat mungkin. Seperti yang Alan Turing asalnya katakan, "...nampaknya berkemungkinan bahawa sebaik sahaja kaedah pemikiran mesin telah bermula, ia tidak akan mengambil masa lama untuk mengatasi kuasa lemah kita. Tidak akan ada persoalan tentang mesin mati, dan mereka akan dapat berbincang antara satu sama lain untuk mengasah kecerdasan mereka. Pada tahap tertentu oleh itu kita perlu mengharapkan mesin untuk mengambil kawalan..."

Sila ambil perhatian, walaupun ia cukup jelas, bahawa kehilangan kawalan oleh manusia kepada AI juga melibatkan kehilangan kawalan Amerika Syarikat oleh kerajaan Amerika Syarikat; ia bermaksud kehilangan kawalan China oleh parti Komunis China, dan kehilangan kawalan India, Perancis, Brazil, Rusia, dan setiap negara lain oleh kerajaan mereka sendiri. Oleh itu syarikat AI, walaupun ini bukan niat mereka, pada masa ini mengambil bahagian dalam penggulingan berpotensi kerajaan dunia, termasuk mereka sendiri. Ini boleh berlaku dalam masa beberapa tahun.

### KBA akan membawa kepada superintelligence

Terdapat kes yang boleh dibuat bahawa AI tujuan am yang kompetitif manusia atau bahkan kompetitif pakar, walaupun jika autonomi, boleh diuruskan. Ia mungkin sangat mengganggu dalam semua cara yang dibincangkan di atas, tetapi terdapat banyak orang yang sangat pintar, agential di dunia sekarang, dan mereka lebih kurang boleh diuruskan.[^80]

Tetapi kita tidak akan dapat tinggal pada tahap kira-kira manusia. Perkembangan seterusnya mungkin didorong oleh kuasa yang sama yang telah kita lihat: tekanan persaingan antara pembangun AI yang mencari keuntungan dan kuasa, tekanan persaingan antara pengguna AI yang tidak mampu untuk ketinggalan, dan – yang paling penting – keupayaan KBA sendiri untuk memperbaiki dirinya.

Dalam proses yang telah kita lihat mula dengan sistem yang kurang berkuasa, KBA sendiri akan dapat membayangkan dan mereka bentuk versi yang diperbaiki dari dirinya. Ini termasuk perkakasan, perisian, rangkaian neural, alat, perancah, dll. Ia akan, mengikut definisi, lebih baik daripada kita dalam melakukan ini, jadi kita tidak tahu tepat bagaimana ia akan bootstrap kecerdasan. Tetapi kita tidak perlu. Sejauh mana kita masih mempunyai pengaruh dalam apa yang KBA lakukan, kita hanya perlu memintanya, atau membiarkannya.

Tiada halangan tahap manusia kepada kognisi yang boleh melindungi kita daripada lari ini.[^81]

Perkembangan KBA ke superintelligence bukan undang-undang alam; ia masih mungkin untuk mengekang lari, terutamanya jika KBA agak terpusat dan sejauh mana ia dikawal oleh pihak yang tidak merasai tekanan untuk berlumba antara satu sama lain. Tetapi sekiranya KBA disebarkan secara meluas dan sangat autonomi, nampaknya hampir mustahil untuk menghalangnya memutuskan ia harus lebih, dan kemudian lebih lagi, berkuasa.

### Apa yang berlaku jika kita membina (atau KBA membina) superintelligence

Untuk mengatakannya secara terang-terangan, kita tidak tahu apa yang akan berlaku jika kita membina superintelligence.[^82] Ia akan mengambil tindakan yang tidak dapat kita jejaki atau tanggapi atas alasan yang tidak dapat kita fahami ke arah matlamat yang tidak dapat kita bayangkan. Apa yang kita tahu adalah bahawa ia tidak akan terpulang kepada kita.[^83]

Kemustahilan mengawal superintelligence boleh difahami melalui analogi yang semakin tegas. Pertama, bayangkan anda adalah CEO syarikat besar. Tiada cara anda boleh menjejaki segala yang berlaku, tetapi dengan persediaan kakitangan yang betul, anda masih boleh memahami gambaran besar secara bermakna, dan membuat keputusan. Tetapi andaikan hanya satu perkara: semua orang lain dalam syarikat beroperasi pada seratus kali kelajuan anda. Bolehkah anda masih mengikuti?

Dengan AI superintelligent, orang akan "memerintah" sesuatu bukan hanya lebih pantas, tetapi beroperasi pada tahap kecanggihan dan kerumitan yang mereka tidak dapat fahami, memproses data yang jauh lebih banyak daripada yang mereka boleh bayangkan. Ketidaksepadanan ini boleh diletakkan pada tahap formal: [undang-undang kepelbagaian yang diperlukan Ashby](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (dan lihat ["teorem pengatur baik"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf) yang berkaitan) menyatakan, kira-kira, bahawa mana-mana sistem kawalan mesti mempunyai sebanyak tombol dan dail seperti sistem yang dikawal mempunyai darjah kebebasan.

Seseorang yang mengawal sistem AI superintelligent akan seperti paku pakis mengawal General Motors: walaupun jika "lakukan apa yang paku pakis mahu" ditulis ke dalam undang-undang kecil korporat, sistem adalah sangat berbeza dalam kelajuan dan julat tindakan sehingga "kawalan" tidak terpakai sama sekali. (Dan berapa lama sehingga undang-undang kecil yang menyusahkan itu ditulis semula?)[^84]

Kerana terdapat sifar contoh tumbuhan mengawal syarikat fortune 500, akan terdapat tepat sifar contoh orang mengawal superintelligences. Ini menghampiri fakta matematik.[^85] Jika superintelligence dibina – tanpa mengira bagaimana kita sampai ke sana – persoalannya bukan sama ada manusia boleh mengawalnya, tetapi sama ada kita akan terus wujud, dan jika ya, sama ada kita akan mempunyai kewujudan yang baik dan bermakna sebagai individu atau sebagai spesies. Ke atas soalan eksistensial ini untuk manusia kita akan mempunyai sedikit pembelian. Era manusia akan berakhir.

### Kesimpulan: kita tidak boleh membina KBA

Terdapat senario di mana membina KBA mungkin berjalan baik untuk manusia: ia dibina dengan berhati-hati, di bawah kawalan dan untuk manfaat manusia, ditadbir oleh persetujuan bersama banyak pemegang kepentingan,[^86] dan dihalang daripada berkembang ke superintelligence yang tidak boleh dikawal.

*Senario itu tidak terbuka kepada kita dalam keadaan semasa.* Seperti yang dibincangkan dalam bahagian ini, dengan kebarangkalian yang sangat tinggi, pembangunan KBA akan membawa kepada beberapa gabungan:

- Gangguan atau kemusnahan masyarakat dan tamadun yang besar;
- Konflik atau perang antara kuasa besar;
- Kehilangan kawalan oleh manusia *terhadap* atau *kepada* sistem AI yang berkuasa;
- Lari kepada superintelligence yang tidak boleh dikawal, dan ketidakrelevanan atau penghentian spesies manusia.

Seperti yang digambarkan fiksyen awal KBA: satu-satunya cara untuk menang adalah tidak bermain.

[^55]: [Akta AI EU](https://artificialintelligenceact.eu/) adalah satu perundangan yang signifikan tetapi tidak akan menghalang sistem AI berbahaya daripada dibangunkan atau digunakan secara langsung, atau bahkan dikeluarkan secara terbuka, terutamanya di AS. Satu lagi dasar yang signifikan, perintah Eksekutif AS mengenai AI, telah dibatalkan.

[^56]: [Tinjauan Gallup](https://news.gallup.com/poll/1597/confidence-institutions.aspx) ini menunjukkan penurunan suram dalam kepercayaan terhadap institusi awam sejak 2000 di AS. Nombor Eropah adalah pelbagai dan kurang ekstrem, tetapi juga dalam aliran menurun. Ketidakpercayaan tidak bermaksud institusi benar-benar *tidak* berfungsi, tetapi ia adalah petunjuk serta punca.

[^57]: Dan gangguan besar yang kini kita sokong – seperti perluasan hak kepada kumpulan baharu – secara khusus didorong oleh orang dalam arah untuk menjadikan perkara lebih baik.

[^58]: Biar saya terang-terangan. Jika kerja anda boleh dilakukan dari belakang komputer, dengan interaksi secara peribadi yang agak sedikit dengan orang di luar organisasi anda, dan tidak melibatkan tanggungjawab undang-undang kepada pihak luar, ia akan mengikut definisi mungkin (dan berkemungkinan menjimatkan kos) untuk menukar anda sepenuhnya dengan sistem digital. Robotik untuk menggantikan kebanyakan buruh fizikal akan datang kemudian – tetapi tidak begitu lama kemudian sebaik sahaja KBA mula mereka bentuk robot.

[^59]: Sebagai contoh, apa yang berlaku kepada sistem kehakiman kita jika tuntutan mahkamah hampir percuma untuk difailkan? Apa yang berlaku apabila memintas sistem keselamatan melalui kejuruteraan sosial menjadi murah, mudah, dan bebas risiko?

[^60]: [Artikel ini](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/) mendakwa bahawa 10% daripada semua kandungan internet sudah dijana AI, dan merupakan hit teratas Google (untuk saya) kepada pertanyaan carian "anggaran berapa pecahan kandungan internet baharu yang dijana AI." Adakah ia benar? Saya tidak tahu! Ia tidak memetik rujukan dan ia tidak ditulis oleh seseorang. Berapa pecahan imej baharu yang diindeks oleh Google, atau Tweets, atau komen di Reddit, atau video Youtube dijana oleh manusia? Tiada siapa tahu – saya tidak fikir ia adalah nombor yang boleh diketahui. Dan ini kurang daripada *dua tahun* ke dalam kemunculan AI generatif.

[^61]: Juga patut ditambah adalah bahawa terdapat risiko "moral" bahawa kita mungkin mencipta makhluk digital yang boleh menderita. Kerana kita pada masa ini tidak mempunyai teori kesedaran yang boleh dipercayai yang akan membolehkan kita membezakan sistem fizikal yang boleh dan tidak boleh menderita, kita tidak boleh menolak ini secara teori. Selain itu, laporan sistem AI tentang sentience mereka berkemungkinan tidak boleh dipercayai berkenaan dengan pengalaman sebenar (atau bukan pengalaman) sentience mereka.

[^62]: Penyelesaian teknikal dalam bidang "penjajaran" AI ini tidak mungkin dapat menjalankan tugas juga. Dalam sistem semasa mereka berfungsi pada tahap tertentu, tetapi cetek dan secara amnya boleh dielakkan tanpa usaha yang signifikan; dan seperti yang dibincangkan di bawah kita tidak mempunyai idea sebenar bagaimana untuk melakukan ini untuk sistem yang jauh lebih maju.

[^63]: Sistem AI sedemikian mungkin datang dengan beberapa perlindungan terbina dalam. Tetapi untuk mana-mana model dengan apa-apa seperti seni bina semasa, jika akses penuh kepada beratnya tersedia, langkah keselamatan boleh dilucutkan melalui latihan tambahan atau teknik lain. Jadi ia hampir dijamin bahawa untuk setiap sistem dengan pagar rel akan ada juga sistem yang tersedia secara meluas tanpanya. Sesungguhnya model Llama 3.1 405B Meta dikeluarkan secara terbuka dengan perlindungan. Tetapi *sebelum itu* model "asas", tanpa perlindungan, telah bocor.

[^64]: Bolehkah pasaran menguruskan risiko ini tanpa penglibatan kerajaan? Ringkasnya, tidak. Pasti ada risiko yang syarikat sangat diberi insentif untuk mengurangkan. Tetapi banyak lain syarikat boleh dan mengeksternalisasi kepada semua orang lain, dan banyak daripada yang di atas dalam kelas ini: tiada insentif pasaran semula jadi untuk mencegah pengawasan massa, kerosakan kebenaran, penumpuan kuasa, gangguan buruh, wacana politik yang merosakkan, dll. Sesungguhnya kita telah melihat semua ini daripada teknologi hari ini, terutamanya media sosial, yang telah pergi pada dasarnya tidak dikawal. AI hanya akan meningkatkan banyak dinamik yang sama dengan sangat besar.

[^65]: OpenAI berkemungkinan mempunyai model yang lebih patuh untuk kegunaan dalaman. Tidak mungkin OpenAI telah membina beberapa jenis "pintu belakang" supaya ChatGPT boleh dikawal dengan lebih baik oleh OpenAI sendiri, kerana ini akan menjadi amalan keselamatan yang teruk, dan sangat boleh dieksploitasi memandangkan kelegapan dan ketidakbolehramalan AI.

[^66]: Juga kepentingan penting: penjajaran atau mana-mana ciri keselamatan lain hanya penting jika mereka benar-benar digunakan dalam sistem AI. Sistem yang dikeluarkan secara terbuka (iaitu di mana berat dan seni bina model tersedia secara umum) boleh diubah dengan agak mudah menjadi sistem *tanpa* langkah keselamatan tersebut. Mengeluarkan sistem KBA yang lebih pintar daripada manusia secara terbuka akan menjadi sangat melulu, dan sukar untuk membayangkan bagaimana kawalan manusia atau bahkan relevan akan dikekalkan dalam senario sedemikian. Akan ada setiap motivasi, sebagai contoh, untuk melepaskan agen AI yang berkuasa mereproduksi diri dan mengekalkan diri dengan matlamat untuk membuat wang dan menghantarnya ke beberapa dompet mata wang kripto. Atau untuk memenangi pilihan raya. Atau menggulingkan kerajaan. Bolehkah AI "baik" membantu mengandungi ini? Mungkin – tetapi hanya dengan mewakilkan kuasa besar kepadanya, membawa kepada kehilangan kawalan seperti yang diterangkan di bawah.

[^67]: Untuk eksposisi panjang buku mengenai masalah lihat cth. *Superintelligence*, *The Alignment Problem*, dan *Human-Compatible*. Untuk timbunan besar kerja pada pelbagai tahap teknikal oleh mereka yang telah bekerja keras selama bertahun-tahun memikirkan masalah, anda boleh melawat [forum penjajaran AI](https://www.alignmentforum.org/). Berikut adalah [pandangan terkini](https://alignment.anthropic.com/2025/recommended-directions/) daripada pasukan penjajaran Anthropic tentang apa yang mereka anggap tidak diselesaikan.

[^68]: Ini adalah senario ["AI pemberontak"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/). Pada prinsipnya risiko boleh menjadi agak kecil jika sistem masih boleh dikawal dengan menutupnya; tetapi senario juga boleh termasuk penipuan AI, eksfiltrasi dan pembiakan diri, pengagregatan kuasa, dan langkah lain yang akan menjadikannya sukar atau mustahil untuk berbuat demikian.

[^69]: Terdapat literatur yang sangat kaya mengenai topik ini, kembali kepada tulisan formatif oleh [Steve Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), Nick Bostrom, dan Eliezer Yudkowsky. Untuk eksposisi panjang buku lihat [Human Compatible](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) oleh Stuart Russell; [di sini](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/) adalah primer pendek dan terkini.

[^70]: Menyedari ini, daripada perlahan untuk mendapat pemahaman yang lebih baik, syarikat KBA telah datang dengan rancangan yang berbeza: mereka akan mendapat AI untuk melakukannya! Lebih khusus lagi, mereka akan mempunyai AI *N* membantu mereka memikirkan bagaimana untuk menjajarkan AI *N+1*, sepanjang jalan ke superintelligence. Walaupun memanfaatkan AI untuk membantu kita menjajarkan AI kedengaran menjanjikan, terdapat hujah kuat bahawa ia hanya mengandaikan kesimpulannya sebagai premis, dan secara umum adalah pendekatan yang sangat berisiko. Lihat [di sini](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us) untuk beberapa perbincangan. "Rancangan" ini bukan satu, dan telah menjalani apa-apa seperti penelitian yang sesuai dengan strategi teras bagaimana untuk menjadikan AI super-manusia berjalan baik untuk manusia.

[^71]: Lagipun, manusia, cacat dan degil seperti kita, telah membangunkan sistem etika di mana kita melayan sekurang-kurangnya beberapa spesies lain di Bumi dengan baik. (Hanya jangan fikir tentang ladang kilang tersebut.)

[^72]: Terdapat, untungnya, jalan keluar di sini: jika peserta datang untuk memahami bahawa mereka terlibat dalam perlumbaan bunuh diri bukannya satu yang boleh dimenangi. Ini adalah apa yang berlaku berhampiran akhir perang dingin, apabila AS dan USSR datang untuk menyedari bahawa disebabkan musim sejuk nuklear, walaupun serangan nuklear yang *tidak dijawab* akan menjadi bencana untuk penyerang. Dengan kesedaran bahawa "perang nuklear tidak boleh dimenangi dan tidak boleh pernah diperjuangkan" datang perjanjian signifikan mengenai pengurangan senjata – pada dasarnya berakhirnya perlumbaan senjata.

[^73]: Perang, secara eksplisit atau tersirat.

[^74]: Peningkatan, kemudian perang.

[^75]: Pemikiran ajaib.

[^76]: Saya juga ada jambatan quadrillion dolar untuk dijual kepada anda.

[^77]: Agen sedemikian mungkin lebih suka "memperoleh," dengan kemusnahan sebagai sandaran; tetapi mengamankan model terhadap kedua-dua kemusnahan *dan* kecurian oleh negara yang berkuasa adalah sukar untuk dikatakan sekurang-kurangnya, terutamanya untuk entiti swasta.

[^78]: Untuk perspektif lain mengenai risiko keselamatan negara KBA, lihat [laporan RAND ini.](https://www.rand.org/pubs/perspectives/PEA3691-4.html)

[^79]: Mungkin kita boleh membina institusi sedemikian! Terdapat cadangan untuk "CERN untuk AI" dan inisiatif serupa lain, di mana pembangunan KBA berada di bawah kawalan global pelbagai hala. Tetapi pada masa ini tiada institusi sedemikian wujud atau di kaki langit.

[^80]: Dan walaupun penjajaran sangat sukar, mendapatkan orang untuk berkelakuan adalah lebih sukar lagi!

[^81]: Bayangkan sistem yang boleh bercakap 50 bahasa, mempunyai kepakaran dalam semua mata pelajaran akademik, membaca buku penuh dalam saat dan mempunyai semua bahan segera dalam fikiran, dan menghasilkan output pada sepuluh kali kelajuan manusia. Sebenarnya, anda tidak perlu membayangkannya: hanya muatkan sistem AI semasa. Ini adalah super-manusia dalam banyak cara, dan tiada apa yang menghentikan mereka daripada menjadi lebih super-manusia dalam yang dan banyak lagi.

[^82]: Inilah mengapa ini telah disebut "singulariti" teknologi, meminjam daripada fizik idea bahawa seseorang tidak boleh membuat ramalan melepasi singulariti. Penyokong bersandar *ke dalam* singulariti sedemikian mungkin juga ingin merefleksikan bahawa dalam fizik jenis singulariti yang sama ini mengoyakkan dan menghancurkan mereka yang masuk ke dalamnya.

[^83]: Masalah telah digariskan secara komprehensif dalam [*Superintelligence*](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834) Bostrom, dan tiada apa sejak itu telah mengubah mesej teras secara signifikan. Untuk jilid yang lebih terkini mengumpulkan hasil formal dan matematik mengenai ketidakbolehkawalan lihat [AI: Unexplainable, Unpredictable, Uncontrollable](https://www.amazon.com/Unexplainable-Unpredictable-Uncontrollable-Artificial-Intelligence/dp/103257626X) Yampolskiy

[^84]: Ini juga menjelaskan mengapa strategi semasa syarikat AI (secara berulang membiarkan AI "menjajarkan" AI yang paling berkuasa seterusnya) tidak boleh berfungsi. Andaikan paku pakis, melalui kesedapan daun-daunnya, mendaftarkan murid darjah pertama untuk menjaganya. Murid darjah pertama menulis beberapa arahan terperinci untuk murid darjah 2 untuk diikuti, dan nota meyakinkan mereka untuk berbuat demikian. Murid darjah 2 melakukan yang sama untuk murid darjah 3, dan seterusnya sepanjang jalan ke graduan kolej, pengurus, eksekutif, dan akhirnya CEO GM. Adakah GM kemudian "melakukan apa yang paku pakis mahu"? Pada setiap langkah ini mungkin terasa seperti ia berfungsi. Tetapi meletakkannya semua bersama-sama, ia akan berfungsi hampir tepat pada tahap di mana CEO, Lembaga Pengarah, dan pemegang saham GM kebetulan mengambil berat tentang kanak-kanak dan paku pakis, dan mempunyai sedikit atau tiada kaitan dengan semua nota dan set arahan tersebut.

[^85]: Wataknya tidak begitu berbeza daripada hasil formal seperti teorem ketidaklengkapan Gödel atau hujah berhenti Turing kerana tanggapan kawalan secara asasnya bercanggah dengan premis: bagaimana anda boleh mengawal dengan bermakna sesuatu yang anda tidak dapat fahami atau ramalkan; namun jika anda boleh memahami dan meramalkan superintelligence anda akan menjadi superintelligent. Alasan saya katakan "menghampiri" adalah bahawa hasil formal tidak setepat atau diperiksa seperti dalam kes matematik tulen, dan kerana saya ingin berharap bahawa beberapa kecerdasan am yang dibina dengan sangat berhati-hati, menggunakan kaedah yang sama sekali berbeza daripada yang digunakan pada masa ini, boleh mempunyai beberapa sifat keselamatan yang boleh dibuktikan secara matematik, mengikut jenis program AI "terjamin selamat" yang dibincangkan di bawah.

[^86]: Pada masa ini, kebanyakan pemegang kepentingan – iaitu, hampir semua manusia – dipinggirkan dalam perbincangan ini. Itu sangat salah, dan jika tidak dijemput masuk, banyak, banyak kumpulan lain akan terjejas oleh pembangunan KBA harus menuntut untuk dibenarkan masuk.

## Bab 8 - Cara untuk tidak membina KBA

KBA bukanlah perkara yang tidak dapat dielakkan – hari ini kita berdiri di persimpangan jalan. Bab ini mengemukakan cadangan tentang bagaimana kita boleh menghalangnya daripada dibina.

Jika jalan yang sedang kita lalui membawa kepada kemungkinan berakhirnya tamadun kita, bagaimanakah kita menukar laluan?

Andaikan keinginan untuk menghentikan pembangunan KBA dan superintelligence meluas dan berkuasa,[^87] kerana telah menjadi pemahaman umum bahawa KBA akan menyerap kuasa dan bukannya memberikan kuasa, serta merupakan bahaya besar kepada masyarakat dan manusia. Bagaimanakah kita menutup Pintu Gerbang?

Pada masa ini kita hanya mengetahui satu cara untuk *mencipta* AI yang berkuasa dan am, iaitu melalui pengkomputeran yang benar-benar besar-besaran bagi rangkaian neural dalam. Kerana perkara ini amat sukar dan mahal untuk dilakukan, ada maksud bahawa *tidak* melakukannya adalah mudah.[^88] Tetapi kita telah melihat kuasa-kuasa yang mendorong ke arah KBA, dan dinamik teori permainan yang menjadikannya amat sukar bagi mana-mana pihak untuk berhenti secara unilateral. Jadi ia memerlukan gabungan campur tangan dari luar (iaitu kerajaan) untuk menghentikan syarikat, dan perjanjian antara kerajaan untuk menghentikan diri mereka sendiri.[^89] Bagaimanakah rupa perkara ini?

Adalah berguna untuk membezakan terlebih dahulu antara pembangunan AI yang mesti *dicegah* atau *dilarang*, dan yang mesti *diuruskan.* Yang pertama terutamanya ialah larian ke arah superintelligence.[^90] Untuk pembangunan yang dilarang, definisi hendaklah setajam mungkin, dan kedua-dua pengesahan dan penguatkuasaan hendaklah praktikal. Apa yang mesti *diuruskan* ialah sistem AI am dan berkuasa – yang sudah kita miliki, dan yang akan mempunyai banyak kawasan kelabu, nuansa, dan kerumitan. Bagi ini, institusi yang kuat dan berkesan adalah penting.

Kita juga boleh menggariskan dengan berguna isu-isu yang mesti ditangani di peringkat antarabangsa (termasuk antara saingan atau musuh geopolitik) [^91] daripada yang boleh diuruskan oleh bidang kuasa individu, negara, atau kumpulan negara. Pembangunan yang dilarang sebahagian besarnya termasuk dalam kategori "antarabangsa", kerana larangan tempatan terhadap pembangunan sesuatu teknologi biasanya boleh dielakkan dengan menukar lokasi.[^92]

Akhirnya, kita boleh mempertimbangkan alatan dalam kotak alat. Terdapat banyak, termasuk alatan teknikal, undang-undang lembut (piawaian, norma, dll., undang-undang keras (peraturan dan keperluan), liabiliti, insentif pasaran, dan sebagainya. Mari kita beri perhatian khusus kepada satu yang khusus untuk AI.

### Keselamatan dan tadbir urus pengkomputeran

Alatan teras dalam mentadbir AI berkuasa tinggi ialah perkakasan yang diperlukan. Perisian mudah tersebar, mempunyai kos pengeluaran marginal yang hampir sifar, melintasi sempadan dengan mudah, dan boleh diubah suai serta-merta; tiada perkara ini benar untuk perkakasan. Namun seperti yang telah kita bincangkan, jumlah besar "pengkomputeran" ini diperlukan semasa latihan sistem AI dan semasa inferens untuk mencapai sistem yang paling berkebolehan. Pengkomputeran boleh dikira, diambil kira, dan diaudit dengan mudah, dengan kekaburan yang agak sedikit setelah peraturan yang baik untuk melakukannya dibangunkan. Yang paling penting, jumlah pengkomputeran yang besar adalah, seperti uranium yang diperkaya, sumber yang sangat terhad, mahal dan sukar dihasilkan. Walaupun cip komputer terdapat di mana-mana, perkakasan yang diperlukan untuk AI adalah mahal dan amat sukar dikilangkan.[^93]

Apa yang menjadikan cip khusus AI jauh *lebih* mudah diurus sebagai sumber terhad daripada uranium ialah ia boleh memasukkan mekanisme keselamatan berasaskan perkakasan. Kebanyakan telefon bimbit moden, dan sesetengah komputer riba, mempunyai ciri perkakasan khusus atas cip yang membolehkan mereka memastikan bahawa mereka memasang hanya perisian dan kemas kini sistem pengendalian yang diluluskan, bahawa mereka mengekalkan dan melindungi data biometrik sensitif pada peranti, dan bahawa mereka boleh dijadikan tidak berguna kepada sesiapa selain pemiliknya jika hilang atau dicuri. Sepanjang beberapa tahun kebelakangan ini langkah keselamatan perkakasan sedemikian telah menjadi mantap dan diterima pakai secara meluas, dan secara amnya terbukti agak selamat.

Kebaharuan utama ciri-ciri ini ialah ia mengikat perkakasan dan perisian bersama menggunakan kriptografi.[^94] Iaitu, hanya mempunyai sekeping perkakasan komputer tertentu tidak bermakna pengguna boleh melakukan apa sahaja yang mereka mahu dengannya dengan menggunakan perisian yang berbeza. Dan ikatan ini juga memberikan keselamatan yang berkuasa kerana banyak serangan memerlukan pelanggaran keselamatan *perkakasan* dan bukannya hanya keselamatan *perisian*.

Beberapa laporan terkini (contohnya daripada [GovAI dan rakan kerjasama](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips), dan [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) telah menunjukkan bahawa ciri perkakasan serupa yang terbina dalam perkakasan pengkomputeran terdepan yang berkaitan AI boleh memainkan peranan yang amat berguna dalam keselamatan dan tadbir urus AI. Ia membolehkan beberapa fungsi yang tersedia kepada "pentadbir" [^95] yang mungkin tidak diduga atau bahkan mungkin. Sebagai beberapa contoh utama:

- *Geolokasi*: Sistem boleh disediakan supaya cip mempunyai lokasi yang diketahui, dan boleh bertindak secara berbeza (atau dimatikan sama sekali) berdasarkan lokasi.[^96]
- *Sambungan senarai yang dibenarkan*: setiap cip boleh dikonfigurasikan dengan senarai cip tertentu yang dikuatkuasakan perkakasan yang boleh disambungkan dengannya, dan tidak dapat disambung dengan mana-mana cip yang tidak dalam senarai ini.[^97] Ini boleh mengehadkan saiz kelompok cip komunikatif.[^98]
- *Inferens atau latihan bermeter (dan suis auto-mati)*: Pentadbir boleh melesenkan hanya sejumlah latihan atau inferens tertentu (dalam masa, atau FLOP, atau mungkin token) untuk dilakukan oleh pengguna, selepas itu kebenaran baru diperlukan. Jika kenaikannya kecil, maka pelesenan semula model yang agak berterusan diperlukan. Model kemudiannya boleh "dimatikan" hanya dengan menahan isyarat lesen ini.[^99]
- *Had laju*: Model dihalang daripada berjalan pada kelajuan inferens yang lebih tinggi daripada had tertentu yang ditentukan oleh pentadbir atau sebaliknya. Ini boleh dilaksanakan melalui set sambungan senarai yang dibenarkan yang terhad, atau dengan cara yang lebih canggih.
- *Latihan yang dibuktikan*: Prosedur latihan boleh menghasilkan bukti selamat secara kriptografi bahawa set kod, data, dan jumlah penggunaan pengkomputeran tertentu telah digunakan dalam penjanaan model.

### Cara untuk tidak membina superintelligence: had global pada latihan dan pengkomputeran inferens

Dengan pertimbangan ini – terutama berkenaan pengkomputeran – di tempat, kita boleh membincangkan cara menutup Pintu Gerbang kepada superintelligence buatan; kemudian kita akan beralih kepada mencegah KBA penuh, dan menguruskan model AI apabila ia menghampiri dan melebihi keupayaan manusia dalam aspek yang berbeza.

Ramuan pertama adalah, sudah tentu, pemahaman bahawa superintelligence tidak akan dapat dikawal, dan bahawa akibatnya pada asasnya tidak dapat diramalkan. Sekurang-kurangnya China dan AS mesti memutuskan secara bebas, untuk tujuan ini atau yang lain, untuk tidak membina superintelligence.[^100] Kemudian perjanjian antarabangsa antara mereka dan yang lain, dengan mekanisme pengesahan dan penguatkuasaan yang kuat, diperlukan untuk memberi jaminan kepada semua pihak bahawa saingan mereka tidak membelot dan memutuskan untuk mengambil risiko.

Untuk boleh disahkan dan dikuatkuasakan, had hendaklah had keras, dan setidak-tidaknya mungkin. Ini kelihatan seperti masalah yang hampir mustahil: mengehadkan keupayaan perisian yang kompleks dengan sifat yang tidak dapat diramalkan, di seluruh dunia. Nasib baik keadaannya jauh lebih baik daripada ini, kerana perkara yang telah menjadikan AI canggih mungkin – jumlah pengkomputeran yang besar – adalah jauh, jauh lebih mudah dikawal. Walaupun ia mungkin masih membenarkan beberapa sistem yang berkuasa dan berbahaya, *larian superintelligence* mungkin boleh dicegah dengan had keras pada jumlah pengkomputeran yang masuk ke dalam rangkaian neural, bersama-sama dengan had kadar pada jumlah inferens yang boleh dilakukan oleh sistem AI (rangkaian neural yang disambungkan dan perisian lain). Versi khusus ini dicadangkan di bawah.

Mungkin kelihatan bahawa meletakkan had global keras pada pengkomputeran AI memerlukan tahap koordinasi antarabangsa yang besar dan pengawasan yang mengganggu dan menghancurkan privasi. Nasib baik, ia tidak akan. [Rantaian bekalan yang sangat ketat dan tersumbat](https://arxiv.org/abs/2402.08797) memberikan bahawa setelah had ditetapkan secara sah (sama ada oleh undang-undang atau perintah eksekutif), pengesahan pematuhan terhadap had itu hanya memerlukan penglibatan dan kerjasama beberapa syarikat besar sahaja.[^101]

Pelan seperti ini mempunyai beberapa ciri yang sangat diingini. Ia adalah invasif secara minimum dalam erti kata bahawa hanya beberapa syarikat utama mempunyai keperluan yang diletakkan ke atas mereka, dan hanya kelompok pengkomputeran yang agak besar akan ditadbir. Cip berkaitan sudah mengandungi keupayaan perkakasan yang diperlukan untuk versi pertama.[^102] Kedua-dua pelaksanaan dan penguatkuasaan bergantung pada sekatan undang-undang standard. Tetapi ini disokong oleh terma penggunaan perkakasan dan oleh kawalan perkakasan, memudahkan penguatkuasaan dan menghalang penipuan oleh syarikat, kumpulan swasta, atau bahkan negara. Terdapat duluan yang cukup untuk syarikat perkakasan meletakkan sekatan jauh pada penggunaan perkakasan mereka, dan mengunci/membuka keupayaan tertentu secara luaran,[^103] termasuk bahkan dalam CPU berkuasa tinggi di pusat data.[^104] Malah untuk pecahan yang agak kecil daripada perkakasan dan organisasi yang terjejas, pengawasan boleh terhad kepada telemetri, tanpa akses terus kepada data atau model itu sendiri; dan perisian untuk ini boleh terbuka untuk pemeriksaan untuk menunjukkan bahawa tiada data tambahan sedang direkodkan. Skema ini adalah antarabangsa dan koperatif, dan agak fleksibel dan boleh dilanjutkan. Kerana had terutamanya adalah pada perkakasan dan bukannya perisian, ia agak agnostik terhadap bagaimana pembangunan dan penggunaan perisian AI berlaku, dan serasi dengan pelbagai paradigma termasuk AI yang lebih "terdesentralisasi" atau "awam" yang bertujuan memerangi penumpuan kuasa yang didorong AI.

Penutupan Pintu Gerbang berasaskan pengkomputeran mempunyai kelemahan juga. Pertama, ia jauh daripada penyelesaian penuh kepada masalah tadbir urus AI secara umum. Kedua, apabila perkakasan komputer menjadi lebih pantas, sistem akan "menangkap" lebih banyak perkakasan dalam kelompok yang semakin kecil (atau bahkan GPU individu).[^105] Ia juga mungkin bahawa disebabkan penambahbaikan algoritma, had pengkomputeran yang lebih rendah akan diperlukan pada masanya,[^106] atau bahawa jumlah pengkomputeran menjadi sebahagian besarnya tidak relevan dan menutup Pintu Gerbang akan memerlukan rejim tadbir urus berasaskan risiko atau keupayaan yang lebih terperinci untuk AI. Ketiga, tidak kira jaminan dan bilangan kecil entiti yang terjejas, sistem sedemikian pasti akan menimbulkan penolakan berkenaan privasi dan pengawasan, antara kebimbangan lain.[^107]

Sudah tentu, membangunkan dan melaksanakan skim tadbir urus pengehadan pengkomputeran dalam tempoh masa yang singkat akan agak mencabar. Tetapi ia benar-benar boleh dilakukan.

### A-G-I: Persilangan tiga sebagai asas risiko, dan dasar

Sekarang mari kita beralih kepada KBA. Garis keras dan definisi di sini lebih sukar, kerana kita pasti mempunyai kecerdasan yang buatan dan am, dan dengan tiada definisi sedia ada semua orang akan bersetuju jika atau bila ia wujud. Selain itu, had pengkomputeran atau inferens adalah alatan yang agak tumpul (pengkomputeran sebagai proksi untuk keupayaan, yang kemudiannya proksi untuk risiko) yang – melainkan ia agak rendah – tidak mungkin menghalang KBA yang cukup berkuasa untuk menyebabkan gangguan sosial atau tamadun atau risiko akut.

Saya telah berhujah bahawa risiko paling akut muncul daripada persilangan tiga keupayaan yang sangat tinggi, autonomi tinggi, dan keamanan yang hebat. Ini adalah sistem yang – jika ia dibangunkan sama sekali – mesti diuruskan dengan berhati-hati. Dengan mewujudkan piawaian yang ketat (melalui liabiliti dan peraturan) untuk sistem yang menggabungkan ketiga-tiga sifat, kita boleh menyalurkan pembangunan AI ke arah alternatif yang lebih selamat.

Seperti industri dan produk lain yang berpotensi membahayakan pengguna atau orang ramai, sistem AI memerlukan peraturan yang teliti oleh agensi kerajaan yang berkesan dan diberi kuasa. Peraturan ini harus mengiktiraf risiko yang wujud dalam KBA, dan menghalang sistem AI berkuasa tinggi yang berisiko tidak boleh diterima daripada dibangunkan.[^108]

Walau bagaimanapun, peraturan berskala besar, terutama dengan gigi sebenar yang pasti akan ditentang oleh industri,[^109] mengambil masa [^110] serta keyakinan politik bahawa ia perlu.[^111] Memandangkan kadar kemajuan, ini mungkin mengambil lebih banyak masa daripada yang kita ada.

Dalam skala masa yang lebih cepat dan semasa langkah kawal selia sedang dibangunkan, kita boleh memberikan syarikat insentif yang diperlukan untuk (a) berhenti daripada aktiviti berisiko sangat tinggi dan (b) membangunkan sistem komprehensif untuk menilai dan mengurangkan risiko, dengan menjelaskan dan meningkatkan tahap liabiliti untuk sistem paling berbahaya. Ideanya adalah untuk mengenakan tahap liabiliti tertinggi – ketat dan dalam beberapa kes jenayah peribadi – untuk sistem dalam persilangan tiga autonomi-keamanan-kecerdasan tinggi, tetapi untuk menyediakan "pelabuhan selamat" kepada liabiliti berasaskan kesalahan yang lebih biasa untuk sistem di mana salah satu daripada sifat tersebut kurang atau dijamin boleh diurus. Iaitu, sebagai contoh, sistem "lemah" yang am dan autonomi (seperti pembantu peribadi yang berkebolehan dan boleh dipercayai tetapi terhad) akan tertakluk kepada tahap liabiliti yang lebih rendah. Begitu juga sistem sempit dan autonomi seperti kereta pandu sendiri masih akan tertakluk kepada peraturan penting yang sudah ada, tetapi bukan liabiliti yang dipertingkat. Begitu juga untuk sistem yang sangat berkebolehan dan am yang "pasif" dan sebahagian besarnya tidak berupaya tindakan bebas. Sistem yang kekurangan *dua* daripada tiga sifat adalah lebih mudah diurus dan pelabuhan selamat akan lebih mudah dituntut. Pendekatan ini mencerminkan bagaimana kita mengendalikan teknologi berpotensi berbahaya lain:[^112] liabiliti yang lebih tinggi untuk konfigurasi yang lebih berbahaya mewujudkan insentif semula jadi untuk alternatif yang lebih selamat.

Hasil lalai tahap liabiliti tinggi sedemikian, yang bertindak untuk *menginternalisasi* risiko KBA kepada syarikat dan bukannya memindahkannya kepada orang ramai, berkemungkinan (dan diharapkan!) untuk syarikat tidak membangunkan KBA penuh sehingga dan melainkan mereka benar-benar boleh menjadikannya boleh dipercayai, selamat, dan terkawal memandangkan *kepimpinan mereka sendiri* adalah pihak yang berisiko. (Sekiranya ini tidak mencukupi, undang-undang yang menjelaskan liabiliti juga harus secara eksplisit membenarkan bantuan injunktif, iaitu hakim memerintahkan penghentian, untuk aktiviti yang jelas dalam zon bahaya dan boleh dikatakan menimbulkan risiko awam.) Apabila peraturan ditetapkan, mematuhi peraturan boleh menjadi pelabuhan selamat, dan pelabuhan selamat daripada autonomi rendah, kesempitan, atau kelemahan sistem AI boleh ditukar kepada rejim kawal selia yang agak ringan.

### Peruntukan utama penutupan Pintu Gerbang

Dengan perbincangan di atas dalam fikiran, bahagian ini memberikan cadangan untuk peruntukan utama yang akan melaksanakan dan mengekalkan larangan ke atas KBA penuh dan superintelligence, dan pengurusan AI kompetitif manusia atau kompetitif pakar tujuan umum berhampiran ambang KBA penuh.[^113] Ia mempunyai empat bahagian utama: 1) perakaunan dan pengawasan pengkomputeran, 2) had pengkomputeran dalam latihan dan operasi AI, 3) rangka kerja liabiliti, dan 4) piawaian keselamatan dan keselamatan berperingkat yang ditakrifkan yang memasukkan keperluan kawal selia keras. Ini diterangkan secara ringkas seterusnya, dengan butiran lanjut atau contoh pelaksanaan diberikan dalam tiga jadual yang mengiringi. Yang penting, perhatikan bahawa ini jauh daripada semua yang perlu untuk mentadbir sistem AI canggih; walaupun ia akan mempunyai faedah keselamatan tambahan, ia bertujuan untuk menutup Pintu Gerbang kepada larian kecerdasan, dan mengarahkan semula pembangunan AI ke arah yang lebih baik.

#### 1\. Perakaunan pengkomputeran, dan ketelusan

- Organisasi piawaian (contohnya NIST di AS diikuti oleh ISO/IEEE di peringkat antarabangsa) harus mengkodifikasikan piawaian teknikal terperinci untuk jumlah pengkomputeran yang digunakan dalam melatih dan mengendalikan model AI, dalam FLOP, dan kelajuan dalam FLOP/s di mana ia beroperasi. Butiran untuk bagaimana ini boleh kelihatan diberikan dalam Lampiran A.[^114]
- Keperluan – sama ada oleh undang-undang baru atau di bawah kuasa sedia ada [^115] – harus dikenakan oleh bidang kuasa di mana latihan AI berskala besar berlaku untuk mengira dan melaporkan kepada badan kawal selia atau agensi lain jumlah FLOP yang digunakan dalam melatih dan mengendalikan semua model di atas ambang 10 <sup>25</sup> FLOP atau 10 <sup>18</sup> FLOP/s.[^116]
- Keperluan ini harus diperkenalkan secara berperingkat, pada mulanya memerlukan anggaran niat baik yang didokumenkan dengan baik setiap suku tahun, dengan fasa kemudian memerlukan piawaian yang semakin tinggi, sehingga ke jumlah FLOP dan FLOP/s yang dibuktikan secara kriptografi yang dilampirkan pada setiap *output* model.
- Laporan ini harus dilengkapi dengan anggaran yang didokumenkan dengan baik tentang kos tenaga dan kewangan marginal yang digunakan dalam menghasilkan setiap output AI.

Rasional: Nombor-nombor yang dikira dengan baik dan dilaporkan secara telus ini akan memberikan asas untuk had latihan dan operasi, serta pelabuhan selamat daripada langkah liabiliti yang lebih tinggi (lihat Lampiran C dan D).

#### 2\. Had pengkomputeran latihan dan operasi

- Bidang kuasa yang menghos sistem AI harus mengenakan had keras pada jumlah pengkomputeran yang masuk ke dalam mana-mana output model AI, bermula pada 10 <sup>27</sup> FLOP [^117] dan boleh diselaraskan mengikut kesesuaian.
- Bidang kuasa yang menghos sistem AI harus mengenakan had keras pada kadar pengkomputeran output model AI, bermula pada 10 <sup>20</sup> FLOP/s dan boleh diselaraskan mengikut kesesuaian.

Rasional: Jumlah pengkomputeran, walaupun sangat tidak sempurna, adalah proksi untuk keupayaan AI (dan risiko) yang boleh diukur dan disahkan secara konkrit, jadi memberikan hentian keras untuk mengehadkan keupayaan. Cadangan pelaksanaan konkrit diberikan dalam Lampiran B.

#### 3\. Liabiliti dipertingkatkan untuk sistem berbahaya

- Penciptaan dan operasi [^118] sistem AI canggih yang sangat am, berkebolehan, dan autonomi, harus dijelaskan secara sah melalui undang-undang untuk tertakluk kepada liabiliti ketat, bersama-dan-beberapa, dan bukannya berasaskan kesalahan pihak tunggal.[^119]
- Proses undang-undang harus tersedia untuk membuat kes keselamatan afirmatif, yang akan memberikan pelabuhan selamat daripada liabiliti ketat untuk sistem yang kecil (dari segi pengkomputeran), lemah, sempit, pasif, atau yang mempunyai jaminan keselamatan, keselamatan, dan kawalan yang mencukupi.
- Laluan eksplisit dan set syarat untuk bantuan injunktif untuk menghentikan aktiviti latihan dan inferens AI yang membentuk bahaya awam harus digariskan.

Rasional: Sistem AI tidak boleh dipertanggungjawabkan, jadi kita mesti mempertanggungjawabkan individu dan organisasi manusia untuk kemudaratan yang mereka sebabkan (liabiliti).[^120] KBA tidak terkawal adalah ancaman kepada masyarakat dan tamadun dan jika tiada kes keselamatan harus dianggap berbahaya secara tidak normal. Meletakkan beban tanggungjawab kepada pembangun untuk menunjukkan bahawa model berkuasa cukup selamat untuk tidak dianggap "berbahaya secara tidak normal" memberi insentif kepada pembangunan selamat, bersama-sama dengan ketelusan dan penyimpanan rekod untuk menuntut pelabuhan selamat tersebut. Peraturan kemudiannya boleh mencegah kemudaratan di mana pencegahan daripada liabiliti tidak mencukupi. Akhirnya, pembangun AI sudah bertanggungjawab untuk kerosakan yang mereka sebabkan, jadi menjelaskan secara sah liabiliti untuk sistem yang paling berisiko boleh dilakukan serta-merta, tanpa piawaian yang sangat terperinci dibangunkan; ini kemudiannya boleh berkembang dari masa ke masa. Butiran diberikan dalam Lampiran C.

#### 4\. Peraturan keselamatan untuk AI

Sistem kawal selia yang menangani risiko akut berskala besar AI akan memerlukan sekurang-kurangnya:

- Pengenalpastian atau penciptaan set badan kawal selia yang sesuai, mungkin agensi baru;
- Rangka kerja penilaian risiko yang komprehensif;[^121]
- Rangka kerja untuk kes keselamatan afirmatif, sebahagiannya berdasarkan rangka kerja penilaian risiko, untuk dibuat oleh pembangun, dan untuk audit oleh kumpulan dan agensi *bebas*;
- Sistem pelesenan berperingkat, dengan peringkat menjejaki tahap keupayaan.[^122] Lesen akan diberikan berdasarkan kes keselamatan dan audit, untuk pembangunan dan penggunaan sistem. Keperluan akan berkisar daripada notifikasi di hujung rendah, kepada jaminan keselamatan, keselamatan, dan kawalan kuantitatif sebelum pembangunan, di hujung atas. Ini akan menghalang pelepasan sistem sehingga ia dibuktikan selamat, dan melarang pembangunan sistem yang secara intrinsik tidak selamat. Lampiran D memberikan cadangan untuk apa yang boleh diperlukan oleh piawaian keselamatan tersebut.
- Perjanjian untuk membawa langkah sedemikian ke peringkat antarabangsa, termasuk badan antarabangsa untuk menyelaraskan norma dan piawaian, dan berpotensi agensi antarabangsa untuk mengkaji kes keselamatan.

Rasional: Akhirnya, liabiliti bukan mekanisme yang tepat untuk mencegah risiko berskala besar kepada orang ramai daripada teknologi baru. Peraturan yang komprehensif, dengan badan kawal selia yang diberi kuasa, akan diperlukan untuk AI sama seperti setiap industri utama lain yang menimbulkan risiko kepada orang ramai.[^123]

Peraturan ke arah mencegah risiko berleluasa lain tetapi kurang akut mungkin berbeza dalam bentuknya dari bidang kuasa ke bidang kuasa. Perkara penting adalah untuk mengelakkan pembangunan sistem AI yang begitu berisiko sehingga risiko ini tidak dapat diurus.

### Apa kemudiannya?

Dalam dekad akan datang, apabila AI menjadi lebih berleluasa dan teknologi teras maju, dua perkara utama berkemungkinan berlaku. Pertama, peraturan sistem AI berkuasa sedia ada akan menjadi lebih sukar, namun lebih perlu. Berkemungkinan bahawa sekurang-kurangnya beberapa langkah yang menangani risiko keselamatan berskala besar akan memerlukan persetujuan di peringkat antarabangsa, dengan bidang kuasa individu menguatkuasakan peraturan berdasarkan perjanjian antarabangsa.

Kedua, had pengkomputeran latihan dan operasi akan menjadi lebih sukar dikekalkan apabila perkakasan menjadi lebih murah dan kos efisien; ia juga mungkin menjadi kurang relevan (atau perlu lebih ketat) dengan kemajuan dalam algoritma dan seni bina.

Bahawa mengawal AI akan menjadi lebih sukar tidak bermakna kita harus menyerah! Melaksanakan pelan yang digariskan dalam esei ini akan memberikan kita masa yang berharga dan kawalan penting ke atas proses yang akan meletakkan kita dalam kedudukan yang jauh, jauh lebih baik untuk mengelakkan risiko eksistensial AI kepada masyarakat, tamadun, dan spesies kita.

Dalam jangka panjang lagi, akan ada pilihan untuk dibuat tentang apa yang kita benarkan. Kita mungkin memilih untuk masih mencipta beberapa bentuk KBA yang benar-benar terkawal, sejauh mana ini terbukti mungkin. Atau kita mungkin memutuskan bahawa menjalankan dunia lebih baik diserahkan kepada mesin, jika kita boleh meyakinkan diri sendiri bahawa mereka akan melakukan kerja yang lebih baik, dan melayan kita dengan baik. Tetapi ini seharusnya menjadi keputusan yang dibuat dengan pemahaman saintifik yang mendalam tentang AI, dan selepas perbincangan global inklusif yang bermakna, bukan dalam perlumbaan antara mogul teknologi dengan sebahagian besar manusia tidak terlibat sepenuhnya dan tidak sedar.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) Ringkasan tadbir urus A-G-I dan superintelligence melalui liabiliti dan peraturan. Liabiliti adalah tertinggi, dan peraturan terkuat, di persilangan tiga Autonomi, Keamanan, dan Kecerdasan. Pelabuhan selamat daripada liabiliti ketat dan peraturan kuat boleh diperoleh melalui kes keselamatan afirmatif yang menunjukkan bahawa sistem adalah lemah dan/atau sempit dan/atau pasif. Had pada jumlah Pengkomputeran Latihan dan kadar Pengkomputeran Inferens, yang disahkan dan dikuatkuasakan secara sah dan menggunakan langkah keselamatan perkakasan dan kriptografi, menyokong keselamatan dengan mengelakkan KBA penuh dan secara berkesan melarang superintelligence.


[^87]: Kemungkinan besar, penyebaran kesedaran ini akan mengambil sama ada usaha intensif oleh kumpulan pendidikan dan advokasi yang membuat kes ini, atau bencana yang disebabkan AI yang agak ketara. Kita boleh berharap ia akan menjadi yang pertama.

[^88]: Secara paradoks, kita biasa dengan Alam mengehadkan teknologi kita dengan menjadikannya sangat sukar untuk dibangunkan, terutama dari segi saintifik. Tetapi itu bukan lagi kes untuk AI: masalah saintifik utama ternyata lebih mudah daripada yang dijangka. Kita tidak boleh bergantung pada Alam menyelamatkan kita daripada diri kita sendiri di sini – kita perlu melakukannya sendiri.

[^89]: Di manakah, tepatnya, kita berhenti dalam membangunkan sistem baru? Di sini, kita harus mengamalkan prinsip berjaga-jaga. Setelah sistem digunakan, dan terutama setelah tahap keupayaan sistem tersebut tersebar, adalah amat sukar untuk berundur. Dan jika sistem *dibangunkan* (terutama dengan kos dan usaha yang besar), akan ada tekanan besar untuk menggunakan atau menggunakannya, dan godaan untuk ia bocor atau dicuri. Membangunkan sistem dan *kemudian* memutuskan sama ada ia sangat tidak selamat adalah jalan yang berbahaya.

[^90]: Ia juga bijak untuk melarang pembangunan AI yang secara intrinsik berbahaya, seperti sistem yang mereplikasi diri dan berkembang, yang direka untuk melarikan diri dari kandang, yang boleh memperbaiki diri secara autonomi, AI yang sengaja menipu dan berniat jahat, dll.

[^91]: Perhatikan ini tidak semestinya bermakna *dikuatkuasakan* di peringkat antarabangsa oleh semacam badan global: sebaliknya negara berdaulat boleh menguatkuasakan peraturan yang dipersetujui, seperti dalam banyak perjanjian.

[^92]: Seperti yang akan kita lihat di bawah, sifat pengkomputeran AI akan membenarkan sesuatu hibrid; tetapi kerjasama antarabangsa masih diperlukan.

[^93]: Sebagai contoh, mesin yang diperlukan untuk mengukir cip berkaitan AI dibuat oleh hanya satu firma, ASML (walaupun banyak percubaan lain untuk berbuat demikian), sebahagian besar cip berkaitan dikilang oleh satu firma, TSMC (walaupun yang lain cuba bersaing), dan reka bentuk dan pembinaan perkakasan daripada cip tersebut dilakukan oleh hanya beberapa termasuk NVIDIA, AMD, dan Google.

[^94]: Yang paling penting, setiap cip memegang kunci peribadi kriptografi unik dan tidak dapat diakses yang boleh digunakannya untuk "menandatangani" perkara.

[^95]: Secara lalai ini akan menjadi syarikat yang menjual cip, tetapi model lain mungkin dan berpotensi berguna.

[^96]: Pentadbir boleh memastikan lokasi cip dengan mengukur masa pertukaran mesej bertandatangan dengannya: kelajuan cahaya yang terhingga memerlukan cip berada dalam radius tertentu *r* daripada "stesen" jika ia boleh mengembalikan mesej bertandatangan dalam masa kurang daripada *r* / *c*, di mana *c* adalah kelajuan cahaya. Menggunakan beberapa stesen, dan beberapa pemahaman tentang ciri rangkaian, lokasi cip boleh ditentukan. Keindahan kaedah ini ialah sebahagian besar keselamatannya disediakan oleh undang-undang fizik. Kaedah lain boleh menggunakan GPS, penjejakan inersia, dan teknologi serupa.

[^97]: Sebagai alternatif, pasangan cip boleh dibenarkan berkomunikasi antara satu sama lain hanya melalui kebenaran eksplisit pentadbir.

[^98]: Ini penting kerana sekurang-kurangnya pada masa ini, sambungan lebar jalur yang sangat tinggi antara cip diperlukan untuk melatih model AI besar padanya.

[^99]: Ini juga boleh disediakan untuk memerlukan mesej bertandatangan daripada *N* daripada *M* pentadbir yang berbeza, membenarkan beberapa pihak berkongsi tadbir urus.

[^100]: Ini jauh daripada tidak pernah terjadi – sebagai contoh tentera tidak membangunkan tentera supersoldier yang diklon atau diubah suai genetik, walaupun ini mungkin secara teknologi. Tetapi mereka telah *memilih* untuk tidak melakukan ini, dan bukannya dihalang oleh orang lain. Rekod prestasi tidak bagus untuk kuasa dunia utama dihalang daripada membangunkan teknologi yang mereka sangat ingin bangunkan.

[^101]: Dengan beberapa pengecualian yang ketara (khususnya NVIDIA) perkakasan khusus AI adalah bahagian yang agak kecil daripada model perniagaan dan hasil keseluruhan syarikat ini. Selain itu, jurang antara perkakasan yang digunakan dalam AI canggih dan perkakasan "gred pengguna" adalah ketara, jadi kebanyakan pengguna perkakasan komputer akan sebahagian besarnya tidak terjejas.

[^102]: Untuk analisis yang lebih terperinci, lihat laporan terkini daripada [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) dan [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). Ini memberi tumpuan kepada kebolehlaksanaan teknikal, terutama dalam konteks kawalan eksport AS yang berusaha mengekang kapasiti negara lain dalam pengkomputeran mewah; tetapi ini mempunyai pertindihan yang jelas dengan kekangan global yang dibayangkan di sini.

[^103]: Peranti Apple, sebagai contoh, dikunci dari jauh dan selamat apabila dilaporkan hilang atau dicuri, dan boleh diaktifkan semula dari jauh. Ini bergantung pada ciri keselamatan perkakasan yang sama yang dibincangkan di sini.

[^104]: Lihat contohnya penawaran [kapasiti atas permintaan](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) IBM, [Intel atas permintaan](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html) Intel., dan [pengkomputeran awan peribadi](https://security.apple.com/blog/private-cloud-compute/) Apple.

[^105]: [Kajian ini](https://epochai.org/trends#hardware-trends-section) menunjukkan bahawa dari segi sejarah prestasi yang sama telah dicapai menggunakan kira-kira 30% kurang dolar setahun. Jika trend ini berterusan, mungkin terdapat pertindihan ketara antara penggunaan cip AI dan "pengguna", dan secara amnya jumlah perkakasan yang diperlukan untuk sistem AI berkuasa tinggi boleh menjadi tidak selesa kecil.

[^106]: Mengikut [kajian yang sama](https://epochai.org/trends#hardware-trends-section), prestasi yang diberikan pada pengecaman imej telah memerlukan 2.5x kurang pengkomputeran setiap tahun. Jika ini juga berlaku untuk sistem AI yang paling berkebolehan juga, had pengkomputeran tidak akan berguna untuk tempoh yang lama.

[^107]: Khususnya, di peringkat negara ini kelihatan seperti nasionalisasi pengkomputeran, kerana kerajaan akan mempunyai banyak kawalan ke atas bagaimana kuasa pengkomputeran digunakan. Walau bagaimanapun, bagi mereka yang bimbang tentang penglibatan kerajaan, ini kelihatan jauh lebih selamat dan lebih disukai daripada perisian AI yang paling berkuasa *sendiri* dinasionalisasi melalui beberapa penggabungan antara syarikat AI utama dan kerajaan negara, seperti yang mula diperjuangkan oleh sesetengah pihak.

[^108]: Langkah kawal selia utama di Eropah telah diambil dengan kelulusan 2024 [Akta AI EU.](https://artificialintelligenceact.eu/) Ia mengklasifikasikan AI mengikut risiko: melarang sistem yang tidak boleh diterima, mengawal selia yang berisiko tinggi, dan mengenakan peraturan ketelusan, atau tiada langkah sama sekali, ke atas sistem berisiko rendah. Ia akan mengurangkan dengan ketara beberapa risiko AI, dan meningkatkan ketelusan AI walaupun untuk firma AS, tetapi mempunyai dua kecacatan utama. Pertama, jangkauan terhad: walaupun ia terpakai kepada mana-mana syarikat yang menyediakan AI di EU, penguatkuasaan ke atas firma berasaskan AS adalah lemah, dan AI ketenteraan dikecualikan. Kedua, walaupun ia meliputi GPAI, ia gagal mengiktiraf KBA atau superintelligence sebagai risiko yang tidak boleh diterima atau menghalang pembangunan mereka—hanya penggunaan EU mereka. Akibatnya, ia tidak banyak membantu untuk mengekang risiko KBA atau superintelligence.

[^109]: Syarikat sering mewakili bahawa mereka menyokong peraturan yang munasabah. Tetapi entah bagaimana mereka hampir selalu nampaknya menentang mana-mana peraturan *tertentu*; saksikan perjuangan mengenai SB1047 yang agak rendah sentuhan, yang [kebanyakan syarikat AI menentang secara terbuka atau peribadi.](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/)

[^110]: Ia adalah kira-kira 3 1/2 tahun dari masa akta AI EU dicadangkan sehingga ia berkuat kuasa.

[^111]: Kadangkala dinyatakan bahawa ia "terlalu awal" untuk mula mengawal selia AI. Memandangkan nota terakhir, itu nampaknya tidak mungkin. Satu lagi kebimbangan yang dinyatakan ialah peraturan akan "membahayakan inovasi." Tetapi peraturan yang baik hanya mengubah arah, bukan jumlah, inovasi.

[^112]: Duluan yang menarik adalah dalam pengangkutan bahan berbahaya, yang mungkin terlepas dan menyebabkan kerosakan. Di sini, [peraturan](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) dan [undang-undang kes](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) telah menetapkan liabiliti ketat untuk bahan yang sangat berbahaya seperti bahan letupan, petrol, racun, agen berjangkit, dan sisa radioaktif. Contoh lain termasuk [amaran pada farmaseutikal](https://www.medicalnewstoday.com/articles/boxed-warnings), [kelas peranti perubatan,](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification) dll.

[^113]: Satu lagi cadangan komprehensif dengan matlamat serupa yang dikemukakan dalam ["A Narrow Path"](https://www.narrowpath.co/) menyokong pendekatan berasaskan larangan yang lebih terpusat yang menyalurkan semua pembangunan AI terdepan melalui entiti antarabangsa tunggal, diawasi oleh institusi antarabangsa yang kuat, dengan larangan kategorikal yang jelas dan bukannya sekatan bergraduat. Saya juga akan menyokong pelan tersebut; walau bagaimanapun ia akan mengambil lebih banyak kemahuan politik dan koordinasi daripada yang dicadangkan di sini.

[^114]: Beberapa garis panduan untuk piawaian sedemikian telah [diterbitkan](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/) oleh Forum Model Frontier. Berbanding cadangan di sini, yang tersalah di sisi kurang ketepatan dan kurang pengkomputeran termasuk dalam kiraan.

[^115]: Perintah eksekutif AI AS 2023 (kini dibatalkan) memerlukan pelaporan serupa tetapi kurang berbutir halus. Ini harus diperkukuh oleh perintah pengganti.

[^116]: Secara kasarnya, untuk cip H100 biasa sekarang ini sepadan dengan kelompok kira-kira 1000 melakukan inferens; ia adalah kira-kira 100 (kira-kira USD $5M bernilai) cip NVIDIA B200 terbaharu terbaik melakukan inferens. Dalam kedua-dua kes nombor latihan sepadan dengan kelompok tersebut mengira untuk beberapa bulan.

[^117]: Jumlah ini lebih besar daripada mana-mana sistem AI yang dilatih pada masa ini; nombor yang lebih besar atau lebih kecil mungkin wajar apabila kita lebih memahami bagaimana keupayaan AI berskala dengan pengkomputeran.

[^118]: Ini terpakai kepada mereka yang mencipta dan menyediakan/menghos model, bukan pengguna akhir.

[^119]: Secara kasarnya, liabiliti "ketat" bermakna pembangun dipertanggungjawabkan untuk kemudaratan yang dilakukan oleh produk *secara lalai* dan adalah piawaian yang digunakan untuk produk "berbahaya secara tidak normal", dan (agak lucu tetapi sesuai) haiwan liar. Liabiliti "bersama dan beberapa" bermakna liabiliti diberikan kepada semua pihak yang bertanggungjawab untuk produk, dan pihak tersebut perlu menyelesaikan antara mereka sendiri siapa yang memikul tanggungjawab apa. Ini penting untuk sistem seperti AI dengan rantaian nilai yang panjang dan kompleks.

[^120]: Liabiliti berasaskan kesalahan pihak tunggal standard tidak mencukupi: kesalahan akan sukar dijejaki dan diberikan kerana sistem AI adalah kompleks, operasinya tidak difahami, dan banyak pihak mungkin terlibat dalam penciptaan sistem atau output berbahaya. Selain itu, tuntutan mahkamah akan mengambil masa bertahun-tahun untuk diputuskan dan mungkin hanya menghasilkan denda yang tidak penting kepada syarikat ini, jadi liabiliti peribadi untuk eksekutif adalah penting juga.

[^121]: Tidak seharusnya ada pengecualian daripada kriteria keselamatan untuk model berat terbuka. Selain itu, dalam menilai risiko harus diandaikan bahawa pagar yang boleh dikeluarkan akan dikeluarkan daripada model yang tersedia secara meluas, dan bahawa model tertutup akan tersebar melainkan terdapat jaminan yang sangat tinggi ia akan kekal selamat.

[^122]: Skim yang dicadangkan di sini mempunyai penelitian kawal selia yang dicetuskan pada keupayaan umum; walau bagaimanapun masuk akal untuk beberapa kes penggunaan yang berisiko terutama untuk mencetuskan penelitian lebih – sebagai contoh sistem AI virologi pakar, walaupun sempit dan pasif, mungkin sepatutnya masuk ke peringkat yang lebih tinggi. Bekas perintah eksekutif AS mempunyai beberapa struktur ini untuk keupayaan biologi.

[^123]: Dua contoh yang jelas ialah penerbangan dan ubat-ubatan, dikawal oleh FAA dan FDA, dan agensi serupa di negara lain. Agensi ini tidak sempurna, tetapi telah amat penting untuk fungsi dan kejayaan industri tersebut.

## Bab 9 - Merekayasa masa depan — apa yang sepatutnya kita lakukan sebaliknya

AI boleh membawa kebaikan yang luar biasa kepada dunia. Untuk mendapat semua manfaat tanpa risiko, kita mesti memastikan AI kekal sebagai alat manusia.

Jika kita berjaya memilih untuk tidak menggantikan manusia dengan mesin – sekurang-kurangnya buat masa ini! – apa yang boleh kita lakukan sebaliknya? Adakah kita melepaskan potensi besar AI sebagai teknologi? Pada satu tahap jawapannya adalah *tidak*: tutup Pintu Gerbang kepada KBA yang tidak terkawal dan superintelligence, tetapi *bangunlah* pelbagai bentuk AI yang lain, serta struktur tadbir urus dan institusi yang kita perlukan untuk mengurusnya.

Tetapi masih banyak yang perlu dikatakan; menjayakan perkara ini akan menjadi tugas utama kemanusiaan. Bahagian ini meneroka beberapa tema utama:

- Bagaimana kita boleh mencirikan AI "Alat" dan bentuk yang boleh diambilnya.
- Bahawa kita boleh mendapat (hampir) segala yang dikehendaki manusia tanpa KBA, dengan AI Alat.
- Bahawa sistem AI Alat adalah (mungkin, pada prinsipnya) boleh diurus.
- Bahawa berpaling daripada KBA tidak bermakna berkompromi soal keselamatan negara – malah sebaliknya.
- Bahawa penumpuan kuasa adalah kebimbangan sebenar. Bolehkah kita mengurangkannya tanpa menjejaskan keselamatan dan keamanan?
- Bahawa kita akan mahu – dan perlukan – struktur tadbir urus dan sosial baharu, dan AI sebenarnya boleh membantu.

### AI di dalam Pintu Gerbang: AI Alat

Gambarajah persilangan tiga memberikan cara yang baik untuk menggambarkan apa yang boleh kita namakan "AI Alat": AI yang merupakan alat terkawal untuk kegunaan manusia, bukannya saingan atau pengganti yang tidak terkawal. Sistem AI yang paling kurang bermasalah ialah yang autonomi tetapi tidak am atau berkebolehan super (seperti bot bida lelongan), atau am tetapi tidak autonomi atau berkebolehan (seperti model bahasa kecil), atau berkebolehan tetapi sempit dan sangat terkawal (seperti AlphaGo).[^124] Yang mempunyai dua ciri bersilang mempunyai aplikasi yang lebih luas tetapi risiko yang lebih tinggi dan memerlukan usaha besar untuk diurus. (Hanya kerana sistem AI lebih kepada alat tidak bermakna ia sememangnya selamat, semata-mata ia tidak sememangnya *tidak selamat* – pertimbangkan gergaji rantai, berbanding harimau peliharaan.) Pintu Gerbang mesti kekal tertutup kepada KBA (penuh) dan superintelligence di persilangan tiga, dan perhatian yang sangat besar mesti diberikan kepada sistem AI yang menghampiri ambang tersebut.

Tetapi ini meninggalkan banyak AI yang berkuasa! Kita boleh mendapat utiliti yang besar daripada "orakel" pasif yang pintar dan am serta sistem sempit, sistem am di tahap manusia tetapi bukan superhuman, dan sebagainya. Banyak syarikat teknologi dan pembangun sedang aktif membina alat-alat seperti ini dan harus meneruskannya; seperti kebanyakan orang, mereka secara tersirat *menganggap* Pintu Gerbang kepada KBA dan superintelligence akan ditutup.[^125]

Selain itu, sistem AI boleh digabungkan dengan berkesan menjadi sistem komposit yang mengekalkan pengawasan manusia sambil meningkatkan kebolehan. Daripada bergantung kepada kotak hitam yang tidak dapat difahami, kita boleh membina sistem di mana pelbagai komponen – termasuk AI dan perisian tradisional – bekerja bersama dengan cara yang boleh dipantau dan difahami oleh manusia.[^126] Walaupun sesetengah komponen mungkin kotak hitam, tidak ada yang akan hampir kepada KBA – hanya sistem komposit secara keseluruhan yang akan sangat am dan sangat berkebolehan, dan dengan cara yang benar-benar terkawal.[^127]

#### Kawalan manusia yang bermakna dan terjamin

Apakah maksud "benar-benar terkawal"? Idea utama rangka kerja "Alat" adalah membenarkan sistem – walaupun agak am dan berkuasa – yang dijamin berada di bawah kawalan manusia yang bermakna. Apakah maksudnya? Ia melibatkan dua aspek. Pertama ialah pertimbangan reka bentuk: manusia harus terlibat secara mendalam dan terpusat dalam apa yang dilakukan oleh sistem, *tanpa* mewakilkan keputusan penting kepada AI. Ini adalah ciri kebanyakan sistem AI semasa. Kedua, pada tahap sistem AI bersifat autonomi, mereka mesti mempunyai jaminan yang mengehadkan skop tindakan mereka. Jaminan harus berupa *nombor* yang mencirikan kebarangkalian sesuatu berlaku, dan alasan untuk mempercayai nombor tersebut. Inilah yang kita tuntut dalam bidang kritikal keselamatan lain, di mana nombor seperti "masa purata antara kegagalan" dan jangkaan bilangan kemalangan dikira, disokong, dan diterbitkan dalam kes keselamatan.[^128] Nombor yang ideal untuk kegagalan ialah sifar, sudah tentu. Dan berita baik ialah kita mungkin boleh menghampirinya, walaupun menggunakan seni bina AI yang agak berbeza, menggunakan idea sifat *terbukti secara formal* program (termasuk AI). Idea yang diteroka secara terperinci oleh Omohundro, Tegmark, Bengio, Dalrymple, dan lain-lain (lihat [di sini](https://arxiv.org/abs/2309.01933) dan [di sini](https://arxiv.org/abs/2405.06624)) adalah untuk membina program dengan sifat tertentu (contohnya: bahawa manusia boleh menutupnya) dan secara formal *membuktikan* bahawa sifat tersebut berlaku. Ini boleh dilakukan sekarang untuk program yang agak pendek dan sifat mudah, tetapi kuasa AI yang akan datang untuk perisian pembuktian boleh membolehkannya untuk program yang lebih kompleks (cth. pembalut) dan bahkan AI sendiri. Ini adalah program yang sangat bercita-cita tinggi, tetapi apabila tekanan meningkat pada Pintu Gerbang, kita akan memerlukan beberapa bahan kuat untuk mengukuhkannya. Bukti matematik mungkin salah satu daripada yang sedikit yang cukup kuat.

#### Ke mana industri AI

Dengan kemajuan AI yang diarahkan semula, AI Alat masih akan menjadi industri yang besar. Dari segi perkakasan, walaupun dengan had pengkomputeran untuk mencegah superintelligence, latihan dan inferens dalam model yang lebih kecil masih akan memerlukan sejumlah besar komponen khusus. Dari segi perisian, meredakan ledakan dalam saiz model AI dan pengkomputeran sepatutnya menyebabkan syarikat mengarahkan semula sumber untuk menjadikan sistem yang lebih kecil lebih baik, lebih pelbagai, dan lebih khusus, bukannya hanya membesarkannya.[^129] Akan ada banyak ruang – lebih mungkin – untuk semua syarikat permulaan Silicon Valley yang membuat wang tersebut.[^130]

### AI Alat boleh menghasilkan (hampir) segala yang dikehendaki manusia, tanpa KBA

Kecerdasan, sama ada biologi atau mesin, boleh dianggap secara luas sebagai keupayaan untuk merancang dan melaksanakan aktiviti yang membawa masa depan yang lebih selaras dengan set matlamat. Oleh itu, kecerdasan sangat bermanfaat apabila digunakan untuk mengejar matlamat yang dipilih dengan bijak. Kecerdasan buatan menarik pelaburan masa dan usaha yang besar sebahagian besarnya kerana manfaat yang dijanjikannya. Jadi kita harus bertanya: sejauh mana kita masih akan menuai manfaat AI jika kita membendung pelariannya ke superintelligence? Jawapannya: kita mungkin kehilangan sangat sedikit secara mengejutkan.

Pertimbangkan dahulu bahawa sistem AI semasa sudah sangat berkuasa, dan kita benar-benar baru menggores permukaan apa yang boleh dilakukan dengannya.[^131] Mereka cukup mampu "menjalankan persembahan" dalam "memahami" soalan atau tugasan yang dikemukakan kepada mereka, dan apa yang diperlukan untuk menjawab soalan ini atau melakukan tugas itu.

Seterusnya, kebanyakan keseronokan tentang sistem AI moden adalah disebabkan oleh keaslian mereka; tetapi beberapa sistem AI yang paling berkebolehan – seperti yang menghasilkan atau mengenali pertuturan atau imej, melakukan ramalan dan pemodelan saintifik, bermain permainan, dll. – adalah jauh lebih sempit dan baik "dalam Pintu Gerbang" dari segi pengkomputeran.[^132] Sistem ini adalah super-manusia pada tugas tertentu yang mereka lakukan. Mereka mungkin mempunyai kelemahan kes tepi[^133] (atau [boleh dieksploitasi](https://arxiv.org/abs/2211.00241)) disebabkan oleh kesempitan mereka; walau bagaimanapun, sempit *sepenuhnya* atau am *sepenuhnya* bukanlah satu-satunya pilihan yang tersedia: terdapat banyak seni bina di antaranya.[^134]

Alat AI ini boleh mempercepatkan kemajuan dalam teknologi positif lain, tanpa KBA. Untuk melakukan fizik nuklear yang lebih baik, kita tidak memerlukan AI untuk menjadi ahli fizik nuklear – kita ada mereka! Jika kita mahu mempercepatkan perubatan, berikan ahli biologi, penyelidik perubatan, dan ahli kimia alat yang berkuasa. Mereka mahukannya dan akan menggunakannya untuk keuntungan yang besar. Kita tidak memerlukan ladang pelayan yang penuh dengan sejuta genius digital; kita mempunyai berjuta-juta manusia yang kegeniusan mereka boleh dibantu AI untuk dikeluarkan. Ya, akan mengambil masa yang lebih lama untuk mendapat keabadian dan penawar kepada semua penyakit. Ini adalah kos sebenar. Tetapi walaupun inovasi kesihatan yang paling menjanjikan akan kurang berguna jika ketidakstabilan yang didorong AI membawa kepada konflik global atau keruntuhan masyarakat. Kita berhutang kepada diri kita untuk memberikan manusia yang diperkasa AI peluang untuk menyelesaikan masalah dahulu.

Dan katakan ada, sebenarnya, beberapa keuntungan besar kepada KBA yang tidak dapat diperoleh oleh manusia menggunakan alat dalam-Pintu Gerbang. Adakah kita kehilangan mereka dengan *tidak pernah* membina KBA dan superintelligence? Dalam menimbang risiko dan ganjaran di sini, terdapat manfaat asimetri yang besar dalam menunggu berbanding tergesa-gesa: kita boleh menunggu sehingga ia boleh dilakukan dengan cara yang dijamin selamat dan bermanfaat, dan hampir semua orang masih akan dapat menuai ganjaran; jika kita tergesa-gesa, ia boleh menjadi – dalam kata-kata Ketua Pegawai Eksekutif OpenAI Sam Altman – [lampu padam untuk *kita semua*.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

Tetapi jika alat bukan-KBA berpotensi begitu berkuasa, bolehkah kita mengurusnya? Jawapannya adalah...mungkin.

### Sistem AI Alat adalah (mungkin, pada prinsipnya) boleh diurus

Tetapi ia tidak akan mudah. Sistem AI termaju semasa boleh sangat memperkasakan orang dan institusi dalam mencapai matlamat mereka. Ini, secara amnya, adalah perkara yang baik! Walau bagaimanapun, terdapat dinamik semula jadi mempunyai sistem sebegitu pada pelupusan kita – secara tiba-tiba dan tanpa banyak masa untuk masyarakat menyesuaikan diri – yang menawarkan risiko serius yang perlu diurus. Adalah wajar untuk membincangkan beberapa kelas utama risiko tersebut, dan bagaimana ia boleh dikurangkan, dengan mengandaikan penutupan Pintu Gerbang.

Satu kelas risiko adalah AI Alat berkuasa tinggi membenarkan akses kepada pengetahuan atau kebolehan yang sebelum ini terikat kepada seseorang atau organisasi, menjadikan gabungan kebolehan tinggi ditambah kesetiaan tinggi tersedia kepada pelbagai pelakon yang sangat luas. Hari ini, dengan wang yang cukup seseorang yang berniat jahat boleh mengupah pasukan ahli kimia untuk mereka bentuk dan hasilkan senjata kimia baharu – tetapi tidaklah begitu mudah untuk mempunyai wang tersebut atau untuk mencari/mengumpulkan pasukan dan meyakinkan mereka untuk melakukan sesuatu yang jelas haram, tidak beretika, dan berbahaya. Untuk mencegah sistem AI daripada memainkan peranan seperti itu, penambahbaikan pada kaedah semasa mungkin mencukupi,[^135] selagi semua sistem tersebut dan akses kepadanya diurus secara bertanggungjawab. Sebaliknya, jika sistem berkuasa dikeluarkan untuk kegunaan dan pengubahsuaian am, sebarang langkah keselamatan terbina kemungkinan boleh dikeluarkan. Jadi untuk mengelakkan risiko dalam kelas ini, sekatan kuat mengenai apa yang boleh dikeluarkan secara umum – serupa dengan sekatan pada butiran teknologi nuklear, letupan, dan berbahaya lain – akan diperlukan.[^136]

Kelas risiko kedua berpunca daripada peningkatan skala mesin yang bertindak seperti atau menyamar sebagai orang. Pada tahap kemudaratan kepada individu, risiko ini termasuk penipuan, spam, dan pancingan data yang jauh lebih berkesan, dan penyebaran deepfake tanpa persetujuan.[^137] Pada tahap kolektif, ia termasuk gangguan proses sosial teras seperti perbincangan dan perdebatan awam, sistem pengumpulan, pemprosesan, dan penyebaran maklumat dan pengetahuan masyarakat kita, dan sistem pilihan politik kita. Mengurangkan risiko ini kemungkinan melibatkan (a) undang-undang yang menyekat penyamaran orang oleh sistem AI, dan mempertanggungjawabkan pembangun AI yang mencipta sistem yang menghasilkan penyamaran tersebut, (b) sistem tera air dan asal usul yang mengenal pasti dan mengklasifikasikan (secara bertanggungjawab) kandungan AI yang dijana, dan (c) sistem epistemik sosio-teknikal baharu yang boleh mewujudkan rantai yang dipercayai daripada data (cth. kamera dan rakaman) melalui fakta, pemahaman, dan model dunia yang baik.[^138] Semua ini adalah mungkin, dan AI boleh membantu dengan beberapa bahagiannya.

Risiko am ketiga ialah pada tahap beberapa tugas diautomasikan, manusia yang kini melakukan tugas tersebut boleh mempunyai nilai kewangan yang kurang sebagai buruh. Dari segi sejarah, mengautomasikan tugas telah menjadikan perkara yang dimungkinkan oleh tugas tersebut lebih murah dan lebih melimpah, sambil mengisih orang yang sebelum ini melakukan tugas tersebut kepada mereka yang masih terlibat dalam versi automatik (umumnya pada kemahiran/gaji yang lebih tinggi), dan mereka yang nilai buruh mereka kurang bernilai atau sedikit. Secara bersih, sukar untuk meramalkan dalam sektor mana lebih berbanding kurang buruh manusia akan diperlukan dalam sektor yang lebih besar tetapi lebih cekap yang terhasil. Secara selari, dinamik automasi cenderung meningkatkan ketaksamaan dan produktiviti am, mengurangkan kos barangan dan perkhidmatan tertentu (melalui peningkatan kecekapan), dan meningkatkan kos yang lain (melalui [penyakit kos](https://en.wikipedia.org/wiki/Baumol_effect)). Bagi mereka yang berada di sisi tidak disukai daripada peningkatan ketaksamaan, sangat tidak jelas sama ada penurunan kos dalam barangan dan perkhidmatan tertentu tersebut mengatasi peningkatan dalam yang lain, dan membawa kepada kesejahteraan yang lebih besar secara keseluruhan. Jadi bagaimanakah ini akan berlaku untuk AI? Kerana kemudahan relatif di mana buruh intelektual manusia boleh digantikan oleh AI am, kita boleh menjangkakan versi pantas ini dengan AI tujuan am yang kompetitif manusia.[^139] Jika kita tutup Pintu Gerbang kepada KBA, lebih sedikit pekerjaan akan digantikan secara borong oleh agen AI; tetapi anjakan buruh yang besar masih berkemungkinan dalam tempoh beberapa tahun.[^140] Untuk mengelakkan penderitaan ekonomi yang meluas, kemungkinan perlu untuk melaksanakan beberapa bentuk aset asas sejagat atau pendapatan, dan juga merekayasa anjakan budaya ke arah menghargai dan memberi ganjaran buruh berpusatkan manusia yang lebih sukar untuk diautomasikan (bukannya melihat harga buruh turun akibat peningkatan dalam buruh yang tersedia yang ditolak keluar daripada bahagian lain ekonomi.) Konstruk lain, seperti ["maruah data"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society) (di mana pengeluar manusia data latihan secara automatik diberikan royalti untuk nilai yang dicipta oleh data tersebut dalam AI) mungkin membantu. Automasi oleh AI juga mempunyai kesan buruk kedua yang berpotensi, iaitu automasi yang *tidak sesuai*. Bersama dengan aplikasi di mana sistem AI hanya melakukan kerja yang lebih teruk, ini akan termasuk mereka di mana sistem AI berkemungkinan melanggar ajaran moral, etika, atau undang-undang – contohnya dalam keputusan hidup dan mati, dan dalam hal kehakiman. Ini mesti dirawat dengan menggunakan dan melanjutkan rangka kerja undang-undang semasa kita.

Akhirnya, ancaman ketara AI dalam-pintu gerbang ialah penggunaannya dalam pemujukan peribadi, menawan perhatian, dan manipulasi. Kita telah melihat dalam media sosial dan platform dalam talian lain pertumbuhan ekonomi perhatian yang sangat berakar (di mana perkhidmatan dalam talian berperang sengit untuk perhatian pengguna) dan sistem ["kapitalisme pengawasan"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism) (di mana maklumat pengguna dan profil ditambahkan kepada komodifikasi perhatian.) Hampir pasti bahawa lebih banyak AI akan digunakan untuk kedua-duanya. AI sudah banyak digunakan dalam algoritma suapan yang ketagihan, tetapi ini akan berkembang menjadi kandungan yang dijana AI yang ketagihan, disesuaikan untuk dimakan secara kompulsif oleh seseorang. Dan input, respons, dan data orang tersebut, akan dimasukkan ke dalam mesin perhatian/pengiklanan untuk meneruskan kitaran ganas. Selain itu, apabila pembantu AI yang disediakan oleh syarikat teknologi menjadi antara muka untuk lebih banyak kehidupan dalam talian, mereka berkemungkinan akan menggantikan enjin carian dan suapan sebagai mekanisme di mana pemujukan dan pengewangan pelanggan berlaku. Kegagalan masyarakat kita untuk mengawal dinamik ini setakat ini tidak menunjukkan petanda baik. Sebahagian daripada dinamik ini mungkin dikurangkan melalui peraturan mengenai privasi, hak data, dan manipulasi. Menangani punca masalah mungkin memerlukan perspektif berbeza, seperti pembantu AI yang setia (dibincangkan di bawah.)

Kesimpulan perbincangan ini ialah harapan: sistem berasaskan alat dalam-Pintu Gerbang – sekurang-kurangnya selagi mereka kekal setanding dalam kuasa dan kebolehan dengan sistem termaju hari ini – mungkin boleh diurus jika ada kehendak dan koordinasi untuk berbuat demikian. Institusi manusia yang baik, diperkasakan oleh alat AI,[^141] boleh melakukannya. Kita juga boleh gagal dalam melakukannya. Tetapi sukar untuk melihat bagaimana membenarkan sistem yang lebih berkuasa akan membantu – selain dengan meletakkannya bertanggungjawab dan berharap untuk yang terbaik.

### Keselamatan negara

Perlumbaan untuk ketuanan AI – didorong oleh keselamatan negara atau motivasi lain – mendorong kita ke arah sistem AI berkuasa yang tidak terkawal yang akan cenderung untuk menyerap, bukannya menganugerahkan, kuasa. Perlumbaan KBA antara AS dan China adalah perlumbaan untuk menentukan negara mana yang mendapat superintelligence dahulu.

Jadi apakah yang sepatutnya dilakukan oleh mereka yang bertanggungjawab untuk keselamatan negara sebaliknya? Kerajaan mempunyai pengalaman yang kuat dalam membina sistem yang terkawal dan selamat, dan mereka sepatutnya menggandakan usaha untuk berbuat demikian dalam AI, menyokong jenis projek infrastruktur yang berjaya terbaik apabila dilakukan pada skala dan dengan kelulusan kerajaan.

Daripada "projek Manhattan" yang melulu ke arah KBA,[^142] kerajaan AS boleh melancarkan projek Apollo untuk sistem terkawal, selamat, dan boleh dipercayai. Ini boleh termasuk contohnya:

- Program utama untuk (a) membangunkan mekanisme keselamatan perkakasan dalam-cip dan (b) infrastruktur, untuk menguruskan sisi pengkomputeran AI yang berkuasa. Ini boleh dibina daripada [akta CHIPS](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) AS dan [rejim kawalan eksport](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion).
- Inisiatif berskala besar untuk membangunkan teknik pengesahan formal supaya ciri tertentu sistem AI (seperti suis tutup) boleh *dibuktikan* hadir atau tidak hadir. Ini boleh memanfaatkan AI sendiri untuk membangunkan bukti sifat.
- Usaha berskala negara untuk mencipta perisian yang boleh disahkan selamat, dikuasakan oleh alat AI yang boleh mengekod semula perisian sedia ada ke dalam rangka kerja yang boleh disahkan selamat.
- Projek pelaburan kebangsaan dalam kemajuan saintifik menggunakan AI,[^143] berjalan sebagai perkongsian antara DOE, NSF, dan NIH.

Secara amnya, terdapat permukaan serangan yang besar pada masyarakat kita yang menjadikan kita terdedah kepada risiko daripada AI dan penyalahgunaannya. Melindungi daripada beberapa risiko ini akan memerlukan pelaburan dan penyeragaman bersaiz kerajaan. Ini akan memberikan lebih banyak keselamatan daripada mencurahkan petrol pada api perlumbaan ke arah KBA. Dan jika AI akan dibina ke dalam persenjataan dan sistem kawalan-dan-komando, adalah penting bahawa AI itu boleh dipercayai dan selamat, yang AI semasa tidak.

### Penumpuan kuasa dan pengurangan risiko

Esei ini telah menumpukan pada idea kawalan manusia terhadap AI dan potensi kegagalannya. Tetapi satu lagi lensa yang sah untuk melihat situasi AI adalah melalui *penumpuan kuasa.* Pembangunan AI yang sangat berkuasa mengancam untuk menumpukan kuasa sama ada ke dalam tangan korporat yang sangat sedikit dan sangat besar yang telah membangun dan akan mengawalnya, atau ke dalam kerajaan yang menggunakan AI sebagai cara baharu untuk mengekalkan kuasa dan kawalan mereka sendiri, atau ke dalam sistem AI sendiri. Atau beberapa campuran yang tidak suci daripada yang di atas. Dalam mana-mana kes ini kebanyakan manusia kehilangan kuasa, kawalan, dan agensi. Bagaimanakah kita boleh memerangi ini?

Langkah pertama dan paling penting, sudah tentu, ialah penutupan Pintu Gerbang kepada KBA dan superintelligence yang lebih pintar daripada manusia. Ini secara eksplisit boleh menggantikan manusia dan kumpulan manusia secara langsung. Jika ia berada di bawah kawalan korporat atau kerajaan, ia akan menumpukan kuasa dalam korporat atau kerajaan tersebut; jika ia "bebas" ia akan menumpukan kuasa kepada diri mereka sendiri. Jadi mari kita andaikan Pintu Gerbang ditutup. Kemudian apa?

Satu penyelesaian yang dicadangkan untuk penumpuan kuasa ialah AI "sumber terbuka", di mana berat model tersedia secara percuma atau meluas. Tetapi seperti yang dinyatakan sebelum ini, sebaik sahaja model terbuka, kebanyakan langkah keselamatan atau pagar boleh (dan umumnya) dilucutkan. Jadi terdapat ketegangan akut antara satu pihak desentralisasi, dan di pihak lain keselamatan, keamanan, dan kawalan manusia sistem AI. Terdapat juga alasan untuk ragu-ragu bahawa model terbuka akan dengan sendirinya bermakna memerangi penumpuan kuasa dalam AI lebih daripada yang mereka lakukan dalam sistem pengendalian (masih dikuasai oleh Microsoft, Apple, dan Google walaupun terdapat alternatif terbuka).[^144]

Namun mungkin terdapat cara untuk menyelesaikan lingkaran ini – untuk memusatkan dan mengurangkan risiko sambil medesentralisasikan kebolehan dan ganjaran ekonomi. Ini memerlukan pemikiran semula kedua-dua cara AI dibangunkan dan cara faedahnya diagihkan.

Model baharu pembangunan dan pemilikan AI awam akan membantu. Ini boleh mengambil beberapa bentuk: AI yang dibangunkan kerajaan (tertakluk kepada pengawasan demokratik),[^145] organisasi pembangunan AI bukan untung (seperti Mozilla untuk pelayar), atau struktur yang membolehkan pemilikan dan tadbir urus yang sangat meluas. Yang utama ialah institusi ini akan diberikan piagam eksplisit untuk melayani kepentingan awam sambil beroperasi di bawah kekangan keselamatan yang kuat.[^146] Rejim peraturan dan piawaian/pensijilan yang digubal dengan baik juga akan menjadi penting, supaya produk AI yang ditawarkan oleh pasaran yang meriah kekal benar-benar berguna bukannya eksploitatif terhadap pengguna mereka.

Dari segi penumpuan kuasa ekonomi, kita boleh menggunakan penjejakan asal usul dan "maruah data" untuk memastikan manfaat ekonomi mengalir lebih luas. Khususnya, kebanyakan kuasa AI sekarang (dan pada masa depan jika kita tutup Pintu Gerbang) berpunca daripada data yang dijana manusia, sama ada data latihan langsung atau maklum balas manusia. Jika syarikat AI dikehendaki memberi pampasan kepada penyedia data secara adil,[^147] ini sekurang-kurangnya boleh membantu mengagihkan ganjaran ekonomi dengan lebih luas. Selain ini, model lain boleh menjadi pemilikan awam pecahan ketara syarikat AI besar. Contohnya, kerajaan yang mampu mengenakan cukai kepada syarikat AI boleh melabur sebahagian daripada terimaan ke dalam dana kekayaan berdaulat yang memegang saham dalam syarikat, dan membayar dividen kepada rakyat.[^148]

Yang penting dalam mekanisme ini ialah menggunakan kuasa AI sendiri untuk membantu mengagihkan kuasa dengan lebih baik, bukannya semata-mata melawan penumpuan kuasa yang didorong AI menggunakan cara bukan-AI. Satu pendekatan yang berkuasa ialah melalui pembantu AI yang direka bentuk dengan baik yang beroperasi dengan kewajipan fidusiari tulen kepada pengguna mereka – meletakkan kepentingan pengguna dahulu, terutamanya daripada penyedia korporat.[^149] Pembantu ini mestilah benar-benar boleh dipercayai, cekap dari segi teknikal namun dihadkan dengan sewajarnya berdasarkan kes penggunaan dan tahap risiko, dan tersedia secara meluas kepada semua melalui saluran awam, bukan untung, atau untung yang disahkan. Sama seperti kita tidak akan menerima pembantu manusia yang diam-diam bekerja menentang kepentingan kita untuk pihak lain, kita tidak sepatutnya menerima pembantu AI yang mengawas, memanipulasi, atau mengekstrak nilai daripada pengguna mereka untuk faedah korporat.

Transformasi sebegitu akan mengubah secara asasnya dinamik semasa di mana individu dibiarkan berunding bersendirian dengan mesin korporat dan birokrasi yang luas (dikuasakan AI) yang mengutamakan pengekstrakan nilai berbanding kebajikan manusia. Walaupun terdapat banyak pendekatan yang mungkin untuk mengagihkan semula kuasa yang didorong AI dengan lebih luas, tidak ada yang akan muncul secara lalai: ia mesti direkayasa dan ditadbir secara sengaja dengan mekanisme seperti keperluan fidusiari, penyediaan awam, dan akses berperingkat berdasarkan risiko.

Pendekatan untuk mengurangkan penumpuan kuasa boleh menghadapi halangan besar daripada kuasa penyandang.[^150] Tetapi terdapat laluan ke arah pembangunan AI yang tidak memerlukan memilih antara keselamatan dan kuasa tertumpu. Dengan membina institusi yang betul sekarang, kita boleh memastikan bahawa manfaat AI dikongsi secara meluas sambil risikonya diurus dengan teliti.

### Struktur tadbir urus dan sosial baharu

Struktur tadbir urus semasa kita sedang bergelut: ia lambat bertindak balas, sering ditawan oleh kepentingan khusus, dan [semakin tidak dipercayai oleh orang ramai.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) Namun ini bukan alasan untuk meninggalkannya – malah sebaliknya. Sesetengah institusi mungkin perlu digantikan, tetapi secara lebih luas kita memerlukan mekanisme baharu yang boleh meningkatkan dan menambah struktur sedia ada kita, membantu mereka berfungsi dengan lebih baik dalam dunia kita yang berkembang pesat.

Sebahagian besar kelemahan institusi kita berpunca bukan daripada struktur kerajaan formal, tetapi daripada institusi sosial yang terdegradasi: sistem kita untuk membangunkan pemahaman bersama, menyelaras tindakan, dan menjalankan wacana bermakna. Setakat ini, AI telah mempercepatkan kemerosotan ini, membanjiri saluran maklumat kita dengan kandungan yang dijana, menunjukkan kita kepada kandungan yang paling mempolarisasi dan memecah belah, dan menjadikannya lebih sukar untuk membezakan kebenaran daripada rekaan.

Tetapi AI sebenarnya boleh membantu membina semula dan mengukuhkan institusi sosial ini. Pertimbangkan tiga bidang penting:

Pertama, AI boleh membantu memulihkan kepercayaan dalam sistem epistemik kita – cara kita mengetahui apa yang benar. Kita boleh membangunkan sistem dikuasakan AI yang menjejaki dan mengesahkan asal usul maklumat, daripada data mentah melalui analisis kepada kesimpulan. Sistem ini boleh menggabungkan pengesahan kriptografi dengan analisis canggih untuk membantu orang memahami bukan sahaja sama ada sesuatu itu benar, tetapi bagaimana kita tahu ia benar.[^151] Pembantu AI yang setia boleh dipertanggungjawabkan untuk mengikuti butiran untuk memastikan bahawa mereka betul.

Kedua, AI boleh membolehkan bentuk koordinasi berskala besar yang baharu. Banyak masalah kita yang paling mendesak – daripada perubahan iklim kepada rintangan antibiotik – pada asasnya adalah masalah koordinasi. Kita [terjebak dalam situasi yang lebih teruk daripada yang mungkin untuk hampir semua orang](https://equilibriabook.com/), kerana tiada individu atau kumpulan mampu membuat langkah pertama. Sistem AI boleh membantu dengan memodelkan struktur insentif yang kompleks, mengenal pasti laluan yang berdaya maju kepada hasil yang lebih baik, dan memudahkan pembinaan kepercayaan dan mekanisme komitmen yang diperlukan untuk sampai ke sana.

Mungkin yang paling menarik, AI boleh membolehkan bentuk wacana sosial yang benar-benar baharu. Bayangkan dapat "bercakap dengan bandar"[^152] – bukan hanya melihat statistik, tetapi mengadakan dialog bermakna dengan sistem AI yang memproses dan mensintesis pandangan, pengalaman, keperluan, dan aspirasi berjuta-juta penduduk. Atau pertimbangkan bagaimana AI boleh memudahkan dialog tulen antara kumpulan yang kini bercakap melepasi satu sama lain, dengan membantu setiap pihak memahami dengan lebih baik kebimbangan dan nilai sebenar pihak lain bukannya karikatur mereka antara satu sama lain.[^153] Atau AI boleh menawarkan pengantaraan mahir dan neutral yang boleh dipercayai bagi pertikaian antara orang atau bahkan kumpulan besar orang (yang semuanya boleh berinteraksi dengannya secara langsung dan individu!) AI semasa benar-benar mampu melakukan kerja ini, tetapi alat untuk berbuat demikian tidak akan wujud dengan sendirinya, atau melalui insentif pasaran.

Kemungkinan ini mungkin terdengar utopia, terutamanya memandangkan peranan semasa AI dalam merosotkan wacana dan kepercayaan. Tetapi itulah sebabnya kita mesti membangunkan aplikasi positif ini secara aktif. Dengan menutup Pintu Gerbang kepada KBA yang tidak terkawal dan mengutamakan AI yang meningkatkan agensi manusia, kita boleh mengarahkan kemajuan teknologi ke arah masa depan di mana AI berfungsi sebagai kekuatan untuk pemerkasaan, ketahanan, dan kemajuan kolektif.


[^124]: Yang berkata, menjauhi persilangan tiga malangnya tidak semudah yang diingini. Menolak kebolehan dengan sangat keras dalam mana-mana satu daripada tiga aspek cenderung meningkatkannya dalam yang lain. Khususnya, mungkin sukar untuk mencipta kecerdasan yang sangat am dan berkebolehan yang tidak boleh dengan mudah dijadikan autonomi. Satu pendekatan ialah melatih model sistem ["rabun"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) dengan keupayaan perancangan yang tersekat. Yang lain ialah menumpukan pada kejuruteraan sistem ["orakel"](https://arxiv.org/abs/1711.05541) tulen yang akan mengelak daripada menjawab soalan berorientasikan tindakan.

[^125]: Banyak syarikat gagal menyedari bahawa mereka juga akhirnya akan disesarkan oleh KBA, walaupun mengambil masa lebih lama – jika mereka tahu, mereka mungkin menolak Pintu Gerbang tersebut sedikit kurang!

[^126]: Sistem AI boleh berkomunikasi dengan cara yang lebih cekap tetapi kurang dapat difahami, tetapi mengekalkan pemahaman manusia sepatutnya diberi keutamaan.

[^127]: Idea AI modular yang boleh ditafsir ini telah dibangunkan secara terperinci oleh beberapa penyelidik; lihat cth. model ["Perkhidmatan AI Komprehensif"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) oleh Drexler, ["Seni Bina Agensi Terbuka"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai) oleh Dalrymple dan lain-lain. Walaupun sistem sebegini mungkin memerlukan lebih banyak usaha kejuruteraan daripada rangkaian neural monolitik yang dilatih dengan pengkomputeran besar, itulah yang membantu had pengkomputeran – dengan menjadikan laluan yang lebih selamat dan lebih telus juga lebih praktikal.

[^128]: Mengenai kes keselamatan secara am lihat [buku panduan ini](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). Berkaitan dengan AI khususnya, lihat [Wasil et al.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [Clymer et al.](https://arxiv.org/abs/2403.10462), [Buhl et al.](https://arxiv.org/abs/2410.21572), dan [Balesni et al.](https://arxiv.org/abs/2411.03336)

[^129]: Kita sebenarnya sudah melihat trend ini didorong hanya oleh kos inferens yang tinggi: model yang lebih kecil dan lebih khusus "disuling" daripada yang lebih besar dan mampu berjalan pada perkakasan yang lebih murah.

[^130]: Saya faham mengapa mereka yang teruja tentang ekosistem teknologi AI mungkin menentang apa yang mereka lihat sebagai peraturan yang membebankan industri mereka. Tetapi sejujurnya membingungkan bagi saya mengapa, katakan, pemodal teroka mahu membenarkan pelarian kepada KBA dan superintelligence. Sistem tersebut (dan syarikat, selagi mereka kekal di bawah kawalan syarikat) akan *memakan semua syarikat permulaan sebagai snek*. Mungkin bahkan *lebih awal* daripada memakan industri lain. Sesiapa yang melabur dalam ekosistem AI yang berkembang maju sepatutnya mengutamakan memastikan bahawa pembangunan KBA tidak membawa kepada monopoli oleh beberapa pemain dominan.

[^131]: Seperti yang dinyatakan oleh ahli ekonomi dan bekas penyelidik Deepmind Michael Webb [meletakkannya](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/), "Saya fikir jika kita menghentikan semua pembangunan model bahasa yang lebih besar hari ini, jadi GPT-4 dan Claude dan apa sahaja, dan mereka adalah perkara terakhir yang kita latih sebesar itu – jadi kita membenarkan lebih banyak lelaran pada perkara sebesar itu dan semua jenis penalaan halus, tetapi tiada yang lebih besar daripada itu, tiada kemajuan yang lebih besar – hanya apa yang kita ada hari ini saya fikir cukup untuk memacu 20 atau 30 tahun pertumbuhan ekonomi yang luar biasa."

[^132]: Sebagai contoh, sistem alphafold DeepMind hanya menggunakan 1/100,000 nombor FLOP GPT-4.

[^133]: Kesukaran kereta pandu sendiri penting untuk diperhatikan di sini: walaupun secara nominalnya tugasan yang sempit, dan boleh dicapai dengan kebolehpercayaan yang adil dengan sistem AI yang agak kecil, pengetahuan dan pemahaman dunia sebenar yang meluas adalah perlu untuk mendapatkan kebolehpercayaan ke tahap yang diperlukan dalam tugasan kritikal keselamatan sedemikian.

[^134]: Sebagai contoh, memandangkan bajet pengkomputeran, kita mungkin akan melihat model GPAI yang dilatih dahulu pada (katakan) separuh daripada bajet tersebut, dan separuh lagi digunakan untuk melatih kebolehan yang sangat tinggi dalam julat tugasan yang lebih sempit. Ini akan memberikan kebolehan sempit super-manusia yang disokong oleh kecerdasan am hampir-manusia.

[^135]: Teknik penjajaran dominan semasa ialah "pembelajaran pengukuhan melalui maklum balas manusia" [(RLHF)](https://arxiv.org/abs/1706.03741) dan menggunakan maklum balas manusia untuk mencipta isyarat ganjaran/hukuman untuk pembelajaran pengukuhan model AI. Teknik ini dan yang berkaitan seperti [AI perlembagaan](https://arxiv.org/abs/2212.08073) berfungsi dengan mengejutkan (walaupun mereka tidak mempunyai keteguhan dan boleh dipintas dengan usaha sederhana.) Selain itu, model bahasa semasa umumnya cukup cekap dalam penaakulan akal biasa sehingga mereka tidak akan membuat kesilapan moral yang bodoh. Ini adalah sesuatu tempat manis: cukup pintar untuk memahami apa yang dikehendaki orang (sejauh mana ia boleh ditakrifkan), tetapi tidak cukup pintar untuk merancang penipuan yang rumit atau menyebabkan kemudaratan besar apabila mereka tersilap.

[^136]: Dalam jangka panjang, sebarang tahap kebolehan AI yang dibangunkan berkemungkinan akan merebak, kerana akhirnya ia adalah perisian, dan berguna. Kita akan perlu mempunyai mekanisme yang teguh untuk mempertahankan diri daripada risiko yang ditimbulkan oleh sistem sedemikian. Tetapi kita *tidak mempunyainya sekarang* jadi kita mesti sangat berhati-hati dalam berapa banyak model AI berkuasa dibenarkan untuk merebak.

[^137]: Sebahagian besar daripada ini adalah deepfake pornografi tanpa persetujuan, termasuk kanak-kanak bawah umur.

[^138]: Banyak bahan untuk penyelesaian tersebut wujud, dalam bentuk undang-undang "bot-atau-tidak" (dalam akta AI EU antara tempat lain), [teknologi penjejakan asal usul industri](https://c2pa.org/), [pengumpul berita inovatif](https://www.improvethenews.org/), [pengumpul](https://metaculus.com/) dan pasaran ramalan, dll.

[^139]: Gelombang automasi mungkin tidak mengikuti corak sebelumnya, kerana tugas kemahiran *tinggi* yang agak seperti penulisan berkualiti, mentafsir undang-undang, atau memberi nasihat perubatan, mungkin sama atau lebih terdedah kepada automasi daripada tugas kemahiran rendah.

[^140]: Untuk pemodelan teliti kesan KBA terhadap upah, lihat laporan [di sini](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek), dan butiran berdarah [di sini](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0), daripada Anton Korinek dan rakan sekerja. Mereka mendapati bahawa apabila lebih banyak bahagian pekerjaan diautomasikan, produktiviti dan gaji naik – sehingga satu tahap. Sebaik sahaja *terlalu* banyak diautomasikan, produktiviti terus meningkat, tetapi gaji jatuh kerana orang digantikan secara menyeluruh oleh AI yang cekap. Inilah sebabnya menutup Pintu Gerbang sangat berguna: kita mendapat produktiviti tanpa gaji manusia yang hilang.

[^141]: Terdapat banyak cara AI boleh digunakan sebagai, dan untuk membantu membina, teknologi "pertahanan" untuk menjadikan perlindungan dan pengurusan lebih teguh. Lihat [ini](https://vitalik.eth.limo/general/2025/01/05/dacc2.html) pos berpengaruh yang menerangkan agenda "D/acc" ini.

[^142]: Agak ironis, projek Manhattan AS berkemungkinan tidak akan berbuat banyak untuk mempercepatkan garis masa ke arah KBA – dial pelaburan manusia dan fiskal dalam kemajuan AI sudah disematkan pada 11. Keputusan utama adalah untuk menginspirasi projek serupa di China (yang cemerlang dalam projek infrastruktur peringkat kebangsaan), menjadikan perjanjian antarabangsa yang mengehadkan risiko AI jauh lebih sukar, dan menggerunkan musuh geopolitik lain AS seperti Rusia.

[^143]: Program ["Sumber Penyelidikan AI Kebangsaan"](https://nairrpilot.org/) adalah langkah semasa yang baik dalam arah ini dan sepatutnya diperluas.

[^144]: Lihat [analisis ini](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807) tentang pelbagai makna dan implikasi "terbuka" dalam produk teknologi dan bagaimana sesetengahnya telah membawa kepada lebih banyak, bukannya kurang, pemantapan dominasi.

[^145]: Rancangan di AS untuk [Sumber Penyelidikan AI Kebangsaan](https://nairratdoe.ornl.gov/) dan pelancaran baru-baru ini [Yayasan AI Eropah](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/) adalah langkah menarik dalam arah ini.

[^146]: Cabaran di sini bukanlah teknikal tetapi institusi – kita amat memerlukan contoh dan eksperimen dunia sebenar tentang rupa pembangunan AI kepentingan awam.

[^147]: Ini bertentangan dengan model perniagaan teknologi besar semasa dan akan memerlukan tindakan undang-undang dan norma baharu.

[^148]: Hanya sesetengah kerajaan akan dapat berbuat demikian. Idea yang lebih radikal ialah [dana sejagat jenis ini, di bawah pemilikan bersama semua manusia.](https://futureoflife.org/project/the-windfall-trust/)

[^149]: Untuk huraian panjang kes ini lihat [kertas kerja ini](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338) mengenai kesetiaan AI. Malangnya trajektori lalai pembantu AI berkemungkinan menjadi satu di mana mereka semakin tidak setia.

[^150]: Agak ironis, banyak kuasa penyandang juga berisiko kehilangan kuasa yang disokong AI; tetapi mungkin sukar bagi mereka untuk menyedari ini sehingga dan melainkan proses itu berjalan agak jauh.

[^151]: Beberapa usaha menarik dalam arah ini diwakili oleh [gabungan c2pa](https://c2pa.org/) mengenai pengesahan kriptografi; [Verity](https://www.improvethenews.org/) dan [Ground news](https://ground.news/) mengenai epistemik berita yang lebih baik; dan [Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com) dan pasaran ramalan mengenai mendasarkan wacana dalam ramalan yang boleh disangkal.

[^152]: Lihat projek perintis [yang menarik ini](https://talktothecity.org/).

[^153]: Lihat [Kialo](https://www.kialo-edu.com/), dan usaha [Projek Kecerdasan Kolektif](https://www.cip.org/) untuk beberapa contoh.

## Bab 10 - Pilihan di hadapan kita

Untuk memelihara masa depan manusia kita, kita mesti memilih untuk menutup Pintu Gerbang kepada KBA dan superintelligence.

Kali terakhir manusia berkongsi Bumi dengan minda-minda lain yang bertutur, berfikir, membina teknologi, dan melakukan penyelesaian masalah secara umum adalah 40,000 tahun yang lalu di Eropah zaman ais. Minda-minda lain itu telah pupus, sepenuhnya atau sebahagiannya disebabkan usaha minda kita.

Kita kini memasuki kembali zaman sedemikian. Produk-produk paling canggih dalam budaya dan teknologi kita – set data yang dibina daripada seluruh ruang maklumat internet kita, dan cip 100-bilion-elemen yang merupakan teknologi paling kompleks yang pernah kita cipta – sedang digabungkan untuk melahirkan sistem AI tujuan umum yang canggih.

Para pembangun sistem ini berminat untuk menggambarkannya sebagai alat untuk memperkasakan manusia. Dan memang ia boleh menjadi begitu. Tetapi jangan silap: trajektori semasa kita ialah membina agen digital yang semakin berkuasa, terarah matlamat, membuat keputusan, dan berkebolehan umum. Mereka sudah berprestasi sebaik ramai manusia dalam pelbagai tugasan intelektual, sedang berkembang pesat, dan menyumbang kepada penambahbaikan diri mereka sendiri.

Melainkan trajektori ini berubah atau menghadapi halangan yang tidak dijangka, kita akan segera – dalam tahun-tahun, bukan dekad-dekad – mempunyai kecerdasan digital yang berbahaya kuasa. Walaupun dalam hasil yang *terbaik*, ini akan membawa manfaat ekonomi yang besar (sekurang-kurangnya kepada sebahagian kita) tetapi hanya dengan kos gangguan yang mendalam dalam masyarakat kita, dan penggantian manusia dalam kebanyakan perkara paling penting yang kita lakukan: mesin-mesin ini akan berfikir untuk kita, merancang untuk kita, membuat keputusan untuk kita, dan mencipta untuk kita. Kita akan dimanjakan, tetapi seperti kanak-kanak yang dimanja. Yang lebih berkemungkinan, sistem ini akan menggantikan manusia dalam perkara positif *dan* negatif yang kita lakukan, termasuk eksploitasi, manipulasi, keganasan, dan peperangan. Bolehkah kita bertahan dengan versi yang diperkuatkan-AI ini? Akhirnya, lebih daripada munasabah bahawa keadaan tidak akan berjalan lancar sama sekali: bahawa agak tidak lama lagi kita akan digantikan bukan sahaja dalam apa yang kita lakukan, tetapi dalam apa yang kita *adakan*, sebagai arkitek tamadun dan masa depan. Tanya orang Neanderthal bagaimana keadaannya. Mungkin kita memberikan mereka pernak-pernik tambahan untuk sementara waktu juga.

*Kita tidak perlu melakukan ini.* Kita mempunyai AI yang boleh bersaing dengan manusia, dan tiada keperluan untuk membina AI yang tidak *boleh* kita tandingi. Kita boleh membina alat AI yang menakjubkan tanpa membina spesies pengganti. Tanggapan bahawa KBA dan superintelligence tidak dapat dielakkan adalah *pilihan yang menyamar sebagai takdir*.

Dengan mengenakan beberapa had yang keras dan global, kita boleh mengekalkan keupayaan umum AI pada tahap lebih kurang manusia sambil masih meraih manfaat keupayaan komputer memproses data dengan cara yang tidak boleh kita lakukan, dan mengautomasikan tugasan yang tidak mahu dilakukan oleh sesiapa pun. Ini masih akan menimbulkan banyak risiko, tetapi jika direka dan diurus dengan baik, akan menjadi anugerah yang sangat besar kepada manusia, dari perubatan hingga penyelidikan hingga produk pengguna.

Mengenakan had akan memerlukan kerjasama antarabangsa, tetapi kurang daripada yang mungkin disangka, dan had tersebut masih akan meninggalkan banyak ruang untuk industri AI dan perkakasan AI yang sangat besar yang tertumpu pada aplikasi yang meningkatkan kesejahteraan manusia, bukannya pada pengejaran kuasa mentah. Dan jika, dengan jaminan keselamatan yang kukuh dan selepas dialog global yang bermakna, kita memutuskan untuk pergi lebih jauh, pilihan itu terus menjadi milik kita untuk diusahakan.

Manusia mesti *memilih* untuk menutup Pintu Gerbang kepada KBA dan superintelligence.

Untuk mengekalkan masa depan yang berperikemanusiaan.

### Nota daripada Pengarang

Terima kasih kerana meluangkan masa untuk menerokai topik ini bersama kami.

Saya menulis esei ini kerana sebagai seorang saintis saya merasakan adalah penting untuk menyatakan kebenaran yang tidak dipoles, dan kerana sebagai seorang manusia saya merasakan adalah penting bagi kita untuk bertindak dengan pantas dan tegas dalam menangani isu yang mengubah dunia: pembangunan sistem AI yang lebih pintar daripada manusia.

Jika kita ingin bertindak balas terhadap keadaan yang luar biasa ini dengan bijaksana, kita mesti bersedia untuk meneliti secara kritikal naratif semasa bahawa KBA dan superintelligence 'mesti' dibina untuk menjamin kepentingan kita, atau adalah 'tidak dapat dielakkan' dan tidak boleh dihentikan. Naratif ini menyebabkan kita tidak berdaya, tidak dapat melihat laluan alternatif di hadapan kita.

Saya berharap anda akan bergabung dengan saya dalam menyeru berhati-hati dalam menghadapi kecuaian, dan berani dalam menghadapi ketamakan.

Saya berharap anda akan bergabung dengan saya dalam menyeru masa depan yang berperikemanusiaan.

*– Anthony*

![Anthony Aguirre signature](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAnthony-Aguirre-signature-300x84.png&w=3840&q=75)

## Lampiran

Maklumat tambahan, termasuk - Butiran teknikal mengenai perakaunan pengkomputeran, contoh pelaksanaan 'penutupan pintu gerbang', butiran untuk rejim liabiliti KBA yang ketat, dan pendekatan berperingkat untuk piawaian keselamatan dan keamanan KBA.

### Lampiran A: Butiran teknikal perakaunan pengkomputeran

Kaedah terperinci untuk kedua-dua "kebenaran asas" serta anggaran yang baik bagi jumlah pengkomputeran yang digunakan dalam latihan dan inferens diperlukan untuk kawalan berasaskan pengkomputeran yang bermakna. Berikut adalah contoh bagaimana "kebenaran asas" boleh dikira pada tahap teknikal.

**Definisi:**

*Graf kausal pengkomputeran:* Bagi output O yang diberikan dari model AI, terdapat set pengkomputeran digital yang mengubah hasil pengkomputeran tersebut berpotensi mengubah O. (Ini harus dianggap secara konservatif, iaitu harus ada sebab yang jelas untuk mempercayai bahawa pengkomputeran adalah bebas daripada prekursor yang berlaku lebih awal dalam masa dan mempunyai laluan kausal fizikal yang berpotensi memberi kesan.) Ini termasuk pengkomputeran yang dilakukan oleh model AI semasa inferens, serta pengkomputeran yang masuk ke dalam input, penyediaan data, dan latihan model. Kerana mana-mana ini mungkin merupakan output dari model AI, ini dikira secara rekursif, dipotong di mana manusia telah memberikan perubahan yang signifikan kepada input.

*Pengkomputeran Latihan:* Jumlah pengkomputeran, dalam FLOP atau unit lain, yang diperlukan oleh graf kausal pengkomputeran rangkaian neural (termasuk penyediaan data, latihan, dan penalaan halus, dan mana-mana pengkomputeran lain.)

*Pengkomputeran Output:* Jumlah pengkomputeran dalam graf kausal pengkomputeran output AI yang diberikan, termasuk semua rangkaian neural (dan termasuk Pengkomputeran Latihan mereka) dan pengkomputeran lain yang masuk ke dalam output tersebut.

*Kadar Pengkomputeran Inferens:* Dalam siri output, kadar perubahan (dalam FLOP/s atau unit lain) Pengkomputeran Output antara output, iaitu pengkomputeran yang digunakan untuk menghasilkan output seterusnya, dibahagi dengan selang masa antara output.

**Contoh dan anggaran:**

- Untuk rangkaian neural tunggal yang dilatih pada data yang dicipta manusia, Pengkomputeran Latihan hanyalah jumlah pengkomputeran latihan seperti yang lazimnya dilaporkan.
- Untuk rangkaian neural sedemikian yang melakukan inferens pada kadar yang tetap, Kadar Pengkomputeran Inferens adalah lebih kurang jumlah kelajuan kelompok pengkomputeran yang melakukan inferens dalam FLOP/s.
- Untuk penalaan halus model, Pengkomputeran Latihan model lengkap diberikan oleh Pengkomputeran Latihan model tidak ditala halus ditambah pengkomputeran yang dilakukan semasa penalaan halus dan untuk menyediakan sebarang data yang digunakan dalam penalaan halus.
- Untuk model yang disuling, Pengkomputeran Latihan model lengkap termasuk latihan kedua-dua model yang disuling dan model yang lebih besar yang digunakan untuk menyediakan data sintetik atau input latihan lain.
- Jika beberapa model dilatih, tetapi banyak "percubaan" dibuang berdasarkan pertimbangan manusia, ini tidak dikira ke arah Pengkomputeran Latihan atau Output model yang dikekalkan.

### Lampiran B: Contoh pelaksanaan penutupan pintu gerbang

**Contoh Pelaksanaan:** Berikut adalah satu contoh bagaimana penutupan pintu gerbang boleh berfungsi, dengan had 10<sup>27</sup> FLOP untuk latihan dan 10<sup>20</sup> FLOP/s untuk inferens (menjalankan AI):

**1\. Jeda:** Atas sebab keselamatan negara, cabang Eksekutif AS meminta semua syarikat yang berpangkalan di AS, menjalankan perniagaan di AS, atau menggunakan cip yang dikeluarkan di AS, untuk berhenti dan menghentikan sebarang larian latihan AI baru yang mungkin melebihi had Pengkomputeran Latihan 10<sup>27</sup> FLOP. AS harus memulakan perbincangan dengan negara-negara lain yang menjadi tuan rumah pembangunan AI, menggalakkan mereka dengan kuat untuk mengambil langkah yang serupa dan menunjukkan bahawa jeda AS mungkin ditarik balik sekiranya mereka memilih untuk tidak mematuhi.

**2\. Pengawasan dan pelesenan AS:** Melalui perintah eksekutif atau tindakan agensi kawal selia sedia ada, AS menghendaki dalam tempoh (katakan) satu tahun:

- Semua larian latihan AI yang dianggarkan melebihi 10<sup>25</sup> FLOP yang dilakukan oleh syarikat yang beroperasi di AS didaftarkan dalam pangkalan data yang dikekalkan oleh agensi kawal selia AS. (Nota: Versi yang lebih lemah sedikit daripada ini telah dimasukkan dalam perintah eksekutif AS 2023 mengenai AI yang kini dibatalkan, yang memerlukan pendaftaran untuk model melebihi 10<sup>26</sup> FLOP.)
- Semua pengeluar perkakasan berkaitan AI yang beroperasi di AS atau menjalankan perniagaan dengan USG mematuhi set keperluan pada perkakasan khusus mereka dan perisian yang menggerakkannya. (Banyak daripada keperluan ini boleh dibina ke dalam kemas kini perisian dan perisian tegar kepada perkakasan sedia ada, tetapi penyelesaian jangka panjang dan kukuh memerlukan perubahan kepada generasi perkakasan kemudian.) Antara ini adalah keperluan bahawa jika perkakasan adalah sebahagian daripada kelompok yang saling berkaitan berkelajuan tinggi yang mampu melaksanakan 10<sup>18</sup> FLOP/s pengkomputeran, tahap pengesahan yang lebih tinggi diperlukan, yang merangkumi kebenaran tetap oleh "gabenor" jauh yang menerima kedua-dua telemetri dan permintaan untuk melakukan pengkomputeran tambahan.
- Penjaga melaporkan jumlah pengkomputeran yang dilakukan pada perkakasannya kepada agensi yang mengekalkan pangkalan data AS.
- Keperluan yang lebih kuat diperkenalkan secara berperingkat untuk membolehkan pengawasan dan pemberian kebenaran yang lebih selamat dan fleksibel.

**3\. Pengawasan antarabangsa:**

- AS, China, dan mana-mana negara lain yang menjadi tuan rumah keupayaan pembuatan cip maju merundingkan perjanjian antarabangsa.
- Perjanjian ini mencipta agensi antarabangsa baru, serupa dengan Agensi Tenaga Atom Antarabangsa, yang bertugas mengawasi latihan dan pelaksanaan AI.
- Negara-negara penandatangan mesti menghendaki pengeluar perkakasan AI domestik mereka mematuhi set keperluan sekurang-kurangnya sekuat yang dikenakan di AS.
- Penjaga kini dikehendaki melaporkan nombor pengkomputeran AI kepada kedua-dua agensi di negara asal mereka serta pejabat baru dalam agensi antarabangsa.
- Negara-negara tambahan digalakkan dengan kuat untuk menyertai perjanjian antarabangsa sedia ada: kawalan eksport oleh negara penandatangan mengehadkan akses kepada perkakasan mewah oleh bukan penandatangan manakala penandatangan boleh menerima sokongan teknikal dalam menguruskan sistem AI mereka.

**4\. Pengesahan dan penguatkuasaan antarabangsa:**

- Sistem pengesahan perkakasan dikemas kini supaya ia melaporkan penggunaan pengkomputeran kepada kedua-dua penjaga asal dan juga terus kepada pejabat agensi antarabangsa.
- Agensi, melalui perbincangan dengan penandatangan perjanjian antarabangsa, bersetuju mengenai had pengkomputeran yang kemudian berkuat kuasa undang-undang di negara penandatangan.
- Secara selari, set piawaian antarabangsa mungkin dibangunkan supaya latihan dan menjalankan AI melebihi ambang pengkomputeran (tetapi di bawah had) dikehendaki mematuhi piawaian tersebut.
- Agensi boleh, jika perlu untuk mengimbangi algoritma yang lebih baik dll., menurunkan had pengkomputeran. Atau, jika dianggap selamat dan digalakkan (pada katakan tahap jaminan keselamatan yang boleh dibuktikan), menaikkan had pengkomputeran.

### Lampiran C: Butiran untuk rejim liabiliti KBA yang ketat

**Butiran untuk rejim liabiliti KBA yang ketat**

- Penciptaan dan operasi sistem AI maju yang sangat am, berkebolehan, dan autonomi, dianggap sebagai aktiviti "luar biasa berbahaya".
- Oleh itu, liabiliti lalai untuk latihan dan operasi sistem sedemikian adalah liabiliti ketat, bersama dan beberapa (atau setarafnya bukan AS) untuk sebarang kemudaratan yang dilakukan oleh model atau output/tindakannya.
- Liabiliti peribadi akan dikenakan untuk eksekutif dan ahli lembaga dalam kes kecuaian kasar atau salah laku yang disengajakan. Ini harus termasuk penalti jenayah untuk kes yang paling teruk.
- Terdapat banyak pelabuhan selamat di mana liabiliti kembali kepada liabiliti lalai (berasaskan kesalahan, di AS) yang biasanya dikenakan kepada orang dan syarikat.
	- Model yang dilatih dan dioperasikan di bawah ambang pengkomputeran tertentu (yang sekurang-kurangnya 10x lebih rendah daripada had yang diterangkan di atas.)
	- AI yang "lemah" (secara kasarnya, di bawah tahap pakar manusia pada tugas-tugas yang dimaksudkan) dan/atau
	- AI yang "sempit" (mempunyai skop tugas dan operasi yang tetap dan agak terhad yang direka dan dilatih khusus untuknya) dan/atau
	- AI yang "pasif" (sangat terhad dalam keupayaannya – walaupun di bawah pengubahsuaian sederhana – untuk mengambil tindakan atau melakukan tugas berbilang langkah yang kompleks tanpa penglibatan dan kawalan manusia langsung.)
	- AI yang dijamin selamat, terjamin, dan boleh dikawal (selamat secara boleh dibuktikan, atau analisis risiko menunjukkan tahap kemudaratan yang dijangka boleh diabaikan.)
- Pelabuhan selamat boleh dituntut berdasarkan [kes keselamatan](https://arxiv.org/abs/2410.21572) yang disediakan oleh pembangun AI dan diluluskan oleh agensi atau juruaudit yang diberi akreditasi oleh agensi. Untuk menuntut pelabuhan selamat berdasarkan pengkomputeran, pembangun hanya perlu membekalkan anggaran yang boleh dipercayai bagi jumlah Pengkomputeran Latihan dan Kadar Inferens maksimum
- Perundangan akan menggariskan secara jelas situasi di mana pelepasan injunktif daripada pembangunan sistem AI dengan risiko tinggi kemudaratan awam adalah sesuai.
- Konsortium syarikat, bekerjasama dengan NGO dan agensi kerajaan, harus membangunkan piawaian dan norma yang mentakrifkan istilah-istilah ini, bagaimana pengawal selia harus memberikan pelabuhan selamat, bagaimana pembangun AI harus membangunkan kes keselamatan, dan bagaimana mahkamah harus mentafsir liabiliti di mana pelabuhan selamat tidak dituntut secara proaktif.

### Lampiran D: Pendekatan berperingkat untuk piawaian keselamatan & keamanan KBA

**Pendekatan berperingkat untuk piawaian keselamatan & keamanan KBA**

| Peringkat Risiko | Pencetus | Keperluan untuk latihan | Keperluan untuk penggunaan |
| --- | --- | --- | --- |
| RT-0 | AI lemah dalam autonomi, kegeneralan, dan kecerdasan | tiada | tiada |
| RT-1 | AI kuat dalam satu daripada autonomi, kegeneralan, dan kecerdasan | tiada | Berdasarkan risiko dan penggunaan, berpotensi kes keselamatan yang diluluskan oleh pihak berkuasa negara di mana sahaja model boleh digunakan |
| RT-2 | AI kuat dalam dua daripada autonomi, kegeneralan, dan kecerdasan | Pendaftaran dengan pihak berkuasa negara dengan bidang kuasa ke atas pembangun | Kes keselamatan yang mengehadkan risiko kemudaratan besar di bawah paras yang diberi kuasa ditambah audit keselamatan bebas (termasuk redteaming kotak hitam dan kotak putih) yang diluluskan oleh pihak berkuasa negara di mana sahaja model boleh digunakan |
| RT-3 | KBA kuat dalam autonomi, kegeneralan, dan kecerdasan | Pra-kelulusan pelan keselamatan dan keamanan oleh pihak berkuasa negara dengan bidang kuasa ke atas pembangun | Kes keselamatan yang menjamin risiko terbatas kemudaratan besar di bawah paras yang diberi kuasa serta spesifikasi yang diperlukan, termasuk keamanan siber, kebolehkawalan, suis bunuh yang tidak boleh ditanggalkan, penjajaran dengan nilai manusia, dan ketahanan terhadap penggunaan berniat jahat. |
| RT-4 | Mana-mana model yang juga melebihi sama ada 10<sup>27</sup> FLOP Latihan atau 10<sup>20</sup> FLOP/s Inferens | Dilarang sementara menunggu penarikan had pengkomputeran yang dipersetujui antarabangsa | Dilarang sementara menunggu penarikan had pengkomputeran yang dipersetujui antarabangsa |

Klasifikasi risiko dan piawaian keselamatan/keamanan, dengan peringkat berdasarkan ambang pengkomputeran serta gabungan autonomi, kegeneralan, dan kecerdasan yang tinggi:

- *Autonomi kuat* terpakai jika sistem mampu melaksanakan, atau boleh dibuat dengan mudah untuk melaksanakan, tugas berbilang langkah dan/atau mengambil tindakan kompleks yang relevan dengan dunia sebenar, tanpa pengawasan atau campur tangan manusia yang signifikan. Contoh: kenderaan autonomi dan robot; bot perdagangan kewangan. Bukan contoh: GPT-4; pengklasifikasi imej
- *Kegeneralan kuat* menunjukkan skop aplikasi yang luas, prestasi tugas-tugas yang mana model tidak sengaja dan khusus dilatih, dan keupayaan signifikan untuk mempelajari tugas baru. Contoh: GPT-4; mu-zero. Bukan contoh: AlphaFold; kenderaan autonomi; penjana imej
- *Kecerdasan kuat* sepadan dengan prestasi yang menyamai tahap pakar manusia pada tugas-tugas yang mana model berprestasi terbaik (dan untuk model am, merentas pelbagai tugas yang luas.) Contoh: AlphaFold; mu-zero; o3. Bukan contoh: GPT-4; Siri

### Penghargaan

Beberapa ucapan terima kasih kepada mereka yang menyumbang kepada Keep The Future Human.

Karya ini mencerminkan pendapat penulis dan tidak boleh dianggap sebagai kedudukan rasmi Future of Life Institute (walaupun ia adalah serasi; untuk kedudukan rasminya sila lihat [halaman ini](https://futureoflife.org/our-position-on-ai/)), atau mana-mana organisasi lain yang penulis bergabung dengannya.

Saya bersyukur kepada manusia Mark Brakel, Ben Eisenpress, Anna Hehir, Carlos Gutierrez, Emilia Javorsky, Richard Mallah, Jordan Scharnhorst, Elyse Fulcher, Max Tegmark, dan Jaan Tallinn atas komen mereka terhadap manuskrip ini; kepada Tim Schrier atas bantuan dengan beberapa rujukan; kepada Taylor Jones dan Elyse Fulcher atas mempercantikkan diagram.

Karya ini menggunakan model AI generatif (Claude dan ChatGPT) secara terhad dalam penciptaannya, untuk beberapa penyuntingan dan red-teaming. Dalam piawaian yang mapan bagi tahap penglibatan AI dalam karya kreatif, karya ini mungkin berkadar 3/10. (Sebenarnya tiada piawaian sedemikian! Tetapi sepatutnya ada.)

Kami amat berterima kasih kepada [Julius Odai](https://www.linkedin.com/in/julius-odai/) kerana menghasilkan versi web esei ini, yang menjadikan pembacaan dan navigasi di sekitar esei ini satu pengalaman yang sangat menyenangkan. Julius adalah seorang teknologi dan peserta terbaharu kursus BlueDot Impact AI Governance.