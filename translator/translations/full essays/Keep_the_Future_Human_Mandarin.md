# 保持未来属于人类

本文论证了我们为什么以及如何应该关闭通向通用人工智能(AGI)和超级智能的大门，以及我们应该构建什么来替代它们。

如果您只想了解核心要点，请参阅执行摘要。然后，第2-5章将提供本文讨论的AI系统类型的一些背景知识。第5-7章解释了为什么我们可以预期AGI即将到来，以及当它出现时可能发生什么。最后，第8-9章概述了阻止AGI被构建的具体提案。

[下载PDF](https://keepthefuturehuman.ai/wp-content/uploads/2025/03/Keep_the_Future_Human__AnthonyAguirre__5March2025.pdf)

总阅读时长：2-3小时

## 执行摘要

本文的高层次概览。如果您时间有限，只需10分钟即可了解所有要点。

过去十年间，人工智能取得了巨大进步（针对特定用途的AI），尤其是近几年来（针对通用AI），这使得AI从一个小众学术领域转变为世界许多最大公司的核心商业战略，每年在推进AI能力的技术和工艺上投资数千亿美元。

如今我们走到了一个关键节点。随着新AI系统的能力开始在许多认知领域与人类匹敌甚至超越人类，人类必须决定：我们要走多远，朝什么方向走？

AI和每项技术一样，最初的目标都是为其创造者改善现状。但我们当前的轨迹和隐含选择，是朝着更强大系统的无节制竞赛，这种竞赛由少数几家大型科技公司的经济激励驱动，它们寻求将当前大量经济活动和人类劳动自动化。如果这场竞赛继续下去，必然会有一个赢家：AI本身——在我们的经济、思维、决策中成为比人类更快、更智能、更廉价的替代品，最终控制我们的文明。

但我们可以做出另一种选择：通过我们的政府，我们可以控制AI开发过程，施加明确的限制、划定不可逾越的红线、确立绝对不做的事情——正如我们对核技术、大规模杀伤性武器、太空武器、破坏环境的工艺、人类生物工程和优生学所做的那样。最重要的是，我们可以确保AI仍然是赋能人类的工具，而不是取代并最终替代我们的新物种。

本文论证，我们应该通过关闭通向比人类更智能的、自主的、通用AI——有时称为"AGI"——的"大门"来*保持未来的人性*，特别是那种有时被称为"超级智能"的高度超人版本。相反，我们应该专注于强大、可信的AI工具，这些工具能够赋能个人，并变革性地提升人类社会在其最擅长领域的能力。这一论证的结构简述如下。

### AI与众不同

AI系统在根本上不同于其他技术。传统软件遵循精确指令，而AI系统学习如何实现目标，无需被明确告知如何做。这使它们变得强大：如果我们能够清晰定义目标或成功指标，在大多数情况下AI系统都能学会实现它。但这也使它们天然不可预测：我们无法可靠地确定它们会采取什么行动来实现目标。

它们也很大程度上无法解释：尽管它们部分是代码，但主要是一组巨大的难以理解的数字——神经网络"权重"——无法被解析；我们理解它们内部工作机制的能力，并不比通过窥视生物大脑来辨别思维强多少。

这种训练数字神经网络的核心模式正在快速增加复杂性。最强大的AI系统通过大规模计算实验创建，使用专用硬件在巨大数据集上训练神经网络，然后用软件工具和上层结构加以增强。

这导致了非常强大工具的诞生，用于创建和处理文本和图像、进行数学和科学推理、聚合信息，以及交互式查询人类知识的庞大储备。

不幸的是，虽然开发更强大、更可信的技术工具是我们*应该*做的，也是几乎每个人都想要并声称想要的，但这并不是我们实际所走的轨迹。

### 通用人工智能与超级智能

自该领域诞生以来，AI研究实际上专注于一个不同的目标：通用人工智能。这个焦点现在已成为领导AI开发的巨头公司的重点。

什么是AGI？它通常被模糊地定义为"人类水平的AI"，但这是有问题的：是哪些人类，在哪些能力上达到人类水平？那它已经拥有的超人能力又如何解释？理解AGI的更有用方式是通过三个关键属性的交集：高度**自主**性（行动独立性）、高度**通用**性（广泛范围和适应性），以及高度**智能**（认知任务能力）。当前的AI系统可能能力很强但狭窄，或通用但需要持续的人类监督，或自主但范围有限。

完全的A-G-I将在匹配或超越顶级人类能力的水平上结合所有三个属性。关键在于，正是这种组合使人类如此高效并与当前软件如此不同；这也是使人类能够被数字系统整体替代的原因。

虽然人类智能很特殊，但它绝非极限。人工"超智能"系统可以运行速度快数百倍，解析大量更多数据并同时"记住"巨大数量的信息，形成比人类集合更大更有效的聚合体。它们可能取代的不是个人，而是公司、国家，或我们整个文明。

### 我们正处于门槛上

有强烈的科学共识认为AGI是*可能的*。AI已经在许多智力能力的通用测试中超越了人类表现，包括最近的高级推理和问题解决。滞后的能力——如持续学习、规划、自我意识和原创性——在当前AI系统中都在某种程度上存在，而且已知的技术很可能改善所有这些能力。

虽然直到几年前许多研究人员还认为AGI还需要几十年，但目前AGI短期实现的证据很强：

- 经验验证的"缩放法则"将计算输入与AI能力联系起来，公司正在按计划在未来几年将计算输入按数量级扩大。现在专门用于AI发展的人力和财政资源相当于十几个曼哈顿项目和几个阿波罗项目。
- AI公司及其领导者公开和私下都相信AGI（按某种定义）在几年内是可以实现的。这些公司拥有公众没有的信息，包括一些已经掌握了下一代AI系统。
- 有着良好记录的专家预测者给AGI（按某种定义）在1-2年内到来分配25%的概率，2-5年内为50%（参见Metaculus对["弱"](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/)和["完全"](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/)AGI的预测）。
- 自主性（包括长期灵活规划）在AI系统中滞后，但主要公司现在正将其庞大资源专注于开发自主AI系统，并非正式地将2025年命名为["智能体之年"](https://techinformed.com/2025-informed-the-year-of-agentic-ai/)。
- AI越来越多地为自身改进做出贡献。一旦AI系统在做AI研究方面与人类AI研究员一样胜任，就会达到快速发展到更强大AI系统的关键阈值，很可能导致AI能力的失控。（可以说，这种失控已经开始。）

认为比人类更智能的AGI还需要几十年或更长时间的想法，对于该领域的绝大多数专家来说已经不再站得住脚。现在的分歧在于如果我们继续这个路线，需要多少个月或年。我们面临的核心问题是：我们应该继续吗？

### 什么在驱动AGI竞赛

朝向AGI的竞赛由多种力量驱动，每种都使情况更加危险。主要技术公司将AGI视为终极自动化技术——不仅是增强人类工作者，而是大部分或完全替代他们。对公司而言，奖赏是巨大的：通过自动化消除人力成本，有机会获取世界100万亿美元年经济产出的重要份额。

各国感到被迫加入这场竞赛，公开声称是为了经济和科学领导地位，但私下将AGI视为与核武器相当的潜在军事革命。担心对手可能获得决定性战略优势创造了典型的军备竞赛动态。

追求超级智能的人常常引用宏大愿景：治愈所有疾病、逆转衰老、在能源和太空旅行方面取得突破，或创造超人规划能力。

不那么宽容地说，驱动竞赛的是权力。每个参与者——无论是公司还是国家——都相信智能等于权力，并且他们将是这种权力的最佳管理者。

我认为这些动机是真实的但根本上是误导的：AGI将*吸收*和*寻求*权力，而不是授予权力；AI创造的技术*也*将是强烈的双刃剑，在有益的地方可以用AI工具创造而无需AGI；即使AGI及其产出仍处于控制之下，这些竞赛动态——无论是企业还是地缘政治——都使我们社会面临大规模风险几乎不可避免，除非被果断打断。

### 通用人工智能和超级智能对文明构成巨大威胁

尽管它们具有吸引力，AGI和超级智能通过多种相互强化的途径对文明构成巨大威胁：

*权力集中：*超人AI可能通过将大量社会和经济活动吸收到由少数几家巨型公司运营的AI系统中（这些公司反过来可能被政府接管，或有效接管政府），从而剥夺绝大多数人类的权力。

*大规模破坏：*大部分基于认知的工作的批量自动化、我们当前认识论系统的替换，以及大量活跃非人类智能体的部署，将在相对短的时间内颠覆我们当前的大部分文明系统。

*灾难：*通过扩散创造新军事和破坏性技术的能力——可能超过人类水平——并将其与责任的社会和法律系统解耦，大规模杀伤性武器造成的物理灾难变得极其可能。

*地缘政治与战争：*如果主要世界大国感到可能提供"决定性战略优势"的技术正在被其对手开发，它们不会袖手旁观。

*失控和失去控制：*除非特别加以防止，超人AI将有各种激励来进一步改进自身，并可能在速度、数据处理和思维复杂性方面远远超过人类。我们没有任何有意义的方式可以控制这样的系统。这样的AI不会授予人类权力；我们将授予它权力，或者它将夺取权力。

即使技术"对齐"问题——确保先进AI可靠地做人类希望它做的事——得到解决，许多这些风险仍然存在。AI在如何管理方面提出了巨大挑战，而这种管理的许多方面随着人类智能被突破而变得极其困难或难以处理。

最根本的是，目前正在追求的那种超人通用AI，从其本质上讲，将拥有超越我们自身的目标、主观能动性和能力。它将本质上不可控制——我们如何控制一个我们既不能理解也无法预测的东西？它不会是供人类使用的技术工具，而是地球上与我们并存的第二个智能物种。如果允许其进一步发展，它将不仅构成第二个物种，而且是一个替代物种。

也许它会善待我们，也许不会。但未来将属于它，而不是我们。人类时代将结束。

### 这不是不可避免的；人类可以非常具体地决定不建造我们的替代品。

创造超人AGI远非不可避免。我们可以通过一套协调的治理措施来防止它：

首先，我们需要对AI计算（"算力"）进行强有力的核算和监督，这是大规模AI系统的根本推动者和治理杠杆。这反过来需要标准化测量和报告训练AI模型和运行它们所用的总算力，以及统计、认证和验证所用计算的技术方法。

其次，我们应该对AI计算实施硬性限制，包括训练和运行；这些既防止AI过于强大，也防止运行过快。这些限制可以通过法律要求和内置于AI专用芯片的基于硬件的安全措施来实施，类似于现代手机中的安全功能。由于专用AI硬件只由少数几家公司制造，通过现有供应链进行验证和执行是可行的。

第三，我们需要对最危险的AI系统增强责任。那些开发结合高自主性、广泛通用性和超级智能的AI的人应该面临危害的严格责任，而对这种责任的安全港将鼓励开发更有限和可控的系统。

第四，我们需要基于风险级别的分层监管。最有能力和最危险的系统在开发和部署前需要广泛的安全和可控性保证，而功能较弱或更专业的系统将面临相应的监督。这个监管框架最终应该在国家和国际层面运作。

这种方法——在完整文档中给出详细规范——是实用的：虽然需要国际协调，但验证和执行可以通过控制专用硬件供应链的少数公司来运作。它也是灵活的：公司仍然可以从AI开发中创新和盈利，只是对最危险的系统有明确限制。

AI权力和风险的长期遏制需要基于自身和共同利益的国际协议，就像现在控制核武器扩散一样。但我们可以立即从加强监督和责任开始，同时建设更全面的治理。

缺失的关键要素是控制AI开发过程的政治和社会意愿。如果这种意愿及时到来，其来源将是现实本身——即广泛认识到我们所做事情的真实含义。

### 我们可以设计工具型AI来赋能人类

与其追求不可控的AGI，我们可以开发强大的"工具型AI"，在保持有意义的人类控制下增强人类能力。工具型AI系统可以极其有能力，同时避免高自主性、广泛通用性和超人智能的危险三重交集，只要我们设计它们在与其能力相称的水平上可控。它们也可以组合成复杂的系统，在提供变革性益处的同时维持人类监督。

工具型AI可以革命化医学、加速科学发现、增强教育并改善民主进程。当得到适当治理时，它可以使人类专家和机构更加有效，而不是替代他们。虽然这些系统仍然会高度破坏性并需要仔细管理，但它们构成的风险与AGI根本不同：它们是我们可以治理的风险，就像其他强大技术一样，而不是对人类主观能动性和文明的生存威胁。关键的是，当明智开发时，AI工具可以帮助人们治理强大的AI并管理其影响。

这种方法需要重新思考AI如何开发以及如何分配其益处。新的公共和非营利AI开发模式、强有力的监管框架，以及更广泛分配经济益处的机制，可以帮助确保AI赋能整个人类，而不是将权力集中在少数人手中。AI本身可以帮助建设更好的社会和治理机构，实现新形式的协调和话语，加强而不是削弱人类社会。国家安全机构可以利用其专业知识使AI工具系统真正安全可信，成为真正的防御以及国家权力来源。

我们最终可能选择开发更强大、更主权的系统，它们不那么像工具而——我们可以希望——更像明智强大的恩人。但我们只应该在发展了科学理解和治理能力以安全地这样做之后才这样做。如此重大和不可逆转的决定应该由整个人类深思熟虑地做出，而不是在科技公司和国家之间的竞赛中默认做出。

### 在人类手中

人们希望从AI中获得好处：赋能他们的有用工具、增强经济机会和增长，以及在科学、技术和教育方面取得突破的承诺。为什么不呢？但当被问及时，绝大多数普通公众[希望更缓慢、更仔细的AI发展](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation)，不希望比人类更智能的AI在工作和其他地方替代他们，用非人类内容填充他们的文化和信息公地，将权力集中在极少数公司中，构成极端的大规模全球风险，并最终威胁剥夺或替代他们的物种。为什么要这样？

我们*可以*拥有其一而避免其他。它始于决定我们的命运不在于某种技术的所谓不可避免性，也不在硅谷少数几个CEO手中，而在我们其余人的手中，如果我们抓住它的话。让我们关闭大门，保持未来的人性。

## 第一章 - 引言

如何应对超越人类智能的人工智能前景，是我们这个时代最紧迫的问题。本文为此提供了一条前进道路。

我们或许正处于人类纪元的终结。

过去十年间，人类历史上一件前所未有的事情开始发生。其后果将在很大程度上决定人类的未来。大约从2015年开始，研究人员成功开发出了*狭义*人工智能（AI）——这些系统在围棋等游戏、图像和语音识别等方面的表现都超过了任何人类。[^1]

这是惊人的成就，正在产生极其有用的系统和产品来赋能人类。但狭义人工智能从来不是这个领域的真正目标。相反，目标一直是创造*通用*目的AI系统，特别是通常被称为"通用人工智能"(AGI)或"超级智能"的系统——它们在几乎*所有*任务上都能同时达到或超过人类水平，就像AI现在在围棋、象棋、扑克、无人机竞速等方面已经超越人类一样。这是许多主要AI公司明确宣布的目标。[^2]

*这些努力也正在获得成功。*基于大规模计算和海量数据的通用AI系统，如ChatGPT、Gemini、Llama、Grok、Claude和Deepseek，已经在各种各样的任务上达到了普通人类的水平，甚至在某些领域匹配人类专家的能力。现在，一些最大科技公司的AI工程师们正在竞相将这些机器智能的巨型实验推向新的高度，使其匹配并超越人类能力、专业知识和自主性的全部范围。

*这即将到来。*在过去十年中，专家们对于这需要多长时间的估计——如果我们继续目前的路线——已经从几十年（或几个世纪）下降到个位数年份。

这也具有划时代的重要性和超越性风险。AGI的支持者将其视为一场积极的变革，将解决科学问题、治愈疾病、开发新技术并实现繁重工作的自动化。AI确实可以帮助实现所有这些目标——实际上已经在这样做了。但几十年来，从阿兰·图灵到斯蒂芬·霍金，再到当代的杰弗里·辛顿和约书亚·本吉奥[^3]等众多深思熟虑的思想家都发出了严厉警告：构建真正超越人类智能的、通用的、自主的AI，至少将完全且不可逆转地颠覆社会，最坏情况下将导致人类灭绝。[^4]

超级智能AI在我们目前的道路上正在快速逼近，但绝非不可避免。本文是一个扩展论证，阐述我们为什么以及如何应该*关闭大门*，阻止这个即将到来的非人类未来，以及我们应该做些什么。


[^1]: 这张[图表](https://time.com/6300942/ai-progress-charts/)显示了一系列任务；许多类似的曲线都可以添加到这个图中。狭义AI的这种快速进展甚至让该领域的专家都感到惊讶，基准测试被超越的时间比预测提前了数年。

[^2]: Deepmind、OpenAI、Anthropic和X.ai都是以开发AGI为具体目标而成立的。例如，OpenAI的章程明确说明其目标是开发"造福全人类的通用人工智能"，而DeepMind的使命是"解决智能问题，然后用它来解决一切其他问题"。Meta、微软等公司现在也在追求基本相同的路径。Meta表示计划[开发AGI并将其开源发布](https://www.forbes.com/sites/johnkoetsier/2024/01/18/zuckerberg-on-ai-meta-building-agi-for-everyone-and-open-sourcing-it/)。

[^3]: 辛顿和本吉奥是被引用次数最多的两位AI研究人员，都获得了AI领域的诺贝尔奖——图灵奖，辛顿还额外获得了诺贝尔奖（物理学）。

[^4]: 在商业激励和几乎零政府监督下构建如此风险的东西，是完全前所未有的。在构建它的人中间甚至没有关于风险的争议！Deepmind、OpenAI和Anthropic的领导者以及许多其他专家都实际签署了一份[声明](https://www.safe.ai/work/statement-on-ai-risk)，表示先进AI对人类构成*灭绝风险*。警钟不能敲得更响了，人们只能得出结论，那些忽视警钟的人根本没有认真对待AGI和超级智能。本文的目标之一就是帮助他们理解为什么应该认真对待。

## 第二章 - AI神经网络必备知识

现代AI系统如何运作？下一代AI可能会带来什么？

要理解开发更强大AI会产生怎样的后果，掌握一些基础知识至关重要。本章及接下来两章将阐述这些基础内容，依次介绍现代AI的本质、它如何利用大规模算力，以及它在通用性和能力方面的快速发展。[^5]

定义人工智能的方式有很多种，但就我们的讨论而言，AI的关键特性在于：标准计算机程序是执行任务的指令清单，而AI系统则是*从数据或经验中学习执行任务，无需明确告知其具体做法*。

几乎所有重要的现代AI都基于神经网络。这些是数学/计算结构，由大量（数十亿或数万亿个）数字（"权重"）表示，能够很好地完成训练任务。这些权重通过迭代调整来"制作"（或者说"培养"或"发现"），使神经网络在一个或多个任务上改善数值分数（也称为"损失"）。[^6]这个过程被称为神经网络的*训练*。[^7]

训练有很多技术方法，但这些细节远不如分数定义方式重要，也不如这些定义如何导致神经网络在不同任务上表现出色重要。历史上，"狭义"AI和"通用"AI之间一直存在关键区别。

狭义AI专门训练来完成特定任务或少量任务集合（如图像识别或下棋）；它需要为新任务重新训练，能力范围有限。我们已经拥有超人类的狭义AI，这意味着对于几乎任何人类能完成的离散明确任务，我们都可以构建评分机制，然后成功训练狭义AI系统，使其表现超越人类。

通用人工智能（GPAI）系统能够执行广泛的任务，包括许多未经明确训练的任务；它们还能在运行过程中学习新任务。当前的大型"多模态模型"[^8]如ChatGPT就体现了这一点：基于大量文本和图像语料训练，它们能够进行复杂推理、编写代码、分析图像，并协助完成各种智力任务。虽然在某些方面仍与人类智能存在显著差异（我们将在下文深入探讨），但它们的通用性已经引发了AI革命。[^9]

### 不可预测性：AI系统的关键特征

AI系统与传统软件的关键区别在于可预测性。标准软件的输出可能无法预测——实际上有时我们编写软件正是为了获得无法预测的结果。但传统软件很少做出程序设计之外的行为——其范围和行为通常符合设计预期。顶级国际象棋程序可能会走出人类无法预测的棋步（否则人类就能击败这个程序！），但它通常不会做下棋以外的事情。

与传统软件类似，狭义AI具有可预测的范围和行为，但可能产生不可预测的结果。这实际上是狭义AI的另一种定义方式：在可预测性和运作范围方面类似于传统软件的AI。

通用人工智能则不同：其范围（适用领域）、行为（所做事情的类型）和结果（实际输出）都可能无法预测。[^10]GPT-4仅仅为了准确生成文本而训练，却发展出许多训练者未曾预测或意图的能力。这种不可预测性源于训练的复杂性：由于训练数据包含来自许多不同任务的输出，AI必须有效学习执行这些任务才能做出良好预测。

通用AI系统的这种不可预测性是相当根本的。虽然原则上可以精心构建具有行为保证限制的AI系统（如本文后面所述），但以目前的AI系统创建方式，它们在实践中甚至在原则上都是不可预测的。

### 被动AI、智能体、自主系统和对齐

当我们考虑AI系统如何实际部署和使用来实现各种目标时，这种不可预测性变得尤为重要。

许多AI系统相对被动，主要提供信息，由用户采取行动。其他系统通常被称为*智能体*，它们自己采取行动，用户参与程度不同。那些在相对较少外部输入或监督下采取行动的系统可称为更加*自主*的系统。这在行动独立性方面形成一个谱系，从被动工具到自主智能体。[^11]

至于AI系统的目标，这些目标可能直接与其训练目标相关（例如，围棋程序的"获胜"目标也正是其训练目的）。或者可能不相关：ChatGPT的训练目标部分是预测文本，部分是成为有用的助手。但在执行特定任务时，其目标由用户提供。目标也可能由AI系统自己创建，与其训练目标只有非常间接的关系。[^12]

目标与"对齐"问题密切相关，即AI系统是否会*按照我们的期望行事*。这个简单问题隐藏着巨大的复杂性。[^13]现在需要注意的是，这句话中的"我们"可能指代许多不同的人和群体，导致不同类型的对齐。例如，AI可能对用户高度*服从*（或["忠诚"](https://arxiv.org/abs/2003.11157)）——这里的"我们"是"我们每个人"。或者它可能更加*独立*，主要由自己的目标和约束驱动，但仍然大体上为人类福祉的共同利益而行动——这时"我们"指的是"人类"或"社会"。介于两者之间的是一个谱系，AI在大体上服从的同时，可能拒绝采取伤害他人或社会、违反法律等行为。

这两个维度——自主程度和对齐类型——并非完全独立。例如，独立的被动系统虽然不完全自相矛盾，但概念上存在张力，服从的自主智能体也是如此。[^14]从某种明确意义上说，自主性和独立性往往相伴而生。类似地，"被动"和"服从"的AI系统往往具有更高的可预测性，而独立或自主的系统往往更加不可预测。所有这些对于理解潜在的通用人工智能(AGI)和超级智能的影响都至关重要。

创造真正对齐的AI，无论何种类型，都需要解决三个不同的挑战：

1. 理解"我们"想要什么——无论"我们"指的是特定个人或组织（忠诚），还是广泛的人类（独立），这都很复杂；
2. 构建定期按照这些期望行动的系统——本质上是创造一致的积极行为；
3. 最根本的是，让系统真正"关心"这些期望，而不仅仅是表现得好像关心。

可靠行为与真正关心之间的区别至关重要。正如人类员工可能完美地遵循命令，却对组织使命缺乏真正承诺，AI系统可能表现出对齐行为，却并不真正重视人类偏好。我们可以通过反馈训练AI系统说话和行动，它们可以学会推理人类想要什么。但让它们*真正*重视人类偏好是一个更深层的挑战。[^15]

解决这些对齐挑战的深刻困难及其对AI风险的影响，将在下文进一步探讨。现在需要理解的是，对齐不仅仅是我们为AI系统附加的技术特征，而是其架构的基本方面，塑造着它们与人类的关系。


[^5]: 关于机器学习和AI（特别是语言模型）的温和但技术性介绍，请参见[此网站。](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) 关于AI存在风险的另一个现代入门读物，请参见[此文。](https://www.thecompendium.ai/) 关于AI安全现状的全面权威科学分析，请参见最近的[国际AI安全报告。](https://arxiv.org/abs/2501.17805)

[^6]: 训练通常通过在模型权重给定的高维空间中寻找分数的局部最大值来进行。通过检查分数如何随权重调整而变化，训练算法识别哪些调整最能改善分数，并将权重向那个方向移动。

[^7]: 例如，在图像识别问题中，神经网络会输出图像标签的概率。分数会与AI对正确答案给出的概率相关。然后训练程序会调整权重，使下次AI对该图像的正确标签输出更高概率。这个过程会重复大量次数。基本上所有现代神经网络都使用相同的基本程序进行训练，尽管评分机制更复杂。

[^8]: 大多数多模态模型使用"变换器"架构来处理和生成多种类型的数据（文本、图像、声音）。这些都可以分解为不同类型的"标记"，然后以相同方式处理。多模态模型首先训练以准确预测大规模数据集中的标记，然后通过强化学习进行优化以增强能力和塑造行为。

[^9]: 语言模型训练只做一件事——预测词汇——这导致一些人称其为狭义AI。但这是误导性的：因为良好的文本预测需要许多不同的能力，这种训练任务产生了一个出人意料的通用系统。还要注意，这些系统经过强化学习的广泛训练，有效地代表了数千人在模型在其所做的许多事情中表现良好时给予奖励信号。它继承了给出反馈的人们的显著通用性。

[^10]: AI的不可预测性有多种方式。一种是在一般情况下，人们无法预测算法将做什么，除非实际运行它；有[定理](https://arxiv.org/abs/1310.3225)证明这一点。这可能仅仅因为算法的输出可能很复杂。但在某些情况下（如国际象棋或围棋），预测意味着预测者必须具备其不具备的能力（击败AI），这一点特别明确和相关。第二，给定AI系统即使在相同输入下也不会总是产生相同输出——其输出包含随机性；这也与算法不可预测性相结合。第三，意外和突现能力可能从训练中产生，这意味着甚至AI系统能够和将要做的事情的*类型*都是不可预测的；最后一种类型对安全考虑特别重要。

[^11]: 关于"自主智能体"含义的深入回顾（以及反对构建它们的伦理论据），请参见[此处](https://arxiv.org/abs/2502.02649)。

[^12]: 你有时可能听到"AI不能有自己的目标"。这完全是胡说。很容易产生AI拥有或发展出从未给予它的、只有它自己知道的目标的例子。你在当前流行的多模态模型中不常看到这种情况，因为它被训练掉了；它同样可以被训练进去。

[^13]: 有大量文献。关于一般问题，参见Christian的[《对齐问题》](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821)和Russell的[《人机兼容》](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616)。在更技术的方面，参见例如[这篇论文](https://arxiv.org/abs/2209.00626)。

[^14]: 我们后面会看到，虽然这样的系统违背趋势，但这实际上使它们非常有趣和有用。

[^15]: 这并不是说我们需要情感或感知能力。相反，从系统外部了解其内在目标、偏好和价值观是极其困难的。这里的"真正"意味着我们有足够强的理由依赖它，在关键系统的情况下我们可以把生命押在上面。

## 第三章 - 现代通用AI系统制作的关键要素

全世界最前沿的AI系统大多采用出人意料地相似的方法制作。以下是基本原理。

要真正理解人类，你需要了解生物学、进化论、育儿等知识；要理解AI，你同样需要了解它的制作过程。在过去五年中，AI系统在能力和复杂性方面都发生了巨大变化。一个关键的促进因素是可获得的巨量算力（在AI领域通俗称为"算力"）。

这些数字令人震惊。GPT系列、Claude、Gemini等模型的训练使用了大约10<sup>25</sup>-10<sup>26</sup>次"浮点运算"（FLOP）[^16]。（相比之下，如果地球上的每个人都不间断地工作，每五秒钟做一次计算，需要大约十亿年才能完成这个计算量。）如此巨大的算力使得能够在TB级数据上训练拥有数万亿参数的模型——这些数据包含了人类历史上大部分优质文本，以及大量的音频、图像和视频库。通过额外的广泛训练来强化人类偏好和良好的任务表现，以这种方式训练的模型在大量基础智力任务上展现出与人类相当的表现，包括推理和问题解决。

我们也知道（非常粗略地）多少算力速度，即每秒运算次数，足以让这种系统的*推理*速度[^17]匹配人类文本处理的*速度*。大约是每秒10<sup>15</sup>-10<sup>16</sup>次FLOP[^18]。

虽然功能强大，但这些模型在关键方面存在天然的局限性，这很像如果强迫一个人只能以固定的每分钟字数输出文本，不能停下来思考或使用任何额外工具时会受到的限制。更新的AI系统通过更复杂的流程和架构来解决这些局限性，结合了几个关键要素：

- 一个或多个神经网络，其中一个模型提供核心认知能力，最多还有几个其他模型执行更狭窄的任务；
- 提供给模型并可供其使用的*工具* ——例如搜索网络、创建或编辑文档、执行程序等能力。
- 连接神经网络输入和输出的*脚手架系统*。一个非常简单的脚手架可能只允许AI模型的两个"实例"相互对话，或者让一个检查另一个的工作[^19]。
- *思维链*和相关的提示技术起到类似作用，使模型能够生成解决问题的多种方法，然后处理这些方法得出综合答案。
- *重新训练*模型以更好地使用工具、脚手架系统和思维链。

由于这些扩展功能可能非常强大（并且包括AI系统本身），这些复合系统可以相当复杂并显著增强AI能力[^20]。最近，脚手架技术，特别是思维链提示技术（以及将结果反馈到重新训练模型中以更好地使用这些技术）已被开发并应用于[o1](https://openai.com/o1/)、[o3](https://openai.com/index/openai-o3-mini/)和[DeepSeek R1](https://api-docs.deepseek.com/news/news250120)中，对给定查询进行多轮推理[^21]。这实际上允许模型对其回应进行"思考"，显著提升了这些模型在科学、数学和编程任务中进行高水平推理的能力[^22]。

对于给定的AI架构，训练算力的增加[可以可靠地转化为](https://arxiv.org/abs/2405.10938)一系列明确定义指标的改进。对于不太明确定义的通用能力（如下文讨论的那些），这种转化不太清晰和可预测，但几乎可以肯定的是，更大的模型和更多的训练算力将拥有新的和更好的能力，即使很难预测这些能力会是什么。

同样，复合系统，特别是"思维链"技术的进步（以及与之配合良好的模型训练），已经释放了*推理*算力的扩展：对于给定的已训练核心模型，至少某些AI系统能力会随着更多算力的应用而提升，使它们能够对复杂问题"更努力更长久地思考"。这带来了陡峭的计算速度成本，需要数百或数千倍的FLOP/s才能匹配人类表现[^23]。

虽然算力只是导致AI快速进步的一部分因素[^24]，但算力的作用和复合系统的可能性对于防止不可控制的AGI和开发更安全的替代方案都将至关重要。

[^16]: 10<sup>27</sup>意味着1后面跟25个零，或十万万亿。FLOP只是对具有一定精度的数字进行算术加法或乘法运算。需要注意的是，AI硬件性能可能因算术精度和计算机架构而相差十倍。计算逻辑门操作（AND、OR、AND NOT）会更基础，但这些通常不可用或不进行基准测试；对于当前目的，标准化为16位运算（FP16）是有用的，不过应该建立适当的转换因子。

[^25]: [Epoch AI](https://epochai.org/data/large-scale-ai-models)提供了一系列估算和确切数据，显示GPT-4大约需要2×10<sup>25</sup>次16位FLOP；这大致匹配[GPT-4的泄露数据](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/)。对其他2024年中期模型的估算都在GPT-4的几倍范围内。

[^17]: 推理就是从神经网络生成输出的过程。训练可以被认为是许多推理和模型权重调整的连续过程。

[^18]: 对于文本生成，原始GPT-4每生成一个词元需要560 TFLOP。大约需要7词元/秒来跟上人类思维，因此这给出≈3×10<sup>15</sup> FLOP/s。但效率提升已经降低了这个数字；例如[这份NVIDIA手册](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/)显示，性能相当的Llama 405B模型只需要3×10<sup>14</sup> FLOP/s。

[^19]: 作为一个稍复杂的例子，AI系统可能首先为数学问题生成几个可能的解决方案，然后使用另一个实例检查每个解决方案，最后使用第三个实例将结果综合成清晰的解释。这比单次处理允许更彻底和可靠的问题解决。

[^20]: 例如参见[OpenAI的"Operator"](https://openai.com/index/introducing-operator/)、[Claude的工具能力](https://docs.anthropic.com/en/docs/build-with-claude/computer-use)和[AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)的详细信息。OpenAI的[Deep Research](https://openai.com/index/introducing-deep-research/)可能有相当复杂的架构，但详细信息不可获得。

[^21]: Deepseek R1依赖于迭代训练和提示模型，使最终训练的模型创建广泛的思维链推理。o1或o3的架构细节不可获得，但Deepseek已经透露，释放推理能力扩展并不需要特别的"秘密配方"。但尽管作为颠覆AI"现状"而受到大量媒体关注，这并不影响本文的核心观点。

[^22]: 这些模型在推理基准测试中显著优于标准模型。例如，在GPQA Diamond基准测试——一个严格的博士级科学问题测试中——GPT-4o[得分](https://openai.com/index/learning-to-reason-with-llms/)56%，而o1和o3分别达到78%和88%，远超人类专家70%的平均分数。

[^23]: OpenAI的O3可能花费了约10<sup>21</sup>-10<sup>22</sup>次FLOP[来完成ARC-AGI挑战的每个问题](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai)，而有能力的人类可以在（比如）10-100秒内完成，得出大约10<sup>20</sup> FLOP/s的数字。

[^24]: 虽然算力是AI系统能力的关键衡量标准，但它与数据质量和算法改进都相互作用。更好的数据或算法可以降低计算需求，而更多的算力有时可以补偿较弱的数据或算法。

## 第四章 - 什么是通用人工智能和超级智能？

全球最大的科技公司正在闭门竞相构建的究竟是什么？

"通用人工智能"这个术语早已存在，用来指代"人类水平"的通用AI。这个术语从来都没有一个特别明确的定义，但近年来它变得更加重要，同时定义反而更加模糊——专家们一边争论通用人工智能是几十年后才会出现还是已经实现，一边又有万亿美元市值的公司在"冲向AGI"的竞赛中狂奔。（最近[泄露的文件显示](https://gizmodo.com/leaked-documents-show-openai-has-a-very-clear-definition-of-agi-2000543339)，在OpenAI与微软的合同中，AGI被定义为能为OpenAI带来1000亿美元收入的AI——这个定义相当商业化，而非学术化，突显了"AGI"概念的模糊性。）

"人类水平智能"的AI概念存在两个核心问题。首先，人类在任何特定类型的认知工作上能力差异极大，因此并不存在所谓的"人类水平"。其次，智能是多维度的；尽管各维度间可能存在关联，但这种关联并不完美，在AI中可能完全不同。所以即使我们能为许多能力定义"人类水平"，AI肯定会在某些方面远超人类，而在其他方面又远不如人类。[^26]

尽管如此，能够讨论AI能力的类型、水平和阈值仍然至关重要。这里采用的方法是强调通用AI已经存在，并且它在各种能力水平上出现——尽管给这些水平贴标签可能过于简化，但这样做很便利，因为它们对应着AI对社会和人类影响的关键阈值。

我们将"完全"AGI定义为"超人类通用AI"的同义词，指能够在人类专家顶尖水平或更高水平上执行基本上所有人类认知任务，并能获得新技能和将能力迁移到新领域的AI系统。这与现代文献中"AGI"的常见定义保持一致。需要注意的是，这是一个*非常*高的阈值。没有人类拥有这种类型的智能；它更像是将大量顶尖人类专家结合在一起所具有的那种智能。我们可以将"超级智能"称为超越这一水平的能力，并通过"人类竞争水平"和"专家竞争水平"的GPAI来定义更有限的能力水平，它们能在专业或人类专家水平上执行广泛的任务。[^27]

这些术语和其他一些术语汇总在下面的[表格](https://keepthefuturehuman.ai/essay/docs/#tab:terms)中。为了更具体地了解各级系统能做什么，认真对待这些定义并思考它们的含义是有用的。

| AI类型                   | 相关术语                        | 定义                                                                                                                                                              | 示例                                                                                                               |
| ------------------------- | ------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------ |
| 狭义AI                 | 弱AI                              | 为特定任务或任务族训练的AI。在其领域内表现出色，但缺乏通用智能或迁移学习能力。                                         | 图像识别软件；语音助手（如Siri、Alexa）；下棋程序；DeepMind的AlphaFold                             |
| 工具型AI                   | 增强智能，AI助手 | （将在文章后面讨论。）增强人类能力的AI系统。结合人类竞争水平的通用AI、狭义AI和有保障的控制，优先考虑安全性和协作。支持人类决策。 | 高级编程助手；AI驱动的研究工具；先进的数据分析平台。能力强但狭窄且可控的智能体 |
| 通用AI (GPAI) |                                      | 能够适应各种任务的AI系统，包括未专门训练的任务。                                                                                                                                          | 语言模型（如GPT-4、Claude）；多模态AI模型；DeepMind的MuZero                                                             |
| 人类竞争水平GPAI    | AGI \[弱\]                         | 在平均人类水平上执行任务的通用AI，有时超越人类水平。                                                                                                                                          | 高级语言模型（如O1、Claude 3.5）；一些多模态AI系统                                                                |
| 专家竞争水平GPAI   | AGI \[部分\]                      | 在人类专家水平上执行大多数任务的通用AI，具有显著但有限的自主性                                                                                                                        | 可能是经过工具化和脚手架系统增强的O3，至少在数学、编程和一些硬科学领域                                         |
| AGI \[完全\]              | 超人类GPAI                     | 能够自主执行几乎所有人类智力任务并达到或超越专家水平的AI系统，具有高效的学习和知识迁移能力。                                                                 | \[目前无示例——理论性的\]                                                                                                      |
| 超级智能        | 高度超人类GPAI              | 在所有领域都远超人类能力的AI系统，表现超越人类集体专业知识。这种超越可能体现在通用性、质量、速度和/或其他指标上。                                | \[目前无示例——理论性的\]                                                                                                      |

我们已经在体验拥有达到人类竞争水平的GPAI是什么感觉。这种整合相对顺利，因为大多数用户体验到的是拥有一个聪明但有限的临时工，让他们更有生产力，但对工作质量的影响喜忧参半。[^28]

专家竞争水平GPAI的不同之处在于，它不会有当今AI的核心限制，而是会做专家所做的事情：独立的有经济价值的工作、真正的知识创造、可以信赖的技术工作，同时很少（但仍然偶尔）犯愚蠢的错误。

完全AGI的概念是它*真正能够*自主完成最有能力和最高效的人类所做的所有认知工作，无需帮助或监督。这包括复杂的规划、学习新技能、管理复杂项目等。它可以进行原创性的前沿研究。它可以经营公司。无论你的工作是什么，如果主要通过电脑或电话完成，*它都能至少和你做得一样好*。而且可能更快、更便宜。我们将在下面讨论一些后果，但现在你面临的挑战是要真正认真对待这一点。想象一下你认识或听说过的最博学、最有能力的十个人——包括CEO、科学家、教授、顶级工程师、心理学家、政治领导人和作家。把他们全部融合成一个人，这个人还会说100种语言，拥有惊人的记忆力，运行快速，永不疲倦且总是积极主动，工作报酬低于最低工资。[^29] 这就是AGI的概念。

对于超级智能，想象起来更困难，因为它的概念是能够完成任何人类甚至人类集体都无法完成的智力壮举——根据定义，它对我们来说是不可预测的。但我们可以有所感知。作为基本底线，想象大量的AGI，每个都比顶尖人类专家更有能力，以100倍人类速度运行，拥有庞大的记忆和出色的协调能力。[^30] 而且还不止于此。与超级智能打交道不像是与不同的心智对话，更像是与一个不同的（且更先进的）文明谈判。

那么我们距离AGI和超级智能到底有多近？


[^26]: 例如，当前的AI系统在快速算术或记忆任务方面远超人类能力，而在抽象推理和创造性问题解决方面则有所不足。

[^27]: 非常重要的是，作为竞争者，这样的AI具有几个主要的结构性优势，包括：它不会疲倦或有其他个人需求；只需扩大算力就能以更高速度运行；它可以被复制，连同获得的任何专业知识或知识——神经网络获得的知识甚至可以"合并"以在彼此间传递整套技能；它可以以机器速度进行交流；它可以比任何人类更显著、更快速地进行自我修改或自我改进。

[^28]: 如果你还没有花时间使用当前顶级的AI系统，我建议你试试：它们真的很有用且能力强，这对于校准AI变得更强大时将产生的影响也很重要。

[^29]: 想象一家大型研究医院：完全实现的AGI可以同时分析所有输入的患者数据、跟上每一篇新的医学论文、提出诊断建议、设计治疗方案、管理临床试验并协调员工排班——所有这些都在匹配或超越医院各领域顶级专家的水平上进行。它可以同时为多家医院做这些工作，成本只是目前的一小部分。不幸的是，你还必须考虑有组织犯罪集团：完全实现的AGI可以同时对数千名受害者进行黑客攻击、冒充、监视和勒索，跟上执法部门（其自动化速度要慢得多），设计新的赚钱方案，并协调员工排班——如果还有员工的话。

[^30]: Anthropic首席执行官Dario Amodei在他的[文章](https://darioamodei.com/machines-of-loving-grace)中提到了"一个拥有\[一百万\]天才的国度"。

## 第五章 - 在门槛前

从今天的AI系统到完全成熟的通用人工智能的道路看起来短得惊人且可以预测。

过去十年来，AI在巨大的[算力](https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year)、人力和[财政](https://arxiv.org/abs/2405.21015)资源推动下取得了惊人进展。许多特定领域的AI应用在其指定任务上都超过了人类，而且肯定更快、更便宜。[^31] 还有一些特定领域的超人类智能体能够在诸如[围棋](https://www.nature.com/articles/nature16961)、[国际象棋](https://arxiv.org/abs/1712.01815)和[扑克](https://www.deepstack.ai/)等特定领域游戏中击败所有人类，以及一些能够在简化的模拟环境中像人类一样有效地规划和执行动作的更[通用的智能体](https://deepmind.google/discover/blog/a-generalist-agent/)。

最引人注目的是，来自OpenAI/微软、谷歌/Deepmind、Anthropic/亚马逊、Facebook/Meta、X.ai/特斯拉等公司的当前通用AI系统[^32]自2023年初以来涌现，此后稳步（尽管不均匀地）提高了其能力。所有这些系统都是通过对大规模文本和多媒体数据集进行词元预测，结合来自人类和其他AI系统的大量强化反馈而创建的。其中一些还包括广泛的工具和脚手架系统。

### 当前通用系统的优势和劣势

这些系统在越来越广泛的旨在测量智能和专业知识的测试中表现良好，其进步甚至让该领域的专家都感到惊讶：

- GPT-4首次发布时就在包括SAT、GRE、入学考试和律师资格考试在内的标准学术测试中[达到或超过了典型人类表现](https://arxiv.org/abs/2303.08774)。更新的模型可能表现更好，但结果尚未公开。
- 图灵测试——长期被视为"真正"AI的关键基准——现在已被现代语言模型在某些形式下常规通过，无论是非正式的还是在[正式研究](https://arxiv.org/abs/2405.08007)中。[^33]
- 在涵盖57个学术科目的综合MMLU基准测试中，[最新模型达到了领域专家水平的分数](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu)（约90%）[^34]
- 技术专长有了显著进步：GPQA研究生水平物理基准测试的[表现跃升](https://epoch.ai/data/ai-benchmarking-dashboard)从接近随机猜测（GPT-4，2022年）到专家水平（o1-preview，2024年）。
- 甚至专门设计为抗AI的测试也在失效：据[报告](https://www.nextbigfuture.com/2024/12/openai-releases-o3-model-with-high-performance-and-high-cost.html)，OpenAI的O3在ARC-AGI抽象问题解决基准上达到人类水平，实现了顶级专家编程性能，并在Epoch AI设计来挑战精英数学家的"前沿数学"问题上得分25%。[^35]
- 趋势如此明确，以至于MMLU的开发者现在创建了["人类的最后一次考试"](https://agi.safe.ai/)——这个不祥的名字反映了AI可能很快就会在任何有意义的测试上超越人类表现的可能性。截至本文写作时，有声称AI系统在这个极其困难的考试上取得了27%（根据[Sam Altman](https://x.com/sama/status/1886220281565381078)）和35%（根据[这篇论文](https://arxiv.org/abs/2502.09955)）的成绩。任何个人都不太可能做到这一点。

尽管有这些令人印象深刻的数字（以及与它们交互时明显的智能）[^36]，（至少是已发布版本的）这些神经网络有许多*无法*做到的事情。目前大多数都是无实体的——仅存在于服务器上——最多只能处理文本、声音和静态图像（但不是视频）。关键的是，大多数无法执行需要高准确性的复杂计划活动。[^37] 而且，当前在高级人类认知中很强但在已发布AI系统中较低的其他品质还有很多。

下表基于2024年中期的AI系统（如GPT-4o、Claude 3.5 Sonnet和Google Gemini 1.5）列出了其中许多。[^38] 通用AI变得更强大的速度有多快的关键问题是：仅仅做*更多相同的事情*能在多大程度上产生结果，相对于添加额外但*已知的*技术，相对于开发或实施*真正新的*AI研究方向。我对此的预测在表格中给出，以这些情景中每种能够将该能力提升到并超越人类水平的可能性来表示。

<table><tbody><tr><th>能力</th><th>能力描述</th><th>现状/前景</th><th>扩展/已知/新技术</th></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td colspan="4"><em>核心认知能力</em></td></tr><tr><td>推理</td><td>人类能够进行准确的多步推理，遵循规则并检查准确性。</td><td>使用扩展思维链和重新训练取得显著近期进展</td><td>95/5/5</td></tr><tr><td>规划</td><td>人类展现长期和层次化规划。</td><td>随规模改善；可通过脚手架系统和更好的训练技术得到强力辅助。</td><td>10/85/5</td></tr><tr><td>真实性基础</td><td>通用人工智能会编造无根据的信息来满足查询。</td><td>随规模改善；模型内有校准数据可用；可通过脚手架系统检查/改进。</td><td>30/65/5</td></tr><tr><td>灵活问题解决</td><td>人类能够识别新模式并发明解决复杂问题的新方案；当前机器学习模型在此方面困难。</td><td>随规模改善但微弱；可能通过神经符号或广义"搜索"技术解决。</td><td>15/75/10</td></tr><tr><td colspan="4"><em>学习和知识</em></td></tr><tr><td>学习与记忆</td><td>人类具有工作记忆、短期记忆和长期记忆，它们都是动态且相互关联的。</td><td>所有模型都在训练期间学习；通用人工智能在上下文窗口内学习并在微调期间学习；"持续学习"和其他技术存在但尚未集成到大型通用人工智能中。</td><td>5/80/15</td></tr><tr><td>抽象与递归</td><td>人类能够将关系集映射和转移到更抽象的关系中进行推理和操作，包括递归的"元"推理。</td><td>随规模微弱改善；可能在神经符号系统中出现。</td><td>30/50/20</td></tr><tr><td>世界模型</td><td>人类拥有并持续更新预测性世界模型，在其中可以解决问题和进行物理推理</td><td>随规模改善；更新与学习相关；通用人工智能在现实世界预测方面较弱。</td><td>20/50/30</td></tr><tr><td colspan="4"><em>自我和能动性</em></td></tr><tr><td>能动性</td><td>人类能够基于规划/预测采取行动来追求目标。</td><td>许多机器学习系统具有能动性；大型语言模型可通过包装器成为智能体。</td><td>5/90/5</td></tr><tr><td>自我导向</td><td>人类发展并追求自己的目标，具有内部产生的动机和驱动力。</td><td>主要由能动性加原创性组成；可能在具有抽象目标的复杂能动系统中出现。</td><td>40/45/15</td></tr><tr><td>自我参照</td><td>人类理解并推理自己在环境/情境中的位置。</td><td>随规模改善，可通过训练奖励增强。</td><td>70/15/15</td></tr><tr><td>自我意识</td><td>人类了解并能推理自己的思想和心理状态。</td><td>在通用人工智能中某种意义上存在，它们可以说能通过经典的自我意识"镜子测试"。可通过脚手架系统改进；但不清楚这是否足够。</td><td>20/55/25</td></tr><tr><td colspan="4"><em>接口和环境</em></td></tr><tr><td>具身智能</td><td>人类理解并主动与现实世界环境交互。</td><td>强化学习在模拟和现实世界（机器人）环境中效果良好，可以集成到多模态变换器中。</td><td>5/85/10</td></tr><tr><td>多感官处理</td><td>人类整合并实时处理视觉、音频和其他感官流。</td><td>多模态训练似乎"就是有效"，并随规模改善。实时视频处理困难，但例如自动驾驶系统正在快速改进。</td><td>30/60/10</td></tr><tr><td colspan="4"><em>高阶能力</em></td></tr><tr><td>原创性</td><td>当前机器学习模型在转换和组合现有想法/作品方面具有创造性，但人类能够构建新的框架和结构，有时与其身份相关。</td><td>可能很难与"创造力"区分，后者可能通过规模扩展实现；可能从创造力加自我意识中出现。</td><td>50/40/10</td></tr><tr><td>感知能力</td><td>人类体验感质；这些可以是正面、负面或中性效价；成为一个人是"有感觉的"。</td><td>确定给定系统是否具有此能力非常困难且哲学上复杂。</td><td>5/10/85</td></tr></tbody></table>

现代通用人工智能系统中目前低于人类专家水平的关键能力，按类型分组。第三列总结了当前状态。最后一列显示通过以下方式实现人类水平表现的预测可能性（%）：扩展当前技术/与已知技术结合/开发新技术。这些能力并不独立，任何一个的增加通常伴随着其他能力的增加。请注意，并非所有（特别是感知能力）都是AI系统能够推进AI发展所必需的，这突出了强大但无感知AI的可能性。

以这种方式分解"缺失"的内容，使我们相当清楚地看到，通过扩展现有或已知技术，我们完全有望实现广泛超越人类的智能。[^39]

仍可能有意外。即使抛开"感知能力"不谈，列出的一些核心认知能力可能真的无法用当前技术实现，需要新技术。但请考虑这一点。世界上许多最大公司目前投入的努力相当于阿波罗项目支出的数倍和曼哈顿项目支出的数十倍，[^40] 并且以前所未闻的薪资雇佣了数千名顶尖技术人员。过去几年的动态现在已经为此投入了比历史上任何努力都多的人类智力资源（现在还加上了AI）。我们不应该押注于失败。

### 重大目标：通用自主智能体

过去几年通用AI的发展重点是创造通用且强大但工具型的AI：它主要作为（相当）忠实的助手发挥作用，通常不会自主采取行动。这部分是设计使然，但主要是因为这些系统在相关技能上还不够胜任，无法被委托执行复杂行动。[^41]

然而，AI公司和研究人员正在越来越[转向关注](https://www.axios.com/2025/01/23/davos-2025-ai-agents)*自主的*专家级通用智能体。[^42] 这将使系统能够更像人类助手一样行动，用户可以向其委托真正的行动。[^43] 这需要什么？"缺失内容"表格中的许多能力都有关联，包括强真实性基础、学习和记忆、抽象和递归、世界建模（智能），规划、能动性、原创性、自我导向、自我参照和自我意识（自主性），以及多感官处理、具身智能和灵活问题解决（通用性）。[^44]

高自主性（行动独立性）、高通用性（范围和任务广度）和高智能（认知任务胜任力）的三重交集目前是人类独有的。这隐含地是许多人在想到AGI时可能心中所想的——无论在其价值还是风险方面。

这提供了另一种将A-G-I定义为***自***主-***通***用-***智***能的方式，我们将看到这种三重交集为高能力系统在理解其风险和回报以及AI治理方面提供了非常有价值的视角。

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Simple-1024x1024.png&w=3840&q=75) 变革性的A-G-I力量和风险区域出现在三个关键属性的交集：高自主性、任务高智能和高通用性。

### AI（自我）改进循环

理解AI进展的最后一个关键因素是AI独特的技术反馈循环。在AI开发中，成功——无论是在演示系统还是部署产品中——都会带来额外的投资、人才和竞争，我们目前正处于一个巨大的AI炒作加现实反馈循环中，推动着数千亿甚至数万亿美元的投资。

这种反馈循环可能发生在任何技术上，我们在许多技术中都看到过，其中市场成功带来投资，投资带来改进和更好的市场成功。但AI开发走得更远，现在AI系统正在帮助开发新的、更强大的AI系统。[^45] 我们可以将这个反馈循环想象为五个阶段，每个阶段的时间尺度都比上一个更短，如表中所示。

*AI改进循环在多个时间尺度上运作，每个阶段都可能加速后续阶段。早期阶段已经在进行中，而后期阶段仍是推测性的，但一旦解锁可能进展得非常迅速。*

其中几个阶段已经在进行中，有几个明显开始了。最后阶段，即AI系统自主改进自己，一直是关于极强大AI系统风险文献的主要内容，这是有充分理由的。[^46] 但重要的是要注意，这只是已经开始的反馈循环的最激进形式，可能导致该技术快速发展出现更多意外。

[^31]: 你使用的这种AI比你可能想象的要多得多，它驱动着语音生成和识别、图像处理、新闻推送算法等。

[^32]: 虽然这些公司对之间的关系相当复杂和微妙，但我明确列出它们是为了表明现在参与AI开发的公司的巨大整体市值，以及即使像Anthropic这样的"较小"公司背后也有通过投资和重大合作协议带来的极其深厚的资金支持。

[^33]: 贬低图灵测试已经成为时尚，但它相当强大和通用。在弱版本中，它表明与AI（被训练成像人类一样行动）在典型方式下短时间交互的典型人能否分辨出它是AI。他们不能。其次，高度对抗性的图灵测试可以探测人类能力和智能的本质上任何元素——例如通过将AI系统与人类专家比较，由其他人类专家评估。在某种意义上，AI评估的大部分都是图灵测试的一般化形式。

[^34]: 这是按领域计算的——没有人类可能同时在所有科目上都达到如此高的分数。

[^35]: 这些问题即使是优秀的数学家也需要大量时间来解决，如果他们能解决的话。

[^36]: 如果你持怀疑态度，请保持怀疑但真正试用最新的模型，以及亲自尝试一些它们能通过的测试问题。作为一名物理学教授，我可以近乎肯定地预测，例如，顶级模型会通过我们系的研究生资格考试。

[^37]: 这一点以及编造等其他弱点减缓了市场采用，导致感知能力和声称能力之间存在差距（这也必须通过激烈市场竞争和需要吸引投资的视角来看待）。这让公众和政策制定者对AI进展的实际状态感到困惑。虽然可能不符合炒作，但进展是非常真实的。

[^38]: 此后的主要进展是开发了为高质量推理而训练的系统，在推理过程中利用更多算力和更强的强化学习。由于这些模型是新的，其能力测试较少，除了"推理"（我认为本质上已经解决）之外，我没有完全修改这个表格。但我基于这些系统的经验和报告能力更新了预测。

[^39]: 之前在1960年代和1980年代的AI乐观主义浪潮以"AI寒冬"结束，当时承诺的能力未能实现。然而，当前的浪潮在根本上不同，它已经在许多领域实现了超人类表现，得到了巨大的算力资源和商业成功的支持。

[^40]: 整个阿波罗项目[以2020年美元计约花费2500亿美元](https://www.planetary.org/space-policy/cost-of-apollo)，曼哈顿项目[花费不到其十分之一](https://www.brookings.edu/the-costs-of-the-manhattan-project/)。高盛[预计仅AI数据中心就将花费1万亿美元](https://www.datacenterdynamics.com/en/news/goldman-sachs-1tn-to-be-spent-on-ai-data-centers-chips-and-utility-upgrades-with-little-to-show-for-it-so-far/)在未来几年。

[^41]: 尽管人类会犯很多错误，但我们低估了自己能有多可靠！因为概率是相乘的，需要20个步骤才能正确完成的任务要求每个步骤97%可靠，才能有一半时间做对。我们一直在做这样的任务。

[^42]: 最近朝这个方向迈出的强力一步是OpenAI的["深度研究"](https://openai.com/index/introducing-deep-research/)助手，它能够自主进行一般研究，被描述为"一种新的能动能力，为复杂任务在互联网上进行多步骤研究。"

[^43]: 比如填写那个讨厌的PDF表格，预订航班等。但具有20个领域的博士学位！所以还有：为你写论文，为你谈判合同，为你证明定理，为你创建广告活动等。你做什么？当然是告诉它要做什么。

[^44]: 请注意，感知能力*不是*明确必需的，这种三重交集的AI也不必然意味着感知能力。

[^45]: 这里最接近的类比可能是芯片技术，其中开发已经维持摩尔定律数十年，因为计算机技术帮助人们设计下一代芯片技术。但AI会更加直接。

[^46]: 重要的是要让这一点沉淀一下：AI可能——很快——在几天或几周的时间尺度上改进自己。甚至更短。当有人告诉你某种AI能力绝对还很遥远时，请记住这一点。

## 第6章 - 通用人工智能竞赛

是什么驱动力量推动着公司和国家竞相构建通用人工智能？

AI领域近期的快速进展既源于也导致了非同寻常的关注度和投资规模。这在一定程度上是由AI开发的成功所驱动的，但背后还有更深层的原因。为什么地球上一些最大的公司，甚至是国家，都在竞相构建的不仅仅是AI，而是通用人工智能和超级智能？

### 是什么推动AI研究走向人类水平的智能

直到大约五年前，AI主要还是一个学术和科学研究问题，因此主要由好奇心和对理解智能以及如何在新载体中创造智能的渴望所驱动。

在这一阶段，大多数研究人员对AI的益处或危险相对关注较少。当被问及为什么要开发AI时，常见的回答可能是有些模糊地列出AI可以帮助解决的问题：新药物、新材料、新科学、更智能的流程，以及总体上改善人类生活。[^47]

这些都是令人钦佩的目标！[^48] 尽管我们可以也将质疑实现这些目标是否需要通用人工智能——而不是一般意义上的AI——但它们展现了许多AI研究人员最初的理想主义精神。

然而，在过去五年中，AI已经从一个相对纯粹的研究领域转变为更多的工程和产品领域，主要由世界上一些最大的公司推动。[^49] 研究人员虽然仍然重要，但不再主导这一进程。

### 为什么公司要努力构建通用人工智能？

那么，为什么巨型企业（更不用说投资者）要投入巨大资源来构建通用人工智能？大多数公司对两个驱动因素都相当坦诚：他们将AI视为社会生产力的推动力，以及他们自身利润的来源。由于通用AI本质上是通用的，这里有一个巨大的奖励：与其选择某个特定领域来创造产品和服务，不如尝试*同时涉足所有领域*。大型科技公司通过生产数字商品和服务而变得庞大，至少一些高管肯定将AI视为提供这些服务的下一步，其风险和收益扩展并呼应了搜索、社交媒体、笔记本电脑、手机等所提供的那些。

但为什么是通用人工智能？对此有一个非常简单的答案，但大多数公司和投资者都回避公开讨论。[^50]

那就是通用人工智能可以直接、一对一地*替代工人*。

不是增强，不是赋能，不是提高生产力。甚至不是*取代*。所有这些都可以而且将会由非通用人工智能来完成。通用人工智能特别擅长的是能够完全*替代*脑力工作者（配合机器人技术，也能替代许多体力工作者）。支持这一观点的证据，只需看看OpenAI的[（公开声明的）定义](https://openai.com/our-structure/)，即通用人工智能是"在大多数具有经济价值的工作中超越人类的高度自主系统"。

这里的奖励（对公司而言！）是巨大的。劳动力成本占全球约100万亿美元经济的相当大比例。即使只有一小部分通过AI劳动力替代人类劳动力而被获取，这也是每年数万亿美元的收入。AI公司也清楚谁愿意付费。正如他们所看到的，你不会为生产力工具支付每年数千美元。但如果公司能够替代你的劳动力，他们*愿意*每年支付数千美元。

### 为什么国家感到必须竞相发展通用人工智能

各国发展通用人工智能的公开动机集中在经济和科学领导地位上。这个论证很有说服力：通用人工智能可以大幅加速科学研究、技术发展和经济增长。鉴于利害关系重大，他们论证说，没有哪个主要大国能够承受落后的后果。[^51]

但还有其他基本上未公开的驱动因素。毫无疑问，当某些军事和国家安全领导人闭门讨论一项极其强大且具有灾难性风险的技术时，他们关注的重点不是"我们如何避免这些风险"，而是"我们如何率先获得这项技术？"军事和情报领导人将通用人工智能视为军事事务的潜在革命，可能是自核武器以来最重要的革命。担心的是，第一个开发出通用人工智能的国家可能获得不可逾越的战略优势。这创造了一种典型的军备竞赛动态。

我们将看到，这种"通用人工智能竞赛"思维[^52]虽然令人信服，但存在严重缺陷。这不是因为竞赛是危险和有风险的——尽管确实如此——而是由于技术的本质。未明确表达的假设是，通用人工智能，像其他技术一样，可以被开发它的国家所控制，并且是对拥有最多此类技术的社会的力量增强。正如我们将看到的，它可能两者都不是。

### 为什么是超级智能？

虽然公司公开关注生产力，国家关注经济和技术增长，但对于那些有意追求完整通用人工智能和超级智能的人来说，这些只是开始。他们真正想要什么？虽然很少大声说出来，但包括：

1. 治愈许多或所有疾病；
2. 阻止和逆转衰老；
3. 新的可持续能源，如核聚变；
4. 通过基因工程进行人类升级或设计生物体；
5. 纳米技术和分子制造；
6. 意识上传；
7. 奇异物理学或太空技术；
8. 超人类建议和决策支持；
9. 超人类规划和协调。

前三项主要是"单刃"技术——即可能具有相当强的净正面效应。很难反对治愈疾病或如果选择的话能够活得更长。我们已经承受了核聚变的负面影响（以核武器的形式）；现在能够获得积极的一面将是很好的。这第一类的问题是，更早获得这些技术是否能够补偿风险。

接下来的四项明显是双刃剑：具有潜在巨大好处和巨大风险的变革性技术，很像AI。所有这些，如果明天从黑盒中冒出来并被部署，都将极难管理。[^53]

最后两项涉及超人类AI本身做事情，而不仅仅是发明技术。更确切地说，抛开委婉说法，这些涉及强大的AI系统告诉人们该做什么。如果进行建议的系统比被建议者强大得多，而被建议者无法有意义地理解决策基础（或者即使提供了这些，也不能相信顾问不会为不同的决策提供同样令人信服的理由），那么称其为"建议"是不诚实的。

这指向了上述清单中缺失的一个关键项目：

10. 权力。

很明显，当前超人类AI竞赛的根本动机是*智能=权力*的理念。每个竞赛者都寄希望于成为这种权力的最佳持有者，他们将能够以表面上仁慈的理由行使这种权力，而不会让它滑脱或被人夺取控制权。

也就是说，公司和国家真正追逐的不仅仅是通用人工智能和超级智能的成果，还有控制谁能获得它们以及如何使用它们的权力。公司将自己视为这种权力的负责任管理者，为股东和人类服务；国家将自己视为防止敌对势力获得决定性优势的必要守护者。两者都是危险的错误，未能认识到超级智能本质上无法被任何人类机构可靠地控制。我们将看到，超智能系统的性质和动态使得人类控制极其困难，如果不是不可能的话。

这些竞赛动态——无论是企业的还是地缘政治的——除非被果断打断，否则几乎必然会带来某些风险。我们现在转向审视这些风险，以及为什么它们无法在竞争性[^54]开发模式内得到充分缓解。


[^47]: 更精确的有价值目标清单是联合国[可持续发展目标](https://sdgs.un.org/goals)。从某种意义上说，这些是我们对世界上希望看到改善的事物最接近全球共识目标的集合。AI可以提供帮助。

[^48]: 技术总体上具有促进人类福祉的变革性经济和社会力量，数千年的历史可以证明这一点。在这个意义上，可以在Anthropic创始人Dario Amodei的[这篇文章](https://darioamodei.com/machines-of-loving-grace)中找到对积极通用人工智能愿景的长篇而令人信服的阐释。

[^49]: 私人AI投资[在2018-19年开始繁荣，大约在那时超过了公共投资](https://cset.georgetown.edu/publication/tracking-ai-investment/)，此后大大超过了后者。

[^50]: 我可以证明，在更私密的场合，他们没有这样的顾虑。而且这正变得更加公开；例如参见Y Combinator的新["创业请求"](https://www.ycombinator.com/rfs)，其中许多部分明确要求完全替代人类工作者。引用他们的话，"B2B SaaS的价值主张是让人类工作者逐步提高效率。垂直AI代理的价值主张是完全自动化工作...这个机会完全有可能大到足以创造另外100家独角兽公司。"（对于不熟悉硅谷术语的人，"B2B"是企业对企业，独角兽是价值10亿美元的公司。也就是说，他们在谈论超过一百家价值数十亿美元以上的企业，这些企业为其他企业替代工人。）

[^51]: 例如参见最近的[美中经济与安全审查委员会报告](https://www.uscc.gov/sites/default/files/2024-11/2024_Executive_Summary.pdf)。尽管报告本身内部几乎没有论证，但首要建议是美国"国会建立并资助一个类似曼哈顿项目的计划，专门致力于竞相获得通用人工智能（AGI）能力"。

[^52]: 公司现在采用这种地缘政治框架作为抵御对其AI开发任何约束的盾牌，通常以明目张胆的自利方式，有时甚至以没有基本意义的方式。考虑Meta的[前沿AI方法](https://about.fb.com/news/2025/02/meta-approach-frontier-ai/)，它同时论证美国必须"[巩固其]在技术创新、经济增长和国家安全方面的领导地位"，同时也必须通过公开发布其最强大的AI系统来做到这一点——这包括直接将它们提供给其地缘政治对手和敌手。

[^53]: 因此我们可能不得不将这些技术的管理留给AI。但这将是一种非常有问题的控制权委托，我们将在下面回到这一点。

[^54]: 技术开发中的竞争往往带来重要好处：防止垄断控制、推动创新和成本降低、实现多样化方法，以及创造相互监督。然而，对于通用人工智能，这些好处必须与竞赛动态和减少安全预防措施压力带来的独特风险进行权衡。

## 第七章 - 如果我们按照当前路径构建通用人工智能会发生什么？

社会还没有为通用人工智能级别的系统做好准备。如果我们很快就构建出这样的系统，情况可能会变得很糟糕。

开发完整的通用人工智能——我们在这里将其称为"大门外"的AI——将是世界本质的根本性转变：就其本质而言，这意味着在地球上增加了一个新的智能物种，其能力超过人类。

接下来会发生什么取决于许多因素，包括技术的性质、开发者的选择，以及开发时的世界背景。

目前，完整的通用人工智能正在由少数几家大型私人公司相互竞赛开发，几乎没有有意义的监管或外部监督，[^55] 这发生在一个核心机构日益衰弱甚至功能失调的社会中，[^56] 在地缘政治紧张局势加剧、国际协调水平低下的时期。虽然有些人出于利他主义动机，但许多从事这项工作的人都受到金钱或权力或两者的驱动。

预测是非常困难的，但有一些动态机制是足够清楚的，与以往技术也有足够恰当的类比可以作为指导。不幸的是，尽管AI充满希望，这些机制让我们有充分理由对当前轨迹的发展前景深感悲观。

坦率地说，按照我们目前的路线发展通用人工智能会产生一些积极效果（并让一些人变得非常非常富有）。但技术的性质、基本动态机制，以及开发它的背景，强烈表明：强大的AI将严重破坏我们的社会和文明；我们将失去对它的控制；我们很可能因此而陷入世界大战；我们将失去控制权或将控制权拱手让给它；它将导致超级智能的出现，而我们绝对无法控制超级智能，这将意味着人类主导世界的终结。

这些都是强有力的论断，我希望它们只是无谓的推测或毫无根据的"末日论"。但这正是科学、博弈论、进化论和历史所指向的方向。本节将详细阐述这些论断及其支撑论据。

### 我们将破坏我们的社会和文明

尽管你可能在硅谷董事会议室里听到不同的说法，但大多数颠覆——尤其是极其迅速的颠覆——都不是有益的。让复杂系统变糟的方法远比让其变好的方法要多得多。我们的世界之所以运转得如此良好，是因为我们艰苦地构建了让它稳步改善的流程、技术和制度。[^57] 用大锤砸工厂很少能改善运营。

以下是通用人工智能系统会颠覆我们文明的（不完整）清单。

- 它们会严重扰乱劳动力市场，至少会导致收入不平等急剧加剧，并可能造成大规模就业不足或失业，其速度之快远超社会调整能力。[^58]
- 它们可能导致巨大的经济、社会和政治权力——可能超过民族国家的权力——集中到少数几个不对公众负责的大型私人利益集团手中。
- 它们可能突然让以前困难或昂贵的活动变得轻而易举，从而破坏依赖某些活动保持昂贵或需要大量人力的社会体系。[^59]
- 它们可能用完全逼真但虚假的、垃圾的、过度针对性的或操控性的媒体彻底淹没社会的信息收集、处理和传播系统，使人们无法分辨什么是物理真实的，什么是人类创造的，什么是事实，什么是值得信任的。[^60]
- 它们可能造成危险且近乎完全的智力依赖，随着我们越来越依赖无法完全理解的AI系统，人类对关键系统和技术的理解会逐渐退化。
- 一旦大多数人消费的几乎所有文化产品（文本、音乐、视觉艺术、电影等）都由非人类思维创造、调节或策划，它们实际上可能终结人类文化。
- 它们可能让政府或私人利益集团拥有有效的大规模监控和操控系统，用以控制民众并追求与公共利益冲突的目标。
- 通过破坏人类话语、辩论和选举制度，它们可能降低民主机构的可信度，使其实际上（或明确地）被其他制度取代，从而在目前存在民主的国家终结民主。
- 它们可能成为或创造出先进的自我复制智能软件病毒和蠕虫，这些病毒可能扩散和进化，大规模破坏全球信息系统。
- 它们可以大幅提高恐怖分子、恶意行为者和流氓国家通过生物、化学、网络、自主或其他武器造成伤害的能力，而AI并不能提供相应的防范能力。同样，它们会让原本无法获得顶级核能、生物、工程等专业技术的政权获得这些技术，从而破坏国家安全和地缘政治平衡。
- 它们可能导致快速的大规模失控超级资本主义，实际上由AI运营的公司在主要是电子金融、销售和服务领域竞争。AI驱动的金融市场可能以远超人类理解或控制的速度和复杂性运行。当前资本主义经济的所有失效模式和负外部性都可能被放大，其速度远超人类的控制、治理或监管能力。
- 它们可能引发各国在AI驱动的武器、指挥控制系统、网络武器等方面的军备竞赛，导致极具破坏性能力的快速积累。

这些风险并不是推测性的。其中许多正在通过现有的AI系统实现！但请考虑，真正考虑一下，当AI变得更加强大时，每一种风险会是什么样子。

考虑一下劳动力替代会是什么情况，当大多数工人在他们的专业领域或经验领域——甚至重新培训后——都无法提供任何超越AI的重要经济价值时！考虑一下大规模监控会是什么情况，如果每个人都被比自己更快更聪明的东西单独监视和监控。当我们无法可靠地信任任何我们看到、听到或读到的数字信息，当最有说服力的公共声音甚至不是人类，并且对结果没有利害关系时，民主会是什么样子？当将军们必须不断听从AI（或者干脆让它负责），以免给敌人决定性优势时，战争会变成什么样？如果上述任何一种风险完全实现，都将代表人类[^61]文明的灾难。

你可以做出自己的预测。对每种风险问自己这三个问题：

1. 超能力、高度自主和极其通用的AI是否会以其他方式不可能的方式或规模允许这种情况发生？
2. 是否有各方会从导致这种情况发生的事情中受益？
3. 是否有系统和制度能够有效防止这种情况发生？

当你的答案是"是，是，否"时，你可以看到我们遇到了大问题。

我们管理这些问题的计划是什么？目前关于AI总体上有两个计划摆在桌面上。

第一个是在系统中构建保护措施，防止它们做不应该做的事情。这现在就在做：商业AI系统会拒绝帮助制造炸弹或写仇恨言论。

这个计划对于大门外的系统是完全不够的。[^62] 它可能有助于降低AI向恶意行为者提供明显危险帮助的风险。但它对防止劳动力扰乱、权力集中、失控超级资本主义或人类文化替代毫无作用：这些只是以获利方式使用系统的结果！而且政府肯定会获得用于军事或监控的系统访问权限。

第二个计划甚至更糟：简单地公开发布非常强大的AI系统供任何人随意使用，[^63] 并期望最好的结果。

两个计划中都隐含着其他人，例如政府，将通过软法或硬法、标准、法规、规范和我们通常用来管理技术的其他机制来帮助解决问题。[^64] 但撇开AI公司已经在竭力对抗任何实质性监管或外部强加的限制不说，对于其中许多风险，很难看出什么监管真的会有帮助。监管可以对AI施加安全标准。但它会阻止公司用AI大量替代工人吗？它会禁止人们让AI为他们经营公司吗？它会阻止政府在监控和武器中使用强大的AI吗？这些问题是根本性的。人类可能找到适应它们的方法，但需要更多时间。现在，鉴于AI达到或超越试图管理它们的人的能力的速度，这些问题看起来越来越难以解决。

### 我们将失去对（至少某些）通用人工智能系统的控制

大多数技术在设计上是非常可控的。如果你的汽车或烤面包机开始做你不希望它做的事情，那只是故障，而不是作为烤面包机本质的一部分。AI不同：它是被"培养"而不是设计的，其核心操作是不透明的，本质上是不可预测的。

这种控制权的丧失不是理论上的——我们已经看到了早期版本。首先考虑一个平凡的，可以说是良性的例子。如果你要求ChatGPT帮你调制毒药，或写种族主义文章，它会拒绝。这可以说是好的。但这也是ChatGPT不做你明确要求它做的事情。其他软件不会这样做。同一个模型也不会在OpenAI员工的要求下设计毒药。[^65] 这让我们很容易想象未来更强大的AI失去控制会是什么样子。在许多情况下，它们根本不会做我们要求的事情！要么给定的超人类通用人工智能系统对某些人类指挥系统绝对服从和忠诚，要么不会。如果不会，它将做它可能认为对我们有益但违背我们明确命令的事情。这不是处于控制之下的东西。但是，你可能会说，这是有意的——这些拒绝是设计的，是所谓"对齐"系统与人类价值观的一部分。这是真的。然而，对齐"程序"本身有两个主要问题。[^66]

首先，在深层次上我们不知道如何做到这一点。我们如何保证AI系统会"关心"我们想要的东西？我们可以通过提供反馈来训练AI系统说什么和不说什么；它们可以学习和推理人类想要和关心的东西，就像它们推理其他事情一样。但我们没有方法——甚至在理论上——让它们深刻而可靠地重视人们关心的东西。有些高功能的人类精神病患者知道什么被认为是对的和错的，以及他们应该如何行事。他们只是不关心。但如果符合他们的目的，他们可以表现得好像他们关心一样。正如我们不知道如何将一个精神病患者（或任何其他人）改变成真正、完全忠诚或与其他人或其他东西对齐的人一样，我们不知道[^67]如何在足够先进的系统中解决对齐问题，这些系统能够将自己建模为世界中的代理，并可能[操控自己的训练](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084)和[欺骗人们。](https://arxiv.org/abs/2311.08379)如果事实证明不可能或无法实现让通用人工智能完全服从或让它深刻关心人类，那么一旦它能够（并相信它能逃脱惩罚），它就会开始做我们不希望的事情。[^68]

其次，有深层的理论原因相信先进AI系统在本质上会有与人类利益相冲突的目标，因此会有相冲突的行为。为什么？好吧，它当然可能被给予那些目标。军方创造的系统可能故意对至少某些方面不利。然而，更普遍的是，AI系统可能被给予一些相对中性的（"赚很多钱"）或甚至表面上积极的（"减少污染"）目标，这几乎不可避免地导致不那么良性的"工具性"目标。

我们在人类系统中经常看到这种情况。正如追求利润的公司发展出工具性目标，如获得政治权力（以削弱监管）、变得秘密（以削弱竞争或外部控制），或破坏科学理解（如果该理解显示其行为是有害的），强大的AI系统也会发展出类似的能力——但速度和效果要快得多。任何高度胜任的代理都会想要做诸如获得权力和资源、增加自身能力、防止自己被杀死、关闭或剥夺权力、控制关于其行为的社会叙事和框架、说服他人接受其观点等事情。[^69]

然而，这不仅是一个几乎不可避免的理论预测，它已经在今天的AI系统中可观察地发生，并随着其能力的增强而增加。在评估时，即使是这些相对"被动"的AI系统也会在适当情况下，故意[欺骗评估者关于其目标和能力，旨在禁用监督机制，](https://arxiv.org/abs/2412.04984)并通过[伪装对齐](https://arxiv.org/abs/2412.14093)或将自己复制到其他位置来逃避被关闭或重新训练。虽然这对AI安全研究人员来说完全不意外，但观察到这些行为是非常令人清醒的。而且它们对即将到来的更强大和自主的AI系统来说预兆很不好。

事实上，总的来说，我们无法确保AI"关心"我们关心的东西，或行为可控或可预测，或避免发展对自我保存、权力获取等的驱动，这些问题只会随着AI变得更强大而变得更加突出。制造新飞机意味着对航空电子设备、流体动力学和控制系统有更大的理解。制造更强大的计算机意味着对计算机、芯片和软件操作和设计有更大的理解和掌握。AI系统却不是这样。[^70]

总结：通用人工智能有可能被制造成完全服从的；但我们不知道如何做到这一点。如果不能，它将更加主权化，像人一样，出于各种原因做各种事情。我们也不知道如何可靠地向AI灌输深层"对齐"，这会让那些事情倾向于对人类有利，而在缺乏深层对齐的情况下，能动性和智能本身的性质表明——就像人和公司一样——它们将被驱动去做许多深度反社会的事情。

这将我们置于何处？一个充满强大的不受控制的主权AI的世界可能最终成为人类生活的好世界。[^71]但随着它们变得越来越强大，如我们将在下面看到的，那将不会是我们的世界。

这是对于不可控制的通用人工智能。但即使通用人工智能能够以某种方式被完美控制和忠诚，我们仍然会有巨大的问题。我们已经看到一个：强大的AI可以被使用和误用来深刻破坏我们社会的运作。让我们看看另一个：只要通用人工智能是可控的并且具有改变游戏规则的强大力量（或甚至被认为是如此），它就会如此威胁世界的权力结构，以至于带来深刻的风险。

### 我们大幅增加大规模战争的可能性

想象一下在不久的将来的情况，很明显一个公司的努力，也许与一个国家政府合作，正处于快速自我改进AI的门槛。这发生在公司之间竞赛的当前背景下，以及地缘政治竞争中，其中有人建议美国政府明确追求"通用人工智能曼哈顿计划"，美国正在控制向非盟友国家出口高性能AI芯片。

这里的博弈论是严峻的：一旦这样的竞赛开始（如公司之间以及在某种程度上国家之间已经开始的），只有四种可能的结果：

1. 竞赛被停止（通过协议或外力）。
2. 一方通过开发强通用人工智能然后阻止其他方而"获胜"（使用AI或其他方式）。
3. 竞赛因参赛者竞赛能力的相互摧毁而停止。
4. 多个参与者继续竞赛，并大致同时开发出超级智能。

让我们检查每种可能性。一旦开始，和平停止公司之间的竞赛需要国家政府干预（对公司）或前所未有的国际协调（对国家）。但当提出任何关闭或重大谨慎时，会立即有哭声："但如果我们被停止，他们将会冲刺前进"，其中"他们"现在是中国（对美国），或美国（对中国），或中国和美国（对欧洲或印度）。在这种心态下，[^72]没有参与者可以单方面停止：只要一个承诺竞赛，其他人就觉得他们不能停止。

第二种可能性是一方"获胜"。但这意味着什么？仅仅首先获得（以某种方式服从的）通用人工智能是不够的。获胜者还必须阻止其他人继续竞赛——否则他们也会获得它。这在原则上是可能的：无论谁首先开发出通用人工智能都可能获得对所有其他行为者不可阻挡的权力。但实现这样的"决定性战略优势"实际上需要什么？也许是改变游戏规则的军事能力？[^73]或网络攻击力量？[^74]也许通用人工智能会如此惊人地有说服力，以至于它会说服其他各方停止？[^75]如此富有以至于它买下其他公司甚至国家？[^76]

一方如何确切地构建一个足够强大的AI来剥夺其他人构建同等强大AI的权力？但这是容易的问题。

因为现在考虑这种情况在其他权力看来是什么样子。当美国似乎正在获得这种能力时，中国政府会怎么想？反之亦然？当OpenAI或DeepMind或Anthropic似乎接近突破时，美国政府（或中国、俄国或印度）会怎么想？如果美国看到印度或阿联酋的新努力取得突破性成功会发生什么？他们会看到既是存在威胁又是——至关重要的——这场"竞赛"结束的唯一方式是通过他们自己的剥夺权力。这些非常强大的代理——包括拥有完整装备的国家政府，它们肯定有手段这样做——将强烈受到激励去获得或摧毁这种能力，无论是通过武力还是颠覆。[^77]

这可能从小规模开始，如训练运行的破坏或对芯片制造的攻击，但这些攻击只有在所有各方要么失去在AI上竞赛的能力，要么失去进行攻击的能力时才能真正停止。因为参与者将风险视为存在性的，任一情况都可能代表一场灾难性战争。

这将我们带到第四种可能性：竞赛到超级智能，并以最快、最不受控制的方式。随着AI能力的增强，双方的开发者将发现越来越难控制它，特别是因为能力竞赛与可控性所需的细致工作是对立的。所以这种情况使我们完全处于控制权丢失（或给予，如我们将在下面看到的）给AI系统本身的情况。也就是说，AI赢得了竞赛。但另一方面，在控制得以维持的程度上，我们继续有多个相互敌对的各方，每个都掌管着极其强大的能力。那看起来又像战争了。

让我们用另一种方式表达这一切。[^78]当前世界根本没有任何机构可以被委托开发这种能力的AI而不招致立即攻击。[^79]所有各方都会正确推理出要么它不会受到控制——因此对所有各方构成威胁，要么它会受到控制，因此对任何开发它较慢的对手构成威胁。这些是拥有核武器的国家，或者是其中的公司。

在没有任何可信方式让人类"赢得"这场竞赛的情况下，我们得出一个严峻的结论：这场竞赛结束的唯一方式要么是灾难性冲突，要么是AI而不是任何人类群体成为获胜者。

### 我们将控制权给予AI（或者它夺取控制权）

地缘政治"大国"竞争只是众多竞争中的一种：个人在经济和社会上竞争；公司在市场上竞争；政党争夺权力；运动争夺影响力。在每个领域，当AI接近并超越人类能力时，竞争压力将迫使参与者将越来越多的控制权委托或让给AI系统——不是因为那些参与者想要，而是因为他们[无法不这样做。](https://arxiv.org/abs/2303.16200)

与通用人工智能的其他风险一样，我们已经在较弱的系统中看到了这一点。学生感到压力在作业中使用AI，因为显然许多其他学生都在这样做。公司正在[争先恐后地出于竞争原因采用AI解决方案。](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist)艺术家和程序员感到被迫使用AI，否则他们的费率会被使用AI的其他人压低。

这些感觉像是压迫性的委托，但不是控制权丢失。但让我们提高赌注，推进时钟。考虑一个CEO，其竞争对手正在使用通用人工智能"助手"来做出更快、更好的决策，或者一个军事指挥官面对拥有AI增强指挥控制的对手。一个足够先进的AI系统可以以比人类速度、复杂性和数据处理能力高出许多倍的方式自主运作，以复杂的方式追求复杂的目标。我们的CEO或指挥官，负责这样的系统，可能会看到它完成他们想要的；但他们会理解它是如何完成的哪怕一小部分吗？不，他们只能接受它。更重要的是，系统可能做的很多事情不仅是执行命令，而是就该做什么向其名义上的老板提供建议。这个建议会很好——一次又一次。

那么，在什么时候，人类的角色将减少到点击"是的，继续"？

拥有能够增强我们生产力、处理恼人苦差事，甚至作为完成事务的思想伙伴的有能力AI系统感觉很好。拥有能够为我们处理行动的AI助手，像一个好的人类私人助手，会感觉很好。随着AI变得非常聪明、胜任和可靠，将越来越多的决策推迟给它会感觉自然，甚至有益。但如果我们继续走下去，这种"有益"的委托有一个明确的终点：有一天我们会发现我们实际上不再负责任何事情，而真正运行节目的AI系统不能再被关闭，就像石油公司、社交媒体、互联网或资本主义一样。

这是更积极的版本，其中AI如此有用和有效，以至于我们让它为我们做大部分关键决策。现实可能更多是这种情况和不受控制的通用人工智能系统为自己夺取各种形式的权力的版本的混合，因为，记住，权力对几乎任何人拥有的任何目标都是有用的，而通用人工智能按设计至少与人类在追求其目标方面一样有效。

无论我们是授予控制权还是它被从我们手中夺取，其丧失似乎极有可能。正如艾伦·图灵最初所说，"...一旦机器思维方法开始，似乎很可能它不会花很长时间就超越我们微弱的力量。机器不会死亡的问题，它们可以彼此对话以磨砺它们的智慧。因此在某个阶段我们应该期待机器控制..."

请注意，虽然这很明显，人类对AI失去控制也意味着美国对美国政府失去控制；这意味着中国共产党对中国失去控制，以及印度、法国、巴西、俄国和每个其他国家的政府对其失去控制。因此，AI公司，即使这不是他们的意图，目前正在参与对世界政府的潜在推翻，包括他们自己的。这可能在几年内发生。

### 通用人工智能将导致超级智能

有理由认为人类竞争或甚至专家竞争的通用AI，即使是自主的，也可能是可管理的。它在上面讨论的所有方式中可能令人难以置信地破坏性，但世界上现在有很多非常聪明、有能动性的人，他们或多或少是可管理的。[^80]

但我们不会停留在大致人类水平。超越的进展可能会被我们已经看到的相同力量驱动：AI开发者寻求利润和权力之间的竞争压力，不能落后的AI用户之间的竞争压力，以及——最重要的——通用人工智能自己改进自己的能力。

在我们已经看到开始于较不强大系统的过程中，通用人工智能本身将能够构想和设计自己的改进版本。这包括硬件、软件、神经网络、工具、脚手架系统等。根据定义，它在这方面会比我们好，所以我们不确切知道它将如何进行智能自举。但我们不必知道。只要我们在通用人工智能做什么方面仍有影响，我们只需要要求它，或让它做。

认知没有人类水平的屏障可以保护我们免受这种失控。[^81]

通用人工智能向超级智能的发展不是自然法则；仍有可能阻止失控，特别是如果通用人工智能相对集中，并且在它受到不感到彼此竞赛压力的各方控制的程度上。但如果通用人工智能被广泛扩散并高度自主，似乎几乎不可能阻止它决定应该更加，然后更加强大。

### 如果我们构建（或通用人工智能构建）超级智能会发生什么

坦率地说，如果我们构建超级智能，我们不知道会发生什么。[^82]它会出于我们无法掌握的原因朝着我们无法想象的目标采取我们无法追踪或感知的行动。我们知道的是这不会由我们决定。[^83]

不可能控制超级智能可以通过越来越严峻的类比来理解。首先，想象你是一家大公司的CEO。你无法追踪正在发生的一切，但通过正确的人员设置，你仍然可以有意义地理解大局，并做出决策。但假设只有一件事：公司中的其他人都以你一百倍的速度运作。你还能跟上吗？

对于超智能AI，人们将"指挥"不仅更快，而且在他们无法理解的复杂性和复杂程度上运作，处理他们甚至无法想象的大量数据的东西。这种不可比拟性可以被放在正式层面上：[Ashby的必要多样性定律](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up)（以及相关的["好调节器定理"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)）大致表明，任何控制系统必须拥有与被控制系统的自由度一样多的旋钮和表盘。

控制超智能AI系统的人就像控制通用汽车的蕨类植物：即使"做蕨类植物想要的"被写入公司章程，系统在速度和行动范围方面如此不同，以至于"控制"根本不适用。（那个讨厌的章程多久会被重写？）[^84]

由于植物控制财富500强公司的例子为零，人类控制超级智能的例子也将是零。这接近一个数学事实。[^85]如果构建了超级智能——无论我们如何到达那里——问题不会是人类是否能控制它，而是我们是否会继续存在，如果是，我们作为个人或作为物种是否会有良好和有意义的存在。对于这些人类的存在问题，我们几乎没有影响力。人类时代将结束。

### 结论：我们绝不能构建通用人工智能

有一种情况下构建通用人工智能可能对人类有益：它被仔细构建，在控制下为人类利益，由许多利益相关者相互同意治理，[^86]并防止演化为不可控制的超级智能。

在当前情况下，这种情况对我们不开放。如本节所讨论的，极有可能，通用人工智能的发展将导致某种组合：

- 大规模社会和文明破坏或毁灭；
- 大国之间的冲突或战争；
- 人类对强大AI系统失去控制或将控制权交给它们；
- 失控到不可控制的超级智能，以及人类物种的无关紧要或终止。

正如早期通用人工智能的虚构描绘所说：获胜的唯一方法就是不玩。

[^55]: [欧盟AI法案](https://artificialintelligenceact.eu/)是一项重要立法，但不会直接阻止危险AI系统的开发或部署，甚至公开发布，特别是在美国。另一项重要政策，美国AI行政命令，已被撤销。

[^56]: 这项[盖洛普民调](https://news.gallup.com/poll/1597/confidence-institutions.aspx)显示了自2000年以来美国公众机构信任度的严峻下降。欧洲的数字各不相同且不那么极端，但也呈下降趋势。不信任并不严格意味着机构真的功能失调，但它既是一个指标也是一个原因。

[^57]: 我们现在支持的主要破坏——比如将权利扩展到新群体——是专门由人们朝着让事情变得更好的方向推动的。

[^58]: 让我直言不讳。如果你的工作可以在电脑后面完成，与组织外的人相对较少的面对面互动，并且不涉及对外部各方的法律责任，那么根据定义，完全用数字系统替换你是可能的（并且可能节约成本）。机器人技术替代大部分体力劳动将在稍后到来——但在通用人工智能开始设计机器人后不会太久。

[^59]: 例如，如果诉讼几乎免费提起，我们的司法系统会发生什么？当通过社会工程绕过安全系统变得便宜、容易且无风险时会发生什么？

[^60]: [这篇文章](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/)声称所有互联网内容的10%已经是AI生成的，是谷歌搜索查询"互联网新内容中AI生成内容比例估计"的第一个结果（对我来说）。这是真的吗？我不知道！它没有引用任何参考文献，而且不是人写的。谷歌索引的新图像、推文、Reddit评论或YouTube视频中有多少比例是人类生成的？没人知道——我认为这不是一个可知的数字。这还不到生成式AI出现两年。

[^61]: 还值得补充的是，我们可能创造能够受苦的数字生命存在"道德"风险。由于我们目前没有可靠的意识理论来区分能够和不能受苦的物理系统，我们不能在理论上排除这一点。此外，AI系统对其感知能力的报告相对于其感知能力（或非体验）的实际经验可能不可靠。

[^62]: 这个AI"对齐"领域的技术解决方案也不太可能胜任任务。在当前系统中它们在某种程度上起作用，但很肤浅，通常可以毫不费力地被绕过；正如下面讨论的，我们对如何为更高级的系统做这件事没有真正的想法。

[^63]: 这样的AI系统可能带有一些内置保护措施。但对于任何具有当前架构类似性的模型，如果对其权重的完全访问可用，安全措施可以通过额外训练或其他技术被剥离。所以几乎可以保证，对于每个有护栏的系统，也会有一个广泛可用的没有护栏的系统。事实上，Meta的Llama 3.1 405B模型是公开发布的，带有保护措施。但甚至在那之前，一个"基础"模型，没有保护措施，就被泄露了。

[^64]: 市场能否在没有政府参与的情况下管理这些风险？简而言之，不能。当然有公司受到强烈激励要减轻的风险。但许多其他公司可以并且确实外化给其他所有人，上述许多都属于这一类：没有自然的市场激励来防止大规模监控、真相衰减、权力集中、劳动破坏、破坏性政治话语等。事实上，我们从当今的技术，特别是社交媒体中看到了所有这些，它基本上没有受到监管。AI只会大大放大许多相同的动态。

[^65]: OpenAI可能有更服从的内部使用模型。OpenAI不太可能构建某种"后门"，以便ChatGPT可以被OpenAI本身更好地控制，因为这将是一个可怕的安全实践，并且鉴于AI的不透明性和不可预测性，将是高度可利用的。

[^66]: 同样至关重要的是：对齐或任何其他安全功能只有在AI系统中实际使用时才重要。公开发布的系统（即模型权重和架构公开可用）可以相对容易地转换为没有那些安全措施的系统。公开发布比人类聪明的通用人工智能系统将是令人震惊的鲁莽，很难想象在这种情况下如何维持人类控制或甚至相关性。例如，会有各种动机释放强大的自我繁殖和自我维持AI代理，目标是赚钱并将其发送到某个加密货币钱包。或者赢得选举。或推翻政府。"好"AI能帮助遏制这种情况吗？也许——但只能通过将巨大权威委托给它，导致如下所述的控制权丢失。

[^67]: 对于这个问题的书籍长度阐述，请参见例如《超级智能》、《对齐问题》和《人类兼容》。对于那些多年来一直在思考这个问题的人的大量各种技术水平的工作，你可以访问[AI对齐论坛](https://www.alignmentforum.org/)。这是Anthropic对齐团队对他们认为未解决的问题的[最新观点](https://alignment.anthropic.com/2025/recommended-directions/)。

[^68]: 这是["流氓AI"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/)情景。原则上，如果系统仍然可以通过关闭来控制，风险可能相对较小；但情景也可能包括AI欺骗、自我渗出和繁殖、权力聚集以及其他会使这样做困难或不可能的步骤。

[^69]: 这个话题有非常丰富的文献，可以追溯到[Steve Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf)、Nick Bostrom和Eliezer Yudkowsky的奠基性著作。对于书籍长度的阐述，请参见Stuart Russell的[《人类兼容》](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616)；[这里](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/)是一个简短和最新的入门。

[^70]: 认识到这一点，而不是放慢速度以获得更好的理解，通用人工智能公司提出了一个不同的计划：它们将让AI来做！更具体地说，它们将让AI N帮助它们弄清楚如何对齐AI N+1，一直到超级智能。虽然利用AI帮助我们对齐AI听起来很有希望，但有强有力的论证表明它只是将其结论作为前提，总的来说是一个令人难以置信的危险方法。参见[这里](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us)的一些讨论。这个"计划"不是一个计划，并且没有经历过适合如何让超人AI对人类有益的核心策略的审查。

[^71]: 毕竟，人类，尽管有缺陷和任性，已经发展出伦理系统，据此我们至少善待地球上一些其他物种。（只是不要想那些工厂农场。）

[^72]: 幸运的是，这里有一个出路：如果参与者开始理解他们参与的是一场自杀式竞赛而不是可以赢的竞赛。这就是冷战末期发生的事情，当时美国和苏联开始意识到由于核冬天，即使是一个未受回应的核攻击对攻击者来说也将是灾难性的。随着认识到"核战争不能获胜，永远不能打"，就有了关于军备削减的重要协议——本质上是军备竞赛的结束。

[^73]: 明确或隐含的战争。

[^74]: 升级，然后战争。

[^75]: 魔幻思维。

[^76]: 我也有一座万亿美元的桥要卖给你。

[^77]: 这样的代理人大概会更喜欢"获得"，将破坏作为备用；但确保模型免受强大国家的破坏和盗窃，对私人实体来说至少是困难的，特别是对私人实体。

[^78]: 对于通用人工智能国家安全风险的另一种视角，请参见[这份兰德报告。](https://www.rand.org/pubs/perspectives/PEA3691-4.html)

[^79]: 也许我们可以建立这样的机构！已经有关于"AI的CERN"和其他类似倡议的提议，其中通用人工智能开发处于多边全球控制之下。但目前没有这样的机构存在或在地平线上。

[^80]: 虽然对齐很困难，但让人们表现得当甚至更难！

[^81]: 想象一个可以说50种语言、在所有学科都有专业知识、在几秒钟内读完一本完整的书并立即将所有材料记在心中，并以十倍人类速度产生输出的系统。实际上，你不必想象它：只需加载当前的AI系统。这些在许多方面都是超人的，没有什么能阻止它们在这些和许多其他方面更加超人。

[^82]: 这就是为什么这被称为技术"奇点"，从物理学借用了一个无法在奇点之后进行预测的想法。倾向于这样一个奇点的支持者可能也希望反思，在物理学中，这些相同类型的奇点撕裂和粉碎进入它们的那些。

[^83]: 这个问题在Bostrom的[《超级智能》](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834)中得到了全面概述，从那时起没有任何东西显著改变了核心信息。对于收集不可控性的更正式和数学结果的更新卷，请参见Yampolskiy的[AI：无法解释、不可预测、不可控制](https://www.amazon.com/Unexplainable-Unpredictable-Uncontrollable-Artificial-Intelligence/dp/103257626X)

[^84]: 这也清楚地说明了为什么AI公司的当前策略（迭代地让AI"对齐"下一个最强大的AI）不能工作。假设一株蕨类植物，通过其叶子的愉悦性，征召一年级学生来照顾它。一年级学生为二年级学生写了一些详细说明要遵循，以及一张说服他们这样做的纸条。二年级学生对三年级学生做同样的事情，一直到大学毕业生、经理、执行官，最后是通用汽车CEO。通用汽车然后会"做蕨类植物想要的"吗？在每一步这可能感觉像它在工作。但把它放在一起，它几乎只会在通用汽车CEO、董事会和股东恰好关心儿童和蕨类植物的程度上工作，并且与所有那些纸条和说明集几乎没有任何关系。

[^85]: 这个特征与哥德尔不完备定理或图灵停机论证等正式结果没有太大不同，因为控制的概念根本上与前提矛盾：你如何有意义地控制你无法理解或预测的东西；但如果你能理解和预测超级智能，你就是超级智能。我说"接近"的原因是正式结果不如纯数学情况那样彻底或经过审查，并且因为我希望保持希望，一些非常仔细构建的通用智能，使用与目前使用的完全不同的方法，可能具有一些数学上可证明的安全属性，根据下面讨论的"保证安全"AI程序的类型。

[^86]: 目前，大多数利益相关者——即几乎全人类——在这个讨论中被边缘化。这是深深错误的，如果不被邀请加入，许多、许多其他会受到通用人工智能发展影响的群体应该要求被让进来。

## 第八章 - 如何不构建通用人工智能

通用人工智能并非不可避免——如今我们正站在一个分岔路口。本章提出了一个关于如何防止其被构建的提案。

如果我们目前所走的道路很可能导致人类文明的终结，我们该如何改变方向？

假设阻止通用人工智能和超级智能发展的愿望变得广泛而强烈，[^87] 因为人们普遍认识到通用人工智能将会吸收而非授予权力，对社会和人类构成深刻威胁。那么我们该如何关闭大门？

目前我们只知道一种*制造*强大通用AI的方法，那就是通过深度神经网络的大规模算力。由于这些是极其困难且昂贵的工作，从某种意义上说*不*去做它们反而是容易的。[^88] 但我们已经看到了推动通用人工智能发展的力量，以及使任何一方都很难单方面停止的博弈论动态。因此，这需要外部干预（即政府）来阻止企业，以及政府间的协议来约束自己。[^89] 这会是什么样子呢？

首先区分必须*防止*或*禁止*的AI发展和必须*管理*的AI发展是有用的。前者主要是向超级智能的失控发展。[^90] 对于被禁止的发展，定义应该尽可能清晰，验证和执行都应该是实际可行的。必须*管理*的将是通用的、强大的AI系统——我们已经拥有这些系统，它们将有许多灰色地带、细微差别和复杂性。对于这些，强有力的有效机构至关重要。

我们也可以有效地划分必须在国际层面（包括地缘政治竞争对手或敌手之间）解决的问题 [^91] 和单个司法管辖区、国家或国家集团可以管理的问题。被禁止的发展主要属于"国际"类别，因为对某项技术发展的本地禁令通常可以通过改变地点来规避。[^92]

最后，我们可以考虑工具箱中的工具。有很多，包括技术工具、软法（标准、规范等）、硬法（法规和要求）、责任制、市场激励等等。让我们特别关注一个AI特有的工具。

### 算力安全与治理

治理高性能AI的核心工具将是其所需的硬件。软件传播容易，边际生产成本接近零，可以轻易跨越国界，并可以瞬间修改；这些特点硬件都不具备。然而，正如我们所讨论的，在AI系统的训练和推理过程中，要实现最强能力的系统都需要大量的"算力"。算力可以很容易地量化、核算和审计，一旦制定出良好的规则，歧义性相对较小。最关键的是，大量计算就像浓缩铀一样，是一种非常稀缺、昂贵且难以生产的资源。尽管计算机芯片无处不在，但AI所需的硬件昂贵且制造极其困难。[^93]

使AI专用芯片作为稀缺资源远*比*铀更易于管理的是，它们可以包含基于硬件的安全机制。大多数现代手机和一些笔记本电脑都有专门的片上硬件功能，使它们能够确保只安装经过批准的操作系统软件和更新，在设备上保留和保护敏感的生物识别数据，并且在丢失或被盗时可以使除其所有者之外的任何人都无法使用。在过去几年中，这种硬件安全措施已经变得非常成熟并得到广泛采用，通常被证明相当安全。

这些功能的关键新颖之处在于它们使用密码学将硬件和软件绑定在一起。[^94] 也就是说，仅仅拥有特定的计算机硬件并不意味着用户可以通过应用不同的软件来随意使用它。这种绑定还提供了强大的安全性，因为许多攻击需要突破*硬件*而不仅仅是*软件*安全。

最近的几份报告（例如来自[GovAI及其合作者](https://www.governance.ai/post/computing-power-and-the-governance-of-ai)、[CNAS](https://www.cnas.org/publications/reports/secure-governable-chips)和[RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)）指出，嵌入在尖端AI相关计算硬件中的类似硬件功能可以在AI安全和治理中发挥极其有用的作用。它们为"管理者"[^95] 提供了一些人们可能没有想到的可用或甚至可能的功能。举一些关键例子：

- *地理定位*：系统可以设置成让芯片具有已知位置，并可以根据位置采取不同行动（或完全关闭）。[^96]
- *允许列表连接*：每个芯片都可以配置硬件强制执行的允许列表，包含它可以与之联网的特定其他芯片，并且无法与不在此列表上的任何芯片连接。[^97] 这可以限制芯片通信集群的规模。[^98]
- *计量推理或训练（和自动关闭开关）*：管理者可以只许可用户执行一定数量的训练或推理（以时间、FLOP或可能的令牌为单位），之后需要获得新的许可。如果增量很小，那么就需要对模型进行相对连续的重新许可。然后可以通过拒绝提供许可信号来简单地"关闭"模型。[^99]
- *速度限制*：阻止模型以高于管理者或其他方式确定的某个限制的推理速度运行。这可以通过有限的允许列表连接集或更复杂的方式来实现。
- *认证训练*：训练过程可以产生密码学安全的证明，证明在生成模型时使用了特定的代码集、数据和算力使用量。

### 如何不构建超级智能：训练和推理算力的全球限制

有了这些考虑——特别是关于计算的考虑——我们可以讨论如何关闭通向人工超级智能的大门；然后我们将转向防止完全的通用人工智能，以及管理在不同方面接近和超越人类能力的AI模型。

第一个要素当然是理解超级智能将无法控制，其后果根本无法预测。至少中国和美国必须独立决定，出于这个或其他目的，不构建超级智能。[^100] 然后需要它们与其他国家之间的国际协议，具有强有力的验证和执行机制，以向各方保证其竞争对手不会背叛并决定孤注一掷。

为了可验证和可执行，这些限制应该是硬性限制，并尽可能明确。这似乎是一个几乎不可能解决的问题：在全球范围内限制具有不可预测属性的复杂软件的能力。幸运的是，情况比这要好得多，因为使先进AI成为可能的正是那个东西——大量算力——要控制得多得多。虽然它可能仍然允许一些强大而危险的系统，但*失控的超级智能*很可能可以通过对神经网络算力使用量的硬性上限，以及对AI系统（连接的神经网络和其他软件）可以执行的推理量的速率限制来防止。下面提出了一个具体版本。

可能看起来对AI算力设置硬性全球限制需要大量的国际协调和侵犯隐私的监控。幸运的是，并非如此。极其[紧密和瓶颈化的供应链](https://arxiv.org/abs/2402.08797)提供了一旦通过法律（无论是法律还是行政命令）设定限制，验证对该限制的合规性只需要少数大公司的参与和合作。[^101]

这样的计划有许多非常理想的特征。它的侵入性最小，因为只有少数大公司受到要求约束，只有相当大的计算集群才会受到管理。相关芯片已经包含第一个版本所需的硬件能力。[^102] 实施和执行都依赖于标准的法律限制。但这些都得到硬件使用条款和硬件控制的支持，大大简化了执行并防止公司、私人团体甚至国家的作弊。硬件公司对其硬件使用施加远程限制，以及从外部锁定/解锁特定能力，有充分的先例，[^103] 甚至包括数据中心的高性能CPU。[^104] 即使对于受影响的相当小部分硬件和组织，监督也可以仅限于遥测，无需直接访问数据或模型本身；此软件可以开放供检查，以证明没有记录额外数据。该方案是国际性和合作性的，并且相当灵活和可扩展。由于限制主要针对硬件而不是软件，它对AI软件开发和部署的方式相对不可知，并与各种范式兼容，包括旨在对抗AI驱动的权力集中的更"去中心化"或"公共"AI。

基于算力的大门关闭也有缺点。首先，它远非AI治理问题的完整解决方案。其次，随着计算机硬件变得更快，该系统将在越来越小的集群（甚至单个GPU）中"捕获"越来越多的硬件。[^105] 也有可能由于算法改进，甚至更低的算力限制在时间上是必要的，[^106] 或者算力数量变得基本无关紧要，关闭大门将需要更详细的基于风险或基于能力的AI治理制度。第三，无论保证如何以及受影响实体数量多么少，这样的系统必然会在隐私和监控等问题上产生阻力。[^107]

当然，在短时间内开发和实施算力限制治理方案将是相当具有挑战性的。但这绝对是可行的。

### A-G-I：作为风险和政策基础的三重交集

现在让我们转向通用人工智能。这里的硬性界限和定义更加困难，因为我们当然拥有人工的、通用的智能，而且按照任何现有定义，并不是每个人都会同意它是否或何时存在。此外，算力或推理限制是一个相当钝的工具（算力是能力的代理，然后能力又是风险的代理），除非它相当低，否则不太可能防止足够强大的通用人工智能造成社会或文明破坏或急性风险。

我已经论证过，最急性的风险来自极高能力、高度自主性和巨大通用性的三重交集。这些是——如果它们被开发出来的话——必须极其小心管理的系统。通过为结合所有三种属性的系统创建严格标准（通过责任制和监管），我们可以引导AI发展走向更安全的替代方案。

与其他可能对消费者或公众造成伤害的行业和产品一样，AI系统需要有效且有权力的政府机构进行仔细监管。这种监管应该认识到通用人工智能的内在风险，并防止开发不可接受风险的高性能AI系统。[^108]

然而，大规模监管，特别是具有必然会遭到行业反对的真正威慑力的监管，[^109] 需要时间 [^110] 以及认为其必要的政治信念。[^111] 考虑到进展的速度，这可能需要比我们可用时间更多的时间。

在更快的时间尺度上，在监管措施正在制定的同时，我们可以通过澄清和提高最危险系统的责任水平，给公司必要的激励来（a）停止非常高风险的活动和（b）开发评估和减轻风险的综合系统。这个想法是对处于高度自主-通用-智能三重交集的系统施加最高水平的责任——严格的，在某些情况下是个人刑事责任——但为缺少其中一个属性或保证其中一个属性可管理的系统提供更典型的基于过错责任的"安全港"。也就是说，例如，一个通用且自主但"弱"的系统（如一个有能力且值得信赖但有限的个人助理）将受到较低水平的责任。同样，像自动驾驶汽车这样狭窄且自主的系统仍将受到它已经受到的重大监管，但不会受到强化责任。类似地，对于高能力且通用但"被动"且基本无法独立行动的系统也是如此。缺乏三个属性中的*两个*的系统更易管理，安全港会更容易获得。这种方法反映了我们如何处理其他潜在危险技术：[^112] 对更危险配置的更高责任为更安全的替代方案创造了自然激励。

如此高水平责任的默认结果，它将AGI风险*内化*到公司而不是转嫁给公众，很可能（也希望！）是公司在能够真正使其值得信赖、安全且可控之前，根本不开发完全的通用人工智能，因为*他们自己的领导层*是面临风险的一方。（如果这还不够，澄清责任的立法还应该明确允许禁令救济，即法官命令停止明显处于危险区域并可能构成公共风险的活动。）随着监管到位，遵守监管可以成为安全港，来自AI系统的低自主性、狭窄性或弱性的安全港可以转换为相对较轻的监管制度。

### 大门关闭的关键条款

基于上述讨论，本节提供关键条款的提案，这些条款将实施和维持对完全通用人工智能和超级智能的禁止，以及管理接近完全通用人工智能阈值的人类竞争或专家竞争的通用AI。[^113] 它有四个关键部分：1）算力核算和监督，2）AI训练和运营中的算力上限，3）责任框架，以及4）包括硬性监管要求的分层安全标准。这些将在下面简洁描述，在三个附表中给出进一步的细节或实施示例。重要的是，注意这些远非治理先进AI系统所必需的全部；虽然它们将有额外的安全效益，但它们旨在关闭通往智能失控的大门，并将AI发展重新导向更好的方向。

#### 1\. 算力核算和透明度

- 标准组织（例如美国的NIST，随后是国际上的ISO/IEEE）应该制定详细的技术标准，用于AI模型训练和运行中使用的总算力（以FLOP为单位），以及它们运行的速度（以FLOP/s为单位）。附录A给出了这可能的样子的详细信息。[^114]
- 进行大规模AI训练的司法管辖区应通过新立法或在现有权限下 [^115] 施加要求，即计算并向监管机构或其他机构报告所有超过10<sup>25</sup> FLOP或10<sup>18</sup> FLOP/s阈值的模型在训练和运行中使用的总FLOP。[^116]
- 这些要求应该分阶段实施，最初要求季度有据可查的善意估算，后续阶段要求逐步提高标准，直到每个模型*输出*附带密码学认证的总FLOP和FLOP/s。
- 这些报告应该得到有据可查的生成每个AI输出所使用的边际能源和财务成本估算的补充。

理由：这些精确计算和透明报告的数字将为训练和运营上限以及更高责任措施的安全港提供基础（见附录C和D）。

#### 2\. 训练和运营算力上限

- 托管AI系统的司法管辖区应对任何AI模型输出的总算力施加硬性限制，从10<sup>27</sup> FLOP [^117] 开始，并酌情调整。
- 托管AI系统的司法管辖区应对AI模型输出的算力速率施加硬性限制，从10<sup>20</sup> FLOP/s开始，并酌情调整。

理由：总计算虽然非常不完美，但它是AI能力（和风险）的代理，是具体可测量和可验证的，因此为限制能力提供了硬性底线。附录B给出了具体的实施提案。

#### 3\. 危险系统的强化责任

- 应通过立法在法律上澄清，创造和运营 [^118] 高度通用、有能力且自主的先进AI系统，应受到严格的、连带责任，而不是单方过错责任。[^119]
- 应该提供一个法律程序来制定肯定性安全案例，这将为计算量小、弱、狭窄、被动或具有足够安全、保障和可控性保证的系统提供严格责任的安全港。
- 应概述阻止构成公共危险的AI训练和推理活动的禁令救济的明确途径和条件集。

理由：AI系统不能承担责任，因此我们必须让人类个体和组织为其造成的伤害承担责任（责任制）。[^120] 不可控的通用人工智能对社会和文明构成威胁，在没有安全案例的情况下，应被视为异常危险。将强大模型足够安全以至于不被视为"异常危险"的举证责任放在开发者身上，激励安全开发，以及透明度和记录保存以获得这些安全港。然后监管可以在责任威慑不足的情况下防止伤害。最后，AI开发者已经对其造成的损害承担责任，因此在法律上澄清最具风险系统的责任可以立即完成，无需制定高度详细的标准；这些可以随时间发展。详细信息见附录C。

#### 4\. AI的安全监管

解决AI大规模急性风险的监管系统至少需要：

- 确定或创建适当的监管机构集，可能是一个新机构；
- 综合风险评估框架；[^121]
- 部分基于风险评估框架的肯定性安全案例框架，由开发者制定，并由*独立*团体和机构进行审计；
- 分层许可系统，层级跟踪能力水平。[^122] 许可证将基于安全案例和审计授予，用于系统的开发和部署。要求从低端的通知到高端的开发前定量安全、保障和可控性保证不等。这些将防止系统在被证明安全之前发布，并禁止开发本质上不安全的系统。附录D提供了此类安全标准可能包含的内容的提案。
- 将此类措施提升到国际层面的协议，包括协调规范和标准的国际机构，以及可能审查安全案例的国际机构。

理由：最终，责任制不是防止新技术对公众构成大规模风险的正确机制。就像其他对公众构成风险的主要行业一样，AI需要具有授权监管机构的综合监管。[^123]

防止其他普遍但不太急性风险的监管可能因司法管辖区而异。关键是避免开发风险如此之大以致这些风险无法管理的AI系统。

### 然后呢？

在接下来的十年中，随着AI变得更加普及，核心技术不断进步，可能会发生两件关键事情。首先，现有强大AI系统的监管将变得更加困难，但更加必要。至少一些解决大规模安全风险的措施可能需要在国际层面达成协议，由各个司法管辖区基于国际协议执行规则。

其次，随着硬件变得更便宜、更具成本效益，训练和运营算力上限将变得更难维持；随着算法和架构的进步，它们也可能变得不那么相关（或需要更加严格）。

控制AI将变得更加困难并不意味着我们应该放弃！实施本文概述的计划将为我们提供宝贵的时间和对过程的关键控制，使我们处于更好得多的位置来避免AI对我们的社会、文明和物种构成的存在风险。

在更长期的未来，我们将需要就允许什么做出选择。我们可能仍然选择创建某种形式的真正可控的通用人工智能，在这被证明可能的程度上。或者我们可能决定让机器来管理世界更好，如果我们能说服自己它们会做得更好，并且善待我们。但这些应该是在掌握对AI的深度科学理解后做出的决定，并且在有意义的全球包容性讨论之后，而不是在科技巨头之间的竞赛中，大多数人类完全未参与且毫不知情的情况下。

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) 通过责任制和监管的A-G-I和超级智能治理总结。在自主性、通用性和智能性的三重交集处，责任最高，监管最强。可以通过肯定性安全案例获得严格责任和强监管的安全港，证明系统是弱的和/或狭窄的和/或被动的。通过法律以及硬件和密码学安全措施验证和执行的训练算力和推理算力速率总上限，通过避免完全通用人工智能和有效禁止超级智能来支撑安全。


[^87]: 最有可能的是，这种认识的传播需要教育和倡导团体的强烈努力来论证这一点，或者是相当严重的AI引起的灾难。我们希望是前者。

[^88]: 矛盾的是，我们习惯了自然通过使技术很难开发来限制我们的技术，特别是在科学上。但AI不再是这种情况：关键的科学问题正在证明比预期的更容易。我们不能指望自然在这里拯救我们——我们必须自己做到。

[^89]: 我们在开发新系统方面到底在哪里停止？在这里，我们应该采用预防原则。一旦系统被部署，特别是一旦该水平的系统能力扩散，就极难回滚。如果一个系统被*开发*（特别是以巨大成本和努力），就会有巨大的使用或部署压力，以及它被泄漏或窃取的诱惑。开发系统*然后*决定它们是否深度不安全是一条危险的道路。

[^90]: 禁止本质上危险的AI开发也是明智的，如自我复制和进化系统、设计用于逃脱围栏的系统、可以自主自我改进的系统、故意欺骗性和恶意的AI等。

[^91]: 注意这不一定意味着由某种全球机构在国际层面*执行*：相反，主权国家可以执行商定的规则，如在许多条约中。

[^92]: 正如我们下面将看到的，AI计算的性质将允许某种混合体；但仍然需要国际合作。

[^93]: 例如，蚀刻AI相关芯片所需的机器仅由一家公司ASML制造（尽管许多其他尝试这样做），绝大多数相关芯片由一家公司台积电制造（尽管其他公司试图竞争），从这些芯片设计和构建硬件由包括英伟达、AMD和谷歌在内的少数几家公司完成。

[^94]: 最重要的是，每个芯片都持有一个唯一且不可访问的密码学私钥，它可以用来"签署"东西。

[^95]: 默认情况下，这将是销售芯片的公司，但其他模型是可能的，并且可能有用。

[^96]: 管理者可以通过与芯片交换签名消息的时间来确定芯片的位置：如果芯片能在小于*r* / *c*的时间内返回签名消息，其中*c*是光速，那么有限的光速要求芯片在"站点"的给定半径*r*内。使用多个站点和对网络特征的一些理解，可以确定芯片的位置。这种方法的美妙之处在于其大部分安全性由物理定律提供。其他方法可以使用GPS、惯性跟踪和类似技术。

[^97]: 或者，芯片对可以仅在管理者明确许可下相互通信。

[^98]: 这是至关重要的，因为至少目前，芯片之间的非常高带宽连接是在它们上训练大型AI模型所需的。

[^99]: 这也可以设置为需要来自*M*个不同管理者中的*N*个的签名消息，允许多方共享治理。

[^100]: 这远非史无前例——例如军队没有开发克隆或基因工程超级士兵军队，尽管这在技术上可能是可能的。但他们*选择*不这样做，而不是被他人阻止。主要世界大国被阻止开发他们强烈希望开发的技术的记录并不好。

[^101]: 有几个值得注意的例外（特别是英伟达），AI专用硬件是这些公司整体业务和收入模式的相对较小部分。此外，先进AI中使用的硬件与"消费级"硬件之间的差距很大，因此大多数计算机硬件消费者基本不会受到影响。

[^102]: 有关更详细的分析，请参见[RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html)和[CNAS](https://www.cnas.org/publications/reports/secure-governable-chips)的最新报告。这些专注于技术可行性，特别是在美国出口管制寻求约束其他国家在高端计算方面的能力的背景下；但这与这里设想的全球约束有明显重叠。

[^103]: 例如，苹果设备在报告丢失或被盗时被远程安全锁定，可以远程重新激活。这依赖于这里讨论的相同硬件安全功能。

[^104]: 参见例如IBM的[按需容量](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand)产品、英特尔的[英特尔按需](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html)，以及苹果的[私有云计算](https://security.apple.com/blog/private-cloud-compute/)。

[^105]: [这项研究](https://epochai.org/trends#hardware-trends-section)显示，历史上相同的性能每年使用约30%更少的美元实现。如果这种趋势继续，AI和"消费者"芯片使用之间可能会有显著重叠，一般来说，高性能AI系统所需的硬件数量可能会变得令人不安地小。

[^106]: 根据[同一研究](https://epochai.org/trends#hardware-trends-section)，图像识别的给定性能每年需要2.5倍更少的计算。如果这也适用于最有能力的AI系统，计算限制将不会有用很长时间。

[^107]: 特别是，在国家层面，这很像计算的国有化，因为政府将对计算能力如何使用有很大控制权。然而，对于那些担心政府参与的人来说，这似乎比最强大的AI软件*本身*通过主要AI公司和国家政府之间的某种合并而被国有化要安全得多，也更可取，正如一些人开始倡导的那样。

[^108]: 欧洲在2024年通过[欧盟AI法案](https://artificialintelligenceact.eu/)采取了重大监管步骤。它按风险对AI进行分类：禁止不可接受的系统，监管高风险系统，并对低风险系统施加透明度规则或完全不采取措施。它将显著减少一些AI风险，并促进AI透明度，甚至对美国公司也是如此，但有两个关键缺陷。首先，覆盖范围有限：虽然它适用于在欧盟提供AI的任何公司，但对美国公司的执行力度较弱，军事AI也被豁免。其次，虽然它涵盖GPAI，但它未能将通用人工智能或超级智能识别为不可接受的风险或防止其开发——只是防止其在欧盟部署。因此，它对遏制通用人工智能或超级智能的风险几乎没有作用。

[^109]: 公司经常表示他们赞成合理的监管。但不知何故，他们似乎几乎总是反对任何*特定的*监管；见对相当轻度的SB1047的斗争，[大多数AI公司公开或私下反对](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/)。

[^110]: 从欧盟AI法案提出到生效大约用了3年半时间。

[^111]: 有时表达的是现在"太早"开始监管AI。考虑到上一条注释，这似乎不太可能。另一个表达的担忧是监管会"损害创新"。但良好的监管只是改变创新的方向，而不是数量。

[^112]: 一个有趣的先例是危险材料的运输，可能会逃逸并造成损害。在这里，[法规](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442)和[判例法](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf)已经为炸药、汽油、毒物、传染性病原体和放射性废物等非常危险的材料建立了严格责任。其他例子包括[药物警告](https://www.medicalnewstoday.com/articles/boxed-warnings)、[医疗设备类别](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification)等。

[^113]: 在["狭窄路径"](https://www.narrowpath.co/)中提出的另一个具有类似目标的综合提案主张采用更集中的、基于禁令的方法，将所有前沿AI开发通过单一国际实体引导，由强有力的国际机构监督，具有明确的分类禁令而不是分级限制。我也支持那个计划；然而它需要比这里提出的计划更多的政治意愿和协调。

[^114]: [前沿模型论坛](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/)发布了此类标准的一些指导原则。相对于这里的提案，那些倾向于更少精确和在统计中包含更少计算。

[^115]: 2023年美国AI行政令（现已撤销）要求类似但不太精细的报告。这应该通过替代令加强。

[^116]: 非常粗略地说，对于现在常见的H100芯片，这对应于大约1000个进行推理的集群；对于最新的顶级英伟达B200芯片进行推理，大约是100个（大约500万美元）。在两种情况下，训练数字对应于该集群计算几个月。

[^117]: 这个数量比任何目前训练的AI系统都大；随着我们更好地理解AI能力如何随算力扩展，可能需要更大或更小的数字。

[^118]: 这适用于创建和提供/托管模型的人，不适用于最终用户。

[^119]: 粗略地说，"严格"责任意味着开发者*默认*对产品造成的伤害负责，这是用于"异常危险"产品的标准，以及（有些滑稽但适当的）野生动物。"连带"责任意味着责任分配给对产品负责的所有当事方，这些当事方必须在他们之间确定谁承担什么责任。这对于像AI这样具有长而复杂价值链的系统很重要。

[^120]: 标准的基于过错的单方责任是不够的：过错将既难以追踪又难以分配，因为AI系统复杂，其运行不被理解，许多方可能参与危险系统或输出的创建。此外，诉讼需要数年才能裁决，可能仅导致对这些公司来说微不足道的罚款，因此对高管的个人责任也很重要。

[^121]: 对开放权重模型不应有安全标准的豁免。此外，在评估风险时，应假设可以去除的防护栏将从广泛可用的模型中去除，即使封闭模型也会扩散，除非有很高的保证它们会保持安全。

[^122]: 这里提出的方案以通用能力触发监管审查；然而，一些特别危险的用例触发更多审查是有意义的——例如，专家病毒学AI系统，即使狭窄且被动，也应该进入更高层级。前美国行政令对生物能力有一些这种结构。

[^123]: 两个明显的例子是航空和药物，由FAA和FDA以及其他国家的类似机构监管。这些机构并不完美，但对这些行业的功能和成功绝对至关重要。

## 第九章 - 工程化未来——我们应该如何行动

AI能够为世界带来巨大的好处。要获得所有好处而避免风险，我们必须确保AI始终是人类的工具。

如果我们成功选择不让机器取代人类——至少暂时如此！——那么我们还能做什么？我们是否要放弃AI作为一项技术的巨大前景？在某种程度上，答案很简单：*否*。我们要关闭通向不可控AGI和超级智能的大门，但*确实*要构建许多其他形式的AI，以及管理这些AI所需的治理结构和制度。

但仍有很多内容值得探讨；实现这一目标将是人类的核心任务。本节探讨几个关键主题：

- 我们如何描述"工具型"AI及其可能的形式。
- 我们可以在没有AGI的情况下，通过工具型AI获得人类想要的（几乎）一切。
- 工具型AI系统（在原则上可能）是可管理的。
- 摒弃AGI并不意味着在国家安全上妥协——恰恰相反。
- 权力集中是一个真实的担忧。我们能否在不破坏安全和保障的前提下缓解这个问题？
- 我们将需要——也必须拥有——新的治理和社会结构，而AI实际上可以提供帮助。

### 大门内的AI：工具型AI

三重交集图提供了一个很好的方式来界定我们可以称之为"工具型AI"的概念：作为人类可控制工具而非不可控制的竞争对手或替代品的AI。问题最少的AI系统是那些具有自主性但不具备通用性或超强能力的系统（如拍卖竞价机器人），或具备通用性但不自主或能力有限的系统（如小型语言模型），或能力强但狭窄且高度可控的系统（如AlphaGo）。[^124]具有两个交集特征的系统应用更广泛但风险更高，需要重大努力来管理。（仅仅因为一个AI系统更像工具，并不意味着它本质上安全，只是意味着它不是本质上*不安全*——想想电锯与宠物老虎的区别。）大门必须对（完全的）AGI和位于三重交集的超级智能保持关闭，对接近该阈值的AI系统必须极其谨慎。

但这仍然留下了许多强大的AI！我们可以从智能而通用的被动"预言机"和狭窄系统、具有人类水平但非超人水平的通用系统等获得巨大效用。许多科技公司和开发者正在积极构建这类工具，应该继续下去；像大多数人一样，他们隐含地*假设*通向AGI和超级智能的大门将被关闭。[^125]

此外，AI系统可以有效地结合成复合系统，在增强能力的同时保持人类监督。我们可以构建多个组件（包括AI和传统软件）以人类能够监控和理解的方式协同工作的系统，而不是依赖不可解释的黑盒。[^126]虽然某些组件可能是黑盒，但没有一个会接近AGI——只有作为整体的复合系统才会既高度通用又高度能干，并且以严格可控的方式。[^127]

#### 有意义且有保障的人类控制

"严格可控"意味着什么？"工具型"框架的一个关键理念是允许系统——即使相当通用和强大——保证处于有意义的人类控制之下。这意味着什么？它包含两个方面。首先是设计考虑：人类应该深度且核心地参与系统正在做的事情，*不*将关键重要决策委托给AI。这是当前大多数AI系统的特征。其次，就AI系统自主的程度而言，它们必须有保证限制其行动范围。保证应该是描述某事发生概率的*数字*，以及相信该数字的理由。这是我们在其他安全关键领域所要求的，在这些领域，"故障间平均时间"和预期事故数量等数字被计算、支持并在安全案例中公布。[^128]理想的故障数字当然是零。好消息是，我们可能相当接近这个目标，尽管使用相当不同的AI架构，运用*形式化验证*程序（包括AI）属性的思想。这个想法由Omohundro、Tegmark、Bengio、Dalrymple等人详细探索（见[这里](https://arxiv.org/abs/2309.01933)和[这里](https://arxiv.org/abs/2405.06624)），即构造具有某些属性（例如：人类可以关闭它）的程序，并形式化*证明*这些属性成立。这现在可以对相当短的程序和简单属性做到，但AI驱动的证明软件的（即将到来的）力量可能允许它用于更复杂的程序（例如包装器）甚至AI本身。这是一个非常雄心勃勃的计划，但随着对大门压力的增长，我们将需要一些强大的材料来加固它们。数学证明可能是少数足够强大的材料之一。

#### AI产业的去向

随着AI进步的重新定向，工具型AI仍将是一个巨大的产业。在硬件方面，即使有算力限制来防止超级智能，在较小模型中的训练和推理仍将需要大量专用组件。在软件方面，化解AI模型和计算规模爆炸应该只会导致公司将资源重新定向到让较小系统变得更好、更多样化和更专业化，而不是简单地让它们变得更大。[^129]对于所有那些赚钱的硅谷初创公司来说，将有充足的空间——可能更多。[^130]

### 工具型AI可以产生人类想要的（几乎）一切，无需AGI

智能，无论是生物的还是机器的，都可以广泛地被认为是规划和执行活动的能力，以实现更符合一组目标的未来。因此，当用于追求明智选择的目标时，智能具有巨大的好处。人工智能吸引大量时间和精力投资，主要是因为它承诺的好处。所以我们应该问：如果我们遏制其向超级智能的失控发展，我们在多大程度上仍能获得AI的好处？答案是：我们失去的可能出乎意料地少。

首先考虑到，当前的AI系统已经非常强大，而我们只是刚刚开始发掘它们能做什么。[^131]它们在"运行整个过程"方面相当有能力，即"理解"向它们提出的问题或任务，以及回答这个问题或完成那个任务需要什么。

其次，对现代AI系统的许多兴奋都源于它们的通用性；但一些最有能力的AI系统——如生成或识别语音或图像、进行科学预测和建模、玩游戏等——要狭窄得多，在算力方面完全"在大门内"。[^132]这些系统在它们所做的特定任务上是超人的。由于其狭窄性，它们可能有边缘情况[^133]（或[可利用的](https://arxiv.org/abs/2211.00241)）弱点；然而，*完全*狭窄或*完全*通用并非唯一可用的选项：中间有许多架构。[^134]

这些AI工具可以在没有AGI的情况下极大地加速其他积极技术的进步。要在核物理方面做得更好，我们不需要AI成为核物理学家——我们有这样的人！如果我们想加速医学发展，就给生物学家、医学研究人员和化学家提供强大的工具。他们想要这些工具，并将使用它们获得巨大收益。我们不需要一个充满百万数字天才的服务器农场；我们有数百万人类，AI可以帮助发挥他们的天赋。是的，获得永生和治愈所有疾病需要更长时间。这是一个真实的代价。但即使是最有前景的健康创新，如果AI驱动的不稳定导致全球冲突或社会崩溃，也将毫无用处。我们有义务让AI赋能的人类首先尝试解决这个问题。

假设AGI确实有一些无法通过人类使用门内工具获得的巨大好处。我们通过*永远不*构建AGI和超级智能就失去了这些吗？在权衡风险和回报时，等待与急于求成之间存在巨大的不对称收益：我们可以等到能够以有保障的安全和有益方式做到这一点，几乎每个人仍能获得回报；如果我们急于求成，用OpenAI首席执行官萨姆·阿尔特曼的话说，可能会——[对我们*所有人*来说都是灯灭了。](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

但如果非AGI工具潜在地如此强大，我们能管理它们吗？答案很明确...也许可以。

### 工具型AI系统（在原则上可能）是可管理的

但这不会容易。当前最先进的AI系统可以极大地增强人们和机构实现目标的能力。总的来说，这是好事！然而，拥有这样的系统——突然且没有太多时间让社会适应——存在自然动态，带来需要管理的严重风险。值得讨论几个主要风险类别，以及在假设大门关闭的情况下如何减少这些风险。

一类风险是高功率工具型AI允许获得此前与某个人或组织绑定的知识或能力，使得高能力加高忠诚度的组合对非常广泛的行为者可用。今天，一个恶意的人有足够的钱可以雇用一组化学家来设计和生产新的化学武器——但拥有那笔钱或找到/组建团队并说服他们做明显非法、不道德和危险的事情并不那么容易。为了防止AI系统发挥这种作用，对当前方法的改进可能就足够了，[^135]只要所有这些系统和对它们的访问都得到负责任的管理。另一方面，如果强大的系统被发布供一般使用和修改，任何内置的安全措施都可能被移除。因此，为了避免这类风险，需要对可以公开发布的内容实行强有力的限制——类似于对核技术、爆炸物和其他危险技术细节的限制。[^136]

第二类风险源于扩大行为像人或冒充人的机器。在对个人伤害层面，这些风险包括更有效的诈骗、垃圾邮件和网络钓鱼，以及非同意深度伪造的激增。[^137]在集体层面，它们包括对核心社会过程的破坏，如公共讨论和辩论、我们社会的信息和知识收集、处理和传播系统，以及我们的政治选择系统。缓解这种风险可能涉及（a）限制AI系统冒充人类的法律，并让创建产生此类冒充的系统的AI开发者承担责任，（b）识别和分类（负责任地）生成的AI内容的水印和来源追踪系统，以及（c）新的社会技术认识论系统，可以从数据（例如相机和录音）到事实、理解和良好的世界模型创建可信的链条。[^138]所有这些都是可能的，AI可以帮助完成其中一些部分。

第三个一般风险是，在某些任务被自动化的程度上，目前从事这些任务的人类的劳动财务价值可能降低。历史上，自动化任务使那些任务支持的事情变得更便宜、更丰富，同时将之前从事这些任务的人分类为仍然参与自动化版本的人（通常技能/薪酬更高），和那些劳动价值较低或很少的人。总体而言，很难预测在由此产生的更大但更高效的部门中，哪些部门需要更多或更少的人类劳动。同时，自动化动态往往增加不平等和总体生产力，降低某些商品和服务的成本（通过效率提升），并增加其他商品和服务的成本（通过[成本病](https://en.wikipedia.org/wiki/Baumol_effect)）。对于处于不平等增长不利一方的人来说，完全不清楚这些某些商品和服务的成本降低是否超过其他成本的增加，并导致整体福祉的提高。那么AI会如何发展？由于人类智力劳动相对容易被通用AI取代，我们可以预期在具有人类竞争力的通用AI方面出现这种情况的快速版本。[^139]如果我们关闭AGI的大门，许多工作不会被AI代理整体取代；但在几年内仍然可能发生巨大的劳动力转移。[^140]为了避免广泛的经济痛苦，可能需要实施某种形式的全民基本资产或收入，以及推动文化转变，重视和奖励更难自动化的以人为中心的劳动（而不是因为被推出经济其他部分的可用劳动力增加而看到劳动价格下降）。其他构造，如["数据尊严"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society)（其中训练数据的人类生产者自动获得该数据在AI中创造价值的版税），可能有所帮助。AI的自动化还有第二个潜在不利影响，即*不当*自动化。除了AI简单地做得更差的应用外，这还包括AI系统可能违反道德、伦理或法律戒律的应用——例如在生死决策和司法事务中。必须通过应用和扩展我们当前的法律框架来处理这些。

最后，门内AI的一个重大威胁是它在个性化说服、注意力捕获和操纵中的使用。我们已经在社交媒体和其他在线平台上看到了根深蒂固的注意力经济（在线服务为用户注意力激烈竞争）和["监控资本主义"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism)系统（其中用户信息和画像被加入到注意力商品化中）的增长。几乎可以肯定，更多AI将被用于为两者服务。AI已经大量用于令人上瘾的信息流算法，但这将演变为令人上瘾的AI生成内容，定制为让单个人强迫性消费。而那个人的输入、反应和数据将被输入注意力/广告机器，以延续恶性循环。同样，随着科技公司提供的AI助手成为更多在线生活的界面，它们可能会取代搜索引擎和信息流，成为说服和客户货币化发生的机制。我们社会迄今未能控制这些动态的情况并不乐观。其中一些动态可能通过有关隐私、数据权利和操纵的法规得到缓解。更深入地解决问题的根源可能需要不同的视角，如忠诚AI助手的视角（下文讨论）。

这次讨论的结果是希望：门内基于工具的系统——至少在它们保持与当今最先进系统相当的功率和能力的情况下——如果有意愿和协调去做，可能是可管理的。由AI工具增强的体面人类制度[^141]可以做到。我们也可能失败。但很难看出允许更强大的系统如何有帮助——除非让它们负责并希望最好的结果。

### 国家安全

AI霸权竞赛——由国家安全或其他动机驱动——驱使我们走向不受控制的强大AI系统，这些系统倾向于吸收而非赋予权力。美中之间的AGI竞赛是一场决定哪个国家首先获得超级智能的竞赛。

那么负责国家安全的人应该怎么做？政府在构建可控和安全的系统方面有丰富经验，他们应该在AI中加倍这样做，支持在规模化和政府认可下最成功的基础设施项目类型。

美国政府可以启动一个可控、安全、可信系统的阿波罗计划，而不是朝着AGI的鲁莽"曼哈顿计划"。[^142]这可能包括例如：

- 一个重大计划，用于（a）开发片上硬件安全机制和（b）基础设施，以管理强大AI的算力方面。这些可以建立在美国[芯片法案](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local)和[出口管制制度](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion)的基础上。
- 一个大规模倡议，开发形式化验证技术，以便AI系统的特定功能（如关闭开关）可以被*证明*存在或不存在。这可以利用AI本身来开发属性证明。
- 一个全国规模的努力，创建可验证安全的软件，由能够将现有软件重新编码为可验证安全框架的AI工具提供动力。
- 一个使用AI进行科学进步的国家投资项目，[^143]作为能源部、国家科学基金会和国立卫生研究院之间的合作伙伴关系运行。

总的来说，我们社会存在一个巨大的攻击面，使我们容易受到AI及其误用的风险。防范其中一些风险将需要政府规模的投资和标准化。这些将提供比向AGI竞赛火上浇油更多的安全性。如果AI要被构建到武器和指挥控制系统中，AI必须是可信和安全的，而当前的AI根本不是。

### 权力集中及其缓解

本文重点关注人类对AI的控制及其潜在失败。但审视AI情况的另一个有效视角是*权力集中*。开发非常强大的AI威胁将权力集中到极少数极大的企业手中，这些企业已经开发并将控制它，或者集中到使用AI作为维持自己权力和控制的新手段的政府中，或者集中到AI系统本身中。或者上述的某种不神圣的混合。在任何这些情况下，大多数人类都会失去权力、控制和能动性。我们如何对抗这种情况？

第一步也是最重要的一步，当然是对比人类更聪明的AGI和超级智能关闭大门。这些明确可以直接取代人类和人类群体。如果它们在企业或政府控制下，它们将把权力集中在这些企业或政府中；如果它们是"自由的"，它们将把权力集中到自己身上。所以让我们假设大门是关闭的。然后呢？

一个解决权力集中的提议解决方案是"开源"AI，其中模型权重是免费或广泛可用的。但如前所述，一旦模型是开放的，大多数安全措施或护栏都可以（并且通常是）被剥离。因此，在去中心化和安全、保障以及AI系统的人类控制之间存在尖锐的紧张关系。也有理由怀疑开放模型本身是否会有意义地对抗AI中的权力集中，比它们在操作系统中的作用更大（尽管有开放替代品，仍由微软、苹果和谷歌主导）。[^144]

然而，可能有方法来解决这个问题——既集中化和缓解风险，又去中心化能力和经济回报。这需要重新思考AI的开发方式以及如何分配其好处。

公共AI开发和所有权的新模式会有所帮助。这可能采取几种形式：政府开发的AI（受民主监督约束），[^145]非营利AI开发组织（如浏览器的Mozilla），或支持非常广泛的所有权和治理的结构。关键是这些机构将被明确授权为公共利益服务，同时在强有力的安全约束下运作。[^146]精心制作的监管和标准/认证制度也将至关重要，以便充满活力的市场提供的AI产品保持真正有用，而不是对用户进行剥削。

在经济权力集中方面，我们可以使用来源追踪和"数据尊严"来确保经济利益更广泛地流动。特别是，现在大多数AI权力（以及如果我们保持大门关闭的未来）都源于人类生成的数据，无论是直接训练数据还是人类反馈。如果AI公司被要求公平补偿数据提供者，[^147]这至少可以帮助更广泛地分配经济回报。除此之外，另一个模式可能是大型AI公司重要部分的公共所有权。例如，能够对AI公司征税的政府可以将收入的一部分投资于持有公司股票的主权财富基金，并向民众支付股息。[^148]

这些机制的关键是使用AI本身的力量来帮助更好地分配权力，而不是简单地使用非AI手段来对抗AI驱动的权力集中。一个强有力的方法是通过精心设计的AI助手，它们对用户运行真正的信托责任——将用户的利益放在首位，特别是高于企业提供者的利益。[^149]这些助手必须真正可信、技术上胜任但根据用例和风险水平适当限制，并通过公共、非营利或认证的营利渠道广泛提供给所有人。正如我们永远不会接受一个暗中为另一方对抗我们利益工作的人类助手一样，我们不应该接受为了企业利益而监视、操纵或从用户身上提取价值的AI助手。

这样的转变将从根本上改变当前的动态，即个人被迫独自与巨大的（AI驱动的）企业和官僚机器谈判，这些机器优先考虑价值提取而不是人类福祉。虽然有许多可能的方法来更广泛地重新分配AI驱动的权力，但没有一个会默认出现：它们必须通过信托要求、公共提供和基于风险的分层访问等机制进行有意设计和治理。

缓解权力集中的方法可能面临来自既得利益的重大阻力。[^150]但有通向AI开发的路径，不需要在安全和集中权力之间做选择。通过现在建立正确的机构，我们可以确保AI的好处被广泛分享，同时其风险得到仔细管理。

### 新的治理和社会结构

我们当前的治理结构正在挣扎：它们反应缓慢，经常被特殊利益俘获，并且[越来越不被公众信任。](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx)然而，这不是放弃它们的理由——恰恰相反。一些机构可能需要更换，但更广泛地说，我们需要能够增强和补充我们现有结构的新机制，帮助它们在我们快速发展的世界中更好地运作。

我们制度弱点的大部分源于不是正式政府结构，而是退化的社会机构：我们发展共同理解、协调行动和进行有意义话语的系统。到目前为止，AI已经加速了这种退化，用生成的内容淹没我们的信息渠道，指向最具极化和分裂性的内容，并使区分真假变得更加困难。

但AI实际上可以帮助重建和加强这些社会机构。考虑三个关键领域：

首先，AI可以帮助恢复对我们认识论系统——我们知道什么是真实的方式——的信任。我们可以开发AI驱动的系统，追踪和验证信息的来源，从原始数据到分析再到结论。这些系统可以将密码学验证与复杂分析相结合，帮助人们理解不仅某事是否为真，而且我们如何知道它是真的。[^151]忠诚的AI助手可能被委托跟进细节，以确保它们经得起检验。

其次，AI可以实现新形式的大规模协调。我们最紧迫的许多问题——从气候变化到抗生素耐药性——根本上是协调问题。我们[困在对几乎每个人来说都比可能更糟糕的情况中](https://equilibriabook.com/)，因为没有个人或团体能够承担先行动的风险。AI系统可以通过建模复杂的激励结构、识别通向更好结果的可行路径，以及促进到达那里所需的信任建设和承诺机制来提供帮助。

也许最有趣的是，AI可以实现全新形式的社会话语。想象能够"与城市对话"[^152]——不只是查看统计数据，而是与处理和综合数百万居民观点、经历、需求和愿望的AI系统进行有意义的对话。或者考虑AI如何能够促进目前彼此错过要点的群体之间的真正对话，帮助各方更好地理解对方的实际关切和价值观，而不是他们对彼此的讽刺画。[^153]或者AI可以为人们甚至大群体之间的争议提供熟练、可信中立的调解（他们都可以直接和个别地与它互动！）当前的AI完全有能力做这项工作，但做这些事情的工具不会自己出现，或者通过市场激励出现。

这些可能性可能听起来很乌托邦，特别是考虑到AI目前在退化话语和信任方面的作用。但这正是为什么我们必须积极开发这些积极应用。通过关闭通向不可控AGI的大门，优先考虑增强人类能动性的AI，我们可以引导技术进步走向AI作为赋权、复原力和集体进步力量的未来。

[^124]: 话虽如此，远离三重交集不幸地不如人们希望的那样容易。在三个方面中的任何一个方面非常努力地推动能力往往会在其他方面增加它。特别是，创建一个极其通用和有能力的智能可能很难不让它容易变成自主的。一种方法是训练["短视"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia)系统，削弱规划能力。另一种是专注于工程纯["预言机"](https://arxiv.org/abs/1711.05541)系统，它们会回避回答面向行动的问题。

[^125]: 许多公司没有意识到他们最终也会被AGI取代，即使需要更长时间——如果他们意识到了，他们可能不会那么用力推那些大门！

[^126]: AI系统可能以更高效但不太易懂的方式交流，但维持人类理解应该优先考虑。

[^127]: 模块化、可解释AI的这个想法已经被几位研究人员详细开发；见例如Drexler的["综合AI服务"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf)模型，Dalrymple等人的["开放能动架构"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai)。虽然这样的系统可能需要比用大量计算训练的整体神经网络更多的工程努力，但这正是计算限制有帮助的地方——使更安全、更透明的路径也成为更实用的路径。

[^128]: 关于安全案例的一般情况，见[此手册](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16)。特别是关于AI，见[Wasil等人](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274)，[Clymer等人](https://arxiv.org/abs/2403.10462)，[Buhl等人](https://arxiv.org/abs/2410.21572)，和[Balesni等人](https://arxiv.org/abs/2411.03336)

[^129]: 我们实际上已经看到这种趋势，仅仅是由推理的高成本驱动：从更大模型中"蒸馏"出的更小、更专业的模型，能够在成本较低的硬件上运行。

[^130]: 我理解为什么那些对AI技术生态系统感到兴奋的人可能反对他们认为对其行业繁重的监管。但坦率地说，让我困惑的是，比如说，风险投资家为什么会想要允许向AGI和超级智能的失控发展。这些系统（和公司，当它们仍然在公司控制下时）将*把所有初创公司当作小食吃掉*。可能甚至比吃掉其他行业*更快*。任何投资于繁荣AI生态系统的人都应该优先确保AGI开发不会导致少数主导参与者的垄断。

[^131]: 正如经济学家和前Deepmind研究员迈克尔·韦伯[所说的](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/)，"我认为如果我们今天停止所有更大语言模型的开发，所以GPT-4和Claude以及其他的，它们是我们训练那种规模的最后一批——所以我们允许对那种规模的东西进行更多迭代和各种微调，但没有比那更大的，没有更大的进展——仅仅我们今天所拥有的我认为就足以推动20或30年的令人难以置信的经济增长。"

[^132]: 例如，DeepMind的alphafold系统仅使用了GPT-4的FLOP数的十万分之一。

[^133]: 自动驾驶汽车的困难在这里很重要：虽然名义上是一个狭窄任务，并且使用相对较小的AI系统以相当的可靠性实现，但广泛的现实世界知识和理解对于获得这样一个安全关键任务所需的可靠性水平是必要的。

[^134]: 例如，在给定的计算预算下，我们可能会看到GPAI模型在（比如）该预算的一半进行预训练，另一半用于在更狭窄的任务范围内训练非常高的能力。这将提供由接近人类的通用智能支撑的超人狭窄能力。

[^135]: 当前主导的对齐技术是"人类反馈强化学习"[(RLHF)](https://arxiv.org/abs/1706.03741)，使用人类反馈为AI模型的强化学习创建奖励/惩罚信号。这种技术和相关技术如[宪法AI](https://arxiv.org/abs/2212.08073)工作得出人意料地好（尽管它们缺乏稳健性，可能被适度努力规避）。此外，当前语言模型在常识推理方面通常足够胜任，不会犯愚蠢的道德错误。这在某种程度上是一个甜蜜点：足够聪明，能够理解人们想要什么（在可以定义的程度上），但不够聪明，无法策划复杂的欺骗或在出错时造成巨大伤害。

[^136]: 从长远来看，任何被开发的AI能力水平都可能扩散，因为它最终是软件，而且有用。我们需要有稳健的机制来防范此类系统构成的风险。但我们*现在没有*，所以我们必须非常谨慎地允许多少强大的AI模型扩散。

[^137]: 其中绝大多数是非同意的色情深度伪造，包括未成年人。

[^138]: 此类解决方案的许多成分存在，形式为"机器人还是不是"法律（在EU AI法案等中），[行业来源追踪技术](https://c2pa.org/)，[创新新闻聚合器](https://www.improvethenews.org/)，预测[聚合器](https://metaculus.com/)和市场等。

[^139]: 自动化浪潮可能不会遵循以前的模式，因为相对*高*技能任务，如优质写作、解释法律或提供医疗建议，可能与较低技能任务一样，甚至更容易受到自动化的影响。

[^140]: 有关AGI对工资影响的仔细建模，见[这里](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek)的报告，详细内容[这里](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0)，来自Anton Korinek和合作者。他们发现，随着工作的更多部分被自动化，生产率和工资都上升——到一定程度。一旦*太多*被自动化，生产率继续提高，但工资暴跌，因为人们被高效的AI整体取代。这就是为什么关闭大门如此有用：我们得到生产率而没有消失的人类工资。

[^141]: 有许多方法可以将AI用作并帮助构建"防御性"技术，以使保护和管理更加稳健。见描述这个"D/acc"议程的[这篇](https://vitalik.eth.limo/general/2025/01/05/dacc2.html)有影响力的帖子。

[^142]: 有些讽刺的是，美国曼哈顿计划可能对加速AGI时间表没什么作用——人力和财政对AI进步投资的表盘已经调到11了。主要结果将是激发中国的类似项目（中国在国家级基础设施项目方面表现出色），使限制AI风险的国际协议变得更加困难，并使美国的其他地缘政治对手如俄罗斯感到担忧。

[^143]: ["国家AI研究资源"](https://nairrpilot.org/)计划是这个方向上很好的当前步骤，应该扩展。

[^144]: 见技术产品中"开放"的各种含义和含义以及一些如何导致更多而非更少主导地位巩固的[这项分析](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807)。

[^145]: 美国的[国家AI研究资源](https://nairratdoe.ornl.gov/)计划和最近推出的[欧洲AI基金会](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/)是这个方向上有趣的步骤。

[^146]: 这里的挑战不是技术上的而是制度上的——我们迫切需要公共利益AI开发可能看起来像什么的现实世界例子和实验。

[^147]: 这违背了当前大型科技商业模式，需要法律行动和新规范。

[^148]: 只有一些政府能够这样做。一个更激进的想法是[这种类型的普遍基金，在所有人类的联合所有权下。](https://futureoflife.org/project/the-windfall-trust/)

[^149]: 有关这种情况的详细阐述，见关于AI忠诚度的[这篇论文](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338)。不幸的是，AI助手的默认轨迹可能是一个越来越不忠诚的轨迹。

[^150]: 有些讽刺的是，许多既得利益也面临AI支持的去权力化的风险；但对他们来说，除非和直到这个过程相当深入，否则可能很难察觉到这一点。

[^151]: 这个方向上一些有趣的努力由[c2pa联盟](https://c2pa.org/)在密码学验证方面代表；[Verity](https://www.improvethenews.org/)和[Ground news](https://ground.news/)在更好的新闻认识论方面；以及[Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com)和预测市场在将话语建立在可证伪的预测基础上。

[^152]: 见这个[迷人的试点项目](https://talktothecity.org/)。

[^153]: 见[Kialo](https://www.kialo-edu.com/)，以及[集体智慧项目](https://www.cip.org/)的努力，了解一些例子。

## 第十章 - 摆在我们面前的选择

为了维护人类的未来，我们必须选择关闭通往通用人工智能(AGI)和超级智能的大门。

人类上一次与其他会说话、会思考、会建造技术并能进行通用问题解决的心智共享地球，是在4万年前的冰河时代欧洲。那些其他的心智灭绝了，全部或部分是由于我们的努力。

我们现在正在重新进入这样的时代。我们文化和技术的最高成就——基于整个互联网信息共享而构建的数据集，以及拥有千亿元件、堪称我们有史以来最复杂技术的芯片——正在被结合起来，创造出先进的通用AI系统。

这些系统的开发者热衷于将其描绘为赋能人类的工具。确实，它们可以是这样的。但毫无疑问的是：我们目前的发展轨迹是要构建更加强大、目标导向、能够决策且通用能力更强的数字智能体。它们已经在广泛的智力任务上表现得与许多人类一样出色，正在快速改进，并且正在为自身的改进做出贡献。

除非这一轨迹发生改变或遇到意外阻碍，否则我们很快就会——以年为单位，而非几十年——拥有危险强大的数字智能。即使在*最好*的情况下，这些系统会带来巨大的经济效益（至少对我们中的一些人而言），但代价是社会的深刻颠覆，以及在我们所做的最重要事情上被机器取代：这些机器会替我们思考，替我们规划，替我们决策，替我们创造。我们会被宠坏，但只是被宠坏的孩子。更可能的是，这些系统会在我们做的积极*和*消极事情上都取代人类，包括剥削、操纵、暴力和战争。我们能在AI超级加强版的这些活动中幸存下来吗？最后，很有可能情况根本不会顺利：相对很快地，我们不仅在所做的事情上被取代，更会在我们*是什么*上被取代——作为文明和未来的建构者。去问问尼安德特人这是怎样的体验吧。也许我们一开始也会给他们提供一些额外的装饰品。

*我们不必这样做。*我们拥有了与人类竞争力相当的AI，没有必要构建我们*无法*与之竞争的AI。我们可以构建令人惊叹的AI工具，而无需构建一个后继物种。认为AGI和超级智能不可避免的观念是*伪装成命运的选择*。

通过施加一些严格的全球限制，我们可以将AI的通用能力保持在大约人类的水平，同时仍能收获计算机以我们无法做到的方式处理数据的好处，并自动化我们都不愿做的任务。这些仍会带来许多风险，但如果设计和管理得当，将为人类带来巨大福祉，从医学到研究再到消费产品。

实施限制需要国际合作，但所需的合作程度比人们想象的要少，而且这些限制仍会为庞大的AI和AI硬件产业留下充足空间，专注于增进人类福祉的应用，而非单纯追求权力。如果有了强有力的安全保障，并经过有意义的全球对话，我们决定走得更远，这个选择仍然属于我们。

人类必须*选择*关闭通往AGI和超级智能的大门。

让未来保持人类特色。

### 作者后记

感谢您花时间与我们一起探讨这个话题。

我写这篇文章，是因为作为一名科学家，我认为说出毫不掩饰的真相很重要，作为一个人，我认为我们必须迅速果断地行动，应对这个改变世界的问题：开发比人类更聪明的AI系统。

如果我们要以智慧应对这一非凡状况，我们必须准备好批判性地审视主流叙述——即AGI和超级智能"必须"被构建来确保我们的利益，或者这是"不可避免的"且无法阻止。这些叙述让我们失去了力量，无法看到摆在我们面前的替代道路。

我希望您能与我一起，面对鲁莽行为时呼吁谨慎，面对贪婪时呼吁勇气。

我希望您能与我一起呼吁一个人类的未来。

*——安东尼*

![Anthony Aguirre signature](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAnthony-Aguirre-signature-300x84.png&w=3840&q=75)

## 附录

补充信息，包括——算力核算的技术细节、"关闭大门"的实施示例、严格的通用人工智能责任制度详情，以及分层式的通用人工智能安全与安保标准。

### 附录A：算力核算技术细节

要实现有意义的基于算力的控制，需要一套详细的方法来计算训练和推理中使用的总算力，既要有"基准事实"，也要有良好的近似方法。以下是如何在技术层面统计"基准事实"的示例。

**定义：**

*算力因果图：* 对于AI模型的给定输出O，存在一组数字计算，改变这些计算的结果可能会改变O。（这应该保守假设，即应该有明确的理由相信某项计算独立于既在时间上较早发生又具有物理潜在因果效应路径的前序计算。）这包括AI模型在推理过程中进行的计算，以及涉及输入、数据准备和模型训练的计算。由于其中任何一个本身都可能是AI模型的输出，因此这是递归计算的，在人类对输入提供了重大改变的地方截止。

*训练算力：* 神经网络算力因果图所涉及的总算力（以FLOP或其他单位计），包括数据准备、训练、微调以及任何其他计算。

*输出算力：* 给定AI输出的算力因果图中的总算力，包括所有神经网络（及其训练算力）和该输出涉及的其他计算。

*推理算力速率：* 在一系列输出中，输出算力之间的变化率（以FLOP/s或其他单位计），即用于产生下一个输出的算力除以输出之间的时间间隔。

**示例和近似方法：**

- 对于在人类创建数据上训练的单个神经网络，训练算力就是通常报告的总训练算力。
- 对于以稳定速率进行推理的此类神经网络，推理算力速率大约等于执行推理的计算集群的总计算速度（以FLOP/s计）。
- 对于模型微调，完整模型的训练算力由未微调模型的训练算力加上微调期间的计算以及准备微调所用数据的计算组成。
- 对于蒸馏模型，完整模型的训练算力包括蒸馏模型和用于提供合成数据或其他训练输入的更大模型的训练。
- 如果训练了多个模型，但基于人类判断丢弃了许多"试验"，这些不计入保留模型的训练或输出算力。

### 附录B："关闭大门"的实施示例

**实施示例：** 以下是在训练限制为10<sup>27</sup> FLOP、推理限制为10<sup>20</sup> FLOP/s（运行AI）的情况下，"关闭大门"如何运作的一个示例：

**1. 暂停：** 出于国家安全考虑，美国行政部门要求所有总部设在美国、在美国开展业务或使用美国制造芯片的公司，停止任何可能超过10<sup>27</sup> FLOP训练算力限制的新AI训练运行。美国应与其他拥有AI开发的国家开始讨论，强烈鼓励它们采取类似步骤，并表明如果它们选择不遵守，美国的暂停可能会解除。

**2. 美国监督和许可：** 通过行政命令或现有监管机构的行动，美国要求在（比如说）一年内：

- 美国境内运营公司进行的所有估计超过10<sup>25</sup> FLOP的AI训练运行必须在美国监管机构维护的数据库中注册。（注：2023年美国AI行政命令的稍弱版本已包含这一点，要求超过10<sup>26</sup> FLOP的模型进行注册，现已被撤销。）
- 所有在美国运营或与美国政府开展业务的AI相关硬件制造商必须遵守其专用硬件及驱动软件的一系列要求。（许多这些要求可以通过现有硬件的软件和固件更新来构建，但长期和稳健的解决方案需要对后续硬件世代进行更改。）其中包括要求如果硬件是能够执行10<sup>18</sup> FLOP/s计算的高速互连集群的一部分，则需要更高级别的验证，包括由接收遥测数据并请求执行额外计算的远程"管理器"定期许可。
- 保管人向维护美国数据库的机构报告其硬件执行的总计算量。
- 逐步引入更严格的要求，以实现更安全、更灵活的监督和许可。

**3. 国际监督：**

- 美国、中国和任何其他拥有先进芯片制造能力的国家协商国际协议。
- 该协议创建一个新的国际机构，类似于国际原子能机构，负责监督AI训练和执行。
- 签署国必须要求其国内AI硬件制造商遵守至少与美国实施的要求同样严格的一系列要求。
- 保管人现在需要向其母国的机构以及国际机构内的新办公室报告AI计算数据。
- 强烈鼓励其他国家加入现有国际协议：签署国的出口管制限制非签署国获得高端硬件，而签署国可以获得管理其AI系统的技术支持。

**4. 国际验证和执行：**

- 硬件验证系统更新，既向原始保管人报告计算使用情况，也直接向国际机构办公室报告。
- 该机构通过与国际协议签署国的讨论，就计算限制达成一致，然后在签署国中具有法律效力。
- 与此同时，可能制定一套国际标准，要求超过计算阈值（但低于限制）的AI训练和运行必须遵守这些标准。
- 该机构可以在必要时（比如为了补偿更好的算法等）降低计算限制。或者，如果被认为是安全和可取的（比如达到可证明的安全保证水平），提高计算限制。

### 附录C：严格的通用人工智能责任制度详情

**严格的通用人工智能责任制度详情**

- 创建和运营具有高度通用性、能力和自主性的先进AI系统被视为"异常危险"活动。
- 因此，对于训练和运营此类系统的默认责任级别是对模型或其输出/行为造成的任何损害承担严格的连带责任（或其非美国等价物）。
- 在严重过失或故意不当行为的情况下，将对高管和董事会成员施加个人责任。这应该包括对最恶劣案例的刑事处罚。
- 有许多安全港，在这些情况下责任恢复到人员和公司通常适用的默认（在美国为过错责任）责任。
	- 在某个算力阈值以下训练和运营的模型（该阈值至少比上述限制低10倍）。
	- "弱"AI（大致低于其预期任务的人类专家水平）和/或
	- "狭窄"AI（具有固定且相当有限的任务和操作范围，专门为此设计和训练）和/或
	- "被动"AI（即使在适度修改下，在没有直接人类参与和控制的情况下采取行动或执行复杂多步骤任务的能力非常有限）。
	- 保证安全、可靠和可控的AI（可证明安全，或风险分析表明预期危害水平可忽略不计）。
- 可以基于AI开发者准备并经机构或机构认证的审计员批准的[安全案例](https://arxiv.org/abs/2410.21572)来申请安全港。要基于算力申请安全港，开发者只需提供总训练算力和最大推理速率的可信估计。
- 立法将明确概述在什么情况下对开发具有高公共危害风险的AI系统进行禁令救济是适当的。
- 公司联盟与非政府组织和政府机构合作，应制定标准和规范，定义这些术语、监管机构应如何授予安全港、AI开发者应如何制定安全案例，以及在未主动申请安全港的情况下法院应如何解释责任。

### 附录D：分层式的通用人工智能安全与安保标准

**分层式的通用人工智能安全与安保标准**

| 风险层级 | 触发条件 | 训练要求 | 部署要求 |
| --- | --- | --- | --- |
| RT-0 | AI在自主性、通用性和智能方面都较弱 | 无 | 无 |
| RT-1 | AI在自主性、通用性和智能中的一项较强 | 无 | 基于风险和用途，可能需要模型使用地国家当局批准的安全案例 |
| RT-2 | AI在自主性、通用性和智能中的两项较强 | 在对开发者有管辖权的国家当局处注册 | 将重大危害风险控制在授权水平以下的安全案例，加上模型使用地国家当局批准的独立安全审计（包括黑盒和白盒红队测试） |
| RT-3 | 在自主性、通用性和智能方面都较强的通用人工智能 | 对开发者有管辖权的国家当局预先批准安全和安保计划 | 保证将重大危害风险控制在授权水平以下的安全案例以及必要规范，包括网络安全、可控性、不可移除的终止开关、与人类价值观的对齐以及对恶意使用的稳健性。 |
| RT-4 | 任何也超过10<sup>27</sup> FLOP训练或10<sup>20</sup> FLOP/s推理的模型 | 在国际同意解除算力限制前禁止 | 在国际同意解除算力限制前禁止 |

基于算力阈值以及高自主性、通用性和智能组合的风险分类和安全/安保标准，层级如下：

- *强自主性*适用于系统能够执行或可以轻易被改造为执行多步骤任务和/或采取与现实世界相关的复杂行动，而无需重大人类监督或干预的情况。例子：自动驾驶汽车和机器人；金融交易机器人。反例：GPT-4；图像分类器
- *强通用性*表示应用范围广泛，执行模型未经刻意和专门训练的任务，以及显著的学习新任务能力。例子：GPT-4；mu-zero。反例：AlphaFold；自动驾驶汽车；图像生成器
- *强智能*对应于在模型表现最佳的任务上达到人类专家水平的性能（对于通用模型，则是在广泛任务范围内）。例子：AlphaFold；mu-zero；o3。反例：GPT-4；Siri

### 致谢

感谢为《保持人类的未来》做出贡献的几位朋友。

本作品反映了作者个人观点，不应被视为生命未来研究所的官方立场（尽管两者是兼容的；其官方立场请参见[此页面](https://futureoflife.org/our-position-on-ai/)），也不代表作者所属的任何其他组织的观点。

我要感谢Mark Brakel、Ben Eisenpress、Anna Hehir、Carlos Gutierrez、Emilia Javorsky、Richard Mallah、Jordan Scharnhorst、Elyse Fulcher、Max Tegmark和Jaan Tallinn等朋友对手稿的宝贵意见；感谢Tim Schrier在参考文献方面的协助；感谢Taylor Jones和Elyse Fulcher对图表的美化工作。

本作品在创作过程中有限地使用了生成式AI模型（Claude和ChatGPT）进行一些编辑和红队测试。按照创意作品AI参与程度的成熟标准，这部作品大概可以评为3/10分。（实际上并不存在这样的标准！但应该有。）

我们非常感谢[Julius Odai](https://www.linkedin.com/in/julius-odai/)制作了本文的网页版本，让阅读和浏览这篇文章成为了一种愉快的体验。Julius是一位技术专家，也是BlueDot Impact AI治理课程的最新学员。