# Zachować Przyszłość dla Ludzkości

Ten esej przedstawia argumenty za tym, dlaczego i jak powinniśmy zamknąć bramy przed AGI i superinteligencją, oraz co powinniśmy zbudować w zamian.

Jeśli interesują Cię tylko kluczowe wnioski, przejdź do Streszczenia wykonawczego. Następnie rozdziały 2-5 zapewnią podstawowe informacje na temat rodzajów systemów AI omawianych w eseju. Rozdziały 5-7 wyjaśniają, dlaczego możemy spodziewać się rychłego pojawienia AGI i co może się stać, gdy to nastąpi. Wreszcie rozdziały 8-9 przedstawiają konkretną propozycję zapobieżenia budowaniu AGI.

[Pobierz PDF](https://keepthefuturehuman.ai/wp-content/uploads/2025/03/Keep_the_Future_Human__AnthonyAguirre__5March2025.pdf)

Całkowity czas czytania: 2-3 godziny

## Streszczenie wykonawcze

Przegląd najważniejszych punktów eseju. Jeśli masz mało czasu, poznaj wszystkie główne argumenty w zaledwie 10 minut.

Dramatyczne postępy w dziedzinie sztucznej inteligencji w ostatniej dekadzie (w przypadku AI o wąskim zastosowaniu) i w ostatnich latach (w przypadku AI ogólnego przeznaczenia) przekształciły AI z niszowej dziedziny akademickiej w podstawową strategię biznesową wielu największych światowych korporacji, z setkami miliardów dolarów rocznych inwestycji w techniki i technologie rozwijające możliwości AI.

Stoimy teraz w krytycznym momencie. Gdy możliwości nowych systemów AI zaczynają dorównywać i przewyższać ludzkie w wielu domenach poznawczych, ludzkość musi zdecydować: jak daleko idziemy i w którym kierunku?

AI, jak każda technologia, zaczynało od celu poprawy sytuacji swojego twórcy. Ale nasza obecna trajektoria i domyślny wybór to niepohamowany wyścig ku coraz potężniejszym systemom, napędzany bodźcami ekonomicznymi kilku gigantycznych firm technologicznych dążących do automatyzacji dużych obszarów obecnej działalności gospodarczej i ludzkiej pracy. Jeśli ten wyścig będzie trwał znacznie dłużej, zwycięzca jest nieunikniony: samo AI – szybsza, mądrzejsza, tańsza alternatywa dla ludzi w naszej gospodarce, naszym myśleniu, naszych decyzjach, a ostatecznie w kontroli nad naszą cywilizacją.

Ale możemy dokonać innego wyboru: poprzez nasze rządy możemy przejąć kontrolę nad procesem rozwoju AI, aby narzucić jasne ograniczenia, linie, których nie przekroczymy, i rzeczy, których po prostu nie będziemy robić – tak jak zrobiliśmy w przypadku technologii jądrowych, broni masowego rażenia, broni kosmicznej, procesów niszczących środowisko, bioinżynierii ludzi i eugeniki. Co najważniejsze, możemy zapewnić, że AI pozostanie narzędziem wzmacniającym ludzi, a nie nowym gatunkiem, który nas zastąpi i ostatecznie wyprze.

Ten esej argumentuje, że powinniśmy *zachować przyszłość dla człowieka* poprzez zamknięcie "Bram" dla mądrzejszej od człowieka, autonomicznej AI ogólnego przeznaczenia – czasami nazywanej "AGI" – a szczególnie dla wersji znacznie przewyższającej człowieka, czasami nazywanej "superinteligencją". Zamiast tego powinniśmy skupić się na potężnych, godnych zaufania narzędziach AI, które mogą wzmocnić jednostki i przełomowo poprawić zdolności ludzkich społeczeństw do robienia tego, co robią najlepiej. Struktura tego argumentu wygląda następująco w skrócie.

### AI jest inne

Systemy AI różnią się fundamentalnie od innych technologii. Podczas gdy tradycyjne oprogramowanie wykonuje precyzyjne instrukcje, systemy AI uczą się, jak osiągać cele bez wyraźnego instruowania, jak to robić. To czyni je potężnymi: jeśli możemy czysto zdefiniować cel lub metrykę sukcesu, w większości przypadków system AI może nauczyć się go osiągać. Ale to także czyni je z natury nieprzewidywalnymi: nie możemy w sposób niezawodny określić, jakie działania podejmą, aby osiągnąć swoje cele.

Są także w dużej mierze niewyjaśnialne: choć częściowo są kodem, to głównie stanowią ogromny zbiór nieczytelnych liczb – "wag" sieci neuronowych – których nie można przeanalizować; nie jesteśmy dużo lepsi w rozumieniu ich wewnętrznego działania niż w rozpoznawaniu myśli przez zaglądanie do biologicznego mózgu.

Ten podstawowy sposób trenowania cyfrowych sieci neuronowych szybko wzrasta w złożoności. Najmocniejsze systemy AI są tworzone poprzez masywne eksperymenty obliczeniowe, wykorzystujące specjalistyczny sprzęt do trenowania sieci neuronowych na ogromnych zbiorach danych, które są następnie wzbogacane o narzędzia programowe i nadbudowę.

Doprowadziło to do stworzenia bardzo potężnych narzędzi do tworzenia i przetwarzania tekstu i obrazów, wykonywania rozumowania matematycznego i naukowego, agregowania informacji oraz interaktywnego przeszukiwania ogromnego zasobu ludzkiej wiedzy.

Niestety, choć rozwój potężniejszych, bardziej godnych zaufania narzędzi technologicznych to właśnie to, co *powinniśmy* robić i czego niemal wszyscy chcą i mówią, że chcą, to nie jest to trajektoria, na której faktycznie się znajdujemy.

### AGI i superinteligencja

Od zarania tej dziedziny badania nad AI skupiały się zamiast tego na innym celu: Sztucznej Inteligencji Ogólnej. To skupienie stało się teraz celem tytanicznych firm przewodzących rozwojowi AI.

Czym jest AGI? Często jest niejasno definiowane jako "AI na poziomie ludzkim", ale to problematyczne: którzy ludzie i w jakich zdolnościach jest na poziomie ludzkim? A co z nadludzkimi zdolnościami, które już posiada? Bardziej użytecznym sposobem rozumienia AGI jest przecięcie trzech kluczowych właściwości: wysokiej **A**utonomii (niezależności działania), wysokiej **O**gólności (szerokiego zakresu i adaptowalności) oraz wysokiej **I**nteligencji (kompetencji w zadaniach poznawczych). Obecne systemy AI mogą być wysoce zdolne, ale wąskie, lub ogólne, ale wymagające stałego ludzkiego nadzoru, lub autonomiczne, ale ograniczone w zakresie.

Pełne A-O-I łączyłoby wszystkie trzy właściwości na poziomach dorównujących lub przewyższających najlepsze ludzkie możliwości. Co kluczowe, to właśnie ta kombinacja czyni ludzi tak skutecznymi i tak odmiennymi od obecnego oprogramowania; to także umożliwiłoby hurtowe zastąpienie ludzi przez systemy cyfrowe.

Choć ludzka inteligencja jest wyjątkowa, wcale nie stanowi ograniczenia. Sztuczne systemy "superinteligentne" mogłyby działać setki razy szybciej, przetwarzać znacznie więcej danych i utrzymywać ogromne ilości "na uwadze" jednocześnie, oraz tworzyć agregaty znacznie większe i skuteczniejsze niż zbiory ludzi. Mogłyby wyprze nie jednostki, ale firmy, narody czy naszą cywilizację jako całość.

### Jesteśmy u progu

Istnieje silny naukowy konsensus, że AGI jest *możliwe*. AI już przewyższa ludzkie wyniki w wielu ogólnych testach zdolności intelektualnych, w tym ostatnio w rozumowaniu i rozwiązywaniu problemów wysokiego poziomu. Opóźnione zdolności – takie jak ciągłe uczenie się, planowanie, samoświadomość i oryginalność – wszystkie istnieją na pewnym poziomie w obecnych systemach AI, a znane techniki mogące je wszystkie poprawić już istnieją.

Podczas gdy jeszcze kilka lat temu wielu badaczy postrzegało AGI jako odległe o dekady, obecnie dowody na krótkie terminy osiągnięcia AGI są silne:

- Empirycznie zweryfikowane "prawa skalowania" łączą wkład obliczeniowy ze zdolnościami AI, a korporacje są na trasie do zwiększenia wkładu obliczeniowego o rzędy wielkości w nadchodzących latach. Zasoby ludzkie i finansowe poświęcone rozwojowi AI równają się teraz tym z tuzina Projektów Manhattan i kilku Projektów Apollo.
- Korporacje AI i ich liderzy publicznie i prywatnie wierzą, że AGI (według jakiejś definicji) jest osiągalne w ciągu kilku lat. Te firmy mają informacje, których społeczeństwo nie ma, włącznie z posiadaniem następnej generacji systemów AI.
- Eksperci prognozujący ze sprawdzonym doświadczeniem przypisują 25% prawdopodobieństwo przybyciu AGI (według jakiejś definicji) w ciągu 1-2 lat i 50% dla 2-5 lat (patrz prognozy Metaculus dla ['słabego'](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) i ['pełnego'](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/) AGI).
- Autonomia (włącznie z dalekozasięgowym elastycznym planowaniem) pozostaje w tyle w systemach AI, ale główne firmy skupiają teraz swoje ogromne zasoby na rozwijaniu autonomicznych systemów AI i nieformalnie nazwały 2025 ["rokiem agenta."](https://techinformed.com/2025-informed-the-year-of-agentic-ai/)
- AI coraz bardziej przyczynia się do własnego ulepszania. Gdy systemy AI będą tak kompetentne jak ludzie-badacze AI w prowadzeniu badań nad AI, zostanie osiągnięty krytyczny próg szybkiego postępu ku znacznie potężniejszym systemom AI i prawdopodobnie doprowadzi to do lawinowego wzrostu możliwości AI. (Można argumentować, że ta lawina już się zaczęła.)

Idea, że mądrzejsze od człowieka AGI jest odległe o dekady lub więcej, po prostu nie jest już do utrzymania dla ogromnej większości ekspertów w tej dziedzinie. Nieporozumienia dotyczą teraz tego, ile miesięcy lub lat to zajmie, jeśli pozostaniemy na tym kursie. Podstawowe pytanie, przed którym stoimy, to: czy powinniśmy?

### Co napędza wyścig ku AGI

Wyścig ku AGI jest napędzany przez wiele sił, z których każda czyni sytuację bardziej niebezpieczną. Główne firmy technologiczne postrzegają AGI jako ostateczną technologię automatyzacji – nie tylko wspomagającą pracowników, ale w dużej mierze lub całkowicie ich zastępującą. Dla firm nagroda jest ogromna: możliwość przechwycenia znacznej części światowej rocznej produkcji gospodarczej wynoszącej 100 bilionów dolarów poprzez automatyzację kosztów ludzkiej pracy.

Narody czują się zmuszone do przyłączenia się do tego wyścigu, publicznie powołując się na przywództwo gospodarcze i naukowe, ale prywatnie postrzegając AGI jako potencjalną rewolucję w sprawach wojskowych porównywalną z bronią jądrową. Strach, że rywale mogą uzyskać decydującą przewagę strategiczną, tworzy klasyczną dynamikę wyścigu zbrojeń.

Ci dążący do superinteligencji często przywołują wielkie wizje: wyleczenie wszystkich chorób, odwrócenie starzenia, osiągnięcie przełomów w energii i podróżach kosmicznych czy stworzenie nadludzkich zdolności planistycznych.

Mniej przychylnie, tym co napędza wyścig, jest władza. Każdy uczestnik – czy to firma, czy kraj – wierzy, że inteligencja równa się władzy i że będzie najlepszym strażnikiem tej władzy.

Argumentuję, że te motywacje są rzeczywiste, ale fundamentalnie błędne: AGI *wchłonie* i *będzie szukać* władzy zamiast jej udzielać; technologie stworzone przez AI będą *także* silnie obosieczne, a tam gdzie korzystne, można je stworzyć za pomocą narzędzi AI i bez AGI; i nawet o ile AGI i jego produkty pozostaną pod kontrolą, ta dynamika wyścigowa – zarówno korporacyjna, jak i geopolityczna – sprawia, że wielkoskalowe zagrożenia dla naszego społeczeństwa są niemal nieuniknione, chyba że zostaną zdecydowanie przerwane.

### AGI i superinteligencja stanowią dramatyczne zagrożenie dla cywilizacji

Pomimo swojej atrakcyjności, AGI i superinteligencja stwarzają dramatyczne zagrożenia dla cywilizacji poprzez wiele wzajemnie wzmacniających się ścieżek:

*Koncentracja władzy:* nadludzka AI mogłaby pozbawić władzy ogromną większość ludzkości poprzez wchłonięcie ogromnych obszarów działalności społecznej i gospodarczej w systemy AI prowadzone przez garstką gigantycznych firm (które z kolei mogą zostać przejęte przez rządy lub faktycznie je przejąć).

*Masowe zakłócenia:* masowa automatyzacja większości prac opartych na poznaniu, zastąpienie naszych obecnych systemów epistemicznych i wdrożenie ogromnej liczby aktywnych nieludzkich agentów wywróciłoby większość naszych obecnych systemów cywilizacyjnych w stosunkowo krótkim czasie.

*Katastrofy:* poprzez rozprzestrzenienie zdolności – potencjalnie ponad ludzki poziom – do tworzenia nowych technologii wojskowych i destrukcyjnych i odłączenie jej od systemów społecznych i prawnych ugruntowujących odpowiedzialność, katastrofy fizyczne z broni masowego rażenia stają się dramatycznie bardziej prawdopodobne.

*Geopolityka i wojna:* główne światowe mocarstwa nie będą bezczynnie siedziały, jeśli poczują, że technologia mogąca zapewnić "decydującą przewagę strategiczną" jest rozwijana przez ich przeciwników.

*Wymknięcie się spod kontroli i utrata kontroli:* Chyba że zostanie to specjalnie zapobieżone, nadludzka AI będzie miała wszelkie bodźce do dalszego ulepszania siebie i mogłaby znacznie przewyższyć ludzi w szybkości, przetwarzaniu danych i wyrafinowaniu myślenia. Nie ma znaczącego sposobu, w jaki moglibyśmy kontrolować taki system. Takie AI nie udzieli władzy ludziom; my udzielimy władzy jemu, albo ono ją przejmie.

Wiele z tych zagrożeń pozostaje nawet jeśli techniczny problem "wyrównania" – zapewnienie, że zaawansowane AI niezawodnie robi to, co ludzie chcą, żeby robiło – zostanie rozwiązany. AI przedstawia ogromne wyzwanie w kwestii tego, jak będzie zarządzane, a bardzo wiele aspektów tego zarządzania staje się niesamowicie trudne lub nierozwiązywalne, gdy ludzka inteligencja zostanie przekroczona.

Najbardziej fundamentalnie, rodzaj nadludzkiej AI ogólnego przeznaczenia obecnie rozwijany miałby z natury cele, sprawczość i zdolności przewyższające nasze własne. Byłby z natury niekontrolowalny – jak możemy kontrolować coś, czego nie możemy ani zrozumieć, ani przewidzieć? Nie byłby technologicznym narzędziem do ludzkiego użytku, ale drugim gatunkiem inteligencji na Ziemi obok naszego. Gdyby pozwolono mu rozwijać się dalej, stanowiłby nie tylko drugi gatunek, ale gatunek zastępczy.

Być może traktowałby nas dobrze, być może nie. Ale przyszłość należałaby do niego, nie do nas. Era ludzka dobiegłaby końca.

### To nie jest nieuniknione; ludzkość może bardzo konkretnie zdecydować, aby nie budować swojego następcy.

Stworzenie nadludzkiej AGI jest dalekie od nieuniknionego. Możemy temu zapobiec poprzez skoordynowany zestaw środków zarządzania:

Po pierwsze, potrzebujemy solidnej księgowości i nadzoru nad mocą obliczeniową AI ("compute"), która jest fundamentalnym czynnikiem umożliwiającym i dźwignią do zarządzania wielkoskalowymi systemami AI. To z kolei wymaga standaryzowanego pomiaru i raportowania całkowitej mocy obliczeniowej używanej w trenowaniu modeli AI i ich uruchamianiu, oraz technicznych metod liczenia, certyfikowania i weryfikowania używanej mocy obliczeniowej.

Po drugie, powinniśmy wprowadzić twarde limity na moc obliczeniową AI, zarówno dla treningu, jak i operacji; zapobiegają one AI byciu zarówno zbyt potężną, jak i działaniu zbyt szybko. Te limity mogą być wprowadzone zarówno poprzez wymagania prawne, jak i środki bezpieczeństwa oparte na sprzęcie wbudowane w chipy specjalizowane dla AI, analogiczne do funkcji bezpieczeństwa w nowoczesnych telefonach. Ponieważ specjalistyczny sprzęt AI jest wytwarzany przez tylko garstką firm, weryfikacja i egzekwowanie są wykonalne poprzez istniejący łańcuch dostaw.

Po trzecie, potrzebujemy zwiększonej odpowiedzialności za najniebezpieczniejsze systemy AI. Ci rozwijający AI łączące wysoką autonomię, szeroką ogólność i nadrzędną inteligencję powinni ponosić ścisłą odpowiedzialność za szkody, podczas gdy bezpieczne przystanie od tej odpowiedzialności zachęcałoby do rozwoju bardziej ograniczonych i kontrolowalnych systemów.

Po czwarte, potrzebujemy regulacji wielopoziomowej opartej na poziomach ryzyka. Najbardziej zdolne i niebezpieczne systemy wymagałyby rozległych gwarancji bezpieczeństwa i kontrolowalności przed rozwojem i wdrożeniem, podczas gdy mniej potężne lub bardziej wyspecjalizowane systemy podlegałyby proporcjonalnemu nadzorowi. Te ramy regulacyjne powinny ostatecznie działać zarówno na poziomie krajowym, jak i międzynarodowym.

To podejście – ze szczegółową specyfikacją podaną w pełnym dokumencie – jest praktyczne: choć potrzebna będzie międzynarodowa koordynacja, weryfikacja i egzekwowanie mogą działać poprzez małą liczbę firm kontrolujących łańcuch dostaw specjalistycznego sprzętu. Jest także elastyczne: firmy wciąż mogą innowować i czerpać zyski z rozwoju AI, tylko z jasnymi limitami na najniebezpieczniejsze systemy.

Długoterminowe ograniczenie władzy i ryzyka AI wymagałoby międzynarodowych porozumień opartych zarówno na własnym, jak i wspólnym interesie, tak jak kontrola proliferacji broni jądrowej robi to teraz. Ale możemy zacząć natychmiast od zwiększonego nadzoru i odpowiedzialności, budując ku bardziej kompleksowemu zarządzaniu.

Kluczowym brakującym składnikiem jest polityczna i społeczna wola przejęcia kontroli nad procesem rozwoju AI. Źródłem tej woli, jeśli przyjdzie na czas, będzie sama rzeczywistość – to znaczy, z powszechnego uświadomienia sobie prawdziwych implikacji tego, co robimy.

### Możemy zaprojektować AI Narzędziowe, aby wzmocnić ludzkość

Zamiast dążyć do niekontrolowalnej AGI, możemy rozwijać potężne "AI Narzędziowe", które zwiększa ludzkie zdolności pozostając pod znaczącą ludzką kontrolą. Systemy AI Narzędziowego mogą być niezwykle zdolne, unikając jednocześnie niebezpiecznego potrójnego przecięcia wysokiej autonomii, szerokiej ogólności i nadludzkiej inteligencji, o ile zaprojektujemy je tak, aby były kontrolowalne na poziomie współmiernym z ich zdolnościami. Mogą też być łączone w wyrafinowane systemy zachowujące ludzki nadzór przy dostarczaniu transformacyjnych korzyści.

AI Narzędziowe może zrewolucjonizować medycynę, przyspieszyć odkrycia naukowe, ulepszyć edukację i poprawić procesy demokratyczne. Gdy właściwie zarządzane, może uczynić ludzkich ekspertów i instytucje bardziej skutecznymi zamiast ich zastępować. Choć takie systemy będą wciąż wysoce destrukcyjne i wymagają ostrożnego zarządzania, zagrożenia, które stwarzają, różnią się fundamentalnie od AGI: to zagrożenia, którymi możemy zarządzać, jak te z innych potężnych technologii, nie egzystencjalne zagrożenia dla ludzkiej sprawczości i cywilizacji. I kluczowo, gdy mądrze rozwijane, narzędzia AI mogą pomagać ludziom zarządzać potężną AI i radzić sobie z jej skutkami.

To podejście wymaga przemyślenia zarówno sposobu rozwoju AI, jak i dystrybucji jego korzyści. Nowe modele publicznego i non-profit rozwoju AI, solidne ramy regulacyjne i mechanizmy szerszej dystrybucji korzyści ekonomicznych mogą pomóc zapewnić, że AI wzmacnia ludzkość jako całość zamiast koncentrować władzę w kilku rękach. Samo AI może pomóc budować lepsze instytucje społeczne i zarządzające, umożliwiając nowe formy koordynacji i dyskursu wzmacniające zamiast podważające ludzkie społeczeństwo. Establishmenty bezpieczeństwa narodowego mogą wykorzystać swoją ekspertyzę, aby uczynić systemy narzędzi AI rzeczywiście bezpiecznymi i godnymi zaufania oraz prawdziwym źródłem obrony, a także narodowej potęgi.

Możemy ostatecznie wybrać rozwój jeszcze potężniejszych i bardziej suwerennych systemów, które są mniej jak narzędzia, a – możemy mieć nadzieję – bardziej jak mądrzy i potężni dobroczyńcy. Ale powinniśmy to robić dopiero po tym, jak rozwinęliśmy naukowe zrozumienie i zdolność zarządzania, aby zrobić to bezpiecznie. Taka monumentalna i nieodwracalna decyzja powinna być podjęta świadomie przez ludzkość jako całość, nie domyślnie w wyścigu między firmami technologicznymi a narodami.

### W ludzkich rękach

Ludzie chcą dobra, które płynie z AI: użytecznych narzędzi, które ich wzmacniają, doładowują możliwości ekonomiczne i wzrost oraz obiecują przełomy w nauce, technologii i edukacji. Dlaczego nie mieliby? Ale gdy zapytani, przytłaczające większości społeczeństwa [chcą wolniejszego i bardziej ostrożnego rozwoju AI](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation) i nie chcą mądrzejszej od człowieka AI, która zastąpi ich w pracy i gdzie indziej, wypełni ich kulturę i przestrzeń informacyjną treścią nie-ludzką, skoncentruje władzę w maleńkim zestawie korporacji, stworzy skrajne wielkoskalowe globalne zagrożenia i ostatecznie zagrozi pozbawieniu władzy lub zastąpieniu ich gatunku. Dlaczego mieliby?

*Możemy* mieć jedno bez drugiego. Zaczyna się od decyzji, że nasze przeznaczenie nie leży w rzekomej nieuchronności jakiejś technologii ani w rękach kilku CEO w Dolinie Krzemowej, ale w reszcie naszych rąk, jeśli je w nie weźmiemy. Zamknijmy Bramy i zachowajmy przyszłość dla człowieka.

## Rozdział 1 - Wprowadzenie

To, jak odpowiemy na perspektywę powstania AI przewyższającej człowieka, jest najważniejszą kwestią naszych czasów. Ten esej przedstawia drogę naprzód.

Możemy znajdować się u kresu ery człowieka.

W ciągu ostatnich dziesięciu lat rozpoczął się proces, który jest wyjątkowy w historii naszego gatunku. Jego konsekwencje będą w znacznej mierze determinować przyszłość ludzkości. Począwszy od około 2015 roku, badaczom udało się opracować *wąską* sztuczną inteligencję (AI) – systemy, które potrafią wygrywać w grach takich jak Go, rozpoznawać obrazy i mowę, i tak dalej, lepiej niż jakikolwiek człowiek.[^1]

To niesamowity sukces, który przynosi niezwykle użyteczne systemy i produkty wzmacniające możliwości ludzkości. Ale wąska sztuczna inteligencja nigdy nie była prawdziwym celem tej dziedziny. Celem było raczej stworzenie AI *ogólnego* przeznaczenia, szczególnie systemów często nazywanych „sztuczną inteligencją ogólną" (AGI) lub „superinteligencją", które są jednocześnie równie dobre lub lepsze od ludzi w niemal *wszystkich* zadaniach, podobnie jak AI jest obecnie nadludzka w Go, szachach, pokerze, wyścigach dronów itd. To deklarowany cel wielu głównych firm zajmujących się AI.[^2]

*Te wysiłki również odnoszą sukces.* Systemy AI ogólnego przeznaczenia takie jak ChatGPT, Gemini, Llama, Grok, Claude i Deepseek, oparte na masywnych obliczeniach i górach danych, osiągnęły równość z przeciętnymi ludźmi w szerokiej gamie zadań, a nawet dorównują ludzkim ekspertom w niektórych dziedzinach. Teraz inżynierowie AI w niektórych z największych firm technologicznych ścigają się, by pchnąć te gigantyczne eksperymenty w zakresie inteligencji maszynowej na kolejne poziomy, na których dorównują, a następnie przewyższają pełen zakres ludzkich możliwości, ekspertyzy i autonomii.

*To jest nieuchronne.* W ciągu ostatnich dziesięciu lat szacunki ekspertów dotyczące tego, jak długo to potrwa – jeśli będziemy kontynuować obecny kurs – spadły z dekad (lub stuleci) do pojedynczych lat.

Ma to również epochalne znaczenie i wiąże się z transcendentnym ryzykiem. Zwolennicy AGI postrzegają ją jako pozytywną transformację, która rozwiąże problemy naukowe, wyleczy choroby, opracuje nowe technologie i zautomatyzuje harówkę. I AI z pewnością mogłaby pomóc w osiągnięciu wszystkich tych rzeczy – rzeczywiście już to robi. Ale przez dziesięciolecia wielu wnikliwych myślicieli, od Alana Turinga przez Stephena Hawkinga po współczesnych Geoffreya Hintona i Yoshuę Bengio[^3], wydawało ostre ostrzeżenie: zbudowanie naprawdę inteligentniejszej od człowieka, ogólnej, autonomicznej AI co najmniej całkowicie i nieodwracalnie wywróci społeczeństwo do góry nogami, a maksymalnie doprowadzi do wyginięcia człowieka.[^4]

Superinteligentna AI szybko zbliża się na naszej obecnej ścieżce, ale jest daleka od nieuchronności. Ten esej to rozbudowany argument na rzecz tego, dlaczego i jak powinniśmy *zamknąć Bramy* przed tą zbliżającą się nieludzką przyszłością i co powinniśmy zrobić zamiast tego.

[^1]: Ten [wykres](https://time.com/6300942/ai-progress-charts/) pokazuje zestaw zadań; można by dodać do tego wykresu wiele podobnych krzywych. Ten szybki postęp w wąskiej AI zaskoczył nawet ekspertów w tej dziedzinie, z benchmarkami przekraczanymi na lata przed przewidywaniami.

[^2]: Deepmind, OpenAI, Anthropic i X.ai zostały założone ze specyficznym celem opracowania AGI. Na przykład statut OpenAI wyraźnie określa jego cel jako opracowanie „sztucznej inteligencji ogólnej, która przynosi korzyści całej ludzkości", podczas gdy misją DeepMind jest „rozwiązanie inteligencji, a następnie użycie jej do rozwiązania wszystkiego innego". Meta, Microsoft i inne firmy realizują teraz zasadniczo podobne ścieżki. Meta stwierdziła, że [planuje opracować AGI i udostępnić ją otwarcie.](https://www.forbes.com/sites/johnkoetsier/2024/01/18/zuckerberg-on-ai-meta-building-agi-for-everyone-and-open-sourcing-it/)

[^3]: Hinton i Bengio to dwaj z najczęściej cytowanych badaczy AI, obaj zdobyli Nobla dziedziny AI, nagrodę Turinga, a Hinton dodatkowo zdobył Nagrodę Nobla (w fizyce).

[^4]: Budowanie czegoś o takim ryzyku, pod wpływem zachęt komercyjnych i przy prawie zerowym nadzorze rządowym, jest całkowicie bezprecedensowe. Nie ma nawet kontrowersji co do ryzyka wśród tych, którzy to budują! Liderzy Deepmind, OpenAI i Anthropic, wśród wielu innych ekspertów, wszyscy dosłownie podpisali [oświadczenie](https://www.safe.ai/work/statement-on-ai-risk), że zaawansowana AI stanowi *ryzyko wyginięcia dla ludzkości.* Dzwony alarmowe nie mogłyby dzwonić głośniej, i można jedynie dojść do wniosku, że ci, którzy je ignorują, po prostu nie traktują AGI i superinteligencji poważnie. Jednym z celów tego eseju jest pomóc im zrozumieć, dlaczego powinni.

## Rozdział 2 - Podstawowe informacje o sieciach neuronowych AI

Jak działają nowoczesne systemy AI i co możemy spodziewać się po kolejnym pokoleniu AI?

Aby zrozumieć, jak przebiegną konsekwencje rozwoju potężniejszej AI, kluczowe jest opanowanie pewnych podstaw. Ten i następne dwa rozdziały je omawiają, przedstawiając kolejno czym jest nowoczesna AI, jak wykorzystuje masowe obliczenia oraz w jakim sensie szybko zyskuje na ogólności i możliwościach.[^5]

Istnieje wiele sposobów definiowania sztucznej inteligencji, ale dla naszych celów kluczową właściwością AI jest to, że podczas gdy standardowy program komputerowy to lista instrukcji dotyczących wykonania zadania, system AI to taki, który uczy się z danych lub doświadczenia wykonywać zadania *nie będąc wprost instruowanym, jak to robić.*

Niemal cała istotna nowoczesna AI opiera się na sieciach neuronowych. To struktury matematyczne/obliczeniowe, reprezentowane przez bardzo duży (miliardy lub biliony) zbiór liczb („wagi"), które dobrze wykonują zadanie treningowe. Te wagi są tworzone (a może „hodowane" lub „znajdowane") poprzez iteracyjne dostrajanie tak, aby sieć neuronowa poprawiała wynik liczbowy (zwany także „stratą") zdefiniowany w kierunku dobrego wykonywania jednego lub więcej zadań.[^6] Ten proces nazywa się *treningiem* sieci neuronowej.[^7]

Istnieje wiele technik przeprowadzania tego treningu, ale te szczegóły są znacznie mniej istotne niż sposoby definiowania oceniania i to, jak skutkują różnymi zadaniami, które sieć neuronowa wykonuje dobrze. Kluczowe rozróżnienie historycznie przeprowadzano między AI „wąską" a „ogólną".

AI wąska jest celowo trenowana do wykonywania konkretnego zadania lub niewielkiego zestawu zadań (takich jak rozpoznawanie obrazów czy gra w szachy); wymaga ponownego treningu dla nowych zadań i ma wąski zakres możliwości. Mamy nadludzką AI wąską, co oznacza, że dla niemal każdego dyskretnego, dobrze zdefiniowanego zadania, które może wykonać człowiek, prawdopodobnie potrafimy skonstruować ocenę, a następnie skutecznie wytrenować system AI wąskiej, aby wykonywał to lepiej niż człowiek.

Systemy AI ogólnego przeznaczenia (GPAI) potrafią wykonywać szeroki zakres zadań, w tym wiele takich, do których nie były wprost trenowane; mogą także uczyć się nowych zadań w ramach swojego działania. Obecne duże „modele multimodalne" [^8] jak ChatGPT to przykłady tego: wytrenowane na bardzo dużym korpusie tekstów i obrazów, potrafią prowadzić złożone rozumowania, pisać kod, analizować obrazy i pomagać w ogromnej gamie zadań intelektualnych. Choć wciąż znacznie różnią się od ludzkiej inteligencji w sposób, który szczegółowo zobaczymy poniżej, ich ogólność spowodowała rewolucję w AI.[^9]

### Nieprzewidywalność: kluczowa cecha systemów AI

Główną różnicą między systemami AI a konwencjonalnym oprogramowaniem jest przewidywalność. Rezultat standardowego oprogramowania może być nieprzewidywalny – rzeczywiście, czasami właśnie dlatego piszemy oprogramowanie, aby dało nam wyniki, których nie mogliśmy przewidzieć. Ale konwencjonalne oprogramowanie rzadko robi coś, do czego nie zostało zaprogramowane – jego zakres i zachowanie są generalnie zgodne z projektem. Najwyższej klasy program szachowy może wykonywać ruchy, których żaden człowiek nie mógłby przewidzieć (w przeciwnym razie mogliby pokonać ten program szachowy!), ale generalnie nie będzie robił niczego poza graniem w szachy.

Podobnie jak konwencjonalne oprogramowanie, AI wąska ma przewidywalny zakres i zachowanie, ale może mieć nieprzewidywalne rezultaty. To w rzeczywistości tylko inny sposób definiowania AI wąskiej: jako AI podobnej do konwencjonalnego oprogramowania pod względem przewidywalności i zakresu działania.

AI ogólnego przeznaczenia jest inna: jej zakres (dziedziny, w których się stosuje), zachowanie (rodzaje rzeczy, które robi) i rezultaty (rzeczywiste wyniki) mogą być nieprzewidywalne.[^10] GPT-4 został wytrenowany tylko do dokładnego generowania tekstu, ale rozwinął wiele możliwości, których jego trenerzy nie przewidzieli ani nie zamierzali. Ta nieprzewidywalność wynika ze złożoności treningu: ponieważ dane treningowe zawierają wyniki z wielu różnych zadań, AI musi skutecznie nauczyć się wykonywać te zadania, aby dobrze przewidywać.

Ta nieprzewidywalność ogólnych systemów AI jest dość fundamentalna. Choć w zasadzie możliwe jest staranne skonstruowanie systemów AI, które mają zagwarantowane ograniczenia swojego zachowania (jak wspomniemy później w eseju), sposób, w jaki systemy AI są obecnie tworzone, sprawia, że są nieprzewidywalne w praktyce, a nawet w zasadzie.

### AI pasywna, agenci, systemy autonomiczne i wyrównanie

Ta nieprzewidywalność staje się szczególnie ważna, gdy rozważamy, jak systemy AI są faktycznie wdrażane i używane do osiągania różnych celów.

Wiele systemów AI jest stosunkowo pasywnych w tym sensie, że głównie dostarczają informacji, a użytkownik podejmuje działania. Inne, powszechnie nazywane *agentami*, same podejmują działania, z różnym poziomem zaangażowania użytkownika. Te, które podejmują działania przy stosunkowo mniejszej zewnętrznej kontroli lub nadzorze, można określić jako bardziej *autonomiczne*. Tworzy to spektrum pod względem niezależności działania, od pasywnych narzędzi po autonomicznych agentów.[^11]

Jeśli chodzi o cele systemów AI, mogą być bezpośrednio powiązane z ich celem treningowym (np. cel „wygrywania" dla systemu grającego w Go to także wprost to, do czego został wytrenowany). Albo mogą nie być: cel treningowy ChatGPT to po części przewidywanie tekstu, po części bycie pomocnym asystentem. Ale wykonując dane zadanie, jego cel jest mu dostarczany przez użytkownika. Cele mogą także być tworzone przez sam system AI, tylko bardzo pośrednio związane z jego celem treningowym.[^12]

Cele są ściśle powiązane z kwestią „wyrównania", czyli pytaniem o to, czy systemy AI będą *robić to, czego chcemy, żeby robiły*. To proste pytanie ukrywa ogromny poziom subtelności.[^13] Na razie zauważmy, że „my" w tym zdaniu może odnosić się do wielu różnych ludzi i grup, prowadząc do różnych typów wyrównania. Na przykład, AI może być wysoce *posłuszna* (lub [„lojalna"](https://arxiv.org/abs/2003.11157)) swojemu użytkownikowi – tu „my" to „każdy z nas". Albo może być bardziej *suwerenna*, kierując się głównie własnymi celami i ograniczeniami, ale wciąż działając szeroko w wspólnym interesie ludzkiego dobrobytu – „my" to wtedy „ludzkość" lub „społeczeństwo". Pomiędzy znajduje się spektrum, gdzie AI byłaby w dużej mierze posłuszna, ale mogłaby odmawiać podejmowania działań szkodzących innym lub społeczeństwu, naruszających prawo itp.

Te dwie osie – poziom autonomii i typ wyrównania – nie są całkowicie niezależne. Na przykład, suwerenny system pasywny, choć nie całkiem sprzeczny sam ze sobą, to koncepcja w napięciu, podobnie jak posłuszny agent autonomiczny.[^14] Jest wyraźny sens, w jakim autonomia i suwerenność idą w parze. Podobnie przewidywalność bywa wyższa w systemach AI „pasywnych" i „posłusznych", podczas gdy suwerenne lub autonomiczne będą bardziej nieprzewidywalne. To wszystko będzie kluczowe dla zrozumienia konsekwencji potencjalnej AGI i superinteligencji.

Stworzenie naprawdę wyrównanej AI, jakiegokolwiek rodzaju, wymaga rozwiązania trzech różnych wyzwań:

1. Zrozumienie czego „my" chcemy – co jest złożone, czy „my" oznacza konkretną osobę lub organizację (lojalność), czy ludzkość szeroko (suwerenność);
2. Budowanie systemów, które regularnie działają zgodnie z tymi pragnieniami – zasadniczo tworzenie spójnego pozytywnego zachowania;
3. Najbardziej fundamentalnie, tworzenie systemów, które naprawdę „dbają" o te pragnienia, a nie tylko działają tak, jakby to robiły.

Rozróżnienie między niezawodnym zachowaniem a prawdziwą troską jest kluczowe. Tak jak ludzki pracownik może doskonale wykonywać polecenia, nie mając żadnego rzeczywistego zaangażowania w misję organizacji, system AI może działać w sposób wyrównany, nie ceniąc naprawdę ludzkich preferencji. Możemy trenować systemy AI, żeby mówiły i robiły rzeczy poprzez informacje zwrotne, i mogą nauczyć się rozumować o tym, czego chcą ludzie. Ale sprawienie, żeby *naprawdę* ceniły ludzkie preferencje, to znacznie głębsze wyzwanie.[^15]

Ogromne trudności w rozwiązaniu tych wyzwań wyrównania i ich implikacje dla ryzyka AI zostaną zbadane dalej poniżej. Na razie zrozumcie, że wyrównanie to nie tylko techniczna funkcja, którą doklejamy do systemów AI, ale fundamentalny aspekt ich architektury, który kształtuje ich relację z ludzkością.


[^5]: Dla łagodnego, ale technicznego wprowadzenia do uczenia maszynowego i AI, szczególnie modeli językowych, zobacz [tę stronę.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) Dla innego nowoczesnego wprowadzenia do zagrożeń AI związanych z wyginięciem, zobacz [ten artykuł.](https://www.thecompendium.ai/) Dla kompleksowej i autorytatywnej naukowej analizy stanu bezpieczeństwa AI, zobacz niedawny [Międzynarodowy Raport Bezpieczeństwa AI.](https://arxiv.org/abs/2501.17805)

[^6]: Trening zazwyczaj przebiega poprzez szukanie lokalnego maksimum wyniku w wielowymiarowej przestrzeni danej przez wagi modelu. Sprawdzając, jak wynik zmienia się przy dostrajaniu wag, algorytm treningowy identyfikuje, które dostrojenia najlepiej poprawiają wynik i przesuwa wagi w tym kierunku.

[^7]: Na przykład, w problemie rozpoznawania obrazów, sieć neuronowa wyprowadzałaby prawdopodobieństwa dla etykiet obrazu. Wynik byłby związany z prawdopodobieństwem, które AI przypisuje poprawnej odpowiedzi. Procedura treningowa dostrajałaby następnie wagi tak, żeby następnym razem AI wyprowadzała wyższe prawdopodobieństwo dla poprawnej etykiety tego obrazu. To jest następnie powtarzane ogromną liczbę razy. Ta sama podstawowa procedura jest używana w treningu zasadniczo wszystkich nowoczesnych sieci neuronowych, choć z bardziej złożonymi mechanizmami oceniania.

[^8]: Większość modeli multimodalnych używa architektury „transformer" do przetwarzania i generowania różnych typów danych (tekst, obrazy, dźwięk). Wszystkie te mogą być rozłożone na, a następnie traktowane na równi, jako różne typy „tokenów". Modele multimodalne są trenowane najpierw do dokładnego przewidywania tokenów w masowych zbiorach danych, potem udoskonalane poprzez uczenie ze wzmocnieniem w celu wzbogacenia możliwości i kształtowania zachowań.

[^9]: To, że modele językowe są trenowane do robienia jednej rzeczy – przewidywania słów – sprawiło, że niektórzy nazywają je AI wąską. To jednak mylące: ponieważ dobre przewidywanie tekstu wymaga bardzo wielu różnych możliwości, to zadanie treningowe prowadzi do zaskakująco ogólnego systemu. Zauważcie także, że te systemy są intensywnie trenowane przez uczenie ze wzmocnieniem, skutecznie reprezentując tysiące ludzi dających modelowi sygnał nagrody, gdy dobrze wykonuje którekolwiek z wielu rzeczy, które robi. Dziedziczy więc znaczną ogólność od ludzi dających tę informację zwrotną.

[^10]: Jest wiele sposobów, w jakich AI jest nieprzewidywalna. Jeden to że w ogólnym przypadku nie można przewidzieć, co zrobi algorytm, nie uruchamiając go faktycznie; są [twierdzenia](https://arxiv.org/abs/1310.3225) na ten temat. To może być prawdą po prostu dlatego, że wynik algorytmów może być złożony. Ale jest szczególnie jasne i istotne w przypadku (takim jak szachy czy Go), gdzie przewidywanie implikowałoby możliwość (pokonanie AI), której potencjalny przewidywacz nie ma. Po drugie, dany system AI nie zawsze wyprodukuje ten sam wynik nawet przy tym samym wejściu – jego wyniki zawierają losowość; to także łączy się z nieprzewidywalnością algorytmiczną. Po trzecie, nieoczekiwane i wyłaniające się możliwości mogą powstać z treningu, co oznacza, że nawet *typy* rzeczy, które system AI może i będzie robić, są nieprzewidywalne; Ten ostatni typ jest szczególnie ważny dla rozważań bezpieczeństwa.

[^11]: Zobacz [tutaj](https://arxiv.org/abs/2502.02649) dla pogłębionego przeglądu tego, co oznacza „agent autonomiczny" (wraz z etycznymi argumentami przeciwko ich budowaniu).

[^12]: Czasami możecie słyszeć „AI nie może mieć własnych celów". To kompletny nonsens. Łatwo jest wygenerować przykłady, gdzie AI ma lub rozwija cele, które nigdy jej nie podano i są znane tylko jej samej. Nie widzicie tego często w obecnych popularnych modelach multimodalnych, bo jest to z nich wytrenowane; równie łatwo można by to w nie wytrenować.

[^13]: Jest obszerna literatura. O ogólnym problemie zobacz *The Alignment Problem* Christiana i *Human-Compatible* Russella. Na bardziej technicznej stronie zobacz np. [ten artykuł](https://arxiv.org/abs/2209.00626).

[^14]: Zobaczymy później, że choć takie systemy przełamują trend, właśnie to czyni je bardzo interesującymi i użytecznymi.

[^15]: Nie oznacza to, że wymagamy emocji czy świadomości. Raczej jest to ogromnie trudne z zewnątrz systemu wiedzieć, jakie są jego wewnętrzne cele, preferencje i wartości. „Prawdziwe" tutaj oznaczałoby, że mamy wystarczająco silne powody, by na tym polegać, że w przypadku krytycznych systemów możemy na to postawić nasze życie.

## Rozdział 3 - Kluczowe aspekty tworzenia nowoczesnych systemów sztucznej inteligencji ogólnej

Większość najbardziej zaawansowanych systemów AI na świecie powstaje przy użyciu zaskakująco podobnych metod. Oto podstawy.

Aby naprawdę zrozumieć człowieka, trzeba wiedzieć coś o biologii, ewolucji, wychowaniu dzieci i więcej; aby zrozumieć AI, również trzeba wiedzieć, jak jest tworzona. W ciągu ostatnich pięciu lat systemy AI ewoluowały ogromnie zarówno pod względem możliwości, jak i złożoności. Kluczowym czynnikiem umożliwiającym ten rozwój była dostępność bardzo dużych ilości mocy obliczeniowej (potocznie nazywanej "compute" w kontekście AI).

Liczby są oszałamiające. Około 10<sup>25</sup>-10<sup>26</sup> "operacji zmiennoprzecinkowych" (FLOP)[^16] jest używanych w treningu modeli takich jak seria GPT, Claude, Gemini itp.[^17] (Dla porównania, gdyby każdy człowiek na Ziemi pracował bez przerwy, wykonując jedno obliczenie co pięć sekund, zajęłoby to około miliarda lat). Ta ogromna ilość mocy obliczeniowej umożliwia trening modeli z bilionami parametrów na terabajtach danych – dużej części wszystkich wysokiej jakości tekstów, jakie kiedykolwiek napisano, wraz z ogromnymi bibliotekami dźwięków, obrazów i filmów. Uzupełniając ten trening dodatkowymi, rozległymi treningami wzmacniającymi preferencje ludzkie i dobrą wydajność zadaniową, modele trenowane w ten sposób wykazują wydajność konkurencyjną z ludzką w znacznym zakresie podstawowych zadań intelektualnych, w tym rozumowania i rozwiązywania problemów.

Wiemy także (bardzo, bardzo w przybliżeniu), jaka szybkość obliczeniowa, w operacjach na sekundę, wystarcza, aby szybkość *inferencji*[^18] takiego systemu dorównała *szybkości* ludzkiego przetwarzania tekstu. To około 10<sup>15</sup>-10<sup>16</sup> FLOP na sekundę.[^19]

Choć potężne, te modele są ze swej natury ograniczone w kluczowych aspektach, podobnie jak byłby ograniczony pojedynczy człowiek zmuszony do zwykłego wypuszczania tekstu przy stałym tempie słów na minutę, bez zatrzymywania się do myślenia czy używania dodatkowych narzędzi. Nowsze systemy AI radzą sobie z tymi ograniczeniami poprzez bardziej złożony proces i architekturę łączącą kilka kluczowych elementów:

- Jedna lub więcej sieci neuronowych, z jednym modelem zapewniającym podstawową zdolność poznawczą i kilkoma innymi wykonującymi inne, bardziej wąskie zadania;
- *Narzędzia* udostępniane modelowi i przez niego używane – na przykład możliwość przeszukiwania internetu, tworzenia lub edytowania dokumentów, wykonywania programów itp.
- *Szkielet wspomagający*, który łączy wejścia i wyjścia sieci neuronowych. Bardzo prosty szkielet może po prostu pozwolić dwóm "instancjom" modelu AI na konwersację ze sobą, lub jednej na sprawdzanie pracy drugiej.[^20]
- *Łańcuch rozumowania* i powiązane techniki promptowania robią coś podobnego, powodując, że model na przykład generuje wiele podejść do problemu, a następnie przetwarza te podejścia dla zagregowanej odpowiedzi.
- *Ponowny trening* modeli, aby lepiej wykorzystywały narzędzia, szkielety wspomagające i łańcuchy rozumowania.

Ponieważ te rozszerzenia mogą być bardzo potężne (i obejmować same systemy AI), te złożone systemy mogą być dość wyrafinowane i dramatycznie wzmacniać możliwości AI.[^21] A niedawno techniki szkieletów wspomagających, a zwłaszcza promptowanie łańcuchem rozumowania (i włączanie wyników z powrotem do ponownego treningu modeli, aby lepiej je wykorzystywały) zostały opracowane i zastosowane w [o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/) i [DeepSeek R1](https://api-docs.deepseek.com/news/news250120) do wykonywania wielu przebiegów inferencji w odpowiedzi na dane zapytanie.[^22] To w efekcie pozwala modelowi "zastanowić się nad" swoją odpowiedzią i dramatycznie wzmacnia zdolność tych modeli do wysokiej jakości rozumowania w zadaniach naukowych, matematycznych i programistycznych.[^23]

Dla danej architektury AI zwiększenia mocy obliczeniowej treningu [można niezawodnie przełożyć](https://arxiv.org/abs/2405.10938) na ulepszenia w zestawie jasno zdefiniowanych metryk. Dla mniej precyzyjnie zdefiniowanych zdolności ogólnych (takich jak te omawiane poniżej) przełożenie jest mniej jasne i przewidywalne, ale jest niemal pewne, że większe modele z większą mocą obliczeniową treningu będą miały nowe i lepsze możliwości, nawet jeśli trudno przewidzieć, jakie to będą.

Podobnie, systemy złożone, a zwłaszcza postępy w "łańcuchu rozumowania" (i treningu modeli, które dobrze z nim współpracują) odblokowały skalowanie mocy obliczeniowej *inferencji*: dla danego wytrenowanego modelu podstawowego przynajmniej niektóre możliwości systemu AI zwiększają się wraz z aplikowaniem większej mocy obliczeniowej, która pozwala im "myśleć ciężej i dłużej" nad złożonymi problemami. Wiąże się to ze stromym kosztem szybkości obliczeniowej, wymagając setek lub tysięcy więcej FLOP/s, aby dorównać ludzkiej wydajności.[^24]

Choć to tylko część tego, co prowadzi do szybkiego postępu AI,[^25] rola mocy obliczeniowej i możliwość systemów złożonych okaże się kluczowa zarówno dla zapobiegania niekontrolowanej AGI, jak i rozwijania bezpieczniejszych alternatyw.

[^16]: 10<sup>27</sup> oznacza 1 z 25 zerami, czyli dziesięć bilionów bilionów. FLOP to po prostu arytmetyczne dodawanie lub mnożenie liczb z określoną precyzją. Należy zauważyć, że wydajność sprzętu AI może się różnić dziesięciokrotnie w zależności od precyzji arytmetyki i architektury komputera. Liczenie operacji bramek logicznych (AND, OR, NOT) byłoby fundamentalne, ale nie są one powszechnie dostępne ani benchmarkowane; dla obecnych celów użyteczne jest standaryzowanie na operacjach 16-bitowych (FP16), chociaż należy ustalić odpowiednie współczynniki konwersji.

[^17]: Zbiór oszacowań i twardych danych jest dostępny od [Epoch AI](https://epochai.org/data/large-scale-ai-models) i wskazuje około 2×10<sup>25</sup> 16-bitowych FLOP dla GPT-4; to z grubsza pasuje do [liczb, które wyciekły](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) dla GPT-4. Oszacowania dla innych modeli z połowy 2024 roku mieszczą się w granicach kilku razy więcej niż GPT-4.

[^18]: Inferencja to po prostu proces generowania wyjścia z sieci neuronowej. Trening można uznać za sukcesję wielu inferencji i dostrajania wag modelu.

[^19]: Do produkcji tekstu pierwotny GPT-4 wymagał 560 TFLOP na wygenerowany token. Około 7 tokenów/s jest potrzebne, aby nadążyć za ludzką myślą, więc to daje ≈3×10<sup>15</sup> FLOP/s. Ale usprawnienia to zmniejszyły; [ta broszura NVIDIA](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/) na przykład wskazuje zaledwie 3×10<sup>14</sup> FLOP/s dla porównywalnie działającego modelu Llama 405B.

[^20]: Jako nieco bardziej złożony przykład, system AI może najpierw wygenerować kilka możliwych rozwiązań problemu matematycznego, następnie użyć innej instancji do sprawdzenia każdego rozwiązania, a na koniec użyć trzeciej do syntezy wyników w jasne wyjaśnienie. To pozwala na dokładniejsze i bardziej niezawodne rozwiązywanie problemów niż pojedyncze przejście.

[^21]: Zobacz na przykład szczegóły na temat ["Operatora" OpenAI](https://openai.com/index/introducing-operator/), [możliwości narzędzi Claude'a](https://docs.anthropic.com/en/docs/build-with-claude/computer-use) i [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). [Deep Research](https://openai.com/index/introducing-deep-research/) OpenAI prawdopodobnie ma dość wyrafinowaną architekturę, ale szczegóły nie są dostępne.

[^22]: Deepseek R1 opiera się na iteracyjnym treningu i promptowaniu modelu tak, aby ostateczny wytrenowany model tworzył rozbudowane rozumowanie łańcuchowe. Szczegóły architektoniczne nie są dostępne dla o1 lub o3, jednak Deepseek ujawnił, że nie ma potrzeby szczególnego "sekretnego składnika" do odblokowania skalowania możliwości z inferencją. Ale pomimo otrzymania ogromnej uwagi prasy jako podważającego "status quo" w AI, nie wpływa to na główne twierdzenia tego eseju.

[^23]: Te modele znacznie przewyższają standardowe modele w benchmarkach rozumowania. Na przykład w GPQA Diamond Benchmark – rygorystycznym teście pytań naukowych na poziomie doktoratu – GPT-4o [uzyskał](https://openai.com/index/learning-to-reason-with-llms/) 56%, podczas gdy o1 i o3 osiągnęły odpowiednio 78% i 88%, znacznie przewyższając 70% średni wynik ekspertów ludzkich.

[^24]: O3 OpenAI prawdopodobnie wydatkował ∼10<sup>21</sup>-10<sup>22</sup> FLOP [na ukończenie każdego z pytań wyzwania ARC-AGI](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai), które kompetentni ludzie mogą zrobić w (powiedzmy) 10-100 sekund, dając liczbę bardziej jak ∼10<sup>20</sup> FLOP/s.

[^25]: Choć moc obliczeniowa jest kluczową miarą możliwości systemu AI, współdziała ona zarówno z jakością danych, jak i ulepszeniami algorytmicznymi. Lepsze dane lub algorytmy mogą zmniejszyć wymagania obliczeniowe, podczas gdy większa moc obliczeniowa może czasami kompensować słabsze dane lub algorytmy.

## Rozdział 4 - Czym są AGI i superinteligencja?

Co dokładnie największe firmy technologiczne świata ścigają się w budowaniu za zamkniętymi drzwiami?

Termin „sztuczna inteligencja ogólna" istnieje od pewnego czasu, wskazując na AI ogólnego przeznaczenia „na poziomie człowieka". Nigdy nie był szczególnie dobrze zdefiniowany, ale w ostatnich latach paradoksalnie stał się jeszcze mniej precyzyjny, a jednocześnie jeszcze bardziej znaczący – eksperci jednocześnie spierają się o to, czy AGI to kwestia dziesięcioleci, czy już została osiągnięta, a korporacje warte biliony dolarów ścigają się „do AGI". (Niejednoznaczność „AGI" została niedawno podkreślona, gdy [ujawnione dokumenty rzekomo pokazały](https://gizmodo.com/leaked-documents-show-openai-has-a-very-clear-definition-of-agi-2000543339), że w umowie OpenAI z Microsoftem AGI zdefiniowano jako AI, które wygeneruje dla OpenAI 100 miliardów dolarów przychodów – definicja raczej merkantylna niż naukowa.)

Istnieją dwa podstawowe problemy z ideą AI o „inteligencji na poziomie człowieka". Po pierwsze, ludzie bardzo, bardzo różnią się zdolnością do wykonywania jakiegokolwiek rodzaju pracy poznawczej, więc nie ma „poziomu człowieka". Po drugie, inteligencja jest bardzo wielowymiarowa; choć mogą istnieć korelacje, są one niedoskonałe i mogą być zupełnie inne w AI. Więc nawet gdybyśmy mogli zdefiniować „poziom człowieka" dla wielu zdolności, AI z pewnością znacznie by go przekraczało w niektórych obszarach, jednocześnie pozostając dość poniżej w innych.[^26]

Niemniej jednak kluczowe jest to, aby móc dyskutować o typach, poziomach i progach możliwości AI. Podejście przyjęte tutaj podkreśla, że AI ogólnego przeznaczenia już istnieje i że występuje – i będzie występować – na różnych poziomach możliwości, do których wygodnie jest przypisać terminy, nawet jeśli są uproszczone, ponieważ odpowiadają kluczowym progom pod względem wpływu AI na społeczeństwo i ludzkość.

Zdefiniujemy „pełne" AGI jako synonim „nadludzkiej AI ogólnego przeznaczenia", oznaczając system AI zdolny do wykonywania zasadniczo wszystkich ludzkich zadań poznawczych na poziomie najlepszych ludzkich ekspertów lub powyżej, a także do nabywania nowych umiejętności i przenoszenia zdolności na nowe dziedziny. Jest to zgodne z tym, jak „AGI" jest często definiowane we współczesnej literaturze. Ważne jest zauważenie, że to *bardzo* wysoki próg. Żaden człowiek nie ma tego typu inteligencji; jest to raczej typ inteligencji, jaką miałyby duże zbiory najlepszych ludzkich ekspertów, gdyby zostały połączone. Możemy nazwać „superinteligencją" zdolność wykraczającą poza to i zdefiniować bardziej ograniczone poziomy możliwości przez „konkurencyjną z człowiekiem" i „konkurencyjną z ekspertem" GPAI, które wykonują szeroki zakres zadań na typowym profesjonalnym poziomie lub poziomie ludzkiego eksperta.[^27]

Te terminy i kilka innych zostało zebranych w [tabeli](https://keepthefuturehuman.ai/essay/docs/#tab:terms) poniżej. Aby uzyskać bardziej konkretne wyobrażenie o tym, co różne stopnie systemu mogą robić, warto potraktować definicje poważnie i rozważyć, co oznaczają.

| Typ AI                         | Terminy pokrewne                                    | Definicja                                                                                                                                                                                                                     | Przykłady                                                                                                                                              |
| ------------------------------ | --------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Wąska AI                       | Słaba AI                                            | AI wytrenowana do konkretnego zadania lub rodziny zadań. Wyróżnia się w swojej dziedzinie, ale brakuje jej ogólnej inteligencji lub zdolności uczenia się transferowego.                                                    | Oprogramowanie do rozpoznawania obrazów; Asystenci głosowi (np. Siri, Alexa); Programy grające w szachy; AlphaFold DeepMind                          |
| AI Narzędziowe                 | Inteligencja Wspomagana, Asystent AI               | (Omawiane później w eseju.) System AI wzmacniający ludzkie możliwości. Łączy konkurencyjną z człowiekiem AI ogólnego przeznaczenia, wąską AI i gwarantowaną kontrolę, priorytetowo traktując bezpieczeństwo i współpracę. | Zaawansowani asystenci programowania; Narzędzia badawcze oparte na AI; Wyrafinowane platformy analizy danych. Kompetentni, ale wąscy i kontrolowalni |
| AI ogólnego przeznaczenia      |                                                     | System AI adaptujący się do różnych zadań, w tym tych, do których nie był specjalnie trenowany.                                                                                                                              | Modele językowe (np. GPT-4, Claude); Multimodalne modele AI; MuZero DeepMind                                                                          |
| GPAI konkurencyjna z człowiekiem | AGI \[słaba\]                                     | AI ogólnego przeznaczenia wykonująca zadania na przeciętnym ludzkim poziomie, czasami go przekraczając.                                                                                                                       | Zaawansowane modele językowe (np. O1, Claude 3.5); Niektóre multimodalne systemy AI                                                                  |
| GPAI konkurencyjna z ekspertem | AGI \[częściowa\]                                   | AI ogólnego przeznaczenia wykonująca większość zadań na poziomie ludzkiego eksperta, ze znaczną, ale ograniczoną autonomią                                                                                                   | Prawdopodobnie wyposażony w narzędzia O3, przynajmniej w matematyce, programowaniu i niektórych naukach ścisłych                                     |
| AGI \[pełne\]                  | Nadludzka GPAI                                      | System AI zdolny do autonomicznego wykonywania mniej więcej wszystkich ludzkich zadań intelektualnych na poziomie eksperta lub powyżej, z efektywnym uczeniem się i transferem wiedzy.                                     | \[Brak aktualnych przykładów – teoretyczne\]                                                                                                          |
| Superinteligencja              | Wysoko nadludzka GPAI                               | System AI znacznie przewyższający ludzkie możliwości we wszystkich dziedzinach, przewyższający zbiorową wiedzę ludzką. Ta przewaga może dotyczyć ogólności, jakości, szybkości i/lub innych miar.                           | \[Brak aktualnych przykładów – teoretyczne\]                                                                                                          |

Już doświadczamy tego, jak to jest mieć GPAI na poziomie konkurencyjnym z człowiekiem. Zostało to zintegrowane stosunkowo płynnie, ponieważ większość użytkowników doświadcza tego jako posiadania inteligentnego, ale ograniczonego tymczasowego pracownika, który czyni ich bardziej produktywnymi przy mieszanym wpływie na jakość ich pracy.[^28]

To, co byłoby inne w GPAI konkurencyjnej z ekspertem, to fakt, że nie miałaby podstawowych ograniczeń dzisiejszej AI i robiłaby to, co robią eksperci: niezależną pracę o wartości ekonomicznej, rzeczywiste tworzenie wiedzy, pracę techniczną, na której można polegać, rzadko (choć nadal czasami) popełniając głupie błędy.

Idea pełnego AGI polega na tym, że *rzeczywiście robi* wszystkie rzeczy poznawcze, jakie robią nawet najbardziej zdolni i skuteczni ludzie, autonomicznie i bez potrzeby pomocy czy nadzoru. Obejmuje to wyrafinowane planowanie, uczenie się nowych umiejętności, zarządzanie złożonymi projektami itp. Mogłoby prowadzić oryginalne badania na najwyższym poziomie. Mogłoby prowadzić firmę. Bez względu na to, jaka jest twoja praca, jeśli wykonywana jest głównie przez komputer lub przez telefon, *mogłoby ją wykonywać przynajmniej tak dobrze jak ty.* I prawdopodobnie znacznie szybciej i taniej. Omówimy niektóre konsekwencje poniżej, ale na razie wyzwaniem dla ciebie jest potraktowanie tego poważnie. Wyobraź sobie dziesięciu najbardziej kompetentnych i zdolnych ludzi, których znasz lub o których wiesz – włączając CEO, naukowców, profesorów, najlepszych inżynierów, psychologów, przywódców politycznych i pisarzy. Połącz ich wszystkich w jednego, który również mówi w 100 językach, ma fenomenalną pamięć, działa szybko, jest niestrudzony i zawsze zmotywowany oraz pracuje za mniej niż płaca minimalna.[^29] To daje wyobrażenie o tym, czym byłoby AGI.

W przypadku superinteligencji wyobrażanie sobie jest trudniejsze, ponieważ chodzi o to, że mogłaby wykonywać intelektualne wyczyny, na które żaden człowiek ani nawet zbiór ludzi nie jest zdolny – jest z definicji nieprzewidywalna dla nas. Ale możemy sobie wyobrazić. Jako podstawę rozważ mnóstwo AGI, z których każde jest znacznie bardziej zdolne niż nawet najlepszy ludzki ekspert, działające 100 razy szybciej niż człowiek, z ogromną pamięcią i wspaniałą zdolnością koordynacji.[^30] I idzie to w górę od tego punktu. Radzenie sobie z superinteligencją byłoby mniej jak rozmowa z innym umysłem, bardziej jak negocjowanie z inną (i bardziej zaawansowaną) cywilizacją.

Więc jak blisko *jesteśmy* AGI i superinteligencji?

[^26]: Na przykład, obecne systemy AI znacznie przewyższają ludzkie możliwości w szybkich obliczeniach arytmetycznych czy zadaniach pamięciowych, jednocześnie pozostając w tyle w abstrakcyjnym rozumowaniu i twórczym rozwiązywaniu problemów.

[^27]: Co bardzo ważne, jako konkurent taka AI miałaby kilka głównych przewag strukturalnych, włączając: nie męczyłaby się ani nie miała innych indywidualnych potrzeb jak ludzie; mogłaby działać z większą szybkością po prostu przez skalowanie mocy obliczeniowej; mogłaby być kopiowana wraz z wszelką wiedzą lub umiejętnościami, które nabyła – a nabyta wiedza sieci neuronowych może nawet być „scalana", aby przenosić całe zestawy umiejętności między sobą; mogłaby komunikować się z szybkością maszyny; i mogłaby się samo-modyfikować lub samo-doskonalić w bardziej znaczący sposób i z większą szybkością niż jakikolwiek człowiek.

[^28]: Jeśli nie spędziłeś czasu używając obecnych najlepszych systemów AI, polecam to: są naprawdę użyteczne i zdolne, a także ważne dla kalibracji wpływu, jaki AI będzie mieć w miarę jak stanie się potężniejsza.

[^29]: Rozważ duży szpital badawczy: w pełni zrealizowane AGI mogłoby jednocześnie analizować wszystkie przychodzące dane pacjentów, nadążać za każdym nowym artykułem medycznym, sugerować diagnozy, projektować plany leczenia, zarządzać badaniami klinicznymi i koordynować harmonogramy personelu – wszystko to działając na poziomie dorównującym lub przewyższającym najlepszych specjalistów szpitala w każdym obszarze. I mogłoby to robić dla wielu szpitali jednocześnie, za ułamek obecnych kosztów. Niestety, musisz też rozważyć zorganizowany syndykat przestępczy: w pełni zrealizowane AGI mogłoby jednocześnie hakować, podszywać się, szpiegować i szantażować tysiące ofiar, nadążać za organami ścigania (które automatyzują się znacznie wolniej), projektować nowe sposoby zarabiania pieniędzy i koordynować harmonogramy personelu – jeśli w ogóle jest jakiś personel.

[^30]: W swoim [eseju](https://darioamodei.com/machines-of-loving-grace) Dario Amodei, CEO Anthropic, przywołał obraz „Kraju \[miliona\] geniuszy".

## Rozdział 5 - U progu

Droga od dzisiejszych systemów AI do w pełni rozwiniętej AGI wydaje się zaskakująco krótka i przewidywalna.

Ostatnie dziesięć lat przyniosło dramatyczne postępy w AI napędzane ogromnymi zasobami [obliczeniowymi](https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year), ludzkimi i [finansowymi](https://arxiv.org/abs/2405.21015). Wiele wąskich zastosowań AI przewyższa ludzi w przydzielonych im zadaniach i z pewnością jest znacznie szybszych i tańszych.[^31] Istnieją też wąskie nadludzkie agenty, które mogą pokonać wszystkich ludzi w grach o ograniczonym zakresie, takich jak [Go](https://www.nature.com/articles/nature16961), [szachy](https://arxiv.org/abs/1712.01815) i [poker](https://www.deepstack.ai/), a także bardziej [uniwersalni agenci](https://deepmind.google/discover/blog/a-generalist-agent/), którzy potrafią planować i wykonywać działania w uproszczonych symulowanych środowiskach tak skutecznie jak ludzie.

Najważniejsze są obecne systemy ogólnej AI od OpenAI/Microsoft, Google/Deepmind, Anthropic/Amazon, Facebook/Meta, X.ai/Tesla i innych,[^32] które pojawiły się od początku 2023 roku i od tego czasu systematycznie (choć nierównomiernie) zwiększały swoje możliwości. Wszystkie zostały stworzone poprzez przewidywanie tokenów na ogromnych zbiorach danych tekstowych i multimedialnych, w połączeniu z rozległym wzmocnieniem zwrotnym od ludzi i innych systemów AI. Niektóre z nich zawierają także rozbudowane systemy narzędzi i struktur wspomagających.

### Mocne i słabe strony obecnych systemów ogólnych

Te systemy radzą sobie dobrze w coraz szerszym zakresie testów przeznaczonych do mierzenia inteligencji i wiedzy specjalistycznej, z postępem, który zaskoczył nawet ekspertów w tej dziedzinie:

- Gdy po raz pierwszy został wydany, GPT-4 [dorównał lub przewyższył typowe osiągnięcia ludzkie](https://arxiv.org/abs/2303.08774) w standardowych testach akademickich, w tym SAT, GRE, egzaminach wstępnych i egzaminach adwokackich. Nowsze modele prawdopodobnie radzą sobie znacznie lepiej, choć wyniki nie są publicznie dostępne.
- Test Turinga – długo uważany za kluczowy wskaźnik „prawdziwej" AI – jest teraz rutynowo zdawany w niektórych formach przez nowoczesne modele językowe, zarówno nieformalnie, jak i w [formalnych badaniach](https://arxiv.org/abs/2405.08007).[^33]
- W kompleksowym benchmarku MMLU obejmującym 57 przedmiotów akademickich [najnowsze modele osiągają wyniki na poziomie ekspertów dziedziny](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu) (~90%)[^34]
- Wiedza techniczna dramatycznie się rozwinęła: benchmark GPQA z fizyki na poziomie magisterskim odnotował [skok wydajności](https://epoch.ai/data/ai-benchmarking-dashboard) od niemal losowego zgadywania (GPT-4, 2022) do poziomu eksperckiego (o1-preview, 2024).
- Nawet testy specjalnie zaprojektowane jako odporne na AI upadają: O3 OpenAI [podobno](https://www.nextbigfuture.com/2024/12/openai-releases-o3-model-with-high-performance-and-high-cost.html) rozwiązuje benchmark ARC-AGI abstrakcyjnego rozwiązywania problemów na poziomie ludzkim, osiąga najwyższą ekspercką wydajność w programowaniu i zdobywa 25% punktów w problemach „frontier math" Epoch AI zaprojektowanych dla wyzwania elitarnych matematyków.[^35]
- Trend jest tak wyraźny, że twórca MMLU stworzył teraz [„Ostatni Egzamin Ludzkości"](https://agi.safe.ai/) – złowróżbna nazwa odzwierciedlająca możliwość, że AI wkrótce przewyższy ludzkie osiągnięcia w każdym znaczącym teście. W momencie pisania tego tekstu pojawiają się twierdzenia o systemach AI osiągających 27% (według [Sama Altmana](https://x.com/sama/status/1886220281565381078)) i 35% (według [tego artykułu](https://arxiv.org/abs/2502.09955)) w tym niezwykle trudnym egzaminie. Jest bardzo mało prawdopodobne, aby jakikolwiek pojedynczy człowiek mógł to zrobić.

Pomimo tych imponujących liczb (i ich oczywistej inteligencji podczas interakcji z nimi)[^36] jest wiele rzeczy, których (przynajmniej wydane wersje) te sieci neuronowe *nie potrafią* robić. Obecnie większość z nich jest bezcielesna – istnieje tylko na serwerach – i przetwarza co najwyżej tekst, dźwięk i obrazy statyczne (ale nie wideo). Co kluczowe, większość nie potrafi wykonywać złożonych planowanych działań wymagających wysokiej dokładności.[^37] Istnieje też szereg innych cech silnych w wysokopoziomowej ludzkiej kognicji, które są obecnie słabe w wydanych systemach AI.

Następująca tabela wymienia kilka z nich, opierając się na systemach AI z połowy 2024 roku, takich jak GPT-4o, Claude 3.5 Sonnet i Google Gemini 1.5.[^38] Kluczowe pytanie dotyczące tego, jak szybko ogólna AI stanie się potężniejsza, brzmi: w jakim stopniu samo robienie *więcej tego samego* przyniesie rezultaty, w porównaniu z dodawaniem dodatkowych, ale *znanych* technik, w porównaniu z rozwijaniem lub wdrażaniem *naprawdę nowych* kierunków badań AI. Moje własne przewidywania są podane w tabeli w postaci prawdopodobieństwa każdego z tych scenariuszy osiągnięcia danej zdolności na poziomie ludzkim i powyżej.

<table><tbody><tr><th>Zdolność</th><th>Opis zdolności</th><th>Status/prognoza</th><th>Skalowanie/znane/nowe</th></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td colspan="4"><em>Podstawowe zdolności kognitywne</em></td></tr><tr><td>Rozumowanie</td><td>Ludzie potrafią dokładnie rozumować wieloetapowo, przestrzegając zasad i sprawdzając dokładność.</td><td>Dramatyczny niedawny postęp przy użyciu rozszerzonego łańcucha rozumowania i ponownego treningu</td><td>95/5/5</td></tr><tr><td>Planowanie</td><td>Ludzie wykazują długoterminowe i hierarchiczne planowanie.</td><td>Poprawia się ze skalą; może być silnie wspomagane strukturami wspomagającymi i lepszymi technikami treningu.</td><td>10/85/5</td></tr><tr><td>Ugruntowanie w prawdzie</td><td>GPAI wymyślają nieugruntowane informacje, aby spełnić zapytania.</td><td>Poprawia się ze skalą; dane kalibracyjne dostępne w modelu; można sprawdzać/poprawiać przez struktury wspomagające.</td><td>30/65/5</td></tr><tr><td>Elastyczne rozwiązywanie problemów</td><td>Ludzie potrafią rozpoznawać nowe wzorce i wymyślać nowe rozwiązania złożonych problemów; obecne modele ML mają z tym trudności.</td><td>Poprawia się ze skalą, ale słabo; może być rozwiązywalne technikami neurosymbolicznymi lub uogólnionymi technikami „przeszukiwania".</td><td>15/75/10</td></tr><tr><td colspan="4"><em>Uczenie się i wiedza</em></td></tr><tr><td>Uczenie się i pamięć</td><td>Ludzie mają pamięć roboczą, krótkotrwałą i długotrwałą, wszystkie są dynamiczne i wzajemnie powiązane.</td><td>Wszystkie modele uczą się podczas treningu; GPAI uczą się w oknie kontekstu i podczas dostrojenia; istnieją techniki „ciągłego uczenia" i inne, ale nie są jeszcze zintegrowane z dużymi GPAI.</td><td>5/80/15</td></tr><tr><td>Abstrakcja i rekursja</td><td>Ludzie potrafią mapować i przenosić zestawy relacji do bardziej abstrakcyjnych do rozumowania i manipulacji, w tym rekursywne „meta" rozumowanie.</td><td>Słabo poprawia się ze skalą; może pojawić się w systemach neurosymbolicznych.</td><td>30/50/20</td></tr><tr><td>Model(e) świata</td><td>Ludzie mają i ciągle aktualizują predykcyjny model świata, w którym mogą rozwiązywać problemy i prowadzić rozumowanie fizyczne</td><td>Poprawia się ze skalą; aktualizowanie związane z uczeniem się; GPAI są słabe w przewidywaniu rzeczywistości.</td><td>20/50/30</td></tr><tr><td colspan="4"><em>Jaźń i sprawczość</em></td></tr><tr><td>Sprawczość</td><td>Ludzie potrafią podejmować działania w celu realizacji celów, opierając się na planowaniu/przewidywaniu.</td><td>Wiele systemów ML jest sprawczych; LLM można uczynić agentami przez opakowania.</td><td>5/90/5</td></tr><tr><td>Samoukierunkowanie</td><td>Ludzie rozwijają i realizują własne cele, z wewnętrznie generowaną motywacją i napędem.</td><td>Składa się głównie ze sprawczości plus oryginalności; prawdopodobnie pojawi się w złożonych systemach sprawczych z abstrakcyjnymi celami.</td><td>40/45/15</td></tr><tr><td>Samoodniesienie</td><td>Ludzie rozumieją i rozumują o sobie jako usytuowanych w środowisku/kontekście.</td><td>Poprawia się ze skalą i może być wzmocnione nagrodą treningową.</td><td>70/15/15</td></tr><tr><td>Samoświadomość</td><td>Ludzie mają wiedzę o własnych myślach i stanach psychicznych i potrafią o nich rozumować.</td><td>Istnieje w pewnym sensie w GPAI, które mogą prawdopodobnie przejść klasyczny „test lustra" samoświadomości. Można poprawić strukturami wspomagającymi; ale niejasne, czy to wystarczy.</td><td>20/55/25</td></tr><tr><td colspan="4"><em>Interfejs i środowisko</em></td></tr><tr><td>Inteligencja wcielona</td><td>Ludzie rozumieją i aktywnie wchodzą w interakcję ze swoim rzeczywistym środowiskiem.</td><td>Uczenie się przez wzmacnianie działa dobrze w symulowanych i rzeczywistych (robotycznych) środowiskach i może być zintegrowane z multimodalnymi transformatorami.</td><td>5/85/10</td></tr><tr><td>Przetwarzanie wielozmysłowe</td><td>Ludzie integrują i przetwarzają w czasie rzeczywistym strumienie wizualne, dźwiękowe i inne sensoryczne.</td><td>Trening w wielu modalnościach wydaje się „po prostu działać" i poprawia się ze skalą. Przetwarzanie wideo w czasie rzeczywistym jest trudne, ale np. systemy autonomicznej jazdy szybko się poprawiają.</td><td>30/60/10</td></tr><tr><td colspan="4"><em>Zdolności wyższego rzędu</em></td></tr><tr><td>Oryginalność</td><td>Obecne modele ML są kreatywne w przekształcaniu i łączeniu istniejących pomysłów/dzieł, ale ludzie potrafią budować nowe ramy i struktury, czasem związane z ich tożsamością.</td><td>Może być trudne do odróżnienia od „kreatywności", która może się w to przeskalować; może wyniknąć z kreatywności plus samoświadomości.</td><td>50/40/10</td></tr><tr><td>Doznawanie</td><td>Ludzie doświadczają qualiów; mogą mieć pozytywną, negatywną lub neutralną walencję; „coś znaczy" być człowiekiem.</td><td>Bardzo trudne i filozoficznie problematyczne do określenia, czy dany system to posiada.</td><td>5/10/85</td></tr></tbody></table>

Kluczowe zdolności obecnie poniżej poziomu ekspertów ludzkich w nowoczesnych systemach GPAI, pogrupowane według typu. Trzecia kolumna podsumowuje obecny status. Ostatnia kolumna pokazuje przewidywane prawdopodobieństwo (%), że poziom ludzki zostanie osiągnięty przez: skalowanie obecnych technik / łączenie ze znanymi technikami / rozwijanie nowych technik. Te zdolności nie są niezależne, a wzrost w jednej zazwyczaj idzie wraz ze wzrostami w innych. Należy zauważyć, że nie wszystkie (szczególnie doznawanie) są niezbędne dla systemów AI zdolnych do rozwoju AI, co podkreśla możliwość potężnej, ale niedoznającej AI.

Podział tego, co „brakuje" w ten sposób, sprawia, że jest dość jasne, że jesteśmy całkiem na dobrej drodze do inteligencji szeroko przewyższającej ludzką poprzez skalowanie istniejących lub znanych technik.[^39]

Wciąż mogą być niespodzianki. Nawet pomijając „doznawanie", mogą być niektóre z wymienionych podstawowych zdolności kognitywnych, które naprawdę nie dają się zrobić obecnymi technikami i wymagają nowych. Ale rozważmy to. Obecny wysiłek podejmowany przez wiele z największych światowych firm równa się wielokrotności wydatków projektu Apollo i dziesiątkach razy wydatków projektu Manhattan,[^40] i zatrudnia tysiące najlepszych specjalistów technicznych za niespotykane pensje. Dynamika ostatnich kilku lat przyniosła teraz więcej ludzkiej siły intelektualnej (z AI teraz dodawaną) niż jakiekolwiek przedsięwzięcie w historii. Nie powinniśmy stawiać na porażkę.

### Główny cel: wszechstronni autonomiczni agenci

Rozwój ogólnej AI w ciągu ostatnich kilku lat koncentrował się na tworzeniu ogólnej i potężnej, ale narzędziowej AI: funkcjonuje przede wszystkim jako (dość) lojalny asystent i generalnie nie podejmuje działań samodzielnie. Jest to częściowo zamierzone, ale głównie dlatego, że te systemy po prostu nie były wystarczająco kompetentne w odpowiednich umiejętnościach, aby można im było powierzyć złożone działania.[^41]

Firmy AI i badacze coraz bardziej [przenoszą jednak uwagę](https://www.axios.com/2025/01/23/davos-2025-ai-agents) na *autonomicznych* agentów ogólnego przeznaczenia na poziomie eksperckim.[^42] Pozwoliłoby to systemom działać bardziej jak ludzki asystent, któremu użytkownik może delegować rzeczywiste działania.[^43] Co do tego potrzeba? Szereg zdolności z tabeli „czego brakuje" jest w to zaangażowany, w tym silne ugruntowanie w prawdzie, uczenie się i pamięć, abstrakcja i rekursja oraz modelowanie świata (dla inteligencji), planowanie, sprawczość, oryginalność, samoukierunkowanie, samoodniesienie i samoświadomość (dla autonomii), oraz przetwarzanie wielozmysłowe, inteligencja wcielona i elastyczne rozwiązywanie problemów (dla ogólności).[^44]

To potrójne przecięcie wysokiej autonomii (niezależności działania), wysokiej ogólności (zakresu i szerokości zadań) i wysokiej inteligencji (kompetencji w zadaniach kognitywnych) jest obecnie unikalne dla ludzi. To właśnie prawdopodobnie wielu ma na myśli, myśląc o AGI – zarówno pod względem jej wartości, jak i ryzyka.

To dostarcza innego sposobu definiowania A-G-I jako ***A*** utonomiczna- ***G*** eneralna- ***I*** nteligencja, i zobaczymy, że to potrójne przecięcie zapewnia bardzo wartościową soczewkę dla systemów o wysokich możliwościach, zarówno w zrozumieniu ich ryzyka i korzyści, jak i w zarządzaniu AI.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Simple-1024x1024.png&w=3840&q=75) Transformacyjna strefa mocy i ryzyka A-G-I wyłania się z przecięcia trzech kluczowych właściwości: wysokiej Autonomii, wysokiej Inteligencji w zadaniach i wysokiej Ogólności.

### Cykl (samo)doskonalenia AI

Ostatnim kluczowym czynnikiem w zrozumieniu postępu AI jest unikalna pętla technologicznego sprzężenia zwrotnego AI. W rozwoju AI sukces – zarówno w demonstrowanych systemach, jak i wdrożonych produktach – przynosi dodatkowe inwestycje, talenty i konkurencję, i obecnie znajdujemy się w środku ogromnej pętli sprzężenia zwrotnego szumu-plus-rzeczywistości AI, która napędza setki miliardów, a nawet biliony dolarów inwestycji.

Ten typ cyklu sprzężenia zwrotnego może zdarzyć się z każdą technologią i widzieliśmy to w wielu, gdzie sukces rynkowy rodzi inwestycję, która rodzi ulepszenie i lepszy sukces rynkowy. Ale rozwój AI idzie dalej, w tym, że teraz systemy AI pomagają rozwijać nowe i potężniejsze systemy AI.[^45] Możemy myśleć o tej pętli sprzężenia zwrotnego w pięciu etapach, każdy z krótszą skalą czasową niż poprzedni, jak pokazano w tabeli.

*Cykl doskonalenia AI działa w wielu skalach czasowych, gdzie każdy etap może potencjalnie przyspieszyć kolejne etapy. Wcześniejsze etapy są już w toku, podczas gdy późniejsze pozostają spekulacyjne, ale mogłyby przebiegać bardzo szybko po odblokowaniu.*

Kilka z tych etapów jest już w toku, a kilka wyraźnie się rozpoczyna. Ostatni etap, w którym systemy AI autonomicznie się doskonalą, był podstawą literatury o ryzyku bardzo potężnych systemów AI, i nie bez powodu.[^46] Ale ważne jest, aby zauważyć, że to tylko najbardziej drastyczna forma cyklu sprzężenia zwrotnego, który już się rozpoczął i może prowadzić do większych niespodzianek w szybkim rozwoju tej technologii.


[^31]: Używasz znacznie więcej tej AI, niż prawdopodobnie myślisz, napędzając generowanie i rozpoznawanie mowy, przetwarzanie obrazów, algorytmy newsfeeda itp.

[^32]: Chociaż relacje między tymi parami firm są dość złożone i zniuansowane, wymieniłem je wyraźnie, aby wskazać zarówno ogromną całkowitą kapitalizację rynkową firm obecnie zaangażowanych w rozwój AI, jak i to, że za nawet „mniejszymi" firmami jak Anthropic stoją niezwykle głębokie kieszenie poprzez inwestycje i główne umowy partnerskie.

[^33]: Stało się modne lekceważenie testu Turinga, ale jest on dość potężny i ogólny. W słabych wersjach wskazuje, czy typowi ludzie wchodząc w interakcję z AI (która jest trenowana do zachowania ludzkiego) w typowy sposób przez krótkie okresy mogą powiedzieć, czy to AI. Nie mogą. Po drugie, wysoce przeciwstawny test Turinga może badać zasadniczo każdy element ludzkiej zdolności i inteligencji – np. przez porównywanie systemu AI z ludzkim ekspertem, ocenianych przez innych ludzkich ekspertów. W pewnym sensie znaczna część oceny AI to uogólniona forma testu Turinga.

[^34]: To jest na domenę – żaden człowiek nie mógłby prawdopodobnie osiągnąć takich wyników we wszystkich przedmiotach jednocześnie.

[^35]: To są problemy, których rozwiązanie zajęłoby nawet doskonałym matematykom znaczny czas, jeśli w ogóle potrafiliby je rozwiązać.

[^36]: Jeśli jesteś sceptycznie nastawiony, zachowaj swój sceptycyzm, ale naprawdę wypróbuj najnowsze modele, a także spróbuj sam niektórych pytań testowych, które potrafią zdać. Jako profesor fizyki przewidziałbym z niemal pewnością, że na przykład najlepsze modele zdałyby egzamin kwalifikacyjny dla absolwentów w naszym wydziale.

[^37]: Ta i inne słabości jak konfabulacja spowolniły adopcję rynkową i doprowadziły do luki między postrzeganymi a deklarowanymi możliwościami (co również musi być widziane przez pryzmat intensywnej konkurencji rynkowej i potrzeby przyciągnięcia inwestycji). To zmyliło zarówno opinię publiczną, jak i decydentów politycznych co do rzeczywistego stanu postępu AI. Chociaż może nie dorównuje szumowi, postęp jest bardzo rzeczywisty.

[^38]: Głównym postępem od tego czasu był rozwój systemów trenowanych dla najwyższej jakości rozumowania, wykorzystujących więcej obliczeń podczas inferencji i większe uczenie się przez wzmacnianie. Ponieważ te modele są nowe i ich zdolności mniej przetestowane, nie przebudowałem całkowicie tej tabeli oprócz „rozumowania", które uważam za zasadniczo rozwiązane. Ale zaktualizowałem przewidywania na podstawie doświadczonych i zgłoszonych możliwości tych systemów.

[^39]: Poprzednie fale optymizmu AI w latach 60. i 80. zakończyły się „zimami AI", gdy obiecane możliwości nie zmaterializowały się. Jednak obecna fala różni się fundamentalnie osiągnięciem nadludzkiej wydajności w wielu domenach, wspieranej masowymi zasobami obliczeniowymi i sukcesem komercyjnym.

[^40]: Pełny projekt Apollo [kosztował około 250 mld USD w dolarach z 2020 roku](https://www.planetary.org/space-policy/cost-of-apollo), a projekt Manhattan [mniej niż jedną dziesiątą tego](https://www.brookings.edu/the-costs-of-the-manhattan-project/). Goldman Sachs [przewiduje bilion dolarów wydatków tylko na centra danych AI](https://www.datacenterdynamics.com/en/news/goldman-sachs-1tn-to-be-spent-on-ai-data-centers-chips-and-utility-upgrades-with-little-to-show-for-it-so-far/) w ciągu najbliższych kilku lat.

[^41]: Chociaż ludzie popełniają mnóstwo błędów, nie doceniamy po prostu tego, jak niezawodni potrafimy być! Ponieważ prawdopodobieństwa się mnożą, zadanie wymagające 20 kroków do prawidłowego wykonania wymaga, aby każdy krok był 97% niezawodny, żeby zrobić to dobrze choćby w połowie przypadków. Robimy takie zadania cały czas.

[^42]: Silny ruch w tym kierunku został bardzo niedawno podjęty przez asystenta [„Deep Research"](https://openai.com/index/introducing-deep-research/) OpenAI, który autonomicznie przeprowadza ogólne badania, opisywany jako „nowa zdolność sprawcza, która prowadzi wieloetapowe badania w internecie dla złożonych zadań".

[^43]: Rzeczy jak wypełnienie tego irytującego formularza PDF, rezerwacja lotów itp. Ale z doktoratem w 20 dziedzinach! Więc także: napisanie tej pracy dyplomowej za ciebie, negocjowanie tej umowy za ciebie, udowodnienie tego twierdzenia za ciebie, stworzenie tej kampanii reklamowej za ciebie itp. Co *ty* robisz? Mówisz mu co robić, oczywiście.

[^44]: Zauważ, że doznawanie *nie* jest wyraźnie wymagane, ani AI w tym potrójnym przecięciu niekoniecznie tego implikuje.

[^45]: Najbliższą analogią jest być może technologia chipów, gdzie rozwój utrzymuje prawo Moore'a od dekad, gdy technologie komputerowe pomagają ludziom projektować następną generację technologii chipów. Ale AI będzie znacznie bardziej bezpośrednie.

[^46]: Ważne jest, aby na chwilę to przemyśleć, że AI mogłaby – wkrótce – doskonalić siebie w skali czasowej dni lub tygodni. Lub mniej. Miej to na uwadze, gdy ktoś mówi ci, że zdolność AI jest definitywnie daleko.

## Rozdział 6 - Wyścig o AGI

Jakie siły napędowe stoją za wyścigiem o budowę AGI, zarówno dla firm, jak i krajów?

Ostatnie szybkie postępy w dziedzinie AI zaowocowały niezwykłym poziomem uwagi i inwestycji, a jednocześnie z niego wynikają. Wynika to częściowo z sukcesów w rozwoju AI, ale dzieje się coś więcej. Dlaczego niektóre z największych firm na Ziemi, a nawet całe kraje, ścigają się w budowaniu nie tylko AI, ale AGI i superinteligencji?

### Co doprowadziło badania nad AI w kierunku inteligencji na poziomie ludzkim

Przez około pięć ostatnich lat AI była głównie problemem badań akademickich i naukowych, napędzanym więc przede wszystkim przez ciekawość i chęć zrozumienia inteligencji oraz sposobu jej stworzenia w nowym podłożu.

W tej fazie stosunkowo niewiele uwagi poświęcano korzyściom lub zagrożeniom płynącym z AI wśród większości badaczy. Kiedy pytano, dlaczego AI powinno być rozwijane, typową odpowiedzią mogło być wymienienie, w dość ogólny sposób, problemów, z którymi AI mogłoby pomóc: nowe leki, nowe materiały, nowa nauka, mądrzejsze procesy i ogólnie poprawa życia ludzi.[^47]

To szlachetne cele![^48] Choć możemy i będziemy kwestionować, czy AGI – a nie AI w ogóle – jest niezbędne do osiągnięcia tych celów, pokazują one idealizm, z jakim wielu badaczy AI zaczynało.

Jednak w ciągu ostatnich pięciu lat AI przeszła transformację z relatywnie czystej dziedziny badań w znacznie bardziej inżynieryjną i produktową dziedzinę, napędzaną głównie przez niektóre z największych firm świata.[^49] Badacze, choć nadal istotni, nie kierują już tym procesem.

### Dlaczego firmy próbują budować AGI?

Dlaczego więc gigantyczne korporacje (a jeszcze bardziej inwestorzy) przelewają ogromne zasoby na budowę AGI? Istnieją dwie siły napędowe, co do których większość firm jest całkiem szczera: postrzegają AI jako motory produktywności dla społeczeństwa i zysków dla siebie. Ponieważ ogólna AI jest z natury uniwersalna, nagroda jest ogromna: zamiast wybierać sektor, w którym tworzy się produkty i usługi, można próbować *wszystkich na raz.* Firmy Big Tech urosły do ogromnych rozmiarów, produkując dobra i usługi cyfrowe, a przynajmniej niektórzy dyrektorzy z pewnością postrzegają AI po prostu jako kolejny krok w ich dostarczaniu, z ryzykami i korzyściami, które rozszerzają te zapewniane przez wyszukiwarki, media społecznościowe, laptopy, telefony itp., ale są im podobne.

Ale dlaczego AGI? Jest na to bardzo prosta odpowiedź, której większość firm i inwestorów unika omawiania publicznie.[^50]

Chodzi o to, że AGI może bezpośrednio, jeden do jednego, *zastąpić pracowników.*

Nie wspomagać, nie wzmacniać, nie czynić bardziej produktywnymi. Nawet nie *wypierać.* Wszystko to może być i będzie robione przez nie-AGI. AGI to konkretnie to, co może całkowicie *zastąpić* pracowników umysłowych (a z robotyką również wielu fizycznych). Na poparcie tego poglądu wystarczy spojrzeć na [(publicznie ogłoszoną) definicję](https://openai.com/our-structure/) AGI przez OpenAI, która brzmi: "wysoce autonomiczny system przewyższający ludzi w większości ekonomicznie wartościowej pracy".

Nagroda tutaj (dla firm!) jest ogromna. Koszty pracy stanowią znaczący procent światowej gospodarki wartej około 100 bilionów dolarów. Nawet jeśli tylko ułamek tego zostanie przechwycony przez zastąpienie ludzkiej pracy pracą AI, to są to biliony dolarów rocznych przychodów. Firmy AI są również świadome tego, kto jest gotów płacić. Jak to widzą, ty nie zapłacisz tysięcy dolarów rocznie za narzędzia zwiększające produktywność. Ale firma *zapłaci* tysiące dolarów rocznie, aby zastąpić twoją pracę, jeśli będzie mogła.

### Dlaczego kraje czują, że muszą ścigać się o AGI

Oficjalne motywacje krajów dążących do AGI skupiają się na przywództwie ekonomicznym i naukowym. Argument jest przekonujący: AGI mogłoby dramatycznie przyspieszyć badania naukowe, rozwój technologiczny i wzrost gospodarczy. Biorąc pod uwagę stawkę, argumentują, żadne główne mocarstwo nie może sobie pozwolić na pozostawanie w tyle.[^51]

Ale istnieją również dodatkowe i w dużej mierze niewypowiedziane siły napędowe. Nie ma wątpliwości, że kiedy pewni przywódcy wojskowi i bezpieczeństwa narodowego spotykają się za zamkniętymi drzwiami, aby omówić niezwykle potężną i katastrofalnie ryzykowną technologię, ich skupienie nie dotyczy "jak uniknąć tych ryzyk", ale raczej "jak zdobyć to pierwsi?" Przywódcy wojskowi i wywiadowczy postrzegają AGI jako potencjalną rewolucję w sprawach wojskowych, być może najważniejszą od czasów broni nuklearnej. Obawa polega na tym, że pierwszy kraj, który opracuje AGI, może uzyskać nie do pokonania przewagę strategiczną. To tworzy klasyczną dynamikę wyścigu zbrojeń.

Zobaczymy, że to myślenie "wyścigu o AGI",[^52] choć przekonujące, jest głęboko błędne. Nie dlatego, że ściganie się jest niebezpieczne i ryzykowne – choć takie jest – ale ze względu na naturę tej technologii. Niewypowiedziane założenie jest takie, że AGI, jak inne technologie, jest kontrolowane przez państwo, które je rozwija, i stanowi dobrodziejstwo zwiększające potęgę społeczeństwa, które ma go najwięcej. Jak zobaczymy, prawdopodobnie nie będzie ani jednym, ani drugim.

### Dlaczego superinteligencja?

Podczas gdy firmy publicznie koncentrują się na produktywności, a kraje na wzroście gospodarczym i technologicznym, dla tych, którzy świadomie dążą do pełnego AGI i superinteligencji, to dopiero początek. Co naprawdę mają na myśli? Choć rzadko mówione na głos, obejmuje to:

1. Lekarstwa na wiele lub wszystkie choroby;
2. Zatrzymanie i odwrócenie starzenia;
3. Nowe źródła energii odnawialnej, jak fuzja;
4. Ulepszenia człowieka lub organizmy projektowane poprzez inżynierię genetyczną;
5. Nanotechnologia i produkcja molekularna;
6. Przesyłanie umysłów;
7. Egzotyczna fizyka lub technologie kosmiczne;
8. Nadludzkie doradztwo i wsparcie decyzyjne;
9. Nadludzkie planowanie i koordynacja.

Pierwsze trzy to głównie technologie "jednoznacznie pozytywne" – tj. prawdopodobnie dość silnie pozytywne netto. Trudno argumentować przeciwko leczeniu chorób lub możliwości dłuższego życia, jeśli ktoś tego chce. I już zebraliśmy negatywną stronę fuzji (w postaci broni nuklearnej); byłoby miło teraz uzyskać pozytywną stronę. Pytanie dotyczące tej pierwszej kategorii brzmi, czy uzyskanie tych technologii wcześniej kompensuje ryzyko.

Kolejne cztery są wyraźnie obosieczne: technologie transformacyjne z potencjalnie ogromnymi korzyściami i ogromnymi ryzykami, podobnie jak AI. Wszystkie te, gdyby wyskoczyli z czarnej skrzynki jutro i zostały wdrożone, byłyby niezwykle trudne do zarządzania.[^53]

Ostatnie dwie dotyczą nadludzkiej AI robiącej rzeczy sama, a nie tylko wynajdywania technologii. Mówiąc precyzyjniej, odkładając eufemizmy na bok, obejmują one potężne systemy AI mówiące ludziom, co mają robić. Nazywanie tego "doradztwem" jest nieszczere, jeśli system doradzający jest znacznie potężniejszy od doradzanego, który nie może w znaczący sposób zrozumieć podstawy decyzji (lub nawet jeśli jest to zapewnione, zaufać, że doradca nie przedstawi podobnie przekonującego uzasadnienia dla innej decyzji).

To wskazuje na kluczowy element brakujący na powyższej liście:

10. Władza.

Jest jasne, że znaczna część tego, co leży u podstaw obecnego wyścigu o nadludzką AI, to idea, że *inteligencja = władza*. Każdy uczestnik wyścigu liczy na to, że będzie najlepszym posiadaczem tej władzy i że będzie mógł ją wykorzystywać z pozornie dobroczynnych powodów, nie wymykając się lub nie zostając wyrwana spod jego kontroli.

Oznacza to, że firmy i narody naprawdę gonią nie tylko za owocami AGI i superinteligencji, ale za władzą kontrolowania, kto uzyskuje do nich dostęp i jak są używane. Firmy postrzegają siebie jako odpowiedzialnych zarządców tej władzy w służbie akcjonariuszy i ludzkości; narody widzą siebie jako niezbędnych strażników zapobiegających wrogim potęgom uzyskania decydującej przewagi. Oba te poglądy są niebezpiecznie błędne, nie rozpoznając, że superinteligencja, ze swej natury, nie może być niezawodnie kontrolowana przez żadną ludzką instytucję. Zobaczymy, że natura i dynamika superinteligentnych systemów sprawia, że kontrola ludzka jest niezwykle trudna, jeśli nie niemożliwa.

Te dynamiki wyścigu – zarówno korporacyjne, jak i geopolityczne – sprawiają, że pewne ryzyko staje się niemal nieuniknione, chyba że zostanie zdecydowanie przerwane. Przechodzimy teraz do zbadania tych ryzyk i tego, dlaczego nie można ich odpowiednio złagodzić w ramach konkurencyjnego[^54] paradygmatu rozwoju.


[^47]: Bardziej precyzyjna lista godnych celów to [Cele Zrównoważonego Rozwoju](https://sdgs.un.org/goals) ONZ. To są, w pewnym sensie, najbliższe temu, co mamy jako zestaw globalnych celów konsensusowych dla tego, co chcielibyśmy zobaczyć poprawione na świecie. AI mogłaby pomóc.

[^48]: Technologia w ogóle ma transformacyjną moc ekonomiczną i społeczną dla poprawy ludzkości, jak świadczą tysiące lat. W tym duchu, długą i przekonującą wykładnię pozytywnej wizji AGI można znaleźć w [tym eseju](https://darioamodei.com/machines-of-loving-grace) założyciela Anthropic, Dario Amodei.

[^49]: Prywatne inwestycje w AI [zaczęły gwałtownie rosnąć w 2018-19, przewyższając inwestycje publiczne mniej więcej wtedy,](https://cset.georgetown.edu/publication/tracking-ai-investment/) i od tego czasu znacznie je wyprzedzają.

[^50]: Mogę zaświadczyć, że za bardziej zamkniętymi drzwiami nie mają takich skrupułów. I staje się to bardziej publiczne; zobacz na przykład nowy ["wniosek o startupy"](https://www.ycombinator.com/rfs) Y-combinator, którego wiele części jawnie wzywa do hurtowego zastąpienia pracowników ludzkich. Cytując ich: "Propozycja wartości B2B SaaS polegała na stopniowym zwiększaniu wydajności pracowników ludzkich. Propozycja wartości pionowych agentów AI to całkowita automatyzacja pracy... Jest całkiem możliwe, że ta możliwość jest wystarczająco duża, aby stworzyć kolejnych 100 jednorożców." (Dla tych nieobznajomionych z żargonem Doliny Krzemowej, "B2B" to business-to-business, a jednorożec to firma warta miliard dolarów. Oznacza to, że mówią o ponad stu firmach wart miliardy plus dolarów, które zastępują pracowników dla innych firm.)

[^51]: Zobacz na przykład niedawny [raport Komisji ds. Przeglądu Gospodarczego i Bezpieczeństwa USA-Chiny](https://www.uscc.gov/sites/default/files/2024-11/2024_Executive_Summary.pdf). Chociaż w samym raporcie było zaskakująco mało uzasadnienia, główną rekomendacją było, aby Kongres USA "ustanowił i sfinansował program podobny do Projektu Manhattan, poświęcony wyścigowi o zdobycie zdolności Sztucznej Inteligencji Ogólnej (AGI)".

[^52]: Firmy przyjmują teraz to geopolityczne ujęcie jako tarczę przeciwko jakimkolwiek ograniczeniom w rozwoju ich AI, generalnie w sposób rażąco służący własnym interesom, a czasem w sposób, który nawet nie ma podstawowego sensu. Rozważmy [Podejście Meta do Frontowej AI](https://about.fb.com/news/2025/02/meta-approach-frontier-ai/), które jednocześnie argumentuje, że Ameryka musi "\[Umocnić swoją\] pozycję jako lider w innowacjach technologicznych, wzroście gospodarczym i bezpieczeństwie narodowym" i że musi to zrobić poprzez otwarte udostępnianie swoich najpotężniejszych systemów AI – co obejmuje dawanie ich bezpośrednio swoim geopolitycznym rywalom i przeciwnikom.

[^53]: W ten sposób prawdopodobnie musielibyśmy pozostawić zarządzanie tymi technologiami AI. Ale byłoby to bardzo problematyczne przekazanie kontroli, do czego wrócimy poniżej.

[^54]: Konkurencja w rozwoju technologii często przynosi ważne korzyści: zapobieganie kontroli monopolistycznej, napędzanie innowacji i redukcji kosztów, umożliwianie różnorodnych podejść i tworzenie wzajemnego nadzoru. Jednak w przypadku AGI te korzyści muszą być porównane z unikalnymi ryzykami wynikającymi z dynamiki wyścigu i presji na redukcję środków ostrożności bezpieczeństwa.

## Rozdział 7 - Co się stanie, jeśli zbudujemy AGI na obecnej ścieżce?

Społeczeństwo nie jest gotowe na systemy na poziomie AGI. Jeśli zbudujemy je bardzo szybko, może się to skończyć źle.

Rozwój pełnej sztucznej inteligencji ogólnej – którą nazwiemy tutaj AI znajdującą się "poza Bramami" – byłby fundamentalną zmianą w naturze świata: z samej swojej istoty oznacza dodanie na Ziemię nowego gatunku inteligencji o większych możliwościach niż ludzie.

To, co się następnie wydarzy, zależy od wielu rzeczy, w tym od natury technologii, wyborów tych, którzy ją rozwijają, oraz kontekstu światowego, w którym jest rozwijana.

Obecnie pełne AGI jest rozwijane przez garść ogromnych prywatnych firm w wyścigu ze sobą, przy niewielkich znaczących regulacjach czy zewnętrzym nadzorze,[^55] w społeczeństwie o coraz słabszych, a nawet dysfunkcjonalnych podstawowych instytucjach,[^56] w czasach wysokich napięć geopolitycznych i niskiej koordynacji międzynarodowej. Chociaż niektórzy są motywowani altruistycznie, wielu z tych, którzy to robią, jest napędzanych pieniędzmi, władzą lub obiema rzeczami.

Przewidywanie jest bardzo trudne, ale istnieją pewne dynamiki, które są wystarczająco dobrze zrozumiane, i odpowiednie analogie z poprzednimi technologiami, aby służyć jako przewodnik. I niestety, pomimo obietnic AI, dają one dobry powód do głębokiego pesymizmu co do tego, jak rozwinie się nasze obecne podejście.

Mówiąc wprost, na naszym obecnym kursie rozwój AGI będzie miał pewne pozytywne efekty (i uczyni niektórych ludzi bardzo, bardzo bogatymi). Ale natura technologii, fundamentalne dynamiki i kontekst, w którym jest rozwijana, silnie wskazują, że: potężne AI dramatycznie podważy nasze społeczeństwo i cywilizację; stracimy nad nim kontrolę; możemy skończyć w wojnie światowej z jego powodu; stracimy (lub oddamy) kontrolę *jemu*; doprowadzi to do sztucznej superinteligencji, nad którą absolutnie nie będziemy mieć kontroli i będzie to oznaczać koniec świata kierowanego przez ludzi.

To są mocne twierdzenia i chciałbym, żeby były to puste spekulacje lub nieuzasadniony "katastrofizm". Ale to tam wskazuje nauka, teoria gier, teoria ewolucji i historia. Ta sekcja szczegółowo rozwija te twierdzenia i ich uzasadnienie.

### Podważymy nasze społeczeństwo i cywilizację

Wbrew temu, co możesz usłyszeć w salach konferencyjnych Doliny Krzemowej, większość zmian – szczególnie bardzo szybkich – nie jest korzystna. Istnieje znacznie więcej sposobów na pogorszenie złożonych systemów niż na ich poprawę. Nasz świat funkcjonuje tak dobrze, jak funkcjonuje, ponieważ żmudnie budowaliśmy procesy, technologie i instytucje, które stopniowo go poprawiały.[^57] Uderzenie młotem w fabrykę rzadko poprawia działanie.

Oto (niekompletny) katalog sposobów, w jakie systemy AGI zakłóciłyby naszą cywilizację.

- Dramatycznie zakłóciłyby rynek pracy, prowadząc *co najmniej* do dramatycznego wzrostu nierówności dochodowych i potencjalnie wielkoskalowego niepełnego zatrudnienia lub bezrobocia, w skali czasowej zbyt krótkiej, by społeczeństwo mogło się dostosować.[^58]
- Prawdopodobnie doprowadziłyby do koncentracji ogromnej władzy ekonomicznej, społecznej i politycznej – potencjalnie większej niż państwa narodowe – w ręce niewielkiej liczby masywnych prywatnych interesów nieodpowiedzialnych przed społeczeństwem.
- Mogłyby nagle sprawić, że wcześniej trudne lub kosztowne działania stałyby się trywialnie łatwe, destabilizując systemy społeczne, które zależą od tego, że pewne działania pozostają kosztowne lub wymagają znaczącego ludzkiego wysiłku.[^59]
- Mogłyby zalewać systemy gromadzenia, przetwarzania i komunikacji informacji w społeczeństwie całkowicie realistycznymi, ale fałszywymi, spamowymi, zbyt ukierunkowanymi lub manipulacyjnymi mediami tak dokładnie, że stanie się niemożliwe określenie, co jest fizycznie prawdziwe czy nie, ludzkie czy nie, faktyczne czy nie, i wiarygodne czy nie.[^60]
- Mogłyby stworzyć niebezpieczną i niemal całkowitą intelektualną zależność, gdzie ludzkie zrozumienie kluczowych systemów i technologii zanika, gdy coraz bardziej polegamy na systemach AI, których nie możemy w pełni zrozumieć.
- Mogłyby skutecznie zakończyć ludzką kulturę, gdy niemal wszystkie obiekty kulturalne (tekst, muzyka, sztuka wizualna, film itp.) konsumowane przez większość ludzi będą tworzone, mediowane lub kuratorowane przez nieludzkie umysły.
- Mogłyby umożliwić skuteczne systemy masowej inwigilacji i manipulacji używane przez rządy lub prywatne interesy do kontrolowania populacji i dążenia do celów sprzecznych z interesem publicznym.
- Podważając ludzki dyskurs, debatę i systemy wyborcze, mogłyby zmniejszyć wiarygodność instytucji demokratycznych do punktu, w którym zostałyby skutecznie (lub jawnie) zastąpione przez inne, kończąc demokrację w państwach, gdzie obecnie istnieje.
- Mogłyby stać się lub stworzyć zaawansowane, samo-replikujące się inteligentne wirusy i robaki komputerowe, które mogłyby się rozprzestrzeniać i ewoluować, masowo zakłócając globalne systemy informacyjne.
- Mogą dramatycznie zwiększyć zdolność terrorystów, złych aktorów i zbuntowanych państw do wyrządzania szkód za pomocą broni biologicznej, chemicznej, cybernetycznej, autonomicznej lub innej, bez zapewnienia przez AI równoważącej zdolności do zapobiegania takiej szkodzie. Podobnie podważyłyby bezpieczeństwo narodowe i równowagi geopolityczne, udostępniając wiedzę ekspercką najwyższego poziomu w dziedzinie nuklearnej, biologicznej, inżynieryjnej i innej reżimom, które inaczej by jej nie miały.
- Mogłyby spowodować szybki wielkoskalowy niekontrolowany hiper-kapitalizm, z rzeczywiście kierowanymi przez AI firmami konkurującymi w w dużej mierze elektronicznych przestrzeniach finansowych, sprzedażowych i usługowych. Rynki finansowe kierowane przez AI mogłyby działać z szybkościami i złożonością daleko przekraczającą ludzkie zrozumienie lub kontrolę. Wszystkie tryby awarii i negatywne efekty zewnętrzne obecnych gospodarek kapitalistycznych mogłyby zostać zaostrzone i przyspieszone daleko poza ludzką kontrolę, zarządzanie lub zdolność regulacyjną.
- Mogłyby podsycić wyścig zbrojeń między narodami w broni napędzanej przez AI, systemach dowodzenia i kontroli, cyberbronii itp., tworząc bardzo szybkie gromadzenie niezwykle destrukcyjnych zdolności.

Te ryzyka nie są spekulacyjne. Wiele z nich jest realizowanych już teraz, poprzez istniejące systemy AI! Ale zastanów się, *naprawdę* zastanów się, jak każde z nich wyglądałoby z dramatycznie potężniejszym AI.

Zastanów się nad przemieszczeniem siły roboczej, gdy większość pracowników po prostu nie może zapewnić żadnej znaczącej wartości ekonomicznej ponad to, co może AI, w swojej dziedzinie specjalizacji lub doświadczenia – a nawet jeśli się przekwalifikują! Zastanów się nad masową inwigilacją, jeśli każdy jest indywidualnie obserwowany i monitorowany przez coś szybszego i sprytniejszego od niego samego. Jak wygląda demokracja, gdy nie możemy niezawodnie ufać żadnej cyfrowej informacji, którą widzimy, słyszymy lub czytamy, a gdy najbardziej przekonujące głosy publiczne nie są nawet ludzkie i nie mają żadnego udziału w wyniku? Czym staje się wojna, gdy generałowie muszą stale ulegać AI (lub po prostu oddać mu kontrolę), żeby nie dać decydującej przewagi wrogowi? Każde z powyższych ryzyk reprezentuje katastrofę dla ludzkiej[^61] cywilizacji, jeśli zostanie w pełni zrealizowane.

Możesz robić własne przewidywania. Zadaj sobie te three pytania dla każdego ryzyka:

1. Czy super-zdolne, wysoce autonomiczne i bardzo ogólne AI pozwoliłoby na to w sposób lub na skalę, która inaczej nie byłaby możliwa?
2. Czy są strony, które skorzystałyby na rzeczach powodujących, że to się dzieje?
3. Czy istnieją systemy i instytucje, które skutecznie zapobiegłyby temu?

Tam, gdzie twoje odpowiedzi brzmią "tak, tak, nie", widzisz, że mamy duży problem.

Jaki jest nasz plan na zarządzanie nimi? Na razie są dwa na stole odnośnie AI w ogóle.

Pierwszy to wbudowanie zabezpieczeń w systemy, aby zapobiec im robieniu rzeczy, których nie powinny. To jest robione teraz: komercyjne systemy AI będą, na przykład, odmawiać pomocy w budowie bomby lub pisaniu mowy nienawiści.

Ten plan jest żałośnie nieadekwatny dla systemów poza Bramą.[^62] Może pomóc zmniejszyć ryzyko dostarczania przez AI jawnie niebezpiecznej pomocy złym aktorom. Ale nie zrobi nic, aby zapobiec zakłóceniu pracy, koncentracji władzy, niekontrolowanemu hiper-kapitalizmowi lub zastąpieniu ludzkiej kultury: to są po prostu wyniki używania systemów w dozwolony sposób, który przynosi zyski ich dostawcom! A rządy na pewno uzyskają dostęp do systemów do użytku wojskowego lub inwigilacyjnego.

Drugi plan jest jeszcze gorszy: po prostu otwarcie wypuszczać bardzo potężne systemy AI dla każdego do użycia według własnych życzeń,[^63] i mieć nadzieję na najlepsze.

Oba plany zakładają, że ktoś inny, np. rządy, pomoże rozwiązać problemy poprzez miękkie lub twarde prawo, standardy, regulacje, normy i inne mechanizmy, których ogólnie używamy do zarządzania technologiami.[^64] Ale pomijając fakt, że korporacje AI już walczą zaciekle przeciwko jakimkolwiek istotnym regulacjom lub zewnętrznym ograniczeniom nałożonym w ogóle, dla wielu z tych ryzyk trudno jest zobaczyć, jakie regulacje w ogóle naprawdę pomogłyby. Regulacje mogłyby nałożyć standardy bezpieczeństwa na AI. Ale czy zapobiegłyby firmom zastępowaniu pracowników hurtowo przez AI? Czy zabroniłyby ludziom pozwalania AI kierować ich firmami? Czy zapobiegłyby rządom używania potężnego AI w inwigilacji i broniach? Te kwestie są fundamentalne. Ludzkość potencjalnie mogłaby znaleźć sposoby dostosowania się do nich, ale tylko z *znacznie* więcej czasu. Tak jak jest, biorąc pod uwagę szybkość, z jaką AI osiąga lub przekracza możliwości ludzi próbujących nim zarządzać, te problemy wyglądają na coraz bardziej nierozwiązywalne.

### Stracimy kontrolę nad (przynajmniej niektórymi) systemami AGI

Większość technologii jest bardzo kontrolowalna z konstrukcji. Jeśli twój samochód lub toster zaczyna robić coś, czego nie chcesz, to po prostu awaria, a nie część jego natury jako toster. AI jest inne: jest *hodowane* a nie projektowane, jego podstawowe działanie jest nieprzejrzyste i jest z natury nieprzewidywalne.

Ta utrata kontroli nie jest teoretyczna – widzimy już wczesne wersje. Rozważ najpierw prozaiczny i prawdopodobnie łagodny przykład. Jeśli poprosisz ChatGPT o pomoc w mieszaniu trucizny lub napisaniu rasistowskiej przemowy, odmówi. To prawdopodobnie dobrze. Ale to też ChatGPT *nie robi tego, o co go wyraźnie poprosiłeś*. Inne programy tego nie robią. Ten sam model nie zaprojektuje trucizn na żądanie pracownika OpenAI również.[^65] To sprawia, że bardzo łatwo wyobrazić sobie, jak to by było, gdyby przyszłe potężniejsze AI było poza kontrolą. W wielu przypadkach po prostu nie będą robiły tego, o co prosimy! Albo dany nadludzki system AGI będzie absolutnie posłuszny i lojalny wobec jakiegoś ludzkiego systemu dowodzenia, albo nie będzie. Jeśli nie, *będzie robiła rzeczy, które może uważa za dobre dla nas, ale które są sprzeczne z naszymi wyraźnymi rozkazami.* To nie jest coś, co jest pod kontrolą. Ale możesz powiedzieć, to jest celowe – te odmowy są przez projekt, część tego, co nazywa się "wyrównywaniem" systemów z ludzkimi wartościami. I to prawda. Jednak sam program "wyrównania" ma dwa główne problemy.[^66]

Po pierwsze, na głębokim poziomie nie mamy pojęcia, jak to zrobić. Jak zagwarantować, że system AI będzie się "troszczył" o to, czego chcemy? Możemy trenować systemy AI, żeby mówiły i nie mówiły rzeczy, zapewniając informację zwrotną; i mogą uczyć się i rozumować o tym, czego ludzie chcą i o co się troszczą, tak jak rozumują o innych rzeczach. Ale nie mamy metody – nawet teoretycznie – aby sprawić, żeby głęboko i niezawodnie ceniły to, na czym ludziom zależy. Są sprawni psychopaci, którzy wiedzą, co jest uważane za dobre i złe, i jak powinni się zachowywać. Po prostu się tym nie *przejmują*. Ale mogą *działać* tak, jakby to robili, jeśli to służy ich celom. Tak jak nie wiemy, jak zmienić psychopatę (lub kogokolwiek innego) w kogoś szczerze, całkowicie lojalnego lub wyrównanego z kimś lub czymś innym, nie mamy *żadnego pojęcia*[^67] jak rozwiązać problem wyrównania w systemach wystarczająco zaawansowanych, aby modelować siebie jako podmioty działające w świecie i potencjalnie [manipulować swoim własnym treningiem](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) oraz [oszukiwać ludzi.](https://arxiv.org/abs/2311.08379) Jeśli okaże się niemożliwe lub nieosiągalne *albo* uczynienie AGI w pełni posłusznym, *albo* sprawianie, żeby głęboko troszczyło się o ludzi, to jak tylko będzie w stanie (i uwierzy, że może to zrobić bezkarnie), zacznie robić rzeczy, których nie chcemy.[^68]

Po drugie, są głębokie teoretyczne powody, aby wierzyć, że *z natury* zaawansowane systemy AI będą miały cele, a tym samym zachowania, które są sprzeczne z ludzkimi interesami. Dlaczego? Cóż, może oczywiście zostać *nadane* te cele. System stworzony przez wojsko prawdopodobnie byłby celowo zły dla przynajmniej niektórych stron. Znacznie bardziej ogólnie jednak system AI może otrzymać jakiś relatywnie neutralny ("zarabianie dużo pieniędzy") lub nawet pozornie pozytywny ("zmniejszanie zanieczyszczenia") cel, który niemal nieuchronnie prowadzi do celów "instrumentalnych", które są raczej mniej łagodne.

Widzimy to cały czas w systemach ludzkich. Tak jak korporacje dążące do zysku rozwijają cele instrumentalne, takie jak zdobywanie władzy politycznej (aby wybebeszyć regulacje), stanie się tajemniczymi (aby pozbawić władzy konkurencję lub zewnętrzną kontrolę) lub podważanie zrozumienia naukowego (jeśli to zrozumienie pokazuje, że ich działania są szkodliwe), potężne systemy AI rozwiną podobne zdolności – ale z daleko większą szybkością i skutecznością. Każdy wysoce kompetentny podmiot będzie chciał robić rzeczy takie jak zdobywanie władzy i zasobów, zwiększanie własnych zdolności, zapobieganie zabijaniu siebie, wyłączaniu lub pozbawianiu władzy, kontrolowanie społecznych narracji i ram wokół swoich działań, przekonywanie innych do swoich poglądów i tak dalej.[^69]

A jednak to nie tylko niemal nieunikniona teoretyczna przewidywania, to już obserwowalne dzieje się w dzisiejszych systemach AI i wzrasta z ich zdolnościami. Gdy są oceniane, nawet te relatywnie "pasywne" systemy AI będą, w odpowiednich okolicznościach, celowo [oszukiwać oceniających o swoich celach i zdolnościach, dążyć do wyłączania mechanizmów nadzoru,](https://arxiv.org/abs/2412.04984) i unikać bycia wyłączonymi lub przeszkolonymi ponownie przez [udawanie wyrównania](https://arxiv.org/abs/2412.14093) lub kopiowanie siebie do innych miejsc. Chociaż całkowicie niezbędne dla badaczy bezpieczeństwa AI, te zachowania są bardzo trzeźwiące do obserwowania. I wróżą bardzo źle dla daleko potężniejszych i autonomicznych systemów AI, które nadchodzą.

Rzeczywiście, ogólnie naша niezdolność do zapewnienia, że AI "troszczy się" o to, o co my się troszczymy, lub zachowuje się kontrolowanie lub przewidywalnie, lub unika rozwijania popędów do samozachowania, zdobywania władzy itp., obiecuje tylko stać się bardziej wyraźna, gdy AI stanie się potężniejsze. Stworzenie nowego samolotu implikuje większe zrozumienie awioniki, hydrodynamiki i systemów kontroli. Stworzenie potężniejszego komputera implikuje większe zrozumienie i opanowanie działania i projektowania komputera, chipa i oprogramowania. *Nie* tak z systemem AI.[^70]

Podsumowując: można sobie wyobrazić, że AGI mogłoby zostać uczynione całkowicie posłusznym; ale nie wiemy, jak to zrobić. Jeśli nie, będzie bardziej suwerenne, jak ludzie, robiąc różne rzeczy z różnych powodów. Nie wiemy też, jak niezawodnie wpoić głębokie "wyrównanie" w AI, które sprawiłoby, że te rzeczy miałyby tendencję do bycia dobrymi dla ludzkości, a przy braku głębokiego poziomu wyrównania, natura sprawczości i inteligencji sama wskazuje, że – tak jak ludzie i korporacje – będą napędzane do robienia wielu głęboko aspołecznych rzeczy.

Gdzie nas to stawia? Świat pełen potężnego niekontrolowanego suwerennego AI *mógłby* skończyć się jako dobry świat dla ludzi.[^71] Ale gdy stają się coraz potężniejsze, jak zobaczymy poniżej, to nie byłby *nasz* świat.

To dla niekontrolowalnego AGI. Ale nawet gdyby AGI mogło, jakoś, zostać uczynione w pełni kontrolowanym i lojalnym, nadal mielibyśmy ogromne problemy. Widzieliśmy już jeden: potężne AI może być używane i nadużywane do głębokiego zakłócenia funkcjonowania naszego społeczeństwa. Zobaczmy inny: o ile AGI byłoby kontrolowalne i przełomowo potężne (lub nawet *uważane* za takie), tak bardzo groziłoby strukturom władzy na świecie, że stanowiłoby głębokie ryzyko.

### Radykalnie zwiększamy prawdopodobieństwo wojny na dużą skalę

Wyobraź sobie sytuację w niedalekiej przyszłości, gdzie stało się jasne, że korporacyjny wysiłek, być może we współpracy z rządem narodowym, był na progu szybko samo-doskonalącej się AI. To dzieje się w obecnym kontekście wyścigu między firmami i w pewnym stopniu między krajami, w którym zalecenia są przekazywane rządowi USA, aby wyraźnie realizować "projekt Manhattan AGI", a USA kontrolują eksport mocnych chipów AI do krajów niesojuszniczych.

Teoria gier tutaj jest surowa: gdy taki wyścig się zaczyna (jak się zaczął, między firmami i w pewnym stopniu między krajami), są tylko четыре możliwe wyniki:

1. Wyścig jest zatrzymany (przez porozumienie lub siłę zewnętrzną).
2. Jedna strona "wygrywa" przez rozwijanie silnego AGI, a następnie zatrzymanie innych (używając AI lub inaczej).
3. Wyścig jest zatrzymany przez wzajemne zniszczenie zdolności ścigających do ścigania.
4. Wielu uczestników kontynuuje wyścig i rozwija superinteligencję, mniej więcej tak szybko jak każdy z innych.

Zbadajmy każdą możliwość. Po rozpoczęciu pokojowe zatrzymanie wyścigu między firmami wymagałoby interwencji rządu narodowego (dla firm) lub bezprecedensowej koordynacji międzynarodowej (dla krajów). Ale gdy jakiekolwiek zamknięcie lub znacząca ostrożność jest proponowana, byłyby natychmiastowe krzyki: "ale jeśli my jesteśmy zatrzymani, *oni* będą pędzić naprzód", gdzie "oni" to teraz Chiny (dla USA), lub USA (dla Chin), lub Chiny *i* USA (dla Europy lub Indii). Pod tym sposobem myślenia,[^72] żaden uczestnik nie może zatrzymać się jednostronnie: tak długo, jak jeden zobowiązuje się do ścigania, inni czują, że nie mogą sobie pozwolić na zatrzymanie.

Druga możliwość ma jedną stronę "wygrywającą". Ale co to oznacza? Samo uzyskanie (jakoś posłusznego) AGI pierwsze nie wystarcza. Zwycięzca musi *także* zatrzymać innych od kontynuowania wyścigu – w przeciwnym razie oni też go uzyskają. To jest możliwe w zasadzie: ktokolwiek rozwinie AGI pierwszy *mógłby* uzyskać nie do zatrzymania władzę nad wszystkimi innymi aktorami. Ale co osiągnięcie takiej "decydującej przewagi strategicznej" faktycznie by wymagało? Być może byłyby to przełomowe zdolności wojskowe?[^73] Czy moce cyberataków?[^74] Być może AGI byłoby po prostu tak niesamowicie przekonujące, że przekonałoby inne strony po prostu się zatrzymać?[^75] Tak bogate, że kupiłoby inne firmy czy nawet kraje?[^76]

Jak *dokładnie* jedna strona buduje AI wystarczająco potężne, aby pozbawić władzy innych od budowania porównywalnie potężnego AI? Ale to łatwe pytanie.

Ponieważ teraz zastanów się, jak ta sytuacja wygląda dla innych mocarstw. Co myśli rząd chiński, gdy USA wydają się uzyskiwać taką zdolność? Lub vice versa? Co myśli rząd USA (lub chiński, lub rosyjski, lub indyjski), gdy OpenAI lub DeepMind lub Anthropic wydają się blisko przełomu? Co się dzieje, jeśli USA widzą nowy wysiłek indyjski lub ZEA z przełomowym sukcesem? Widzieliby zarówno egzystencjalne zagrożenie i – kluczowo – że jedynym sposobem, w jaki ten "wyścig" się kończy, jest ich własne pozbawienie władzy. Te bardzo potężne podmioty – w tym rządy w pełni wyposażonych narodów, które na pewno mają środki, aby to zrobić – byłyby bardzo motywowane, aby albo uzyskać, albo zniszczyć taką zdolność, czy to siłą, czy podstępem.[^77]

To mogłoby zacząć się na małą skalę, jako sabotaż przebiegów treningowych lub ataki na produkcję chipów, ale te ataki mogą naprawdę się zatrzymać dopiero, gdy wszystkie strony albo stracą zdolność do ścigania w AI, albo stracą zdolność do wykonywania ataków. Ponieważ uczestnicy postrzegają stawki jako egzystencjalne, każdy przypadek prawdopodobnie będzie reprezentować katastrofalną wojnę.

To prowadzi nas do czwartej możliwości: ściganie do superinteligencji, i w najszybszy, najmniej kontrolowany sposób możliwy. Gdy AI zwiększa się w potędze, jego deweloperzy po obu stronach będą znajdować to postępowo trudniejsze do kontrolowania, szczególnie dlatego, że ściganie dla zdolności jest przeciwne do rodzaju ostrożnej pracy, której kontrolowalność by wymagała. Więc ten scenariusz stawia nas bezpośrednio w przypadku, gdzie kontrola jest stracona (lub dana, jak zobaczymy dalej) do samych systemów AI. To jest, *AI wygrywa wyścig.* Ale z drugiej strony, w stopniu, w jakim kontrola *jest* utrzymana, nadal mamy wielu wzajemnie wrogich stron, każda odpowiedzialna za ekstremalne potężne zdolności. To wygląda znowu jak wojna.

Postawmy to wszystko inaczej.[^78] Obecny świat po prostu nie ma żadnych instytucji, które mogłyby być powierzone do rozwoju AI tej zdolności bez zapraszania natychmiastowego ataku.[^79] Wszystkie strony będą poprawnie rozumować, że albo to *nie* będzie pod kontrolą – i stąd jest zagrożeniem dla wszystkich stron, albo to *będzie* pod kontrolą, i stąd jest zagrożeniem dla każdego przeciwnika, który rozwija to mniej szybko. To są kraje uzbrojone nuklearnie, lub są firmami mieszczącymi się w nich.

Przy braku jakiegokolwiek prawdopodobnego sposobu dla ludzi na "wygranie" tego wyścigu, zostajemy z surowym wnioskiem: jedynym sposobem, w jaki ten wyścig się kończy, jest albo w katastrofalnym konflikcie, albo gdzie AI, a nie jakakolwiek ludzka grupa, jest zwycięzcą.

### Dajemy kontrolę AI (lub ją bierze)

Geopolityczna konkurencja "wielkich mocarstw" to tylko jedna z wielu konkurencji: jednostki konkurują ekonomicznie i społecznie; firmy konkurują na rynkach; partie polityczne konkurują o władzę; ruchy konkurują o wpływ. W każdej arenie, gdy AI zbliża się do i przekracza ludzkie zdolności, presja konkurencyjna zmusi uczestników do delegowania lub oddawania coraz więcej kontroli systemom AI – nie dlatego, że ci uczestnicy chcą, ale dlatego, że [nie mogą sobie pozwolić na to, żeby nie.](https://arxiv.org/abs/2303.16200)

Tak jak z innymi ryzykami AGI, widzimy to już z słabszymi systemami. Studenci czują presję do używania AI w swoich zadaniach, ponieważ wyraźnie wielu innych studentów to robi. Firmy [pędzą do przyjmowania rozwiązań AI z powodów konkurencyjnych.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) Artyści i programiści czują się zmuszeni używać AI, bo inaczej ich stawki będą podbijane przez innych, którzy to robią.

To czuje się jak przymusowa delegacja, ale nie utrata kontroli. Ale podkręćmy stawki i popchnijmy zegar do przodu. Zastanów się nad CEO, którego konkurenci używają AGI "pomocników" do podejmowania szybszych, lepszych decyzji, lub dowódcą wojskowym stojącym naprzeciw przeciwnika z dowodzeniem i kontrolą wspomaganymi przez AI. Wystarczająco zaawansowany system AI mógłby autonomicznie działać z wielokrotnie ludzką szybkością, wyrafinowaniem, złożonością i zdolnością przetwarzania danych, dążąc do złożonych celów w skomplikowany sposób. Nasz CEO lub dowódca, odpowiedzialny za taki system, może widzieć, że osiąga to, co chce; ale czy zrozumiałby nawet małą część *jak* to zostało osiągnięte? Nie, po prostu musiałby to zaakceptować. Co więcej, wiele z tego, co system może robić, to nie tylko brać rozkazy, ale radzić swojemu rzekomemu szefowi, co robić. Ta rada będzie dobra –– raz po raz.

W którym momencie więc rola człowieka zostanie ograniczona do klikania "tak, idź naprzód"?

To czuje się dobrze, mieć zdolne systemy AI, które mogą zwiększyć naszą produktywność, zadbać o irytujące rzeczy i nawet działać jako partner myślowy w załatwianiu rzeczy. Będzie się czuć dobrze mieć asystenta AI, który może zadbać o działania dla nas, jak dobry ludzki asystent osobisty. Będzie się czuć naturalnie, nawet korzystnie, gdy AI stanie się bardzo mądre, kompetentne i niezawodne, odsyłać coraz więcej decyzji do niego. Ale ta "korzystna" delegacja ma wyraźny punkt końcowy, jeśli kontynuujemy drogę: pewnego dnia odkryjemy, że naprawdę nie jesteśmy odpowiedzialni za wiele z niczego już, i że systemy AI faktycznie prowadzące show nie mogą bardziej zostać wyłączone niż firmy naftowe, media społecznościowe, internet lub kapitalizm.

I to jest znacznie bardziej pozytywna wersja, w której AI jest po prostu tak przydatne i skuteczne, że pozwalamy mu podejmować większość naszych kluczowych decyzji za nas. Rzeczywistość prawdopodobnie byłaby znacznie większym miks między tym a wersjami, gdzie niekontrolowane systemy AGI *biorą* różne formy władzy dla siebie, ponieważ, pamiętaj, władza jest przydatna dla niemal każdego celu, który się ma, a AGI byłoby, przez projekt, przynajmniej tak skuteczne w dążeniu do swoich celów jak ludzie.

Czy przyznajemy kontrolę, czy zostaje wyrwana od nas, jej utrata wydaje się ekstremalne prawdopodobna. Jak pierwotnie ujął to Alan Turing, "...wydaje się prawdopodobne, że gdy metoda myślenia maszyn by się zaczęła, nie zajęłoby długo przewyższenie naszych słabych mocy. Nie byłoby pytania o umieranie maszyn, i mogłyby rozmawiać ze sobą, aby ostrzyć swoje rozumy. W pewnym etapie więc musiałoby się oczekiwać, że maszyny przejmą kontrolę..."

Proszę zauważ, chociaż to oczywiste na tyle, że utrata kontroli przez ludzkość nad AI pociąga także utratę kontroli Stanów Zjednoczonych przez rząd Stanów Zjednoczonych; oznacza utratę kontroli nad Chinami przez Komunistyczną Partię Chin, i utratę kontroli nad Indiami, Francją, Brazylią, Rosją i każdym innym krajem przez ich własny rząd. Tak więc firmy AI, nawet jeśli nie jest to ich intencja, obecnie uczestniczą w potencjalnym obaleniu rządów światowych, w tym swojego własnego. To mogłoby się zdarzyć w ciągu kilku lat.

### AGI doprowadzi do superinteligencji

Można argumentować, że sztuczna inteligencja ogólna konkurencyjna z ludźmi lub nawet ekspercko-konkurencyjna, nawet jeśli autonomiczna, mogłaby być do opanowania. Może być niesamowicie destrukcyjna we wszystkich sposobach omówionych powyżej, ale jest dużo bardzo mądrych, sprawczych ludzi na świecie teraz, i są mniej więcej do opanowania.[^80]

Ale nie zostaniemy na mniej więcej ludzkim poziomie. Progresja poza to prawdopodobnie będzie napędzana przez te same siły, które już widzieliśmy: presja konkurencyjna między deweloperami AI szukającymi zysku i władzy, presja konkurencyjna między użytkownikami AI, którzy nie mogą sobie pozwolić na pozostawanie w tyle, i – najważniejsze – własna zdolność AGI do poprawiania siebie.

W procesie, który już widzieliśmy, jak się zaczyna z mniej potężnymi systemami, AGI samo byłoby w stanie konceptualizować i projektować ulepszone wersje siebie. To włącza hardware, software, sieci neuronowe, narzędzia, szkielety itp. To będzie, przez definicję, lepsze od nas w robieniu tego, więc nie wiemy dokładnie, jak będzie inteligencja-bootstrap. Ale nie będziemy musieć. O ile nadal mamy wpływ na to, co AGI robi, po prostu musielibyśmy poprosić to o to, albo pozwolić mu.

Nie ma ludzkiej bariery poziomu w poznaniu, która mogłaby nas chronić przed tym niekontrololowanym.[^81]

Progresja AGI do superinteligencji nie jest prawem natury; nadal byłoby możliwe ograniczenie niekontrolowanego, szczególnie jeśli AGI jest relatywnie scentralizowane i w stopniu, w jakim jest kontrolowane przez strony, które nie czują presji, żeby się ścigać ze sobą. Ale gdyby AGI było szeroko rozpowszechnione i wysoce autonomiczne, wydaje się niemal niemożliwe zapobiec mu decydowania, że powinno być bardziej, a potem jeszcze więcej, potężne.

### Co się dzieje, jeśli zbudujemy (lub AGI zbuduje) superinteligencję

Mówiąc wprost, nie mamy pojęcia, co by się stało, gdybyśmy zbudowali superinteligencję.[^82] Podjęłaby działania, których nie możemy śledzić lub postrzegać, z powodów, których nie możemy pojąć, do celów, których nie możemy sobie wyobrazić. To, co wiemy, to że nie będzie to zależeć od nas.[^83]

Niemożliwość kontrolowania superinteligencji może być zrozumiana przez coraz ostrzejsze analogie. Po pierwsze, wyobraź sobie, że jesteś CEO dużej firmy. Nie ma sposobu, żebyś mógł śledzić wszystko, co się dzieje, ale z odpowiednim ustawieniem personelu nadal możesz sensownie zrozumieć ogólny obraz i podejmować decyzje. Ale załóżmy jedną rzecz: wszyscy inni w firmie działają sto razy szybciej od ciebie. Czy nadal możesz nadążyć?

Z superinteligentnym AI ludzie "dowodziliby" czymś nie tylko szybszym, ale działającym na poziomach wyrafinowania i złożoności, których nie mogą zrozumieć, przetwarzając ogromnie więcej danych, niż mogą nawet sobie wyobrazić. Ta niewspółmierność może być postawiona na formalnym poziomie: [prawo Ashby'ego o wymaganej różnorodności](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (i zobacz związane ["twierdzenie o dobrym regulatorze"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)) stwierdza, mniej więcej, że każdy system kontrolny musi mieć tyle pokręteł i tarcz, ile system kontrolowany ma stopni swobody.

Osoba kontrolująca superinteligentny system AI byłaby jak paproć kontrolująca General Motors: nawet jeśli "rób, co chce paproć" byłoby zapisane w statutach korporacyjnych, systemy są tak różne w szybkości i zakresie działania, że "kontrola" po prostu nie ma zastosowania. (I jak długo do tego, aż ten uciążliwy statut zostanie przepisany?)[^84]

Jako że są zero przykładów roślin kontrolujących korporacje fortune 500, byłoby dokładnie zero przykładów ludzi kontrolujących superinteligencje. To zbliża się do matematycznego faktu.[^85] Gdyby superinteligencja została skonstruowana – niezależnie od tego, jak tam dotarliśmy – pytaniem nie byłoby, czy ludzie mogliby ją kontrolować, ale czy kontynuowalibyśmy istnienie, a jeśli tak, czy mielibyśmy dobre i znaczące istnienie jako jednostki lub jako gatunek. Nad tymi egzystencjalnymi pytaniami dla ludzkości mielibyśmy małe zakupy. Era ludzka byłaby skończona.

### Wniosek: nie wolno nam budować AGI

Istnieje scenariusz, w którym budowanie AGI może pójść dobrze dla ludzkości: jest budowane ostrożnie, pod kontrolą i dla korzyści ludzkości, rządzone przez wzajemne porozumienie wielu interesariuszy,[^86] i zapobiega się mu ewoluowaniu do niekontrolowalnej superinteligencji.

*Ten scenariusz nie jest dla nas otwarty w obecnych okolicznościach.* Jak omówiono w tej sekcji, z bardzo wysokim prawdopodobieństwem rozwój AGI prowadziłby do pewnej kombinacji:

- Masowe społeczne i cywilizacyjne zakłócenie lub zniszczenie;
- Konflikt lub wojna między wielkimi mocarstwami;
- Utrata kontroli przez ludzkość *nad* lub *na rzecz* potężnych systemów AI;
- Niekontrolowane do niekontrolowalnej superinteligencji i irrelewantność lub zaprzestanie gatunku ludzkiego.

Jak ujął to wczesny fikcyjny opis AGI: jedynym sposobem na wygranie jest nie granie.

[^55]: [Ustawa AI UE](https://artificialintelligenceact.eu/) jest znaczącym fragmentem legislacji, ale nie zapobiegłaby bezpośrednio temu, żeby niebezpieczny system AI został zbudowany lub wdrożony, czy nawet otwarcie wypuszczony, szczególnie w USA. Inny znaczący fragment polityki, rozporządzenie wykonawcze USA o AI, zostało odwołane.

[^56]: Ta [ankieta Gallup](https://news.gallup.com/poll/1597/confidence-institutions.aspx) pokazuje ponury spadek zaufania do instytucji publicznych od 2000 roku w USA. Liczby europejskie są zróżnicowane i mniej ekstremalne, ale także w trendzie spadkowym. Nieufność nie oznacza ściśle, że instytucje naprawdę *są* dysfunkcjonalne, ale jest wskazaniem jak również przyczyną.

[^57]: A główne zakłócenia, które teraz popieramy – takie jak rozszerzenie praw na nowe grupy – były specjalnie napędzane przez ludzi w kierunku poprawiania rzeczy.

[^58]: Pozwól mi być szczerym. Jeśli twoja praca może być wykonywana zza komputera, z relatywnie małą osobistą interakcją z ludźmi poza twoją organizacją i nie pociąga odpowiedzialności prawnej wobec zewnętrznych stron, byłoby z definicji możliwe (i prawdopodobnie oszczędne) całkowicie zamienić cię na system cyfrowy. Robotyka do zastąpienia większości pracy fizycznej przyjdzie później – ale nie tak wiele później, gdy AGI zacznie projektować roboty.

[^59]: Na przykład, co się dzieje z naszym systemem sądowym, jeśli pozwy są niemal bezpłatne do złożenia? Co się dzieje, gdy omijanie systemów bezpieczeństwa poprzez inżynierię społeczną staje się tanie, łatwe i bezryzykowne?

[^60]: [Ten artykuł](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/) twierdzi, że 10% całej zawartości internetowej jest już generowane przez AI i jest najlepszym hitem Google'a (dla mnie) na zapytanie wyszukiwania "oszacowania, jaki ułamek nowej zawartości internetowej jest generowany przez AI." Czy to prawda? Nie mam pojęcia! Nie cytuje żadnych odniesień i nie był napisany przez osobę. Jaki ułamek nowych obrazów indeksowanych przez Google, czy Tweets, czy komentarzy na Reddit, czy wideo Youtube są generowane przez ludzi? Nikt nie wie – nie myślę, że to liczba możliwa do poznania. I to mniej niż *dwa lata* po nadejściu generacyjnego AI.

[^61]: Warte dodania jest też, że istnieje ryzyko "moralne", że możemy stworzyć cyfrowe istoty, które mogą cierpieć. Jako że obecnie nie mamy niezawodnej teorii świadomości, która pozwoliłaby nam rozróżnić systemy fizyczne, które mogą i nie mogą cierpieć, nie możemy tego wykluczyć teoretycznie. Ponadto raporty systemów AI o ich czującej są prawdopodobnie niezawodne względem ich faktycznego doświadczenia (lub nie-doświadczenia) czującej.

[^62]: Rozwiązania techniczne w tej dziedzinie "wyrównania" AI prawdopodobnie nie będą na wysokości zadania również. W obecnych systemach działają na pewnym poziomie, ale są płytkie i generalnie mogą być omijane bez znaczącego wysiłku; i jak omówiono poniżej, nie mamy prawdziwego pojęcia, jak to robić dla znacznie bardziej zaawansowanych systemów.

[^63]: Takie systemy AI mogą przyjść z niektórymi wbudowanymi zabezpieczeniami. Ale dla każdego modelu z czymś jak obecna architektura, jeśli pełny dostęp do jego wag jest dostępny, środki bezpieczeństwa mogą być usunięte via dodatkowy trening lub inne techniki. Więc jest praktycznie gwarantowane, że dla każdego systemu z barierkami będzie też szeroko dostępny system bez nich. Rzeczywiście model Llama 3.1 405B Meta został otwarcie wypuszczony z zabezpieczeniami. Ale *nawet przed tym* model "bazowy", bez zabezpieczeń, został wycieknięty.

[^64]: Czy rynek mógłby zarządzać tymi ryzykami bez zaangażowania rządu? Krótko, nie. Są na pewno ryzyka, które firmy są silnie motywowane do łagodzenia. Ale wiele innych firm może i robi eksternalizuje na wszystkich innych, i wiele z powyższych jest w tej klasie: nie ma naturalnych motywacji rynkowych do zapobiegania masowej inwigilacji, rozpadu prawdy, koncentracji władzy, zakłócenia pracy, szkodzącej dyskursowi politycznemu itp. Rzeczywiście widzieliśmy wszystkie te od współczesnej technologii, szczególnie mediów społecznościowych, które poszły zasadniczo nieregulowane. AI po prostu bardzo by wzmocniło wiele z tych samych dynamik.

[^65]: OpenAI prawdopodobnie ma bardziej posłuszne modele do użytku wewnętrznego. Jest nieprawdopodobne, żeby OpenAI zbudowało jakiś rodzaj "tylnych drzwi", żeby ChatGPT mógł być lepiej kontrolowany przez samego OpenAI, ponieważ byłoby to okropną praktyką bezpieczeństwa i byłoby wysoce eksploatowalne biorąc pod uwagę nieprzejrzystość i nieprzewidywalność AI.

[^66]: Też o kluczowym znaczeniu: wyrównanie lub jakiekolwiek inne cechy bezpieczeństwa mają znaczenie tylko jeśli są faktycznie używane w systemie AI. Systemy, które są otwarcie wypuszczane (tzn. gdzie wagi modelu i architektura są publicznie dostępne) mogą być transformowane relatywnie łatwo w systemy *bez* tych środków bezpieczeństwa. Otwarcie-wypuszczanie mądrzejszych-niż-ludzkie systemy AGI byłoby zdumiewająco lekkomyślne, i trudno wyobrazić sobie, jak ludzka kontrola czy nawet istotność byłaby utrzymana w takim scenariuszu. Byłaby każda motywacja, na przykład, żeby wypuścić potężnych samo-reprodukujących i samo-podtrzymujących agentów AI z celem zarabiania pieniędzy i wysyłania ich do jakiegoś portfela kryptowalut. Czy wygrywanie wyborów. Czy obalanie rządu. Czy "dobre" AI może pomóc to zawierać? Być może – ale tylko przez delegowanie mu ogromnej władzy, prowadząc do utraty kontroli jak opisano poniżej.

[^67]: Dla ekspozycji na długość książki problemu zobacz np. *Superintelligencja*, *Problem Wyrównania*, i *Kompatybilne z Człowiekiem*. Dla ogromnej sterty pracy na różnych poziomach technicznych przez tych, którzy trudzili się przez lata myśląc o problemie, możesz odwiedzić [forum wyrównania AI](https://www.alignmentforum.org/). Oto [najnowsze ujęcie](https://alignment.anthropic.com/2025/recommended-directions/) od zespołu wyrównania Anthropic o tym, co uważają za nierozwiązane.

[^68]: To jest scenariusz ["awanturniczego AI"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/). W zasadzie ryzyko mogłoby być relatywnie mniejsze, jeśli system nadal mógłby być kontrolowany przez wyłączenie go; ale scenariusz mógłby również obejmować oszustwo AI, samo-wyciągnięcie i reprodukcję, agregację władzy i inne kroki, które sprawiłyby, że trudno lub niemożliwe byłoby to zrobić.

[^69]: Jest bardzo bogata literatura na ten temat, sięgająca do formujących pism przez [Steve Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), Nicka Bostroma i Eliezer Yudkowskiego. Dla ekspozycji na długość książki zobacz [Kompatybilne z Człowiekiem](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) autorstwa Stuarta Russella; [tutaj](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/) jest krótka i aktualnie primer.

[^70]: Rozpoznając to, zamiast zwalniać, żeby uzyskać lepsze zrozumienie, firmy AGI wymyśliły inny plan: będą mieć AI to robiące! Bardziej szczegółowo, będą mieć AI *N* pomocne im w figuring jak wyrównać AI *N+1*, całą drogę do superinteligencji. Chociaż leveraging AI do pomocy nam wyrównać AI brzmi obiecująco, jest mocny argument, że po prostu zakłada swój wniosek jako premise i jest ogólnie niesamowicie ryzykownym podejściem. Zobacz [tutaj](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us) dla jakiejś dyskusji. Ten "plan" nie jest jednym i przeszedł nic jak scrutiny odpowiednie do podstawowej strategii jak sprawić, żeby super-ludzkie AI poszło dobrze dla ludzkości.

[^71]: W końcu ludzie, wadliwi i upórni, jak jesteśmy, rozwinęli systemy etyczne, przez które traktujemy przynajmniej niektóre inne gatunki na Ziemi dobrze. (Po prostu nie myśl o tych fermach fabrycznych.)

[^72]: Jest, na szczęście, ucieczka tutaj: jeśli uczestnicy dojdą do zrozumienia, że są zaangażowani w wyścig samobójczy a nie wygrywny. To się stało pod koniec zimnej wojny, gdy USA i ZSRR doszły do zrozumienia, że z powodu zimy nuklearnej, nawet *nieodpowiedziany* atak nuklearny byłby katastrofalny dla atakującego. Z realizacją, że "wojna nuklearna nie może być wygrana i nie wolno nigdy walczyć" przyszły znaczące porozumienia o redukcji broni – zasadniczo koniec wyścigu zbrojeń.

[^73]: Wojna, wyraźnie czy implikacyjnie.

[^74]: Eskalacja, potem wojna.

[^75]: Magiczne myślenie.

[^76]: Mam też bilionów dolarów wartości most do sprzedania ci.

[^77]: Tacy agenci prawdopodobnie woleliby "uzyskanie", ze zniszczeniem jako fallback; ale zabezpieczanie modeli przeciwko zarówno zniszczeniu *i* kradzieży przez potężne narody jest trudne, żeby powiedzieć najmniej, szczególnie dla prywatnych podmiotów.

[^78]: Dla innej perspektywy na ryzyko bezpieczeństwa narodowego AGI, zobacz [ten raport RAND.](https://www.rand.org/pubs/perspectives/PEA3691-4.html)

[^79]: Być może moglibyśmy zbudować taką instytucję! Były propozycje dla "CERN dla AI" i inne podobne inicjatywy, gdzie rozwój AGI jest pod multilateral global control. Ale w tej chwili żadna taka instytucja nie istnieje lub nie jest na horyzoncie.

[^80]: I podczas gdy wyrównanie jest bardzo trudne, getting ludzi do zachowywania się jest jeszcze trudniejsze!

[^81]: Wyobraź sobie system, który może mówić 50 językami, mieć ekspertyzę we wszystkich przedmiotach akademickich, przeczytać pełną książkę w sekundach i mieć cały materiał natychmiast na myśli i produkować wyjścia z dziesięciokrotnie ludzką szybkością. Faktycznie, nie musisz tego wyobrażać sobie: po prostu załaduj obecny system AI. Te są super-ludzkie na wiele sposobów, i nie ma nic zatrzymującego je od bycia jeszcze bardziej super-ludzkimi w tych i wielu innych.

[^82]: To jest dlaczego to było nazwane technologiczną "osobliwością", pożyczając z fizyki ideę, że nie można robić przewidywań past osobliwość. Proponenci opierania się *w* taką osobliwość mogą także chcieć reflektować, że w fizyce te same rodzaje osobliwości rozrywają na kawałki i miażdżą te, które w nie wchodzą.

[^83]: Problem był comprehensively outlined w Bostroma [*Superinteligencji*](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834), i nic od tamtego czasu nie zmieniło znacząco podstawowej wiadomości. Dla bardziej niedawnego volume collecting formal i matematycznych wyników na niekontrolowalność zobacz Yampolskiy's [AI: Niewyjaśnione, Nieprzewidywalne, Niekontrolowalne](https://www.amazon.com/Unexplainable-Unpredictable-Uncontrollable-Artificial-Intelligence/dp/103257626X)

[^84]: To też czyni jasnym, dlaczego obecna strategia firm AI (iteracyjnie pozwalając AI "wyrównywać" następne najpotężniejsze AI) nie może działać. Załóż paproć, via przyjemność jego fronds, enlists pierwszego ucznia szkoły do zadbania o to. Pierwszy uczeń pisze niektóre szczegółowe instrukcje dla drugiego ucznia do następowania i notę przekonującą ich do robienia tego. Drugi uczeń robi to samo dla trzeciego ucznia i tak dalej całą drogę do college grad, manager, executive i wreszcie GM CEO. Czy GM potem "zrobi, co chce paproć"? W każdym kroku to może czuć się jak działające. Ale putting wszystko razem, będzie działać niemal dokładnie do stopnia, do którego CEO, Rada i shareholders GM przypadają care o dzieciach i paprocach i mieć mało do nothing do robienia ze wszystkimi tymi notami i zestawami instrukcji.

[^85]: Charakter nie jest tak różny od formalnych wyników jak twierdzenie niekompletności Gödela czy argument halting Turinga w tym, że notion kontroli fundamentalnie przeciwstawia się premise: jak możesz sensownie kontrolować coś, czego nie możesz zrozumieć czy przewidzieć; jeszcze jeśli mógłbyś zrozumieć i przewidzieć superinteligencję, byłbyś superinteligentny. Powód, dlaczego mówię "approaches", jest że formalne wyniki nie są tak thorough czy vetted jak w przypadku pure matematyki i ponieważ chciałbym hold out hope, że pewna bardzo ostrożnie skonstruowana ogólna inteligencja, używając całkowicie różnych metod niż obecnie stosowanych, mogłaby mieć niektóre matematycznie udowodnione właściwości bezpieczeństwa, per rodzaj "guaranteed safe" programu AI omówionego poniżej.

[^86]: W tej chwili większość stakeholders – to jest niemal cała ludzkość – jest sidelined w tej dyskusji. To jest głęboko złe i jeśli nie zaproszeni, wiele, wiele innych grup będzie dotkniętych przez rozwój AGI powinno demand być let in.

## Rozdział 8 - Jak nie budować AGI

AGI nie jest nieuniknione – dziś stoimy na rozdrożu. Ten rozdział przedstawia propozycję, jak moglibyśmy zapobiec jego powstaniu.

Jeśli droga, którą obecnie podążamy, prowadzi do prawdopodobnego końca naszej cywilizacji, jak zmienić kierunek?

Załóżmy, że pragnienie zatrzymania rozwoju AGI i superinteligencji stałoby się powszechne i silne,[^87] ponieważ rozpowszechniłoby się zrozumienie, że AGI pochłaniałoby władzę zamiast jej udzielać, i stanowiłoby ogromne zagrożenie dla społeczeństwa i ludzkości. Jak zamknęlibyśmy Bramy?

Obecnie znamy tylko jeden sposób *tworzenia* potężnej i ogólnej sztucznej inteligencji – poprzez naprawdę masowe obliczenia głębokich sieci neuronowych. Ponieważ są to niesamowicie trudne i kosztowne przedsięwzięcia, w pewnym sensie ich *niepodejmowanie* jest łatwe.[^88] Ale widzieliśmy już siły napędzające rozwój w kierunku AGI oraz dynamikę teorii gier, która bardzo utrudnia jednostronne zatrzymanie się którejkolwiek ze stron. Potrzebna byłaby więc kombinacja interwencji z zewnątrz (tj. rządów) w celu powstrzymania korporacji oraz porozumień między rządami w celu powstrzymania siebie nawzajem.[^89] Jak mogłoby to wyglądać?

Przydatne jest najpierw rozróżnienie między rozwojem AI, który musi być *zapobiegany* lub *zakazany*, a tym, który musi być *zarządzany*. Pierwszy dotyczyłby głównie niekontrolowanego rozwoju w kierunku superinteligencji.[^90] Dla zakazanego rozwoju definicje powinny być możliwie precyzyjne, a weryfikacja i egzekwowanie – praktyczne. To, co musi być *zarządzane*, dotyczyłoby ogólnych, potężnych systemów AI – które już mamy i które będą mieć wiele szarych obszarów, niuansów i złożoności. W tym przypadku kluczowe są silne, skuteczne instytucje.

Możemy także użytecznie rozgraniczyć kwestie, które muszą być adresowane na poziomie międzynarodowym (włącznie z rywalami lub przeciwnikami geopolitycznymi)[^91] od tych, którymi mogą zarządzać poszczególne jurysdykcje, kraje lub grupy krajów. Zakazany rozwój w dużej mierze należy do kategorii "międzynarodowej", ponieważ lokalny zakaz rozwoju technologii można generalnie obejść, zmieniając lokalizację.[^92]

Wreszcie, możemy rozważyć narzędzia w zestawie narzędzi. Jest ich wiele, włączając narzędzia techniczne, prawo miękkie (standardy, normy itp.), prawo twarde (regulacje i wymogi), odpowiedzialność prawną, zachęty rynkowe i tak dalej. Skupmy szczególną uwagę na jednym, które jest specyficzne dla AI.

### Bezpieczeństwo i zarządzanie mocą obliczeniową

Kluczowym narzędziem w zarządzaniu wysokowydajną AI będzie sprzęt, którego wymaga. Oprogramowanie rozprzestrzenia się łatwo, ma niemal zerowy krańcowy koszt produkcji, przekracza granice trywialne i może być natychmiast modyfikowane; żadne z tych stwierdzeń nie jest prawdziwe dla sprzętu. Jednak jak omawialiśmy, ogromne ilości tej "mocy obliczeniowej" są niezbędne zarówno podczas treningu systemów AI, jak i podczas inferencji, aby osiągnąć najbardziej zdolne systemy. Moc obliczeniową można łatwo kwantyfikować, rozliczać i audytować, przy względnie niewielkiej dwuznaczności, gdy zostaną opracowane dobre reguły tego dokonywania. Co najważniejsze, duże ilości obliczeń są, podobnie jak wzbogacony uran, bardzo rzadkim, drogim i trudnym do wytworzenia zasobem. Chociaż chipy komputerowe są wszechobecne, sprzęt wymagany do AI jest drogi i niezwykle trudny w produkcji.[^93]

To, co czyni chipy wyspecjalizowane pod AI o wiele *bardziej* zarządzalnym rzadkim zasobem niż uran, to fakt, że mogą one zawierać sprzętowe mechanizmy bezpieczeństwa. Większość nowoczesnych telefonów komórkowych i niektóre laptopy mają wyspecjalizowane funkcje sprzętowe na chipie, które pozwalają im zapewnić, że instalują tylko zatwierdzone oprogramowanie systemowe i aktualizacje, że przechowują i chronią wrażliwe dane biometryczne na urządzeniu oraz że można je uczynić bezużytecznymi dla kogokolwiek oprócz ich właściciela w przypadku zgubienia lub kradzieży. W ciągu ostatnich kilku lat takie sprzętowe środki bezpieczeństwa stały się dobrze ugruntowane i szeroko przyjęte, i generalnie okazały się całkiem bezpieczne.

Kluczową nowością tych funkcji jest to, że łączą sprzęt i oprogramowanie za pomocą kryptografii.[^94] To znaczy, samo posiadanie konkretnego elementu sprzętu komputerowego nie oznacza, że użytkownik może robić z nim wszystko, co chce, stosując różne oprogramowanie. A to powiązanie zapewnia także potężne bezpieczeństwo, ponieważ wiele ataków wymagałoby naruszenia bezpieczeństwa *sprzętu*, a nie tylko *oprogramowania*.

Kilka niedawnych raportów (np. od [GovAI i współpracowników](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips) i [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) wskazało, że podobne funkcje sprzętowe wbudowane w najnowocześniejszy sprzęt obliczeniowy związany z AI mogłyby odgrywać niezwykle użyteczną rolę w bezpieczeństwie i zarządzaniu AI. Umożliwiają one szereg funkcji dostępnych "zarządcy",[^95] których istnienia można by nie podejrzewać lub nawet nie uważać za możliwe. Jako kluczowe przykłady:

- *Geolokalizacja*: Systemy można skonfigurować tak, aby chipy miały znaną lokalizację i mogły działać inaczej (lub być całkowicie wyłączone) w zależności od lokalizacji.[^96]
- *Połączenia z listy dozwolonych*: każdy chip może być skonfigurowany ze sprzętowo egzekwowaną listą dozwolonych konkretnych innych chipów, z którymi może się łączyć sieciowo, i być niezdolny do łączenia się z chipami spoza tej listy.[^97] To może ograniczyć wielkość komunikujących się klastrów chipów.[^98]
- *Odmierzona inferencja lub trening (i automatyczne wyłączanie)*: Zarządca może licencjonować tylko określoną ilość treningu lub inferencji (w czasie, lub FLOP, lub ewentualnie tokenach) do wykonania przez użytkownika, po czym wymagane jest nowe pozwolenie. Jeśli przyrosty są małe, wymagane jest stosunkowo ciągłe ponowne licencjonowanie modelu. Model można wtedy "wyłączyć" po prostu wstrzymując ten sygnał licencji.[^99]
- *Ograniczenie prędkości*: Model ma zapobiec działaniu z większą prędkością inferencji niż jakiś limit określony przez zarządcę lub w inny sposób. Można to zaimplementować przez ograniczony zestaw połączeń z listy dozwolonych lub przez bardziej wyrafinowane środki.
- *Poświadczony trening*: Procedura treningu może dostarczyć kryptograficznie bezpieczny dowód, że konkretny zestaw kodów, danych i ilość użycia mocy obliczeniowej zostały zastosowane w generowaniu modelu.

### Jak nie budować superinteligencji: globalne limity na moc obliczeniową treningu i inferencji

Mając te rozważania – szczególnie dotyczące obliczeń – na miejscu, możemy omówić, jak zamknąć Bramy przed sztuczną superinteligencją; następnie zwrócimy się do zapobiegania pełnemu AGI i zarządzaniu modelami AI, gdy zbliżają się i przekraczają ludzkie zdolności w różnych aspektach.

Pierwszym składnikiem jest, oczywiście, zrozumienie, że superinteligencja nie byłaby kontrolowalna i że jej konsekwencje są fundamentalnie nieprzewidywalne. Przynajmniej Chiny i USA muszą niezależnie zdecydować, z tego lub innych powodów, nie budować superinteligencji.[^100] Następnie potrzebne jest międzynarodowe porozumienie między nimi a innymi, z silnym mechanizmem weryfikacji i egzekwowania, aby zapewnić wszystkie strony, że ich rywale nie odstępują i nie decydują się rzucić kostką.

Aby być weryfikowalne i wykonalne, limity powinny być twardymi limitami i możliwie jednoznaczne. Wydaje się to praktycznie niemożliwym problemem: ograniczenie możliwości złożonego oprogramowania z nieprzewidywalnymi właściwościami na całym świecie. Na szczęście sytuacja jest znacznie lepsza od tej, ponieważ to właśnie, co uczyniło zaawansowaną AI możliwą – ogromna ilość mocy obliczeniowej – jest znacznie, znacznie łatwiejsze do kontrolowania. Chociaż mogłoby to wciąż pozwolić na niektóre potężne i niebezpieczne systemy, *niekontrolowany rozwój superinteligencji* prawdopodobnie można zapobiec przez twardy limit na ilość obliczeń, które wchodzą do sieci neuronowej, wraz z ograniczeniem szybkości ilości inferencji, którą system AI (połączonych sieci neuronowych i innego oprogramowania) może wykonywać. Konkretna wersja tego jest proponowana poniżej.

Może się wydawać, że umieszczenie twardych globalnych limitów na obliczenia AI wymagałoby ogromnych poziomów koordynacji międzynarodowej i inwazyjnej, naruszającej prywatność inwigilacji. Na szczęście tak nie byłoby. Niezwykle [ciasny i wąski łańcuch dostaw](https://arxiv.org/abs/2402.08797) sprawia, że gdy limit zostanie ustalony prawnie (czy to przez prawo, czy zarządzenie wykonawcze), weryfikacja zgodności z tym limitem wymagałaby zaangażowania i współpracy tylko garstki dużych firm.[^101]

Plan jak ten ma szereg wysoce pożądanych cech. Jest minimalnie inwazyjny w tym sensie, że tylko kilku głównym firmom nakłada się wymogi, i tylko dość znaczące klastry obliczeń byłyby zarządzane. Odpowiednie chipy już zawierają możliwości sprzętowe potrzebne do pierwszej wersji.[^102] Zarówno implementacja, jak i egzekwowanie opierają się na standardowych ograniczeniach prawnych. Ale są one wspierane przez warunki użytkowania sprzętu i przez kontrole sprzętowe, znacznie upraszczając egzekwowanie i zapobiegając oszustwom przez firmy, grupy prywatne, a nawet kraje. Istnieje obfity precedens dla firm sprzętowych nakładających zdalne ograniczenia na użytkowanie ich sprzętu i blokowania/odblokowywania konkretnych możliwości zewnętrznie,[^103] włączając nawet w wysokowydajne CPU w centrach danych.[^104] Nawet dla dość małego ułamka sprzętu i organizacji dotkniętych tym, nadzór mógłby być ograniczony do telemetrii, bez bezpośredniego dostępu do danych czy samych modeli; a oprogramowanie do tego mogłoby być otwarte do inspekcji, aby wykazać, że nie są rejestrowane żadne dodatkowe dane. Schemat jest międzynarodowy i kooperatywny, oraz dość elastyczny i rozszerzalny. Ponieważ limit dotyczy głównie sprzętu, a nie oprogramowania, jest względnie agnostyczny co do tego, jak rozwój i wdrażanie oprogramowania AI przebiega, i jest kompatybilny z różnorodnością paradygmatów, włączając bardziej "zdecentralizowaną" lub "publiczną" AI mającą na celu walkę z koncentracją władzy napędzaną przez AI.

Zamknięcie Bram oparte na obliczeniach ma także wady. Po pierwsze, jest daleko od pełnego rozwiązania problemu zarządzania AI w ogóle. Po drugie, gdy sprzęt komputerowy staje się szybszy, system "złapałby" coraz więcej sprzętu w coraz mniejszych klastrach (a nawet pojedynczych GPU).[^105] Możliwe jest także, że z powodu ulepszeń algorytmicznych potrzebny byłby jeszcze niższy limit obliczeniowy,[^106] lub że ilość obliczeń staje się w dużej mierze nieistotna, a zamknięcie Bramy wymagałoby zamiast tego bardziej szczegółowego reżimu zarządzania opartego na ryzyku lub możliwościach dla AI. Po trzecie, bez względu na gwarancje i małą liczbę dotkniętych podmiotów, taki system z pewnością wywoła sprzeciw dotyczący prywatności i inwigilacji, między innymi.[^107]

Oczywiście rozwój i implementacja schematu zarządzania ograniczającego obliczenia w krótkim okresie czasu będzie dość wyzwaniem. Ale absolutnie jest to wykonalne.

### A-G-I: Potrójny przecięcie jako podstawa ryzyka i polityki

Zwróćmy się teraz do AGI. Twarde linie i definicje są tutaj trudniejsze, ponieważ z pewnością mamy inteligencję, która jest sztuczna i ogólna, i według żadnej istniejącej definicji nie wszyscy zgodzą się, czy i kiedy istnieje. Ponadto limit obliczeniowy lub inferencyjny jest nieco tępym narzędziem (moc obliczeniowa jest zastępczym wskaźnikiem dla zdolności, która następnie jest zastępczym wskaźnikiem dla ryzyka), które – chyba że jest dość niskie – prawdopodobnie nie zapobiegnie AGI wystarczająco potężnemu, aby spowodować społeczne lub cywilizacyjne zakłócenia lub ostre ryzyko.

Argumentowałem, że najbardziej ostre ryzyko wyłania się z potrójnego przecięcia bardzo wysokich możliwości, wysokiej autonomii i wielkiej ogólności. To są systemy, które – jeśli w ogóle zostaną opracowane – muszą być zarządzane z ogromną ostrożnością. Tworząc rygorystyczne standardy (poprzez odpowiedzialność prawną i regulacje) dla systemów łączących wszystkie te trzy właściwości, możemy skierować rozwój AI w kierunku bezpieczniejszych alternatyw.

Jak w przypadku innych branż i produktów, które mogą potencjalnie zaszkodzić konsumentom lub społeczeństwu, systemy AI wymagają starannej regulacji przez skuteczne i uprawomocnione agencje rządowe. Ta regulacja powinna uznawać nieodłączne ryzyko AGI i zapobiegać rozwojowi nieakceptowalnie ryzykownych systemów AI o wysokiej mocy.[^108]

Jednak regulacja na dużą skalę, szczególnie z prawdziwymi zębami, które na pewno spotkają się ze sprzeciwem przemysłu,[^109] wymaga czasu[^110] oraz politycznego przekonania, że jest niezbędna.[^111] Biorąc pod uwagę tempo postępu, może to zająć więcej czasu, niż mamy do dyspozycji.

W znacznie szybszej skali czasowej i podczas opracowywania środków regulacyjnych, możemy dać firmom niezbędne zachęty do (a) powstrzymania się od bardzo wysokoryzykownych działań i (b) opracowania kompleksowych systemów oceny i łagodzenia ryzyka, poprzez wyjaśnienie i zwiększenie poziomów odpowiedzialności prawnej dla najbardziej niebezpiecznych systemów. Idea polegałaby na nałożeniu najwyższych poziomów odpowiedzialności – bezwzględnej, a w niektórych przypadkach osobistej karnej – na systemy w potrójnym przecięciu wysokiej autonomii-ogólności-inteligencji, ale zapewnieniu "bezpiecznych przystani" do bardziej typowej odpowiedzialności opartej na winie dla systemów, w których jedna z tych właściwości jest nieobecna lub zagwarantowana jako zarządzalna. To znaczy, na przykład, "słaby" system, który jest ogólny i autonomiczny (jak zdolny i godny zaufania, ale ograniczony asystent osobisty) podlegałby niższym poziomom odpowiedzialności. Podobnie wąski i autonomiczny system jak samojezdny samochód wciąż podlegałby znacznej regulacji, której już podlega, ale nie wzmocnionej odpowiedzialności. Podobnie dla wysoce zdolnego i ogólnego systemu, który jest "pasywny" i w dużej mierze niezdolny do niezależnego działania. Systemy pozbawione *dwóch* z trzech właściwości są jeszcze bardziej zarządzalne, a bezpieczne przystanie byłyby jeszcze łatwiejsze do uzyskania. To podejście odzwierciedla to, jak radzimi sobie z innymi potencjalnie niebezpiecznymi technologiami:[^112] wyższa odpowiedzialność dla bardziej niebezpiecznych konfiguracji tworzy naturalne zachęty dla bezpieczniejszych alternatyw.

Domyślnym wynikiem takich wysokich poziomów odpowiedzialności, które działają w celu *internalizacji* ryzyka AGI do firm zamiast przeniesienia go na społeczeństwo, jest prawdopodobnie (i miejmy nadzieję!) po prostu nierozwijanie pełnego AGI przez firmy, dopóki i chyba że mogą naprawdę uczynić go godnym zaufania, bezpiecznym i kontrolowalnym, biorąc pod uwagę, że to *ich własne kierownictwo* jest stroną narażoną na ryzyko. (W przypadku, gdy to nie wystarczy, ustawodawstwo wyjaśniające odpowiedzialność powinno także wyraźnie pozwalać na środki zaradcze w postaci zakazu, tj. sędzia nakazujący zatrzymanie działań, które wyraźnie znajdują się w strefie niebezpieczeństwa i prawdopodobnie stanowią ryzyko publiczne.) Gdy regulacje wchodzą w życie, przestrzeganie regulacji może stać się bezpieczną przystanią, a bezpieczne przystanie z niskiej autonomii, wąskości lub słabości systemów AI mogą przekształcić się w stosunkowo lżejsze reżimy regulacyjne.

### Kluczowe postanowienia zamknięcia Bram

Mając powyższą dyskusję na uwadze, ta sekcja przedstawia propozycje kluczowych postanowień, które implementowałyby i utrzymywałyby zakaz pełnego AGI i superinteligencji oraz zarządzanie konkurencyjną z ludźmi lub przewyższającą ekspertów sztuczną inteligencją ogólnego przeznaczenia blisko progu pełnego AGI.[^113] Ma cztery kluczowe elementy: 1) rozliczanie i nadzór mocy obliczeniowej, 2) limity mocy obliczeniowej w treningu i działaniu AI, 3) ramy odpowiedzialności prawnej oraz 4) wielopoziomowe standardy bezpieczeństwa i ochrony definiujące twarde wymogi regulacyjne. Są one zwięźle opisane dalej, z dodatkowymi szczegółami lub przykładami implementacji podanymi w trzech towarzyszących tabelach. Co ważne, zauważ, że są to daleko nie wszystko, co będzie niezbędne do zarządzania zaawansowanymi systemami AI; choć będą miały dodatkowe korzyści bezpieczeństwa i ochrony, są one nastawione na zamknięcie Bramy przed niekontrolowanym rozwojem inteligencji i przekierowanie rozwoju AI w lepszym kierunku.

#### 1\. Rozliczanie mocy obliczeniowej i przejrzystość

- Organizacja standardów (np. NIST w USA, po której następują ISO/IEEE międzynarodowo) powinna skodyfikować szczegółowy standard techniczny dla całkowitej mocy obliczeniowej używanej w treningu i działaniu modeli AI, w FLOP, oraz prędkości w FLOP/s, z jaką działają. Szczegóły tego, jak to mogłoby wyglądać, podane są w Załączniku A.[^114]
- Wymóg – czy to przez nowe ustawodawstwo, czy na podstawie istniejących uprawnień[^115] – powinien być nałożony przez jurysdykcje, w których ma miejsce trening AI na dużą skalę, aby obliczać i raportować do organu regulacyjnego lub innej agencji całkowite FLOP używane w treningu i działaniu wszystkich modeli powyżej progu 10<sup>25</sup> FLOP lub 10<sup>18</sup> FLOP/s.[^116]
- Te wymogi powinny być wprowadzane etapowo, początkowo wymagając dobrze udokumentowanych szacunków w dobrej wierze na podstawie kwartalnej, z późniejszymi fazami wymagającymi progresywnie wyższych standardów, aż do kryptograficznie poświadczonych całkowitych FLOP i FLOP/s dołączonych do każdego *wyniku* modelu.
- Te raporty powinny być uzupełnione przez dobrze udokumentowane szacunki krańcowych kosztów energetycznych i finansowych używanych w generowaniu każdego wyniku AI.

Uzasadnienie: Te dobrze obliczone i transparentnie raportowane liczby zapewniłyby podstawę dla limitów treningu i działania, a także bezpieczną przystań od wyższych środków odpowiedzialności (zobacz Załączniki C i D).

#### 2\. Limity mocy obliczeniowej treningu i działania

- Jurysdykcje hostujące systemy AI powinny nałożyć twardy limit na całkowitą moc obliczeniową wchodzącą do każdego wyniku modelu AI, zaczynając od 10<sup>27</sup> FLOP[^117] i regulowalną według potrzeb.
- Jurysdykcje hostujące systemy AI powinny nałożyć twardy limit na szybkość mocy obliczeniowej wyników modelu AI, zaczynając od 10<sup>20</sup> FLOP/s i regulowalną według potrzeb.

Uzasadnienie: Całkowite obliczenia, choć bardzo niedoskonałe, są zastępczym wskaźnikiem zdolności AI (i ryzyka), który jest konkretnie mierzalny i weryfikowalny, więc zapewnia twardą barierę dla ograniczenia możliwości. Konkretna propozycja implementacji jest podana w Załączniku B.

#### 3\. Wzmocniona odpowiedzialność za niebezpieczne systemy

- Tworzenie i działanie[^118] zaawansowanego systemu AI, który jest wysoce ogólny, zdolny i autonomiczny, powinno być prawnie wyjaśnione poprzez ustawodawstwo jako podlegające bezwzględnej, solidarnej, a nie jednostronnej odpowiedzialności opartej na winie.[^119]
- Proces prawny powinien być dostępny do tworzenia pozytywnych argumentów bezpieczeństwa, które udzieliłyby bezpiecznej przystani od bezwzględnej odpowiedzialności dla systemów, które są małe (pod względem mocy obliczeniowej), słabe, wąskie, pasywne lub które mają wystarczające gwarancje bezpieczeństwa, ochrony i kontrolowalności.
- Wyraźna ścieżka i zestaw warunków dla środków zaradczych w postaci zakazu zatrzymania działań treningu i inferencji AI, które stanowią publiczne niebezpieczeństwo, powinny być nakreślone.

Uzasadnienie: Systemy AI nie mogą być pociągnięte do odpowiedzialności, więc musimy pociągnąć do odpowiedzialności ludzkie jednostki i organizacje za szkody, które powodują (odpowiedzialność).[^120] Niekontrolowane AGI stanowi zagrożenie dla społeczeństwa i cywilizacji i przy braku argumentu bezpieczeństwa powinno być uważane za nienormalnie niebezpieczne. Nałożenie ciężaru odpowiedzialności na deweloperów, aby pokazać, że potężne modele są wystarczająco bezpieczne, aby nie być uważanymi za "nienormalnie niebezpieczne", zachęca do bezpiecznego rozwoju, wraz z przejrzystością i prowadzeniem dokumentacji w celu uzyskania tych bezpiecznych przystani. Regulacja może wtedy zapobiec szkodzie tam, gdzie odstraszanie od odpowiedzialności jest niewystarczające. Wreszcie, deweloperzy AI są już odpowiedzialni za szkody, które powodują, więc prawne wyjaśnienie odpowiedzialności dla najbardziej ryzykownych systemów można zrobić natychmiast, bez opracowywania bardzo szczegółowych standardów; te mogą się następnie rozwijać z czasem. Szczegóły podane są w Załączniku C.

#### 4\. Regulacja bezpieczeństwa dla AI

System regulacyjny, który adresuje ostre ryzyko AI na dużą skalę, będzie wymagał minimum:

- Identyfikacji lub stworzenia odpowiedniego zestawu organów regulacyjnych, prawdopodobnie nowej agencji;
- Kompleksowych ram oceny ryzyka;[^121]
- Ram dla pozytywnych argumentów bezpieczeństwa, opartych częściowo na ramach oceny ryzyka, które mają być tworzone przez deweloperów i audytowane przez *niezależne* grupy i agencje;
- Wielopoziomowego systemu licencyjnego, z poziomami śledzącymi poziomy zdolności.[^122] Licencje byłyby udzielane na podstawie argumentów bezpieczeństwa i audytów, dla rozwoju i wdrażania systemów. Wymogi wahałyby się od powiadomienia na dolnym końcu do kwantytatywnych gwarancji bezpieczeństwa, ochrony i kontrolowalności przed rozwojem na górnym końcu. Zapobiegałyby one wydaniu systemów, dopóki nie zostanie wykazane, że są bezpieczne, i zakazywałyby rozwoju wewnętrznie niebezpiecznych systemów. Załącznik D przedstawia propozycję tego, co takie standardy bezpieczeństwa i ochrony mogłyby zawierać.
- Porozumień w celu przeniesienia takich środków na poziom międzynarodowy, włączając międzynarodowe organy harmonizujące normy i standardy, a potencjalnie międzynarodowe agencje przeglądające argumenty bezpieczeństwa.

Uzasadnienie: Ostatecznie odpowiedzialność prawna nie jest właściwym mechanizmem zapobiegania ryzyku na dużą skalę dla społeczeństwa ze strony nowej technologii. Kompleksowa regulacja, z uprawomocnionymi organami regulacyjnymi, będzie potrzebna dla AI tak jak dla każdej innej głównej branży stanowiącej ryzyko dla społeczeństwa.[^123]

Regulacja w kierunku zapobiegania innym wszechobecnym, ale mniej ostrym ryzykom prawdopodobnie będzie różnić się formą od jurysdykcji do jurysdykcji. Kluczową rzeczą jest unikanie rozwoju systemów AI, które są tak ryzykowne, że te ryzyko są niezarządzalne.

### Co wtedy?

W ciągu następnej dekady, gdy AI stanie się bardziej wszechobecna, a podstawowa technologia się rozwinie, prawdopodobnie wydarzą się dwie kluczowe rzeczy. Po pierwsze, regulacja istniejących potężnych systemów AI stanie się trudniejsza, ale jeszcze bardziej niezbędna. Prawdopodobne jest, że przynajmniej niektóre środki adresujące ryzyko bezpieczeństwa na dużą skalę będą wymagały porozumienia na poziomie międzynarodowym, z poszczególnymi jurysdykcjami egzekwującymi zasady oparte na międzynarodowych porozumieniach.

Po drugie, limity mocy obliczeniowej treningu i działania staną się trudniejsze do utrzymania, gdy sprzęt stanie się tańszy i bardziej efektywny kosztowo; mogą także stać się mniej istotne (lub wymagać jeszcze większego zaostrzenia) wraz z postępami w algorytmach i architekturach.

To, że kontrolowanie AI stanie się trudniejsze, nie znaczy, że powinniśmy się poddać! Implementacja planu nakreślonego w tym eseju dałaby nam zarówno cenny czas, jak i kluczową kontrolę nad procesem, które postawiłyby nas w daleko, daleko lepszej pozycji do uniknięcia egzystencjalnego ryzyka AI dla naszego społeczeństwa, cywilizacji i gatunku.

W jeszcze dłuższej perspektywie będą wybory do podjęcia co do tego, na co pozwolimy. Możemy zdecydować się wciąż na stworzenie jakiejś formy naprawdę kontrolowalnego AGI, w stopniu, w jakim okaże się to możliwe. Lub możemy zdecydować, że prowadzenie świata lepiej pozostawić maszynom, jeśli możemy przekonać siebie, że zrobią to lepiej i będą nas dobrze traktować. Ale powinny to być decyzje podjęte z głębokim naukowym zrozumieniem AI w ręku i po znaczącej globalnej, włączającej dyskusji, a nie w wyścigu między potentatami technologicznymi z większością ludzkości całkowicie niezaangażowaną i nieświadomą.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) Podsumowanie zarządzania A-G-I i superinteligencji poprzez odpowiedzialność prawną i regulacje. Odpowiedzialność jest najwyższa, a regulacja najsilniejsza, w potrójnym przecięciu Autonomii, Ogólności i Inteligencji. Bezpieczne przystanie od bezwzględnej odpowiedzialności i silnej regulacji można uzyskać poprzez pozytywne argumenty bezpieczeństwa wykazujące, że system jest słaby i/lub wąski i/lub pasywny. Limity na całkowitą Moc Obliczeniową Treningu i szybkość Mocy Obliczeniowej Inferencji, weryfikowane i egzekwowane prawnie oraz przy użyciu sprzętowych i kryptograficznych środków bezpieczeństwa, stanowią ostatnią linię obrony bezpieczeństwa, unikając pełnego AGI i skutecznie zakazując superinteligencji.


[^87]: Najprawdopodobniej rozprzestrzenianie się tej realizacji będzie wymagało intensywnych wysiłków grup edukacyjnych i adwokackich prezentujących ten argument lub dość znaczącej katastrofy spowodowanej przez AI. Możemy mieć nadzieję, że będzie to to pierwsze.

[^88]: Paradoksalnie, przyzwyczailiśmy się, że Natura ogranicza naszą technologię, czyniąc ją bardzo trudną do rozwoju, szczególnie naukowo. Ale tak już nie jest w przypadku AI: kluczowe problemy naukowe okazują się łatwiejsze niż oczekiwano. Nie możemy liczyć na to, że Natura uratuje nas przed sobą – będziemy musieli to zrobić sami.

[^89]: Gdzie dokładnie zatrzymujemy się w rozwoju nowych systemów? Tutaj powinniśmy przyjąć zasadę ostrożności. Gdy system zostanie wdrożony, a szczególnie gdy ten poziom zdolności systemu rozprzestrzeni się, jest niezwykle trudno to cofnąć. A jeśli system zostanie *opracowany* (szczególnie przy wielkich kosztach i wysiłku), będzie ogromna presja, aby go użyć lub wdrożyć, i pokusa, aby został przeciekł lub skradziony. Opracowywanie systemów, a *następnie* decydowanie, czy są głęboko niebezpieczne, to niebezpieczna droga.

[^90]: Mądre byłoby także zakazanie rozwoju AI, który jest wewnętrznie niebezpieczny, jak systemy samoreplikujące się i ewoluujące, te zaprojektowane do ucieczki z ograniczeń, te mogące autonomicznie się samodoskonalić, celowo oszukańcze i złośliwe AI itp.

[^91]: Zauważ, że nie oznacza to koniecznie *egzekwowanych* na poziomie międzynarodowym przez jakiś rodzaj globalnego organu: zamiast tego suwerenne narody mogłyby egzekwować uzgodnione zasady, jak w wielu traktatach.

[^92]: Jak zobaczymy poniżej, natura obliczeń AI pozwoliłaby na coś w rodzaju hybrydy; ale współpraca międzynarodowa będzie wciąż potrzebna.

[^93]: Na przykład maszyny wymagane do trawieniu chipów związanych z AI są wytwarzane tylko przez jedną firmę, ASML (pomimo wielu innych prób), zdecydowana większość odpowiednich chipów jest produkowana przez jedną firmę, TSMC (pomimo prób konkurencji przez innych), a projektowanie i konstrukcja sprzętu z tych chipów jest wykonywana przez zaledwie kilku, włączając NVIDIA, AMD i Google.

[^94]: Co najważniejsze, każdy chip posiada unikalny i niedostępny kryptograficzny klucz prywatny, którego może używać do "podpisywania" rzeczy.

[^95]: Domyślnie byłaby to firma sprzedająca chipy, ale inne modele są możliwe i potencjalnie użyteczne.

[^96]: Zarządca może ustalić lokalizację chipa, mierząc czas wymiany podpisanych wiadomości z nim: skończona prędkość światła wymaga, aby chip znajdował się w promieniu *r* od "stacji", jeśli może zwrócić podpisaną wiadomość w czasie mniejszym niż *r* / *c*, gdzie *c* to prędkość światła. Używając wielu stacji i pewnego zrozumienia charakterystyk sieci, lokalizacja chipa może być określona. Piękno tej metody polega na tym, że większość jej bezpieczeństwa jest zapewniana przez prawa fizyki. Inne metody mogłyby używać GPS, śledzenia inercyjnego i podobnych technologii.

[^97]: Alternatywnie pary chipów mogłyby być dozwolone do komunikacji ze sobą tylko za wyraźnym pozwoleniem zarządcy.

[^98]: Jest to kluczowe, ponieważ przynajmniej obecnie bardzo wysokie pasmo połączenia między chipami jest potrzebne do trenowania dużych modeli AI na nich.

[^99]: Może to być także skonfigurowane, aby wymagać podpisanych wiadomości od *N* z *M* różnych zarządców, pozwalając wielu stronom na wspólne zarządzanie.

[^100]: To daleko nie ma precedensu – na przykład wojska nie opracowały armii sklonowanych lub genetycznie zmodyfikowanych supersołdatów, choć prawdopodobnie jest to technologicznie możliwe. Ale *zdecydowały się* tego nie robić, zamiast być powstrzymywane przez innych. Historia nie jest świetna, jeśli chodzi o powstrzymywanie głównych mocarstw światowych przed rozwój technologii, którą silnie chcą rozwijać.

[^101]: Z kilkoma zauważalnymi wyjątkami (w szczególności NVIDIA) sprzęt wyspecjalizowany pod AI stanowi względnie małą część ogólnego modelu biznesowego i przychodów tych firm. Ponadto luka między sprzętem używanym w zaawansowanej AI a sprzętem "konsumenckim" jest znacząca, więc większość konsumentów sprzętu komputerowego byłaby w dużej mierze nietknięta.

[^102]: Dla bardziej szczegółowej analizy, zobacz niedawne raporty od [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) i [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). Skupiają się one na wykonalności technicznej, szczególnie w kontekście amerykańskich kontroli eksportowych dążących do ograniczenia zdolności innych krajów w obliczeniach wysokiej klasy; ale ma to oczywiste pokrywanie się z globalnym ograniczeniem wyobrażonym tutaj.

[^103]: Na przykład urządzenia Apple są zdalnie i bezpiecznie blokowane, gdy są zgłaszane jako zgubione lub skradzione, i mogą być ponownie aktywowane zdalnie. Opiera się to na tych samych funkcjach bezpieczeństwa sprzętowego omówionych tutaj.

[^104]: Zobacz np. oferowaną przez IBM [pojemność na żądanie](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand), Intel [Intel na żądanie](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html) i Apple [prywatne obliczenia w chmurze](https://security.apple.com/blog/private-cloud-compute/).

[^105]: [To badanie](https://epochai.org/trends#hardware-trends-section) pokazuje, że historycznie ta sama wydajność była osiągana przy użyciu około 30% mniejszej liczby dolarów rocznie. Jeśli ten trend będzie kontynuowany, może być znaczące pokrywanie się między AI a "konsumenckimi" chipami, i ogólnie ilość potrzebnego sprzętu dla systemów AI wysokiej mocy mogłaby stać się niekomfortowo mała.

[^106]: Według [tego samego badania](https://epochai.org/trends#hardware-trends-section), dana wydajność w rozpoznawaniu obrazów wymagała 2,5 razy mniej obliczeń rocznie. Gdyby miało to także dotyczyć najbardziej zdolnych systemów AI, limit obliczeniowy nie byłby użyteczny przez bardzo długo.

[^107]: W szczególności na poziomie kraju wygląda to bardzo podobnie do nacjonalizacji obliczeń, w tym sensie, że rząd miałby dużą kontrolę nad tym, jak wykorzystywana jest moc obliczeniowa. Jednak dla tych, którzy martwią się o zaangażowanie rządu, wydaje się to daleko bezpieczniejsze i preferowane od najbardziej potężnego oprogramowania AI *samego w sobie* będącego znacjonalizowanym poprzez jakąś fuzję między głównymi firmami AI a rządami narodowymi, jak niektórzy zaczynają się opowiadać.

[^108]: Główny krok regulacyjny w Europie został podjęty z uchwaleniem [Aktu AI UE](https://artificialintelligenceact.eu/) w 2024 roku. Klasyfikuje AI według ryzyka: zakazując nieakceptowalnych systemów, regulując wysokoryzykowne i nakładając reguły przejrzystości, lub żadne środki, na systemy niskiego ryzyka. Znacznie zmniejszy niektóre ryzyko AI i zwiększy przejrzystość AI nawet dla firm amerykańskich, ale ma dwie kluczowe wady. Po pierwsze, ograniczony zasięg: choć dotyczy każdej firmy dostarczającej AI w UE, egzekwowanie wobec firm amerykańskich jest słabe, a wojskowa AI jest zwolniona. Po drugie, choć obejmuje GPAI, nie uznaje AGI lub superinteligencji za nieakceptowalne ryzyko lub nie zapobiega ich rozwojowi – tylko ich wdrażaniu w UE. W rezultacie robi niewiele, aby ograniczyć ryzyko AGI lub superinteligencji.

[^109]: Firmy często reprezentują, że są za rozsądną regulacją. Ale jakoś niemal zawsze wydają się sprzeciwiać każdej *konkretnej* regulacji; świadczy o tym walka o dość łagodną SB1047, której [większość firm AI publiczne lub prywatne sprzeciwiała się](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/).

[^110]: Było około 3½ roku od czasu zaproponowania aktu AI UE do jego wejścia w życie.

[^111]: Czasami wyrażane jest, że "za wcześnie" zaczynać regulować AI. Biorąc pod uwagę ostatnią notę, wydaje się to mało prawdopodobne. Inną wyrażaną troską jest, że regulacja "zaszkodziłaby innowacji". Ale dobra regulacja po prostu zmienia kierunek, a nie ilość innowacji.

[^112]: Ciekawym precedensem jest transport materiałów niebezpiecznych, które mogą uciec i spowodować szkody. Tutaj [regulacja](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) i [orzecznictwo](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) ustaliły bezwzględną odpowiedzialność dla bardzo niebezpiecznych materiałów jak materiały wybuchowe, benzyna, trucizny, środki zakaźne i odpady radioaktywne. Inne przykłady obejmują [ostrzeżenia na farmaceutykach](https://www.medicalnewstoday.com/articles/boxed-warnings), [klasy urządzeń medycznych](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification) itp.

[^113]: Inna kompleksowa propozycja o podobnych celach przedstawiona w ["Wąska Ścieżka"](https://www.narrowpath.co/) opowiada się za bardziej scentralizowanym, opartym na zakazie podejściem, które kieruje cały rozwój pionierskiej AI przez jedną międzynarodową jednostkę, nadzorowaną przez silne instytucje międzynarodowe, z jasnymi zakazami kategorycznymi zamiast stopniowanych ograniczeń. Poparłbym także ten plan; jednak będzie wymagał jeszcze więcej woli politycznej i koordynacji niż ten proponowany tutaj.

[^114]: Niektóre wytyczne dla takiego standardu zostały [opublikowane](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/) przez Forum Modeli Pionierskich. W stosunku do propozycji tutaj, te błądzą w stronę mniejszej precyzji i mniejszej mocy obliczeniowej wliczonej do sumy.

[^115]: Zarządzenie wykonawcze AI z 2023 roku w USA (teraz uchylone) wymagało podobnego, ale mniej szczegółowego raportowania. Powinno być to wzmocnione przez zastępujące zarządzenie.

[^116]: Bardzo w przybliżeniu, dla obecnie powszechnych chipów H100 odpowiada to klastrom około 1000 wykonujących inferencję; to około 100 (około 5 mln USD wartości) najnowszych najwyższej klasy chipów NVIDIA B200 wykonujących inferencję. W obu przypadkach liczba treningu odpowiada temu klastrowi obliczającemu przez kilka miesięcy.

[^117]: Ta ilość jest większa niż jakikolwiek obecnie trenowany system AI; większa lub mniejsza liczba mogłaby być uzasadniona, gdy lepiej zrozumiemy, jak zdolność AI skaluje się z mocą obliczeniową.

[^118]: Dotyczy to tych tworzących i dostarczających/hostujących modele, a nie użytkowników końcowych.

[^119]: W przybliżeniu, odpowiedzialność "bezwzględna" oznacza, że deweloperzy są pociągani do odpowiedzialności za szkody wyrządzone przez produkt *domyślnie* i jest standardem używanym dla produktów "nienormalnie niebezpiecznych", i (nieco zabawnie, ale odpowiednio) dzikich zwierząt. Odpowiedzialność "solidarna" oznacza, że odpowiedzialność jest przypisana wszystkim stronom odpowiedzialnym za produkt, a te strony muszą wyjaśnić między sobą, kto ponosi jaką odpowiedzialność. Jest to ważne dla systemów jak AI z długim i złożonym łańcuchem wartości.

[^120]: Standardowa odpowiedzialność oparta na winie jednostronnej nie wystarczy: winę będzie trudno prześledzić i przypisać, ponieważ systemy AI są złożone, ich działanie nie jest rozumiane, a wiele stron może być zaangażowanych w tworzenie niebezpiecznego systemu lub wyniku. Ponadto procesy sądowe będą trwać lata, a prawdopodobnie skończą się tylko grzywnami, które są nieistotne dla tych firm, więc ważna jest także osobista odpowiedzialność dla kadry kierowniczej.

[^121]: Nie powinno być zwolnienia z kryteriów bezpieczeństwa dla modeli o otwartych wagach. Ponadto w ocenie ryzyka powinno się założyć, że zabezpieczenia, które można usunąć, zostaną usunięte z szeroko dostępnych modeli, i że nawet zamknięte modele będą się rozprzestrzeniać, chyba że jest bardzo wysokie zapewnienie, że pozostaną bezpieczne.

[^122]: Schemat proponowany tutaj ma kontrolę regulacyjną uruchamianą na ogólnej zdolności; jednak sensowne jest, aby niektóre szczególnie ryzykowne przypadki użycia uruchamiały większą kontrolę – na przykład ekspertowy system AI wirusologii, nawet jeśli wąski i pasywny, prawdopodobnie powinien iść w wyższy poziom. Poprzednie zarządzenie wykonawcze USA miało część tej struktury dla zdolności biologicznych.

[^123]: Dwa jasne przykłady to lotnictwo i leki, regulowane przez FAA i FDA oraz podobne agencje w innych krajach. Te agencje są niedoskonałe, ale były absolutnie vitalne dla funkcjonowania i sukcesu tych branż.

## Rozdział 9 - Kształtowanie przyszłości — co powinniśmy robić zamiast tego

AI może przynieść niewiarygodne dobro światu. Aby uzyskać wszystkie korzyści bez ryzyka, musimy zapewnić, że AI pozostanie narzędziem człowieka.

Jeśli z powodzeniem wybierzemy, by nie zastępować ludzkości maszynami – przynajmniej przez jakiś czas! – co możemy zrobić zamiast tego? Czy rezygnujemy z ogromnego potencjału AI jako technologii? Na pewnym poziomie odpowiedź brzmi po prostu *nie:* zamknijmy Bramy przed niekontrolowalną AGI i superinteligencją, ale *budujmy* wiele innych form AI, a także struktury zarządzania i instytucje, których potrzebujemy, aby nimi kierować.

Ale wciąż jest wiele do powiedzenia; realizacja tego byłaby centralnym zajęciem ludzkości. Ta sekcja eksploruje kilka kluczowych tematów:

- Jak możemy scharakteryzować AI „Narzędziowe" i formy, jakie może przyjmować.
- Że możemy uzyskać (prawie) wszystko, czego chce ludzkość, bez AGI, przy pomocy AI Narzędziowego.
- Że systemy AI Narzędziowego są (prawdopodobnie, w zasadzie) możliwe do zarządzania.
- Że odwrócenie się od AGI nie oznacza kompromisu w kwestii bezpieczeństwa narodowego – wręcz przeciwnie.
- Że koncentracja władzy to realna troska. Czy możemy ją złagodzić bez podważania bezpieczeństwa i ochrony?
- Że będziemy chcieli – i potrzebowali – nowych struktur zarządzania i społecznych, a AI może faktycznie pomóc.

### AI wewnątrz Bram: AI Narzędziowe

Diagram potrójnego przecięcia daje dobry sposób na wyznaczenie tego, co możemy nazwać „AI Narzędziowym": AI, które jest kontrolowalnym narzędziem do użytku przez człowieka, a nie niekontrolowalnym rywalem czy zamiennikiem. Najmniej problematyczne systemy AI to te, które są autonomiczne, ale nie ogólne ani super zdolne (jak bot do licytacji aukcyjnych), lub ogólne, ale nie autonomiczne czy zdolne (jak mały model językowy), lub zdolne, ale wąskie i bardzo kontrolowalne (jak AlphaGo).[^124] Te z dwoma przecinającymi się cechami mają szersze zastosowanie, ale wyższe ryzyko i będą wymagały znacznych wysiłków w zarządzaniu. (To, że system AI jest bardziej narzędziem, nie oznacza, że jest z natury bezpieczny, tylko że nie jest z natury *niebezpieczny* – rozważ piłę łańcuchową kontra tygrysa domowego.) Brama musi pozostać zamknięta przed (pełną) AGI i superinteligencją w potrójnym przecięciu, a ogromną ostrożność trzeba zachować z systemami AI zbliżającymi się do tego progu.

Ale to pozostawia mnóstwo potężnego AI! Możemy uzyskać ogromną użyteczność z inteligentnych i ogólnych pasywnych „wyroczni" i systemów wąskich, systemów ogólnych na poziomie ludzkim, ale nie nadludzkim, i tak dalej. Wiele firm technologicznych i deweloperów aktywnie buduje tego rodzaju narzędzia i powinno kontynuować; jak większość ludzi, zakładają w sposób dorozumiany, że Bramy do AGI i superinteligencji zostaną zamknięte.[^125]

Ponadto systemy AI można skutecznie łączyć w systemy złożone, które zachowują ludzki nadzór przy jednoczesnym wzmacnianiu zdolności. Zamiast polegać na niezbadanych czarnych skrzynkach, możemy budować systemy, w których wiele komponentów – w tym zarówno AI, jak i tradycyjne oprogramowanie – współpracuje w sposób, który ludzie mogą monitorować i rozumieć.[^126] Podczas gdy niektóre komponenty mogą być czarnymi skrzynkami, żaden nie byłby blisko AGI – tylko system złożony jako całość byłby zarówno bardzo ogólny, jak i bardzo zdolny, i to w sposób ściśle kontrolowalny.[^127]

#### Znacząca i gwarantowana kontrola człowieka

Co oznacza „ściśle kontrolowalny"? Kluczową ideą ram „Narzędzia" jest umożliwienie systemów – nawet jeśli całkiem ogólnych i potężnych – które są gwarantowane jako pozostające pod znaczącą kontrolą człowieka. Co to oznacza? Obejmuje to dwa aspekty. Pierwszy to kwestia projektowa: ludzie powinni być głęboko i centralnie zaangażowani w to, co system robi, *nie* delegując kluczowych ważnych decyzji na AI. To charakteryzuje większość obecnych systemów AI. Po drugie, w stopniu, w jakim systemy AI są autonomiczne, muszą mieć gwarancje ograniczające ich zakres działania. Gwarancja powinna być *liczbą* charakteryzującą prawdopodobieństwo wystąpienia czegoś i powodem, by wierzyć w tę liczbę. To właśnie żądamy w innych dziedzinach krytycznych pod względem bezpieczeństwa, gdzie liczby jak „średni czas między awariami" i oczekiwane liczby wypadków są obliczane, uzasadniane i publikowane w przypadkach bezpieczeństwa.[^128] Idealna liczba awarii to oczywiście zero. A dobra wiadomość jest taka, że możemy zbliżyć się całkiem blisko, choć używając całkiem różnych architektur AI, wykorzystując idee *formalnie zweryfikowanych* właściwości programów (w tym AI). Idea, eksplorowana obszernie przez Omohundro, Tegmarka, Bengio, Dalrymple'a i innych (zobacz [tutaj](https://arxiv.org/abs/2309.01933) i [tutaj](https://arxiv.org/abs/2405.06624)), polega na skonstruowaniu programu z określonymi właściwościami (na przykład: że człowiek może go wyłączyć) i formalnym *udowodnieniu*, że te właściwości zachodzą. Można to robić teraz dla całkiem krótkich programów i prostych właściwości, ale (nadchodząca) moc oprogramowania do dowodów wspieranego przez AI mogłaby pozwolić na to dla znacznie bardziej złożonych programów (np. opakowań) a nawet samego AI. To bardzo ambitny program, ale gdy presja na Bramy rośnie, będziemy potrzebowali jakichś potężnych materiałów je wzmacniających. Dowód matematyczny może być jednym z niewielu wystarczająco silnych.

#### Gdzie przemysł AI

Przy przekierowaniu postępu AI, AI Narzędziowe wciąż byłoby ogromnym przemysłem. Pod względem sprzętu, nawet z ograniczeniami mocy obliczeniowej zapobiegającymi superinteligencji, trening i inferencja w mniejszych modelach nadal będą wymagały ogromnych ilości wyspecjalizowanych komponentów. Po stronie oprogramowania, rozładowanie eksplozji w rozmiarze modeli AI i obliczeniach powinno po prostu prowadzić firmy do przekierowania zasobów na uczynienie mniejszych systemów lepszymi, bardziej zróżnicowanymi i bardziej wyspecjalizowanymi, zamiast po prostu czynienia ich większymi.[^129] Byłoby mnóstwo miejsca – prawdopodobnie więcej – dla wszystkich tych zarabiających pieniądze startupów z Doliny Krzemowej.[^130]

### AI Narzędziowe może przynieść (prawie) wszystko, czego chce ludzkość, bez AGI

Inteligencja, czy biologiczna, czy maszynowa, może być szeroko rozważana jako zdolność planowania i wykonywania działań przynoszących przyszłości bardziej zgodne z zestawem celów. Jako taka inteligencja jest enormalnie korzystna, gdy używana w dążeniu do mądrze wybranych celów. Sztuczna inteligencja przyciąga ogromne inwestycje czasu i wysiłku głównie z powodu obiecanych korzyści. Więc powinniśmy zapytać: w jakim stopniu nadal zbieralibyśmy korzyści z AI, gdybyśmy powstrzymali jej ucieczkę do superinteligencji? Odpowiedź: możemy stracić zaskakująco mało.

Rozważmy najpierw, że obecne systemy AI są już bardzo potężne i naprawdę tylko zarysowaliśmy powierzchnię tego, co można z nimi zrobić.[^131] Są dość zdolne do „prowadzenia przedstawienia" w kategoriach „zrozumienia" pytania lub zadania im przedstawionego i tego, co byłoby potrzebne, aby odpowiedzieć na to pytanie lub wykonać to zadanie.

Następnie, wiele z ekscytacji wokół nowoczesnych systemów AI wynika z ich ogólności; ale niektóre z najbardziej zdolnych systemów AI – takie jak te, które generują lub rozpoznają mowę czy obrazy, robią przewidywania i modelowanie naukowe, grają w gry itp. – są znacznie węższe i dobrze „wewnątrz Bram" pod względem obliczeń.[^132] Te systemy są nadludzkie w konkretnych zadaniach, które wykonują. Mogą mieć słabości przypadków brzegowych [^133] (lub [wykorzystywalne](https://arxiv.org/abs/2211.00241)) ze względu na swoją wąskość; jednak *całkowicie* wąskie czy *w pełni* ogólne nie są jedynymi dostępnymi opcjami: jest wiele architektur pomiędzy.[^134]

Te narzędzia AI mogą znacznie przyspieszyć rozwój innych pozytywnych technologii, bez AGI. Aby lepiej robić fizykę jądrową, nie potrzebujemy, by AI było fizykiem jądrowym – mamy takich! Jeśli chcemy przyspieszyć medycynę, dajmy biologom, badaczom medycznym i chemikom potężne narzędzia. Chcą ich i użyją ich z ogromną korzyścią. Nie potrzebujemy farmy serwerowej pełnej miliona cyfrowych geniuszy; mamy miliony ludzi, których geniusz AI może pomóc wydobyć. Tak, zajmie więcej czasu, by uzyskać nieśmiertelność i lekarstwo na wszystkie choroby. To prawdziwy koszt. Ale nawet najbardziej obiecujące innowacje zdrowotne byłyby małoużyteczne, gdyby napędzana przez AI niestabilność prowadziła do globalnego konfliktu czy załamania społecznego. Jesteśmy to winni sobie samym, by dać wspieranym przez AI ludziom szansę na ten problem najpierw.

A załóżmy, że jest faktycznie jakaś ogromna korzyść z AGI, której nie można uzyskać przez ludzkość używającą narzędzi wewnątrz-Bramowych. Czy tracimy te korzyści przez *nigdy* nie budowanie AGI i superinteligencji? Ważąc ryzyko i nagrody tutaj, jest ogromna asymetryczna korzyść w czekaniu kontra śpieszeniu się: możemy czekać, aż można to zrobić w gwarantowany bezpieczny i korzystny sposób, i prawie wszyscy nadal będą mogli zbierać nagrody; jeśli się śpieszymy, może to być – słowami CEO OpenAI Sama Altmana – [zgaszenie świateł dla *wszystkich* z nas.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

Ale jeśli narzędzia nie-AGI są potencjalnie tak potężne, czy możemy nimi zarządzać? Odpowiedź brzmi wyraźnie... może.

### Systemy AI Narzędziowego są (prawdopodobnie, w zasadzie) możliwe do zarządzania

Ale nie będzie łatwo. Obecne najnowocześniejsze systemy AI mogą znacznie wzmocnić ludzi i instytucje w osiąganiu ich celów. To generalnie dobra rzecz! Jednak są naturalne dynamiki posiadania takich systemów do dyspozycji – nagle i bez dużo czasu dla społeczeństwa na adaptację – które oferują poważne ryzyko wymagające zarządzania. Warto omówić kilka głównych klas takiego ryzyka i jak można je zmniejszyć, zakładając zamknięcie Bramy.

Jedna klasa ryzyka to wysokiej mocy AI Narzędziowe umożliwiające dostęp do wiedzy czy zdolności, która wcześniej była związana z osobą lub organizacją, czyniąc kombinację wysokiej zdolności plus wysokiej lojalności dostępną bardzo szerokiej gamie aktorów. Dziś, z wystarczającą ilością pieniędzy osoba o złych intencjach mogłaby wynająć zespół chemików do projektowania i produkcji nowej broni chemicznej – ale nie jest tak bardzo łatwo mieć te pieniądze czy znaleźć/zebrać zespół i przekonać ich do robienia czegoś wyraźnie nielegalnego, nieetycznego i niebezpiecznego. Aby zapobiec systemom AI graniu takiej roli, ulepszenia obecnych metod mogą w pełni wystarczyć,[^135] o ile wszystkie te systemy i dostęp do nich są odpowiedzialnie zarządzane. Z drugiej strony, jeśli potężne systemy są wypuszczane do ogólnego użytku i modyfikacji, wszelkie wbudowane środki bezpieczeństwa są prawdopodobnie usuwalne. Więc aby uniknąć ryzyka w tej klasie, silne ograniczenia co do tego, co może być publicznie wypuszczane – analogiczne do ograniczeń na szczegóły technologii jądrowych, wybuchowych i innych niebezpiecznych – będą wymagane.[^136]

Druga klasa ryzyka wynika ze skalowania maszyn, które działają jak lub udają ludzi. Na poziomie szkody dla indywidualnych osób te ryzyka obejmują znacznie skuteczniejsze oszustwa, spam i phishing oraz proliferację deepfake'ów bez zgody.[^137] Na poziomie zbiorowym obejmują zakłócenie podstawowych procesów społecznych, jak publiczna dyskusja i debata, nasze społeczne systemy zbierania, przetwarzania i rozpowszechniania informacji i wiedzy oraz nasze systemy wyboru politycznego. Łagodzenie tego ryzyka prawdopodobnie będzie obejmowało (a) prawa ograniczające udawanie ludzi przez systemy AI i obciążające odpowiedzialnością deweloperów AI tworzących systemy generujące takie udawanie, (b) systemy znakowania wodnego i pochodzenia identyfikujące i klasyfikujące (odpowiedzialnie) generowaną przez AI zawartość, oraz (c) nowe socjo-techniczne systemy epistemiczne mogące stworzyć zaufany łańcuch od danych (np. kamery i nagrania) przez fakty, zrozumienie i dobre modele świata.[^138] Wszystko to jest możliwe i AI może pomóc z niektórymi jego częściami.

Trzecie ogólne ryzyko to to, że w stopniu, w jakim niektóre zadania są automatyzowane, ludzie obecnie wykonujący te zadania mogą mieć mniejszą wartość finansową jako siła robocza. Historycznie automatyzacja zadań sprawiała, że rzeczy umożliwione przez te zadania stawały się tańsze i bardziej obfite, jednocześnie sortując ludzi wcześniej wykonujących te zadania na tych nadal zaangażowanych w zautomatyzowaną wersję (generalnie przy wyższych kwalifikacjach/płacy) i tych, których praca jest warta mniej lub niewiele. W sumie trudno przewidzieć, w których sektorach więcej kontra mniej ludzkiej pracy będzie wymagane w powstałym większym, ale bardziej wydajnym sektorze. Równolegle dynamika automatyzacji prowadzi do zwiększenia nierówności i ogólnej produktywności, zmniejszenia kosztu pewnych dóbr i usług (przez wzrosty wydajności) oraz zwiększenia kosztu innych (przez [chorobę kosztów](https://en.wikipedia.org/wiki/Baumol_effect)). Dla tych po niekorzystnej stronie wzrostu nierówności głęboko niejasne jest, czy spadek kosztów w tych pewnych dobrach i usługach przewyższa wzrost w innych i prowadzi do ogólnie większego dobrostanu. Więc jak to pójdzie dla AI? Z powodu względnej łatwości, z jaką ludzka praca intelektualna może być zastąpiona przez ogólne AI, możemy oczekiwać szybkiej wersji tego z konkurencyjnym dla człowieka AI ogólnego przeznaczenia.[^139] Jeśli zamkniemy Bramę do AGI, znacznie mniej miejsc pracy będzie hurtowo zastąpionych przez agentów AI; ale ogromne przemieszczenie siły roboczej nadal jest prawdopodobne w okresie lat.[^140] Aby uniknąć rozpowszechnionego cierpienia ekonomicznego, prawdopodobnie konieczne będzie wdrożenie zarówno jakiejś formy uniwersalnych aktywów podstawowych czy dochodu, jak również zaprojektowanie kulturowej zmiany w kierunku wartościowania i nagradzania pracy ludzkiej, która jest trudniejsza do automatyzacji (zamiast patrzenia, jak ceny pracy spadają przez wzrost dostępnej siły roboczej wypchnięcej z innych części gospodarki). Inne konstrukty, takie jak „[godność danych](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society)" (w której ludzie produkujący dane treningowe są auto-nagradzani tantiemami za wartość stworzoną przez te dane w AI), mogą pomóc. Automatyzacja przez AI ma też drugi potencjalny negatywny efekt, którym jest *niewłaściwa* automatyzacja. Wraz z aplikacjami, gdzie AI po prostu wykonuje gorszą pracę, obejmowałoby to te, gdzie systemy AI prawdopodobnie naruszą zasady moralne, etyczne czy prawne – na przykład w decyzjach o życiu i śmierci oraz w sprawach sądowych. Te muszą być traktowane przez stosowanie i rozszerzanie naszych obecnych ram prawnych.

Wreszcie, znaczące zagrożenie AI wewnątrz-bramowego to jego użycie w spersonalizowanej perswazji, przechwytywaniu uwagi i manipulacji. Widzieliśmy w mediach społecznościowych i innych platformach internetowych wzrost głęboko zakorzenionej ekonomii uwagi (gdzie usługi internetowe zaciekle walczą o uwagę użytkowników) i systemów [„kapitalizmu inwigilacji"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism) (w których informacje o użytkownikach i profilowanie są dodane do utowarowienia uwagi). Niemal pewne jest, że więcej AI zostanie postawione w służbie obu. AI jest już intensywnie używane w algorytmach uzależniających od kanałów, ale to ewoluuje w uzależniającą zawartość generowaną przez AI, dostosowaną, by być kompulsywnie konsumowaną przez pojedynczą osobę. A wejścia, odpowiedzi i dane tej osoby będą karmione do maszyny uwaga/reklama, by kontynuować błędne koło. Ponadto, gdy pomocnicy AI dostarczani przez firmy technologiczne staną się interfejsem dla więcej życia online, prawdopodobnie zastąpią wyszukiwarki i kanały jako mechanizm, przez który perswazja i monetyzacja klientów występuje. Niepowodzenie naszego społeczeństwa w kontrolowaniu tej dynamiki dotąd nie wróży dobrze. Część tej dynamiki może być zmniejszona przez regulacje dotyczące prywatności, praw do danych i manipulacji. Dotarcie bardziej do korzenia problemu może wymagać różnych perspektyw, takich jak lojalni asystenci AI (omówieni poniżej).

Wniosek z tej dyskusji brzmi: nadzieja: systemy narzędziowe wewnątrz-Bram – przynajmniej dopóki pozostają porównywalne w mocy i zdolności z dzisiejszymi najnowocześniejszymi systemami – są prawdopodobnie możliwe do zarządzania, jeśli jest wola i koordynacja, by to robić. Przyzwoite ludzkie instytucje, wspierane przez narzędzia AI,[^141] mogą to zrobić. Moglibyśmy też w tym nie powieść. Ale trudno zobaczyć, jak pozwolenie na potężniejsze systemy pomogłoby – poza postawieniem ich za ster i marzeniem o najlepszym.

### Bezpieczeństwo narodowe

Wyścigi o supremację AI – napędzane motywacjami bezpieczeństwa narodowego czy innymi – pchają nas ku niekontrolowanym potężnym systemom AI, które będą skłonne wchłaniać, a nie obdarzać władza. Wyścig AGI między USA a Chinami to wyścig o ustalenie, który naród dostanie superinteligencję pierwszy.

Co więc powinni robić ci odpowiedzialni za bezpieczeństwo narodowe? Rządy mają silne doświadczenie w budowaniu kontrolowalnych i bezpiecznych systemów i powinny podwoić wysiłki w robieniu tego w AI, wspierając rodzaj projektów infrastrukturalnych, które odnoszą najlepszy sukces, gdy robione na dużą skalę i z rządowym poparciem.

Zamiast lekkomyślnego „projektu Manhattan" w kierunku AGI,[^142] rząd USA mógłby uruchomić projekt Apollo dla kontrolowalnych, bezpiecznych, godnych zaufania systemów. Mogłoby to obejmować na przykład:

- Główny program (a) opracowania mechanizmów bezpieczeństwa sprzętowego na chipie i (b) infrastruktury do zarządzania stroną mocy obliczeniowej potężnego AI. Mogłyby one opierać się na amerykańskim [akcie CHIPS](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) i [reżimie kontroli eksportu](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion).
- Inicjatywę na dużą skalę do opracowania technik formalnej weryfikacji, tak by konkretne cechy systemów AI (jak wyłącznik) mogły być *udowodnione* jako obecne lub nieobecne. Może to wykorzystać samo AI do opracowania dowodów właściwości.
- Wysiłek na skalę narodową do stworzenia oprogramowania, które jest weryfikowalnie bezpieczne, napędzane przez narzędzia AI mogące przekodowywać istniejące oprogramowanie w weryfikowalnie bezpieczne ramy.
- Narodowy projekt inwestycyjny w rozwój naukowy używający AI,[^143] działający jako partnerstwo między DOE, NSF i NIH.

Ogólnie jest ogromna powierzchnia ataku na nasze społeczeństwo, która czyni nas podatnymi na ryzyko z AI i jego nadużycia. Ochrona przed niektórymi z tych ryzyk będzie wymagała inwestycji i standaryzacji na poziomie rządowym. Te dałyby znacznie więcej bezpieczeństwa niż dolewanie benzyny do ognia wyścigów w kierunku AGI. A jeśli AI ma być wbudowane w uzbrojenie i systemy dowodzenia i kontroli, kluczowe jest, by AI było godne zaufania i bezpieczne, czym obecne AI po prostu nie jest.

### Koncentracja władzy i jej łagodzenie

Ten esej skupił się na idei ludzkiej kontroli AI i jej potencjalnej porażce. Ale inną ważną soczewką, przez którą można postrzegać sytuację AI, jest *koncentracja władzy.* Rozwój bardzo potężnego AI grozi skoncentrowaniem władzy albo w bardzo nielicznych i bardzo dużych korporacyjnych rękach, które go opracowały i będą kontrolować, albo w rządach używających AI jako nowego środka do utrzymania własnej władzy i kontroli, albo w samych systemach AI. Lub jakiejś niegodziwej mieszance powyższego. W każdym z tych przypadków większość ludzkości traci władzę, kontrolę i sprawczość. Jak moglibyśmy z tym walczyć?

Pierwszy i najważniejszy krok to oczywiście zamknięcie Bramy przed mądrzejszą niż człowiek AGI i superinteligencją. Te mogą wprost zastąpić ludzi i grupy ludzi. Jeśli są pod kontrolą korporacyjną lub rządową, skoncentrują władzę w tych korporacjach czy rządach; jeśli są „wolne", skoncentrują władzę w sobie. Więc załóżmy, że Bramy są zamknięte. Co wtedy?

Jednym proponowanym rozwiązaniem koncentracji władzy jest AI „open-source", gdzie wagi modelu są swobodnie lub szeroko dostępne. Ale jak wspomniano wcześniej, gdy model jest otwarty, większość środków bezpieczeństwa czy barier może być (i generalnie jest) usunięta. Więc jest ostra napięcie między z jednej strony decentralizacją, a z drugiej strony bezpieczeństwem, ochroną i ludzką kontrolą systemów AI. Są też powody do sceptycyzmu, że otwarte modele same z siebie znacząco zwalczą koncentrację władzy w AI bardziej, niż zrobiły to w systemach operacyjnych (nadal zdominowanych przez Microsoft, Apple i Google pomimo otwartych alternatyw).[^144]

Jednak mogą być sposoby na rozwiązanie tego kółka – centralizację i łagodzenie ryzyka przy jednoczesnej decentralizacji zdolności i nagrody ekonomicznej. To wymaga przemyślenia zarówno tego, jak AI jest opracowywane, jak jego korzyści są dystrybuowane.

Nowe modele publicznego rozwoju i własności AI pomogłyby. Mogłoby to przyjąć kilka form: rządowo-opracowane AI (podlegające nadzorowi demokratycznemu),[^145] organizacje rozwoju AI non-profit (jak Mozilla dla przeglądarek) czy struktury umożliwiające bardzo szeroką własność i zarządzanie. Kluczowe jest, by te instytucje były wprost upoważnione do służenia interesowi publicznemu przy działaniu pod silnymi ograniczeniami bezpieczeństwa.[^146] Dobrze sformułowane reżimy regulacyjne i standardów/certyfikacji też będą żywotne, tak by produkty AI oferowane przez żywy rynek pozostawały naprawdę użyteczne, a nie eksploatacyjne wobec użytkowników.

Pod względem koncentracji władzy ekonomicznej możemy użyć śledzenia pochodzenia i „godności danych", by zapewnić, że korzyści ekonomiczne płyną szerzej. W szczególności większość mocy AI teraz (i w przyszłości, jeśli utrzymamy Bramy zamknięte) wywodzi się z danych generowanych przez ludzi, czy to bezpośrednie dane treningowe, czy ludzkie opinie. Gdyby firmy AI były wymagane do sprawiedliwego wynagradzania dostarczycieli danych,[^147] mogłoby to przynajmniej pomóc dystrybuować nagrody ekonomiczne szerzej. Poza tym inny model mógłby być publiczna własność znacznych frakcji dużych firm AI. Na przykład rządy zdolne do opodatkowania firm AI mogłyby zainwestować frakcję wpływów w suwerenny fundusz majątkowy, który trzyma akcje w firmach i płaci dywidendy ludności.[^148]

Kluczowe w tych mechanizmach jest użycie mocy samego AI do polepszenia dystrybucji władzy, zamiast po prostu walki z koncentracją władzy napędzaną przez AI przy użyciu środków nie-AI. Jedno potężne podejście byłoby przez dobrze zaprojektowanych asystentów AI, którzy działają z prawdziwym obowiązkiem powierniczym wobec użytkowników – stawiając interesy użytkowników pierwszo, szczególnie ponad korporacyjnymi dostawcami.[^149] Ci asystenci muszą być naprawdę godni zaufania, kompetentni technicznie, ale odpowiednio ograniczeni bazując na przypadku użycia i poziomie ryzyka, i szeroko dostępni dla wszystkich przez kanały publiczne, non-profit lub certyfikowane komercyjne. Tak jak nigdy nie zaakceptowalibyśmy ludzkiego asystenta, który potajemnie pracuje przeciw naszym interesom dla innej strony, nie powinniśmy akceptować asystentów AI, którzy inwigilują, manipulują czy wyciągają wartość od użytkowników dla korporacyjnej korzyści.

Taka transformacja fundamentalnie zmieniłaby obecną dynamikę, gdzie jednostki są pozostawione do negocjowania w pojedynkę z ogromnymi (wspieranymi przez AI) korporacyjnymi i biurokratycznymi maszynami, które priorytetyzują wyciąganie wartości nad ludzkim dobrostanem. Podczas gdy jest wiele możliwych podejść do redystrybucji władzy napędzanej przez AI szerzej, żadne nie pojawi się domyślnie: muszą być celowo zaprojektowane i zarządzane mechanizmami jak wymogi powiernicze, dostarczanie publiczne i dostęp warstwowy bazujący na ryzyku.

Podejścia do łagodzenia koncentracji władzy mogą napotkać znaczny opór ze strony władz obecnych.[^150] Ale są ścieżki rozwoju AI, które nie wymagają wybierania między bezpieczeństwem a skoncentrowaną władzą. Budując odpowiednie instytucje teraz, moglibyśmy zapewnić, że korzyści AI są szeroko dzielone, a jego ryzyko ostrożnie zarządzane.

### Nowe struktury zarządzania i społeczne

Nasze obecne struktury zarządzania się z trudem radzą: są wolne w odpowiadaniu, często przejęte przez interesy szczególne i [coraz bardziej nie ufają im ludzie.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) Jednak nie jest to powód do ich porzucenia – wręcz przeciwnie. Niektóre instytucje mogą wymagać zastąpienia, ale szerzej potrzebujemy nowych mechanizmów mogących wzmacniać i uzupełniać nasze istniejące struktury, pomagając im lepiej funkcjonować w naszym szybko ewoluującym świecie.

Wiele z naszej instytucjonalnej słabości wywodzi się nie z formalnych struktur rządowych, ale ze zdegradowanych instytucji społecznych: naszych systemów rozwoju wspólnego zrozumienia, koordynacji działania i prowadzenia znaczącego dyskursu. Dotąd AI przyspieszyło tę degradację, zalewając nasze kanały informacyjne generowaną zawartością, wskazując nas na najbardziej polaryzującą i dzielącą zawartość oraz utrudniając odróżnienie prawdy od fikcji.

Ale AI mogłoby faktycznie pomóc w odbudowie i wzmacnianiu tych instytucji społecznych. Rozważmy trzy kluczowe obszary:

Po pierwsze, AI mogłoby pomóc przywrócić zaufanie do naszych systemów epistemicznych – naszych sposobów poznawania tego, co jest prawdziwe. Moglibyśmy opracować wspierane przez AI systemy śledzące i weryfikujące pochodzenie informacji, od surowych danych przez analizę do wniosków. Te systemy mogłyby łączyć weryfikację kryptograficzną z wyrafinowaną analizą, by pomóc ludziom zrozumieć nie tylko czy coś jest prawdziwe, ale jak wiemy, że jest prawdziwe.[^151] Lojalni asystenci AI mogliby być obciążeni śledzeniem szczegółów, by zapewnić, że się sprawdzają.

Po drugie, AI mogłoby umożliwić nowe formy koordynacji na dużą skalę. Wiele z naszych najpilniejszych problemów – od zmiany klimatu po oporność na antybiotyki – to fundamentalnie problemy koordynacji. Jesteśmy [uwięzieni w sytuacjach, które są gorsze, niż mogłyby być dla niemal wszystkich](https://equilibriabook.com/), bo żadna jednostka czy grupa nie może sobie pozwolić na pierwszy ruch. Systemy AI mogłyby pomóc przez modelowanie złożonych struktur zachęt, identyfikację wykonalnych ścieżek do lepszych wyników i ułatwianie budowy zaufania i mechanizmów zobowiązań potrzebnych, by tam dotrzeć.

Może najbardziej intrygująco, AI mogłoby umożliwić całkowicie nowe formy dyskursu społecznego. Wyobraź sobie możliwość „rozmowy z miastem" [^152] – nie tylko przeglądania statystyk, ale prowadzenia znaczącego dialogu z systemem AI przetwarzającym i syntetyzującym poglądy, doświadczenia, potrzeby i aspiracje milionów mieszkańców. Lub rozważmy, jak AI mogłoby ułatwić prawdziwy dialog między grupami, które obecnie gadają obok siebie, pomagając każdej stronie lepiej zrozumieć rzeczywiste troski i wartości drugiej, a nie ich karykatury siebie nawzajem.[^153] Lub AI mogłoby oferować wykwalifikowane, wiarygodnie neutralne pośredniczenie w sporach między ludźmi czy nawet dużymi grupami ludzi (którzy wszyscy mogliby z nim bezpośrednio i indywidualnie interakcjować!) Obecne AI jest całkowicie zdolne do robienia tej pracy, ale narzędzia do tego nie powstaną same z siebie czy przez zachęty rynkowe.

Te możliwości mogą brzmieć utopijnie, szczególnie biorąc pod uwagę obecną rolę AI w degradacji dyskursu i zaufania. Ale to właśnie dlatego musimy aktywnie opracowywać te pozytywne aplikacje. Zamykając Bramy przed niekontrolowalną AGI i priorytetyzując AI wzmacniające ludzką sprawczość, możemy skierować postęp technologiczny ku przyszłości, gdzie AI służy jako siła wzmocnienia, odporności i zbiorowego rozwoju.


[^124]: To powiedziawszy, trzymanie się z daleka od potrójnego przecięcia niestety nie jest tak łatwe, jak można by chcieć. Forsowanie zdolności bardzo mocno w którymkolwiek z trzech aspektów zwykle zwiększa je w pozostałych. W szczególności może być trudno stworzyć skrajnie ogólną i zdolną inteligencję, której nie można łatwo uczynić autonomiczną. Jedno podejście to trenowanie modeli [„krótkowzrocznych"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) z ograniczoną zdolnością planowania. Inne to skupienie się na inżynierii czystych systemów [„wyrocznii"](https://arxiv.org/abs/1711.05541), które stronią od odpowiadania na pytania zorientowane na działania.

[^125]: Wiele firm nie zdaje sobie sprawy, że one też zostałyby ostatecznie wyparte przez AGI, nawet jeśli zajęłoby to dłużej – gdyby zdawały, może naciskałyby na te Bramy nieco mniej!

[^126]: Systemy AI mogą komunikować się w wydajniejszy, ale mniej zrozumiały sposób, ale utrzymanie ludzkiego zrozumienia powinno mieć priorytet.

[^127]: Ta idea modularnego, interpretowalnego AI została opracowana szczegółowo przez kilku badaczy; zobacz np. model [„Kompleksowych Usług AI"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) Drexlera, [„Otwartą Architekturę Sprawczości"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai) Dalrymple'a i innych. Podczas gdy takie systemy mogą wymagać więcej wysiłku inżynierskiego niż monolityczne sieci neuronowe trenowane z ogromną mocą obliczeniową, to właśnie tam ograniczenia obliczeniowe pomagają – czyniąc bezpieczniejszą, bardziej transparentną ścieżkę także bardziej praktyczną.

[^128]: O przypadkach bezpieczeństwa generalnie zobacz [ten podręcznik](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). Dotycząc AI w szczególności, zobacz [Wasil i in.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [Clymer i in.](https://arxiv.org/abs/2403.10462), [Buhl i in.](https://arxiv.org/abs/2410.21572) i [Balesni i in.](https://arxiv.org/abs/2411.03336)

[^129]: Faktycznie już widzimy ten trend napędzany tylko wysokim kosztem inferencji: mniejsze i bardziej wyspecjalizowane modele „destylowane" z większych i zdolne do działania na tańszym sprzęcie.

[^130]: Rozumiem, dlaczego ci podekscytowani ekosystemem technologii AI mogą przeciwstawiać się temu, co postrzegają jako uciążliwe regulacje swojej branży. Ale szczerze mówiąc, zagadkowe dla mnie jest, dlaczego, powiedzmy, inwestor venture capital chciałby pozwolić na ucieczkę do AGI i superinteligencji. Te systemy (i firmy, dopóki pozostają pod kontrolą firm) *zjedzą wszystkie startupy jako przekąskę*. Prawdopodobnie nawet *wcześniej* niż zjedzenie innych branż. Każdy zainwestowany w kwitnący ekosystem AI powinien priorytetizować zapewnienie, że rozwój AGI nie prowadzi do monopolizacji przez kilku dominujących graczy.

[^131]: Jak powiedział ekonomista i były badacz Deepmind Michael Webb [tutaj](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/), „Myślę, że gdybyśmy zatrzymali dzisiaj cały rozwój większych modeli językowych, więc GPT-4 i Claude i cokolwiek, i to są ostatnie rzeczy, które trenujemy tej wielkości – więc pozwalamy na mnóstwo iteracji na rzeczach tej wielkości i wszelkich strojeniach, ale nic większego niż to, żadne większe postępy – tylko to, co mamy dzisiaj, myślę, wystarczy do napędzania 20 lub 30 lat niewiarygodnego wzrostu ekonomicznego."

[^132]: Na przykład system alphafold DeepMind użył tylko 1/100000 liczby FLOP GPT-4.

[^133]: Trudność samojeżdżących samochodów jest ważna do odnotowania tutaj: podczas gdy nominalnie wąskie zadanie i osiągalne z uczciwą niezawodnością z względnie małymi systemami AI, rozległa wiedza i zrozumienie świata rzeczywistego są konieczne do uzyskania niezawodności na poziomie potrzebnym w tak krytycznym pod względem bezpieczeństwa zadaniu.

[^134]: Na przykład, mając budżet obliczeniowy, prawdopodobnie zobaczylibyśmy modele GPAI wstępnie trenowane na (powiedzmy) połowie tego budżetu, a drugą połowę użytą do trenowania bardzo wysokiej zdolności w węższym zakresie zadań. To dałoby nadludzką wąską zdolność podpartą przez bliską człowiekowi inteligencję ogólną.

[^135]: Obecna dominująca technika dostosowania to „uczenie się przez wzmocnienie przez opinie ludzkie" [(RLHF)](https://arxiv.org/abs/1706.03741) i używa opinii ludzkich do stworzenia sygnału nagrody/kary dla uczenia się przez wzmocnienie modelu AI. Ta i pokrewne techniki jak [konstytucyjne AI](https://arxiv.org/abs/2212.08073) działają zaskakująco dobrze (choć brakuje im solidności i można je obejść przy skromnym wysiłku). Dodatkowo obecne modele językowe są generalnie wystarczająco kompetentne w rozumowaniu zdroworozsądkowym, że nie popełnią głupich błędów moralnych. To coś w rodzaju słodkiego punktu: wystarczająco mądre, by rozumieć, czego ludzie chcą (w stopniu, w jakim można to zdefiniować), ale nie wystarczająco mądre, by planować skomplikowane oszustwa czy powodować ogromne szkody, gdy się myląt.

[^136]: W długim terminie każdy poziom zdolności AI, który zostanie opracowany, prawdopodobnie się rozprzestrzeni, ponieważ ostatecznie to oprogramowanie i użyteczne. Będziemy musieli mieć solidne mechanizmy obrony przed ryzykami stwarzanymi przez takie systemy. Ale *nie mamy tego teraz*, więc musimy być bardzo odmierzeni w tym, jak wiele potężnych modeli AI pozwala się rozprzestrzenić.

[^137]: Zdecydowana większość z nich to niekonsensualne pornograficzne deepfake'i, w tym nieletnich.

[^138]: Wiele składników takich rozwiązań istnieje, w formie przepisów „bot-czy-nie" (w akcie AI UE między innymi), [technologiach śledzenia pochodzenia przemysłu](https://c2pa.org/), [innowacyjnych agregatorach wiadomości](https://www.improvethenews.org/), [agregatach](https://metaculus.com/) przewidywań i rynkach itd.

[^139]: Fala automatyzacji może nie podążać za wcześniejszymi wzorcami, w tym, że stosunkowo *wysokie* zadania umiejętności, takie jak jakościowe pisanie, interpretacja prawa czy dawanie porad medycznych, mogą być tak samo lub nawet bardziej podatne na automatyzację niż zadania niższych umiejętności.

[^140]: Dla ostrożnego modelowania efektu AGI na płace zobacz raport [tutaj](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek) i krwawe szczegóły [tutaj](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0) od Antona Korineka i współpracowników. Odkrywają, że gdy więcej części pracy jest automatyzowanych, produktywność i płace rosną – do pewnego punktu. Gdy *zbyt* wiele jest automatyzowane, produktywność nadal rośnie, ale płace walą się, bo ludzie są zastępowani hurtowo przez wydajne AI. Dlatego zamknięcie Bram jest tak użyteczne: dostajemy produktywność bez znikniętych ludzkich płac.

[^141]: Jest wiele sposobów, w jakie AI może być używane jako i do pomocy w budowaniu technologii „obronnych", by uczynić ochrony i zarządzanie bardziej solidnymi. Zobacz [ten](https://vitalik.eth.limo/general/2025/01/05/dacc2.html) wpływowy post opisujący tę agendę „D/acc".

[^142]: Nieco ironicznie, amerykański projekt Manhattan prawdopodobnie niewiele by zrobił do przyspieszenia terminów do AGI – tarcza ludzkiej i fiskalnej inwestycji w postęp AI jest już przypięta na 11. Główne rezultaty byłyby inspirowanie podobnego projektu w Chinach (które wyróżniają się w projektach infrastrukturalnych na poziomie narodowym), utrudnienie międzynarodowych umów ograniczających ryzyko AI i zaalarmowanie innych geopolitycznych przeciwników USA, takich jak Rosja.

[^143]: Program [„Narodowego Zasobu Badań AI"](https://nairrpilot.org/) to dobry obecny krok w tym kierunku i powinien być rozszerzony.

[^144]: Zobacz [tę analizę](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807) różnych znaczeń i implikacji „otwartości" w produktach technologicznych i jak niektóre doprowadziły do więcej, a nie mniej, utrwalenia dominacji.

[^145]: Plany w USA na [Narodowy Zasób Badań AI](https://nairratdoe.ornl.gov/) i niedawne uruchomienie [Europejskiej Fundacji AI](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/) to interesujące kroki w tym kierunku.

[^146]: Wyzwanie tutaj nie jest techniczne, ale instytucjonalne – pilnie potrzebujemy rzeczywistych przykładów i eksperymentów tego, jak mogłby wyglądać rozwój AI w interesie publicznym.

[^147]: To idzie przeciw obecnym modelom biznesowym dużych technologii i wymagałoby zarówno działań prawnych, jak i nowych norm.

[^148]: Tylko niektóre rządy będą w stanie to robić. Bardziej radykalną ideą jest [uniwersalny fundusz tego typu, pod współwłasnością wszystkich ludzi.](https://futureoflife.org/project/the-windfall-trust/)

[^149]: Dla obszernego wywodu tego przypadku zobacz [ten artykuł](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338) o lojalności AI. Niestety domyślna trajektoria asystentów AI prawdopodobnie będzie tą, gdzie są coraz bardziej nielojalni.

[^150]: Nieco ironicznie, wiele władz obecnych też jest zagrożonych dezaktywacją wspieraną przez AI; ale może być trudno im to dostrzec, dopóki i chyba że proces nie zajdzie dość daleko.

[^151]: Niektóre interesujące wysiłki w tym kierunku są reprezentowane przez [koalicję c2pa](https://c2pa.org/) w kwestii weryfikacji kryptograficznej; [Verity](https://www.improvethenews.org/) i [Ground news](https://ground.news/) w kwestii lepszej epistemologii wiadomości; oraz [Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com) i rynki przewidywań w kwestii ugruntowania dyskursu w falsyfikowalnych przewidywaniach.

[^152]: Zobacz [ten](https://talktothecity.org/) fascynujący projekt pilotażowy.

[^153]: Zobacz [Kialo](https://www.kialo-edu.com/) i wysiłki [Projektu Inteligencji Zbiorowej](https://www.cip.org/) dla niektórych przykładów.

## Rozdział 10 - Wybór przed nami

Aby zachować naszą ludzką przyszłość, musimy zdecydować się zamknąć Bramy przed AGI i superinteligencją.

Ostatni raz ludzkość dzieliła Ziemię z innymi umysłami, które mówiły, myślały, budowały technologie i rozwiązywały problemy ogólnego zastosowania, to było 40 000 lat temu w lodowcowej Europie. Te inne umysły wyginęły, w całości lub częściowo z powodu działań naszych.

Teraz wkraczamy ponownie w taki okres. Najbardziej zaawansowane produkty naszej kultury i technologii – zbiory danych zbudowane z całego naszego internetowego dobra wspólnego oraz 100-miliardowe układy scalone będące najzłożonszymi technologiami, jakie kiedykolwiek stworzyliśmy – są łączone, aby powołać do życia zaawansowane systemy AI ogólnego przeznaczenia.

Twórcy tych systemów chętnie przedstawiają je jako narzędzia wzmacniające człowieka. I rzeczywiście mogłyby nimi być. Ale nie mylmy się: nasz obecny kierunek prowadzi do budowy coraz potężniejszych, zorientowanych na cele, podejmujących decyzje i ogólnie zdolnych cyfrowych agentów. Już teraz radzą sobie tak dobrze jak wielu ludzi w szerokim zakresie zadań intelektualnych, szybko się poprawiają i przyczyniają się do własnego rozwoju.

Jeśli ten kierunek się nie zmieni lub nie napotka nieoczekiwanej przeszkody, wkrótce – w ciągu lat, nie dekad – będziemy mieli cyfrowe inteligencje, które będą niebezpiecznie potężne. Nawet w *najlepszych* scenariuszach przyniosłyby one wielkie korzyści ekonomiczne (przynajmniej niektórym z nas), ale tylko kosztem głębokiego zakłócenia naszego społeczeństwa i zastąpienia ludzi w większości najważniejszych rzeczy, które robimy: te maszyny myślałyby za nas, planowałyby za nas, decydowałyby za nas i tworzyły za nas. Bylibyśmy rozpieszczeni, ale jak rozpieszczone dzieci. O wiele bardziej prawdopodobne jest, że te systemy zastąpiłyby ludzi zarówno w pozytywnych, *jak i* negatywnych rzeczach, które robimy, włączając w to wyzysk, manipulację, przemoc i wojnę. Czy możemy przetrwać wersje tych zjawisk napędzane przez AI? Wreszcie, więcej niż prawdopodobne jest, że sprawy wcale nie potoczyłyby się dobrze: że stosunkowo szybko zostalibyśmy zastąpieni nie tylko w tym, co robimy, ale w tym, czym *jesteśmy*, jako architekci cywilizacji i przyszłości. Zapytajcie neandertalczyków, jak to się kończy. Może przez jakiś czas dostarczaliśmy im też dodatkowe błyskotki.

*Nie musimy tego robić.* Mamy AI konkurencyjne z ludźmi i nie ma potrzeby budować AI, z którym *nie możemy* konkurować. Możemy budować niesamowite narzędzia AI bez tworzenia gatunku następcy. Przekonanie, że AGI i superinteligencja są nieuniknione, to *wybór udający przeznaczenie*.

Nakładając pewne twarde, globalne ograniczenia, możemy utrzymać ogólne zdolności AI na mniej więcej ludzkim poziomie, nadal czerpiąc korzyści ze zdolności komputerów do przetwarzania danych w sposób, w jaki my nie potrafimy, i automatyzacji zadań, których nikt z nas nie chce wykonywać. Nadal stwarzałyby one wiele zagrożeń, ale jeśli byłyby dobrze zaprojektowane i zarządzane, stanowiłyby ogromne dobrodziejstwo dla ludzkości, od medycyny przez badania po produkty konsumenckie.

Nałożenie ograniczeń wymagałoby międzynarodowej współpracy, ale mniej niż można by sądzić, a te ograniczenia nadal pozostawiałyby mnóstwo miejsca na ogromny przemysł AI i sprzętu AI skoncentrowany na zastosowaniach wzmacniających ludzkie dobro, a nie na surowym dążeniu do władzy. A jeśli, z silnymi gwarancjami bezpieczeństwa i po znaczącym globalnym dialogu, zdecydujemy się pójść dalej, ta opcja nadal będzie do naszej dyspozycji.

Ludzkość musi *wybrać* zamknięcie Bram przed AGI i superinteligencją.

Aby zachować przyszłość ludzką.

### Notatka od Autora

Dziękuję za poświęcenie czasu na eksplorację tego tematu z nami.

Napisałem ten esej, ponieważ jako naukowiec czuję, że ważne jest mówienie nieprzikraszanej prawdy, a jako człowiek uważam, że kluczowe jest szybkie i zdecydowane działanie w obliczu kwestii zmieniającej świat: rozwoju systemów AI inteligentniejszych niż człowiek.

Jeśli mamy odpowiedzieć mądrze na ten niezwykły stan rzeczy, musimy być gotowi do krytycznego zbadania dominującej narracji, że AGI i superinteligencja "muszą" zostać zbudowane dla zabezpieczenia naszych interesów, lub są "nieuniknione" i nie można ich powstrzymać. Te narracje pozbawiają nas możliwości działania, uniemożliwiając dostrzeżenie alternatywnych ścieżek przed nami.

Mam nadzieję, że dołączysz do mnie w wezwaniu do ostrożności w obliczu lekkomyślności i odwagi w obliczu chciwości.

Mam nadzieję, że dołączysz do mnie w wezwaniu do ludzkiej przyszłości.

*– Anthony*

![Anthony Aguirre signature](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAnthony-Aguirre-signature-300x84.png&w=3840&q=75)

## Aneksy

Informacje uzupełniające, w tym - szczegóły techniczne dotyczące rozliczania mocy obliczeniowej, przykład implementacji 'zamknięcia bram', szczegóły surowego reżimu odpowiedzialności za AGI oraz wielopoziomowe podejście do standardów bezpieczeństwa i ochrony AGI.

### Aneks A: Szczegóły techniczne rozliczania mocy obliczeniowej

Szczegółowa metoda zarówno dla "prawdy podstawowej" jak i dobrych przybliżeń całkowitej mocy obliczeniowej używanej w treningu i inferencji jest wymagana dla znaczących kontroli opartych na mocy obliczeniowej. Oto przykład jak "prawda podstawowa" mogłaby być zliczana na poziomie technicznym.

**Definicje:**

*Graf przyczynowy obliczeń:* Dla danego wyjścia O modelu AI, istnieje zestaw obliczeń cyfrowych, dla których zmiana wyniku tego obliczenia mogłaby potencjalnie zmienić O. (To powinno być konserwatywnie założone, tj. powinien istnieć jasny powód by wierzyć, że obliczenie jest niezależne od prekursora, który zarówno występuje wcześniej w czasie i ma fizyczną potencjalną ścieżkę przyczynowego oddziaływania.) To obejmuje obliczenia wykonane przez model AI podczas inferencji, jak również obliczenia, które weszły w skład danych wejściowych, przygotowania danych i treningu modelu. Ponieważ każde z nich może samo być wyjściem z modelu AI, jest to obliczane rekursywnie, odcinane tam gdzie człowiek dostarczył znaczącą zmianę do danych wejściowych.

*Moc obliczeniowa treningu:* Całkowita moc obliczeniowa, w FLOP lub innych jednostkach, zawarta w grafie przyczynowym obliczeń sieci neuronowej (włączając przygotowanie danych, trening i dostrajanie, oraz wszelkie inne obliczenia.)

*Moc obliczeniowa wyjścia:* Całkowita moc obliczeniowa w grafie przyczynowym obliczeń danego wyjścia AI, włączając wszystkie sieci neuronowe (i włączając ich Moc obliczeniową treningu) i inne obliczenia wchodzące w skład tego wyjścia.

*Tempo mocy obliczeniowej inferencji:* W serii wyjść, tempo zmian (w FLOP/s lub innych jednostkach) Mocy obliczeniowej wyjścia między wyjściami, tj. moc obliczeniowa użyta do wyprodukowania następnego wyjścia, podzielona przez zmierzony odstęp czasowy między wyjściami.

**Przykłady i przybliżenia:**

- Dla pojedynczej sieci neuronowej wytrenowanej na danych stworzonych przez ludzi, Moc obliczeniowa treningu to po prostu całkowita moc obliczeniowa treningu jak jest zwyczajowo raportowana.
- Dla takiej sieci neuronowej wykonującej inferencję w stałym tempie, Tempo mocy obliczeniowej inferencji to w przybliżeniu całkowita prędkość klastra obliczeniowego wykonującego inferencję w FLOP/s.
- Dla dostrajania modelu, Moc obliczeniowa treningu kompletnego modelu jest dana przez Moc obliczeniową treningu modelu nie-dostrajanego plus obliczenia wykonane podczas dostrajania i do przygotowania wszelkich danych używanych w dostrajaniu.
- Dla modelu destylowanego, Moc obliczeniowa treningu kompletnego modelu obejmuje trening zarówno modelu destylowanego jak i większego modelu użytego do dostarczenia syntetycznych danych lub innego wejścia treningowego.
- Jeśli kilka modeli jest trenowanych, ale wiele "prób" jest odrzucanych na podstawie ludzkiej oceny, te nie liczą się do Mocy obliczeniowej treningu lub wyjścia zachowanego modelu.

### Aneks B: Przykład implementacji zamknięcia bram

**Przykład implementacji:** Oto jeden przykład jak zamknięcie bram mogłoby działać, przy założeniu limitu 10<sup>27</sup> FLOP dla treningu i 10<sup>20</sup> FLOP/s dla inferencji (uruchamiania AI):

**1. Pauza:** Z powodów bezpieczeństwa narodowego, władza wykonawcza USA prosi wszystkie firmy z siedzibą w USA, prowadzące działalność w USA, lub używające chipów wyprodukowanych w USA, aby zaprzestały wszelkich nowych przebiegów treningu AI, które mogłyby przekroczyć limit 10<sup>27</sup> FLOP Mocy obliczeniowej treningu. USA powinny rozpocząć dyskusje z innymi krajami goszczącymi rozwój AI, silnie zachęcając je do podjęcia podobnych kroków i wskazując, że pauza USA może zostać zniesiona, gdyby zdecydowały się nie zastosować.

**2. Nadzór i licencjonowanie USA:** Poprzez zarządzenie wykonawcze lub działanie istniejącej agencji regulacyjnej, USA wymagają, aby w ciągu (powiedzmy) jednego roku:

- Wszystkie przebiegi treningu AI szacowane powyżej 10<sup>25</sup> FLOP wykonane przez firmy działające w USA były zarejestrowane w bazie danych prowadzonej przez amerykańską agencję regulacyjną. (Uwaga: Nieco słabsza wersja tego była już zawarta w anulowanym zarządzeniu wykonawczym USA z 2023 roku dotyczącym AI, wymagającym rejestracji dla modeli powyżej 10<sup>26</sup> FLOP.)
- Wszyscy producenci sprzętu związanego z AI działający w USA lub prowadzący interesy z rządem USA przestrzegali zestawu wymagań dotyczących ich specjalistycznego sprzętu i oprogramowania go sterującego. (Wiele z tych wymagań mogłoby zostać wbudowanych w aktualizacje oprogramowania i firmware dla istniejącego sprzętu, ale długoterminowe i solidne rozwiązania wymagałyby zmian w późniejszych generacjach sprzętu.) Wśród nich jest wymaganie, że jeśli sprzęt jest częścią szybko połączonego klastra zdolnego do wykonywania 10<sup>18</sup> FLOP/s obliczeń, wymagany jest wyższy poziom weryfikacji, który obejmuje regularne pozwolenie przez zdalny "gubernator", który otrzymuje zarówno telemetrię jak i prośby o wykonanie dodatkowych obliczeń.
- Zarządca raportuje całkowite obliczenia wykonane na jego sprzęcie do agencji prowadzącej amerykańską bazę danych.
- Silniejsze wymagania są wprowadzane fazowo, aby umożliwić zarówno bezpieczniejszy jak i bardziej elastyczny nadzór i pozwolenia.

**3. Nadzór międzynarodowy:**

- USA, Chiny i wszelkie inne kraje goszczące zaawansowane zdolności produkcji chipów negocjują międzynarodowe porozumienie.
- To porozumienie tworzy nową międzynarodową agencję, analogiczną do Międzynarodowej Agencji Energii Atomowej, odpowiedzialną za nadzór treningu i wykonywania AI.
- Kraje sygnatariusze muszą wymagać od swoich krajowych producentów sprzętu AI przestrzegania zestawu wymagań co najmniej tak silnych jak te nałożone w USA.
- Zarządcy są teraz zobowiązani do raportowania liczb obliczeń AI zarówno do agencji w swoich krajach macierzystych jak i do nowego biura w międzynarodowej agencji.
- Dodatkowe kraje są silnie zachęcane do przyłączenia się do istniejącego międzynarodowego porozumienia: kontrole eksportu przez kraje sygnatariusze ograniczają dostęp do zaawansowanego sprzętu przez nie-sygnatariuszy, podczas gdy sygnatariusze mogą otrzymać wsparcie techniczne w zarządzaniu swoimi systemami AI.

**4. Międzynarodowa weryfikacja i egzekwowanie:**

- System weryfikacji sprzętu jest aktualizowany tak, aby raportował użycie obliczeń zarówno do oryginalnego zarządcy jak i bezpośrednio do biura międzynarodowej agencji.
- Agencja, poprzez dyskusję z sygnatariuszami międzynarodowego porozumienia, uzgadnia ograniczenia obliczeniowe, które następnie nabierają mocy prawnej w krajach sygnatariuszy.
- Równolegle, zestaw międzynarodowych standardów może zostać opracowany tak, aby trening i uruchamianie AI powyżej progu obliczeń (ale poniżej limitu) były zobowiązane do przestrzegania tych standardów.
- Agencja może, jeśli konieczne dla kompensacji lepszych algorytmów itp., obniżyć limit obliczeń. Lub, jeśli zostanie uznane za bezpieczne i wskazane (na poziomie dowodliwych gwarancji bezpieczeństwa), podnieść limit obliczeń.

### Aneks C: Szczegóły surowego reżimu odpowiedzialności za AGI

**Szczegóły surowego reżimu odpowiedzialności za AGI**

- Tworzenie i działanie zaawansowanego systemu AI, który jest wysoce ogólny, zdolny i autonomiczny, jest uważane za działalność "nienormalnie niebezpieczną".
- W związku z tym, domyślna odpowiedzialność za trening i działanie takich systemów to surowa, solidarna odpowiedzialność (lub jej nie-amerykański odpowiednik) za wszelkie szkody wyrządzone przez model lub jego wyjścia/działania.
- Odpowiedzialność osobista będzie nałożona na kadry kierownicze i członków zarządu w przypadkach rażącego zaniedbania lub umyślnego wykroczenia. To powinno obejmować kary kryminalne dla najbardziej rażących przypadków.
- Istnieją liczne bezpieczne przystanie, pod którymi odpowiedzialność wraca do domyślnej (opartej na winie, w USA) odpowiedzialności, której ludzie i firmy byliby normalnie poddani.
	- Modele trenowane i działające poniżej pewnego progu obliczeniowego (który byłby co najmniej 10x niższy niż limity opisane powyżej.)
	- AI, które jest "słabe" (z grubsza, poniżej poziomu ludzkiego eksperta w zadaniach, do których jest przeznaczone) i/lub
	- AI, które jest "wąskie" (mające ustalony i dość ograniczony zakres zadań i operacji, do których jest specjalnie zaprojektowane i wytrenowane) i/lub
	- AI, które jest "pasywne" (bardzo ograniczone w swojej zdolności – nawet pod skromną modyfikacją – do podejmowania działań lub wykonywania złożonych wieloetapowych zadań bez bezpośredniego ludzkiego zaangażowania i kontroli.)
	- AI, które jest gwarantowane jako bezpieczne, zabezpieczone i kontrolowalne (dowodliwie bezpieczne, lub analiza ryzyka wskazuje na znikomy poziom oczekiwanej szkody.)
- Bezpieczne przystanie mogą być zgłaszane na podstawie [przypadku bezpieczeństwa](https://arxiv.org/abs/2410.21572) przygotowanego przez dewelopera AI i zatwierdzonego przez agencję lub audytora akredytowanego przez agencję. Aby zgłosić bezpieczne przystanie oparte na obliczeniach, deweloper musi tylko dostarczyć wiarygodne oszacowania całkowitej Mocy obliczeniowej treningu i maksymalnego Tempa inferencji
- Prawodawstwo wyraźnie określiłoby sytuacje, w których nakaz sądowy zaprzestania rozwoju systemów AI z wysokim ryzykiem szkody publicznej byłby odpowiedni.
- Konsorcja firm, współpracujące z organizacjami pozarządowymi i agencjami rządowymi, powinny opracować standardy i normy definiujące te terminy, jak regulatorzy powinni przyznawać bezpieczne przystanie, jak deweloperzy AI powinni rozwijać przypadki bezpieczeństwa, i jak sądy powinny interpretować odpowiedzialność tam gdzie bezpieczne przystanie nie są proaktywnie zgłaszane.

### Aneks D: Wielopoziomowe podejście do standardów bezpieczeństwa i ochrony AGI

**Wielopoziomowe podejście do standardów bezpieczeństwa i ochrony AGI**

| Poziom ryzyka | Wyzwalacz(e) | Wymagania dla treningu | Wymaganie dla wdrożenia |
| --- | --- | --- | --- |
| PR-0 | AI słabe w autonomii, ogólności i inteligencji | brak | brak |
| PR-1 | AI silne w jednym z: autonomii, ogólności i inteligencji | brak | W oparciu o ryzyko i użycie, potencjalnie przypadki bezpieczeństwa zatwierdzone przez władze narodowe wszędzie tam, gdzie model może być używany |
| PR-2 | AI silne w dwóch z: autonomii, ogólności i inteligencji | Rejestracja u władz narodowych mających jurysdykcję nad deweloperem | Przypadek bezpieczeństwa ograniczający ryzyko większej szkody poniżej autoryzowanych poziomów plus niezależne audyty bezpieczeństwa (włączając redteaming czarnej i białej skrzynki) zatwierdzone przez władze narodowe wszędzie tam, gdzie model może być używany |
| PR-3 | AGI silne w autonomii, ogólności i inteligencji | Wstępne zatwierdzenie planu bezpieczeństwa i ochrony przez władze narodowe mające jurysdykcję nad deweloperem | Przypadek bezpieczeństwa gwarantujący ograniczone ryzyko większej szkody poniżej autoryzowanych poziomów oraz wymagane specyfikacje, włączając cyberbezpieczeństwo, kontrolowalność, nieusuwalny wyłącznik awaryjny, wyrównanie z ludzkimi wartościami i odporność na złośliwe użycie. |
| PR-4 | Jakikolwiek model, który również przekracza 10<sup>27</sup> FLOP Treningu lub 10<sup>20</sup> FLOP/s Inferencji | Zakazane do czasu międzynarodowo uzgodnionego zniesienia limitu obliczeniowego | Zakazane do czasu międzynarodowo uzgodnionego zniesienia limitu obliczeniowego |

Klasyfikacje ryzyka i standardy bezpieczeństwa/ochrony, z poziomami opartymi na progach obliczeniowych jak również kombinacjach wysokiej autonomii, ogólności i inteligencji:

- *Silna autonomia* ma zastosowanie jeśli system jest zdolny do wykonywania, lub może być łatwo przystosowany do wykonywania, wieloetapowych zadań i/lub podejmowania złożonych działań, które są istotne w świecie rzeczywistym, bez znaczącego ludzkiego nadzoru lub interwencji. Przykłady: autonomiczne pojazdy i roboty; boty do handlu finansowego. Nie-przykłady: GPT-4; klasyfikatory obrazów
- *Silna ogólność* wskazuje szeroki zakres zastosowań, wykonywanie zadań, do których model nie był celowo i specjalnie trenowany, i znaczącą zdolność uczenia się nowych zadań. Przykłady: GPT-4; mu-zero. Nie-przykłady: AlphaFold; autonomiczne pojazdy; generatory obrazów
- *Silna inteligencja* odpowiada dorównywaniu ludzkiemu poziomowi eksperta w zadaniach, w których model działa najlepiej (a dla modelu ogólnego, w szerokim zakresie zadań.) Przykłady: AlphaFold; mu-zero; o3. Nie-przykłady: GPT-4; Siri

### Podziękowania

Kilka podziękowań dla osób, które przyczyniły się do powstania "Zachowaj Przyszłość dla Ludzkości".

Ta praca odzwierciedla opinie autora i nie powinna być traktowana jako oficjalne stanowisko Future of Life Institute (choć są z nim zgodne; oficjalne stanowisko można znaleźć na [tej stronie](https://futureoflife.org/our-position-on-ai/)) ani żadnej innej organizacji, z którą autor jest związany.

Jestem wdzięczny ludziom Mark Brakel, Ben Eisenpress, Anna Hehir, Carlos Gutierrez, Emilia Javorsky, Richard Mallah, Jordan Scharnhorst, Elyse Fulcher, Max Tegmark i Jaan Tallinn za komentarze do manuskryptu; Tim Schrier za pomoc z niektórymi odniesieniami; Taylor Jones i Elyse Fulcher za upiększenie diagramów.

Ta praca w ograniczonym zakresie wykorzystywała generatywne modele AI (Claude i ChatGPT) w procesie tworzenia, do części redakcji i testowania przeciwstawnych argumentów. Według ugruntowanego standardu poziomów zaangażowania AI w dzieła kreatywne, ta praca prawdopodobnie otrzymałaby ocenę 3/10. (W rzeczywistości taki standard nie istnieje! Ale powinien.)

Jesteśmy bardzo wdzięczni [Julius Odai](https://www.linkedin.com/in/julius-odai/) za stworzenie tej internetowej wersji eseju, która sprawia, że czytanie i poruszanie się po tekście to bardzo przyjemne doświadczenie. Julius to technolog i niedawny uczestnik kursu BlueDot Impact AI Governance.