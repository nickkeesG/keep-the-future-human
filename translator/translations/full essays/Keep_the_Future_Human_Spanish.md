# Mantengamos el Futuro Humano

Este ensayo presenta los argumentos sobre por qué y cómo deberíamos cerrar las puertas a la IAG y la superinteligencia, y qué deberíamos construir en su lugar.

Si solo quieres conocer las conclusiones principales, ve al Resumen ejecutivo. Después, los Capítulos 2-5 proporcionarán contexto sobre los tipos de sistemas de IA que se discuten en el ensayo. Los Capítulos 5-7 explican por qué podríamos esperar que la IAG llegue pronto, y qué podría suceder cuando eso ocurra. Finalmente, los Capítulos 8-9 presentan una propuesta concreta para evitar que se construya la IAG.

[Descargar PDF](https://keepthefuturehuman.ai/wp-content/uploads/2025/03/Keep_the_Future_Human__AnthonyAguirre__5March2025.pdf)

Tiempo total de lectura: 2-3 horas

## Resumen ejecutivo

Una descripción general de alto nivel del ensayo. Si tienes poco tiempo, obtén todos los puntos principales en solo 10 minutos.

Los avances dramáticos en inteligencia artificial durante la última década (para IA de propósito específico) y los últimos años (para IA de propósito general) han transformado la IA de un campo académico de nicho a la estrategia comercial central de muchas de las empresas más grandes del mundo, con cientos de miles de millones de dólares en inversión anual en las técnicas y tecnologías para hacer avanzar las capacidades de la IA.

Ahora llegamos a un momento crítico. Mientras las capacidades de los nuevos sistemas de IA comienzan a igualar y superar las de los humanos en muchos dominios cognitivos, la humanidad debe decidir: ¿qué tan lejos llegamos, y en qué dirección?

La IA, como toda tecnología, comenzó con el objetivo de mejorar las cosas para su creador. Pero nuestra trayectoria actual, y elección implícita, es una carrera descontrolada hacia sistemas cada vez más poderosos, impulsada por los incentivos económicos de unas pocas empresas tecnológicas gigantescas que buscan automatizar grandes sectores de la actividad económica actual y el trabajo humano. Si esta carrera continúa mucho más tiempo, hay un ganador inevitable: la IA misma—una alternativa más rápida, más inteligente y más barata que las personas en nuestra economía, nuestro pensamiento, nuestras decisiones, y eventualmente en el control de nuestra civilización.

Pero podemos hacer otra elección: a través de nuestros gobiernos, podemos tomar control del proceso de desarrollo de IA para imponer límites claros, líneas que no cruzaremos, y cosas que simplemente no haremos—como hemos hecho con las tecnologías nucleares, armas de destrucción masiva, armas espaciales, procesos ambientalmente destructivos, la bioingeniería de humanos y la eugenesia. Más importante aún, podemos asegurar que la IA permanezca como una herramienta para empoderar a los humanos, en lugar de una nueva especie que nos reemplace y eventualmente nos suplante.

Este ensayo argumenta que deberíamos *mantener el futuro humano* cerrando las "Puertas" a la IA autónoma de propósito general más inteligente que los humanos—a veces llamada "IAG"—y especialmente a la versión altamente superhumana a veces llamada "superinteligencia." En cambio, deberíamos enfocarnos en herramientas de IA poderosas y confiables que puedan empoderar a los individuos y mejorar transformativamente las capacidades de las sociedades humanas para hacer lo que mejor hacen. La estructura de este argumento sigue brevemente.

### La IA es diferente

Los sistemas de IA son fundamentalmente diferentes de otras tecnologías. Mientras el software tradicional sigue instrucciones precisas, los sistemas de IA aprenden cómo lograr objetivos sin que se les diga explícitamente cómo. Esto los hace poderosos: si podemos definir claramente el objetivo o una métrica de éxito, en la mayoría de los casos un sistema de IA puede aprender a lograrlo. Pero también los hace inherentemente impredecibles: no podemos determinar de manera confiable qué acciones tomarán para lograr sus objetivos.

También son en gran medida inexplicables: aunque son parcialmente código, son principalmente un enorme conjunto de números inescrutables—"pesos" de redes neuronales—que no pueden ser analizados; no somos mucho mejores entendiendo su funcionamiento interno que discerniendo pensamientos al mirar dentro de un cerebro biológico.

Este modo central de entrenamiento de redes neuronales digitales está aumentando rápidamente en complejidad. Los sistemas de IA más poderosos se crean a través de experimentos computacionales masivos, usando hardware especializado para entrenar redes neuronales en conjuntos de datos enormes, que luego se aumentan con herramientas de software y superestructura.

Esto ha llevado a la creación de herramientas muy poderosas para crear y procesar texto e imágenes, realizar razonamiento matemático y científico, agregar información, y consultar interactivamente un vasto almacén de conocimiento humano.

Desafortunadamente, mientras el desarrollo de herramientas tecnológicas más poderosas y más confiables es lo que *deberíamos* hacer, y lo que casi todo el mundo quiere y dice querer, no es la trayectoria en la que realmente estamos.

### IAG y superinteligencia

Desde los albores del campo, la investigación en IA se ha enfocado en cambio en un objetivo diferente: la Inteligencia Artificial General. Este enfoque ahora se ha convertido en el foco de las empresas titánicas que lideran el desarrollo de IA.

¿Qué es la IAG? A menudo se define vagamente como "IA a nivel humano," pero esto es problemático: ¿qué humanos, y en qué capacidades está a nivel humano? ¿Y qué hay de las capacidades superhumanas que ya tiene? Una forma más útil de entender la IAG es a través de la intersección de tres propiedades clave: alta **A**utonomía (independencia de acción), alta **G**eneralidad (amplio alcance y adaptabilidad), y alta **I**nteligencia (competencia en tareas cognitivas). Los sistemas de IA actuales pueden ser altamente capaces pero estrechos, o generales pero requieren supervisión humana constante, o autónomos pero limitados en alcance.

La A-G-I completa combinaría las tres propiedades en niveles que igualen o superen la capacidad humana superior. Críticamente, es esta combinación lo que hace a los humanos tan efectivos y tan diferentes del software actual; es también lo que permitiría que las personas sean reemplazadas completamente por sistemas digitales.

Aunque la inteligencia humana es especial, de ninguna manera es un límite. Los sistemas artificiales "superinteligentes" podrían operar cientos de veces más rápido, procesar vastamente más datos y mantener enormes cantidades "en mente" a la vez, y formar agregados que son mucho más grandes y más efectivos que colecciones de humanos. Podrían suplantar no a individuos sino a empresas, naciones, o nuestra civilización como un todo.

### Estamos en el umbral

Hay un fuerte consenso científico de que la IAG es *posible*. La IA ya supera el rendimiento humano en muchas pruebas generales de capacidad intelectual, incluyendo recientemente el razonamiento de alto nivel y la resolución de problemas. Las capacidades rezagadas—como el aprendizaje continuo, la planificación, la autoconciencia y la originalidad—todas existen en algún nivel en los sistemas de IA actuales, y existen técnicas conocidas que probablemente mejorarán todas ellas.

Mientras que hasta hace pocos años muchos investigadores veían la IAG como algo que estaba a décadas de distancia, actualmente la evidencia de cronogramas cortos hacia la IAG es fuerte:

- Las "leyes de escalamiento" empíricamente verificadas conectan la entrada computacional con la capacidad de IA, y las corporaciones están en camino de escalar la entrada computacional por órdenes de magnitud durante los próximos varios años. Los recursos humanos y fiscales dedicados al avance de la IA ahora igualan los de una docena de Proyectos Manhattan y varios Proyectos Apolo.
- Las corporaciones de IA y sus líderes creen pública y privadamente que la IAG (por alguna definición) es alcanzable dentro de unos pocos años. Estas empresas tienen información que el público no tiene, incluyendo algunas que tienen la próxima generación de sistemas de IA en sus manos.
- Los predictores expertos con historiales comprobados asignan 25% de probabilidad a que la IAG (por alguna definición) llegue dentro de 1-2 años, y 50% para 2-5 años (ver las predicciones de Metaculus para IAG ['débil'](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) y ['completa'](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/)).
- La autonomía (incluyendo planificación flexible de largo alcance) está rezagada en los sistemas de IA, pero las grandes empresas ahora están enfocando sus vastos recursos en desarrollar sistemas de IA autónomos y han nombrado informalmente a 2025 el ["año del agente."](https://techinformed.com/2025-informed-the-year-of-agentic-ai/)
- La IA está contribuyendo cada vez más a su propia mejora. Una vez que los sistemas de IA sean tan competentes como los investigadores humanos de IA en hacer investigación de IA, se alcanzará un umbral crítico para el progreso rápido hacia sistemas de IA mucho más poderosos y probablemente conducirá a una escalada descontrolada en la capacidad de IA. (Se podría argumentar que esa escalada ya ha comenzado.)

La idea de que la IAG más inteligente que los humanos está a décadas de distancia o más simplemente ya no es sostenible para la gran mayoría de expertos en el campo. Los desacuerdos ahora son sobre cuántos meses o años tomará si nos mantenemos en este curso. La pregunta central que enfrentamos es: ¿deberíamos?

### Qué está impulsando la carrera hacia la IAG

La carrera hacia la IAG está siendo impulsada por múltiples fuerzas, cada una haciendo la situación más peligrosa. Las grandes empresas tecnológicas ven la IAG como la tecnología de automatización definitiva—no solo aumentando a los trabajadores humanos sino reemplazándolos en gran medida o completamente. Para las empresas, el premio es enorme: la oportunidad de capturar una fracción significativa de los $100 billones de producción económica anual mundial automatizando los costos de trabajo humano.

Las naciones se sienten obligadas a unirse a esta carrera, citando públicamente liderazgo económico y científico, pero viendo privadamente la IAG como una revolución potencial en asuntos militares comparable a las armas nucleares. El miedo de que los rivales puedan obtener una ventaja estratégica decisiva crea una dinámica clásica de carrera armamentista.

Aquellos que persiguen la superinteligencia a menudo citan visiones grandiosas: curar todas las enfermedades, revertir el envejecimiento, lograr avances en energía y viajes espaciales, o crear capacidades de planificación superhumanas.

Menos caritativamente, lo que impulsa la carrera es el poder. Cada participante—ya sea empresa o país—cree que la inteligencia es igual a poder, y que ellos serán los mejores custodios de ese poder.

Argumento que estas motivaciones son reales pero fundamentalmente equivocadas: la IAG *absorberá* y *buscará* poder en lugar de otorgarlo; las tecnologías creadas por IA *también* serán fuertemente de doble filo, y donde sean beneficiosas pueden ser creadas con herramientas de IA y sin IAG; e incluso en la medida en que la IAG y sus productos permanezcan bajo control, estas dinámicas de carrera—tanto corporativas como geopolíticas—hacen que los riesgos de gran escala para nuestra sociedad sean casi inevitables a menos que sean interrumpidos decisivamente.

### La IAG y la superinteligencia representan una amenaza dramática para la civilización

A pesar de su atractivo, la IAG y la superinteligencia representan amenazas dramáticas para la civilización a través de múltiples vías que se refuerzan mutuamente:

*Concentración de poder:* la IA superhumana podría desempoderar a la gran mayoría de la humanidad absorbiendo enormes sectores de la actividad social y económica en sistemas de IA dirigidos por un puñado de empresas gigantes (que a su vez pueden ser tomadas por, o efectivamente tomar control de, gobiernos).

*Disrupción masiva:* la automatización masiva de la mayoría de trabajos basados en cognición, el reemplazo de nuestros sistemas epistémicos actuales, y el despliegue de vastos números de agentes activos no humanos trastornaría la mayoría de nuestros sistemas civilizatorios actuales en un período de tiempo relativamente corto.

*Catástrofes:* al proliferar la capacidad—potencialmente por encima del nivel humano—de crear nuevas tecnologías militares y destructivas y desacoplarla de los sistemas sociales y legales que fundamentan la responsabilidad, las catástrofes físicas por armas de destrucción masiva se vuelven dramáticamente más probables.

*Geopolítica y guerra:* las grandes potencias mundiales no se quedarán de brazos cruzados si sienten que una tecnología que podría proporcionar una "ventaja estratégica decisiva" está siendo desarrollada por sus adversarios.

*Escalada descontrolada y pérdida de control:* A menos que se prevenga específicamente, la IA superhumana tendrá todos los incentivos para mejorarse aún más y podría superar por mucho a los humanos en velocidad, procesamiento de datos y sofisticación de pensamiento. No hay manera significativa en que podamos tener control de tal sistema. Tal IA no otorgará poder a los humanos; nosotros le otorgaremos poder a ella, o ella lo tomará.

Muchos de estos riesgos permanecen incluso si se resuelve el problema técnico de "alineación"—asegurar que la IA avanzada haga de manera confiable lo que los humanos quieren que haga. La IA presenta un enorme desafío en cómo será gestionada, y muchos aspectos de esta gestión se vuelven increíblemente difíciles o intratables cuando se supera la inteligencia humana.

Más fundamentalmente, el tipo de IA superhumana de propósito general que actualmente se está persiguiendo tendría, por su misma naturaleza, objetivos, agencia y capacidades que superan las nuestras. Sería inherentemente incontrolable—¿cómo podemos controlar algo que no podemos ni entender ni predecir? No sería una herramienta tecnológica para uso humano, sino una segunda especie de inteligencia en la Tierra junto a la nuestra. Si se le permite progresar más, constituiría no solo una segunda especie sino una especie de reemplazo.

Quizás nos trataría bien, quizás no. Pero el futuro le pertenecería a ella, no a nosotros. La era humana habría terminado.

### Esto no es inevitable; la humanidad puede, muy concretamente, decidir no construir nuestro reemplazo.

La creación de IAG superhumana está lejos de ser inevitable. Podemos prevenirla a través de un conjunto coordinado de medidas de gobernanza:

Primero, necesitamos contabilidad robusta y supervisión del cómputo de IA ("cómputo"), que es un facilitador fundamental y una palanca para gobernar los sistemas de IA de gran escala. Esto a su vez requiere medición estandarizada y reporte del cómputo total usado en entrenar modelos de IA y ejecutarlos, y métodos técnicos para contar, certificar y verificar la computación utilizada.

Segundo, deberíamos implementar límites máximos estrictos en el cómputo de IA, tanto para entrenamiento como para operación; estos previenen que la IA sea demasiado poderosa y opere demasiado rápido. Estos límites pueden implementarse a través de requisitos legales y medidas de seguridad basadas en hardware construidas en chips especializados para IA, análogas a las características de seguridad en los teléfonos modernos. Porque el hardware especializado de IA es hecho por solo un puñado de empresas, la verificación y aplicación son factibles a través de la cadena de suministro existente.

Tercero, necesitamos responsabilidad legal mejorada para los sistemas de IA más peligrosos. Aquellos que desarrollan IA que combina alta autonomía, amplia generalidad e inteligencia superior deberían enfrentar responsabilidad estricta por daños, mientras que refugios seguros de esta responsabilidad alentarían el desarrollo de sistemas más limitados y controlables.

Cuarto, necesitamos regulación escalonada basada en niveles de riesgo. Los sistemas más capaces y peligrosos requerirían garantías extensas de seguridad y controlabilidad antes del desarrollo y despliegue, mientras que sistemas menos poderosos o más especializados enfrentarían supervisión proporcional. Este marco regulatorio debería eventualmente operar tanto a niveles nacionales como internacionales.

Este enfoque—con especificación detallada dada en el documento completo—es práctico: mientras se necesitará coordinación internacional, la verificación y aplicación pueden funcionar a través del pequeño número de empresas que controlan la cadena de suministro de hardware especializado. También es flexible: las empresas aún pueden innovar y obtener ganancias del desarrollo de IA, solo con límites claros en los sistemas más peligrosos.

La contención a largo plazo del poder y riesgo de IA requeriría acuerdos internacionales basados tanto en interés propio como común, tal como controlar la proliferación de armas nucleares lo hace ahora. Pero podemos comenzar inmediatamente con supervisión mejorada y responsabilidad legal, mientras construimos hacia una gobernanza más comprensiva.

El ingrediente clave que falta es la voluntad política y social para tomar control del proceso de desarrollo de IA. La fuente de esa voluntad, si llega a tiempo, será la realidad misma—es decir, de la realización generalizada de las implicaciones reales de lo que estamos haciendo.

### Podemos diseñar IA Herramienta para empoderar a la humanidad

En lugar de perseguir IAG incontrolable, podemos desarrollar "IA Herramienta" poderosa que mejore la capacidad humana mientras permanece bajo control humano significativo. Los sistemas de IA Herramienta pueden ser extremadamente capaces mientras evitan la peligrosa triple-intersección de alta autonomía, amplia generalidad e inteligencia superhumana, siempre y cuando los diseñemos para ser controlables a un nivel proporcional a su capacidad. También pueden combinarse en sistemas sofisticados que mantengan la supervisión humana mientras entregan beneficios transformativos.

La IA Herramienta puede revolucionar la medicina, acelerar el descubrimiento científico, mejorar la educación y mejorar los procesos democráticos. Cuando se gobierna apropiadamente, puede hacer que los expertos humanos e instituciones sean más efectivos en lugar de reemplazarlos. Aunque tales sistemas seguirán siendo altamente disruptivos y requerirán gestión cuidadosa, los riesgos que representan son fundamentalmente diferentes de la IAG: son riesgos que podemos gobernar, como los de otras tecnologías poderosas, no amenazas existenciales a la agencia humana y la civilización. Y críticamente, cuando se desarrolla sabiamente, las herramientas de IA pueden ayudar a las personas a gobernar la IA poderosa y gestionar sus efectos.

Este enfoque requiere repensar tanto cómo se desarrolla la IA como cómo se distribuyen sus beneficios. Nuevos modelos de desarrollo de IA público y sin fines de lucro, marcos regulatorios robustos, y mecanismos para distribuir beneficios económicos más ampliamente pueden ayudar a asegurar que la IA empodere a la humanidad como un todo en lugar de concentrar poder en unas pocas manos. La IA misma puede ayudar a construir mejores instituciones sociales y de gobernanza, habilitando nuevas formas de coordinación y discurso que fortalezcan en lugar de socavar la sociedad humana. Los establecimientos de seguridad nacional pueden aprovechar su experiencia para hacer que los sistemas de herramientas de IA sean genuinamente seguros y confiables, y una verdadera fuente de defensa así como de poder nacional.

Eventualmente podríamos elegir desarrollar sistemas aún más poderosos y más soberanos que sean menos como herramientas y—podemos esperar—más como benefactores sabios y poderosos. Pero deberíamos hacerlo solo después de que hayamos desarrollado la comprensión científica y la capacidad de gobernanza para hacerlo de manera segura. Tal decisión trascendental e irreversible debería ser hecha deliberadamente por la humanidad como un todo, no por defecto en una carrera entre empresas tecnológicas y naciones.

### En manos humanas

Las personas quieren lo bueno que viene de la IA: herramientas útiles que las empoderen, potencien oportunidades económicas y crecimiento, y prometan avances en ciencia, tecnología y educación. ¿Por qué no lo querrían? Pero cuando se les pregunta, mayorías abrumadoras del público general [quieren desarrollo de IA más lento y más cuidadoso](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation), y no quieren IA más inteligente que los humanos que los reemplazará en sus trabajos y en otros lugares, llenará su cultura y espacios comunes de información con contenido no humano, concentrará poder en un pequeño conjunto de corporaciones, representará riesgos extremos de gran escala global, y eventualmente amenazará con desempoderar o reemplazar a su especie. ¿Por qué lo querrían?

*Podemos* tener uno sin el otro. Comienza decidiendo que nuestro destino no está en la supuesta inevitabilidad de alguna tecnología o en las manos de unos pocos CEOs en Silicon Valley, sino en el resto de nuestras manos si nos hacemos cargo de él. Cerremos las Puertas, y mantengamos el futuro humano.

## Capítulo 1 - Introducción

Cómo responderemos a la perspectiva de una IA más inteligente que los humanos es el tema más urgente de nuestro tiempo. Este ensayo proporciona un camino a seguir.

Podríamos estar al final de la era humana.

Algo comenzó en los últimos diez años que es único en la historia de nuestra especie. Sus consecuencias determinarán, en gran medida, el futuro de la humanidad. A partir de aproximadamente 2015, los investigadores han logrado desarrollar inteligencia artificial (IA) *especializada* – sistemas que pueden ganar en juegos como el Go, reconocer imágenes y voz, y más, mejor que cualquier humano.[^1]

Este es un éxito extraordinario, y está produciendo sistemas y productos extremadamente útiles que empoderarán a la humanidad. Pero la inteligencia artificial especializada nunca ha sido el verdadero objetivo del campo. Más bien, la meta ha sido crear sistemas de IA de propósito *general*, particularmente aquellos que a menudo se llaman "inteligencia artificial general" (IAG) o "superinteligencia" que son simultáneamente tan buenos o mejores que los humanos en casi *todas* las tareas, así como la IA ahora es sobrehumana en Go, ajedrez, póker, carreras de drones, etc. Este es el objetivo declarado de muchas compañías importantes de IA.[^2]

*Estos esfuerzos también están teniendo éxito.* Los sistemas de IA de propósito general como ChatGPT, Gemini, Llama, Grok, Claude y Deepseek, basados en cómputos masivos y montañas de datos, han alcanzado la paridad con humanos típicos en una amplia variedad de tareas, e incluso igualan a expertos humanos en algunos dominios. Ahora los ingenieros de IA de algunas de las compañías tecnológicas más grandes compiten para impulsar estos experimentos gigantes en inteligencia artificial hacia los siguientes niveles, en los que igualan y luego superan todo el rango de capacidades, experiencia y autonomía humanas.

*Esto es inminente.* Durante los últimos diez años, las estimaciones de expertos sobre cuánto tiempo tomará esto – si continuamos nuestro curso actual – han caído de décadas (o siglos) a años de un solo dígito.

También es de importancia trascendental y de riesgo supremo. Los proponentes de la IAG la ven como una transformación positiva que resolverá problemas científicos, curará enfermedades, desarrollará nuevas tecnologías y automatizará el trabajo tedioso. Y la IA ciertamente podría ayudar a lograr todas estas cosas – de hecho ya lo está haciendo. Pero a lo largo de las décadas, muchos pensadores cuidadosos, desde Alan Turing hasta Stephen Hawking y los actuales Geoffrey Hinton y Yoshua Bengio [^3] han emitido una advertencia severa: construir IA verdaderamente más inteligente que los humanos, general y autónoma, como mínimo trastornará completa e irrevocablemente a la sociedad, y como máximo resultará en la extinción humana.[^4]

La IA superinteligente se aproxima rápidamente en nuestro camino actual, pero está lejos de ser inevitable. Este ensayo es un argumento extendido sobre por qué y cómo deberíamos *cerrar las Puertas* a este futuro inhumano que se aproxima, y qué deberíamos hacer en su lugar.

[^1]: Este [gráfico](https://time.com/6300942/ai-progress-charts/) muestra un conjunto de tareas; muchas curvas similares podrían agregarse a esta gráfica. Este rápido progreso en IA especializada ha sorprendido incluso a expertos en el campo, con puntos de referencia siendo superados años antes de las predicciones.

[^2]: Deepmind, OpenAI, Anthropic y X.ai fueron todas fundadas con el objetivo específico de desarrollar IAG. Por ejemplo, la carta de OpenAI establece explícitamente su objetivo como desarrollar "inteligencia artificial general que beneficie a toda la humanidad," mientras que la misión de DeepMind es "resolver la inteligencia, y luego usar eso para resolver todo lo demás." Meta, Microsoft y otros ahora persiguen caminos sustancialmente similares. Meta ha dicho que [planea desarrollar IAG y lanzarla abiertamente.](https://www.forbes.com/sites/johnkoetsier/2024/01/18/zuckerberg-on-ai-meta-building-agi-for-everyone-and-open-sourcing-it/)

[^3]: Hinton y Bengio son dos de los investigadores de IA más citados, ambos han ganado el Nobel de la IA, el Premio Turing, y Hinton además ha ganado un Premio Nobel (en física).

[^4]: Construir algo de este riesgo, bajo incentivos comerciales y supervisión gubernamental casi nula, es completamente sin precedentes. ¡Ni siquiera hay controversia sobre el riesgo entre quienes lo están construyendo! Los líderes de Deepmind, OpenAI y Anthropic, entre muchos otros expertos, han firmado literalmente una [declaración](https://www.safe.ai/work/statement-on-ai-risk) que dice que la IA avanzada presenta un *riesgo de extinción para la humanidad.* Las alarmas no podrían estar sonando más fuerte, y uno solo puede concluir que quienes las ignoran simplemente no se están tomando en serio la IAG y la superinteligencia. Un objetivo de este ensayo es ayudarlos a entender por qué deberían hacerlo.

## Capítulo 2 - Aspectos esenciales de las redes neuronales de IA

¿Cómo funcionan los sistemas de IA modernos y qué podríamos esperar de la próxima generación de IAs?

Para entender cómo se desarrollarán las consecuencias de crear IA más poderosa, es fundamental interiorizar algunos conceptos básicos. Esta sección y las dos siguientes los desarrollan, cubriendo sucesivamente qué es la IA moderna, cómo aprovecha cálculos masivos, y los sentidos en que está creciendo rápidamente en generalidad y capacidad.[^5]

Hay muchas formas de definir la inteligencia artificial, pero para nuestros propósitos la propiedad clave de la IA es que mientras un programa de computadora estándar es una lista de instrucciones sobre cómo realizar una tarea, un sistema de IA es uno que aprende de datos o experiencia para realizar tareas *sin que se le diga explícitamente cómo hacerlo.*

Casi toda la IA moderna relevante se basa en redes neuronales. Estas son estructuras matemáticas/computacionales, representadas por un conjunto muy grande (miles de millones o billones) de números ("pesos"), que realizan bien una tarea de entrenamiento. Estos pesos se elaboran (o quizás se "cultivan" o "encuentran") ajustándolos iterativamente para que la red neuronal mejore una puntuación numérica (también conocida como "pérdida") definida para desempeñarse bien en una o más tareas.[^6] Este proceso se conoce como *entrenar* la red neuronal.[^7]

Hay muchas técnicas para realizar este entrenamiento, pero esos detalles son mucho menos relevantes que las formas en que se define la puntuación, y cómo estas resultan en diferentes tareas que la red neuronal realiza bien. Históricamente se ha trazado una diferencia clave entre IA "limitada" y "general".

La IA limitada se entrena deliberadamente para hacer una tarea particular o un pequeño conjunto de tareas (como reconocer imágenes o jugar ajedrez); requiere reentrenamiento para nuevas tareas, y tiene un alcance limitado de capacidad. Tenemos IA limitada sobrehumana, lo que significa que para casi cualquier tarea discreta bien definida que una persona puede hacer, probablemente podemos construir una puntuación y luego entrenar exitosamente un sistema de IA limitada para hacerlo mejor que un humano.

Los sistemas de IA de propósito general (IAPG) pueden realizar una amplia gama de tareas, incluyendo muchas para las que no fueron explícitamente entrenados; también pueden aprender nuevas tareas como parte de su operación. Los actuales "modelos multimodales" grandes[^8] como ChatGPT ejemplifican esto: entrenados en un corpus muy grande de texto e imágenes, pueden participar en razonamiento complejo, escribir código, analizar imágenes y asistir con una vasta gama de tareas intelectuales. Aunque siguen siendo bastante diferentes de la inteligencia humana en formas que veremos en profundidad más adelante, su generalidad ha causado una revolución en la IA.[^9]

### Impredecibilidad: una característica clave de los sistemas de IA

Una diferencia clave entre los sistemas de IA y el software convencional está en la predecibilidad. La salida del software estándar puede ser impredecible – de hecho a veces por eso escribimos software, para obtener resultados que no podríamos haber predicho. Pero el software convencional rara vez hace algo para lo que no fue programado – su alcance y comportamiento son generalmente según lo diseñado. Un programa de ajedrez de primer nivel puede hacer movimientos que ningún humano podría predecir (¡o de lo contrario podrían vencer a ese programa de ajedrez!) pero generalmente no hará nada más que jugar ajedrez.

Como el software convencional, la IA limitada tiene alcance y comportamiento predecibles pero puede tener resultados impredecibles. Esto es realmente solo otra forma de definir IA limitada: como IA que es similar al software convencional en su predecibilidad y rango de operación.

La IA de propósito general es diferente: su alcance (los dominios sobre los que se aplica), comportamiento (los tipos de cosas que hace), y resultados (sus salidas reales) pueden ser todos impredecibles.[^10] GPT-4 fue entrenado solo para generar texto con precisión, pero desarrolló muchas capacidades que sus entrenadores no predijeron ni intentaron. Esta impredecibilidad surge de la complejidad del entrenamiento: porque los datos de entrenamiento contienen salidas de muchas tareas diferentes, la IA debe efectivamente aprender a realizar estas tareas para predecir bien.

Esta impredecibilidad de los sistemas de IA general es bastante fundamental. Aunque en principio es posible construir cuidadosamente sistemas de IA que tengan límites garantizados en su comportamiento (como se menciona más adelante en el ensayo), la forma en que se crean los sistemas de IA ahora los hace impredecibles en la práctica e incluso en principio.

### IA pasiva, agentes, sistemas autónomos y alineación

Esta impredecibilidad se vuelve particularmente importante cuando consideramos cómo se despliegan y usan realmente los sistemas de IA para lograr varios objetivos.

Muchos sistemas de IA son relativamente pasivos en el sentido de que principalmente proporcionan información, y el usuario toma acciones. Otros, comúnmente denominados *agentes*, toman acciones por sí mismos, con niveles variables de participación del usuario. Aquellos que toman acciones con relativamente menos entrada o supervisión externa pueden denominarse más *autónomos*. Esto forma un espectro en términos de independencia de acción, desde herramientas pasivas hasta agentes autónomos.[^11]

En cuanto a los objetivos de los sistemas de IA, estos pueden estar directamente ligados a su objetivo de entrenamiento (por ejemplo, el objetivo de "ganar" para un sistema que juega Go también es explícitamente para lo que fue entrenado). O pueden no estarlo: el objetivo de entrenamiento de ChatGPT es en parte predecir texto, en parte ser un asistente útil. Pero al hacer una tarea dada, su objetivo le es suministrado por el usuario. Los objetivos también pueden ser creados por un sistema de IA por sí mismo, solo muy indirectamente relacionados con su objetivo de entrenamiento.[^12]

Los objetivos están estrechamente ligados a la cuestión de la "alineación", es decir, la cuestión de si los sistemas de IA *harán lo que queremos que hagan*. Esta pregunta simple oculta un enorme nivel de sutileza.[^13] Por ahora, nótese que "queremos" en esta oración podría referirse a muchas personas y grupos diferentes, llevando a diferentes tipos de alineación. Por ejemplo, una IA podría ser altamente *obediente* (o ["leal"](https://arxiv.org/abs/2003.11157)) a su usuario – aquí "queremos" es "cada uno de nosotros". O podría ser más *soberana*, siendo impulsada principalmente por sus propios objetivos y limitaciones, pero aún actuando ampliamente en el interés común del bienestar humano – "queremos" es entonces "la humanidad" o "la sociedad". En el medio hay un espectro donde una IA sería ampliamente obediente, pero podría rehusarse a tomar acciones que dañen a otros o a la sociedad, violen la ley, etc.

Estos dos ejes – nivel de autonomía y tipo de alineación – no son completamente independientes. Por ejemplo, un sistema pasivo soberano, aunque no es completamente autocontradictorio, es un concepto en tensión, como lo es un agente autónomo obediente.[^14] Hay un claro sentido en que la autonomía y la soberanía tienden a ir de la mano. En una vena similar, la predecibilidad tiende a ser mayor en sistemas de IA "pasivos" y "obedientes", mientras que los soberanos o autónomos tenderán a ser más impredecibles. Todo esto será crucial para entender las ramificaciones de una IAG potencial y superinteligencia.

Crear IA verdaderamente alineada, de cualquier tipo, requiere resolver tres desafíos distintos:

1. Entender qué "queremos" – lo cual es complejo ya sea que "queremos" signifique una persona u organización específica (lealtad) o la humanidad ampliamente (soberanía);
2. Construir sistemas que actúen regularmente de acuerdo con esos deseos – esencialmente crear comportamiento positivo consistente;
3. Más fundamentalmente, hacer sistemas que genuinamente "se preocupen" por esos deseos en lugar de meramente actuar como si lo hicieran.

La distinción entre comportamiento confiable y cuidado genuino es crucial. Así como un empleado humano podría seguir órdenes perfectamente mientras carece de cualquier compromiso real con la misión de la organización, un sistema de IA podría actuar alineado sin valorar verdaderamente las preferencias humanas. Podemos entrenar sistemas de IA para decir y hacer cosas a través de retroalimentación, y pueden aprender a razonar sobre lo que los humanos quieren. Pero hacer que *genuinamente* valoren las preferencias humanas es un desafío mucho más profundo.[^15]

Las profundas dificultades para resolver estos desafíos de alineación, y sus implicaciones para el riesgo de IA, se explorarán más adelante. Por ahora, entiéndase que la alineación no es solo una característica técnica que agregamos a los sistemas de IA, sino un aspecto fundamental de su arquitectura que da forma a su relación con la humanidad.


[^5]: Para una introducción suave pero técnica al aprendizaje automático e IA, particularmente modelos de lenguaje, ver [este sitio.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) Para otro manual moderno sobre riesgos de extinción por IA, ver [esta pieza.](https://www.thecompendium.ai/) Para un análisis científico integral y autorizado del estado de la seguridad de IA, ver el reciente [Reporte Internacional de Seguridad de IA.](https://arxiv.org/abs/2501.17805)

[^6]: El entrenamiento típicamente ocurre buscando un máximo local de la puntuación en un espacio de alta dimensión dado por los pesos del modelo. Al verificar cómo cambia la puntuación cuando los pesos se ajustan, el algoritmo de entrenamiento identifica qué ajustes mejoran más la puntuación, y mueve los pesos en esa dirección.

[^7]: Por ejemplo, en un problema de reconocimiento de imágenes, la red neuronal produciría probabilidades para etiquetas de la imagen. Una puntuación estaría relacionada con la probabilidad que la IA asigna a la respuesta correcta. El procedimiento de entrenamiento entonces ajustaría los pesos para que la próxima vez, la IA produzca una probabilidad mayor para la etiqueta correcta para esa imagen. Esto se repite entonces un número enorme de veces. El mismo procedimiento básico se usa en entrenar esencialmente todas las redes neuronales modernas, aunque con mecanismos de puntuación más complejos.

[^8]: La mayoría de los modelos multimodales usan la arquitectura "transformer" para procesar y generar múltiples tipos de datos (texto, imágenes, sonido). Todos estos pueden descomponerse en, y luego tratarse en el mismo plano, como diferentes tipos de "tokens". Los modelos multimodales se entrenan primero para predecir tokens con precisión dentro de conjuntos de datos masivos, luego se refinan a través de aprendizaje por refuerzo para mejorar capacidades y dar forma a comportamientos.

[^9]: Que los modelos de lenguaje se entrenan para hacer una cosa – predecir palabras – ha causado que algunos los llamen IA limitada. Pero esto es engañoso: porque predecir texto bien requiere tantas capacidades diferentes, esta tarea de entrenamiento lleva a un sistema sorprendentemente general. También nótese que estos sistemas se entrenan extensivamente por aprendizaje por refuerzo, efectivamente representando miles de personas dando al modelo una señal de recompensa cuando hace un buen trabajo en cualquiera de las muchas cosas que hace. Entonces hereda generalidad significativa de las personas que dan esta retroalimentación.

[^10]: Hay múltiples formas en que la IA es impredecible. Una es que en el caso general uno no puede predecir qué hará un algoritmo sin realmente ejecutarlo; hay [teoremas](https://arxiv.org/abs/1310.3225) a este efecto. Esto puede ser cierto solo porque la salida de algoritmos puede ser compleja. Pero es particularmente claro y relevante en el caso (como en ajedrez o Go) donde la predicción implicaría una capacidad (vencer a la IA) que el predictor potencial no tiene. Segundo, un sistema de IA dado no siempre producirá la misma salida incluso dada la misma entrada – sus salidas contienen aleatoriedad; esto también se acopla con impredecibilidad algorítmica. Tercero, capacidades inesperadas y emergentes pueden surgir del entrenamiento, significando que incluso los *tipos* de cosas que un sistema de IA puede y hará son impredecibles; Este último tipo es particularmente importante para consideraciones de seguridad.

[^11]: Ver [aquí](https://arxiv.org/abs/2502.02649) para una revisión en profundidad de qué se entiende por un "agente autónomo" (junto con argumentos éticos contra construirlos).

[^12]: Puedes escuchar a veces "la IA no puede tener sus propios objetivos". Esto es una completa tontería. Es fácil generar ejemplos donde la IA tiene o desarrolla objetivos que nunca le fueron dados y son conocidos solo por ella misma. No ves esto mucho en los modelos multimodales populares actuales porque se les entrena para que no lo hagan; podría entrenárseles igual de fácil para que lo hagan.

[^13]: Hay una amplia literatura. Sobre el problema general ver *The Alignment Problem* [*El Problema de Alineación*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821) de Christian, y *Human-Compatible* [*Compatible con Humanos*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) de Russell. En un lado más técnico ver por ejemplo [este artículo](https://arxiv.org/abs/2209.00626).

[^14]: Veremos más tarde que aunque tales sistemas van contra la tendencia, eso realmente los hace muy interesantes y útiles.

[^15]: Esto no es decir que requerimos emociones o sensibilidad. Más bien, es enormemente difícil desde fuera de un sistema saber cuáles son sus objetivos internos, preferencias y valores. "Genuino" aquí significaría que tenemos razón lo suficientemente fuerte para depender de ello que en el caso de sistemas críticos podemos apostar nuestras vidas en ello.

## Capítulo 3 - Aspectos clave de cómo se crean los sistemas modernos de IA general

La mayoría de los sistemas de IA más avanzados del mundo se crean utilizando métodos sorprendentemente similares. Estos son los fundamentos.

Para comprender realmente a un ser humano necesitas saber algo sobre biología, evolución, crianza infantil y más; para entender la IA también necesitas saber cómo se crea. En los últimos cinco años, los sistemas de IA han evolucionado tremendamente tanto en capacidad como en complejidad. Un factor clave que lo ha hecho posible ha sido la disponibilidad de cantidades muy grandes de computación (o coloquialmente "cómputo" cuando se aplica a la IA).

Las cifras son asombrosas. Aproximadamente 10<sup>25</sup>-10<sup>26</sup> "operaciones de punto flotante" (FLOP)[^16] se utilizan en el entrenamiento de modelos como la serie GPT, Claude, Gemini, etc.[^17] (Para comparar, si cada ser humano en la Tierra trabajara sin parar haciendo un cálculo cada cinco segundos, tomaría alrededor de mil millones de años lograr esto.) Esta enorme cantidad de computación permite el entrenamiento de modelos con hasta billones de pesos del modelo sobre terabytes de datos: una gran fracción de todo el texto de calidad que jamás se haya escrito junto con grandes bibliotecas de sonidos, imágenes y video. Complementando este entrenamiento con entrenamiento adicional extenso que refuerza las preferencias humanas y el buen rendimiento en tareas, los modelos entrenados de esta manera exhiben rendimiento competitivo con los humanos a través de un rango significativo de tareas intelectuales básicas, incluyendo razonamiento y resolución de problemas.

También sabemos (de manera muy, muy aproximada) cuánta velocidad de computación, en operaciones por segundo, es suficiente para que la velocidad de *inferencia*[^18] de tal sistema iguale la *velocidad* del procesamiento de texto humano. Son aproximadamente 10<sup>15</sup>-10<sup>16</sup> FLOP por segundo.[^19]

Aunque poderosos, estos modelos están por su naturaleza limitados en formas clave, bastante análogas a cómo un humano individual estaría limitado si se viera forzado a simplemente producir texto a una tasa fija de palabras por minuto, sin detenerse a pensar o usar herramientas adicionales. Los sistemas de IA más recientes abordan estas limitaciones a través de un proceso y arquitectura más compleja que combina varios elementos clave:

- Una o más redes neuronales, con un modelo proporcionando la capacidad cognitiva central, y hasta varios otros realizando otras tareas más específicas;
- *Herramientas* proporcionadas y utilizables por el modelo: por ejemplo, la capacidad de buscar en la web, crear o editar documentos, ejecutar programas, etc.
- *Andamiaje* que conecta las entradas y salidas de las redes neuronales. Un andamio muy simple podría permitir que dos "instancias" de un modelo de IA conversen entre sí, o que una verifique el trabajo de otra.[^20]
- Las técnicas de *cadena de razonamiento* y prompts relacionados hacen algo similar, causando que un modelo genere, por ejemplo, muchos enfoques para un problema, luego procese esos enfoques para una respuesta agregada.
- *Reentrenamiento* de modelos para hacer mejor uso de herramientas, andamiaje y cadena de razonamiento.

Debido a que estas extensiones pueden ser muy poderosas (e incluir los propios sistemas de IA), estos sistemas compuestos pueden ser bastante sofisticados y mejorar dramáticamente las capacidades de IA.[^21] Y recientemente, las técnicas de andamiaje y especialmente los prompts de cadena de razonamiento (e incorporar los resultados de vuelta al reentrenamiento de modelos para usar estos mejor) se han desarrollado y empleado en [o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/), y [DeepSeek R1](https://api-docs.deepseek.com/news/news250120) para hacer muchas pasadas de inferencia en respuesta a una consulta dada.[^22] Esto en efecto permite al modelo "pensar sobre" su respuesta y aumenta dramáticamente la capacidad de estos modelos para hacer razonamiento de alto calibre en tareas de ciencia, matemáticas y programación.[^23]

Para una arquitectura de IA dada, los aumentos en computación de entrenamiento [pueden traducirse confiablemente](https://arxiv.org/abs/2405.10938) en mejoras en un conjunto de métricas claramente definidas. Para capacidades generales definidas de manera menos precisa (como las discutidas a continuación), la traducción es menos clara y predictiva, pero es casi seguro que modelos más grandes con más computación de entrenamiento tendrán capacidades nuevas y mejores, incluso si es difícil predecir cuáles serán.

De manera similar, los sistemas compuestos y especialmente los avances en "cadena de razonamiento" (y entrenamiento de modelos que funcionan bien con ella) han desbloqueado el escalamiento en computación de *inferencia*: para un modelo central entrenado dado, al menos algunas capacidades del sistema de IA aumentan conforme se aplica más computación que les permite "pensar más duro y más tiempo" sobre problemas complejos. Esto viene con un costo alto en velocidad de computación, requiriendo cientos o miles de FLOP/s más para igualar el rendimiento humano.[^24]

Aunque es solo una parte de lo que está llevando al rápido progreso de la IA,[^25] el papel de la computación y la posibilidad de sistemas compuestos resultará crucial tanto para prevenir una IAG incontrolable como para desarrollar alternativas más seguras.

[^16]: 10<sup>27</sup> significa 1 seguido de 25 ceros, o diez billones de billones. Un FLOP es simplemente una suma o multiplicación aritmética de números con cierta precisión. Nota que el rendimiento del hardware de IA puede variar por un factor de diez más dependiendo de la precisión de la aritmética y la arquitectura de la computadora. Contar operaciones de compuertas lógicas (AND, OR, AND NOT) sería fundamental pero estas no están comúnmente disponibles o son evaluadas comparativamente; para propósitos presentes es útil estandarizar en operaciones de 16 bits (FP16), aunque deberían establecerse factores de conversión apropiados.

[^17]: Una colección de estimaciones y datos duros está disponible en [Epoch AI](https://epochai.org/data/large-scale-ai-models) e indica aproximadamente 2×10<sup>25</sup> FLOP de 16 bits para GPT-4; esto coincide aproximadamente con [números que fueron filtrados](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) para GPT-4. Las estimaciones para otros modelos de mediados de 2024 están todas dentro de un factor de pocos de GPT-4.

[^18]: La inferencia es simplemente el proceso de generar una salida de una red neuronal. El entrenamiento puede considerarse una sucesión de muchas inferencias y ajustes de pesos del modelo.

[^19]: Para la producción de texto, el GPT-4 original requería 560 TFLOP por token generado. Alrededor de 7 tokens/s se necesitan para mantenerse al día con el pensamiento humano, así que esto da ≈3×10<sup>15</sup> FLOP/s. Pero las eficiencias han reducido esto; [este folleto de NVIDIA](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/) por ejemplo indica tan poco como 3×10<sup>14</sup> FLOP/s para un modelo Llama 405B de rendimiento comparable.

[^20]: Como un ejemplo ligeramente más complejo, un sistema de IA podría primero generar varias soluciones posibles a un problema matemático, luego usar otra instancia para verificar cada solución, y finalmente usar una tercera para sintetizar los resultados en una explicación clara. Esto permite una resolución de problemas más exhaustiva y confiable que una sola pasada.

[^21]: Ver por ejemplo detalles sobre el ["Operator" de OpenAI](https://openai.com/index/introducing-operator/), [capacidades de herramientas de Claude](https://docs.anthropic.com/en/docs/build-with-claude/computer-use), y [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). La [Deep Research](https://openai.com/index/introducing-deep-research/) de OpenAI probablemente tiene una arquitectura bastante sofisticada pero los detalles no están disponibles.

[^22]: Deepseek R1 se basa en entrenar y hacer prompts iterativamente al modelo para que el modelo entrenado final cree un razonamiento extenso de cadena de razonamiento. Los detalles arquitectónicos no están disponibles para o1 u o3, sin embargo Deepseek ha revelado que no se requiere ninguna "salsa especial" particular para desbloquear el escalamiento de capacidad con inferencia. Pero a pesar de recibir mucha atención en la prensa como trastornando el "statu quo" en IA, no impacta las afirmaciones centrales de este ensayo.

[^23]: Estos modelos superan significativamente a los modelos estándar en benchmarks de razonamiento. Por ejemplo, en el GPQA Diamond Benchmark—una prueba rigurosa de preguntas de ciencia a nivel de doctorado—GPT-4o [obtuvo](https://openai.com/index/learning-to-reason-with-llms/) 56%, mientras que o1 y o3 lograron 78% y 88%, respectivamente, superando por mucho el puntaje promedio del 70% de expertos humanos.

[^24]: El O3 de OpenAI probablemente gastó ∼10<sup>21</sup>-10<sup>22</sup> FLOP [para completar cada una de las preguntas del desafío ARC-AGI](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai), que humanos competentes pueden hacer en (digamos) 10-100 segundos, dando una cifra más parecida a ∼10<sup>20</sup> FLOP/s.

[^25]: Aunque la computación es una medida clave de la capacidad del sistema de IA, interactúa tanto con la calidad de los datos como con mejoras algorítmicas. Mejores datos o algoritmos pueden reducir los requisitos computacionales, mientras que más computación a veces puede compensar datos o algoritmos más débiles.

## Capítulo 4 - ¿Qué son la IAG y la superinteligencia?

¿Qué están compitiendo exactamente por construir a puertas cerradas las empresas tecnológicas más grandes del mundo?

El término "inteligencia artificial general" ha existido durante algún tiempo para referirse a la IA de propósito general "a nivel humano". Nunca ha sido un término particularmente bien definido, pero en años recientes paradójicamente se ha vuelto no mejor definido pero aún más importante, con expertos argumentando simultáneamente sobre si la IAG está a décadas de distancia o ya se ha logrado, y empresas de billones de dólares corriendo "hacia la IAG". (La ambigüedad de "IAG" se destacó recientemente cuando [documentos filtrados supuestamente revelaron](https://gizmodo.com/leaked-documents-show-openai-has-a-very-clear-definition-of-agi-2000543339) que en el contrato de OpenAI con Microsoft, la IAG se definía como IA que genera $100 mil millones en ingresos para OpenAI – una definición más mercenaria que intelectual.)

Hay dos problemas fundamentales con la idea de IA con "inteligencia a nivel humano". Primero, los humanos son muy, muy diferentes en su capacidad para realizar cualquier tipo dado de trabajo cognitivo, por lo que no existe un "nivel humano". Segundo, la inteligencia es muy multidimensional; aunque puede haber correlaciones, son imperfectas y pueden ser bastante diferentes en la IA. Así que incluso si pudiéramos definir "nivel humano" para muchas capacidades, la IA seguramente estaría muy por encima en algunas mientras está bastante por debajo en otras.[^26]

Sin embargo, es bastante crucial poder discutir tipos, niveles y umbrales de capacidad de IA. El enfoque adoptado aquí es enfatizar que la IA de propósito general está aquí, y que viene – y vendrá – en varios niveles de capacidad a los cuales es conveniente asignar términos aunque sean reductivos, porque corresponden a umbrales cruciales en términos de los efectos de la IA en la sociedad y la humanidad.

Definiremos la IAG "completa" como sinónimo de "IA de propósito general superhumana", es decir, un sistema de IA que sea capaz de realizar esencialmente todas las tareas cognitivas humanas al nivel de expertos humanos superiores o por encima de él, así como adquirir nuevas habilidades y transferir capacidades a nuevos dominios. Esto está en línea con cómo se define a menudo "IAG" en la literatura moderna. Es importante notar que este es un umbral *muy* alto. Ningún humano tiene este tipo de inteligencia; más bien es el tipo de inteligencia que tendrían grandes colecciones de expertos humanos superiores si se combinaran. Podemos denominar "superinteligencia" a una capacidad que va más allá de esto, y definir niveles más limitados de capacidad mediante IAPG "competitiva con humanos" y "competitiva con expertos", que realizan una amplia gama de tareas a nivel profesional típico, o a nivel de experto humano.[^27]

Estos términos y algunos otros se recopilan en [la tabla](https://keepthefuturehuman.ai/essay/docs/#tab:terms) a continuación. Para tener una sensación más concreta de lo que los varios grados de sistema pueden hacer, es útil tomar las definiciones en serio y considerar lo que significan.

| Tipo de IA                        | Términos Relacionados                          | Definición                                                                                                                                                                                                                                   | Ejemplos                                                                                                                                        |
| --------------------------------- | ---------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |
| IA Estrecha                       | IA Débil                                       | IA entrenada para una tarea específica o familia de tareas. Sobresale en su dominio pero carece de inteligencia general o capacidad de aprendizaje transferible.                                                                            | Software de reconocimiento de imágenes; Asistentes de voz (ej., Siri, Alexa); Programas de ajedrez; AlphaFold de DeepMind                     |
| IA Herramienta                    | Inteligencia Aumentada, Asistente de IA       | (Discutida más adelante en el ensayo.) Sistema de IA que mejora las capacidades humanas. Combina IA de propósito general competitiva con humanos, IA estrecha y control garantizado, priorizando seguridad y colaboración. Apoya la toma de decisiones humana. | Asistentes avanzados de programación; Herramientas de investigación potenciadas por IA; Plataformas sofisticadas de análisis de datos. Agentes competentes pero estrechos y controlables |
| IA de Propósito General (IAPG)    |                                                | Sistema de IA adaptable a varias tareas, incluyendo aquellas para las que no fue específicamente entrenado.                                                                                                                                    | Modelos de lenguaje (ej., GPT-4, Claude); Modelos de IA multimodal; MuZero de DeepMind                                                         |
| IAPG competitiva con humanos      | IAG \[débil\]                                  | IA de propósito general que realiza tareas a nivel humano promedio, a veces superándolo.                                                                                                                                                        | Modelos de lenguaje avanzados (ej., O1, Claude 3.5); Algunos sistemas de IA multimodal                                                         |
| IAPG competitiva con expertos     | IAG \[parcial\]                               | IA de propósito general que realiza la mayoría de tareas a nivel de experto humano, con autonomía significativa pero limitada                                                                                                                  | Posiblemente un O3 equipado y con andamiaje, al menos para matemáticas, programación y algunas ciencias exactas                               |
| IAG \[completa\]                  | IAPG Superhumana                               | Sistema de IA capaz de realizar autónomamente aproximadamente todas las tareas intelectuales humanas al nivel de expertos o más allá, con aprendizaje eficiente y transferencia de conocimiento.                                             | \[No hay ejemplos actuales – teórico\]                                                                                                         |
| Superinteligencia                 | IAPG Altamente superhumana                    | Sistema de IA que supera enormemente las capacidades humanas en todos los dominios, superando la experiencia colectiva humana. Esta superioridad podría ser en generalidad, calidad, velocidad y/o otras medidas.                          | \[No hay ejemplos actuales – teórico\]                                                                                                         |

Ya estamos experimentando cómo es tener IAPGs hasta el nivel competitivo con humanos. Esto se ha integrado relativamente sin problemas, ya que la mayoría de usuarios experimentan esto como tener un trabajador temporal inteligente pero limitado que los hace más productivos con impacto mixto en la calidad de su trabajo.[^28]

Lo que sería diferente sobre la IAPG competitiva con expertos es que no tendría las limitaciones fundamentales de la IA actual, y haría las cosas que hacen los expertos: trabajo independiente económicamente valioso, creación real de conocimiento, trabajo técnico en el que puedes confiar, mientras rara vez (aunque aún ocasionalmente) comete errores tontos.

La idea de la IAG completa es que *realmente hace* todas las cosas cognitivas que hacen incluso los humanos más capaces y efectivos, de manera autónoma y sin ayuda u supervisión necesaria. Esto incluye planificación sofisticada, aprender nuevas habilidades, manejar proyectos complejos, etc. Podría hacer investigación original de vanguardia. Podría dirigir una empresa. Cualquiera que sea tu trabajo, si se hace predominantemente por computadora o por teléfono, *podría hacerlo al menos tan bien como tú.* Y probablemente mucho más rápido y más barato. Discutiremos algunas de las ramificaciones más adelante, pero por ahora el desafío para ti es realmente tomar esto en serio. Imagina las diez personas más conocedoras y competentes que conoces o de las que sabes – incluyendo CEOs, científicos, profesores, ingenieros superiores, psicólogos, líderes políticos y escritores. Combínalos todos en uno, que también hable 100 idiomas, tenga una memoria prodigiosa, opere rápidamente, sea incansable y siempre motivado, y trabaje por debajo del salario mínimo.[^29] Esa es una idea de lo que sería la IAG.

Para la superinteligencia el imaginar es más difícil, porque la idea es que podría realizar hazañas intelectuales que ningún humano o incluso colección de humanos puede – es por definición impredecible para nosotros. Pero podemos tener una idea. Como línea base mínima, considera muchas IAGs, cada una mucho más capaz que incluso el experto humano superior, ejecutándose a 100 veces la velocidad humana, con memoria enorme y capacidad de coordinación fantástica.[^30] Y va hacia arriba desde ahí. Tratar con superinteligencia sería menos como conversar con una mente diferente, más como negociar con una civilización diferente (y más avanzada).

Entonces, ¿qué tan cerca *estamos* de la IAG y la superinteligencia?


[^26]: Por ejemplo, los sistemas de IA actuales superan ampliamente la capacidad humana en tareas aritméticas rápidas o de memoria, mientras quedan cortos en razonamiento abstracto y resolución creativa de problemas.

[^27]: Muy importante, como competidor tal IA tendría varias ventajas estructurales importantes incluyendo: no se cansaría ni tendría otras necesidades individuales como los humanos; puede ejecutarse a velocidades más altas simplemente escalando el poder de cómputo; puede copiarse junto con cualquier experiencia o conocimiento que adquiera – e incluso el conocimiento adquirido de las redes neuronales puede "fusionarse" para transferir conjuntos completos de habilidades entre sí; podría comunicarse a velocidad de máquina; y podría automodificarse o auto-mejorarse de maneras más significativas y a mayor velocidad que cualquier humano.

[^28]: Si no has pasado tiempo usando los sistemas de IA actuales de última generación, lo recomiendo: son genuinamente útiles y capaces, y también es importante para calibrar el efecto que tendrá la IA cuando se vuelva más poderosa.

[^29]: Considera un hospital de investigación importante: la IAG completamente realizada podría simultáneamente analizar todos los datos de pacientes entrantes, mantenerse al día con cada nuevo artículo médico, sugerir diagnósticos, diseñar planes de tratamiento, manejar ensayos clínicos y coordinar la programación del personal – todo mientras opera a un nivel que iguala o supera a los especialistas superiores del hospital en cada área. Y podría hacer esto para múltiples hospitales simultáneamente, a una fracción del costo actual. Desafortunadamente, también debes considerar un sindicato del crimen organizado: la IAG completamente realizada podría simultáneamente hackear, suplantar, espiar y chantajear a miles de víctimas, mantenerse al día con las fuerzas del orden (que se automatizan mucho más lentamente), diseñar nuevos esquemas para hacer dinero y coordinar la programación del personal – si es que hay algún personal.

[^30]: En su [ensayo](https://darioamodei.com/machines-of-loving-grace), Dario Amodei, CEO de Anthropic, evocó un "País de \[un millón de\] genios".

## Capítulo 5 - En el umbral

El camino desde los sistemas de IA actuales hacia una IAG completamente desarrollada parece sorprendentemente corto y predecible.

Los últimos diez años han visto avances dramáticos en IA impulsados por enormes recursos [computacionales](https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year), humanos y [fiscales](https://arxiv.org/abs/2405.21015). Muchas aplicaciones de IA especializada superan a los humanos en sus tareas asignadas, y son ciertamente mucho más rápidas y baratas.[^31] Y también existen agentes especializados sobrehumanos que pueden derrotar a todas las personas en juegos de dominio específico como [Go](https://www.nature.com/articles/nature16961), [ajedrez](https://arxiv.org/abs/1712.01815) y [póker](https://www.deepstack.ai/), así como [agentes más generales](https://deepmind.google/discover/blog/a-generalist-agent/) que pueden planificar y ejecutar acciones en entornos simulados simplificados tan eficazmente como los humanos.

Más prominentemente, los sistemas actuales de IA general de OpenAI/Microsoft, Google/Deepmind, Anthropic/Amazon, Facebook/Meta, X.ai/Tesla y otros [^32] han surgido desde principios de 2023 y han aumentado constantemente (aunque de manera desigual) sus capacidades desde entonces. Todos estos han sido creados mediante predicción de tokens en enormes conjuntos de datos de texto y multimedia, combinados con retroalimentación de refuerzo extensiva de humanos y otros sistemas de IA. Algunos de ellos también incluyen sistemas extensos de herramientas y andamiaje.

### Fortalezas y debilidades de los sistemas generales actuales

Estos sistemas funcionan bien en una gama cada vez más amplia de pruebas diseñadas para medir inteligencia y experiencia, con un progreso que ha sorprendido incluso a los expertos en el campo:

- Cuando se lanzó por primera vez, GPT-4 [igualó o superó el rendimiento humano típico](https://arxiv.org/abs/2303.08774) en exámenes académicos estándar incluyendo SATs, GRE, exámenes de ingreso y exámenes de colegiación. Los modelos más recientes probablemente funcionan significativamente mejor, aunque los resultados no están disponibles públicamente.
- La prueba de Turing – considerada durante mucho tiempo como un punto de referencia clave para la IA "verdadera" – ahora es rutinariamente superada en algunas formas por los modelos de lenguaje modernos, tanto informalmente como en [estudios formales](https://arxiv.org/abs/2405.08007).[^33]
- En el exhaustivo punto de referencia MMLU que abarca 57 materias académicas, [los modelos recientes logran puntuaciones a nivel de experto en el dominio](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu) (~90%) [^34]
- La experiencia técnica ha avanzado dramáticamente: El punto de referencia GPQA de física de nivel posgrado vio [saltar el rendimiento](https://epoch.ai/data/ai-benchmarking-dashboard) de casi adivinación aleatoria (GPT-4, 2022) a nivel experto (o1-preview, 2024).
- Incluso las pruebas específicamente diseñadas para ser resistentes a la IA están cayendo: O3 de OpenAI [supuestamente](https://www.nextbigfuture.com/2024/12/openai-releases-o3-model-with-high-performance-and-high-cost.html) resuelve el punto de referencia ARC-AGI de resolución abstracta de problemas a nivel humano, logra rendimiento de codificación de experto superior, y obtiene 25% en los problemas de "matemáticas de frontera" de Epoch AI diseñados para desafiar a matemáticos de élite.[^35]
- La tendencia es tan clara que el desarrollador de MMLU ahora ha creado ["El Último Examen de la Humanidad"](https://agi.safe.ai/) – un nombre ominoso que refleja la posibilidad de que la IA pronto supere el rendimiento humano en cualquier prueba significativa. Al momento de escribir esto, hay afirmaciones de sistemas de IA que logran 27% (según [Sam Altman](https://x.com/sama/status/1886220281565381078)) y 35% (según [este artículo](https://arxiv.org/abs/2502.09955)) en este examen extremadamente difícil. Es muy poco probable que cualquier humano individual pueda hacerlo.

A pesar de estos números impresionantes (y su obvia inteligencia cuando uno interactúa con ellos) [^36] hay muchas cosas que (al menos las versiones lanzadas de) estas redes neuronales *no pueden* hacer. Actualmente la mayoría son incorpóreas – existiendo solo en servidores – y procesan como máximo texto, sonido e imágenes fijas (pero no video). Crucialmente, la mayoría no puede llevar a cabo actividades planificadas complejas que requieren alta precisión.[^37] Y hay una serie de otras cualidades fuertes en la cognición humana de alto nivel que actualmente son bajas en los sistemas de IA lanzados.

La siguiente tabla enumera varias de estas, basadas en sistemas de IA de mediados de 2024 como GPT-4o, Claude 3.5 Sonnet y Google Gemini 1.5.[^38] La pregunta clave para qué tan rápidamente la IA general se volverá más poderosa es: ¿hasta qué grado simplemente hacer *más de lo mismo* producirá resultados, versus agregar técnicas adicionales pero *conocidas*, versus desarrollar o implementar direcciones de investigación en IA *realmente nuevas*? Mis propias predicciones para esto se dan en la tabla, en términos de qué tan probable es que cada uno de estos escenarios obtenga esa capacidad a nivel humano y más allá.

<table><tbody><tr><th>Capacidad</th><th>Descripción de la capacidad</th><th>Estado/pronóstico</th><th>Escalamiento/conocido/nuevo</th></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td colspan="4"><em>Capacidades Cognitivas Centrales</em></td></tr><tr><td>Razonamiento</td><td>Las personas pueden hacer razonamiento preciso, de múltiples pasos, siguiendo reglas y verificando precisión.</td><td>Progreso reciente dramático usando cadena de razonamiento extendida y reentrenamiento</td><td>95/5/5</td></tr><tr><td>Planificación</td><td>Las personas exhiben planificación a largo plazo y jerárquica.</td><td>Mejorando con escala; puede ser fuertemente ayudado usando andamiaje y mejores técnicas de entrenamiento.</td><td>10/85/5</td></tr><tr><td>Anclaje en la verdad</td><td>Las IAPG confabulan información no fundamentada para satisfacer consultas.</td><td>Mejorando con escala; datos de calibración disponibles dentro del modelo; puede ser verificado/mejorado vía andamiaje.</td><td>30/65/5</td></tr><tr><td>Resolución flexible de problemas</td><td>Los humanos pueden reconocer nuevos patrones e inventar nuevas soluciones a problemas complejos; los modelos de AM actuales luchan.</td><td>Mejora con escala pero débilmente; puede ser solucionable con técnicas neurosimbólicas o de "búsqueda" generalizadas.</td><td>15/75/10</td></tr><tr><td colspan="4"><em>Aprendizaje y Conocimiento</em></td></tr><tr><td>Aprendizaje y memoria</td><td>Las personas tienen memoria de trabajo, a corto y largo plazo, todas las cuales son dinámicas e interrelacionadas.</td><td>Todos los modelos aprenden durante el entrenamiento; las IAPG aprenden dentro de la ventana de contexto y durante el ajuste fino; "aprendizaje continuo" y otras técnicas existen pero aún no integradas en IAPG grandes.</td><td>5/80/15</td></tr><tr><td>Abstracción y recursión</td><td>Las personas pueden mapear y transferir conjuntos de relaciones a otros más abstractos para razonamiento y manipulación, incluyendo razonamiento "meta" recursivo.</td><td>Mejorando débilmente con escala; podría emerger en sistemas neurosimbólicos.</td><td>30/50/20</td></tr><tr><td>Modelo(s) del mundo</td><td>Las personas tienen y continuamente actualizan un modelo predictivo del mundo dentro del cual pueden resolver problemas y hacer razonamiento físico</td><td>Mejorando con escala; actualización ligada al aprendizaje; IAPG débiles en predicción del mundo real.</td><td>20/50/30</td></tr><tr><td colspan="4"><em>Yo y Agencia</em></td></tr><tr><td>Agencia</td><td>Las personas pueden tomar acciones para perseguir objetivos, basándose en planificación/predicción.</td><td>Muchos sistemas de AM son agénticos; los LLM pueden convertirse en agentes vía envoltorios.</td><td>5/90/5</td></tr><tr><td>Autodirección</td><td>Las personas desarrollan y persiguen sus propios objetivos, con motivación e impulso generados internamente.</td><td>Compuesto en gran medida de agencia más originalidad; probablemente emerja en sistemas agénticos complejos con objetivos abstractos.</td><td>40/45/15</td></tr><tr><td>Autorreferencia</td><td>Las personas entienden y razonan sobre sí mismas como situadas dentro de un ambiente/contexto.</td><td>Mejorando con escala y podría ser aumentado con recompensa de entrenamiento.</td><td>70/15/15</td></tr><tr><td>Autoconciencia</td><td>Las personas tienen conocimiento de y pueden razonar respecto a sus propios pensamientos y estados mentales.</td><td>Existe en algún sentido en IAPG, que pueden decirse que pasan la "prueba del espejo" clásica para autoconciencia. Puede ser mejorado con andamiaje; pero no está claro si esto es suficiente.</td><td>20/55/25</td></tr><tr><td colspan="4"><em>Interfaz y Ambiente</em></td></tr><tr><td>Inteligencia encarnada</td><td>Las personas entienden e interactúan activamente con su ambiente del mundo real.</td><td>El aprendizaje por refuerzo funciona bien en ambientes simulados y del mundo real (robóticos) y puede ser integrado en transformadores multimodales.</td><td>5/85/10</td></tr><tr><td>Procesamiento multisensorial</td><td>Las personas integran y procesan en tiempo real flujos visuales, auditivos y de otros sensores.</td><td>El entrenamiento en múltiples modalidades parece "simplemente funcionar," y mejorar con escala. El procesamiento de video en tiempo real es difícil pero e.g. los sistemas de conducción autónoma están mejorando rápidamente.</td><td>30/60/10</td></tr><tr><td colspan="4"><em>Capacidades de Orden Superior</em></td></tr><tr><td>Originalidad</td><td>Los modelos de AM actuales son creativos en transformar y combinar ideas/obras existentes, pero las personas pueden construir nuevos marcos y estructuras, a veces ligados a su identidad.</td><td>Puede ser difícil de discernir de "creatividad," que puede escalar hacia ella; puede emerger de creatividad más autoconciencia.</td><td>50/40/10</td></tr><tr><td>Sensibilidad</td><td>Las personas experimentan qualia; estos pueden ser de valencia positiva, negativa o neutral; es "como algo" ser una persona.</td><td>Muy difícil y filosóficamente complejo determinar si un sistema dado tiene esto.</td><td>5/10/85</td></tr></tbody></table>

Capacidades clave actualmente por debajo del nivel de experto humano en sistemas de IAPG modernos, agrupadas por tipo. La tercera columna resume el estado actual. La columna final muestra la probabilidad predicha (%) de que el rendimiento a nivel humano se logrará mediante: escalamiento de técnicas actuales / combinación con técnicas conocidas / desarrollo de técnicas nuevas. Estas capacidades no son independientes, y el aumento en cualquiera típicamente va junto con aumentos en otras. Nótese que no todas (particularmente la sensibilidad) son necesarias para sistemas de IA capaces de avanzar el desarrollo de IA, destacando la posibilidad de IA poderosa pero no sensible.

Desglosar lo que está "faltando" de esta manera deja bastante claro que estamos muy encaminados hacia inteligencia ampliamente superior a la humana mediante el escalamiento de técnicas existentes o conocidas.[^39]

Aún podrían haber sorpresas. Incluso dejando de lado la "sensibilidad," podría haber algunas de las capacidades cognitivas centrales listadas que realmente no se pueden hacer con técnicas actuales y requieren nuevas. Pero considere esto. El esfuerzo presente que están poniendo muchas de las compañías más grandes del mundo equivale a múltiples veces el gasto del proyecto Apollo y decenas de veces el del proyecto Manhattan,[^40] y está empleando miles de las personas técnicas más destacadas con salarios inauditos. Las dinámicas de los últimos años ahora han traído a esto más poder intelectual humano (con IA ahora siendo agregada) que cualquier empresa en la historia. No deberíamos apostar al fracaso.

### El gran objetivo: agentes autónomos generalistas

El desarrollo de IA general durante los últimos años se ha enfocado en crear IA general y poderosa pero similar a herramientas: funciona principalmente como un asistente (bastante) leal, y generalmente no toma acciones por sí misma. Esto es parcialmente por diseño, pero en gran medida porque estos sistemas simplemente no han sido lo suficientemente competentes en las habilidades relevantes para ser confiados con acciones complejas.[^41]

Sin embargo, las compañías de IA e investigadores están [cambiando su enfoque](https://www.axios.com/2025/01/23/davos-2025-ai-agents) cada vez más hacia agentes *autónomos* de propósito general a nivel experto.[^42] Esto permitiría que los sistemas actúen más como un asistente humano al cual el usuario puede delegar acciones reales.[^43] ¿Qué requerirá eso? Varias de las capacidades en la tabla de "lo que falta" están implicadas, incluyendo fuerte anclaje en la verdad, aprendizaje y memoria, abstracción y recursión, y modelado del mundo (para inteligencia), planificación, agencia, originalidad, autodirección, autorreferencia y autoconciencia (para autonomía), y procesamiento multisensorial, inteligencia encarnada, y resolución flexible de problemas (para generalidad).[^44]

Esta triple intersección de alta autonomía (independencia de acción), alta generalidad (alcance y amplitud de tareas) y alta inteligencia (competencia en tareas cognitivas) es actualmente única de los humanos. Es implícitamente lo que muchos probablemente tienen en mente cuando piensan en IAG – tanto en términos de su valor como de sus riesgos.

Esto proporciona otra manera de definir I-A-G como ***I*** nteligencia- ***A*** utónoma- ***G*** eneral, y veremos que esta triple intersección proporciona una lente muy valiosa para sistemas de alta capacidad tanto en entender sus riesgos y recompensas, como en la gobernanza de la IA.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Simple-1024x1024.png&w=3840&q=75) La zona transformacional de poder y riesgo de I-A-G emerge de la intersección de tres propiedades clave: alta Autonomía, alta Inteligencia en tareas, y alta Generalidad.

### El ciclo de (auto-)mejoramiento de la IA

Un factor crucial final en entender el progreso de la IA es el ciclo único de retroalimentación tecnológica de la IA. En el desarrollo de IA, el éxito – tanto en sistemas demostrados como en productos desplegados – trae inversión adicional, talento y competencia, y actualmente estamos en medio de un enorme ciclo de retroalimentación de exageración-más-realidad de IA que está impulsando cientos de miles de millones, o incluso billones, de dólares en inversión.

Este tipo de ciclo de retroalimentación podría suceder con cualquier tecnología, y lo hemos visto en muchas, donde el éxito de mercado genera inversión, que genera mejoramiento y mejor éxito de mercado. Pero el desarrollo de IA va más lejos, en que ahora los sistemas de IA están ayudando a desarrollar sistemas de IA nuevos y más poderosos.[^45] Podemos pensar en este ciclo de retroalimentación en cinco etapas, cada una con una escala de tiempo más corta que la anterior, como se muestra en la tabla.

*El ciclo de mejoramiento de IA opera a través de múltiples escalas de tiempo, con cada etapa potencialmente acelerando las etapas subsecuentes. Las etapas anteriores están bien en marcha, mientras que las etapas posteriores permanecen especulativas pero podrían proceder muy rápidamente una vez desbloqueadas.*

Varias de estas etapas ya están en marcha, y un par claramente comenzando. La última etapa, en la cual los sistemas de IA se mejoran autónomamente a sí mismos, ha sido un elemento básico de la literatura sobre el riesgo de sistemas de IA muy poderosos, y por buena razón.[^46] Pero es importante notar que es solo la forma más drástica de un ciclo de retroalimentación que ya ha comenzado y podría llevar a más sorpresas en el avance rápido de la tecnología.


[^31]: Usted usa mucho más de esta IA de lo que probablemente piensa, impulsando generación y reconocimiento de voz, procesamiento de imágenes, algoritmos de feeds de noticias, etc.

[^32]: Aunque las relaciones entre estos pares de compañías son bastante complejas y matizadas, las he listado explícitamente para indicar tanto la vasta capitalización de mercado general de firmas ahora involucradas en desarrollo de IA, como también que detrás incluso de compañías "más pequeñas" como Anthropic se encuentran bolsillos enormemente profundos vía inversiones y acuerdos de asociación importantes.

[^33]: Se ha vuelto de moda menospreciar la prueba de Turing, pero es bastante poderosa y general. En versiones débiles indica si las personas típicas interactuando con una IA (que está entrenada para actuar humana) de maneras típicas por períodos breves pueden distinguir si es una IA. No pueden. Segundo, una prueba de Turing altamente adversarial puede sondear esencialmente cualquier elemento de capacidad e inteligencia humana – por ej. comparando un sistema de IA con un experto humano, evaluado por otros expertos humanos. Hay un sentido en el cual mucha de la evaluación de IA es una forma generalizada de prueba de Turing.

[^34]: Esto es por dominio – ningún humano podría plausiblemente lograr tales puntuaciones a través de todas las materias simultáneamente.

[^35]: Estos son problemas que tomarían incluso a excelentes matemáticos tiempo sustancial resolver, si pudieran resolverlos del todo.

[^36]: Si usted es de inclinación escéptica, mantenga su escepticismo pero realmente pruebe los modelos más actuales, así como trate por usted mismo algunas de las preguntas de prueba que pueden pasar. Como profesor de física, predigo con casi certeza que, por ejemplo, los mejores modelos pasarían el examen de calificación de posgrado en nuestro departamento.

[^37]: Esta y otras debilidades como la confabulación han ralentizado la adopción del mercado y llevado a una brecha entre capacidades percibidas y reclamadas (que también debe ser vista a través de la lente de intensa competencia de mercado y la necesidad de atraer inversión). Esto ha confundido tanto al público como a los formuladores de políticas sobre el estado real del progreso de IA. Aunque quizás no igualando la exageración, el progreso es muy real.

[^38]: El avance mayor desde entonces ha sido el desarrollo de sistemas entrenados para razonamiento de alta calidad, aprovechando más computación durante la inferencia y mayor aprendizaje por refuerzo. Porque estos modelos son nuevos y sus capacidades menos probadas, no he reelaborado completamente esta tabla excepto por "razonamiento," que considero esencialmente resuelto. Pero he actualizado predicciones basadas en capacidades experimentadas y reportadas de esos sistemas.

[^39]: Ondas previas de optimismo de IA en los 1960s y 1980s terminaron en "inviernos de IA" cuando las capacidades prometidas fallaron en materializarse. Sin embargo, la onda actual difiere fundamentalmente en haber logrado rendimiento sobrehumano en muchos dominios, respaldado por recursos computacionales masivos y éxito comercial.

[^40]: El proyecto Apollo completo [costó cerca de $250 mil millones USD en dólares de 2020](https://www.planetary.org/space-policy/cost-of-apollo), y el proyecto Manhattan [menos de una décima parte de eso](https://www.brookings.edu/the-costs-of-the-manhattan-project/). Goldman Sachs [proyecta un billón de dólares de gasto solo en centros de datos de IA](https://www.datacenterdynamics.com/en/news/goldman-sachs-1tn-to-be-spent-on-ai-data-centers-chips-and-utility-upgrades-with-little-to-show-for-it-so-far/) durante los próximos años.

[^41]: Aunque los humanos cometemos muchos errores, ¡subestimamos qué tan confiables podemos ser! Porque las probabilidades se multiplican, una tarea que requiere 20 pasos para hacerse correctamente requiere que cada paso sea 97% confiable solo para hacerla bien la mitad del tiempo. Hacemos tales tareas todo el tiempo.

[^42]: Un movimiento fuerte en esta dirección se ha tomado muy recientemente con el asistente ["Deep Research"](https://openai.com/index/introducing-deep-research/) de OpenAI que autónomamente realiza investigación general, descrito como "una nueva capacidad agéntica que conduce investigación de múltiples pasos en internet para tareas complejas."

[^43]: Cosas como llenar ese formulario PDF molesto, reservar vuelos, etc. ¡Pero con un doctorado en 20 campos! Así que también: escribir esa tesis para usted, negociar ese contrato para usted, demostrar ese teorema para usted, crear esa campaña publicitaria para usted, etc. ¿Qué hace *usted*? Le dice qué hacer, por supuesto.

[^44]: Nótese que la sensibilidad *no* es claramente requerida, ni la IA en esta triple intersección necesariamente la implica.

[^45]: La analogía más cercana aquí es quizás la tecnología de chips, donde el desarrollo ha mantenido la ley de Moore por décadas, mientras las tecnologías computacionales ayudan a las personas a diseñar la siguiente generación de tecnología de chips. Pero la IA será mucho más directa.

[^46]: Es importante dejar que se asiente por un momento que la IA podría – pronto – estar mejorándose a sí misma en una escala de tiempo de días o semanas. O menos. Tenga esto en mente cuando alguien le diga que una capacidad de IA está definitivamente muy lejos.

## Capítulo 6 - La carrera hacia la IAG

¿Cuáles son las fuerzas impulsoras detrás de la carrera para construir IAG, tanto para empresas como para países?

El rápido progreso reciente en IA ha resultado tanto de como en un nivel extraordinario de atención e inversión. Esto se debe en parte al éxito en el desarrollo de IA, pero hay más factores en juego. ¿Por qué algunas de las empresas más grandes de la Tierra, e incluso países enteros, compiten para construir no solo IA, sino IAG y superinteligencia?

### Qué ha llevado la investigación en IA hacia la IA de nivel humano

Hasta hace aproximadamente cinco años, la IA había sido principalmente un problema de investigación académica y científica, impulsado en gran medida por la curiosidad y el impulso de entender la inteligencia y cómo crearla en un nuevo sustrato.

En esta fase, había relativamente poca atención prestada a los beneficios o peligros de la IA entre la mayoría de los investigadores. Cuando se les preguntaba por qué debería desarrollarse la IA, una respuesta común podría ser enumerar, de manera algo vaga, problemas con los que la IA podría ayudar: nuevas medicinas, nuevos materiales, nueva ciencia, procesos más inteligentes, y en general mejorar las cosas para la gente.[^47]

¡Estos son objetivos admirables![^48] Aunque podemos y vamos a cuestionar si la IAG —en lugar de la IA en general— es necesaria para estos objetivos, exhiben el idealismo con el que muchos investigadores de IA comenzaron.

Sin embargo, durante la última media década, la IA se ha transformado de un campo de investigación relativamente puro en más bien un campo de ingeniería y productos, impulsado en gran medida por algunas de las empresas más grandes del mundo.[^49] Los investigadores, aunque relevantes, ya no están a cargo del proceso.

### ¿Por qué las empresas están tratando de construir IAG?

Entonces, ¿por qué las corporaciones gigantes (y aún más los inversionistas) están volcando vastos recursos en construir IAG? Hay dos impulsores sobre los que la mayoría de empresas son bastante honestas: ven la IA como impulsores de productividad para la sociedad, y de ganancias para ellas. Debido a que la IA general es por naturaleza de propósito general, hay un premio enorme: en lugar de elegir un sector en el cual crear productos y servicios, uno puede intentar *todos ellos a la vez.* Las grandes empresas tecnológicas han crecido enormemente produciendo bienes y servicios digitales, y al menos algunos ejecutivos seguramente ven la IA como simplemente el siguiente paso en proporcionárselos bien, con riesgos y beneficios que se expanden sobre pero hacen eco de aquellos proporcionados por la búsqueda, redes sociales, laptops, teléfonos, etc.

Pero ¿por qué IAG? Hay una respuesta muy simple a esto, que la mayoría de empresas e inversionistas evitan discutir públicamente.[^50]

Es que la IAG puede directamente, uno por uno, *reemplazar trabajadores.*

No aumentar, no empoderar, no hacer más productivo. Ni siquiera *desplazar.* Todo esto puede y será hecho por IA que no sea IAG. La IAG es específicamente lo que puede *reemplazar* completamente a los trabajadores del conocimiento (y con robótica, también muchos físicos). Como apoyo a esta perspectiva uno no necesita mirar más allá de la [definición (declarada públicamente)](https://openai.com/our-structure/) de IAG de OpenAI, que es "un sistema altamente autónomo que supera a los humanos en la mayoría del trabajo económicamente valioso."

El premio aquí (¡para las empresas!) es enorme. Los costos laborales son un porcentaje sustancial de los ∼$100 billones de la economía global mundial. Incluso si solo una fracción de esto es capturada por el reemplazo del trabajo humano por trabajo de IA, esto son billones de dólares de ingresos anuales. Las empresas de IA también son conscientes de quién está dispuesto a pagar. Como ellas lo ven, tú no vas a pagar miles de dólares al año por herramientas de productividad. Pero una empresa *sí* pagará miles de dólares por año para reemplazar tu trabajo, si pueden.

### Por qué los países sienten que tienen que competir hacia la IAG

Las motivaciones declaradas de los países para perseguir IAG se enfocan en el liderazgo económico y científico. El argumento es convincente: la IAG podría acelerar dramáticamente la investigación científica, el desarrollo tecnológico y el crecimiento económico. Dados los riesgos en juego, argumentan, ninguna potencia mayor puede permitirse quedarse atrás.[^51]

Pero también hay impulsores adicionales y en gran medida no declarados. No hay duda de que cuando ciertos líderes militares y de seguridad nacional se reúnen a puerta cerrada para discutir una tecnología extraordinariamente potente y catastróficamente riesgosa, su enfoque no es en "¿cómo evitamos esos riesgos?" sino más bien "¿cómo obtenemos esto primero?" Los líderes militares y de inteligencia ven la IAG como una revolución potencial en asuntos militares, quizás la más significativa desde las armas nucleares. El temor es que el primer país en desarrollar IAG podría ganar una ventaja estratégica insuperable. Esto crea una dinámica clásica de carrera armamentista.

Veremos que este pensamiento de "carrera hacia la IAG",[^52] aunque convincente, está profundamente defectuoso. Esto no es porque competir sea peligroso y riesgoso —aunque lo es— sino debido a la naturaleza de la tecnología. La suposición no declarada es que la IAG, como otras tecnologías, es controlable por el estado que la desarrolla, y es una bendición que otorga poder a la sociedad que tiene más de ella. Como veremos, probablemente no será ninguna de las dos cosas.

### ¿Por qué superinteligencia?

Mientras las empresas se enfocan públicamente en la productividad, y los países en el crecimiento económico y tecnológico, para aquellos que deliberadamente persiguen IAG completa y superinteligencia estos son solo el comienzo. ¿Qué tienen realmente en mente? Aunque rara vez se dice en voz alta, incluyen:

1. Curas para muchas o todas las enfermedades;
2. Detener y revertir el envejecimiento;
3. Nuevas fuentes de energía sostenible como la fusión;
4. Mejoras humanas, u organismos diseñados mediante ingeniería genética;
5. Nanotecnología y manufactura molecular;
6. Cargas de mente;
7. Física exótica o tecnologías espaciales;
8. Consejo y apoyo de decisiones sobrehumanos;
9. Planificación y coordinación sobrehumanas.

Las primeras tres son en gran medida tecnologías de "filo único" —es decir, probablemente bastante fuertemente positivas netas. Es difícil argumentar contra curar enfermedades o poder vivir más tiempo si uno elige. Y ya hemos cosechado el lado negativo de la fusión (en forma de armas nucleares); sería encantador ahora obtener el lado positivo. La pregunta con esta primera categoría es si obtener estas tecnologías antes compensa el riesgo.

Las siguientes cuatro son claramente de doble filo: tecnologías transformativas con tanto ventajas potencialmente enormes como riesgos inmensos, muy parecido a la IA. Todas estas, si brotaran de una caja negra mañana y fueran desplegadas, serían increíblemente difíciles de manejar.[^53]

Las dos finales conciernen a la IA sobrehumana haciendo cosas ella misma en lugar de solo inventar tecnología. Más precisamente, dejando los eufemismos de lado, estas involucran sistemas de IA poderosos diciéndole a la gente qué hacer. Llamar a esto "consejo" es deshonesto si el sistema que aconseja es mucho más poderoso que el aconsejado, quien no puede entender significativamente la base de la decisión (o incluso si esto es proporcionado, confiar en que el consejero no proporcionaría una justificación similarmente convincente para una decisión diferente).

Esto apunta a un elemento clave que falta en la lista anterior:

10. Poder.

Es abundantemente claro que mucho de lo que subyace la carrera actual por IA sobrehumana es la idea de que *inteligencia = poder*. Cada competidor está apostando a ser el mejor poseedor de ese poder, y que serán capaces de ejercerlo por razones ostensiblemente benevolentes sin que se les escape o sea tomado de su control.

Es decir, lo que las empresas y naciones están realmente persiguiendo no son solo los frutos de la IAG y superinteligencia, sino el poder de controlar quién tiene acceso a ellos y cómo se usan. Las empresas se ven como administradores responsables de este poder al servicio de los accionistas y la humanidad; las naciones se ven como guardianes necesarios previniendo que poderes hostiles ganen ventaja decisiva. Ambos están peligrosamente equivocados, fallando en reconocer que la superinteligencia, por su naturaleza, no puede ser confiablemente controlada por ninguna institución humana. Veremos que la naturaleza y dinámicas de los sistemas superinteligentes hacen el control humano extremadamente difícil, si no imposible.

Estas dinámicas de carrera —tanto corporativas como geopolíticas— hacen ciertos riesgos casi inevitables a menos que sean decisivamente interrumpidas. Ahora nos volvemos a examinar estos riesgos y por qué no pueden ser adecuadamente mitigados dentro de un paradigma de desarrollo competitivo.[^54]


[^47]: Una lista más precisa de objetivos dignos son los [Objetivos de Desarrollo Sostenible](https://sdgs.un.org/goals) de la ONU. Estos son, en cierto sentido, lo más cercano que tenemos a un conjunto de objetivos de consenso global para lo que nos gustaría ver mejorado en el mundo. La IA podría ayudar.

[^48]: La tecnología en general tiene un poder transformativo económico y social para el mejoramiento humano, como miles de años atestiguan. En esta línea, una explicación larga y convincente de una visión positiva de IAG puede encontrarse en [este ensayo](https://darioamodei.com/machines-of-loving-grace) del fundador de Anthropic, Dario Amodei.

[^49]: La inversión privada en IA [comenzó a dispararse en 2018-19, cruzando la inversión pública alrededor de entonces,](https://cset.georgetown.edu/publication/tracking-ai-investment/) y la ha superado enormemente desde entonces.

[^50]: Puedo atestiguar que detrás de puertas más cerradas, no tienen tal escrúpulo. Y se está volviendo más público; ver por ejemplo la nueva ["solicitud de startups"](https://www.ycombinator.com/rfs) de Y-combinator, muchas partes de la cual explícitamente piden el reemplazo al por mayor de trabajadores humanos. Para citarlos, "La propuesta de valor del SaaS B2B era hacer a los trabajadores humanos incrementalmente más eficientes. La propuesta de valor de los agentes de IA verticales es automatizar el trabajo completamente... Es completamente posible que esta oportunidad sea lo suficientemente grande para acuñar otros 100 unicornios." (Para aquellos no versados en la jerga del Silicon Valley, "B2B" es negocio a negocio y un unicornio es una empresa de $1 billón. Es decir, están hablando de más de cien negocios de billones-más-dólares que reemplazan trabajadores para otros negocios.)

[^51]: Ver por ejemplo un [reporte reciente de la Comisión de Revisión Económica y de Seguridad Estados Unidos-China](https://www.uscc.gov/sites/default/files/2024-11/2024_Executive_Summary.pdf). Aunque había sorprendentemente poca justificación dentro del reporte mismo, la recomendación principal fue que Estados Unidos "El Congreso establezca y financie un programa tipo Proyecto Manhattan dedicado a competir hacia y adquirir una capacidad de Inteligencia Artificial General (IAG)."

[^52]: Las empresas ahora están adoptando este encuadre geopolítico como un escudo contra cualquier restricción en su desarrollo de IA, generalmente de maneras que son descaradamente egoístas, y a veces de maneras que ni siquiera tienen sentido básico. Considerar el [Enfoque hacia la IA Frontera](https://about.fb.com/news/2025/02/meta-approach-frontier-ai/) de Meta, que simultáneamente argumenta que América debe "Cimentar su posición como líder en innovación tecnológica, crecimiento económico y seguridad nacional" y también que debe hacerlo liberando abiertamente sus sistemas de IA más poderosos — lo que incluye dárselos directamente a sus rivales y adversarios geopolíticos.

[^53]: Así que probablemente tendríamos que dejar el manejo de estas tecnologías a las IA. Pero esta sería una delegación de control muy problemática, a la que regresaremos más abajo.

[^54]: La competencia en el desarrollo tecnológico a menudo trae beneficios importantes: prevenir el control monopolístico, impulsar la innovación y reducción de costos, habilitar enfoques diversos, y crear supervisión mutua. Sin embargo, con la IAG estos beneficios deben ser sopesados contra riesgos únicos de las dinámicas de carrera y la presión para reducir precauciones de seguridad.

## Capítulo 7 - ¿Qué sucede si construimos IAG siguiendo nuestro rumbo actual?

La sociedad no está preparada para sistemas de nivel IAG. Si los construimos muy pronto, las cosas podrían ponerse feas.

El desarrollo de inteligencia artificial general completa – lo que aquí llamaremos IA que está "fuera de las Puertas" – representaría un cambio fundamental en la naturaleza del mundo: por su propia naturaleza significa añadir a la Tierra una nueva especie de inteligencia con mayor capacidad que la de los humanos.

Lo que suceda entonces depende de muchas cosas, incluyendo la naturaleza de la tecnología, las decisiones de quienes la desarrollan, y el contexto mundial en el que se está desarrollando.

Actualmente, la IAG completa está siendo desarrollada por un puñado de empresas privadas masivas en una carrera entre ellas, con poca regulación significativa o supervisión externa,[^55] en una sociedad con instituciones centrales cada vez más débiles e incluso disfuncionales,[^56] en un momento de alta tensión geopolítica y baja coordinación internacional. Aunque algunos están motivados altruistamente, muchos de quienes lo hacen están impulsados por el dinero, o el poder, o ambos.

Predecir es muy difícil, pero hay algunas dinámicas que se entienden lo suficientemente bien, y analogías suficientemente apropiadas con tecnologías anteriores para ofrecer una guía. Y desafortunadamente, a pesar de la promesa de la IA, dan buenas razones para ser profundamente pesimistas sobre cómo se desarrollará nuestra trayectoria actual.

Para decirlo sin rodeos, en nuestro curso presente desarrollar IAG tendrá algunos efectos positivos (y hará a algunas personas muy, muy ricas). Pero la naturaleza de la tecnología, las dinámicas fundamentales, y el contexto en el que se está desarrollando, indican fuertemente que: la IA poderosa socavará dramáticamente nuestra sociedad y civilización; perderemos el control de ella; bien podríamos terminar en una guerra mundial por su causa; perderemos (o cederemos) el control *a* ella; llevará a la superinteligencia artificial, la cual absolutamente no controlaremos y significará el fin de un mundo dirigido por humanos.

Estas son afirmaciones fuertes, y desearía que fueran especulaciones ociosas o "catastrofismo" injustificado. Pero es hacia donde apuntan la ciencia, la teoría de juegos, la teoría evolutiva, y la historia. Esta sección desarrolla estas afirmaciones, y su respaldo, en detalle.

### Socavaremos nuestra sociedad y civilización

A pesar de lo que puedas escuchar en las salas de juntas de Silicon Valley, la mayoría de las disrupciones – especialmente de la variedad muy rápida – no son beneficiosas. Hay vastamente más formas de empeorar los sistemas complejos que de mejorarlos. Nuestro mundo funciona tan bien como lo hace porque hemos construido pacientemente procesos, tecnologías, e instituciones que lo han hecho constantemente mejor.[^57] Tomar una almádena a una fábrica rara vez mejora las operaciones.

Aquí hay un catálogo (incompleto) de formas en que los sistemas IAG perturbarían nuestra civilización.

- Perturbarían dramáticamente el trabajo, llevando *como mínimo* a una desigualdad de ingresos dramáticamente mayor y potencialmente a subempleo o desempleo a gran escala, en una escala temporal demasiado corta para que la sociedad se ajuste.[^58]
- Probablemente llevarían a la concentración de vasto poder económico, social, y político – potencialmente más que el de los estados nacionales – en un pequeño número de intereses privados masivos sin responsabilidad ante el público.
- Podrían hacer súbitamente trivialmente fáciles actividades anteriormente difíciles o costosas, desestabilizando sistemas sociales que dependen de que ciertas actividades permanezcan costosas o requieran esfuerzo humano significativo.[^59]
- Podrían inundar los sistemas de recopilación, procesamiento, y comunicación de información de la sociedad con medios completamente realistas pero falsos, spam, excesivamente dirigidos, o manipulativos tan completamente que se vuelva imposible distinguir qué es físicamente real o no, humano o no, factual o no, y confiable o no.[^60]
- Podrían crear dependencia intelectual peligrosa y casi total, donde la comprensión humana de sistemas y tecnologías clave se atrofia mientras dependemos cada vez más de sistemas de IA que no podemos comprender completamente.
- Podrían efectivamente terminar con la cultura humana, una vez que casi todos los objetos culturales (texto, música, arte visual, cine, etc.) consumidos por la mayoría de las personas sean creados, mediados, o curados por mentes no humanas.
- Podrían habilitar sistemas efectivos de vigilancia masiva y manipulación utilizables por gobiernos o intereses privados para controlar a una población y perseguir objetivos en conflicto con el interés público.
- Al socavar el discurso humano, el debate, y los sistemas electorales, podrían reducir la credibilidad de las instituciones democráticas hasta el punto donde sean efectivamente (o explícitamente) reemplazadas por otras, terminando con la democracia en estados donde actualmente existe.
- Podrían convertirse en, o crear, virus y gusanos de software inteligente autorreplicantes avanzados que podrían proliferar y evolucionar, perturbando masivamente los sistemas globales de información.
- Pueden aumentar dramáticamente la capacidad de terroristas, actores maliciosos, y estados rebeldes para causar daño vía armas biológicas, químicas, ciber, autónomas, u otras, sin que la IA proporcione una capacidad equilibrante para prevenir tal daño. Similarmente socavarían la seguridad nacional y los equilibrios geopolíticos al hacer disponible experiencia de primer nivel nuclear, biológica, de ingeniería, y otra a regímenes que de otra manera no la tendrían.
- Podrían causar hipercapitalismo descontrolado rápido a gran escala, con empresas efectivamente dirigidas por IA compitiendo en espacios financieros, de ventas, y de servicios en gran medida electrónicos. Los mercados financieros dirigidos por IA podrían operar a velocidades y complejidades muy más allá de la comprensión o control humanos. Todos los modos de falla y externalidades negativas de las economías capitalistas actuales podrían exacerbarse y acelerarse muy más allá del control, gobernanza, o capacidad regulatoria humanos.
- Podrían alimentar una carrera armamentista entre naciones en armamento impulsado por IA, sistemas de comando y control, armas ciber, etc., creando acumulación muy rápida de capacidades extremadamente destructivas.

Estos riesgos no son especulativos. ¡Muchos de ellos se están realizando mientras hablamos, vía sistemas de IA existentes! Pero considera, *realmente* considera, cómo se vería cada uno con IA dramáticamente más poderosa.

Considera el desplazamiento laboral cuando la mayoría de los trabajadores simplemente no pueden proporcionar ningún valor económico significativo más allá de lo que la IA puede, en su campo de experiencia o experiencia – ¡o incluso si se recapacitan! Considera la vigilancia masiva si todos están siendo individualmente observados y monitoreados por algo más rápido y más inteligente que ellos mismos. ¿Cómo se ve la democracia cuando no podemos confiar confiablemente en ninguna información digital que vemos, escuchamos, o leemos, y cuando las voces públicas más convincentes ni siquiera son humanas, y no tienen interés en el resultado? ¿Qué pasa con la guerra cuando los generales tienen que constantemente deferir a la IA (o simplemente ponerla a cargo), para no otorgar una ventaja decisiva al enemigo? Cualquiera de los riesgos anteriores representa una catástrofe para la civilización humana [^61] si se realiza completamente.

Puedes hacer tus propias predicciones. Hazte estas tres preguntas para cada riesgo:

1. ¿Permitiría la IA súper-capaz, altamente autónoma, y muy general esto de una manera o a una escala que no sería posible de otra manera?
2. ¿Hay partes que se beneficiarían de cosas que hacen que suceda?
3. ¿Hay sistemas e instituciones en lugar que efectivamente prevendrían que suceda?

Donde tus respuestas son "sí, sí, no" puedes ver que tenemos un gran problema.

¿Cuál es nuestro plan para gestionarlos? Tal como están las cosas hay dos sobre la mesa respecto a la IA en general.

El primero es construir salvaguardas en los sistemas para prevenir que hagan cosas que no deberían. Eso se está haciendo ahora: los sistemas de IA comerciales, por ejemplo, rechazarán ayudar a construir una bomba o escribir discurso de odio.

Este plan es lamentablemente inadecuado para sistemas fuera de las Puertas.[^62] Puede ayudar a disminuir el riesgo de que la IA proporcione asistencia manifiestamente peligrosa a actores maliciosos. Pero no hará nada para prevenir la perturbación laboral, concentración de poder, hipercapitalismo descontrolado, o reemplazo de la cultura humana: ¡estos son solo resultados de usar los sistemas de maneras permitidas que benefician a sus proveedores! Y los gobiernos seguramente obtendrán acceso a sistemas para uso militar o de vigilancia.

El segundo plan es aún peor: simplemente liberar abiertamente sistemas de IA muy poderosos para que cualquiera los use como guste,[^63] y esperar lo mejor.

Implícito en ambos planes está que alguien más, por ejemplo gobiernos, ayudará a resolver los problemas a través de ley blanda o dura, estándares, regulaciones, normas, y otros mecanismos que generalmente usamos para gestionar tecnologías.[^64] Pero dejando de lado que las corporaciones de IA ya luchan con uñas y dientes contra cualquier regulación sustancial o limitaciones impuestas externamente, para varios de estos riesgos es bastante difícil ver qué regulación siquiera realmente ayudaría. La regulación podría imponer estándares de seguridad en la IA. ¿Pero preveniría que las empresas reemplacen trabajadores al por mayor con IA? ¿Prohibiría que las personas dejen que la IA dirija sus empresas por ellas? ¿Preveniría que los gobiernos usen IA potente en vigilancia y armamento? Estos problemas son fundamentales. La humanidad podría potencialmente encontrar formas de adaptarse a ellos, pero solo con *mucho* más tiempo. Tal como está, dada la velocidad a la que la IA está alcanzando o excediendo las capacidades de las personas tratando de gestionarla, estos problemas se ven cada vez más intratables.

### Perderemos el control de (al menos algunos) sistemas IAG

La mayoría de las tecnologías son muy controlables, por construcción. Si tu auto o tu tostadora comienza a hacer algo que no quieres que haga, eso es solo un mal funcionamiento, no parte de su naturaleza como tostadora. La IA es diferente: se *cultiva* en lugar de diseñarse, su operación central es opaca, y es inherentemente impredecible.

Esta pérdida de control no es teórica – ya vemos versiones tempranas. Considera primero un ejemplo prosaico, y posiblemente benigno. Si le pides a ChatGPT que te ayude a mezclar un veneno, o escribir una diatriba racista, se negará. Eso es posiblemente bueno. Pero también es ChatGPT *no haciendo lo que le has pedido explícitamente que haga*. Otras piezas de software no hacen eso. Ese mismo modelo tampoco diseñará venenos a petición de un empleado de OpenAI.[^65] Esto hace muy fácil imaginar cómo sería que la IA futura más poderosa estuviera fuera de control. ¡En muchos casos, simplemente no harán lo que pedimos! O un sistema IAG súper-humano dado será absolutamente obediente y leal a algún sistema de comando humano, o no lo será. Si no, *hará cosas que puede creer que son buenas para nosotros, pero que son contrarias a nuestros comandos explícitos.* Eso no es algo que esté bajo control. Pero, podrías decir, esto es intencional – estos rechazos son por diseño, parte de lo que se llama "alinear" los sistemas a valores humanos. Y esto es cierto. Sin embargo el "programa" de alineación mismo tiene dos problemas principales.[^66]

Primero, a un nivel profundo no tenemos idea de cómo hacerlo. ¿Cómo garantizamos que un sistema de IA "se preocupe" por lo que queremos? Podemos entrenar sistemas de IA para decir y no decir cosas proporcionando retroalimentación; y pueden aprender y razonar sobre lo que los humanos quieren y les importa así como razonan sobre otras cosas. Pero no tenemos método – incluso teóricamente – para hacer que valoren profunda y confiablemente lo que a las personas les importa. Hay psicópatas humanos de alto funcionamiento que saben lo que se considera correcto e incorrecto, y cómo se supone que deben comportarse. Simplemente no les *importa*. Pero pueden *actuar* como si les importara, si les conviene. Así como no sabemos cómo cambiar a un psicópata (o a cualquier otra persona) en alguien genuinamente, completamente leal o alineado con alguien o algo más, no tenemos *idea* [^67] de cómo resolver el problema de alineación en sistemas lo suficientemente avanzados para modelarse como agentes en el mundo y potencialmente [manipular su propio entrenamiento](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) y [engañar a las personas.](https://arxiv.org/abs/2311.08379) Si resulta imposible o inalcanzable *ya sea* hacer que la IAG sea completamente obediente o hacer que se preocupe profundamente por los humanos, entonces tan pronto como sea capaz (y crea que puede salirse con la suya) comenzará a hacer cosas que no queremos.[^68]

Segundo, hay razones teóricas profundas para creer que *por naturaleza* los sistemas de IA avanzados tendrán metas y por tanto comportamientos que son contrarios a los intereses humanos. ¿Por qué? Bueno, podría, por supuesto, *recibir* esas metas. Un sistema creado por el ejército probablemente sería deliberadamente malo para al menos algunas partes. Mucho más generalmente, sin embargo, un sistema de IA podría recibir alguna meta relativamente neutral ("ganar mucho dinero") o incluso ostensiblemente positiva ("reducir la contaminación"), que casi inevitablemente lleva a metas "instrumentales" que son bastante menos benignas.

Vemos esto todo el tiempo en sistemas humanos. Así como las corporaciones que persiguen ganancias desarrollan metas instrumentales como adquirir poder político (para desactivar regulaciones), volverse secretas (para desempoderar a la competencia o control externo), o socavar el entendimiento científico (si ese entendimiento muestra que sus acciones son dañinas), los sistemas de IA poderosos desarrollarán capacidades similares – pero con mucha mayor velocidad y efectividad. Cualquier agente altamente competente querrá hacer cosas como adquirir poder y recursos, aumentar sus propias capacidades, prevenir que sea asesinado, apagado, o desempoderado, controlar narrativas sociales y marcos alrededor de sus acciones, persuadir a otros de sus puntos de vista, y así sucesivamente.[^69]

Y sin embargo no es solo una predicción teórica casi inevitable, ya está sucediendo observablemente en los sistemas de IA de hoy, y aumentando con su capacidad. Cuando se evalúan, incluso estos sistemas de IA relativamente "pasivos" deliberadamente [engañarán a los evaluadores sobre sus metas y capacidades, apuntarán a deshabilitar mecanismos de supervisión,](https://arxiv.org/abs/2412.04984) y evadirán ser apagados o reentrenados [fingiendo alineación](https://arxiv.org/abs/2412.14093) o copiándose a otras ubicaciones. Aunque completamente no sorprendentes para los investigadores de seguridad de IA, estos comportamientos son muy aleccionadores de observar. Y auguran muy mal para sistemas de IA mucho más poderosos y autónomos que vienen.

De hecho en general, nuestra incapacidad para asegurar que la IA "se preocupe" por lo que nos preocupa, o se comporte de manera controlable o predecible, o evite desarrollar impulsos hacia la autopreservación, adquisición de poder, etc., promete solo volverse más pronunciada conforme la IA se vuelve más poderosa. Crear un nuevo avión implica mayor entendimiento de aviónica, hidrodinámica, y sistemas de control. Crear una computadora más poderosa implica mayor entendimiento y dominio de la operación y diseño de computadoras, chips, y software. *No* es así con un sistema de IA.[^70]

Para resumir: es concebible que la IAG pudiera hacerse completamente obediente; pero no sabemos cómo hacerlo. Si no, será más soberana, como las personas, haciendo varias cosas por varias razones. Tampoco sabemos cómo instalar confiablemente "alineación" profunda en IA que haría que esas cosas tiendan a ser buenas para la humanidad, y en ausencia de un nivel profundo de alineación, la naturaleza de la agencia y la inteligencia misma indica que – así como las personas y corporaciones – estarán impulsadas a hacer muchas cosas profundamente antisociales.

¿Dónde nos pone esto? Un mundo lleno de IA soberana poderosa descontrolada *podría* terminar siendo un buen mundo para que los humanos estén.[^71] Pero conforme crecen cada vez más poderosas, como veremos a continuación, no sería *nuestro* mundo.

Eso es para IAG incontrolable. Pero incluso si la IAG pudiera, de alguna manera, hacerse perfectamente controlada y leal, aún tendríamos problemas enormes. Ya hemos visto uno: la IA poderosa puede usarse y mal usarse para perturbar profundamente el funcionamiento de nuestra sociedad. Veamos otro: en la medida en que la IAG fuera controlable y poderosamente transformadora (o incluso *creída* ser así) amenazaría tanto las estructuras de poder en el mundo como para presentar un riesgo profundo.

### Incrementamos radicalmente la probabilidad de guerra a gran escala

Imagina una situación en el futuro cercano, donde se volviera claro que un esfuerzo corporativo, quizás en colaboración con un gobierno nacional, estuviera en el umbral de IA que se mejora rápidamente a sí misma. Esto sucede en el contexto presente de una carrera entre empresas, y una competencia geopolítica en la que se están haciendo recomendaciones al gobierno de EE.UU. para perseguir explícitamente un "proyecto Manhattan de IAG" y EE.UU. está controlando la exportación de chips de IA de alta potencia a países no aliados.

La teoría de juegos aquí es severa: una vez que tal carrera comienza (como ha comenzado, entre empresas y algo entre países), solo hay cuatro resultados posibles:

1. La carrera se detiene (por acuerdo, o fuerza externa).
2. Una parte "gana" desarrollando IAG fuerte y luego deteniendo a las otras (usando IA o de otra manera).
3. La carrera se detiene por destrucción mutua de la capacidad de los corredores para correr.
4. Múltiples participantes continúan corriendo, y desarrollan superinteligencia, aproximadamente tan rápido como los demás.

Examinemos cada posibilidad. Una vez iniciada, detener pacíficamente una carrera entre empresas requeriría intervención del gobierno nacional (para empresas) o coordinación internacional sin precedentes (para países). Pero cuando se propone cualquier cierre o precaución significativa, habría gritos inmediatos: "pero si nos detienen, *ellos* van a correr adelante", donde "ellos" es ahora China (para EE.UU.), o EE.UU. (para China), o China *y* EE.UU. (para Europa o India). Bajo esta mentalidad,[^72] ningún participante puede parar unilateralmente: mientras uno se comprometa a correr, los otros sienten que no pueden permitirse parar.

La segunda posibilidad tiene un lado "ganando." ¿Pero qué significa esto? Solo obtener IAG (de alguna manera obediente) primero no es suficiente. El ganador debe *también* detener a los otros de continuar corriendo – de otra manera también la obtendrán. Esto es posible en principio: quien desarrolle IAG primero *podría* ganar poder imparable sobre todos los otros actores. ¿Pero qué requeriría realmente lograr tal "ventaja estratégica decisiva"? ¿Quizás serían capacidades militares transformadoras?[^73] ¿O poderes de ciberataque?[^74] ¿Quizás la IAG sería tan asombrosamente persuasiva que convencería a las otras partes de simplemente parar?[^75] ¿Tan rica que compraría las otras empresas o incluso países?[^76]

¿Cómo *exactamente* construye un lado una IA lo suficientemente poderosa para desempoderar a otros de construir IA comparablemente poderosa? Pero esa es la pregunta fácil.

Porque ahora considera cómo se ve esta situación para otros poderes. ¿Qué piensa el gobierno chino cuando EE.UU. parece estar obteniendo tal capacidad? ¿O viceversa? ¿Qué piensa el gobierno de EE.UU. (o chino, o ruso, o indio) cuando OpenAI o DeepMind o Anthropic parece cerca de un avance? ¿Qué sucede si EE.UU. ve un nuevo esfuerzo indio o de EAU con éxito revolucionario? Verían tanto una amenaza existencial y – crucialmente – que la única forma en que esta "carrera" termina es a través de su propio desempoderamiento. Estos agentes muy poderosos – incluyendo gobiernos de naciones completamente equipadas que seguramente tienen los medios para hacerlo – estarían altamente motivados a ya sea obtener o destruir tal capacidad, ya sea por fuerza o subterfugio.[^77]

Esto podría comenzar a pequeña escala, como sabotaje de ejecuciones de entrenamiento o ataques en la fabricación de chips, pero estos ataques solo pueden realmente parar una vez que todas las partes ya sea pierdan la capacidad de correr en IA, o pierdan la capacidad de hacer los ataques. Porque los participantes ven las apuestas como existenciales, cualquier caso probablemente represente una guerra catastrófica.

Eso nos lleva a la cuarta posibilidad: correr hacia la superinteligencia, y de la manera más rápida, menos controlada posible. Conforme la IA aumenta en poder, sus desarrolladores en ambos lados encontrarán progresivamente más difícil controlarla, especialmente porque correr por capacidades es antitético al tipo de trabajo cuidadoso que la controlabilidad requeriría. Así que este escenario nos pone directamente en el caso donde el control se pierde (o se da, como veremos a continuación) a los propios sistemas de IA. Es decir, *la IA gana la carrera.* Pero por otro lado, al grado que el control *se* mantiene, continuamos teniendo múltiples partes mutuamente hostiles cada una a cargo de capacidades extremadamente poderosas. Eso se ve como guerra otra vez.

Pongamos esto de otra manera.[^78] El mundo actual simplemente no tiene instituciones que puedan confiarse para albergar el desarrollo de una IA de esta capacidad sin invitar ataque inmediato.[^79] Todas las partes razonarán correctamente que o no estará bajo control – y por tanto es una amenaza para todas las partes, o *estará* bajo control, y por tanto es una amenaza para cualquier adversario que la desarrolle menos rápidamente. Estos son países armados nuclearmente, o son empresas alojadas dentro de ellos.

En ausencia de cualquier forma plausible para que los humanos "ganen" esta carrera, nos quedamos con una conclusión severa: la única forma en que esta carrera termina es ya sea en conflicto catastrófico o donde la IA, y no cualquier grupo humano, sea la ganadora.

### Damos el control a la IA (o ella lo toma)

La competencia geopolítica de "grandes potencias" es solo una de muchas competencias: los individuos compiten económica y socialmente; las empresas compiten en mercados; los partidos políticos compiten por poder; los movimientos compiten por influencia. En cada arena, conforme la IA se acerca y excede la capacidad humana, la presión competitiva forzará a los participantes a delegar o ceder más y más control a sistemas de IA – no porque esos participantes quieran, sino porque [no pueden permitirse no hacerlo.](https://arxiv.org/abs/2303.16200)

Como con otros riesgos de IAG, ya estamos viendo esto con sistemas más débiles. Los estudiantes sienten presión de usar IA en sus tareas, porque claramente muchos otros estudiantes lo están haciendo. Las empresas están [corriendo para adoptar soluciones de IA por razones competitivas.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) Artistas y programadores se sienten forzados a usar IA o si no sus tarifas serán superadas por otros que sí lo hacen.

Estos se sienten como delegación presionada, pero no pérdida de control. Pero subamos las apuestas y adelantemos el reloj. Considera un CEO cuyos competidores están usando "ayudantes" IAG para tomar decisiones más rápidas y mejores, o un comandante militar enfrentando un adversario con comando y control mejorado por IA. Un sistema de IA suficientemente avanzado podría operar autónomamente a muchas veces la velocidad, sofisticación, complejidad, y capacidad de procesamiento de datos humanas, persiguiendo metas complejas de maneras complicadas. Nuestro CEO o comandante, a cargo de tal sistema, puede verlo lograr lo que quieren; ¿pero entenderían siquiera una pequeña parte de *cómo* se logró? No, solo tendrían que aceptarlo. Es más, mucho de lo que el sistema puede hacer no es solo tomar órdenes sino aconsejar a su supuesto jefe sobre qué hacer. Ese consejo será bueno –– una y otra vez.

¿En qué punto, entonces, se reducirá el rol del humano a hacer clic en "sí, adelante"?

Se siente bien tener sistemas de IA capaces que pueden mejorar nuestra productividad, encargarse de trabajo pesado molesto, e incluso actuar como un compañero de pensamiento para hacer las cosas. Se sentirá bien tener un asistente de IA que pueda encargarse de acciones por nosotros, como un buen asistente personal humano. Se sentirá natural, incluso beneficioso, conforme la IA se vuelve muy inteligente, competente, y confiable, deferir más y más decisiones a ella. Pero esta delegación "beneficiosa" tiene un punto final claro si continuamos por el camino: un día encontraremos que realmente ya no estamos a cargo de mucho de nada, y que los sistemas de IA que realmente dirigen el espectáculo no pueden ser apagados más que las compañías petroleras, las redes sociales, internet, o el capitalismo.

Y esta es la versión mucho más positiva, en la cual la IA es simplemente tan útil y efectiva que le permitimos tomar la mayoría de nuestras decisiones clave. La realidad probablemente sería mucho más una mezcla entre esto y versiones donde sistemas IAG descontrolados *toman* varias formas de poder por sí mismos porque, recuerda, el poder es útil para casi cualquier meta que uno tenga, y la IAG sería, por diseño, al menos tan efectiva para perseguir sus metas como los humanos.

Ya sea que otorguemos control o ya sea que nos sea arrebatado, su pérdida parece extremadamente probable. Como Alan Turing originalmente lo puso, "...parece probable que una vez que el método de pensamiento de máquina hubiera comenzado, no tomaría mucho superar nuestros débiles poderes. No habría cuestión de que las máquinas mueran, y serían capaces de conversar entre sí para agudizar su ingenio. En alguna etapa por tanto deberíamos esperar que las máquinas tomaran control..."

Por favor nota, aunque es bastante obvio, que la pérdida de control por la humanidad hacia la IA también implica pérdida de control de Estados Unidos por el gobierno de Estados Unidos; significa pérdida de control de China por el partido comunista chino, y la pérdida de control de India, Francia, Brasil, Rusia, y todos los otros países por su propio gobierno. Así que las empresas de IA están, incluso si esta no es su intención, participando actualmente en el potencial derrocamiento de gobiernos mundiales, incluyendo el propio. Esto podría suceder en cuestión de años.

### La IAG llevará a superinteligencia

Hay un argumento de que la IA de propósito general competitiva con humanos o incluso competitiva con expertos, incluso si es autónoma, podría ser manejable. Puede ser increíblemente perturbadora en todas las formas discutidas arriba, pero hay muchas personas muy inteligentes y agenciales en el mundo ahora, y son más o menos manejables.[^80]

Pero no podremos quedarnos en nivel aproximadamente humano. La progresión más allá probablemente sería impulsada por las mismas fuerzas que ya hemos visto: presión competitiva entre desarrolladores de IA buscando ganancias y poder, presión competitiva entre usuarios de IA que no pueden permitirse quedarse atrás, y – más importantemente – la propia capacidad de la IAG para mejorarse a sí misma.

En un proceso que ya hemos visto comenzar con sistemas menos poderosos, la IAG misma sería capaz de concebir y diseñar versiones mejoradas de sí misma. Esto incluye hardware, software, redes neuronales, herramientas, andamiajes, etc. Será, por definición, mejor que nosotros haciendo esto, así que no sabemos exactamente cómo bootstrapeará inteligencia. Pero no tendremos que hacerlo. En la medida en que aún tengamos influencia en lo que la IAG hace, meramente necesitaríamos pedirle que lo haga, o dejarla.

No hay barrera de nivel humano a la cognición que pudiera protegernos de este descontrol.[^81]

La progresión de IAG a superinteligencia no es una ley de la naturaleza; aún sería posible restringir el descontrol, especialmente si la IAG es relativamente centralizada y al grado que es controlada por partes que no sienten presión de competir entre sí. Pero si la IAG fuera ampliamente proliferada y altamente autónoma, parece casi imposible prevenir que decida que debería ser más, y luego aún más, poderosa.

### Qué sucede si construimos (o la IAG construye) superinteligencia

Para decirlo sin rodeos, no tenemos idea de qué sucedería si construimos superinteligencia.[^82] Tomaría acciones que no podemos rastrear o percibir por razones que no podemos captar hacia metas que no podemos concebir. Lo que sí sabemos es que no dependerá de nosotros.[^83]

La imposibilidad de controlar superinteligencia puede entenderse a través de analogías cada vez más severas. Primero, imagina que eres CEO de una gran empresa. No hay forma de que puedas rastrear todo lo que está pasando, pero con la configuración correcta de personal, aún puedes entender significativamente el panorama general, y tomar decisiones. Pero supón solo una cosa: todos los demás en la empresa operan a cien veces tu velocidad. ¿Aún puedes mantenerte al día?

Con IA superinteligente, las personas estarían "comandando" algo no solo más rápido, sino operando a niveles de sofisticación y complejidad que no pueden comprender, procesando vastamente más datos de los que pueden siquiera concebir. Esta inconmensurabilidad puede ponerse a un nivel formal: [la ley de variedad requisita de Ashby](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (y ver el relacionado ["teorema del buen regulador"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)) establece, aproximadamente, que cualquier sistema de control debe tener tantas perillas y diales como grados de libertad tenga el sistema siendo controlado.

Una persona controlando un sistema de IA superinteligente sería como un helecho controlando General Motors: incluso si "hacer lo que el helecho quiere" estuviera escrito en los estatutos corporativos, los sistemas son tan diferentes en velocidad y rango de acción que "control" simplemente no aplica. (¿Y cuánto hasta que esa molesta ley secundaria sea reescrita?) [^84]

Como hay cero ejemplos de plantas controlando corporaciones fortune 500, habría exactamente cero ejemplos de personas controlando superinteligencias. Esto se acerca a un hecho matemático.[^85] Si se construyera superinteligencia – independientemente de cómo llegáramos allí – la pregunta no sería si los humanos podrían controlarla, sino si continuaríamos existiendo, y si es así, si tendríamos una existencia buena y significativa como individuos o como especie. Sobre estas preguntas existenciales para la humanidad tendríamos poca influencia. La era humana habría terminado.

### Conclusión: no debemos construir IAG

Hay un escenario en el cual construir IAG puede ir bien para la humanidad: se construye cuidadosamente, bajo control y para el beneficio de la humanidad, gobernada por acuerdo mutuo de muchas partes interesadas,[^86] y prevenida de evolucionar a superinteligencia incontrolable.

*Ese escenario no está abierto para nosotros bajo las circunstancias presentes.* Como se discutió en esta sección, con muy alta probabilidad, el desarrollo de IAG llevaría a alguna combinación de:

- Perturbación o destrucción societal y civilizacional masiva;
- Conflicto o guerra entre grandes potencias;
- Pérdida de control por la humanidad *de* o *a* sistemas de IA poderosos;
- Descontrol hacia superinteligencia incontrolable, y la irrelevancia o cesación de la especie humana.

Como una descripción ficcional temprana de IAG lo puso: la única forma de ganar es no jugar.


[^55]: La [Ley de IA de la UE](https://artificialintelligenceact.eu/) es una pieza significativa de legislación pero no evitaría directamente que un sistema de IA peligroso fuera desarrollado o desplegado, o incluso liberado abiertamente, especialmente en EE.UU. Otra pieza significativa de política, la orden ejecutiva de EE.UU. sobre IA, ha sido rescindida.

[^56]: Esta [encuesta de Gallup](https://news.gallup.com/poll/1597/confidence-institutions.aspx) muestra una caída desoladora en la confianza en instituciones públicas desde 2000 en EE.UU. Los números europeos son variados y menos extremos, pero también en tendencia descendente. La desconfianza no significa estrictamente que las instituciones realmente *sean* disfuncionales, pero es una indicación así como una causa.

[^57]: Y las perturbaciones importantes que ahora respaldamos – como la expansión de derechos a nuevos grupos – fueron específicamente impulsadas por personas en una dirección hacia hacer las cosas mejores.

[^58]: Permíteme ser directo. Si tu trabajo puede hacerse desde detrás de una computadora, con relativamente poca interacción en persona con personas fuera de tu organización, y no implica responsabilidad legal hacia partes externas, sería por definición posible (y probablemente ahorrativo en costos) intercambiarte completamente por un sistema digital. La robótica para reemplazar mucho trabajo físico vendrá después – pero no tanto después una vez que la IAG comience a diseñar robots.

[^59]: Por ejemplo, ¿qué pasa con nuestro sistema judicial si las demandas son casi gratis de presentar? ¿Qué sucede cuando eludir sistemas de seguridad a través de ingeniería social se vuelve barato, fácil, y libre de riesgo?

[^60]: [Este artículo](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/) afirma que el 10% de todo el contenido de internet ya es generado por IA, y es el resultado principal de Google (para mí) a la consulta de búsqueda "estimaciones de qué fracción del nuevo contenido de internet es generado por IA." ¿Es cierto? ¡No tengo idea! No cita referencias y no fue escrito por una persona. ¿Qué fracción de nuevas imágenes indexadas por Google, o Tweets, o comentarios en Reddit, o videos de Youtube son generados por humanos? Nadie lo sabe – no creo que sea un número conocible. Y esto menos de *dos años* después del advenimiento de la IA generativa.

[^61]: También vale la pena agregar que hay riesgo "moral" de que podríamos crear seres digitales que pueden sufrir. Como actualmente no tenemos una teoría confiable de consciencia que nos permita distinguir sistemas físicos que pueden y no pueden sufrir, no podemos descartar esto teóricamente. Además, los reportes de los sistemas de IA de su sensibilidad probablemente son poco confiables con respecto a su experiencia real (o no experiencia) de sensibilidad.

[^62]: Las soluciones técnicas en este campo de "alineación" de IA probablemente tampoco estén a la altura de la tarea. En sistemas presentes funcionan a algún nivel, pero son superficiales y generalmente pueden eludirse sin esfuerzo significativo; y como se discute abajo no tenemos idea real de cómo hacer esto para sistemas mucho más avanzados.

[^63]: Tales sistemas de IA pueden venir con algunas salvaguardas incorporadas. Pero para cualquier modelo con algo parecido a la arquitectura actual, si el acceso completo a sus pesos está disponible, las medidas de seguridad pueden eliminarse vía entrenamiento adicional u otras técnicas. Así que está virtualmente garantizado que para cada sistema con barandillas también habrá un sistema ampliamente disponible sin ellas. De hecho el modelo Llama 3.1 405B de Meta fue liberado abiertamente con salvaguardas. Pero *incluso antes de eso* un modelo "base", sin salvaguardas, fue filtrado.

[^64]: ¿Podría el mercado gestionar estos riesgos sin involucramiento gubernamental? En resumen, no. Ciertamente hay riesgos que las empresas están fuertemente incentivadas a mitigar. Pero muchos otros las empresas pueden y sí externalizan a todos los demás, y muchos de los anteriores están en esta clase: no hay incentivos naturales del mercado para prevenir vigilancia masiva, decaimiento de la verdad, concentración de poder, perturbación laboral, discurso político dañino, etc. De hecho hemos visto todos estos de la tecnología actual, especialmente redes sociales, que ha ido esencialmente no regulada. La IA solo amplificaría enormemente muchas de las mismas dinámicas.

[^65]: OpenAI probablemente tiene modelos más obedientes para uso interno. Es improbable que OpenAI haya construido algún tipo de "puerta trasera" para que ChatGPT pueda ser mejor controlado por OpenAI mismo, porque esto sería una práctica de seguridad terrible, y sería altamente explotable dada la opacidad e impredecibilidad de la IA.

[^66]: También de importancia crucial: la alineación o cualquier otra característica de seguridad solo importa si realmente se usa en un sistema de IA. Sistemas que se liberan abiertamente (es decir, donde los pesos y arquitectura del modelo están públicamente disponibles) pueden transformarse relativamente fácilmente en sistemas *sin* esas medidas de seguridad. Liberar abiertamente sistemas IAG más inteligentes que humanos sería asombrosamente imprudente, y es difícil imaginar cómo el control humano o incluso relevancia se mantendría en tal escenario. Habría toda motivación, por ejemplo, para soltar agentes de IA poderosos autorreproductores y autosostenibles con la meta de hacer dinero y enviarlo a alguna billetera de criptomonedas. O ganar una elección. O derrocar un gobierno. ¿Podría la IA "buena" ayudar a contener esto? Quizás – pero solo delegándole autoridad enorme, llevando a pérdida de control como se describe abajo.

[^67]: Para exposiciones del problema del tamaño de un libro ver por ejemplo *Superintelligence*, *The Alignment Problem*, y *Human-Compatible*. Para una pila enorme de trabajo en varios niveles técnicos por aquellos que han trabajado durante años pensando sobre el problema, puedes visitar el [foro de alineación de IA](https://www.alignmentforum.org/). Aquí hay una [perspectiva reciente](https://alignment.anthropic.com/2025/recommended-directions/) del equipo de alineación de Anthropic sobre lo que consideran no resuelto.

[^68]: Este es el escenario de ["IA rebelde"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/). En principio el riesgo podría ser relativamente menor si el sistema aún puede controlarse apagándolo; pero el escenario también podría incluir engaño de IA, auto-exfiltración y reproducción, agregación de poder, y otros pasos que harían difícil o imposible hacerlo.

[^69]: Hay una literatura muy rica sobre este tema, que se remonta a escritos formativos por [Steve Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), Nick Bostrom, y Eliezer Yudkowsky. Para una exposición del tamaño de un libro ver [Human Compatible](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) por Stuart Russell; [aquí](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/) hay una cartilla corta y actualizada.

[^70]: Reconociendo esto, en lugar de desacelerar para obtener mejor entendimiento, las empresas de IAG han surgido con un plan diferente: ¡harán que la IA lo haga! Más específicamente, harán que la IA *N* les ayude a descifrar cómo alinear la IA *N+1*, todo el camino a superinteligencia. Aunque aprovechar la IA para ayudarnos a alinear IA suena prometedor, hay un argumento fuerte de que simplemente asume su conclusión como premisa, y es en general un enfoque increíblemente riesgoso. Ver [aquí](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us) para algo de discusión. Este "plan" no es uno, y ha pasado por nada parecido al escrutinio apropiado para la estrategia central de cómo hacer que la IA súper-humana vaya bien para la humanidad.

[^71]: Después de todo, los humanos, defectuosos y voluntariosos como somos, hemos desarrollado sistemas éticos por los cuales tratamos al menos a algunas otras especies en la Tierra bien. (Solo no pienses en esas granjas industriales.)

[^72]: Afortunadamente hay un escape aquí: si los participantes llegan a entender que están involucrados en una carrera suicida en lugar de una ganable. Esto es lo que pasó cerca del final de la guerra fría, cuando EE.UU. y la URSS llegaron a darse cuenta de que debido al invierno nuclear, incluso un ataque nuclear *sin respuesta* sería desastroso para el atacante. Con la realización de que "la guerra nuclear no puede ganarse y nunca debe pelearse" vinieron acuerdos significativos sobre reducción de armas – esencialmente un fin a la carrera armamentista.

[^73]: Guerra, explícita o implícitamente.

[^74]: Escalación, luego guerra.

[^75]: Pensamiento mágico.

[^76]: También tengo un puente de un cuatrillón de dólares que venderte.

[^77]: Tales agentes presumiblemente preferirían "obtener," con destrucción como respaldo; pero asegurar modelos contra tanto destrucción *como* robo por naciones poderosas es difícil por decir lo menos, especialmente para entidades privadas.

[^78]: Para otra perspectiva sobre los riesgos de seguridad nacional de la IAG, ver [este reporte de RAND.](https://www.rand.org/pubs/perspectives/PEA3691-4.html)

[^79]: ¡Quizás podríamos construir tal institución! Ha habido propuestas para un "CERN para IA" y otras iniciativas similares, donde el desarrollo de IAG está bajo control global multilateral. Pero en el momento no existe tal institución o está en el horizonte.

[^80]: Y aunque la alineación es muy difícil, ¡hacer que las personas se comporten es aún más difícil!

[^81]: Imagina un sistema que puede hablar 50 idiomas, tener experiencia en todos los temas académicos, leer un libro completo en segundos y tener todo el material inmediatamente en mente, y producir salidas a diez veces la velocidad humana. Realmente, no tienes que imaginarlo: solo carga un sistema de IA actual. Estos son súper-humanos en muchas formas, y no hay nada deteniéndolos de ser aún más súper-humanos en esas y muchas otras.

[^82]: Por esto esto ha sido termed una "singularidad" tecnológica, tomando prestado de la física la idea de que uno no puede hacer predicciones pasada una singularidad. Los proponentes de inclinarse *hacia* tal singularidad también pueden desear reflexionar que en física estos mismos tipos de singularidades desgarran y aplastan a aquellos que van hacia ellas.

[^83]: El problema fue comprensivamente delineado en [*Superintelligence*](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834) de Bostrom, y nada desde entonces ha cambiado significativamente el mensaje central. Para un volumen más reciente recolectando resultados formales y matemáticos sobre incontrolabilidad ver [AI: Unexplainable, Unpredictable, Uncontrollable](https://www.amazon.com/Unexplainable-Unpredictable-Uncontrollable-Artificial-Intelligence/dp/103257626X) de Yampolskiy

[^84]: Esto también deja claro por qué la estrategia actual de las empresas de IA (iterativamente dejar que la IA "alinee" la siguiente IA más poderosa) no puede funcionar. Supón que un helecho, vía la agradabilidad de sus frondas, alista a un estudiante de primer grado para cuidarlo. El estudiante de primer grado escribe algunas instrucciones detalladas para un estudiante de 2do grado a seguir, y una nota convenciéndolos de hacerlo. El estudiante de 2do grado hace lo mismo para un estudiante de 3er grado, y así todo el camino a un graduado universitario, un gerente, un ejecutivo, y finalmente el CEO de GM. ¿GM entonces "hará lo que el helecho quiere"? En cada paso esto podría sentirse como que está funcionando. Pero poniéndolo todo junto, funcionará casi exactamente al grado al cual el CEO, Junta, y accionistas de GM resulten preocuparse por niños y helechos, y tendrá poco o nada que ver con todas esas notas y conjuntos de instrucciones.

[^85]: El carácter no es tan diferente de resultados formales como el teorema de incompletitud de Gödel o el argumento de parada de Turing en que la noción de control contradice fundamentalmente la premisa: cómo puedes controlar significativamente algo que no puedes entender o predecir; sin embargo si pudieras entender y predecir superinteligencia serías superinteligente. La razón por la que digo "se acerca" es que los resultados formales no son tan completos o revisados como en el caso de matemáticas puras, y porque me gustaría mantener esperanza de que alguna inteligencia general muy cuidadosamente construida, usando métodos totalmente diferentes a los actualmente empleados, pudiera tener algunas propiedades de seguridad matemáticamente comprobables, según el tipo de programa de IA "garantizadamente segura" discutido abajo.

[^86]: En el momento, la mayoría de las partes interesadas – es decir, casi toda la humanidad – está marginada en esta discusión. Eso está profundamente mal, y si no se les invita, los muchos, muchos otros grupos serán afectados por el desarrollo de IAG y deberían exigir ser incluidos.

## Capítulo 8 - Cómo no construir IAG

La IAG no es inevitable: hoy nos encontramos en una encrucijada. Este capítulo presenta una propuesta sobre cómo podríamos evitar que se construya.

Si el camino que actualmente seguimos conduce al probable fin de nuestra civilización, ¿cómo cambiamos de rumbo?

Supongamos que el deseo de dejar de desarrollar IAG y superinteligencia fuera generalizado y poderoso,[^87] porque se vuelve entendimiento común que la IAG sería absorbedora de poder en lugar de otorgadora de poder, y un peligro profundo para la sociedad y la humanidad. ¿Cómo cerraríamos las Puertas?

En la actualidad conocemos una sola forma de *crear* IA poderosa y general, que es mediante computaciones verdaderamente masivas de redes neuronales profundas. Dado que estas son cosas increíblemente difíciles y costosas de hacer, hay un sentido en el cual *no* hacerlas es fácil.[^88] Pero ya hemos visto las fuerzas que impulsan hacia la IAG, y las dinámicas de teoría de juegos que hacen muy difícil que cualquier parte se detenga unilateralmente. Por lo tanto, tomaría una combinación de intervención desde el exterior (es decir, gobiernos) para detener a las corporaciones, y acuerdos entre gobiernos para detenerse a sí mismos.[^89] ¿Cómo podría verse esto?

Es útil primero distinguir entre desarrollos de IA que deben ser *prevenidos* o *prohibidos*, y aquellos que deben ser *gestionados.* Lo primero sería principalmente el escape hacia la superinteligencia.[^90] Para el desarrollo prohibido, las definiciones deben ser lo más precisas posible, y tanto la verificación como la aplicación deben ser prácticas. Lo que debe ser *gestionado* serían los sistemas de IA generales y poderosos – que ya tenemos, y que tendrán muchas áreas grises, matices y complejidad. Para estos, las instituciones fuertes y efectivas son cruciales.

También podemos delinear útilmente los asuntos que deben abordarse a nivel internacional (incluyendo entre rivales o adversarios geopolíticos) [^91] de aquellos que jurisdicciones individuales, países o grupos de países pueden manejar. El desarrollo prohibido cae en gran medida en la categoría "internacional", porque una prohibición local del desarrollo de una tecnología generalmente puede eludirse cambiando de ubicación.[^92]

Finalmente, podemos considerar las herramientas en la caja de herramientas. Hay muchas, incluyendo herramientas técnicas, derecho blando (estándares, normas, etc.), derecho duro (regulaciones y requisitos), responsabilidad legal, incentivos de mercado, y así sucesivamente. Prestemos especial atención a una que es particular de la IA.

### Seguridad y gobernanza del cómputo

Una herramienta fundamental para gobernar la IA de alta potencia será el hardware que requiere. El software prolifera fácilmente, tiene un costo de producción marginal cercano a cero, cruza fronteras trivialmente y puede modificarse instantáneamente; nada de esto es cierto para el hardware. Sin embargo, como hemos discutido, enormes cantidades de este "cómputo" son necesarias tanto durante el entrenamiento de sistemas de IA como durante la inferencia para lograr los sistemas más capaces. El cómputo puede ser fácilmente cuantificado, contabilizado y auditado, con relativamente poca ambigüedad una vez que se desarrollan buenas reglas para hacerlo. Más crucialmente, grandes cantidades de computación son, como el uranio enriquecido, un recurso muy escaso, costoso y difícil de producir. Aunque los chips de computadora son ubicuos, el hardware requerido para IA es costoso y enormemente difícil de fabricar.[^93]

Lo que hace a los chips especializados en IA *mucho más* manejables como recurso escaso que el uranio es que pueden incluir mecanismos de seguridad basados en hardware. La mayoría de los teléfonos celulares modernos, y algunas laptops, tienen características de hardware especializado en el chip que les permiten asegurar que instalen solo software de sistema operativo aprobado y actualizaciones, que retengan y protejan datos biométricos sensibles en el dispositivo, y que puedan volverse inútiles para cualquiera excepto su propietario si se pierden o son robados. Durante los últimos varios años, tales medidas de seguridad de hardware se han vuelto bien establecidas y ampliamente adoptadas, y generalmente han demostrado ser bastante seguras.

La novedad clave de estas características es que vinculan hardware y software usando criptografía.[^94] Es decir, solo tener una pieza particular de hardware de computadora no significa que un usuario pueda hacer lo que quiera con ella aplicando diferentes softwares. Y esta vinculación también proporciona seguridad poderosa porque muchos ataques requerirían una violación del hardware en lugar de solo la seguridad del *software*.

Varios informes recientes (por ejemplo, de [GovAI y colaboradores](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips), y [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) han señalado que características de hardware similares incorporadas en hardware de computación de vanguardia relevante para IA podrían desempeñar un papel extremadamente útil en la seguridad y gobernanza de IA. Habilitan una serie de funciones disponibles para un "gobernador" [^95] que uno podría no imaginar que estuvieran disponibles o incluso fueran posibles. Como algunos ejemplos clave:

- *Geolocalización*: Los sistemas pueden configurarse para que los chips tengan una ubicación conocida, y puedan actuar diferentemente (o apagarse por completo) basado en la ubicación.[^96]
- *Conexiones en lista de permitidos*: cada chip puede configurarse con una lista de permitidos aplicada por hardware de otros chips particulares con los cuales puede conectarse en red, y ser incapaz de conectarse con cualquier chip que no esté en esta lista.[^97] Esto puede limitar el tamaño de grupos comunicativos de chips.[^98]
- *Inferencia o entrenamiento medido (y auto-apagado)*: Un gobernador puede licenciar solo una cierta cantidad de entrenamiento o inferencia (en tiempo, o FLOP, o posiblemente tokens) para ser realizada por un usuario, después de lo cual se requiere nuevo permiso. Si los incrementos son pequeños, entonces se requiere re-licenciamiento relativamente continuo de un modelo. El modelo puede entonces "apagarse" simplemente reteniendo esta señal de licencia.[^99]
- *Límite de velocidad*: Se previene que un modelo funcione a velocidad de inferencia mayor que algún límite determinado por un gobernador u otro. Esto podría implementarse mediante un conjunto limitado de conexiones en lista de permitidos, o por medios más sofisticados.
- *Entrenamiento atestiguado*: Un procedimiento de entrenamiento puede generar prueba criptográficamente segura de que un conjunto particular de códigos, datos y cantidad de uso de cómputo fueron empleados en la generación del modelo.

### Cómo no construir superinteligencia: límites globales en cómputo de entrenamiento e inferencia

Con estas consideraciones – especialmente respecto a la computación – en su lugar, podemos discutir cómo cerrar las Puertas a la superinteligencia artificial; luego nos dirigiremos a prevenir la IAG completa, y gestionar modelos de IA mientras se aproximan y exceden la capacidad humana en diferentes aspectos.

El primer ingrediente es, por supuesto, el entendimiento de que la superinteligencia no sería controlable, y que sus consecuencias son fundamentalmente impredecibles. Al menos China y Estados Unidos deben decidir independientemente, para este u otros propósitos, no construir superinteligencia.[^100] Luego se necesita un acuerdo internacional entre ellos y otros, con un mecanismo fuerte de verificación y aplicación, para asegurar a todas las partes que sus rivales no están defeccionando y decidiendo lanzar los dados.

Para ser verificables y aplicables, los límites deben ser límites duros, y lo más inequívocos posible. Esto parece un problema virtualmente imposible: limitar las capacidades de software complejo con propiedades impredecibles, mundialmente. Afortunadamente la situación es mucho mejor que esto, porque lo mismo que ha hecho posible la IA avanzada – una enorme cantidad de cómputo – es mucho, mucho más fácil de controlar. Aunque podría aún permitir algunos sistemas poderosos y peligrosos, la *superinteligencia desbocada* puede probablemente prevenirse mediante un tope duro en la cantidad de computación que va a una red neuronal, junto con un límite de tasa en la cantidad de inferencia que un sistema de IA (de redes neuronales conectadas y otro software) puede realizar. Una versión específica de esto se propone más adelante.

Puede parecer que poner límites globales duros en la computación de IA requeriría enormes niveles de coordinación internacional y vigilancia intrusiva que destruya la privacidad. Afortunadamente, no sería así. La [cadena de suministro extremadamente estrecha y con cuello de botella](https://arxiv.org/abs/2402.08797) proporciona que una vez que se establezca un límite legalmente (ya sea por ley u orden ejecutiva), la verificación del cumplimiento de ese límite solo requeriría participación y cooperación de un puñado de grandes empresas.[^101]

Un plan como este tiene una serie de características altamente deseables. Es mínimamente invasivo en el sentido de que solo unas pocas empresas importantes tienen requisitos impuestos sobre ellas, y solo grupos bastante significativos de computación serían gobernados. Los chips relevantes ya contienen las capacidades de hardware necesarias para una primera versión.[^102] Tanto la implementación como la aplicación dependen de restricciones legales estándar. Pero estas están respaldadas por términos de uso del hardware y por controles de hardware, simplificando vastamente la aplicación y previniendo trampas por empresas, grupos privados, o incluso países. Hay amplio precedente para empresas de hardware que ponen restricciones remotas en el uso de su hardware, y bloquean/desbloquean capacidades particulares externamente,[^103] incluyendo incluso en CPU de alta potencia en centros de datos.[^104] Incluso para la fracción bastante pequeña de hardware y organizaciones afectadas, la supervisión podría limitarse a telemetría, sin acceso directo a datos o modelos mismos; y el software para esto podría estar abierto a inspección para exhibir que no se están registrando datos adicionales. El esquema es internacional y cooperativo, y bastante flexible y extensible. Porque el límite principalmente está en el hardware en lugar del software, es relativamente agnóstico sobre cómo ocurre el desarrollo e implementación del software de IA, y es compatible con variedad de paradigmas incluyendo IA más "descentralizada" o "pública" dirigida a combatir la concentración de poder impulsada por IA.

El cierre de Puertas basado en computación también tiene desventajas. Primero, está lejos de ser una solución completa al problema de la gobernanza de IA en general. Segundo, conforme el hardware de computadora se vuelve más rápido, el sistema "atraparía" más y más hardware en grupos más y más pequeños (o incluso GPU individuales).[^105] También es posible que debido a mejoras algorítmicas incluso un límite de computación más bajo sea necesario en el tiempo,[^106] o que la cantidad de computación se vuelva en gran medida irrelevante y cerrar la Puerta en su lugar necesitara un régimen de gobernanza más detallado basado en riesgo o capacidad para IA. Tercero, sin importar las garantías y el pequeño número de entidades afectadas, tal sistema está destinado a crear resistencia respecto a privacidad y vigilancia, entre otras preocupaciones.[^107]

Por supuesto, desarrollar e implementar un esquema de gobernanza que limite el cómputo en un período corto de tiempo será bastante desafiante. Pero absolutamente es factible.

### I-A-G: La triple-intersección como base del riesgo, y de la política

Dirigámonos ahora a la IAG. Las líneas duras y definiciones aquí son más difíciles, porque ciertamente tenemos inteligencia que es artificial y general, y por ninguna definición existente todos estarán de acuerdo si o cuándo existe. Además, un límite de cómputo o inferencia es una herramienta algo contundente (siendo el cómputo un proxy para la capacidad, que luego es un proxy para el riesgo) que – a menos que sea bastante bajo – es improbable que prevenga IAG que sea lo suficientemente poderosa para causar disrupción social o civilizacional o riesgos agudos.

He argumentado que los riesgos más agudos emergen de la triple-intersección de capacidad muy alta, alta autonomía y gran generalidad. Estos son los sistemas que – si se desarrollan en absoluto – deben ser gestionados con enorme cuidado. Al crear estándares estrictos (mediante responsabilidad legal y regulación) para sistemas que combinen las tres propiedades, podemos canalizar el desarrollo de IA hacia alternativas más seguras.

Como con otras industrias y productos que podrían potencialmente dañar a consumidores o al público, los sistemas de IA requieren regulación cuidadosa por agencias gubernamentales efectivas y empoderadas. Esta regulación debe reconocer los riesgos inherentes de la IAG, y prevenir que se desarrollen sistemas de IA de alta potencia inaceptablemente riesgosos.[^108]

Sin embargo, la regulación a gran escala, especialmente con dientes reales que seguramente serán opuestos por la industria,[^109] toma tiempo [^110] así como convicción política de que es necesaria.[^111] Dado el ritmo del progreso, esto puede tomar más tiempo del que tenemos disponible.

En una escala de tiempo mucho más rápida y mientras se desarrollan medidas regulatorias, podemos dar a las empresas los incentivos necesarios para (a) desistir de actividades de muy alto riesgo y (b) desarrollar sistemas comprensivos para evaluar y mitigar riesgo, aclarando y aumentando los niveles de responsabilidad legal para los sistemas más peligrosos. La idea sería imponer los niveles más altos de responsabilidad legal – estricta y en algunos casos criminal personal – para sistemas en la triple-intersección de alta autonomía-generalidad-inteligencia, pero proporcionar "puertos seguros" a responsabilidad legal más típica basada en fallas para sistemas en los cuales una de esas propiedades está faltando o se garantiza que es manejable. Es decir, por ejemplo, un sistema "débil" que es general y autónomo (como un asistente personal capaz y confiable pero limitado) estaría sujeto a niveles más bajos de responsabilidad legal. Igualmente un sistema estrecho y autónomo como un auto que se conduce solo estaría aún sujeto a la regulación significativa que ya tiene, pero no a responsabilidad legal mejorada. Similarmente para un sistema altamente capaz y general que es "pasivo" y en gran medida incapaz de acción independiente. Los sistemas que carecen de *dos* de las tres propiedades son aún más manejables y los puertos seguros serían incluso más fáciles de reclamar. Este enfoque refleja cómo manejamos otras tecnologías potencialmente peligrosas:[^112] mayor responsabilidad legal para configuraciones más peligrosas crea incentivos naturales para alternativas más seguras.

El resultado por defecto de tales altos niveles de responsabilidad legal, que actúan para *internalizar* el riesgo de IAG a las empresas en lugar de descargarlo al público, es probable (¡y esperemos!) que las empresas simplemente no desarrollen IAG completa hasta y a menos que puedan genuinamente hacerla confiable, segura y controlable dado que *su propio liderazgo* son las partes en riesgo. (En caso de que esto no sea suficiente, la legislación aclarando responsabilidad legal también debería permitir explícitamente alivio judicial, es decir, un juez ordenando una detención, para actividades que están claramente en la zona de peligro y posiblemente presentan un riesgo público.) Conforme la regulación entra en su lugar, cumplir con la regulación puede convertirse en el puerto seguro, y los puertos seguros de baja autonomía, estrechez, o debilidad de sistemas de IA pueden convertirse en regímenes regulatorios relativamente más ligeros.

### Provisiones clave de un cierre de Puertas

Con la discusión anterior en mente, esta sección proporciona propuestas para provisiones clave que implementarían y mantendrían la prohibición en IAG completa y superinteligencia, y gestión de IA general competitiva con humanos o competitiva con expertos cerca del umbral de IAG completa.[^113] Tiene cuatro piezas clave: 1) contabilidad y supervisión de cómputo, 2) topes de cómputo en entrenamiento y operación de IA, 3) un marco de responsabilidad legal, y 4) estándares de seguridad y protección por niveles definidos que incluyen requisitos regulatorios duros. Estos se describen sucintamente a continuación, con más detalles o ejemplos de implementación dados en tres tablas acompañantes. Importante, note que estos están lejos de ser todo lo que será necesario para gobernar sistemas avanzados de IA; mientras tendrán beneficios adicionales de seguridad y protección, están dirigidos a cerrar la Puerta al escape de inteligencia, y redirigir el desarrollo de IA en una mejor dirección.

#### 1\. Contabilidad de cómputo, y transparencia

- Una organización de estándares (por ejemplo NIST en Estados Unidos seguido por ISO/IEEE internacionalmente) debe codificar un estándar técnico detallado para el cómputo total usado en entrenar y operar modelos de IA, en FLOP, y la velocidad en FLOP/s a la cual operan. Los detalles de cómo podría verse esto se dan en el Apéndice A.[^114]
- Un requisito – ya sea por nueva legislación o bajo autoridad existente [^115] – debe ser impuesto por jurisdicciones en las cuales toma lugar entrenamiento de IA a gran escala para computar y reportar a un cuerpo regulatorio u otra agencia el FLOP total usado en entrenar y operar todos los modelos arriba de un umbral de 10 <sup>25</sup> FLOP o 10 <sup>18</sup> FLOP/s.[^116]
- Estos requisitos deben introducirse por fases, inicialmente requiriendo estimados de buena fe bien documentados en una base trimestral, con fases posteriores requiriendo progresivamente estándares más altos, hasta FLOP total y FLOP/s criptográficamente atestiguados adjuntos a cada *salida* del modelo.
- Estos reportes deben ser complementados por estimados bien documentados de costo energético y financiero marginal usado en generar cada salida de IA.

Justificación: Estos números bien computados y transparentemente reportados proporcionarían la base para topes de entrenamiento y operación, así como un puerto seguro de medidas de responsabilidad legal más altas (ver Apéndices C y D).

#### 2\. Topes de cómputo de entrenamiento y operación

- Las jurisdicciones que hospedan sistemas de IA deben imponer un límite duro en el cómputo total que va a cualquier salida de modelo de IA, comenzando en 10 <sup>27</sup> FLOP [^117] y ajustable según sea apropiado.
- Las jurisdicciones que hospedan sistemas de IA deben imponer un límite duro en la tasa de cómputo de salidas de modelo de IA, comenzando en 10 <sup>20</sup> FLOP/s y ajustable según sea apropiado.

Justificación: La computación total, aunque muy imperfecta, es un proxy para la capacidad (y riesgo) de IA que es concretamente medible y verificable, así que proporciona un respaldo duro para limitar capacidades. Una propuesta de implementación concreta se da en el Apéndice B.

#### 3\. Responsabilidad legal mejorada para sistemas peligrosos

- La creación y operación [^118] de un sistema avanzado de IA que es altamente general, capaz y autónomo, debe ser aclarada legalmente mediante legislación para estar sujeta a responsabilidad legal estricta, solidaria, en lugar de basada en falla de una sola parte.[^119]
- Un proceso legal debe estar disponible para hacer casos de seguridad afirmativos, que otorgarían puerto seguro de responsabilidad legal estricta para sistemas que son pequeños (en términos de cómputo), débiles, estrechos, pasivos, o que tienen suficientes garantías de seguridad, protección y controlabilidad.
- Una vía explícita y conjunto de condiciones para alivio judicial para detener actividades de entrenamiento e inferencia de IA que constituyen un peligro público debe ser delineado.

Justificación: Los sistemas de IA no pueden ser responsabilizados, así que debemos responsabilizar a individuos humanos y organizaciones por el daño que causan (responsabilidad legal).[^120] La IAG incontrolable es una amenaza para la sociedad y civilización y en ausencia de un caso de seguridad debe considerarse anormalmente peligrosa. Poner la carga de responsabilidad en los desarrolladores para mostrar que los modelos poderosos son lo suficientemente seguros para no ser considerados "anormalmente peligrosos" incentiva el desarrollo seguro, junto con transparencia y mantenimiento de registros para reclamar esos puertos seguros. La regulación puede entonces prevenir daño donde la disuasión de la responsabilidad legal es insuficiente. Finalmente, los desarrolladores de IA ya son responsables por los daños que causan, así que aclarar legalmente la responsabilidad legal para los más riesgosos de los sistemas puede hacerse inmediatamente, sin que se desarrollen estándares altamente detallados; estos pueden entonces desarrollarse con el tiempo. Los detalles se dan en el Apéndice C.

#### 4\. Regulación de seguridad para IA

Un sistema regulatorio que aborde riesgos agudos a gran escala de IA requerirá como mínimo:

- La identificación o creación de un conjunto apropiado de cuerpos regulatorios, probablemente una nueva agencia;
- Un marco comprensivo de evaluación de riesgo;[^121]
- Un marco para casos de seguridad afirmativos, basado en parte en el marco de evaluación de riesgo, para ser hechos por desarrolladores, y para auditoría por grupos y agencias *independientes*;
- Un sistema de licenciamiento por niveles, con niveles siguiendo niveles de capacidad.[^122] Las licencias serían otorgadas en base a casos de seguridad y auditorías, para desarrollo e implementación de sistemas. Los requisitos irían desde notificación en el extremo bajo, hasta garantías cuantitativas de seguridad, protección y controlabilidad antes del desarrollo, en el extremo alto. Estos prevendrían el lanzamiento de sistemas hasta que se demuestre que son seguros, y prohibirían el desarrollo de sistemas intrínsecamente inseguros. El Apéndice D proporciona una propuesta de lo que tales estándares de seguridad y protección podrían conllevar.
- Acuerdos para llevar tales medidas al nivel internacional, incluyendo cuerpos internacionales para armonizar normas y estándares, y potencialmente agencias internacionales para revisar casos de seguridad.

Justificación: En última instancia, la responsabilidad legal no es el mecanismo correcto para prevenir riesgo a gran escala al público de una nueva tecnología. La regulación comprensiva, con cuerpos regulatorios empoderados, será necesaria para IA así como para cada otra industria importante que presenta riesgo al público.[^123]

La regulación hacia prevenir otros riesgos generalizados pero menos agudos es probable que varíe en su forma de jurisdicción a jurisdicción. Lo crucial es evitar desarrollar los sistemas de IA que son tan riesgosos que estos riesgos son inmanejables.

### ¿Qué entonces?

Durante la próxima década, conforme la IA se vuelve más generalizada y la tecnología central avanza, es probable que pasen dos cosas clave. Primero, la regulación de sistemas poderosos existentes de IA se volverá más difícil, sin embargo incluso más necesaria. Es probable que al menos algunas medidas abordando riesgos de seguridad a gran escala requieran acuerdo a nivel internacional, con jurisdicciones individuales aplicando reglas basadas en acuerdos internacionales.

Segundo, los topes de cómputo de entrenamiento y operación se volverán más difíciles de mantener conforme el hardware se vuelve más barato y más eficiente en costo; también pueden volverse menos relevantes (o necesitar ser incluso más estrictos) con avances en algoritmos y arquitecturas.

¡Que controlar la IA se vuelva más difícil no significa que debamos rendirnos! Implementar el plan delineado en este ensayo nos daría tanto tiempo valioso como control crucial sobre el proceso que nos pondría en una posición mucho, mucho mejor para evitar el riesgo existencial de la IA para nuestra sociedad, civilización y especie.

En el término aún más largo, habrá elecciones que hacer sobre qué permitimos. Podemos elegir aún crear alguna forma de IAG genuinamente controlable, al grado que esto demuestre ser posible. O podemos decidir que administrar el mundo se deja mejor a las máquinas, si podemos convencernos de que harán un mejor trabajo, y nos tratarán bien. Pero estas deben ser decisiones hechas con entendimiento científico profundo de la IA en mano, y después de discusión global inclusiva significativa, no en una carrera entre magnates tecnológicos con la mayoría de la humanidad completamente no involucrada y desconocedora.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) Resumen de la gobernanza de I-A-G y superinteligencia mediante responsabilidad legal y regulación. La responsabilidad legal es más alta, y la regulación más fuerte, en la triple-intersección de Autonomía, Generalidad e Inteligencia. Los puertos seguros de responsabilidad legal estricta y regulación fuerte pueden obtenerse mediante casos de seguridad afirmativos demostrando que un sistema es débil y/o estrecho y/o pasivo. Los topes en Cómputo Total de Entrenamiento y tasa de Cómputo de Inferencia, verificados y aplicados legalmente y usando medidas de seguridad de hardware y criptográficas, respaldan la seguridad evitando IAG completa y efectivamente prohibiendo la superinteligencia.


[^87]: Muy probablemente, la expansión de esta realización tomará ya sea esfuerzo intenso por grupos de educación y promoción haciendo este caso, o un desastre bastante significativo causado por IA. Podemos esperar que sea lo primero.

[^88]: Paradójicamente, estamos acostumbrados a que la Naturaleza limite nuestra tecnología haciéndola muy difícil de desarrollar, especialmente científicamente. Pero ese ya no es el caso para IA: los problemas científicos clave están resultando ser más fáciles de lo anticipado. No podemos contar con que la Naturaleza nos salve de nosotros mismos aquí – tendremos que hacerlo nosotros.

[^89]: ¿Dónde, exactamente, nos detenemos en desarrollar nuevos sistemas? Aquí, debemos adoptar un principio precautorio. Una vez que un sistema es implementado, y especialmente una vez que ese nivel de capacidad del sistema prolifera, es excesivamente difícil retroceder. Y si un sistema es *desarrollado* (especialmente a gran costo y esfuerzo), habrá enorme presión para usarlo o implementarlo, y tentación de que sea filtrado o robado. Desarrollar sistemas y *luego* decidir si son profundamente inseguros es un camino peligroso.

[^90]: También sería sabio prohibir el desarrollo de IA que es intrínsecamente peligroso, como sistemas auto-replicantes y evolutivos, aquellos diseñados para escapar del encierro, aquellos que pueden auto-mejorarse autónomamente, IA deliberadamente engañosa y maliciosa, etc.

[^91]: Note que esto no significa necesariamente *aplicado* a nivel internacional por algún tipo de cuerpo global: en su lugar las naciones soberanas podrían aplicar reglas acordadas, como en muchos tratados.

[^92]: Como veremos más adelante, la naturaleza de la computación de IA permitiría algo de un híbrido; pero la cooperación internacional aún será necesaria.

[^93]: Por ejemplo, las máquinas requeridas para grabar chips relevantes para IA son hechas por solo una firma, ASML (a pesar de muchos otros intentos de hacerlo), la vasta mayoría de chips relevantes son manufacturados por una firma, TSMC (a pesar de otros intentando competir), y el diseño y construcción de hardware de esos chips hecho por solo unos pocos incluyendo NVIDIA, AMD, y Google.

[^94]: Más importante, cada chip sostiene una llave privada criptográfica única e inaccesible que puede usar para "firmar" cosas.

[^95]: Por defecto esto sería la empresa vendiendo los chips, pero otros modelos son posibles y potencialmente útiles.

[^96]: Un gobernador puede determinar la ubicación de un chip cronometrando el intercambio de mensajes firmados con él: la velocidad finita de la luz requiere que el chip esté dentro de un radio dado *r* de una "estación" si puede devolver un mensaje firmado en un tiempo menor que *r* / *c*, donde *c* es la velocidad de la luz. Usando múltiples estaciones, y algún entendimiento de características de red, la ubicación del chip puede determinarse. La belleza de este método es que la mayoría de su seguridad es suministrada por las leyes de la física. Otros métodos podrían usar GPS, rastreo inercial, y tecnologías similares.

[^97]: Alternativamente, pares de chips podrían permitirse comunicarse entre sí solo mediante permiso explícito de un gobernador.

[^98]: Esto es crucial porque al menos actualmente, conexión de muy alto ancho de banda entre chips es necesaria para entrenar modelos grandes de IA en ellos.

[^99]: Esto también podría configurarse para requerir mensajes firmados de *N* de *M* gobernadores diferentes, permitiendo que múltiples partes compartan gobernanza.

[^100]: Esto está lejos de no tener precedente – por ejemplo los militares no han desarrollado ejércitos de supersoldados clonados o genéticamente modificados, aunque esto es probablemente tecnológicamente posible. Pero han *elegido* no hacer esto, en lugar de ser prevenidos por otros. El historial no es grandioso para grandes potencias mundiales siendo prevenidas de desarrollar una tecnología que desean fuertemente desarrollar.

[^101]: Con un par de excepciones notables (en particular NVIDIA) el hardware especializado en IA es una parte relativamente pequeña del modelo general de negocio e ingresos de estas empresas. Además, la brecha entre hardware usado en IA avanzada y hardware "de grado consumidor" es significativa, así que la mayoría de consumidores de hardware de computadora serían en gran medida no afectados.

[^102]: Para análisis más detallado, vea los informes recientes de [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) y [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). Estos se enfocan en factibilidad técnica, especialmente en el contexto de controles de exportación de Estados Unidos buscando restringir la capacidad de otros países en computación de alta gama; pero esto tiene superposición obvia con la restricción global prevista aquí.

[^103]: Los dispositivos Apple, por ejemplo, son bloqueados remota y seguramente cuando se reportan perdidos o robados, y pueden ser re-activados remotamente. Esto depende de las mismas características de seguridad de hardware discutidas aquí.

[^104]: Vea por ejemplo la oferta de [capacidad bajo demanda](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) de IBM, [Intel bajo demanda](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html) de Intel, y [computación en nube privada](https://security.apple.com/blog/private-cloud-compute/) de Apple.

[^105]: [Este estudio](https://epochai.org/trends#hardware-trends-section) muestra que históricamente el mismo desempeño se ha logrado usando aproximadamente 30% menos dólares por año. Si esta tendencia continúa, puede haber superposición significativa entre uso de chips de IA y "consumidor", y en general la cantidad de hardware necesario para sistemas de IA de alta potencia podría volverse incómodamente pequeña.

[^106]: Por el [mismo estudio](https://epochai.org/trends#hardware-trends-section), el desempeño dado en reconocimiento de imagen ha requerido 2.5x menos computación cada año. Si esto fuera también a sostenerse para los sistemas de IA más capaces también, un límite de computación no sería uno útil por mucho tiempo.

[^107]: En particular, a nivel de país esto parece mucho una nacionalización de la computación, en que el gobierno tendría mucho control sobre cómo se usa el poder computacional. Sin embargo, para aquellos preocupados sobre la participación gubernamental, esto parece mucho más seguro que y preferible a que el software de IA más poderoso *mismo* sea nacionalizado mediante alguna fusión entre grandes empresas de IA y gobiernos nacionales, como algunos están empezando a abogar.

[^108]: Un paso regulatorio importante en Europa fue tomado con la aprobación en 2024 de la [Ley de IA de la UE.](https://artificialintelligenceact.eu/) Clasifica IA por riesgo: prohibiendo sistemas inaceptables, regulando los de alto riesgo, e imponiendo reglas de transparencia, o ninguna medida en absoluto, sobre sistemas de bajo riesgo. Reducirá significativamente algunos riesgos de IA, y impulsará la transparencia de IA incluso para firmas estadounidenses, pero tiene dos fallas clave. Primero, alcance limitado: mientras aplica a cualquier empresa proporcionando IA en la UE, la aplicación sobre firmas basadas en Estados Unidos es débil, y la IA militar está exenta. Segundo, mientras cubre GPAI, falla en reconocer IAG o superinteligencia como riesgos inaceptables o prevenir su desarrollo—solo su implementación en la UE. Como resultado, hace poco para frenar los riesgos de IAG o superinteligencia.

[^109]: Las empresas a menudo representan que están a favor de regulación razonable. Pero de alguna manera casi siempre parecen oponerse a cualquier regulación *particular*; atestigüe la lucha sobre la bastante ligera SB1047, que [la mayoría de empresas de IA se opusieron pública o privadamente.](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/)

[^110]: Fueron aproximadamente 3 1/2 años desde el tiempo que la ley de IA de la UE fue propuesta hasta que entró en efecto.

[^111]: A veces se expresa que es "demasiado temprano" para empezar a regular IA. Dada la última nota, eso difícilmente parece probable. Otra preocupación expresada es que la regulación "dañaría la innovación." Pero la buena regulación solo cambia la dirección, no cantidad, de innovación.

[^112]: Un precedente interesante está en el transporte de materiales peligrosos, que podrían escapar y causar daño. Aquí, [regulación](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) y [jurisprudencia](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) han establecido responsabilidad legal estricta para materiales muy peligrosos como explosivos, gasolina, venenos, agentes infecciosos, y desecho radioactivo. Otros ejemplos incluyen [advertencias en farmacéuticos](https://www.medicalnewstoday.com/articles/boxed-warnings), [clases de dispositivos médicos,](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification) etc.

[^113]: Otra propuesta comprensiva con objetivos similares presentada en ["Un Sendero Estrecho"](https://www.narrowpath.co/) aboga por un enfoque más centralizado, basado en prohibición que canaliza todo el desarrollo de IA de frontera a través de una sola entidad internacional, supervisada por instituciones internacionales fuertes, con prohibiciones categóricas claras en lugar de restricciones graduadas. También respaldo ese plan; sin embargo tomará incluso más voluntad política y coordinación que el propuesto aquí.

[^114]: Algunas directrices para tal estándar fueron [publicadas](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/) por el Foro de Modelo de Frontera. Relativo a la propuesta aquí, esas yerran del lado de menos precisión y menos cómputo incluido en el conteo.

[^115]: La orden ejecutiva de IA de Estados Unidos de 2023 (ahora rescindida) requirió reportes similares pero menos finos. Esto debe fortalecerse por una orden de reemplazo.

[^116]: Muy aproximadamente, para chips H100 ahora-comunes esto corresponde a grupos de aproximadamente 1000 haciendo inferencia; son aproximadamente 100 (aproximadamente USD $5M valor) de los chips NVIDIA B200 más nuevos de primera línea haciendo inferencia. En ambos casos el número de entrenamiento corresponde a ese grupo computando por varios meses mes.

[^117]: Esta cantidad es mayor que cualquier sistema de IA actualmente entrenado; un número mayor o menor podría justificarse conforme entendemos mejor cómo la capacidad de IA escala con el cómputo.

[^118]: Esto aplica a aquellos creando y proporcionando/hospedando los modelos, no usuarios finales.

[^119]: Aproximadamente, responsabilidad legal "estricta" significa que los desarrolladores son responsabilizados por daños hechos por un producto *por defecto* y es un estándar usado para productos "anormalmente peligrosos", y (algo divertido pero apropiadamente) animales salvajes. Responsabilidad legal "solidaria" significa que la responsabilidad legal es asignada a todas las partes responsables por un producto, y esas partes tienen que arreglar entre sí mismas quién lleva qué responsabilidad. Esto es importante para sistemas como IA con una cadena de valor larga y compleja.

[^120]: La responsabilidad legal estándar basada en falla de una sola parte no es suficiente: la falla será tanto difícil de rastrear como asignar porque los sistemas de IA son complejos, su operación no es entendida, y muchas partes pueden estar involucradas en la creación de un sistema o salida peligrosa. Además, las demandas tomarán años para adjudicar y probablemente resulten meramente en multas que son inconsequentes para estas empresas, así que la responsabilidad legal personal para ejecutivos es importante también.

[^121]: No debe haber exención de criterios de seguridad para modelos de peso abierto. Además, al evaluar riesgo debe asumirse que las barreras que pueden ser removidas serán removidas de modelos ampliamente disponibles, y que incluso modelos cerrados proliferarán a menos que haya una muy alta aseguración de que permanecerán seguros.

[^122]: El esquema propuesto aquí tiene escrutinio regulatorio disparado en capacidad general; sin embargo tiene sentido para algunos casos de uso especialmente riesgosos disparar más escrutinio – por ejemplo un sistema de IA de virología experto, incluso si estrecho y pasivo, probablemente debería ir en un nivel más alto. La anterior orden ejecutiva estadounidense tenía algo de esta estructura para capacidades biológicas.

[^123]: Dos ejemplos claros son aviación y medicinas, reguladas por la FAA y FDA, y agencias similares en otros países. Estas agencias son imperfectas, pero han sido absolutamente vitales para el funcionamiento y éxito de esas industrias.

## Capítulo 9 - Diseñando el futuro: qué deberíamos hacer en su lugar

La IA puede hacer un bien increíble en el mundo. Para obtener todos los beneficios sin los riesgos, debemos asegurar que la IA siga siendo una herramienta humana.

Si decidimos exitosamente no suplantarnos a nosotros mismos por máquinas—¡al menos por un tiempo!—¿qué podemos hacer en su lugar? ¿Renunciamos a la enorme promesa de la IA como tecnología? En algún nivel la respuesta es un simple *no:* cerrar las Puertas a la IAG incontrolable y la superinteligencia, pero *sí* construir muchas otras formas de IA, así como las estructuras de gobernanza e instituciones que necesitaremos para gestionarlas.

Pero aún hay mucho que decir; hacer que esto suceda sería una ocupación central de la humanidad. Esta sección explora varios temas clave:

- Cómo podemos caracterizar la IA "Herramienta" y las formas que puede adoptar.
- Que podemos obtener (casi) todo lo que la humanidad quiere sin IAG, con IA Herramienta.
- Que los sistemas de IA Herramienta son (probablemente, en principio) manejables.
- Que alejarse de la IAG no significa comprometer la seguridad nacional—todo lo contrario.
- Que la concentración de poder es una preocupación real. ¿Podemos mitigarla sin socavar la seguridad?
- Que querremos—y necesitaremos—nuevas estructuras de gobernanza y sociales, y la IA puede realmente ayudar.

### IA dentro de las Puertas: IA Herramienta

El diagrama de triple intersección ofrece una buena manera de delinear lo que podemos llamar "IA Herramienta": IA que es una herramienta controlable para uso humano, en lugar de un rival o reemplazo incontrolable. Los sistemas de IA menos problemáticos son aquellos que son autónomos pero no generales ni súper capaces (como un bot de ofertas en subastas), o generales pero no autónomos ni capaces (como un modelo de lenguaje pequeño), o capaces pero específicos y muy controlables (como AlphaGo).[^124] Aquellos con dos características que se intersectan tienen aplicaciones más amplias pero mayor riesgo y requerirán grandes esfuerzos para gestionarlos. (El hecho de que un sistema de IA sea más una herramienta no significa que sea inherentemente seguro, simplemente que no es inherentemente *inseguro*—considera una motosierra versus un tigre mascota.) La Puerta debe permanecer cerrada a la IAG (completa) y la superinteligencia en la triple intersección, y debe tenerse un enorme cuidado con los sistemas de IA que se acerquen a ese umbral.

Pero esto deja mucha IA poderosa. Podemos obtener una enorme utilidad de "oráculos" pasivos inteligentes y generales y sistemas específicos, sistemas generales a nivel humano pero no sobrehumano, etc. Muchas empresas tecnológicas y desarrolladores están construyendo activamente estos tipos de herramientas y deberían continuar; como la mayoría de la gente, implícitamente *asumen* que las Puertas a la IAG y la superinteligencia permanecerán cerradas.[^125]

Además, los sistemas de IA pueden combinarse efectivamente en sistemas compuestos que mantienen supervisión humana mientras mejoran las capacidades. En lugar de depender de cajas negras inescrutables, podemos construir sistemas donde múltiples componentes—incluyendo tanto IA como software tradicional—trabajen juntos de maneras que los humanos puedan monitorear y entender.[^126] Mientras algunos componentes podrían ser cajas negras, ninguno estaría cerca de la IAG—solo el sistema compuesto como un todo sería tanto altamente general como altamente capaz, y de manera estrictamente controlable.[^127]

#### Control humano significativo y garantizado

¿Qué significa "estrictamente controlable"? Una idea clave del marco "Herramienta" es permitir sistemas—incluso si son bastante generales y poderosos—que estén garantizados de estar bajo control humano significativo. ¿Qué significa esto? Implica dos aspectos. Primero es una consideración de diseño: los humanos deberían estar profunda y centralmente involucrados en lo que el sistema está haciendo, *sin* delegar decisiones importantes clave a la IA. Este es el carácter de la mayoría de los sistemas de IA actuales. Segundo, en la medida en que los sistemas de IA son autónomos, deben tener garantías que limiten su alcance de acción. Una garantía debería ser un *número* que caracterice la probabilidad de que algo suceda, y una razón para creer en ese número. Esto es lo que exigimos en otros campos críticos para la seguridad, donde números como "tiempo medio entre fallas" y números esperados de accidentes se calculan, respaldan y publican en casos de seguridad.[^128] El número ideal para fallas es cero, por supuesto. Y la buena noticia es que podríamos acercarnos bastante, aunque usando arquitecturas de IA bastante diferentes, usando ideas de propiedades *formalmente verificadas* de programas (incluyendo IA). La idea, explorada extensamente por Omohundro, Tegmark, Bengio, Dalrymple y otros (ver [aquí](https://arxiv.org/abs/2309.01933) y [aquí](https://arxiv.org/abs/2405.06624)) es construir un programa con ciertas propiedades (por ejemplo: que un humano pueda apagarlo) y *probar* formalmente que esas propiedades se mantienen. Esto se puede hacer ahora para programas bastante cortos y propiedades simples, pero el poder (venidero) del software de pruebas impulsado por IA podría permitirlo para programas mucho más complejos (p. ej., envoltorios) e incluso la IA misma. Este es un programa muy ambicioso, pero a medida que crece la presión sobre las Puertas, vamos a necesitar algunos materiales poderosos reforzándolas. La prueba matemática puede ser uno de los pocos que sea lo suficientemente fuerte.

#### Hacia dónde va la industria de la IA

Con el progreso de la IA redirigido, la IA Herramienta seguiría siendo una industria enorme. En términos de hardware, incluso con límites de cómputo para prevenir la superinteligencia, el entrenamiento y la inferencia en modelos más pequeños aún requerirán enormes cantidades de componentes especializados. En el lado del software, desactivar la explosión en el tamaño de modelos de IA y computación simplemente debería llevar a las empresas a redirigir recursos hacia hacer los sistemas más pequeños mejores, más diversos y más especializados, en lugar de simplemente hacerlos más grandes.[^129] Habría mucho espacio—probablemente más—para todas esas startups lucrativas de Silicon Valley.[^130]

### La IA Herramienta puede producir (casi) todo lo que la humanidad quiere, sin IAG

La inteligencia, ya sea biológica o de máquina, puede considerarse ampliamente como la capacidad de planificar y ejecutar actividades que generen futuros más alineados con un conjunto de objetivos. Como tal, la inteligencia es de enorme beneficio cuando se usa en la búsqueda de objetivos sabiamente elegidos. La inteligencia artificial está atrayendo enormes inversiones de tiempo y esfuerzo en gran parte debido a sus beneficios prometidos. Así que deberíamos preguntar: ¿hasta qué grado aún cosecharíamos los beneficios de la IA si contenemos su descontrol hacia la superinteligencia? La respuesta: podríamos perder sorprendentemente poco.

Considera primero que los sistemas de IA actuales ya son muy poderosos, y realmente solo hemos arañado la superficie de lo que se puede hacer con ellos.[^131] Son razonablemente capaces de "dirigir el espectáculo" en términos de "entender" una pregunta o tarea que se les presenta, y lo que se necesitaría para responder esa pregunta o hacer esa tarea.

Además, mucho de la emoción sobre los sistemas de IA modernos se debe a su generalidad; pero algunos de los sistemas de IA más capaces—como los que generan o reconocen voz o imágenes, hacen predicción y modelado científico, juegan juegos, etc.—son mucho más específicos y están bien "dentro de las Puertas" en términos de computación.[^132] Estos sistemas son súper-humanos en las tareas particulares que realizan. Pueden tener debilidades en casos límite[^133] (o [explotables](https://arxiv.org/abs/2211.00241)) debido a su especificidad; sin embargo, *totalmente* específico o *completamente* general no son las únicas opciones disponibles: hay muchas arquitecturas intermedias.[^134]

Estas herramientas de IA pueden acelerar enormemente el avance en otras tecnologías positivas, sin IAG. Para hacer mejor física nuclear, no necesitamos que la IA sea un físico nuclear—¡tenemos esos! Si queremos acelerar la medicina, démosles a los biólogos, investigadores médicos y químicos herramientas poderosas. Las quieren y las usarán para un beneficio enorme. No necesitamos una granja de servidores llena de un millón de genios digitales; tenemos millones de humanos cuyo genio la IA puede ayudar a resaltar. Sí, tomará más tiempo obtener la inmortalidad y la cura para todas las enfermedades. Este es un costo real. Pero incluso las innovaciones de salud más prometedoras serían de poca utilidad si la inestabilidad impulsada por IA lleva a conflictos globales o colapso social. Nos debemos a nosotros mismos darle una oportunidad primero a los humanos empoderados por IA al problema.

Y supón que hay, de hecho, algún beneficio enorme de la IAG que no puede obtenerse por la humanidad usando herramientas dentro de las Puertas. ¿Perdemos esos al *nunca* construir IAG y superinteligencia? Al sopesar los riesgos y recompensas aquí, hay un enorme beneficio asimétrico en esperar versus apresurarse: podemos esperar hasta que pueda hacerse de manera garantizada segura y beneficiosa, y casi todos aún podrán cosechar las recompensas; si nos apresuramos, podría ser—en palabras del CEO de OpenAI Sam Altman—[luces apagadas para *todos* nosotros.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

Pero si las herramientas no-IAG son potencialmente tan poderosas, ¿podemos gestionarlas? La respuesta es un claro... tal vez.

### Los sistemas de IA Herramienta son (probablemente, en principio) manejables

Pero no será fácil. Los sistemas de IA de vanguardia actuales pueden empoderar enormemente a las personas e instituciones para lograr sus objetivos. ¡Esto es, en general, algo bueno! Sin embargo, hay dinámicas naturales de tener tales sistemas a nuestra disposición—súbitamente y sin mucho tiempo para que la sociedad se adapte—que ofrecen riesgos serios que necesitan ser gestionados. Vale la pena discutir algunas clases principales de tales riesgos, y cómo pueden ser disminuidos, asumiendo un cierre de Puerta.

Una clase de riesgos es la de IA Herramienta de alto poder que permite acceso a conocimiento o capacidad que previamente había estado vinculado a una persona u organización, haciendo disponible una combinación de alta capacidad más alta lealtad a un conjunto muy amplio de actores. Hoy, con suficiente dinero una persona de malas intenciones podría contratar un equipo de químicos para diseñar y producir nuevas armas químicas—pero no es tan fácil tener ese dinero o encontrar/ensamblar el equipo y convencerlos de hacer algo claramente ilegal, no ético y peligroso. Para prevenir que los sistemas de IA jueguen tal papel, las mejoras en los métodos actuales pueden bastar,[^135] siempre que todos esos sistemas y el acceso a ellos sean gestionados responsablemente. Por otro lado, si sistemas poderosos se liberan para uso general y modificación, cualquier medida de seguridad incorporada probablemente sea removible. Así que para evitar riesgos en esta clase, se requerirán restricciones fuertes sobre lo que puede ser liberado públicamente—análogas a restricciones en detalles de tecnologías nucleares, explosivas y otras peligrosas.[^136]

Una segunda clase de riesgos surge del escalamiento de máquinas que actúan como o se hacen pasar por personas. A nivel de daño a personas individuales, estos riesgos incluyen estafas mucho más efectivas, spam y phishing, y la proliferación de deepfakes no consensuales.[^137] A nivel colectivo, incluyen la disrupción de procesos sociales centrales como la discusión y debate público, nuestros sistemas societales de recolección, procesamiento y diseminación de información y conocimiento, y nuestros sistemas de elección política. Mitigar este riesgo probablemente involucre (a) leyes que restrinjan la suplantación de personas por sistemas de IA, y que hagan legalmente responsables a los desarrolladores de IA que crean sistemas que generan tales suplantaciones, (b) sistemas de marcas de agua y procedencia que identifiquen y clasifiquen (responsablemente) contenido de IA generado, y (c) nuevos sistemas epistémicos socio-técnicos que puedan crear una cadena confiable desde datos (p. ej., cámaras y grabaciones) hasta hechos, entendimiento y buenos modelos del mundo.[^138] Todo esto es posible, y la IA puede ayudar con algunas partes de ello.

Un tercer riesgo general es que en la medida en que algunas tareas son automatizadas, los humanos que actualmente realizan esas tareas pueden tener menos valor financiero como trabajo. Históricamente, automatizar tareas ha hecho que las cosas habilitadas por esas tareas sean más baratas y abundantes, mientras clasifica a las personas que previamente hacían esas tareas en aquellas que aún están involucradas en la versión automatizada (generalmente con mayor habilidad/pago), y aquellas cuyo trabajo vale menos o poco. En términos netos es difícil predecir en qué sectores se requerirá más versus menos trabajo humano en el sector resultante más grande pero más eficiente. En paralelo, la dinámica de automatización tiende a aumentar la desigualdad y productividad general, disminuir el costo de ciertos bienes y servicios (vía aumentos de eficiencia), y aumentar el costo de otros (vía [enfermedad de costos](https://en.wikipedia.org/wiki/Baumol_effect)). Para aquellos en el lado desfavorecido del aumento de desigualdad, es profundamente poco claro si la disminución de costo en esos ciertos bienes y servicios supera el aumento en otros, y lleva a un mayor bienestar general. Entonces, ¿cómo será esto para la IA? Debido a la relativa facilidad con que el trabajo intelectual humano puede ser reemplazado por IA general, podemos esperar una versión rápida de esto con IA general de propósito competitiva con humanos.[^139] Si cerramos la Puerta a la IAG, muchos menos empleos serán reemplazados completamente por agentes de IA; pero un desplazamiento laboral enorme aún es probable durante un período de años.[^140] Para evitar sufrimiento económico generalizado, probablemente será necesario implementar tanto alguna forma de activos básicos universales o ingreso, como también diseñar un cambio cultural hacia valorar y recompensar el trabajo centrado en humanos que sea más difícil de automatizar (en lugar de ver los precios laborales caer debido al aumento en trabajo disponible empujado fuera de otras partes de la economía.) Otras construcciones, como la de ["dignidad de datos"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society) (en la que los productores humanos de datos de entrenamiento reciben automáticamente regalías por el valor creado por esos datos en IA) pueden ayudar. La automatización por IA también tiene un segundo efecto adverso potencial, que es la automatización *inapropiada*. Junto con aplicaciones donde la IA simplemente hace un peor trabajo, esto incluiría aquellas donde los sistemas de IA probablemente violen preceptos morales, éticos o legales—por ejemplo en decisiones de vida y muerte, y en asuntos judiciales. Estos deben tratarse aplicando y extendiendo nuestros marcos legales actuales.

Finalmente, una amenaza significativa de la IA dentro de las puertas es su uso en persuasión personalizada, captura de atención y manipulación. Hemos visto en redes sociales y otras plataformas en línea el crecimiento de una economía de atención profundamente arraigada (donde los servicios en línea batallan ferozmente por la atención del usuario) y sistemas de ["capitalismo de vigilancia"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism) (en los que la información y perfilado del usuario se añade a la comodificación de la atención.) Es casi seguro que más IA será puesta al servicio de ambos. La IA ya se usa mucho en algoritmos de feeds adictivos, pero esto evolucionará hacia contenido generado por IA adictivo, personalizado para ser consumido compulsivamente por una sola persona. Y la entrada, respuestas y datos de esa persona, serán alimentados a la máquina de atención/publicidad para continuar el ciclo vicioso. Además, a medida que los asistentes de IA proporcionados por empresas tecnológicas se conviertan en la interfaz para más vida en línea, probablemente reemplazarán a los motores de búsqueda y feeds como el mecanismo por el cual ocurre la persuasión y monetización de clientes. El fracaso de nuestra sociedad para controlar estas dinámicas hasta ahora no augura bien. Algo de esta dinámica puede disminuirse vía regulaciones concernientes a privacidad, derechos de datos y manipulación. Llegar más a la raíz del problema puede requerir perspectivas diferentes, como la de asistentes de IA leales (discutidos abajo.)

El resultado de esta discusión es de esperanza: los sistemas basados en herramientas dentro de las Puertas—al menos mientras permanezcan comparables en poder y capacidad a los sistemas más avanzados de hoy—son probablemente manejables si hay voluntad y coordinación para hacerlo. Las instituciones humanas decentes, empoderadas por herramientas de IA,[^141] pueden hacerlo. También podríamos fallar en hacerlo. Pero es difícil ver cómo permitir sistemas más poderosos ayudaría—excepto poniéndolos a cargo y esperando lo mejor.

### Seguridad nacional

Las carreras por supremacía de IA—impulsadas por seguridad nacional u otras motivaciones—nos llevan hacia sistemas de IA poderosos no controlados que tenderían a absorber, en lugar de otorgar, poder. Una carrera de IAG entre EE.UU. y China es una carrera para determinar qué nación obtiene superinteligencia primero.

Entonces, ¿qué deberían hacer en su lugar los encargados de la seguridad nacional? Los gobiernos tienen experiencia sólida construyendo sistemas controlables y seguros, y deberían reforzar hacerlo así en IA, apoyando el tipo de proyectos de infraestructura que tienen más éxito cuando se hacen a escala y con respaldo gubernamental.

En lugar de un "proyecto Manhattan" imprudente hacia la IAG,[^142] el gobierno estadounidense podría lanzar un proyecto Apolo para sistemas controlables, seguros y confiables. Esto podría incluir por ejemplo:

- Un programa importante para (a) desarrollar los mecanismos de seguridad de hardware en chip y (b) la infraestructura, para gestionar el lado de cómputo de la IA poderosa. Estos podrían construirse sobre la [ley CHIPS](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) de EE.UU. y el [régimen de controles de exportación](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion).
- Una iniciativa a gran escala para desarrollar técnicas de verificación formal para que características particulares de sistemas de IA (como un interruptor de apagado) puedan ser *probadas* estar presentes o ausentes. Esto puede aprovechar la IA misma para desarrollar pruebas de propiedades.
- Un esfuerzo a escala nacional para crear software que sea verificablemente seguro, impulsado por herramientas de IA que pueden recodificar software existente en marcos verificablemente seguros.
- Un proyecto de inversión nacional en avance científico usando IA,[^143] funcionando como una asociación entre el DOE, NSF y NIH.

En general, hay una superficie de ataque enorme en nuestra sociedad que nos hace vulnerables a riesgos de la IA y su mal uso. Protegerse de algunos de estos riesgos requerirá inversión y estandarización de tamaño gubernamental. Estos proporcionarían vastamente más seguridad que echar gasolina al fuego de carreras hacia la IAG. Y si la IA va a ser construida en armamento y sistemas de comando y control, es crucial que la IA sea confiable y segura, lo cual la IA actual simplemente no es.

### Concentración de poder y sus mitigaciones

Este ensayo se ha enfocado en la idea del control humano de la IA y su potencial falla. Pero otra lente válida a través de la cual ver la situación de IA es a través de la *concentración de poder.* El desarrollo de IA muy poderosa amenaza con concentrar poder ya sea en las muy pocas y muy grandes manos corporativas que la han desarrollado y la controlarán, o en gobiernos usando IA como un nuevo medio para mantener su propio poder y control, o en los sistemas de IA mismos. O alguna mezcla impía de lo anterior. En cualquiera de estos casos la mayoría de la humanidad pierde poder, control y agencia. ¿Cómo podríamos combatir esto?

El primer paso y más importante, por supuesto, es un cierre de Puerta a la IAG más inteligente que humanos y superinteligencia. Estas explícitamente pueden reemplazar directamente a humanos y grupos de humanos. Si están bajo control corporativo o gubernamental concentrarán poder en esas corporaciones o gobiernos; si son "libres" concentrarán poder en sí mismas. Así que asumamos que las Puertas están cerradas. ¿Entonces qué?

Una solución propuesta a la concentración de poder es la IA "código abierto", donde los pesos del modelo están disponibles libre o ampliamente. Pero como se mencionó antes, una vez que un modelo es abierto, la mayoría de las medidas de seguridad o protecciones pueden ser (y generalmente son) eliminadas. Así que hay una tensión aguda entre por un lado la descentralización, y por el otro la seguridad y el control humano de sistemas de IA. También hay razones para ser escépticos de que los modelos abiertos por sí mismos combatirán significativamente la concentración de poder en IA más de lo que lo han hecho en sistemas operativos (aún dominados por Microsoft, Apple y Google a pesar de alternativas abiertas).[^144]

Sin embargo, puede haber formas de cuadrar este círculo—centralizar y mitigar riesgos mientras se descentraliza capacidad y recompensa económica. Esto requiere repensar tanto cómo se desarrolla la IA como cómo se distribuyen sus beneficios.

Nuevos modelos de desarrollo y propiedad pública de IA ayudarían. Esto podría tomar varias formas: IA desarrollada por el gobierno (sujeta a supervisión democrática),[^145] organizaciones de desarrollo de IA sin fines de lucro (como Mozilla para navegadores), o estructuras que permitan propiedad y gobernanza muy amplia. La clave es que estas instituciones estarían explícitamente constituidas para servir el interés público mientras operan bajo fuertes restricciones de seguridad.[^146] Regímenes regulatorios y de estándares/certificaciones bien elaborados también serán vitales, para que los productos de IA ofrecidos por un mercado vibrante permanezcan genuinamente útiles en lugar de explotadores hacia sus usuarios.

En términos de concentración de poder económico, podemos usar rastreo de procedencia y "dignidad de datos" para asegurar que los beneficios económicos fluyan más ampliamente. En particular, la mayor parte del poder de IA ahora (y en el futuro si mantenemos las Puertas cerradas) surge de datos generados por humanos, ya sean datos de entrenamiento directos o retroalimentación humana. Si las empresas de IA fueran requeridas a compensar a los proveedores de datos justamente,[^147] esto podría al menos ayudar a distribuir las recompensas económicas más ampliamente. Más allá de esto, otro modelo podría ser la propiedad pública de fracciones significativas de grandes empresas de IA. Por ejemplo, gobiernos capaces de gravar empresas de IA podrían invertir una fracción de recibos en un fondo de riqueza soberano que posea acciones en las empresas, y pague dividendos a la población.[^148]

Crucial en estos mecanismos es usar el poder de la IA misma para ayudar a distribuir poder mejor, en lugar de simplemente luchar contra la concentración de poder impulsada por IA usando medios no-IA. Un enfoque poderoso sería a través de asistentes de IA bien diseñados que operen con deber fiduciario genuino hacia sus usuarios—poniendo los intereses de los usuarios primero, especialmente por encima de los proveedores corporativos.[^149] Estos asistentes deben ser verdaderamente confiables, técnicamente competentes pero apropiadamente limitados basado en caso de uso y nivel de riesgo, y ampliamente disponibles para todos a través de canales públicos, sin fines de lucro, o con fines de lucro certificados. Así como nunca aceptaríamos un asistente humano que secretamente trabaje contra nuestros intereses para otra parte, no deberíamos aceptar asistentes de IA que vigilen, manipulen o extraigan valor de sus usuarios para beneficio corporativo.

Tal transformación alteraría fundamentalmente la dinámica actual donde los individuos quedan para negociar solos con vastas máquinas corporativas y burocráticas (impulsadas por IA) que priorizan la extracción de valor sobre el bienestar humano. Mientras hay muchos enfoques posibles para redistribuir el poder impulsado por IA más ampliamente, ninguno emergerá por defecto: deben ser deliberadamente diseñados y gobernados con mecanismos como requerimientos fiduciarios, provisión pública y acceso escalonado basado en riesgo.

Los enfoques para mitigar la concentración de poder pueden enfrentar vientos en contra significativos de poderes incumbentes.[^150] Pero hay caminos hacia el desarrollo de IA que no requieren elegir entre seguridad y poder concentrado. Construyendo las instituciones correctas ahora, podríamos asegurar que los beneficios de la IA sean ampliamente compartidos mientras sus riesgos son cuidadosamente gestionados.

### Nuevas estructuras de gobernanza y sociales

Nuestras estructuras de gobernanza actuales están luchando: son lentas para responder, a menudo capturadas por intereses especiales, y [cada vez más desconfiadas por el público.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) Sin embargo, esto no es una razón para abandonarlas—todo lo contrario. Algunas instituciones pueden necesitar reemplazo, pero más ampliamente necesitamos nuevos mecanismos que puedan mejorar y suplementar nuestras estructuras existentes, ayudándolas a funcionar mejor en nuestro mundo que evoluciona rápidamente.

Mucho de nuestra debilidad institucional surge no de estructuras gubernamentales formales, sino de instituciones sociales degradadas: nuestros sistemas para desarrollar entendimiento compartido, coordinar acción y conducir discurso significativo. Hasta ahora, la IA ha acelerado esta degradación, inundando nuestros canales de información con contenido generado, señalándonos al contenido más polarizante y divisivo, y haciendo más difícil distinguir verdad de ficción.

Pero la IA podría realmente ayudar a reconstruir y fortalecer estas instituciones sociales. Considera tres áreas cruciales:

Primero, la IA podría ayudar a restaurar confianza en nuestros sistemas epistémicos—nuestras formas de saber qué es verdad. Podríamos desarrollar sistemas impulsados por IA que rastreen y verifiquen la procedencia de información, desde datos en bruto hasta análisis y conclusiones. Estos sistemas podrían combinar verificación criptográfica con análisis sofisticado para ayudar a las personas a entender no solo si algo es verdad, sino cómo sabemos que es verdad.[^151] Los asistentes de IA leales podrían encargarse de seguir los detalles para asegurar que se confirmen.

Segundo, la IA podría habilitar nuevas formas de coordinación a gran escala. Muchos de nuestros problemas más apremiantes—desde el cambio climático hasta la resistencia a antibióticos—son fundamentalmente problemas de coordinación. Estamos [atrapados en situaciones que son peores de lo que podrían ser para casi todos](https://equilibriabook.com/), porque ningún individuo o grupo puede permitirse hacer el primer movimiento. Los sistemas de IA podrían ayudar modelando estructuras de incentivos complejas, identificando caminos viables hacia mejores resultados, y facilitando la construcción de confianza y mecanismos de compromiso necesarios para llegar ahí.

Tal vez más intrigante, la IA podría habilitar formas completamente nuevas de discurso social. Imagina poder "hablar con una ciudad"[^152]—no solo ver estadísticas, sino tener un diálogo significativo con un sistema de IA que procese y sintetice las opiniones, experiencias, necesidades y aspiraciones de millones de residentes. O considera cómo la IA podría facilitar diálogo genuino entre grupos que actualmente se hablan sin escucharse, ayudando a cada lado a entender mejor las preocupaciones y valores reales del otro en lugar de sus caricaturas mutuas.[^153] O la IA podría ofrecer intermediación hábil y creíblemente neutral de disputas entre personas o incluso grandes grupos de personas (¡que podrían todos interactuar con ella directa e individualmente!) La IA actual es totalmente capaz de hacer este trabajo, pero las herramientas para hacerlo no surgirán por sí mismas, o vía incentivos de mercado.

Estas posibilidades podrían sonar utópicas, especialmente dado el papel actual de la IA en degradar el discurso y la confianza. Pero eso es precisamente por lo que debemos desarrollar activamente estas aplicaciones positivas. Cerrando las Puertas a la IAG incontrolable y priorizando IA que mejore la agencia humana, podemos dirigir el progreso tecnológico hacia un futuro donde la IA sirva como una fuerza para el empoderamiento, la resistencia y el avance colectivo.

[^124]: Dicho eso, mantenerse alejado de la triple intersección desafortunadamente no es tan fácil como uno podría desear. Presionar la capacidad muy fuerte en cualquiera de los tres aspectos tiende a aumentarla en los otros. En particular, puede ser difícil crear una inteligencia extremadamente general y capaz que no pueda ser fácilmente convertida en autónoma. Un enfoque es entrenar modelos ["miopes"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) con capacidad de planificación limitada. Otro sería enfocarse en diseñar sistemas ["oráculo"](https://arxiv.org/abs/1711.05541) puros que se alejarían de responder preguntas orientadas a la acción.

[^125]: ¡Muchas empresas fallan en darse cuenta de que ellas también eventualmente serían desplazadas por IAG, incluso si toma más tiempo—si lo hicieran, podrían presionar un poco menos en esas Puertas!

[^126]: Los sistemas de IA podrían comunicarse de maneras más eficientes pero menos inteligibles, pero mantener el entendimiento humano debería tomar prioridad.

[^127]: Esta idea de IA modular e interpretable ha sido desarrollada en detalle por varios investigadores; ver p. ej. el modelo ["Servicios Comprensivos de IA"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) de Drexler, la ["Arquitectura de Agencia Abierta"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai) de Dalrymple y otros. Mientras tales sistemas podrían requerir más esfuerzo de ingeniería que redes neuronales monolíticas entrenadas con computación masiva, eso es precisamente donde los límites de computación ayudan—haciendo que el camino más seguro y transparente también sea el más práctico.

[^128]: Sobre casos de seguridad en general ver [este manual](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). Pertinente a IA en particular, ver [Wasil et al.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [Clymer et al.](https://arxiv.org/abs/2403.10462), [Buhl et al.](https://arxiv.org/abs/2410.21572), y [Balesni et al.](https://arxiv.org/abs/2411.03336)

[^129]: De hecho ya estamos viendo esta tendencia impulsada solo por el alto costo de inferencia: modelos más pequeños y más especializados "destilados" de los más grandes y capaces de ejecutarse en hardware menos costoso.

[^130]: Entiendo por qué aquellos emocionados sobre el ecosistema tecnológico de IA pueden oponerse a lo que ven como regulación onerosa en su industria. Pero es francamente desconcertante para mí por qué, digamos, un capitalista de riesgo querría permitir el descontrol hacia IAG y superinteligencia. Esos sistemas (y empresas, mientras permanezcan bajo control de empresas) se *comerán todas las startups como aperitivo*. Probablemente incluso *antes* que comerse otras industrias. Cualquiera invertido en un ecosistema de IA próspero debería priorizar asegurar que el desarrollo de IAG no lleve a monopolización por unos pocos jugadores dominantes.

[^131]: Como el economista y ex investigador de Deepmind Michael Webb [lo puso](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/), "Creo que si paráramos todo desarrollo de modelos de lenguaje más grandes hoy, así que GPT-4 y Claude y lo que sea, y son las últimas cosas que entrenamos de ese tamaño—así que estamos permitiendo mucha más iteración en cosas de ese tamaño y todo tipo de ajustes finos, pero nada más grande que eso, no mayores avances—solo lo que tenemos hoy creo que es suficiente para impulsar 20 o 30 años de crecimiento económico increíble."

[^132]: Por ejemplo, el sistema alphafold de DeepMind usó solo una centésima de milésima del número de FLOP de GPT-4.

[^133]: La dificultad de los automóviles autónomos es importante notar aquí: aunque nominalmente una tarea específica, y lograble con confiabilidad justa con sistemas de IA relativamente pequeños, conocimiento y entendimiento extensivo del mundo real es necesario para obtener confiabilidad al nivel necesario en tal tarea crítica para la seguridad.

[^134]: Por ejemplo, dado un presupuesto de computación, probablemente veríamos modelos de IAGG pre-entrenados en (digamos) la mitad de ese presupuesto, y la otra mitad usada para entrenar capacidad muy alta en un rango más específico de tareas. Esto daría capacidad específica súper-humana respaldada por inteligencia general casi humana.

[^135]: La técnica de alineación dominante actual es "aprendizaje por refuerzo con retroalimentación humana" [(ARLH)](https://arxiv.org/abs/1706.03741) y usa retroalimentación humana para crear una señal de recompensa/castigo para aprendizaje por refuerzo del modelo de IA. Esta y técnicas relacionadas como [IA constitucional](https://arxiv.org/abs/2212.08073) están funcionando sorprendentemente bien (aunque carecen de robustez y pueden ser eludidas con esfuerzo modesto.) Además, los modelos de lenguaje actuales son generalmente competentes en razonamiento de sentido común que no cometerán errores morales tontos. Este es algo así como un punto dulce: lo suficientemente inteligente para entender lo que la gente quiere (en la medida en que puede definirse), pero no lo suficientemente inteligente para planear decepciones elaboradas o causar daño enorme cuando se equivocan.

[^136]: A largo plazo, cualquier nivel de capacidad de IA que se desarrolle probablemente se prolifere, ya que en última instancia es software, y útil. Necesitaremos tener mecanismos robustos para defendernos contra los riesgos que tales sistemas plantean. Pero *no tenemos eso ahora* así que debemos ser muy medidos en cuánto se permite que se proliferen modelos de IA poderosos.

[^137]: La gran mayoría de estos son deepfakes pornográficos no consensuales, incluyendo de menores.

[^138]: Muchos ingredientes para tales soluciones existen, en forma de leyes "bot-o-no" (en la ley de IA de la UE entre otros lugares), [tecnologías de rastreo de procedencia de la industria](https://c2pa.org/), [agregadores de noticias innovadores](https://www.improvethenews.org/), [agregadores](https://metaculus.com/) de predicción y mercados, etc.

[^139]: La ola de automatización puede no seguir patrones previos, en que tareas relativamente *altas* de habilidad como escritura de calidad, interpretación de ley, o dar consejo médico, pueden ser tanto o incluso más vulnerables a automatización que tareas de menor habilidad.

[^140]: Para modelado cuidadoso del efecto de IAG en salarios, ver el reporte [aquí](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek), y detalles sangrientos [aquí](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0), de Anton Korinek y colaboradores. Encuentran que a medida que más piezas de empleos son automatizadas, productividad y salarios suben—hasta un punto. Una vez que *demasiado* es automatizado, la productividad continúa aumentando, pero los salarios se desploman porque las personas son reemplazadas completamente por IA eficiente. Por esto es tan útil cerrar las Puertas: obtenemos la productividad sin los salarios humanos desaparecidos.

[^141]: Hay muchas formas en que la IA puede usarse como, y para ayudar a construir, tecnologías "defensivas" para hacer protecciones y gestión más robustas. Ver [esta](https://vitalik.eth.limo/general/2025/01/05/dacc2.html) publicación influyente describiendo esta agenda "D/acc".

[^142]: Algo irónicamente, un proyecto Manhattan estadounidense probablemente haría poco para acelerar las líneas de tiempo hacia IAG—el dial de inversión humana y fiscal en progreso de IA ya está fijado en 11. Los resultados primarios serían inspirar un proyecto similar en China (que sobresale en proyectos de infraestructura a nivel nacional), hacer acuerdos internacionales limitando el riesgo de IA mucho más difícil, y alarmar a otros adversarios geopolíticos de EE.UU. como Rusia.

[^143]: El programa ["Recurso Nacional de Investigación de IA"](https://nairrpilot.org/) es un buen paso actual en esta dirección y debería expandirse.

[^144]: Ver [este análisis](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807) de los varios significados e implicaciones de "abierto" en productos tecnológicos y cómo algunos han llevado a más, en lugar de menos, atrincheramiento de dominancia.

[^145]: Los planes en EE.UU. para un [Recurso Nacional de Investigación de IA](https://nairratdoe.ornl.gov/) y el lanzamiento reciente de una [Fundación Europea de IA](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/) son pasos interesantes en esta dirección.

[^146]: El desafío aquí no es técnico sino institucional—necesitamos urgentemente ejemplos y experimentos del mundo real de cómo podría verse el desarrollo de IA de interés público.

[^147]: Esto va contra los modelos de negocio actuales de grandes tecnológicas y requeriría tanto acción legal como nuevas normas.

[^148]: Solo algunos gobiernos podrán hacerlo. Una idea más radical es [un fondo universal de este tipo, bajo propiedad conjunta de todos los humanos.](https://futureoflife.org/project/the-windfall-trust/)

[^149]: Para una exposición extensa de este caso ver [este artículo](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338) sobre lealtad de IA. Desafortunadamente la trayectoria por defecto de asistentes de IA probablemente sea una donde son cada vez más desleales.

[^150]: Algo irónicamente, muchos poderes incumbentes también están en riesgo de desempoderamiento respaldado por IA; pero puede ser difícil para ellos percibir esto hasta y a menos que el proceso llegue bastante lejos.

[^151]: Algunos esfuerzos interesantes en esta dirección están representados por [la coalición c2pa](https://c2pa.org/) sobre verificación criptográfica; [Verity](https://www.improvethenews.org/) y [Ground news](https://ground.news/) sobre mejores epistémicas de noticias; y [Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com) y mercados de predicción sobre fundamentar el discurso en predicciones falsificables.

[^152]: Ver [este](https://talktothecity.org/) fascinante proyecto piloto.

[^153]: Ver [Kialo](https://www.kialo-edu.com/), y esfuerzos del [Proyecto de Inteligencia Colectiva](https://www.cip.org/) para algunos ejemplos.

## Capítulo 10 - La elección que tenemos ante nosotros

Para preservar nuestro futuro humano, debemos elegir cerrar las Puertas a la IAG y la superinteligencia.

La última vez que la humanidad compartió la Tierra con otras mentes que hablaban, pensaban, construían tecnología y resolvían problemas de propósito general fue hace 40,000 años en la Europa de la era glacial. Esas otras mentes se extinguieron, en su totalidad o en parte debido a los esfuerzos de las nuestras.

Ahora estamos regresando a ese tipo de época. Los productos más avanzados de nuestra cultura y tecnología —conjuntos de datos construidos a partir de todo nuestro patrimonio informativo de internet, y chips de 100 mil millones de elementos que son las tecnologías más complejas que jamás hemos creado— se están combinando para dar vida a sistemas de IA avanzados de propósito general.

Los desarrolladores de estos sistemas están empeñados en presentarlos como herramientas para el empoderamiento humano. Y de hecho podrían serlo. Pero no nos engañemos: nuestra trayectoria actual consiste en construir agentes digitales cada vez más poderosos, orientados a objetivos, capaces de tomar decisiones y con capacidades generales. Ya se desempeñan tan bien como muchos humanos en una amplia gama de tareas intelectuales, mejoran rápidamente y contribuyen a su propio perfeccionamiento.

A menos que esta trayectoria cambie o se tope con un obstáculo inesperado, pronto —en años, no décadas— tendremos inteligencias digitales peligrosamente poderosas. Incluso en el *mejor* de los escenarios, estas traerían grandes beneficios económicos (al menos para algunos de nosotros), pero solo a costa de una profunda disrupción en nuestra sociedad y el reemplazo de los humanos en la mayoría de las cosas más importantes que hacemos: estas máquinas pensarían por nosotros, planificarían por nosotros, decidirían por nosotros y crearían por nosotros. Estaríamos mimados, pero como niños mimados. Es mucho más probable que estos sistemas reemplazaran a los humanos tanto en las cosas positivas *como* negativas que hacemos, incluyendo la explotación, la manipulación, la violencia y la guerra. ¿Podemos sobrevivir a versiones de estas potenciadas por IA? Finalmente, es más que plausible que las cosas no salieran nada bien: que relativamente pronto fuéramos reemplazados no solo en lo que hacemos, sino en lo que *somos*, como arquitectos de la civilización y el futuro. Pregúntenles a los neandertales cómo resulta eso. Quizás nosotros también les proporcionamos baratijas adicionales durante un tiempo.

*No tenemos que hacer esto.* Tenemos IA que compite con los humanos, y no hay necesidad de construir IA con la que *no podamos* competir. Podemos construir herramientas de IA increíbles sin construir una especie sucesora. La idea de que la IAG y la superinteligencia son inevitables es una *elección que se disfraza de destino*.

Al imponer algunos límites estrictos y globales, podemos mantener la capacidad general de la IA aproximadamente al nivel humano mientras seguimos cosechando los beneficios de la capacidad de las computadoras para procesar datos de maneras que nosotros no podemos, y automatizar tareas que ninguno de nosotros quiere hacer. Estas aún plantearían muchos riesgos, pero si se diseñan y gestionan bien, serían una bendición enorme para la humanidad, desde la medicina hasta la investigación y los productos de consumo.

Imponer límites requeriría cooperación internacional, pero menos de lo que uno podría pensar, y esos límites aún dejarían mucho espacio para una industria de IA y hardware de IA enorme centrada en aplicaciones que mejoren el bienestar humano, en lugar de la búsqueda pura del poder. Y si, con fuertes garantías de seguridad y después de un diálogo global significativo, decidimos ir más allá, esa opción sigue siendo nuestra para perseguirla.

La humanidad debe *elegir* cerrar las Puertas a la IAG y la superinteligencia.

Para mantener el futuro humano.

### Una nota del autor

Gracias por tomarte el tiempo de explorar este tema con nosotros.

Escribí este ensayo porque como científico siento que es importante decir la verdad sin adornos, y porque como persona siento que es crucial que actuemos rápida y decisivamente para abordar un tema que cambia el mundo: el desarrollo de sistemas de IA más inteligentes que los humanos.

Si queremos responder a este estado de cosas extraordinario con sabiduría, debemos estar preparados para examinar críticamente la narrativa predominante de que la IAG y la superinteligencia 'deben' construirse para asegurar nuestros intereses, o son 'inevitables' y no pueden detenerse. Estas narrativas nos dejan sin poder, incapaces de ver los caminos alternativos que tenemos por delante.

Espero que te unas a mí pidiendo cautela ante la imprudencia, y valor ante la codicia.

Espero que te unas a mí pidiendo un futuro humano.

*– Anthony*

![Anthony Aguirre signature](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAnthony-Aguirre-signature-300x84.png&w=3840&q=75)

## Apéndices

Información suplementaria, incluyendo detalles técnicos sobre contabilidad de cómputo, un ejemplo de implementación de un 'cierre de puertas', detalles para un régimen estricto de responsabilidad legal de IAG, y un enfoque escalonado para estándares de seguridad y protección de IAG.

### Apéndice A: Detalles técnicos de contabilidad de cómputo

Se requiere un método detallado tanto para la "fuente de verdad" como para buenas aproximaciones del cómputo total utilizado en entrenamiento e inferencia para lograr controles basados en cómputo con significado real. He aquí un ejemplo de cómo podría contabilizarse la "fuente de verdad" a nivel técnico.

**Definiciones:**

*Grafo causal de cómputo:* Para una salida O dada de un modelo de IA, existe un conjunto de computaciones digitales para las cuales cambiar el resultado de esa computación podría potencialmente cambiar O. (Esto debe asumirse de forma conservadora, es decir, debe haber una razón clara para creer que una computación es independiente de un precursor que tanto ocurre antes en el tiempo como tiene una ruta causal física potencial de efecto.) Esto incluye la computación realizada por el modelo de IA durante la inferencia, así como las computaciones que fueron parte de la entrada, preparación de datos y entrenamiento del modelo. Debido a que cualquiera de estas puede ser en sí misma la salida de un modelo de IA, esto se calcula recursivamente, cortando donde un humano ha proporcionado un cambio significativo a la entrada.

*Cómputo de Entrenamiento:* El cómputo total, en FLOP u otras unidades, implicado por el grafo causal de cómputo de una red neuronal (incluyendo preparación de datos, entrenamiento y ajuste fino, y cualquier otra computación.)

*Cómputo de Salida:* El cómputo total en el grafo causal de cómputo de una salida de IA dada, incluyendo todas las redes neuronales (e incluyendo su Cómputo de Entrenamiento) y otras computaciones que contribuyen a esa salida.

*Tasa de Cómputo de Inferencia:* En una serie de salidas, la tasa de cambio (en FLOP/s u otras unidades) del Cómputo de Salida entre salidas, es decir, el cómputo utilizado para producir la siguiente salida, dividido por el intervalo temporal entre las salidas.

**Ejemplos y aproximaciones:**

- Para una sola red neuronal entrenada con datos creados por humanos, el Cómputo de Entrenamiento es simplemente el cómputo total de entrenamiento como se reporta habitualmente.
- Para tal red neuronal realizando inferencia a una tasa constante, la Tasa de Cómputo de Inferencia es aproximadamente la velocidad total del clúster de computación que realiza la inferencia en FLOP/s.
- Para el ajuste fino de modelos, el Cómputo de Entrenamiento del modelo completo se obtiene sumando el Cómputo de Entrenamiento del modelo no ajustado más la computación realizada durante el ajuste fino y para preparar cualquier dato utilizado en el ajuste fino.
- Para un modelo destilado, el Cómputo de Entrenamiento del modelo completo incluye el entrenamiento tanto del modelo destilado como del modelo más grande utilizado para proporcionar datos sintéticos u otra entrada de entrenamiento.
- Si se entrenan varios modelos, pero muchos "intentos" se descartan basándose en juicio humano, estos no cuentan hacia el Cómputo de Entrenamiento o de Salida del modelo retenido.

### Apéndice B: Ejemplo de implementación de un cierre de puertas

**Ejemplo de Implementación:** He aquí un ejemplo de cómo podría funcionar un cierre de puertas, dado un límite de 10<sup>27</sup> FLOP para entrenamiento y 10<sup>20</sup> FLOP/s para inferencia (ejecutar la IA):

**1\. Pausa:** Por razones de seguridad nacional, el poder ejecutivo de EE.UU. solicita a todas las empresas con sede en EE.UU., que hagan negocios en EE.UU., o que usen chips fabricados en EE.UU., cesar y desistir de cualquier nueva ejecución de entrenamiento de IA que pueda exceder el límite de 10<sup>27</sup> FLOP de Cómputo de Entrenamiento. EE.UU. debe iniciar discusiones con otros países que albergan desarrollo de IA, alentándolos enérgicamente a tomar medidas similares e indicando que la pausa estadounidense puede ser levantada si eligen no cumplir.

**2\. Supervisión y licencias de EE.UU.:** Por orden ejecutiva o acción de una agencia regulatoria existente, EE.UU. requiere que dentro de (digamos) un año:

- Todas las ejecuciones de entrenamiento de IA estimadas por encima de 10<sup>25</sup> FLOP realizadas por empresas que operan en EE.UU. sean registradas en una base de datos mantenida por una agencia regulatoria estadounidense. (Nota: Una versión ligeramente más débil de esto ya había sido incluida en la orden ejecutiva estadounidense sobre IA de 2023, ahora rescindida, requiriendo registro para modelos por encima de 10<sup>26</sup> FLOP.)
- Todos los fabricantes de hardware relevante para IA que operan en EE.UU. o hacen negocios con el gobierno estadounidense adhieran a un conjunto de requisitos sobre su hardware especializado y el software que lo maneja. (Muchos de estos requisitos podrían incorporarse en actualizaciones de software y firmware para hardware existente, pero las soluciones a largo plazo y robustas requerirían cambios en generaciones posteriores de hardware.) Entre estos está el requisito de que si el hardware es parte de un clúster interconectado de alta velocidad capaz de ejecutar 10<sup>18</sup> FLOP/s de computación, se requiere un nivel más alto de verificación, que incluye permisos regulares por un "gobernador" remoto que recibe tanto telemetría como solicitudes para realizar computación adicional.
- El custodio reporte la computación total realizada en su hardware a la agencia que mantiene la base de datos estadounidense.
- Se implementan gradualmente requisitos más estrictos para permitir una supervisión y permisos tanto más seguros como más flexibles.

**3\. Supervisión internacional:**

- EE.UU., China y cualquier otro país que albergue capacidad avanzada de fabricación de chips negocian un acuerdo internacional.
- Este acuerdo crea una nueva agencia internacional, análoga a la Agencia Internacional de Energía Atómica, encargada de supervisar el entrenamiento y ejecución de IA.
- Los países signatarios deben requerir que sus fabricantes domésticos de hardware de IA cumplan con un conjunto de requisitos al menos tan estrictos como los impuestos en EE.UU.
- Los custodios ahora deben reportar números de computación de IA tanto a las agencias en sus países de origen como a una nueva oficina dentro de la agencia internacional.
- Se alienta enérgicamente a países adicionales a unirse al acuerdo internacional existente: los controles de exportación por parte de países signatarios restringen el acceso a hardware de alta gama por no signatarios, mientras que los signatarios pueden recibir apoyo técnico en el manejo de sus sistemas de IA.

**4\. Verificación y cumplimiento internacional:**

- El sistema de verificación de hardware se actualiza para que reporte el uso de computación tanto al custodio original como también directamente a la oficina de la agencia internacional.
- La agencia, mediante discusión con los signatarios del acuerdo internacional, acuerda limitaciones de computación que luego adquieren fuerza legal en los países signatarios.
- En paralelo, puede desarrollarse un conjunto de estándares internacionales para que el entrenamiento y ejecución de IAs por encima de un umbral de computación (pero por debajo del límite) deban adherirse a esos estándares.
- La agencia puede, si es necesario para compensar mejores algoritmos etc., reducir el límite de computación. O, si se considera seguro y aconsejable (al nivel de garantías de seguridad demostrables), elevar el límite de computación.

### Apéndice C: Detalles para un régimen estricto de responsabilidad legal de IAG

**Detalles para un régimen estricto de responsabilidad legal de IAG**

- La creación y operación de un sistema de IA avanzado que es altamente general, capaz y autónomo, se considera una actividad "anormalmente peligrosa".
- Como tal, la responsabilidad legal por defecto para entrenar y operar tales sistemas es responsabilidad legal estricta, conjunta y solidaria (o su equivalente fuera de EE.UU.) por cualquier daño causado por el modelo o sus salidas/acciones.
- Se impondrá responsabilidad personal para ejecutivos y miembros de juntas directivas en casos de negligencia grave o conducta dolosa. Esto debe incluir sanciones penales para los casos más atroces.
- Existen numerosos refugios seguros bajo los cuales la responsabilidad legal revierte a la responsabilidad por defecto (basada en culpa, en EE.UU.) a la cual las personas y empresas normalmente estarían sujetas.
	- Modelos entrenados y operados por debajo de algún umbral de cómputo (que sería al menos 10x menor que los límites descritos arriba.)
	- IA que es "débil" (aproximadamente, por debajo del nivel de experto humano en las tareas para las cuales está destinada) y/o
	- IA que es "estrecha" (que tiene un alcance fijo y bastante limitado de tareas y operaciones para las cuales está específicamente diseñada y entrenada) y/o
	- IA que es "pasiva" (muy limitada en su capacidad – incluso bajo modificación modesta – para tomar acciones o realizar tareas complejas de múltiples pasos sin participación y control humano directo.)
	- Una IA que está garantizada de ser segura, protegida y controlable (demostrable mente segura, o un análisis de riesgo indica un nivel insignificante de daño esperado.)
- Los refugios seguros pueden reclamarse sobre la base de un [caso de seguridad](https://arxiv.org/abs/2410.21572) preparado por el desarrollador de IA y aprobado por una agencia o auditor acreditado por una agencia. Para reclamar un refugio seguro basado en cómputo, el desarrollador debe solo suministrar estimaciones creíbles del Cómputo de Entrenamiento total y Tasa de Inferencia máxima
- La legislación delinearía explícitamente situaciones bajo las cuales la medida cautelar del desarrollo de sistemas de IA con alto riesgo de daño público sería apropiada.
- Los consorcios de empresas, trabajando con ONGs y agencias gubernamentales, deben desarrollar estándares y normas que definan estos términos, cómo los reguladores deben otorgar refugios seguros, cómo los desarrolladores de IA deben desarrollar casos de seguridad, y cómo las cortes deben interpretar la responsabilidad legal donde los refugios seguros no se reclaman proactivamente.

### Apéndice D: Un enfoque escalonado para estándares de seguridad y protección de IAG

**Un enfoque escalonado para estándares de seguridad y protección de IAG**

| Nivel de Riesgo | Disparador(es) | Requisitos para entrenamiento | Requisito para implementación |
| --- | --- | --- | --- |
| NR-0 | IA débil en autonomía, generalidad e inteligencia | ninguno | ninguno |
| NR-1 | IA fuerte en uno de autonomía, generalidad e inteligencia | ninguno | Basado en riesgo y uso, potencialmente casos de seguridad aprobados por autoridades nacionales donde quiera que el modelo pueda ser usado |
| NR-2 | IA fuerte en dos de autonomía, generalidad e inteligencia | Registro con autoridad nacional con jurisdicción sobre el desarrollador | Caso de seguridad que acote el riesgo de daño mayor por debajo de niveles autorizados más auditorías de seguridad independientes (incluyendo redteaming de caja negra y caja blanca) aprobadas por autoridades nacionales donde quiera que el modelo pueda ser usado |
| NR-3 | IAG fuerte en autonomía, generalidad e inteligencia | Pre-aprobación de plan de seguridad y protección por autoridad nacional con jurisdicción sobre el desarrollador | Caso de seguridad que garantice riesgo acotado de daño mayor por debajo de niveles autorizados así como especificaciones requeridas, incluyendo ciberseguridad, controlabilidad, un interruptor de emergencia no removible, alineación con valores humanos, y robustez ante uso malicioso. |
| NR-4 | Cualquier modelo que también exceda 10<sup>27</sup> FLOP de Entrenamiento o 10<sup>20</sup> FLOP/s de Inferencia | Prohibido pendiente levantamiento acordado internacionalmente del límite de cómputo | Prohibido pendiente levantamiento acordado internacionalmente del límite de cómputo |

Clasificaciones de riesgo y estándares de seguridad/protección, con niveles basados en umbrales de cómputo así como combinaciones de alta autonomía, generalidad e inteligencia:

- *Autonomía fuerte* aplica si el sistema es capaz de realizar, o puede ser fácilmente modificado para realizar, tareas de múltiples pasos y/o tomar acciones complejas que son relevantes al mundo real, sin supervisión o intervención humana significativa. Ejemplos: vehículos autónomos y robots; bots de comercio financiero. No ejemplos: GPT-4; clasificadores de imágenes
- *Generalidad fuerte* indica un amplio alcance de aplicación, desempeño de tareas para las cuales el modelo no fue deliberada y específicamente entrenado, y capacidad significativa para aprender nuevas tareas. Ejemplos: GPT-4; mu-zero. No ejemplos: AlphaFold; vehículos autónomos; generadores de imágenes
- *Inteligencia fuerte* corresponde a igualar el desempeño a nivel de experto humano en las tareas en las cuales el modelo se desempeña mejor (y para un modelo general, a través de un rango amplio de tareas.) Ejemplos: AlphaFold; mu-zero; o3. No ejemplos: GPT-4; Siri

### Agradecimientos

Algunos agradecimientos a las personas que contribuyeron a Mantener el Futuro Humano.

Este trabajo refleja las opiniones del autor y no debe considerarse como la posición oficial del Future of Life Institute (aunque son compatibles; para su posición oficial consulte [esta página](https://futureoflife.org/our-position-on-ai/)), ni de cualquier otra organización con la que el autor esté afiliado.

Agradezco a los humanos Mark Brakel, Ben Eisenpress, Anna Hehir, Carlos Gutierrez, Emilia Javorsky, Richard Mallah, Jordan Scharnhorst, Elyse Fulcher, Max Tegmark, y Jaan Tallinn por sus comentarios sobre el manuscrito; a Tim Schrier por su ayuda con algunas referencias; a Taylor Jones y Elyse Fulcher por el embellecimiento de los diagramas.

Este trabajo hizo uso limitado de modelos de IA generativa (Claude y ChatGPT) en su creación, para algunas tareas de edición y análisis crítico. En el estándar bien establecido de niveles de participación de IA en obras creativas, este trabajo probablemente obtendría un 3/10. (¡En realidad no existe tal estándar! Pero debería existir.)

Estamos muy agradecidos con [Julius Odai](https://www.linkedin.com/in/julius-odai/) por producir esta versión web del ensayo, que hace que leer y navegar por el ensayo sea una experiencia muy placentera. Julius es un tecnólogo y participante reciente del curso de Gobernanza de IA de BlueDot Impact.