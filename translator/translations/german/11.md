# Anhänge

Ergänzende Informationen, einschließlich technischer Details zur Rechenleistungserfassung, eines Beispiels für die Umsetzung einer „Torschließung", Details für ein striktes AGI-Haftungsregime und eines gestuften Ansatzes für AGI-Sicherheits- und Sicherheitsstandards.

## Anhang A: Technische Details zur Rechenleistungserfassung

Eine detaillierte Methode sowohl für die „Ground Truth" als auch für gute Näherungswerte der gesamten Rechenleistung, die beim Training und bei der Inferenz verwendet wird, ist für aussagekräftige rechenleistungsbasierte Kontrollen erforderlich. Hier ist ein Beispiel dafür, wie die „Ground Truth" auf technischer Ebene erfasst werden könnte.

**Definitionen:**

*Rechenkausaler Graph:* Für eine gegebene Ausgabe O eines KI-Modells gibt es eine Menge digitaler Berechnungen, bei denen eine Änderung des Ergebnisses dieser Berechnung potenziell O verändern könnte. (Dies sollte konservativ angenommen werden, d.h. es sollte einen klaren Grund geben zu glauben, dass eine Berechnung unabhängig von einem Vorläufer ist, der sowohl zeitlich früher auftritt als auch einen physisch möglichen kausalen Wirkungspfad hat.) Dies umfasst Berechnungen, die vom KI-Modell während der Inferenz durchgeführt werden, sowie Berechnungen, die in die Eingabe, Datenvorbereitung und das Training des Modells eingeflossen sind. Da jede davon selbst die Ausgabe eines KI-Modells sein kann, wird dies rekursiv berechnet und dort abgeschnitten, wo ein Mensch eine erhebliche Änderung an der Eingabe vorgenommen hat.

*Training-Rechenleistung:* Die gesamte Rechenleistung in FLOP oder anderen Einheiten, die durch den rechenkausalen Graphen eines neuronalen Netzwerks entsteht (einschließlich Datenvorbereitung, Training und Feinabstimmung sowie anderer Berechnungen).

*Ausgabe-Rechenleistung:* Die gesamte Rechenleistung im rechenkausalen Graphen einer gegebenen KI-Ausgabe, einschließlich aller neuronalen Netzwerke (und einschließlich ihrer Training-Rechenleistung) und anderer Berechnungen, die in diese Ausgabe eingehen.

*Inferenz-Rechenleistungsrate:* In einer Serie von Ausgaben die Änderungsrate (in FLOP/s oder anderen Einheiten) der Ausgabe-Rechenleistung zwischen den Ausgaben, d.h. die Rechenleistung, die zur Erzeugung der nächsten Ausgabe verwendet wird, geteilt durch das zeitliche Intervall zwischen den Ausgaben.

**Beispiele und Näherungswerte:**

- Für ein einzelnes neuronales Netzwerk, das mit von Menschen erstellten Daten trainiert wurde, ist die Training-Rechenleistung einfach die gesamte Training-Rechenleistung, wie sie üblicherweise berichtet wird.
- Für ein solches neuronales Netzwerk, das Inferenz mit gleichmäßiger Rate durchführt, entspricht die Inferenz-Rechenleistungsrate ungefähr der Gesamtgeschwindigkeit des Rechenclusters, der die Inferenz in FLOP/s durchführt.
- Bei der Modell-Feinabstimmung ergibt sich die Training-Rechenleistung des vollständigen Modells aus der Training-Rechenleistung des nicht feinabgestimmten Modells plus der Berechnung, die während der Feinabstimmung und zur Vorbereitung aller bei der Feinabstimmung verwendeten Daten durchgeführt wurde.
- Für ein destilliertes Modell umfasst die Training-Rechenleistung des vollständigen Modells das Training sowohl des destillierten Modells als auch des größeren Modells, das zur Bereitstellung synthetischer Daten oder anderer Trainingseingaben verwendet wurde.
- Wenn mehrere Modelle trainiert werden, aber viele „Versuche" aufgrund menschlicher Beurteilung verworfen werden, zählen diese nicht zur Training- oder Ausgabe-Rechenleistung des behaltenen Modells.

## Anhang B: Beispiel für die Umsetzung einer Torschließung

**Umsetzungsbeispiel:** Hier ist ein Beispiel dafür, wie eine Torschließung funktionieren könnte, mit einem Grenzwert von 10<sup>27</sup> FLOP für das Training und 10<sup>20</sup> FLOP/s für die Inferenz (Betrieb der KI):

**1. Pause:** Aus Gründen der nationalen Sicherheit fordert die US-Exekutive alle Unternehmen mit Sitz in den USA, die Geschäfte in den USA tätigen oder in den USA hergestellte Chips verwenden, auf, alle neuen KI-Trainingsläufe einzustellen, die den Grenzwert von 10<sup>27</sup> FLOP Training-Rechenleistung überschreiten könnten. Die USA sollten Gespräche mit anderen Ländern beginnen, die KI-Entwicklung beherbergen, sie nachdrücklich ermutigen, ähnliche Schritte zu unternehmen, und signalisieren, dass die US-Pause aufgehoben werden könnte, falls sie sich entscheiden, nicht zu kooperieren.

**2. US-Aufsicht und Lizenzierung:** Durch Erlass der Exekutive oder Maßnahmen einer bestehenden Regulierungsbehörde verlangen die USA, dass innerhalb von (beispielsweise) einem Jahr:

- Alle KI-Trainingsläufe über geschätzten 10<sup>25</sup> FLOP, die von in den USA operierenden Unternehmen durchgeführt werden, in einer von einer US-Regulierungsbehörde verwalteten Datenbank registriert werden. (Hinweis: Eine etwas schwächere Version davon war bereits in der inzwischen widerrufenen US-Durchführungsverordnung zu KI von 2023 enthalten, die eine Registrierung für Modelle über 10<sup>26</sup> FLOP verlangte.)
- Alle KI-relevanten Hardware-Hersteller, die in den USA operieren oder Geschäfte mit der US-Regierung tätigen, sich an eine Reihe von Anforderungen für ihre spezialisierte Hardware und die sie antreibende Software halten. (Viele dieser Anforderungen könnten durch Software- und Firmware-Updates für bestehende Hardware umgesetzt werden, aber langfristige und robuste Lösungen würden Änderungen an späteren Hardware-Generationen erfordern.) Dazu gehört die Anforderung, dass wenn die Hardware Teil eines Hochgeschwindigkeits-Clusters ist, der 10<sup>18</sup> FLOP/s Berechnungen ausführen kann, eine höhere Verifizierungsebene erforderlich ist, die regelmäßige Genehmigung durch einen entfernten „Governor" einschließt, der sowohl Telemetrie als auch Anfragen zur Durchführung zusätzlicher Berechnungen erhält.
- Der Verwalter die gesamte auf seiner Hardware durchgeführte Berechnung an die die US-Datenbank führende Behörde meldet.
- Stärkere Anforderungen schrittweise eingeführt werden, um sowohl sicherere als auch flexiblere Aufsicht und Genehmigungsverfahren zu ermöglichen.

**3. Internationale Aufsicht:**

- Die USA, China und alle anderen Länder mit fortgeschrittener Chip-Herstellungskapazität verhandeln ein internationales Abkommen.
- Dieses Abkommen schafft eine neue internationale Behörde, analog zur Internationalen Atomenergiebehörde, die mit der Überwachung von KI-Training und -Ausführung beauftragt ist.
- Unterzeichnerstaaten müssen von ihren heimischen KI-Hardware-Herstellern verlangen, dass sie sich an Anforderungen halten, die mindestens so streng sind wie die in den USA auferlegten.
- Verwalter sind nun verpflichtet, KI-Berechnungszahlen sowohl an Behörden in ihren Heimatländern als auch an ein neues Büro innerhalb der internationalen Behörde zu melden.
- Zusätzliche Länder werden stark ermutigt, dem bestehenden internationalen Abkommen beizutreten: Exportkontrollen durch Unterzeichnerstaaten beschränken den Zugang zu High-End-Hardware für Nicht-Unterzeichner, während Unterzeichner technische Unterstützung bei der Verwaltung ihrer KI-Systeme erhalten können.

**4. Internationale Verifikation und Durchsetzung:**

- Das Hardware-Verifizierungssystem wird aktualisiert, sodass es die Rechenleistungsnutzung sowohl an den ursprünglichen Verwalter als auch direkt an das internationale Behördenbüro meldet.
- Die Behörde einigt sich über Diskussionen mit den Unterzeichnern des internationalen Abkommens auf Rechenleistungsbeschränkungen, die dann Rechtskraft in den Unterzeichnerländern erlangen.
- Parallel dazu kann eine Reihe internationaler Standards entwickelt werden, sodass Training und Betrieb von KIs über einem Rechenleistungsschwellenwert (aber unter dem Grenzwert) sich an diese Standards halten müssen.
- Die Behörde kann, falls notwendig zur Kompensation besserer Algorithmen usw., den Rechenleistungsgrenzwert senken. Oder, falls es als sicher und ratsam erachtet wird (auf der Ebene beweisbarer Sicherheitsgarantien), den Rechenleistungsgrenzwert erhöhen.

## Anhang C: Details für ein striktes AGI-Haftungsregime

**Details für ein striktes AGI-Haftungsregime**

- Die Entwicklung und der Betrieb eines fortgeschrittenen KI-Systems, das hochgradig allgemein, leistungsfähig und autonom ist, wird als „ungewöhnlich gefährliche" Aktivität betrachtet.
- Als solche ist die standardmäßige Haftung für Training und Betrieb solcher Systeme eine strenge, gesamtschuldnerische Haftung (oder deren Nicht-US-Äquivalent) für alle durch das Modell oder seine Ausgaben/Handlungen verursachten Schäden.
- Persönliche Haftung wird für Führungskräfte und Vorstandsmitglieder bei grober Fahrlässigkeit oder vorsätzlichem Fehlverhalten auferlegt. Dies sollte strafrechtliche Sanktionen für die schwerwiegendsten Fälle einschließen.
- Es gibt zahlreiche Schutzbestimmungen, unter denen die Haftung zur Standard-(verschuldensabhängigen, in den USA) Haftung zurückkehrt, der Menschen und Unternehmen normalerweise unterliegen würden.
	- Modelle, die unter einem bestimmten Rechenleistungsschwellenwert (der mindestens 10x niedriger wäre als die oben beschriebenen Obergrenzen) trainiert und betrieben werden.
	- KI, die „schwach" ist (grob gesagt, unter dem Niveau menschlicher Experten bei den Aufgaben, für die sie bestimmt ist) und/oder
	- KI, die „eng" ist (mit einem festen und ziemlich begrenzten Aufgabenbereich und Operationen, für die sie speziell entworfen und trainiert wurde) und/oder
	- KI, die „passiv" ist (sehr begrenzt in ihrer Fähigkeit – selbst unter bescheidener Modifikation – Handlungen auszuführen oder komplexe mehrstufige Aufgaben ohne direkte menschliche Beteiligung und Kontrolle durchzuführen).
	- Eine KI, die garantiert sicher, geschützt und kontrollierbar ist (beweisbar sicher oder eine Risikoanalyse zeigt ein vernachlässigbares Niveau erwarteter Schäden an).
- Schutzbestimmungen können auf der Basis eines [Safety Case](https://arxiv.org/abs/2410.21572) beansprucht werden, der vom KI-Entwickler erstellt und von einer Behörde oder einem von einer Behörde akkreditierten Prüfer genehmigt wurde. Um eine Schutzbestimmung basierend auf Rechenleistung zu beanspruchen, muss der Entwickler nur glaubwürdige Schätzungen der gesamten Training-Rechenleistung und maximalen Inferenzrate liefern.
- Die Gesetzgebung würde explizit Situationen umreißen, unter denen einstweilige Verfügungen gegen die Entwicklung von KI-Systemen mit hohem Risiko öffentlicher Schäden angemessen wären.
- Unternehmenskonsortien sollten in Zusammenarbeit mit NGOs und Regierungsbehörden Standards und Normen entwickeln, die diese Begriffe definieren, wie Regulierungsbehörden Schutzbestimmungen gewähren sollten, wie KI-Entwickler Safety Cases entwickeln sollten und wie Gerichte Haftung interpretieren sollten, wo Schutzbestimmungen nicht proaktiv beansprucht werden.

## Anhang D: Ein gestufter Ansatz für AGI-Sicherheits- und Schutzstandards

**Ein gestufter Ansatz für AGI-Sicherheits- und Schutzstandards**

| Risikostufe | Auslöser | Anforderungen für Training | Anforderungen für Einsatz |
| --- | --- | --- | --- |
| RT-0 | KI schwach in Autonomie, Allgemeinheit und Intelligenz | keine | keine |
| RT-1 | KI stark in einem von Autonomie, Allgemeinheit und Intelligenz | keine | Basierend auf Risiko und Verwendung, potenziell Safety Cases, die von nationalen Behörden genehmigt wurden, wo auch immer das Modell verwendet werden kann |
| RT-2 | KI stark in zwei von Autonomie, Allgemeinheit und Intelligenz | Registrierung bei nationaler Behörde mit Zuständigkeit für den Entwickler | Safety Case, der das Risiko schwerer Schäden unter autorisierten Niveaus begrenzt, plus unabhängige Sicherheitsprüfungen (einschließlich Black-Box- und White-Box-Red-Teaming), genehmigt von nationalen Behörden, wo auch immer das Modell verwendet werden kann |
| RT-3 | AGI stark in Autonomie, Allgemeinheit und Intelligenz | Vorab-Genehmigung des Sicherheits- und Schutzplans durch nationale Behörde mit Zuständigkeit für den Entwickler | Safety Case, der begrenztes Risiko schwerer Schäden unter autorisierten Niveaus sowie erforderliche Spezifikationen garantiert, einschließlich Cybersicherheit, Kontrollierbarkeit, einem nicht entfernbaren Notschalter, Alignment mit menschlichen Werten und Robustheit gegen böswillige Nutzung |
| RT-4 | Jedes Modell, das auch entweder 10<sup>27</sup> FLOP Training oder 10<sup>20</sup> FLOP/s Inferenz überschreitet | Verboten bis zur international vereinbarten Aufhebung der Rechenleistungsobergrenze | Verboten bis zur international vereinbarten Aufhebung der Rechenleistungsobergrenze |

Risikoklassifizierungen und Sicherheits-/Schutzstandards, mit Stufen basierend auf Rechenleistungsschwellenwerten sowie Kombinationen hoher Autonomie, Allgemeinheit und Intelligenz:

- *Starke Autonomie* gilt, wenn das System in der Lage ist, mehrstufige Aufgaben durchzuführen und/oder komplexe Handlungen zu unternehmen, die real-weltrelevant sind, ohne signifikante menschliche Aufsicht oder Intervention, oder leicht dazu gebracht werden kann. Beispiele: autonome Fahrzeuge und Roboter; Finanzhandels-Bots. Gegenbeispiele: GPT-4; Bildklassifizierer
- *Starke Allgemeinheit* zeigt einen weiten Anwendungsbereich an, Durchführung von Aufgaben, für die das Modell nicht absichtlich und spezifisch trainiert wurde, und erhebliche Fähigkeit, neue Aufgaben zu erlernen. Beispiele: GPT-4; mu-zero. Gegenbeispiele: AlphaFold; autonome Fahrzeuge; Bildgeneratoren
- *Starke Intelligenz* entspricht der Angleichung an die Leistung menschlicher Experten bei den Aufgaben, bei denen das Modell am besten abschneidet (und bei einem allgemeinen Modell über einen breiten Aufgabenbereich). Beispiele: AlphaFold; mu-zero; o3. Gegenbeispiele: GPT-4; Siri