# Kapitel 2 - Wichtiges Grundwissen über neuronale KI-Netzwerke

Wie funktionieren moderne KI-Systeme und was könnte bei der nächsten Generation von KI-Systemen auf uns zukommen?

Um zu verstehen, welche Folgen die Entwicklung leistungsfähigerer KI haben wird, müssen wir uns zunächst einige Grundlagen aneignen. Dieses und die nächsten beiden Kapitel erarbeiten diese Grundlagen und behandeln der Reihe nach, was moderne KI ausmacht, wie sie massive Berechnungen nutzt und in welcher Hinsicht ihre Allgemeinheit und Leistungsfähigkeit rasant zunimmt.[^1]

Es gibt viele Möglichkeiten, künstliche Intelligenz zu definieren, doch für unsere Zwecke liegt die Kerneigenschaft von KI darin, dass sie – im Gegensatz zu einem herkömmlichen Computerprogramm, das eine Liste von Anweisungen zur Ausführung einer Aufgabe darstellt – aus Daten oder Erfahrungen lernt, um Aufgaben zu erledigen, *ohne dass ihr explizit gesagt wird, wie sie dies tun soll.*

Nahezu alle bedeutsamen modernen KI-Systeme basieren auf neuronalen Netzwerken. Dies sind mathematische/rechnerische Strukturen, die durch eine sehr große Anzahl (Milliarden oder Billionen) von Zahlen („Gewichtungen") repräsentiert werden und eine Trainingsaufgabe gut erfüllen. Diese Gewichtungen werden durch iterative Anpassungen erstellt (oder vielleicht eher „gezüchtet" oder „gefunden"), sodass das neuronale Netzwerk einen numerischen Wert (auch „Loss" genannt) verbessert, der darauf ausgelegt ist, eine oder mehrere Aufgaben gut zu erfüllen.[^2] Dieser Vorgang wird als *Training* des neuronalen Netzwerks bezeichnet.[^3]

Es gibt viele Techniken für dieses Training, doch diese Details sind weit weniger relevant als die Art, wie die Bewertung definiert wird und wie daraus unterschiedliche Aufgaben entstehen, die das neuronale Netzwerk gut erfüllt. Historisch wurde eine wichtige Unterscheidung zwischen „enger" und „allgemeiner" KI getroffen.

Enge KI wird gezielt darauf trainiert, eine bestimmte Aufgabe oder eine kleine Gruppe von Aufgaben zu erfüllen (wie etwa Bilderkennung oder Schachspielen); sie erfordert ein erneutes Training für neue Aufgaben und hat einen begrenzten Fähigkeitsbereich. Wir verfügen bereits über übermenschliche enge KI, das heißt, für nahezu jede abgegrenzte, klar definierte Aufgabe, die ein Mensch erledigen kann, können wir wahrscheinlich eine Bewertung erstellen und dann erfolgreich ein enges KI-System trainieren, das diese besser erledigt als ein Mensch.

Allzweck-KI-Systeme können ein breites Spektrum von Aufgaben erfüllen, einschließlich vieler, für die sie nicht explizit trainiert wurden; sie können auch neue Aufgaben als Teil ihres Betriebs erlernen. Aktuelle große „multimodale Modelle"[^4] wie ChatGPT sind dafür beispielhaft: Trainiert auf einem sehr großen Textkorpus und Bildern können sie komplexe Schlussfolgerungen ziehen, Code schreiben, Bilder analysieren und bei einer Vielzahl intellektueller Aufgaben helfen. Obwohl sie sich noch erheblich von menschlicher Intelligenz unterscheiden, wie wir unten ausführlich sehen werden, hat ihre Allgemeinheit eine Revolution in der KI ausgelöst.[^5]

## Unvorhersagbarkeit: ein Schlüsselmerkmal von KI-Systemen

Ein wesentlicher Unterschied zwischen KI-Systemen und herkömmlicher Software liegt in der Vorhersagbarkeit. Die Ausgabe herkömmlicher Software kann unvorhersagbar sein – tatsächlich schreiben wir manchmal genau deshalb Software, um Ergebnisse zu erhalten, die wir nicht hätten vorhersagen können. Aber herkömmliche Software tut selten etwas, wofür sie nicht programmiert wurde – ihr Umfang und Verhalten entsprechen im Allgemeinen dem vorgesehenen Design. Ein erstklassiges Schachprogramm mag Züge machen, die kein Mensch vorhersagen könnte (sonst könnte er dieses Schachprogramm schlagen!), aber es wird im Allgemeinen nichts anderes tun als Schach spielen.

Wie herkömmliche Software hat enge KI einen vorhersagbaren Umfang und vorhersagbares Verhalten, kann aber unvorhersagbare Ergebnisse haben. Dies ist eigentlich nur eine andere Art, enge KI zu definieren: als KI, die herkömmlicher Software in ihrer Vorhersagbarkeit und ihrem Anwendungsbereich ähnelt.

Allzweck-KI ist anders: ihr Umfang (die Bereiche, in denen sie anwendbar ist), ihr Verhalten (die Art der Dinge, die sie tut) und ihre Ergebnisse (ihre tatsächlichen Ausgaben) können alle unvorhersagbar sein.[^6] GPT-4 wurde nur darauf trainiert, Text akkurat zu generieren, entwickelte aber viele Fähigkeiten, die seine Trainer weder vorhergesagt noch beabsichtigt hatten. Diese Unvorhersagbarkeit entspringt der Komplexität des Trainings: Da die Trainingsdaten Ausgaben vieler verschiedener Aufgaben enthalten, muss die KI effektiv lernen, diese Aufgaben zu erfüllen, um gut vorhersagen zu können.

Diese Unvorhersagbarkeit allgemeiner KI-Systeme ist ziemlich grundlegend. Obwohl es prinzipiell möglich ist, KI-Systeme sorgfältig so zu konstruieren, dass sie garantierte Grenzen ihres Verhaltens haben (wie später in diesem Essay erwähnt), sind KI-Systeme, wie sie derzeit entwickelt werden, sowohl praktisch als auch prinzipiell unvorhersagbar.

## Passive KI, Agenten, autonome Systeme und Alignment

Diese Unvorhersagbarkeit wird besonders wichtig, wenn wir betrachten, wie KI-Systeme tatsächlich eingesetzt und verwendet werden, um verschiedene Ziele zu erreichen.

Viele KI-Systeme sind relativ passiv in dem Sinne, dass sie hauptsächlich Informationen bereitstellen und der Benutzer Handlungen vornimmt. Andere, gemeinhin als *Agenten* bezeichnet, führen selbst Handlungen aus, mit unterschiedlichen Graden der Beteiligung eines Benutzers. Solche, die Handlungen mit relativ weniger externem Input oder Überwachung ausführen, können als *autonomer* bezeichnet werden. Dies bildet ein Spektrum in Bezug auf die Handlungsunabhängigkeit, von passiven Werkzeugen bis hin zu autonomen Agenten.[^7]

Was die Ziele von KI-Systemen angeht, so können diese direkt mit ihrem Trainingsziel verknüpft sein (z.B. ist das Ziel des „Gewinnens" für ein Go-spielendes System auch explizit das, wofür es trainiert wurde). Oder sie sind es nicht: ChatGPTs Trainingsziel besteht teilweise darin, Text vorherzusagen, teilweise darin, ein hilfreicher Assistent zu sein. Aber bei einer gegebenen Aufgabe wird sein Ziel vom Benutzer vorgegeben. Ziele können auch von einem KI-System selbst erstellt werden, nur sehr indirekt mit seinem Trainingsziel verbunden.[^8]

Ziele sind eng mit der Frage des „Alignments" verknüpft, das heißt der Frage, ob KI-Systeme *das tun werden, was wir von ihnen wollen*. Diese einfache Frage verbirgt ein enormes Maß an Subtilität.[^9] Beachten Sie vorerst, dass „wir" in diesem Satz sich auf viele verschiedene Personen und Gruppen beziehen könnte, was zu verschiedenen Arten von Alignment führt. Ein KI-System könnte beispielsweise seinem Benutzer gegenüber hochgradig *gehorsam* (oder [„loyal"](https://arxiv.org/abs/2003.11157)) sein – hier ist „wir" gleich „jeder von uns". Oder es könnte *souveräner* sein, primär von seinen eigenen Zielen und Einschränkungen angetrieben, aber dennoch weitgehend im gemeinsamen Interesse menschlichen Wohlbefindens handeln – „wir" ist dann „die Menschheit" oder „die Gesellschaft". Dazwischen liegt ein Spektrum, in dem eine KI weitgehend gehorsam wäre, aber möglicherweise Handlungen verweigert, die anderen oder der Gesellschaft schaden, gegen das Gesetz verstoßen usw.

Diese beiden Achsen – Grad der Autonomie und Art des Alignments – sind nicht völlig unabhängig voneinander. Zum Beispiel ist ein souveränes passives System, obwohl nicht ganz selbstwidersprüchlich, ein spannungsreiches Konzept, ebenso wie ein gehorsamer autonomer Agent.[^10] Es gibt einen klaren Sinn, in dem Autonomie und Souveränität Hand in Hand gehen. In ähnlicher Weise ist die Vorhersagbarkeit bei „passiven" und „gehorsamen" KI-Systemen tendenziell höher, während souveräne oder autonome eher unvorhersagbarer sind. All dies wird entscheidend sein für das Verständnis der Auswirkungen einer möglichen AGI und Superintelligenz.

Die Schaffung wirklich ausgerichteter KI, welcher Art auch immer, erfordert die Lösung dreier verschiedener Herausforderungen:

1. Verstehen, was „wir" wollen – was komplex ist, ob „wir" nun eine bestimmte Person oder Organisation (Loyalität) oder die Menschheit im weiteren Sinne (Souveränität) meint;
2. Systeme bauen, die regelmäßig in Übereinstimmung mit diesen Wünschen handeln – im Wesentlichen konsistent positives Verhalten schaffen;
3. Am grundlegendsten: Systeme schaffen, die sich wirklich um diese Wünsche „sorgen", anstatt nur so zu handeln, als täten sie es.

Die Unterscheidung zwischen zuverlässigem Verhalten und echter Sorge ist entscheidend. Genau wie ein menschlicher Angestellter Befehlen perfekt folgen könnte, während ihm jede echte Verpflichtung gegenüber der Mission der Organisation fehlt, könnte ein KI-System sich ausgerichtet verhalten, ohne wirklich menschliche Präferenzen zu schätzen. Wir können KI-Systeme durch Feedback dazu trainieren, Dinge zu sagen und zu tun, und sie können lernen, darüber zu urteilen, was Menschen wollen. Aber sie dazu zu bringen, menschliche Präferenzen *wirklich* zu schätzen, ist eine weit tiefgreifendere Herausforderung.[^11]

Die tiefgreifenden Schwierigkeiten bei der Lösung dieser Alignment-Herausforderungen und ihre Auswirkungen auf KI-Risiken werden weiter unten näher untersucht. Verstehen Sie vorerst, dass Alignment nicht nur ein technisches Merkmal ist, das wir KI-Systemen anhängen, sondern ein grundlegender Aspekt ihrer Architektur, der ihre Beziehung zur Menschheit prägt.

[^1]: Für eine sanfte, aber technische Einführung in maschinelles Lernen und KI, insbesondere Sprachmodelle, siehe [diese Website.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) Für eine weitere moderne Einführung in KI-Existenzrisiken siehe [diesen Artikel.](https://www.thecompendium.ai/) Für eine umfassende und maßgebliche wissenschaftliche Analyse des Stands der KI-Sicherheit siehe den aktuellen [International AI Safety Report.](https://arxiv.org/abs/2501.17805)

[^2]: Das Training erfolgt typischerweise durch die Suche nach einem lokalen Maximum des Scores in einem hochdimensionalen Raum, der durch die Modellgewichte gegeben ist. Indem geprüft wird, wie sich der Score ändert, wenn Gewichte angepasst werden, identifiziert der Trainingsalgorithmus, welche Anpassungen den Score am stärksten verbessern, und bewegt die Gewichte in diese Richtung.

[^3]: Bei einem Bilderkennungsproblem würde das neuronale Netzwerk beispielsweise Wahrscheinlichkeiten für Labels des Bildes ausgeben. Ein Score würde mit der Wahrscheinlichkeit zusammenhängen, die die KI der korrekten Antwort zuordnet. Das Trainingsverfahren würde dann Gewichte so anpassen, dass die KI beim nächsten Mal eine höhere Wahrscheinlichkeit für das korrekte Label für dieses Bild ausgibt. Dies wird dann sehr oft wiederholt. Dasselbe Grundverfahren wird beim Training im Wesentlichen aller modernen neuronalen Netzwerke verwendet, wenn auch mit komplexeren Bewertungsmechanismen.

[^4]: Die meisten multimodalen Modelle nutzen die „Transformer"-Architektur zur Verarbeitung und Generierung mehrerer Datentypen (Text, Bilder, Ton). Diese können alle in verschiedene Arten von „Tokens" zerlegt und dann gleichberechtigt behandelt werden. Multimodale Modelle werden zunächst darauf trainiert, Tokens in massiven Datensätzen akkurat vorherzusagen, dann durch Reinforcement Learning verfeinert, um Fähigkeiten zu verbessern und Verhaltensweisen zu formen.

[^5]: Dass Sprachmodelle darauf trainiert werden, eine Sache zu tun – Wörter vorherzusagen – hat manche dazu veranlasst, sie als enge KI zu bezeichnen. Aber das ist irreführend: Weil die gute Vorhersage von Text so viele verschiedene Fähigkeiten erfordert, führt diese Trainingsaufgabe zu einem überraschend allgemeinen System. Beachten Sie auch, dass diese Systeme umfangreich durch Reinforcement Learning trainiert werden, was effektiv Tausende von Menschen repräsentiert, die dem Modell ein Belohnungssignal geben, wenn es bei einer der vielen Sachen, die es tut, gute Arbeit leistet. Es erbt dann erhebliche Allgemeinheit von den Menschen, die dieses Feedback geben.

[^6]: Es gibt mehrere Arten, in denen KI unvorhersagbar ist. Eine ist, dass man im allgemeinen Fall nicht vorhersagen kann, was ein Algorithmus tun wird, ohne ihn tatsächlich auszuführen; es gibt [Theoreme](https://arxiv.org/abs/1310.3225) zu diesem Effekt. Das kann einfach daran liegen, dass die Ausgabe von Algorithmen komplex sein kann. Aber es ist besonders klar und relevant in dem Fall (wie bei Schach oder Go), wo die Vorhersage eine Fähigkeit implizieren würde (die KI zu schlagen), die der potentielle Vorhersagende nicht hat. Zweitens wird ein gegebenes KI-System nicht immer dieselbe Ausgabe bei derselben Eingabe produzieren – seine Ausgaben enthalten Zufälligkeit; das koppelt sich auch mit algorithmischer Unvorhersagbarkeit. Drittens können unerwartete und emergente Fähigkeiten aus dem Training entstehen, was bedeutet, dass sogar die *Arten* von Dingen, die ein KI-System kann und tun wird, unvorhersagbar sind. Dieser letzte Typ ist besonders wichtig für Sicherheitsüberlegungen.

[^7]: Siehe [hier](https://arxiv.org/abs/2502.02649) für eine ausführliche Übersicht darüber, was mit einem „autonomen Agenten" gemeint ist (zusammen mit ethischen Argumenten gegen deren Entwicklung).

[^8]: Sie hören vielleicht manchmal „KI kann keine eigenen Ziele haben." Das ist absoluter Unsinn. Es ist leicht, Beispiele zu generieren, wo KI Ziele hat oder entwickelt, die ihr nie gegeben wurden und nur ihr selbst bekannt sind. Sie sehen das bei aktuellen populären multimodalen Modellen nicht viel, weil es aus ihnen heraustrainiert wird; es könnte genauso leicht in sie hineintrainiert werden.

[^9]: Es gibt eine umfangreiche Literatur. Zum allgemeinen Problem siehe Christians [*The Alignment Problem*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821) und Russells [*Human-Compatible*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616). Auf einer technischeren Seite siehe z.B. [dieses Paper](https://arxiv.org/abs/2209.00626).

[^10]: Wir werden später sehen, dass solche Systeme, obwohl sie dem Trend widersprechen, tatsächlich sehr interessant und nützlich sind.

[^11]: Das heißt nicht, dass wir Emotionen oder Empfindungsfähigkeit benötigen. Vielmehr ist es von außen außerordentlich schwierig zu wissen, was die inneren Ziele, Präferenzen und Werte eines Systems sind. „Wirklich" würde hier bedeuten, dass wir starke genug Gründe haben, uns darauf zu verlassen, dass wir im Fall kritischer Systeme unser Leben darauf setzen können.