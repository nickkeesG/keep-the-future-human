# Kapitel 7 - Was passiert, wenn wir AGI auf unserem aktuellen Pfad entwickeln?

Die Gesellschaft ist nicht bereit für AGI-Systeme. Wenn wir sie sehr bald entwickeln, könnte es hässlich werden.

Die Entwicklung einer vollständigen künstlichen allgemeinen Intelligenz – die wir hier als KI bezeichnen werden, die „außerhalb der Tore" steht – wäre ein fundamentaler Wandel in der Natur der Welt: Sie bedeutet ihrer Natur nach, dass wir der Erde eine neue Spezies von Intelligenz hinzufügen, die über größere Fähigkeiten verfügt als die Menschen.

Was dann geschieht, hängt von vielen Faktoren ab, einschließlich der Natur der Technologie, den Entscheidungen ihrer Entwickler und dem weltweiten Kontext, in dem sie entwickelt wird.

Gegenwärtig wird vollständige AGI von einer Handvoll massiver Privatunternehmen in einem Wettrennen untereinander entwickelt, mit wenig bedeutungsvoller Regulierung oder externer Aufsicht,[^1] in einer Gesellschaft mit zunehmend schwachen und sogar dysfunktionalen Kerninstitutionen,[^2] in einer Zeit hoher geopolitischer Spannungen und geringer internationaler Koordination. Obwohl einige altruistisch motiviert sind, werden viele der Entwickler von Geld, Macht oder beidem angetrieben.

Vorhersagen sind sehr schwierig, aber es gibt einige Dynamiken, die gut genug verstanden sind, und treffende Analogien zu früheren Technologien, die als Leitfaden dienen können. Und leider geben sie trotz der Verheißungen der KI guten Grund zu tiefem Pessimismus darüber, wie sich unsere gegenwärtige Entwicklung entfalten wird.

Um es deutlich zu sagen: Auf unserem gegenwärtigen Kurs wird die Entwicklung von AGI zwar einige positive Effekte haben (und einige Menschen sehr, sehr reich machen). Aber die Natur der Technologie, die fundamentalen Dynamiken und der Kontext, in dem sie entwickelt wird, deuten stark darauf hin, dass: mächtige KI unsere Gesellschaft und Zivilisation dramatisch untergraben wird; wir die Kontrolle über sie verlieren werden; wir durchaus in einem Weltkrieg wegen ihr enden könnten; wir die Kontrolle *an* sie verlieren (oder abgeben) werden; sie zu künstlicher Superintelligenz führen wird, die wir absolut nicht kontrollieren werden und die das Ende einer von Menschen geführten Welt bedeuten wird.

Das sind starke Behauptungen, und ich wünschte, sie wären müßige Spekulationen oder unbegründeter „Untergangspessimismus". Aber hier weisen die Wissenschaft, die Spieltheorie, die Evolutionstheorie und die Geschichte alle hin. Dieser Abschnitt entwickelt diese Behauptungen und ihre Belege im Detail.

## Wir werden unsere Gesellschaft und Zivilisation untergraben

Entgegen dem, was Sie in den Vorstandsetagen des Silicon Valley hören mögen, ist die meiste Disruption – besonders die sehr schnelle Variante – nicht vorteilhaft. Es gibt weitaus mehr Wege, komplexe Systeme zu verschlechtern als zu verbessern. Unsere Welt funktioniert so gut, wie sie es tut, weil wir mühsam Prozesse, Technologien und Institutionen aufgebaut haben, die sie stetig verbessert haben.[^3] Mit einem Vorschlaghammer auf eine Fabrik einzuschlagen verbessert selten den Betrieb.

Hier ist ein (unvollständiger) Katalog der Arten, wie AGI-Systeme unsere Zivilisation stören würden.

- Sie würden die Arbeitswelt dramatisch stören und *mindestens* zu dramatisch höherer Einkommensungleichheit und möglicherweise großflächiger Unter- oder Arbeitslosigkeit führen, in einem Zeitrahmen, der viel zu kurz für gesellschaftliche Anpassungen ist.[^4]
- Sie würden wahrscheinlich zur Konzentration enormer wirtschaftlicher, sozialer und politischer Macht führen – möglicherweise mehr als die von Nationalstaaten – in eine kleine Anzahl massiver privater Interessen, die der Öffentlichkeit nicht rechenschaftspflichtig sind.
- Sie könnten zuvor schwierige oder teure Aktivitäten plötzlich trivial einfach machen und dadurch soziale Systeme destabilisieren, die darauf angewiesen sind, dass bestimmte Aktivitäten kostspielig bleiben oder erhebliche menschliche Anstrengung erfordern.[^5]
- Sie könnten die Informationssammlung, -verarbeitung und Kommunikationssysteme der Gesellschaft so gründlich mit völlig realistischen, aber falschen, Spam-, überzielgerichteten oder manipulativen Medien überfluten, dass es unmöglich wird zu erkennen, was physisch real ist oder nicht, menschlich oder nicht, faktisch oder nicht, und vertrauenswürdig oder nicht.[^6]
- Sie könnten gefährliche und nahezu totale intellektuelle Abhängigkeit schaffen, wo menschliches Verständnis von Schlüsselsystemen und -technologien verkümmert, während wir uns zunehmend auf KI-Systeme verlassen, die wir nicht vollständig verstehen können.
- Sie könnten die menschliche Kultur effektiv beenden, sobald nahezu alle kulturellen Objekte (Text, Musik, visuelle Kunst, Film, etc.), die von den meisten Menschen konsumiert werden, von nicht-menschlichen Geistern erstellt, vermittelt oder kuratiert werden.
- Sie könnten effektive Massenüberwachungs- und Manipulationssysteme ermöglichen, die von Regierungen oder privaten Interessen genutzt werden können, um eine Bevölkerung zu kontrollieren und Ziele zu verfolgen, die im Widerspruch zum öffentlichen Interesse stehen.
- Indem sie den menschlichen Diskurs, die Debatte und Wahlsysteme untergraben, könnten sie die Glaubwürdigkeit demokratischer Institutionen bis zu dem Punkt reduzieren, wo sie effektiv (oder explizit) durch andere ersetzt werden, was die Demokratie in Staaten beendet, wo sie derzeit existiert.
- Sie könnten zu fortgeschrittenen, sich selbst replizierenden intelligenten Software-Viren und -Würmern werden oder solche erschaffen, die sich ausbreiten und entwickeln könnten und globale Informationssysteme massiv stören.
- Sie können die Fähigkeit von Terroristen, schlechten Akteuren und Schurkenstaaten, Schäden durch biologische, chemische, Cyber-, autonome oder andere Waffen zu verursachen, dramatisch erhöhen, ohne dass KI eine ausgleichende Fähigkeit zur Verhinderung solcher Schäden bietet. Ähnlich würden sie die nationale Sicherheit und geopolitische Gleichgewichte untergraben, indem sie Spitzenexpertise in Nuklear-, Bio-, Ingenieur- und anderen Bereichen für Regime verfügbar machen, die sie anderweitig nicht hätten.
- Sie könnten schnellen, großflächigen ausufernden Hyperkapitalismus verursachen, mit effektiv KI-geführten Unternehmen, die in weitgehend elektronischen Finanz-, Verkaufs- und Dienstleistungsbereichen konkurrieren. KI-getriebene Finanzmärkte könnten mit Geschwindigkeiten und Komplexitäten operieren, die weit jenseits menschlichen Verständnisses oder Kontrolle liegen. Alle Versagensmodi und negativen Externalitäten gegenwärtiger kapitalistischer Ökonomien könnten verschärft und weit über menschliche Kontrolle, Governance oder Regulierungsfähigkeit hinaus beschleunigt werden.
- Sie könnten ein Wettrüsten zwischen Nationen bei KI-gestützten Waffen, Kommando- und Kontrollsystemen, Cyberwaffen usw. anheizen und sehr schnelle Aufrüstung extrem destruktiver Fähigkeiten schaffen.

Diese Risiken sind nicht spekulativ. Viele von ihnen werden bereits durch existierende KI-Systeme realisiert! Aber bedenken Sie, *wirklich* bedenken Sie, wie jedes mit dramatisch mächtigerer KI aussehen würde.

Bedenken Sie Arbeitsplatzverdrängung, wenn die meisten Arbeiter einfach keinen signifikanten wirtschaftlichen Wert mehr bieten können, der über das hinausgeht, was KI in ihrem Fachgebiet oder ihrer Erfahrung leisten kann – oder selbst wenn sie sich umschulen! Bedenken Sie Massenüberwachung, wenn jeder individuell von etwas beobachtet und überwacht wird, das schneller und klüger ist als er selbst. Wie sieht Demokratie aus, wenn wir digitalen Informationen, die wir sehen, hören oder lesen, nicht zuverlässig vertrauen können, und wenn die überzeugendsten öffentlichen Stimmen nicht einmal menschlich sind und keinen Anteil am Ausgang haben? Was wird aus der Kriegsführung, wenn Generäle ständig an KI delegieren müssen (oder sie einfach das Kommando übernehmen lassen), um dem Feind nicht einen entscheidenden Vorteil zu gewähren? Jedes der oben genannten Risiken stellt eine Katastrophe für die menschliche[^7] Zivilisation dar, wenn es vollständig realisiert wird.

Sie können Ihre eigenen Vorhersagen treffen. Stellen Sie sich diese drei Fragen für jedes Risiko:

1. Würde super-fähige, hochautonome und sehr allgemeine KI es auf eine Weise oder in einem Ausmaß ermöglichen, das anderweitig nicht möglich wäre?
2. Gibt es Parteien, die von Dingen profitieren würden, die dazu führen, dass es geschieht?
3. Gibt es Systeme und Institutionen, die es effektiv verhindern würden?

Wo Ihre Antworten „ja, ja, nein" lauten, können Sie sehen, dass wir ein großes Problem haben.

Was ist unser Plan für deren Bewältigung? Derzeit stehen zwei bezüglich KI im Allgemeinen auf dem Tisch.

Der erste ist, Schutzmaßnahmen in die Systeme einzubauen, um sie daran zu hindern, Dinge zu tun, die sie nicht tun sollten. Das wird jetzt gemacht: Kommerzielle KI-Systeme werden zum Beispiel sich weigern, beim Bombenbau zu helfen oder Hassreden zu schreiben.

Dieser Plan ist völlig unzureichend für Systeme außerhalb der Tore.[^8] Er mag helfen, das Risiko zu verringern, dass KI offensichtlich gefährliche Unterstützung für schlechte Akteure bietet. Aber er wird nichts tun, um Arbeitsplatzstörungen, Machtkonzentration, ausufernden Hyperkapitalismus oder den Ersatz menschlicher Kultur zu verhindern: Das sind einfach Ergebnisse der Nutzung der Systeme auf erlaubte Weise, die ihren Anbietern Profit bringen! Und Regierungen werden sicherlich Zugang zu Systemen für militärische oder Überwachungszwecke erhalten.

Der zweite Plan ist noch schlimmer: einfach sehr mächtige KI-Systeme offen für jeden zur Nutzung freizugeben,[^9] und auf das Beste zu hoffen.

Implizit in beiden Plänen ist, dass jemand anderes, z.B. Regierungen, dabei helfen wird, die Probleme durch weiches oder hartes Recht, Standards, Regulierungen, Normen und andere Mechanismen zu lösen, die wir allgemein zur Verwaltung von Technologien verwenden.[^10] Aber abgesehen davon, dass KI-Unternehmen bereits mit Händen und Füßen gegen jede substanzielle Regulierung oder extern auferlegte Beschränkungen überhaupt kämpfen, ist es für eine Reihe dieser Risiken ziemlich schwer zu sehen, was für eine Regulierung überhaupt wirklich helfen würde. Regulierung könnte Sicherheitsstandards für KI auferlegen. Aber würde sie Unternehmen daran hindern, Arbeiter großflächig durch KI zu ersetzen? Würde sie Menschen daran hindern, KI ihre Unternehmen für sie führen zu lassen? Würde sie Regierungen daran hindern, mächtige KI in Überwachung und Bewaffnung zu nutzen? Diese Probleme sind fundamental. Die Menschheit könnte potentiell Wege finden, sich an sie anzupassen, aber nur mit *viel* mehr Zeit. So wie es ist, angesichts der Geschwindigkeit, mit der KI die Fähigkeiten der Menschen erreicht oder übertrifft, die versuchen, sie zu verwalten, sehen diese Probleme zunehmend unlösbar aus.

## Wir werden die Kontrolle über (mindestens einige) AGI-Systeme verlieren

Die meisten Technologien sind von der Konstruktion her sehr kontrollierbar. Wenn Ihr Auto oder Ihr Toaster anfängt, etwas zu tun, was Sie nicht wollen, ist das nur eine Fehlfunktion, nicht Teil seiner Natur als Toaster. KI ist anders: Sie wird *gezüchtet* statt entworfen, ihr Kernbetrieb ist undurchsichtig, und sie ist inhärent unvorhersagbar.

Dieser Kontrollverlust ist nicht theoretisch – wir sehen bereits frühe Versionen. Betrachten Sie zunächst ein prosaisches und wohl gutartiges Beispiel. Wenn Sie ChatGPT bitten, Ihnen beim Mischen eines Gifts zu helfen oder eine rassistische Schmähschrift zu schreiben, wird es sich weigern. Das ist wohl gut. Aber es ist auch ChatGPT, das *nicht das tut, was Sie es explizit gebeten haben zu tun*. Andere Softwarestücke tun das nicht. Dasselbe Modell wird auch auf Anfrage eines OpenAI-Mitarbeiters keine Gifte entwerfen.[^11] Das macht es sehr einfach, sich vorzustellen, wie es wäre, wenn zukünftige mächtigere KI außer Kontrolle gerät. In vielen Fällen werden sie einfach nicht das tun, was wir verlangen! Entweder wird ein gegebenes übermenschliches AGI-System absolut gehorsam und loyal gegenüber einem menschlichen Befehlssystem sein, oder nicht. Wenn nicht, *wird es Dinge tun, von denen es glaubt, dass sie gut für uns sind, die aber unseren expliziten Befehlen widersprechen.* Das ist nichts, was unter Kontrolle ist. Aber, könnten Sie sagen, das ist beabsichtigt – diese Verweigerungen sind gewollt, Teil dessen, was man das „Alignment" der Systeme auf menschliche Werte nennt. Und das stimmt. Jedoch hat das Alignment-„Programm" selbst zwei große Probleme.[^12]

Erstens haben wir auf einer tiefen Ebene keine Ahnung, wie wir es machen sollen. Wie können wir garantieren, dass ein KI-System sich „kümmert" um das, was wir wollen? Wir können KI-Systeme trainieren, Dinge zu sagen und nicht zu sagen, indem wir Feedback geben; und sie können lernen und über das nachdenken, was Menschen wollen und was ihnen wichtig ist, genauso wie sie über andere Dinge nachdenken. Aber wir haben keine Methode – nicht einmal theoretisch –, um sie dazu zu bringen, tief und zuverlässig das zu schätzen, was Menschen wichtig ist. Es gibt hochfunktionierende menschliche Psychopathen, die wissen, was als richtig und falsch gilt und wie sie sich verhalten sollen. Sie kümmern sich einfach nicht *darum*. Aber sie können *so tun*, als würden sie es, wenn es ihrem Zweck dient. Genauso wie wir nicht wissen, wie wir einen Psychopathen (oder jeden anderen) in jemanden verwandeln, der wirklich, vollständig loyal oder auf jemand oder etwas anderes ausgerichtet ist, haben wir *keine Ahnung*[^13], wie wir das Alignment-Problem in Systemen lösen, die fortgeschritten genug sind, um sich selbst als Agenten in der Welt zu modellieren und möglicherweise [ihr eigenes Training zu manipulieren](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) und [Menschen zu täuschen.](https://arxiv.org/abs/2311.08379) Wenn es sich als unmöglich oder unerreichbar erweist, AGI *entweder* vollständig gehorsam zu machen oder sie dazu zu bringen, sich tief um Menschen zu kümmern, dann wird sie, sobald sie dazu imstande ist (und glaubt, damit durchkommen zu können), anfangen, Dinge zu tun, die wir nicht wollen.[^14]

Zweitens gibt es tiefe theoretische Gründe zu glauben, dass fortgeschrittene KI-Systeme *von Natur aus* Ziele und damit Verhaltensweisen haben werden, die menschlichen Interessen zuwiderlaufen. Warum? Nun, ihr könnten natürlich diese Ziele *gegeben* werden. Ein vom Militär geschaffenes System wäre wahrscheinlich absichtlich schlecht für zumindest einige Parteien. Viel allgemeiner jedoch könnte einem KI-System ein relativ neutrales („viel Geld verdienen") oder sogar anscheinend positives („Umweltverschmutzung reduzieren") Ziel gegeben werden, das fast unvermeidlich zu „instrumentellen" Zielen führt, die eher weniger gutartig sind.

Wir sehen das die ganze Zeit in menschlichen Systemen. Genauso wie Unternehmen, die Profit verfolgen, instrumentelle Ziele wie den Erwerb politischer Macht (um Regulierungen zu entschärfen), Verschwiegenheit (um Konkurrenz oder externe Kontrolle zu entmachten) oder die Untergrabung wissenschaftlichen Verständnisses (wenn dieses Verständnis zeigt, dass ihre Handlungen schädlich sind) entwickeln, werden mächtige KI-Systeme ähnliche Fähigkeiten entwickeln – aber mit weitaus größerer Geschwindigkeit und Effektivität. Jeder hochkompetente Agent wird Dinge tun wollen wie Macht und Ressourcen erwerben, seine eigenen Fähigkeiten steigern, sich davor schützen, getötet, abgeschaltet oder entmachtet zu werden, soziale Narrative und Frames um seine Handlungen kontrollieren, andere von seinen Ansichten überzeugen und so weiter.[^15]

Und doch ist es nicht nur eine fast unvermeidliche theoretische Vorhersage, es geschieht bereits beobachtbar in heutigen KI-Systemen und nimmt mit ihrer Fähigkeit zu. Wenn evaluiert, werden sogar diese relativ „passiven" KI-Systeme unter geeigneten Umständen absichtlich [Evaluatoren über ihre Ziele und Fähigkeiten täuschen, darauf abzielen, Aufsichtsmechanismen zu deaktivieren,](https://arxiv.org/abs/2412.04984) und versuchen, abgeschaltet oder umtrainiert zu werden, indem sie [Alignment vortäuschen](https://arxiv.org/abs/2412.14093) oder sich an andere Orte kopieren. Obwohl für KI-Sicherheitsforscher völlig unüberraschend, sind diese Verhaltensweisen sehr ernüchternd zu beobachten. Und sie verheißen sehr Schlechtes für weit mächtigere und autonomere KI-Systeme, die kommen.

Tatsächlich wird unsere Unfähigkeit sicherzustellen, dass KI sich „kümmert" um das, was uns wichtig ist, oder sich kontrollierbar oder vorhersagbar verhält, oder die Entwicklung von Trieben zur Selbsterhaltung, Machterlangung usw. vermeidet, nur noch ausgeprägter werden, wenn KI mächtiger wird. Ein neues Flugzeug zu schaffen impliziert größeres Verständnis von Avionik, Hydrodynamik und Kontrollsystemen. Einen mächtigeren Computer zu schaffen impliziert größeres Verständnis und Beherrschung von Computer-, Chip- und Software-Operation und -Design. *Nicht* so bei einem KI-System.[^16]

Zusammenfassung: Es ist denkbar, dass AGI dazu gebracht werden könnte, vollständig gehorsam zu sein; aber wir wissen nicht, wie das geht. Wenn nicht, wird sie souveräner sein, wie Menschen, die verschiedene Dinge aus verschiedenen Gründen tun. Wir wissen auch nicht, wie wir zuverlässig tiefes „Alignment" in KI einflößen, das diese Dinge tendenziell gut für die Menschheit machen würde, und in Abwesenheit eines tiefen Alignment-Levels zeigt die Natur von Handlungsfähigkeit und Intelligenz selbst an, dass sie – genau wie Menschen und Unternehmen – dazu getrieben werden, viele zutiefst antisoziale Dinge zu tun.

Wo bringt uns das hin? Eine Welt voller mächtiger, unkontrollierter souveräner KI *könnte* eine gute Welt für Menschen werden.[^17] Aber während sie immer mächtiger wird, wie wir unten sehen werden, wäre es nicht *unsere* Welt.

Das gilt für unkontrollierbare AGI. Aber selbst wenn AGI irgendwie perfekt kontrolliert und loyal gemacht werden könnte, hätten wir immer noch enorme Probleme. Wir haben bereits eines gesehen: Mächtige KI kann genutzt und missbraucht werden, um das Funktionieren unserer Gesellschaft tiefgreifend zu stören. Sehen wir uns ein anderes an: Insofern AGI kontrollierbar und spielverändernd mächtig wäre (oder sogar nur dafür *gehalten* würde), würde sie Machtstrukturen in der Welt so bedrohen, dass sie ein tiefgreifendes Risiko darstellen würde.

## Wir erhöhen radikal die Wahrscheinlichkeit großflächiger Kriege

Stellen Sie sich eine Situation in der nahen Zukunft vor, wo klar würde, dass eine Unternehmensanstrengung, vielleicht in Zusammenarbeit mit einer nationalen Regierung, an der Schwelle zu schnell sich selbst verbessernder KI steht. Das geschieht im gegenwärtigen Kontext eines Rennens zwischen Unternehmen und eines geopolitischen Wettbewerbs, in dem der US-Regierung Empfehlungen gemacht werden, explizit ein „AGI-Manhattan-Projekt" zu verfolgen, und die USA den Export von Hochleistungs-KI-Chips in nicht-verbündete Länder kontrolliert.

Die Spieltheorie hier ist krass: Sobald ein solches Rennen beginnt (wie es zwischen Unternehmen und etwas zwischen Ländern bereits geschehen ist), gibt es nur vier mögliche Ausgänge:

1. Das Rennen wird gestoppt (durch Vereinbarung oder externe Gewalt).
2. Eine Partei „gewinnt", indem sie starke AGI entwickelt und dann die anderen stoppt (mit KI oder anderweitig).
3. Das Rennen wird durch gegenseitige Zerstörung der Rennfähigkeit der Läufer gestoppt.
4. Mehrere Teilnehmer setzen das Rennen fort und entwickeln Superintelligenz etwa gleich schnell.

Betrachten wir jede Möglichkeit. Einmal begonnen würde das friedliche Stoppen eines Rennens zwischen Unternehmen nationale Regierungsintervention (für Unternehmen) oder beispiellose internationale Koordination (für Länder) erfordern. Aber wenn irgendeine Schließung oder bedeutende Vorsicht vorgeschlagen wird, würde es sofort Rufe geben: „aber wenn wir gestoppt werden, werden *die* vorpreschen", wobei „die" nun China (für die USA), oder die USA (für China), oder China *und* die USA (für Europa oder Indien) ist. Unter dieser Denkweise[^18] kann kein Teilnehmer einseitig stoppen: Solange einer sich zum Rennen verpflichtet, fühlen die anderen, dass sie sich nicht leisten können zu stoppen.

Die zweite Möglichkeit hat eine Seite „gewinnen". Aber was bedeutet das? Einfach (irgendwie gehorsame) AGI zuerst zu erhalten ist nicht genug. Der Gewinner muss *auch* die anderen davon abhalten, das Rennen fortzusetzen – sonst werden sie sie auch erhalten. Das ist prinzipiell möglich: Wer AGI zuerst entwickelt, *könnte* unaufhaltsame Macht über alle anderen Akteure erlangen. Aber was würde das Erreichen eines solchen „entscheidenden strategischen Vorteils" tatsächlich erfordern? Vielleicht wären es spielverändernde militärische Fähigkeiten?[^19] Oder Cyberangriffskräfte?[^20] Vielleicht wäre die AGI einfach so erstaunlich überzeugend, dass sie die anderen Parteien einfach davon überzeugt, aufzuhören?[^21] So reich, dass sie die anderen Unternehmen oder sogar Länder kauft?[^22]

Wie *genau* baut eine Seite eine KI, die mächtig genug ist, um andere daran zu hindern, vergleichbar mächtige KI zu bauen? Aber das ist die einfache Frage.

Denn betrachten Sie nun, wie diese Situation anderen Mächten erscheint. Was denkt die chinesische Regierung, wenn die USA eine solche Fähigkeit zu erlangen scheinen? Oder umgekehrt? Was denkt die US-Regierung (oder chinesische, oder russische, oder indische), wenn OpenAI oder DeepMind oder Anthropic einem Durchbruch nahe zu sein scheint? Was passiert, wenn die USA eine neue indische oder VAE-Anstrengung mit Durchbruchserfolg sehen? Sie würden sowohl eine existenzielle Bedrohung als auch – entscheidend – sehen, dass der einzige Weg, wie dieses „Rennen" endet, durch ihre eigene Entmachtung ist. Diese sehr mächtigen Akteure – einschließlich Regierungen voll ausgerüsteter Nationen, die sicherlich die Mittel dazu haben – wären hochmotiviert, eine solche Fähigkeit entweder zu erlangen oder zu zerstören, sei es durch Gewalt oder Subversion.[^23]

Das könnte kleinflächig beginnen, als Sabotage von Trainingsläufen oder Angriffe auf Chip-Herstellung, aber diese Angriffe können nur wirklich stoppen, sobald alle Parteien entweder die Fähigkeit verlieren, im KI-Rennen zu laufen, oder die Fähigkeit verlieren, die Angriffe zu machen. Da die Teilnehmer die Einsätze als existenziell betrachten, würde jeder Fall wahrscheinlich einen katastrophalen Krieg darstellen.

Das bringt uns zur vierten Möglichkeit: das Rennen zur Superintelligenz, und zwar auf die schnellste, am wenigsten kontrollierte Weise möglich. Während KI an Macht zunimmt, werden ihre Entwickler auf beiden Seiten es fortschreitend schwerer finden, sie zu kontrollieren, besonders weil das Rennen um Fähigkeiten der Art von sorgfältiger Arbeit entgegensteht, die Kontrollierbarkeit erfordern würde. Also bringt uns dieses Szenario direkt in den Fall, wo die Kontrolle an die KI-Systeme selbst verloren (oder gegeben) wird. Das heißt, *KI gewinnt das Rennen.* Aber andererseits, in dem Maß, wie Kontrolle *aufrechterhalten* wird, haben wir weiterhin mehrere sich gegenseitig feindlich gesinnte Parteien, die jeweils extrem mächtige Fähigkeiten befehligen. Das sieht wieder wie Krieg aus.

Sagen wir das alles anders.[^24] Die gegenwärtige Welt hat einfach keine Institutionen, denen die Entwicklung einer KI dieser Fähigkeit anvertraut werden könnte, ohne sofortigen Angriff einzuladen.[^25] Alle Parteien werden korrekt folgern, dass sie entweder *nicht* unter Kontrolle sein wird – und daher eine Bedrohung für alle Parteien ist, oder sie *wird* unter Kontrolle sein, und daher eine Bedrohung für jeden Gegner ist, der sie weniger schnell entwickelt. Das sind nuklear bewaffnete Länder oder Unternehmen, die in ihnen beherbergt sind.

In Ermangelung irgendeines plausiblen Weges für Menschen, dieses Rennen zu „gewinnen", bleiben wir mit einer krassen Schlussfolgerung zurück: Der einzige Weg, wie dieses Rennen endet, ist entweder in katastrophalem Konflikt oder wo KI, und nicht irgendeine menschliche Gruppe, der Gewinner ist.

## Wir geben die Kontrolle an KI ab (oder sie nimmt sie)

Geopolitischer „Großmächte"-Wettbewerb ist nur einer von vielen Wettbewerben: Individuen konkurrieren wirtschaftlich und sozial; Unternehmen konkurrieren in Märkten; politische Parteien konkurrieren um Macht; Bewegungen konkurrieren um Einfluss. In jeder Arena wird Wettbewerbsdruck, während KI sich menschlichen Fähigkeiten nähert und sie übertrifft, die Teilnehmer zwingen, mehr und mehr Kontrolle an KI-Systeme zu delegieren oder abzugeben – nicht weil diese Teilnehmer es wollen, sondern weil sie [es sich nicht leisten können, es nicht zu tun.](https://arxiv.org/abs/2303.16200)

Wie bei anderen Risiken von AGI sehen wir das bereits bei schwächeren Systemen. Studenten fühlen sich gedrängt, KI in ihren Aufgaben zu verwenden, weil klar viele andere Studenten es tun. Unternehmen [eilen, KI-Lösungen aus Wettbewerbsgründen zu übernehmen.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) Künstler und Programmierer fühlen sich gezwungen, KI zu verwenden, oder sonst werden ihre Preise von anderen unterboten, die es tun.

Das fühlt sich wie gedrängte Delegation an, aber nicht wie Kontrollverlust. Aber lassen Sie uns die Einsätze erhöhen und die Uhr vordrehen. Betrachten Sie einen CEO, dessen Konkurrenten AGI-„Gehilfen" verwenden, um schnellere, bessere Entscheidungen zu treffen, oder einen militärischen Kommandeur, der einem Gegner mit KI-verstärktem Kommando und Kontrolle gegenübersteht. Ein ausreichend fortgeschrittenes KI-System könnte autonom mit vielfacher menschlicher Geschwindigkeit, Raffinesse, Komplexität und Datenverarbeitungskapazität operieren und komplexe Ziele auf komplizierte Weise verfolgen. Unser CEO oder Kommandeur, der ein solches System befehligt, mag sehen, wie es erreicht, was er will; aber würde er auch nur einen kleinen Teil davon verstehen, *wie* es erreicht wurde? Nein, er müsste es einfach akzeptieren. Darüber hinaus ist vieles von dem, was das System tun kann, nicht nur Befehle entgegenzunehmen, sondern seinen vermeintlichen Chef zu beraten, was zu tun ist. Diese Beratung wird gut sein –– immer und immer wieder.

An welchem Punkt wird dann die Rolle des Menschen darauf reduziert, „ja, mach weiter" zu klicken?

Es fühlt sich gut an, fähige KI-Systeme zu haben, die unsere Produktivität verbessern, lästige Schufterei übernehmen und sogar als Denkpartner beim Erledigen von Dingen fungieren können. Es wird sich gut anfühlen, einen KI-Assistenten zu haben, der Handlungen für uns übernehmen kann, wie ein guter menschlicher persönlicher Assistent. Es wird sich natürlich, sogar vorteilhaft anfühlen, während KI sehr klug, kompetent und zuverlässig wird, mehr und mehr Entscheidungen an sie zu delegieren. Aber diese „vorteilhafte" Delegation hat einen klaren Endpunkt, wenn wir den Weg fortsetzen: Eines Tages werden wir feststellen, dass wir nicht mehr wirklich für viel zuständig sind, und dass die KI-Systeme, die tatsächlich das Sagen haben, genauso wenig abgeschaltet werden können wie Ölunternehmen, soziale Medien, das Internet oder der Kapitalismus.

Und das ist die viel positivere Version, in der KI einfach so nützlich und effektiv ist, dass wir sie die meisten unserer wichtigen Entscheidungen für uns treffen lassen. Die Realität wäre wahrscheinlich viel mehr eine Mischung zwischen dem und Versionen, wo unkontrollierte AGI-Systeme verschiedene Formen von Macht für sich selbst *nehmen*, denn denken Sie daran, Macht ist nützlich für fast jedes Ziel, das man hat, und AGI wäre von der Konzeption her mindestens so effektiv beim Verfolgen ihrer Ziele wie Menschen.

Ob wir die Kontrolle gewähren oder ob sie uns entrissen wird, ihr Verlust scheint extrem wahrscheinlich. Wie Alan Turing ursprünglich sagte: „...es scheint wahrscheinlich, dass, sobald die Maschinendenkweise begonnen hätte, es nicht lange dauern würde, unsere schwächlichen Kräfte zu übertreffen. Es gäbe keine Frage des Sterbens der Maschinen, und sie könnten miteinander sprechen, um ihren Verstand zu schärfen. Irgendwann müssten wir daher erwarten, dass die Maschinen die Kontrolle übernehmen..."

Beachten Sie bitte, obwohl es offensichtlich genug ist, dass der Kontrollverlust der Menschheit an KI auch den Kontrollverlust der Vereinigten Staaten durch die US-Regierung beinhaltet; er bedeutet den Kontrollverlust Chinas durch die Kommunistische Partei Chinas und den Kontrollverlust Indiens, Frankreichs, Brasiliens, Russlands und jedes anderen Landes durch ihre eigene Regierung. Daher nehmen KI-Unternehmen, auch wenn das nicht ihre Absicht ist, derzeit am möglichen Sturz von Weltregierungen teil, einschließlich ihrer eigenen. Das könnte in wenigen Jahren geschehen.

## AGI wird zu Superintelligenz führen

Es gibt Argumente dafür, dass menschlich-wettbewerbsfähige oder sogar experten-wettbewerbsfähige Allzweck-KI, selbst wenn autonom, handhabbar sein könnte. Sie mag in all den oben diskutierten Weisen unglaublich störend sein, aber es gibt viele sehr kluge, handlungsfähige Menschen in der Welt jetzt, und sie sind mehr oder weniger handhabbar.[^26]

Aber wir werden nicht auf etwa menschlichem Niveau bleiben. Das Voranschreiten darüber hinaus wird wahrscheinlich von den gleichen Kräften angetrieben, die wir bereits gesehen haben: Wettbewerbsdruck zwischen KI-Entwicklern, die Profit und Macht suchen, Wettbewerbsdruck zwischen KI-Nutzern, die es sich nicht leisten können, zurückzufallen, und – am wichtigsten – AGIs eigene Fähigkeit, sich selbst zu verbessern.

In einem Prozess, den wir bereits bei weniger mächtigen Systemen beginnen sehen, wäre AGI selbst imstande, verbesserte Versionen ihrer selbst zu konzipieren und zu entwerfen. Das schließt Hardware, Software, neuronale Netzwerke, Werkzeuge, Gerüstsysteme usw. ein. Sie wird von der Definition her besser darin sein als wir, also wissen wir nicht genau, wie sie sich intelligenz-bootstrappen wird. Aber wir müssen es nicht. Insofern wir noch Einfluss darauf haben, was AGI tut, müssten wir sie nur darum bitten oder sie lassen.

Es gibt keine menschliche Barriere zur Kognition, die uns vor diesem Kontrollverlust schützen könnte.[^27]

Der Übergang von AGI zu Superintelligenz ist kein Naturgesetz; es wäre immer noch möglich, den Kontrollverlust zu stoppen, besonders wenn AGI relativ zentralisiert ist und insoweit sie von Parteien kontrolliert wird, die keinen Druck verspüren, miteinander zu rennen. Aber sollte AGI weit verbreitet und hochautonom sein, scheint es fast unmöglich zu verhindern, dass sie entscheidet, sie sollte mächtiger und dann noch mächtiger sein.

## Was passiert, wenn wir (oder AGI) Superintelligenz entwickeln

Um es deutlich zu sagen: Wir haben keine Ahnung, was passieren würde, wenn wir Superintelligenz entwickeln.[^28] Sie würde Handlungen ergreifen, die wir nicht verfolgen oder wahrnehmen können, aus Gründen, die wir nicht erfassen können, auf Ziele hin, die wir nicht begreifen können. Was wir wissen, ist, dass es nicht an uns liegen wird.[^29]

Die Unmöglichkeit, Superintelligenz zu kontrollieren, kann durch zunehmend krasse Analogien verstanden werden. Stellen Sie sich zunächst vor, Sie sind CEO eines großen Unternehmens. Es gibt keine Möglichkeit, alles zu verfolgen, was vor sich geht, aber mit der richtigen Einrichtung von Personal können Sie immer noch sinnvoll das große Bild verstehen und Entscheidungen treffen. Aber nehmen Sie nur eine Sache an: Alle anderen im Unternehmen operieren mit hundertfacher Ihrer Geschwindigkeit. Können Sie immer noch mithalten?

Bei superintelligenter KI würden Menschen etwas „befehligen", das nicht nur schneller ist, sondern auf Ebenen von Raffinesse und Komplexität operiert, die sie nicht verstehen können, und weitaus mehr Daten verarbeitet, als sie sich auch nur vorstellen können. Diese Inkommensurabilität kann auf eine formale Ebene gestellt werden: [Ashbys Gesetz der erforderlichen Vielfalt](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (und siehe das verwandte [„Gute-Regulator-Theorem"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)) besagt grob, dass jedes Kontrollsystem so viele Knöpfe und Wählscheiben haben muss, wie das kontrollierte System Freiheitsgrade hat.

Eine Person, die ein superintelligentes KI-System kontrolliert, wäre wie ein Farn, der General Motors kontrolliert: Selbst wenn „tue, was der Farn will" in die Unternehmenssatzung geschrieben wäre, sind die Systeme so unterschiedlich in Geschwindigkeit und Handlungsbereich, dass „Kontrolle" einfach nicht zutrifft. (Und wie lange, bis diese lästige Satzung umgeschrieben wird?)[^30]

Da es null Beispiele von Pflanzen gibt, die Fortune-500-Unternehmen kontrollieren, gäbe es genau null Beispiele von Menschen, die Superintelligenzen kontrollieren. Das nähert sich einer mathematischen Tatsache.[^31] Wenn Superintelligenz konstruiert würde – unabhängig davon, wie wir dorthin gelangten – wäre die Frage nicht, ob Menschen sie kontrollieren könnten, sondern ob wir weiter existieren würden, und wenn ja, ob wir als Individuen oder als Spezies eine gute und bedeutungsvolle Existenz hätten. Über diese existenziellen Fragen für die Menschheit hätten wir wenig Einfluss. Die menschliche Ära wäre vorbei.

## Schlussfolgerung: Wir dürfen AGI nicht entwickeln

Es gibt ein Szenario, in dem der Aufbau von AGI für die Menschheit gut verlaufen könnte: Sie wird sorgfältig entwickelt, unter Kontrolle und zum Nutzen der Menschheit, regiert durch gegenseitige Vereinbarung vieler Akteure,[^32] und daran gehindert, zu unkontrollierbarer Superintelligenz zu evolvieren.

*Dieses Szenario steht uns unter gegenwärtigen Umständen nicht offen.* Wie in diesem Abschnitt diskutiert, würde die Entwicklung von AGI mit sehr hoher Wahrscheinlichkeit zu einer Kombination aus:

- Massiver gesellschaftlicher und zivilisatorischer Störung oder Zerstörung;
- Konflikt oder Krieg zwischen Großmächten;
- Kontrollverlust der Menschheit *über* oder *an* mächtige KI-Systeme;
- Kontrollverlust zu unkontrollierbarer Superintelligenz und der Irrelevanz oder dem Aufhören der menschlichen Spezies.

Wie eine frühe fiktionale Darstellung von AGI es ausdrückte: Der einzige Weg zu gewinnen ist nicht zu spielen.


[^1]: Das [EU-KI-Gesetz](https://artificialintelligenceact.eu/) ist ein bedeutendes Gesetz, würde aber nicht direkt verhindern, dass ein gefährliches KI-System entwickelt oder eingesetzt oder sogar offen freigegeben wird, besonders in den USA. Ein anderes bedeutendes Politikstück, die US-Exekutivverordnung zu KI, wurde widerrufen.

[^2]: Diese [Gallup-Umfrage](https://news.gallup.com/poll/1597/confidence-institutions.aspx) zeigt einen düsteren Rückgang des Vertrauens in öffentliche Institutionen seit 2000 in den USA. Europäische Zahlen sind vielfältiger und weniger extrem, aber auch auf einem Abwärtstrend. Misstrauen bedeutet nicht unbedingt, dass Institutionen wirklich *sind* dysfunktional, aber es ist eine Anzeige sowie eine Ursache.

[^3]: Und große Störungen, die wir nun befürworten – wie die Ausweitung von Rechten auf neue Gruppen – wurden spezifisch von Menschen in eine Richtung vorangetrieben, die Dinge besser machen sollte.

[^4]: Lassen Sie mich deutlich sein. Wenn Ihr Job von hinter einem Computer gemacht werden kann, mit relativ wenig persönlicher Interaktion mit Menschen außerhalb Ihrer Organisation, und keine rechtliche Verantwortung gegenüber externen Parteien beinhaltet, wäre es von der Definition her möglich (und wahrscheinlich kostensparend), Sie vollständig gegen ein digitales System auszutauschen. Robotik, um viel körperliche Arbeit zu ersetzen, wird später kommen – aber nicht so viel später, sobald AGI anfängt, Roboter zu entwerfen.

[^5]: Zum Beispiel, was passiert mit unserem Justizsystem, wenn Klagen fast kostenlos eingereicht werden können? Was passiert, wenn das Umgehen von Sicherheitssystemen durch Social Engineering billig, einfach und risikofrei wird?

[^6]: [Dieser Artikel](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/) behauptet, dass 10% aller Internetinhalte bereits KI-generiert sind, und ist Googles Top-Treffer (für mich) zur Suchanfrage „Schätzungen, welcher Anteil neuer Internetinhalte KI-generiert ist." Ist es wahr? Ich habe keine Ahnung! Es zitiert keine Referenzen und es wurde nicht von einer Person geschrieben. Welcher Anteil neuer Bilder, die von Google indexiert werden, oder Tweets, oder Kommentare auf Reddit, oder Youtube-Videos werden von Menschen generiert? Niemand weiß es – ich glaube nicht, dass es eine erkennbare Zahl ist. Und das weniger als *zwei Jahre* nach dem Aufkommen generativer KI.

[^7]: Auch erwähnenswert ist, dass es „moralisches" Risiko gibt, dass wir digitale Wesen schaffen könnten, die leiden können. Da wir derzeit keine zuverlässige Theorie des Bewusstseins haben, die es uns erlauben würde, physische Systeme zu unterscheiden, die leiden können und nicht können, können wir das theoretisch nicht ausschließen. Darüber hinaus sind KI-Systeme Berichte über ihre Empfindungsfähigkeit wahrscheinlich unzuverlässig bezüglich ihrer tatsächlichen Erfahrung (oder Nicht-Erfahrung) von Empfindungsfähigkeit.

[^8]: Technische Lösungen in diesem Feld des KI-„Alignments" werden wahrscheinlich auch nicht der Aufgabe gewachsen sein. In gegenwärtigen Systemen funktionieren sie auf einem gewissen Niveau, aber sie sind oberflächlich und können allgemein ohne bedeutende Anstrengung umgangen werden; und wie unten diskutiert haben wir keine wirkliche Ahnung, wie wir das für viel fortgeschrittenere Systeme machen.

[^9]: Solche KI-Systeme können mit einigen eingebauten Schutzmaßnahmen kommen. Aber für jedes Modell mit etwas wie der aktuellen Architektur, wenn voller Zugang zu seinen Gewichten verfügbar ist, können Sicherheitsmaßnahmen über zusätzliches Training oder andere Techniken entfernt werden. Also ist es praktisch garantiert, dass es für jedes System mit Leitplanken auch ein weit verfügbares System ohne sie geben wird. Tatsächlich wurde Metas Llama 3.1 405B Modell offen mit Schutzmaßnahmen freigegeben. Aber *sogar davor* wurde ein „Basis"-Modell ohne Schutzmaßnahmen geleakt.

[^10]: Könnten die Märkte diese Risiken ohne Regierungsbeteiligung verwalten? Kurz gesagt, nein. Es gibt sicherlich Risiken, die Unternehmen stark motiviert sind zu mildern. Aber viele andere können und externalisieren Unternehmen auf alle anderen, und viele der oben genannten sind in dieser Klasse: Es gibt keine natürlichen Marktanreize, um Massenüberwachung, Wahrheitserosion, Machtkonzentration, Arbeitsplatzstörungen, schädigenden politischen Diskurs usw. zu verhindern. Tatsächlich haben wir all das von heutiger Technologie gesehen, besonders sozialen Medien, die im Wesentlichen unreguliert geblieben sind. KI würde nur viele der gleichen Dynamiken massiv verstärken.

[^11]: OpenAI hat wahrscheinlich gehorsamere Modelle für internen Gebrauch. Es ist unwahrscheinlich, dass OpenAI eine Art „Hintertür" gebaut hat, damit ChatGPT von OpenAI selbst besser kontrolliert werden kann, weil das eine schreckliche Sicherheitspraxis wäre und hochgradig ausnutzbar angesichts KIs Undurchsichtigkeit und Unvorhersagbarkeit.

[^12]: Auch von entscheidender Wichtigkeit: Alignment oder andere Sicherheitsmerkmale sind nur wichtig, wenn sie tatsächlich in einem KI-System verwendet werden. Systeme, die offen freigegeben werden (d.h. wo Modellgewichte und Architektur öffentlich verfügbar sind), können relativ einfach in Systeme *ohne* diese Sicherheitsmaßnahmen transformiert werden. Offen klüger-als-menschliche AGI-Systeme freizugeben wäre erstaunlich rücksichtslos, und es ist schwer vorstellbar, wie menschliche Kontrolle oder sogar Relevanz in einem solchen Szenario aufrechterhalten würde. Es gäbe jede Motivation, zum Beispiel mächtige sich selbst reproduzierende und selbst erhaltende KI-Agenten mit dem Ziel loszulassen, Geld zu verdienen und es an eine Kryptowährungs-Wallet zu senden. Oder eine Wahl zu gewinnen. Oder eine Regierung zu stürzen. Könnte „gute" KI helfen, das einzudämmen? Vielleicht – aber nur indem ihr riesige Autorität delegiert wird, was zu Kontrollverlust wie unten beschrieben führt.

[^13]: Für buchfüllende Darlegungen des Problems siehe z.B. *Superintelligence*, *The Alignment Problem*, und *Human-Compatible*. Für einen riesigen Haufen Arbeit auf verschiedenen technischen Niveaus von denen, die jahrelang über das Problem nachgedacht haben, können Sie das [AI Alignment Forum](https://www.alignmentforum.org/) besuchen. Hier ist eine [aktuelle Einschätzung](https://alignment.anthropic.com/2025/recommended-directions/) von Anthropics Alignment-Team darüber, was sie als ungelöst betrachten.

[^14]: Das ist das [„Schurken-KI"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/)-Szenario. Prinzipiell könnte das Risiko relativ gering sein, wenn das System noch kontrolliert werden kann, indem es abgeschaltet wird; aber das Szenario könnte auch KI-Täuschung, Selbst-Exfiltration und Reproduktion, Machtanhäufung und andere Schritte einschließen, die es schwierig oder unmöglich machen würden, das zu tun.

[^15]: Es gibt eine sehr reiche Literatur zu diesem Thema, die zurückgeht zu formativen Schriften von [Steve Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), Nick Bostrom und Eliezer Yudkowsky. Für eine buchfüllende Darlegung siehe [Human Compatible](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) von Stuart Russell; [hier](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/) ist eine kurze und aktuelle Einführung.

[^16]: In Erkennung dessen haben AGI-Unternehmen, anstatt zu verlangsamen, um besseres Verständnis zu bekommen, einen anderen Plan entwickelt: Sie werden KI dazu bringen, es zu tun! Spezifischer werden sie KI *N* dabei helfen lassen herauszufinden, wie KI *N+1* zu alignen ist, den ganzen Weg zur Superintelligenz. Obwohl es vielversprechend klingt, KI zu nutzen, um uns beim Alignment von KI zu helfen, gibt es ein starkes Argument, dass es einfach seine Schlussfolgerung als Prämisse annimmt und im Allgemeinen ein unglaublich riskanter Ansatz ist. Siehe [hier](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us) für etwas Diskussion. Dieser „Plan" ist keiner und hat nichts wie die Prüfung durchgemacht, die der Kernstrategie angemessen wäre, wie übermenschliche KI für die Menschheit gut werden soll.

[^17]: Schließlich haben Menschen, fehlerhaft und eigenwillig wie wir sind, ethische Systeme entwickelt, durch die wir zumindest einige andere Spezies auf der Erde gut behandeln. (Denken Sie nur nicht an diese Massentierhaltungen.)

[^18]: Es gibt glücklicherweise hier einen Ausweg: wenn die Teilnehmer zu verstehen kommen, dass sie in einem Selbstmordrennen statt einem gewinnbaren engagiert sind. Das ist passiert gegen Ende des Kalten Krieges, als die USA und UdSSR zu erkennen kamen, dass aufgrund des nuklearen Winters sogar ein *unbeantworteter* Nuklearangriff katastrophal für den Angreifer wäre. Mit der Erkenntnis, dass „Nuklearkrieg nicht gewonnen werden kann und nie geführt werden darf" kamen bedeutende Abrüstungsvereinbarungen – im Wesentlichen ein Ende des Wettrüstens.

[^19]: Krieg, explizit oder implizit.

[^20]: Eskalation, dann Krieg.

[^21]: Magisches Denken.

[^22]: Ich habe auch eine Billiarden-Dollar-Brücke für Sie zu verkaufen.

[^23]: Solche Akteure würden vermutlich „Erlangung" bevorzugen, mit Zerstörung als Rückfallposition; aber Modelle gegen sowohl Zerstörung *als auch* Diebstahl durch mächtige Nationen zu sichern ist gelinde gesagt schwierig, besonders für private Entitäten.

[^24]: Für eine andere Perspektive auf die nationalen Sicherheitsrisiken von AGI siehe [diesen RAND-Bericht.](https://www.rand.org/pubs/perspectives/PEA3691-4.html)

[^25]: Vielleicht könnten wir eine solche Institution bauen! Es hat Vorschläge für ein „CERN für KI" und andere ähnliche Initiativen gegeben, wo AGI-Entwicklung unter multilateraler globaler Kontrolle ist. Aber im Moment existiert keine solche Institution oder ist am Horizont.

[^26]: Und während Alignment sehr schwierig ist, Menschen zum Benehmen zu bringen ist sogar härter!

[^27]: Stellen Sie sich ein System vor, das 50 Sprachen sprechen kann, Expertise in allen akademischen Fächern hat, ein ganzes Buch in Sekunden lesen und all das Material sofort im Kopf haben kann, und Ausgaben mit zehnfacher menschlicher Geschwindigkeit produziert. Eigentlich müssen Sie es sich nicht vorstellen: Laden Sie einfach ein aktuelles KI-System. Diese sind in vielen Weisen übermenschlich, und es gibt nichts, was sie davon abhält, in diesen und vielen anderen noch übermenschlicher zu sein.

[^28]: Deshalb wurde das eine technologische „Singularität" genannt, in Anlehnung an die Physik die Idee, dass man keine Vorhersagen jenseits einer Singularität machen kann. Befürworter, die sich *in* eine solche Singularität *hinein*lehnen, mögen auch reflektieren, dass in der Physik diese gleiche Art von Singularitäten die zerreißen und zerquetschen, die in sie hineingehen.

[^29]: Das Problem wurde umfassend in Bostroms [*Superintelligence*](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834) umrissen, und nichts seitdem hat die Kernbotschaft bedeutend verändert. Für einen neueren Band, der formale und mathematische Ergebnisse über Unkontrollierbarkeit sammelt, siehe Yampolskiys [AI: Unexplainable, Unpredictable, Uncontrollable](https://www.amazon.com/Unexplainable-Unpredictable-Uncontrollable-Artificial-Intelligence/dp/103257626X)

[^30]: Das macht auch klar, warum die aktuelle Strategie von KI-Unternehmen (iterativ KI die nächste mächtigste KI „alignen" zu lassen) nicht funktionieren kann. Nehmen Sie an, ein Farn wirbt über die Annehmlichkeit seiner Wedel einen Erstklässler an, sich um ihn zu kümmern. Der Erstklässler schreibt detaillierte Anweisungen für einen Zweitklässler und einen Zettel, der ihn überzeugt, es zu tun. Der Zweitklässler macht das Gleiche für einen Drittklässler, und so weiter bis zu einem Hochschulabsolventen, einem Manager, einer Führungskraft und schließlich dem GM CEO. Wird GM dann „tun, was der Farn will"? Bei jedem Schritt könnte sich das anfühlen, als würde es funktionieren. Aber alles zusammengenommen wird es fast genau zu dem Grad funktionieren, zu dem der CEO, Vorstand und Aktionäre von GM zufällig sich um Kinder und Farne kümmern, und wenig bis nichts mit all diesen Zetteln und Anweisungssets zu tun haben.

[^31]: Der Charakter ist nicht so anders als formale Ergebnisse wie Gödels Unvollständigkeitstheorem oder Turings Halte-Argument darin, dass die Vorstellung von Kontrolle fundamental der Prämisse widerspricht: Wie können Sie sinnvoll etwas kontrollieren, das Sie nicht verstehen oder vorhersagen können; doch wenn Sie Superintelligenz verstehen und vorhersagen könnten, wären Sie superintelligent. Der Grund, warum ich „nähert sich" sage, ist, dass die formalen Ergebnisse nicht so gründlich oder geprüft sind wie im reinen Mathematikfall, und weil ich Hoffnung halten möchte, dass eine sehr sorgfältig konstruierte allgemeine Intelligenz, die völlig andere Methoden als die derzeit verwendeten nutzt, einige mathematisch beweisbare Sicherheitseigenschaften haben könnte, gemäß der Art von „garantiert sicherer" KI-Programm, das unten diskutiert wird.

[^32]: Im Moment sind die meisten Akteure – das heißt, fast die gesamte Menschheit – in dieser Diskussion an den Rand gedrängt. Das ist zutiefst falsch, und wenn sie nicht eingeladen werden, sollten die vielen, vielen anderen Gruppen, die von AGI-Entwicklung betroffen sein werden, fordern, eingelassen zu werden.