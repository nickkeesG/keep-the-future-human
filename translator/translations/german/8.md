# Kapitel 8 - Wie man keine AGI baut

AGI ist nicht unvermeidlich – heute stehen wir an einer Wegkreuzung. Dieses Kapitel präsentiert einen Vorschlag dafür, wie wir verhindern könnten, dass sie gebaut wird.

Wenn der Weg, auf dem wir uns derzeit befinden, wahrscheinlich zum Ende unserer Zivilisation führt, wie wechseln wir dann den Weg?

Angenommen, der Wunsch, die Entwicklung von AGI und Superintelligenz zu stoppen, wäre weit verbreitet und einflussreich,[^1] weil es zum Allgemeinverständnis wird, dass AGI eher macht-absorbierend als macht-gewährend wäre und eine tiefgreifende Gefahr für Gesellschaft und Menschheit darstellt. Wie würden wir die Tore schließen?

Derzeit kennen wir nur einen Weg, mächtige und allgemeine KI zu *erschaffen*, nämlich über wirklich massive Berechnungen tiefer neuronaler Netzwerke. Da diese unglaublich schwierig und teuer durchzuführen sind, ist es gewissermaßen einfach, sie *nicht* zu tun.[^2] Aber wir haben bereits die Kräfte gesehen, die in Richtung AGI treiben, und die spieltheoretischen Dynamiken, die es für jeden Akteur sehr schwierig machen, einseitig zu stoppen. Es würde also eine Kombination aus Eingriffen von außen (d.h. Regierungen) brauchen, um Unternehmen zu stoppen, und Vereinbarungen zwischen Regierungen, um sich selbst zu stoppen.[^3] Wie könnte das aussehen?

Es ist zunächst nützlich, zwischen KI-Entwicklungen zu unterscheiden, die *verhindert* oder *verboten* werden müssen, und solchen, die *verwaltet* werden müssen. Erstere wären vor allem der Kontrollverlust zur Superintelligenz.[^4] Für verbotene Entwicklung sollten Definitionen so präzise wie möglich sein, und sowohl Verifikation als auch Durchsetzung sollten praktikabel sein. Was *verwaltet* werden muss, wären allgemeine, mächtige KI-Systeme – die wir bereits haben und die viele Grauzonen, Nuancen und Komplexität aufweisen werden. Für diese sind starke, effektive Institutionen entscheidend.

Wir können auch nützlicherweise zwischen Problemen unterscheiden, die auf internationaler Ebene (einschließlich zwischen geopolitischen Rivalen oder Gegnern) angegangen werden müssen[^5] und solchen, die einzelne Rechtsprechungen, Länder oder Ländergruppen bewältigen können. Verbotene Entwicklung fällt größtenteils in die „internationale" Kategorie, weil ein lokales Verbot der Entwicklung einer Technologie in der Regel durch einen Ortswechsel umgangen werden kann.[^6]

Schließlich können wir die Werkzeuge im Werkzeugkasten betrachten. Es gibt viele, darunter technische Werkzeuge, weiches Recht (Standards, Normen, etc.), hartes Recht (Vorschriften und Anforderungen), Haftung, Marktanreize und so weiter. Richten wir besondere Aufmerksamkeit auf eines, das spezifisch für KI ist.

## Rechenleistungssicherheit und -governance

Ein zentrales Werkzeug bei der Governance hochleistungsfähiger KI wird die Hardware sein, die sie benötigt. Software proliferiert leicht, hat nahezu null Grenzkosten der Produktion, überschreitet Grenzen trivial und kann sofort modifiziert werden; nichts davon trifft auf Hardware zu. Doch wie wir besprochen haben, sind riesige Mengen dieser „Rechenleistung" sowohl während des Trainings von KI-Systemen als auch während der Inferenz notwendig, um die leistungsfähigsten Systeme zu erreichen. Rechenleistung kann leicht quantifiziert, bilanziert und auditiert werden, mit relativ wenig Mehrdeutigkeit, sobald gute Regeln dafür entwickelt sind. Am wichtigsten ist, dass große Mengen an Rechenleistung, wie angereichertes Uran, eine sehr knappe, teure und schwer zu produzierende Ressource sind. Obwohl Computerchips allgegenwärtig sind, ist die für KI benötigte Hardware teuer und außerordentlich schwierig herzustellen.[^7]

Was KI-spezialisierte Chips *weitaus* handhabbarer als knappe Ressource im Vergleich zu Uran macht, ist, dass sie hardware-basierte Sicherheitsmechanismen beinhalten können. Die meisten modernen Handys und einige Laptops haben spezialisierte Hardware-Features auf dem Chip, die es ihnen ermöglichen sicherzustellen, dass sie nur genehmigte Betriebssystem-Software und Updates installieren, dass sie sensible biometrische Daten geräteintern behalten und schützen, und dass sie für jeden anderen als ihren Besitzer nutzlos gemacht werden können, wenn sie verloren oder gestohlen werden. In den letzten Jahren sind solche Hardware-Sicherheitsmaßnahmen gut etabliert und weit verbreitet geworden und haben sich im Allgemeinen als recht sicher erwiesen.

Das Schlüsselelement dieser Features ist, dass sie Hardware und Software mittels Kryptografie miteinander verbinden.[^8] Das heißt, nur weil man ein bestimmtes Stück Computerhardware besitzt, bedeutet das nicht, dass ein Nutzer durch Anwendung unterschiedlicher Software alles damit machen kann, was er will. Und diese Verbindung bietet auch mächtige Sicherheit, weil viele Angriffe einen Bruch der *Hardware*-Sicherheit erfordern würden, nicht nur der *Software*-Sicherheit.

Mehrere kürzliche Berichte (z.B. von [GovAI und Kooperationspartnern](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips) und [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) haben darauf hingewiesen, dass ähnliche Hardware-Features, die in modernste KI-relevante Computer-Hardware eingebettet sind, eine extrem nützliche Rolle in KI-Sicherheit und -Governance spielen könnten. Sie ermöglichen eine Reihe von Funktionen, die einem „Gouverneur"[^9] zur Verfügung stehen, die man nicht vermuten würde, dass sie verfügbar oder sogar möglich wären. Als einige wichtige Beispiele:

- *Geolokalisierung*: Systeme können so eingerichtet werden, dass Chips einen bekannten Standort haben und je nach Standort unterschiedlich agieren (oder ganz abgeschaltet werden) können.[^10]
- *Zugelassene Verbindungen*: Jeder Chip kann mit einer hardware-durchgesetzten Zulassungsliste bestimmter anderer Chips konfiguriert werden, mit denen er sich vernetzen kann, und unfähig sein, sich mit Chips zu verbinden, die nicht auf dieser Liste stehen.[^11] Dies kann die Größe kommunikativer Cluster von Chips begrenzen.[^12]
- *Dosierte Inferenz oder Training (und automatische Abschaltung)*: Ein Gouverneur kann nur eine bestimmte Menge Training oder Inferenz (in Zeit, oder FLOP, oder möglicherweise Token) lizenzieren, die von einem Nutzer durchgeführt werden darf, danach ist neue Erlaubnis erforderlich. Wenn die Schritte klein sind, dann ist relativ kontinuierliche Neulizenzierung eines Modells erforderlich. Das Modell kann dann einfach „ausgeschaltet" werden, indem man dieses Lizenzsignal verweigert.[^13]
- *Geschwindigkeitsbegrenzung*: Ein Modell wird daran gehindert, mit höherer Inferenz-Geschwindigkeit zu laufen als ein Limit, das von einem Gouverneur oder anderweitig bestimmt wird. Dies könnte über eine begrenzte Anzahl zugelassener Verbindungen oder durch raffiniertere Mittel implementiert werden.
- *Beglaubigtes Training*: Ein Trainingsprozess kann kryptografisch sicheren Beweis liefern, dass ein bestimmter Satz von Codes, Daten und Menge der Rechenleistungsnutzung bei der Generierung des Modells verwendet wurden.

## Wie man keine Superintelligenz baut: globale Limits für Training und Inferenz-Rechenleistung

Mit diesen Überlegungen – insbesondere bezüglich der Rechenleistung – im Hinterkopf können wir besprechen, wie die Tore zur künstlichen Superintelligenz geschlossen werden können; wir wenden uns dann der Verhinderung vollständiger AGI zu und der Verwaltung von KI-Modellen, während sie sich menschlichen Fähigkeiten in verschiedenen Aspekten nähern und sie übertreffen.

Die erste Zutat ist natürlich das Verständnis, dass Superintelligenz nicht kontrollierbar wäre und dass ihre Konsequenzen fundamental unvorhersagbar sind. Zumindest China und die USA müssen unabhängig voneinander entscheiden, aus diesem oder anderen Zwecken keine Superintelligenz zu bauen.[^14] Dann ist eine internationale Vereinbarung zwischen ihnen und anderen nötig, mit einem starken Verifikations- und Durchsetzungsmechanismus, um alle Parteien zu versichern, dass ihre Rivalen nicht abtrünnig werden und beschließen zu würfeln.

Um verifizierbar und durchsetzbar zu sein, sollten die Limits harte Grenzen sein und so eindeutig wie möglich. Das scheint ein praktisch unmögliches Problem zu sein: die Fähigkeiten komplexer Software mit unvorhersagbaren Eigenschaften weltweit zu begrenzen. Glücklicherweise ist die Situation viel besser als das, weil genau das, was fortgeschrittene KI möglich gemacht hat – eine riesige Menge an Rechenleistung – viel, viel einfacher zu kontrollieren ist. Obwohl es immer noch einige mächtige und gefährliche Systeme erlauben könnte, kann ein *Kontrollverlust zur Superintelligenz* wahrscheinlich durch eine harte Obergrenze für die Menge an Rechenleistung verhindert werden, die in ein neuronales Netzwerk fließt, zusammen mit einem Ratenlimit für die Menge an Inferenz, die ein KI-System (aus verbundenen neuronalen Netzwerken und anderer Software) durchführen kann. Eine spezifische Version davon wird unten vorgeschlagen.

Es mag scheinen, als würde das Setzen harter globaler Limits für KI-Rechenleistung riesige Mengen internationaler Koordination und aufdringlicher, privatsphäre-zerstörender Überwachung erfordern. Glücklicherweise wäre das nicht der Fall. Die extrem [enge und als Flaschenhals wirkende Lieferkette](https://arxiv.org/abs/2402.08797) sorgt dafür, dass, sobald ein Limit rechtlich gesetzt ist (sei es durch Gesetz oder Exekutiverlass), die Verifikation der Einhaltung dieses Limits nur die Beteiligung und Kooperation einer Handvoll großer Unternehmen erfordern würde.[^15]

Ein solcher Plan hat eine Reihe höchst wünschenswerter Eigenschaften. Er ist minimal invasiv in dem Sinne, dass nur wenige große Unternehmen Anforderungen auferlegt bekommen, und nur ziemlich bedeutende Rechenleistungscluster würden regiert. Die relevanten Chips enthalten bereits die Hardware-Fähigkeiten, die für eine erste Version benötigt werden.[^16] Sowohl Implementierung als auch Durchsetzung beruhen auf standardmäßigen rechtlichen Beschränkungen. Aber diese werden durch Nutzungsbedingungen der Hardware und durch Hardware-Kontrollen unterstützt, was die Durchsetzung drastisch vereinfacht und Betrug durch Unternehmen, private Gruppen oder sogar Länder verhindert. Es gibt reichlich Präzedenzfälle dafür, dass Hardware-Unternehmen remote Beschränkungen für ihre Hardware-Nutzung setzen und bestimmte Fähigkeiten extern sperren/entsperren,[^17] auch bei hochleistungsfähigen CPUs in Rechenzentren.[^18] Selbst für den ziemlich kleinen Anteil an Hardware und Organisationen, die betroffen sind, könnte die Aufsicht auf Telemetrie beschränkt werden, ohne direkten Zugang zu Daten oder Modellen selbst; und die Software dafür könnte zur Inspektion offen sein, um zu zeigen, dass keine zusätzlichen Daten aufgezeichnet werden. Das Schema ist international und kooperativ, und recht flexibel und erweiterbar. Da das Limit hauptsächlich auf Hardware und nicht auf Software liegt, ist es relativ agnostisch bezüglich dessen, wie KI-Software-Entwicklung und -Deployment stattfindet, und ist kompatibel mit einer Vielfalt von Paradigmen einschließlich „dezentralisierter" oder „öffentlicher" KI, die darauf abzielt, KI-getriebene Machtkonzentration zu bekämpfen.

Ein rechenleistungsbasierter Torschluss hat aber auch Nachteile. Erstens ist er weit von einer vollständigen Lösung für das Problem der KI-Governance im Allgemeinen entfernt. Zweitens würde das System, wenn Computer-Hardware schneller wird, „mehr und mehr Hardware in kleineren und kleineren Clustern (oder sogar einzelnen GPUs) erfassen".[^19] Es ist auch möglich, dass aufgrund algorithmischer Verbesserungen ein noch niedrigeres Rechenleistungslimit rechtzeitig notwendig würde,[^20] oder dass die Menge der Rechenleistung weitgehend irrelevant wird und das Schließen des Tores stattdessen ein detaillierteres risiko-basiertes oder fähigkeits-basiertes Governance-Regime für KI erforderlich machen würde. Drittens wird ein solches System ungeachtet der Garantien und der kleinen Anzahl betroffener Einheiten zwangsläufig Widerstand bezüglich Privatsphäre und Überwachung schaffen, unter anderem.[^21]

Natürlich wird die Entwicklung und Implementierung eines rechenleistungs-limitierenden Governance-Systems in kurzer Zeit recht herausfordernd sein. Aber es ist absolut machbar.

## A-G-I: Die Dreifachschnittstelle als Basis für Risiko und Politik

Wenden wir uns nun der AGI zu. Harte Linien und Definitionen sind hier schwieriger, weil wir sicherlich Intelligenz haben, die künstlich und allgemein ist, und nach keiner bestehenden Definition wird jeder einverstanden sein, ob oder wann sie existiert. Außerdem ist ein Rechenleistungs- oder Inferenz-Limit ein etwas stumpfes Werkzeug (Rechenleistung ist ein Stellvertreter für Fähigkeit, die dann ein Stellvertreter für Risiko ist), das – außer es ist ziemlich niedrig – unwahrscheinlich AGI verhindern wird, die mächtig genug ist, um gesellschaftliche oder zivilisatorische Störungen oder akute Risiken zu verursachen.

Ich habe argumentiert, dass die akutesten Risiken aus der Dreifachschnittstelle sehr hoher Fähigkeit, hoher Autonomie und großer Allgemeinheit entstehen. Das sind die Systeme, die – falls sie überhaupt entwickelt werden – mit enormer Sorgfalt verwaltet werden müssen. Durch das Schaffen strenger Standards (durch Haftung und Regulierung) für Systeme, die alle drei Eigenschaften kombinieren, können wir die KI-Entwicklung in Richtung sichererer Alternativen lenken.

Wie bei anderen Industrien und Produkten, die möglicherweise Verbraucher oder die Öffentlichkeit schädigen könnten, benötigen KI-Systeme sorgfältige Regulierung durch effektive und ermächtigte Regierungsbehörden. Diese Regulierung sollte die inhärenten Risiken von AGI erkennen und inakzeptabel riskante hochleistungsfähige KI-Systeme daran hindern, entwickelt zu werden.[^22]

Jedoch dauert großangelegte Regulierung, besonders mit echten Zähnen, die sicher von der Industrie bekämpft werden,[^23] Zeit[^24] sowie politische Überzeugung, dass sie notwendig ist.[^25] Angesichts des Fortschrittstempos kann das mehr Zeit dauern, als uns zur Verfügung steht.

Auf einem viel schnelleren Zeitrahmen und während regulatorische Maßnahmen entwickelt werden, können wir Unternehmen die notwendigen Anreize geben, (a) von sehr riskanten Aktivitäten abzusehen und (b) umfassende Systeme für die Bewertung und Minderung von Risiken zu entwickeln, indem wir Haftungsniveaus für die gefährlichsten Systeme klären und erhöhen. Die Idee wäre, die allerhöchsten Haftungsniveaus – strenge und in einigen Fällen persönliche kriminelle – für Systeme in der Dreifachschnittstelle hoher Autonomie-Allgemeinheit-Intelligenz aufzuerlegen, aber „sichere Häfen" zu typischerer verschuldensbasierter Haftung für Systeme zu bieten, bei denen eine dieser Eigenschaften fehlt oder garantiert handhabbar ist. Das heißt, zum Beispiel ein „schwaches" System, das allgemein und autonom ist (wie ein fähiger und vertrauenswürdiger aber begrenzter persönlicher Assistent) würde niedrigeren Haftungsniveaus unterliegen. Ebenso würde ein enges und autonomes System wie ein selbstfahrendes Auto immer noch der bedeutenden Regulierung unterliegen, der es bereits unterliegt, aber nicht verstärkter Haftung. Ähnlich für ein hochfähiges und allgemeines System, das „passiv" und weitgehend unfähig zu unabhängiger Aktion ist. Systeme, denen *zwei* der drei Eigenschaften fehlen, sind noch handhabbarer und sichere Häfen wären noch einfacher zu beanspruchen. Dieser Ansatz spiegelt wider, wie wir mit anderen potenziell gefährlichen Technologien umgehen:[^26] höhere Haftung für gefährlichere Konfigurationen schafft natürliche Anreize für sicherere Alternativen.

Das Standardergebnis solch hoher Haftungsniveaus, die dazu dienen, AGI-Risiko zu *internalisieren* für Unternehmen anstatt es auf die Öffentlichkeit abzuwälzen, ist wahrscheinlich (und hoffentlich!), dass Unternehmen einfach keine vollständige AGI entwickeln, bis und außer sie sie wirklich vertrauenswürdig, sicher und kontrollierbar machen können, da *ihre eigene Führung* die Parteien sind, die Risiken tragen. (Falls das nicht ausreicht, sollte die Gesetzgebung zur Klärung der Haftung auch explizit einstweiligen Rechtsschutz ermöglichen, d.h. einen Richter, der einen Stopp für Aktivitäten anordnet, die eindeutig in der Gefahrenzone sind und argumentativ ein öffentliches Risiko darstellen.) Während Regulierung eingeführt wird, kann die Einhaltung von Regulierung zum sicheren Hafen werden, und die sicheren Häfen von niedriger Autonomie, Engheit oder Schwäche von KI-Systemen können sich in relativ leichtere Regulierungsregime verwandeln.

## Schlüsselbestimmungen eines Torschlusses

Mit der obigen Diskussion im Hinterkopf bietet dieser Abschnitt Vorschläge für Schlüsselbestimmungen, die ein Verbot vollständiger AGI und Superintelligenz implementieren und aufrechterhalten würden, und die Verwaltung menschenwettbewerbsfähiger oder expertenwettbewerbsfähiger Allzweck-KI nahe der vollständigen AGI-Schwelle.[^27] Er hat vier Schlüsselteile: 1) Rechenleistungsbilanzierung und -aufsicht, 2) Rechenleistungsobergrenzen beim Training und Betrieb von KI, 3) ein Haftungsrahmen, und 4) gestufte Sicherheits- und Schutzstandards, die harte regulatorische Anforderungen beinhalten. Diese werden als nächstes knapp beschrieben, mit weiteren Details oder Implementierungsbeispielen in drei begleitenden Tabellen. Wichtig zu bemerken ist, dass diese weit davon entfernt sind, alles zu sein, was notwendig sein wird, um fortgeschrittene KI-Systeme zu regieren; während sie zusätzliche Sicherheits- und Schutzvorteile haben werden, zielen sie darauf ab, das Tor zum Intelligenz-Kontrollverlust zu schließen und die KI-Entwicklung in eine bessere Richtung umzuleiten.

### 1\. Rechenleistungsbilanzierung und Transparenz

- Eine Standardorganisation (z.B. NIST in den USA gefolgt von ISO/IEEE international) sollte einen detaillierten technischen Standard für die Gesamt-Rechenleistung kodifizieren, die beim Training und Betrieb von KI-Modellen verwendet wird, in FLOP, und die Geschwindigkeit in FLOP/s, mit der sie operieren. Details dafür, wie das aussehen könnte, sind in Anhang A gegeben.[^28]
- Eine Anforderung – entweder durch neue Gesetzgebung oder unter bestehender Autorität[^29] – sollte von Rechtsprechungen auferlegt werden, in denen großangelegtes KI-Training stattfindet, um die Gesamt-FLOP zu berechnen und an eine Regulierungsbehörde oder andere Agentur zu berichten, die beim Training und Betrieb aller Modelle über einer Schwelle von 10<sup>25</sup> FLOP oder 10<sup>18</sup> FLOP/s verwendet werden.[^30]
- Diese Anforderungen sollten schrittweise eingeführt werden, zunächst mit gut dokumentierten Schätzungen nach Treu und Glauben auf vierteljährlicher Basis, mit späteren Phasen, die progressiv höhere Standards erfordern, bis hin zu kryptografisch beglaubigten Gesamt-FLOP und FLOP/s, die jeder Modell-*Ausgabe* beigefügt sind.
- Diese Berichte sollten durch gut dokumentierte Schätzungen der marginalen Energie- und Finanzkosten ergänzt werden, die bei der Generierung jeder KI-Ausgabe verwendet werden.

Begründung: Diese gut berechneten und transparent berichteten Zahlen würden die Basis für Training- und Betriebsobergrenzen bieten sowie einen sicheren Hafen vor höheren Haftungsmaßnahmen (siehe Anhänge C und D).

### 2\. Training- und Betrieb-Rechenleistungsobergrenzen

- Rechtsprechungen, die KI-Systeme beherbergen, sollten eine harte Grenze für die Gesamt-Rechenleistung auferlegen, die in jede KI-Modell-Ausgabe fließt, beginnend bei 10<sup>27</sup> FLOP[^31] und anpassbar wie angemessen.
- Rechtsprechungen, die KI-Systeme beherbergen, sollten eine harte Grenze für die Rechenleistungsrate von KI-Modell-Ausgaben auferlegen, beginnend bei 10<sup>20</sup> FLOP/s und anpassbar wie angemessen.

Begründung: Gesamt-Rechenleistung ist, obwohl sehr unvollkommen, ein Stellvertreter für KI-Fähigkeit (und Risiko), der konkret messbar und verifizierbar ist, also eine harte Absicherung für die Begrenzung von Fähigkeiten bietet. Ein konkreter Implementierungsvorschlag ist in Anhang B gegeben.

### 3\. Verstärkte Haftung für gefährliche Systeme

- Die Schaffung und der Betrieb[^32] eines fortgeschrittenen KI-Systems, das hochgradig allgemein, fähig und autonom ist, sollte per Gesetzgebung rechtlich klargestellt werden, dass es strenger, gesamtschuldnerischer und nicht einzelpartei-verschuldensbasierter Haftung unterliegt.[^33]
- Ein rechtliches Verfahren sollte verfügbar sein, um bejahende Sicherheitsfälle zu machen, die sicheren Hafen vor strenger Haftung für Systeme gewähren würden, die klein sind (in Bezug auf Rechenleistung), schwach, eng, passiv oder die ausreichende Sicherheits-, Schutz- und Kontrollierbarkeitsgarantien haben.
- Ein expliziter Pfad und eine Reihe von Bedingungen für einstweiligen Rechtsschutz zum Stoppen von KI-Training- und Inferenz-Aktivitäten, die eine öffentliche Gefahr darstellen, sollten umrissen werden.

Begründung: KI-Systeme können nicht verantwortlich gemacht werden, also müssen wir menschliche Individuen und Organisationen für Schäden verantwortlich machen, die sie verursachen (Haftung).[^34] Unkontrollierbare AGI ist eine Bedrohung für Gesellschaft und Zivilisation und sollte in Abwesenheit eines Sicherheitsfalls als abnormal gefährlich betrachtet werden. Die Beweislast auf Entwickler zu legen zu zeigen, dass mächtige Modelle sicher genug sind, um nicht als „abnormal gefährlich" betrachtet zu werden, schafft Anreize für sichere Entwicklung, zusammen mit Transparenz und Protokollführung, um diese sicheren Häfen zu beanspruchen. Regulierung kann dann Schaden verhindern, wo Abschreckung durch Haftung unzureichend ist. Schließlich sind KI-Entwickler bereits haftbar für Schäden, die sie verursachen, also kann die rechtliche Klärung der Haftung für die riskantesten Systeme sofort erfolgen, ohne dass hochdetaillierte Standards entwickelt werden müssen; diese können sich dann über die Zeit entwickeln. Details sind in Anhang C gegeben.

### 4\. Sicherheitsregulierung für KI

Ein Regulierungssystem, das großangelegte akute Risiken von KI adressiert, wird mindestens erfordern:

- Die Identifikation oder Schaffung einer angemessenen Reihe von Regulierungsbehörden, wahrscheinlich eine neue Agentur;
- Ein umfassendes Risikobewertungsrahmenwerk;[^35]
- Ein Rahmenwerk für bejahende Sicherheitsfälle, teilweise basierend auf dem Risikobewertungsrahmenwerk, die von Entwicklern gemacht werden sollen, und für Auditing durch *unabhängige* Gruppen und Agenturen;
- Ein gestuftes Lizenzsystem, bei dem Stufen Fähigkeitsniveaus verfolgen.[^36] Lizenzen würden auf Basis von Sicherheitsfällen und Audits für Entwicklung und Deployment von Systemen gewährt. Anforderungen würden von Benachrichtigung am unteren Ende bis zu quantitativen Sicherheits-, Schutz- und Kontrollierbarkeitsgarantien vor der Entwicklung am oberen Ende reichen. Diese würden die Freigabe von Systemen verhindern, bis sie als sicher demonstriert sind, und die Entwicklung inhärent unsicherer Systeme verbieten. Anhang D bietet einen Vorschlag dafür, was solche Sicherheits- und Schutzstandards beinhalten könnten.
- Vereinbarungen, solche Maßnahmen auf die internationale Ebene zu bringen, einschließlich internationaler Gremien zur Harmonisierung von Normen und Standards, und potentiell internationaler Agenturen zur Überprüfung von Sicherheitsfällen.

Begründung: Letztendlich ist Haftung nicht der richtige Mechanismus für die Verhinderung großangelegter Risiken für die Öffentlichkeit durch eine neue Technologie. Umfassende Regulierung mit ermächtigten Regulierungsbehörden wird für KI genauso nötig sein wie für jede andere große Industrie, die ein Risiko für die Öffentlichkeit darstellt.[^37]

Regulierung zur Verhinderung anderer pervasiver aber weniger akuter Risiken wird wahrscheinlich in ihrer Form von Rechtsprechung zu Rechtsprechung variieren. Das Entscheidende ist, die Entwicklung der KI-Systeme zu vermeiden, die so riskant sind, dass diese Risiken nicht handhabbar sind.

## Was dann?

Über das nächste Jahrzehnt, während KI allgegenwärtiger wird und die Kerntechnologie voranschreitet, werden wahrscheinlich zwei Schlüsseldinge passieren. Erstens wird die Regulierung bestehender mächtiger KI-Systeme schwieriger werden, aber noch notwendiger. Es ist wahrscheinlich, dass zumindest einige Maßnahmen zur Adressierung großangelegter Sicherheitsrisiken Vereinbarung auf internationaler Ebene erfordern werden, wobei einzelne Rechtsprechungen auf internationalen Vereinbarungen basierende Regeln durchsetzen.

Zweitens werden Training- und Betrieb-Rechenleistungsobergrenzen schwerer aufrechtzuerhalten, da Hardware billiger und kosteneffizienter wird; sie könnten auch weniger relevant werden (oder noch enger sein müssen) mit Fortschritten in Algorithmen und Architekturen.

Dass die Kontrolle von KI schwieriger wird, bedeutet nicht, dass wir aufgeben sollten! Die Implementierung des in diesem Essay umrissenen Plans würde uns sowohl wertvolle Zeit als auch entscheidende Kontrolle über den Prozess geben, die uns in eine weit, weit bessere Position bringen würde, das existentielle Risiko von KI für unsere Gesellschaft, Zivilisation und Spezies zu vermeiden.

Auf noch längere Sicht werden Entscheidungen zu treffen sein, was wir erlauben. Wir können uns immer noch dafür entscheiden, eine Form wirklich kontrollierbarer AGI zu schaffen, soweit das sich als möglich erweist. Oder wir können entscheiden, dass die Führung der Welt besser den Maschinen überlassen wird, wenn wir uns selbst überzeugen können, dass sie einen besseren Job dabei machen und uns gut behandeln werden. Aber das sollten Entscheidungen sein, die mit tiefem wissenschaftlichem Verständnis von KI in der Hand getroffen werden, und nach bedeutsamer globaler inklusiver Diskussion, nicht in einem Rennen zwischen Tech-Moguln mit dem Großteil der Menschheit völlig unbeteiligt und unwissend.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) Zusammenfassung der A-G-I und Superintelligenz-Governance via Haftung und Regulierung. Haftung ist am höchsten, und Regulierung am stärksten, bei der Dreifachschnittstelle von Autonomie, Allgemeinheit und Intelligenz. Sichere Häfen vor strenger Haftung und starker Regulierung können durch bejahende Sicherheitsfälle erhalten werden, die demonstrieren, dass ein System schwach und/oder eng und/oder passiv ist. Obergrenzen für Gesamt-Training-Rechenleistung und Inferenz-Rechenleistungsrate, verifiziert und durchgesetzt rechtlich und unter Verwendung von Hardware- und kryptografischen Sicherheitsmaßnahmen, sichern Sicherheit ab, indem sie vollständige AGI vermeiden und Superintelligenz effektiv verbieten.

[^1]: Höchstwahrscheinlich wird die Verbreitung dieser Erkenntnis entweder intensive Anstrengungen von Bildungs- und Advocacy-Gruppen erfordern, die diesen Fall machen, oder eine ziemlich bedeutende KI-verursachte Katastrophe. Wir können hoffen, dass es ersteres sein wird.

[^2]: Paradoxerweise sind wir es gewohnt, dass die Natur unsere Technologie begrenzt, indem sie sie sehr schwer zu entwickeln macht, besonders wissenschaftlich. Aber das ist bei KI nicht mehr der Fall: Die wichtigsten wissenschaftlichen Probleme erweisen sich als einfacher als erwartet. Wir können nicht darauf zählen, dass die Natur uns hier vor uns selbst rettet – wir werden es selbst tun müssen.

[^3]: Wo genau stoppen wir bei der Entwicklung neuer Systeme? Hier sollten wir ein Vorsorgeprinzip anwenden. Sobald ein System deployed ist, und besonders sobald dieses Niveau der Systemfähigkeit proliferiert, ist es außerordentlich schwierig zurückzurollen. Und wenn ein System *entwickelt* ist (besonders unter großen Kosten und Anstrengungen), wird es enormen Druck geben, es zu nutzen oder zu deployen, und Versuchung für es, geleakt oder gestohlen zu werden. Systeme zu entwickeln und *dann* zu entscheiden, ob sie tiefgreifend unsicher sind, ist ein gefährlicher Weg.

[^4]: Es wäre auch weise, KI-Entwicklung zu verbieten, die intrinsisch gefährlich ist, wie sich selbst replizierende und evolvierende Systeme, solche, die darauf ausgelegt sind, aus der Einschließung zu entkommen, solche, die sich autonom selbst verbessern können, absichtlich täuschende und bösartige KI, etc.

[^5]: Bemerke, das bedeutet nicht notwendigerweise auf internationaler Ebene *durchgesetzt* von irgendeiner Art globaler Körperschaft: stattdessen könnten souveräne Nationen vereinbarte Regeln durchsetzen, wie in vielen Verträgen.

[^6]: Wie wir unten sehen werden, würde die Natur der KI-Berechnung etwas Hybrides erlauben; aber internationale Kooperation wird immer noch nötig sein.

[^7]: Zum Beispiel werden die Maschinen, die zum Ätzen KI-relevanter Chips benötigt werden, nur von einer Firma hergestellt, ASML (trotz vieler anderer Versuche, das zu tun), die überwiegende Mehrheit relevanter Chips wird von einer Firma hergestellt, TSMC (trotz anderer, die versuchen zu konkurrieren), und das Design und die Konstruktion von Hardware aus diesen Chips wird nur von wenigen gemacht, einschließlich NVIDIA, AMD und Google.

[^8]: Am wichtigsten hält jeder Chip einen einzigartigen und unzugänglichen kryptografischen privaten Schlüssel, den er verwenden kann, um Dinge zu „signieren".

[^9]: Standardmäßig wäre das das Unternehmen, das die Chips verkauft, aber andere Modelle sind möglich und potentiell nützlich.

[^10]: Ein Gouverneur kann den Standort eines Chips durch Timing des Austauschs signierter Nachrichten mit ihm ermitteln: Die endliche Lichtgeschwindigkeit erfordert, dass der Chip innerhalb eines gegebenen Radius *r* einer „Station" ist, wenn er eine signierte Nachricht in einer Zeit weniger als *r* / *c* zurückgeben kann, wobei *c* die Lichtgeschwindigkeit ist. Durch mehrere Stationen und einiges Verständnis der Netzwerk-Charakteristika kann der Standort des Chips bestimmt werden. Die Schönheit dieser Methode ist, dass der Großteil ihrer Sicherheit von den Gesetzen der Physik geliefert wird. Andere Methoden könnten GPS, Trägheitsverfolgung und ähnliche Technologien verwenden.

[^11]: Alternativ könnten Paare von Chips nur mit expliziter Erlaubnis eines Gouverneurs miteinander kommunizieren dürfen.

[^12]: Das ist entscheidend, weil zumindest derzeit sehr hohe Bandbreite-Verbindung zwischen Chips nötig ist, um große KI-Modelle auf ihnen zu trainieren.

[^13]: Das könnte auch so eingerichtet werden, dass signierte Nachrichten von *N* von *M* verschiedenen Gouverneuren erforderlich sind, was mehreren Parteien erlaubt, Governance zu teilen.

[^14]: Das ist bei weitem nicht beispiellos – zum Beispiel haben Militärs keine Armeen geklonter oder genetisch veränderter Supersoldaten entwickelt, obwohl das wahrscheinlich technologisch möglich ist. Aber sie haben sich *entschieden*, das nicht zu tun, anstatt von anderen daran gehindert zu werden. Die Erfolgsbilanz ist nicht großartig dafür, dass große Weltmächte daran gehindert werden, eine Technologie zu entwickeln, die sie stark entwickeln wollen.

[^15]: Mit ein paar bemerkenswerten Ausnahmen (insbesondere NVIDIA) ist die KI-spezialisierte Hardware ein relativ kleiner Teil des Gesamtgeschäfts und Umsatzmodells dieser Unternehmen. Außerdem ist die Lücke zwischen Hardware, die in fortgeschrittener KI verwendet wird, und „Verbraucher-grade" Hardware bedeutend, also wären die meisten Verbraucher von Computer-Hardware weitgehend unbeeinträchtigt.

[^16]: Für detailliertere Analyse, siehe die kürzlichen Berichte von [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) und [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). Diese fokussieren auf technische Machbarkeit, besonders im Kontext von US-Exportkontrollen, die die Kapazität anderer Länder in high-end Berechnung beschränken sollen; aber das hat offensichtliche Überschneidung mit der hier vorgestellten globalen Beschränkung.

[^17]: Apple-Geräte zum Beispiel werden remote und sicher gesperrt, wenn sie als verloren oder gestohlen gemeldet werden, und können remote reaktiviert werden. Das beruht auf denselben Hardware-Sicherheits-Features, die hier diskutiert werden.

[^18]: Siehe z.B. IBMs [capacity on demand](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) Angebot, Intels [Intel on demand.](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html), und Apples [private cloud compute](https://security.apple.com/blog/private-cloud-compute/).

[^19]: [Diese Studie](https://epochai.org/trends#hardware-trends-section) zeigt, dass historisch dieselbe Performance mit etwa 30% weniger Dollar pro Jahr erreicht wurde. Wenn dieser Trend anhält, könnte es bedeutende Überschneidung zwischen KI- und „Verbraucher"-Chip-Nutzung geben, und im Allgemeinen könnte die Menge benötigter Hardware für hochleistungsfähige KI-Systeme unbequem klein werden.

[^20]: Laut der [gleichen Studie](https://epochai.org/trends#hardware-trends-section) hat gegebene Performance bei Bilderkennung 2,5x weniger Berechnung pro Jahr erfordert. Wenn das auch für die fähigsten KI-Systeme gelten würde, wäre ein Berechnungslimit nicht sehr lange nützlich.

[^21]: Insbesondere auf Länderebene sieht das sehr nach einer Nationalisierung der Berechnung aus, insofern die Regierung viel Kontrolle darüber hätte, wie Rechenleistung genutzt wird. Jedoch für die, die sich wegen Regierungsbeteiligung sorgen, scheint das bei weitem sicherer und vorzuziehen zur mächtigsten KI-*Software* selbst, die via einer Fusion zwischen großen KI-Unternehmen und nationalen Regierungen nationalisiert wird, wie einige zu befürworten beginnen.

[^22]: Ein großer regulatorischer Schritt in Europa wurde mit der Verabschiedung des [EU AI Act](https://artificialintelligenceact.eu/) 2024 gemacht. Er klassifiziert KI nach Risiko: verbietet inakzeptable Systeme, reguliert hochriskante und erlegt Transparenzregeln oder gar keine Maßnahmen für niedrigrisikante Systeme auf. Er wird einige KI-Risiken bedeutend reduzieren und KI-Transparenz sogar für US-Firmen stärken, aber hat zwei Schlüsselmängel. Erstens begrenzte Reichweite: während er für jede Firma gilt, die KI in der EU anbietet, ist die Durchsetzung über US-basierte Firmen schwach, und Militär-KI ist ausgenommen. Zweitens, während er GPAI abdeckt, erkennt er AGI oder Superintelligenz nicht als inakzeptable Risiken oder verhindert ihre Entwicklung – nur ihr EU-Deployment. Als Ergebnis tut er wenig, um die Risiken von AGI oder Superintelligenz einzudämmen.

[^23]: Unternehmen vertreten oft, dass sie für vernünftige Regulierung sind. Aber irgendwie scheinen sie fast immer jede *bestimmte* Regulierung zu bekämpfen; siehe den Kampf um das ziemlich leichte SB1047, dem [die meisten KI-Unternehmen öffentlich oder privat widersprochen haben.](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/)

[^24]: Es war etwa 3 1/2 Jahre von dem Zeitpunkt, als der EU AI Act vorgeschlagen wurde, bis er in Kraft trat.

[^25]: Es wird manchmal ausgedrückt, dass es „zu früh" sei, mit der Regulierung von KI zu beginnen. Angesichts der letzten Anmerkung scheint das kaum wahrscheinlich. Eine andere ausgedrückte Sorge ist, dass Regulierung „Innovation schaden" würde. Aber gute Regulierung ändert nur die Richtung, nicht die Menge der Innovation.

[^26]: Ein interessanter Präzedenzfall ist beim Transport gefährlicher Materialien, die entkommen und Schaden verursachen könnten. Hier haben [Regulierung](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) und [Rechtsprechung](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) strenge Haftung für sehr gefährliche Materialien wie Sprengstoff, Benzin, Gifte, infektiöse Agenten und radioaktiven Abfall etabliert. Andere Beispiele beinhalten [Warnungen auf Pharmazeutika](https://www.medicalnewstoday.com/articles/boxed-warnings), [Klassen von Medizingeräten,](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification) etc.

[^27]: Ein anderer umfassender Vorschlag mit ähnlichen Zielen, der in ["A Narrow Path"](https://www.narrowpath.co/) vorgestellt wird, befürwortet einen zentralisierteren, verbots-basierten Ansatz, der alle Frontier-KI-Entwicklung durch eine einzige internationale Entität leitet, überwacht von starken internationalen Institutionen, mit klaren kategorialen Verboten anstatt graduierten Beschränkungen. Ich würde auch diesen Plan unterstützen; jedoch wird er noch mehr politischen Willen und Koordination erfordern als der hier vorgeschlagene.

[^28]: Einige Richtlinien für einen solchen Standard wurden vom Frontier Model Forum [veröffentlicht](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/). Relativ zu dem hier vorgeschlagenen neigen diese dazu, weniger Präzision und weniger in der Abrechnung enthaltene Rechenleistung zu haben.

[^29]: Der US-KI-Exekutiverlass von 2023 (jetzt widerrufen) erforderte ähnliche aber weniger feinkörnige Berichterstattung. Das sollte durch einen ersetzenden Erlass gestärkt werden.

[^30]: Sehr grob entspricht das für jetzt-gebräuchliche H100-Chips Clustern von etwa 1000, die Inferenz machen; es sind etwa 100 (etwa USD $5M wert) der allerneuesten NVIDIA B200-Chips, die Inferenz machen. In beiden Fällen entspricht die Training-Zahl diesem Cluster, der mehrere Monate rechnet.

[^31]: Diese Menge ist größer als jedes derzeit trainierte KI-System; eine größere oder kleinere Zahl könnte gerechtfertigt sein, da wir besser verstehen, wie KI-Fähigkeit mit Rechenleistung skaliert.

[^32]: Das gilt für die, die die Modelle erstellen und anbieten/hosten, nicht Endnutzer.

[^33]: Grob bedeutet „strenge" Haftung, dass Entwickler *standardmäßig* für Schäden verantwortlich gemacht werden, die von einem Produkt verursacht werden, und ist ein Standard, der für „abnormal gefährliche" Produkte verwendet wird, und (etwas amüsant aber angemessen) wilde Tiere. „Gesamtschuldnerische" Haftung bedeutet, dass Haftung all den Parteien zugewiesen wird, die für ein Produkt verantwortlich sind, und diese Parteien müssen unter sich ausmachen, wer welche Verantwortung trägt. Das ist wichtig für Systeme wie KI mit einer langen und komplexen Wertschöpfungskette.

[^34]: Standard verschuldensbasierte einzelpartei Haftung reicht nicht: Verschulden wird sowohl schwierig zu verfolgen als auch zuzuweisen sein, weil KI-Systeme komplex sind, ihr Betrieb nicht verstanden wird, und viele Parteien bei der Schaffung eines gefährlichen Systems oder einer Ausgabe beteiligt sein können. Zusätzlich werden Klagen Jahre dauern zu verhandeln und wahrscheinlich nur in Geldstrafen resultieren, die für diese Unternehmen belanglos sind, also ist persönliche Haftung für Führungskräfte ebenfalls wichtig.

[^35]: Es sollte keine Ausnahme von Sicherheitskriterien für Open-Weight-Modelle geben. Außerdem sollte bei der Risikobewertung angenommen werden, dass Leitplanken, die entfernt werden können, von weit verfügbaren Modellen entfernt werden, und dass sogar geschlossene Modelle proliferieren werden, außer es gibt sehr hohe Sicherheit, dass sie sicher bleiben.

[^36]: Das hier vorgeschlagene Schema hat regulatorische Prüfung, die durch allgemeine Fähigkeit ausgelöst wird; jedoch macht es Sinn für einige besonders riskante Anwendungsfälle, mehr Prüfung auszulösen – zum Beispiel sollte ein Experten-Virologie-KI-System, selbst wenn eng und passiv, wahrscheinlich in eine höhere Stufe gehen. Der frühere US-Exekutiverlass hatte etwas von dieser Struktur für biologische Fähigkeiten.

[^37]: Zwei klare Beispiele sind Luftfahrt und Medizin, reguliert von der FAA und FDA, und ähnlichen Agenturen in anderen Ländern. Diese Agenturen sind unvollkommen, aber sind absolut vital für das Funktionieren und den Erfolg dieser Industrien gewesen.