# Kapitel 9 - Die Zukunft gestalten — was wir stattdessen tun sollten

KI kann unglaublich viel Gutes in der Welt bewirken. Um alle Vorteile ohne die Risiken zu erhalten, müssen wir sicherstellen, dass KI ein menschliches Werkzeug bleibt.

Wenn wir erfolgreich die Entscheidung treffen, die Menschheit nicht durch Maschinen zu ersetzen – zumindest für eine Weile! – was können wir stattdessen tun? Geben wir das enorme Potenzial der KI als Technologie auf? Auf einer gewissen Ebene ist die Antwort ein einfaches *Nein:* Schließt die Tore zu unkontrollierbarer AGI und Superintelligenz, aber baut *sehr wohl* viele andere Formen von KI sowie die Governance-Strukturen und Institutionen auf, die wir benötigen, um sie zu verwalten.

Aber es gibt noch viel zu sagen; dies zu verwirklichen wäre eine zentrale Aufgabe der Menschheit. Dieser Abschnitt erkundet mehrere Schlüsselthemen:

- Wie wir "Werkzeug"-KI charakterisieren können und welche Formen sie annehmen kann.
- Dass wir (fast) alles erreichen können, was die Menschheit will, ohne AGI, mit Werkzeug-KI.
- Dass Werkzeug-KI-Systeme (wahrscheinlich, prinzipiell) beherrschbar sind.
- Dass sich von AGI abzuwenden nicht bedeutet, bei der nationalen Sicherheit Kompromisse einzugehen – ganz im Gegenteil.
- Dass Machtkonzentration ein reales Problem darstellt. Können wir sie abmildern, ohne Sicherheit zu untergraben?
- Dass wir neue Governance- und Gesellschaftsstrukturen wollen und brauchen werden, und KI kann tatsächlich dabei helfen.

## KI innerhalb der Tore: Werkzeug-KI

Das Drei-Schnittmengen-Diagramm bietet eine gute Möglichkeit, das abzugrenzen, was wir "Werkzeug-KI" nennen können: KI, die ein kontrollierbares Werkzeug für den menschlichen Gebrauch ist, anstatt ein unkontrollierbarer Rivale oder Ersatz. Die am wenigsten problematischen KI-Systeme sind solche, die autonom, aber nicht allgemein oder superkompetent sind (wie ein Auktionsbietbot), oder allgemein, aber nicht autonom oder kompetent (wie ein kleines Sprachmodell), oder kompetent, aber eng und sehr kontrollierbar (wie AlphaGo).[^1] Systeme mit zwei sich überschneidenden Eigenschaften haben breitere Anwendung, aber höheres Risiko und werden erhebliche Managementanstrengungen erfordern. (Nur weil ein KI-System eher ein Werkzeug ist, bedeutet das nicht, dass es inhärent sicher ist, lediglich dass es nicht inhärent *unsicher* ist – man denke an eine Kettensäge im Vergleich zu einem Haustiger.) Das Tor muss zur (vollen) AGI und Superintelligenz an der dreifachen Schnittstelle geschlossen bleiben, und enorme Sorgfalt muss bei KI-Systemen walten, die sich dieser Schwelle nähern.

Aber das lässt viel mächtige KI übrig! Wir können enormen Nutzen aus intelligenten und allgemeinen passiven "Orakeln" und engen Systemen ziehen, aus allgemeinen Systemen auf menschlichem, aber nicht übermenschlichem Niveau, und so weiter. Viele Tech-Unternehmen und Entwickler bauen aktiv solche Werkzeuge und sollten damit fortfahren; wie die meisten Menschen *nehmen* sie implizit an, dass die Tore zu AGI und Superintelligenz geschlossen werden.[^2]

Außerdem können KI-Systeme effektiv zu zusammengesetzten Systemen kombiniert werden, die menschliche Aufsicht wahren und gleichzeitig die Leistungsfähigkeit steigern. Anstatt uns auf unergründliche Black Boxes zu verlassen, können wir Systeme bauen, in denen mehrere Komponenten – sowohl KI als auch traditionelle Software – auf Weisen zusammenarbeiten, die Menschen überwachen und verstehen können.[^3] Während einige Komponenten Black Boxes sein mögen, wäre keine nahe an AGI – nur das zusammengesetzte System als Ganzes wäre sowohl hochallgemein als auch hochkompetent, und das auf eine strikt kontrollierbare Weise.[^4]

### Sinnvolle und garantierte menschliche Kontrolle

Was bedeutet "strikt kontrollierbar"? Eine Schlüsselidee des "Werkzeug"-Rahmens ist es, Systeme zu ermöglichen – auch wenn sie ziemlich allgemein und mächtig sind –, die garantiert unter sinnvoller menschlicher Kontrolle stehen. Was bedeutet das? Es beinhaltet zwei Aspekte. Erstens ist es eine Designüberlegung: Menschen sollten tief und zentral in das involviert sein, was das System tut, *ohne* wichtige Entscheidungen an die KI zu delegieren. Das ist der Charakter der meisten aktuellen KI-Systeme. Zweitens müssen autonome KI-Systeme, soweit sie autonom sind, Garantien haben, die ihren Handlungsbereich begrenzen. Eine Garantie sollte eine *Zahl* sein, die die Wahrscheinlichkeit charakterisiert, dass etwas passiert, und einen Grund zu glauben, dass diese Zahl stimmt. Das ist es, was wir in anderen sicherheitskritischen Bereichen verlangen, wo Zahlen wie "mittlere Zeit zwischen Ausfällen" und erwartete Anzahl von Unfällen berechnet, untermauert und in Sicherheitsnachweisen veröffentlicht werden.[^5] Die ideale Zahl für Ausfälle ist natürlich null. Und die gute Nachricht ist, dass wir ziemlich nahe herankommen könnten, allerdings mit ganz anderen KI-Architekturen, unter Verwendung von Ideen *formal verifizierter* Eigenschaften von Programmen (einschließlich KI). Die Idee, ausführlich von Omohundro, Tegmark, Bengio, Dalrymple und anderen erforscht (siehe [hier](https://arxiv.org/abs/2309.01933) und [hier](https://arxiv.org/abs/2405.06624)), besteht darin, ein Programm mit bestimmten Eigenschaften zu konstruieren (zum Beispiel: dass ein Mensch es abschalten kann) und formal zu *beweisen*, dass diese Eigenschaften gelten. Dies kann jetzt für ziemlich kurze Programme und einfache Eigenschaften getan werden, aber die (kommende) Macht KI-gestützter Beweissoftware könnte es für viel komplexere Programme (z.B. Wrapper) und sogar KI selbst ermöglichen. Das ist ein sehr ehrgeiziges Programm, aber da der Druck auf die Tore wächst, werden wir einige mächtige Materialien brauchen, die sie verstärken. Mathematischer Beweis könnte einer der wenigen sein, der stark genug ist.

### Wohin mit der KI-Industrie

Mit umgelenktem KI-Fortschritt wäre Werkzeug-KI immer noch eine enorme Industrie. Was die Hardware betrifft, würden selbst mit Rechenleistungsobergrenzen zur Verhinderung von Superintelligenz Training und Inferenz in kleineren Modellen immer noch riesige Mengen spezialisierter Komponenten erfordern. Auf der Software-Seite sollte die Entschärfung der Explosion bei KI-Modell- und Rechengrößen einfach dazu führen, dass Unternehmen Ressourcen darauf umleiten, die kleineren Systeme besser, vielfältiger und spezialisierter zu machen, anstatt sie einfach größer zu machen.[^6] Es gäbe reichlich Raum – wahrscheinlich mehr – für all jene gewinnbringenden Silicon Valley-Startups.[^7]

## Werkzeug-KI kann (fast) alles liefern, was die Menschheit will, ohne AGI

Intelligenz, ob biologisch oder maschinell, kann im Großen und Ganzen als die Fähigkeit betrachtet werden, Aktivitäten zu planen und auszuführen, die Zukünfte herbeiführen, die mehr im Einklang mit einer Reihe von Zielen stehen. Als solche ist Intelligenz von enormem Nutzen, wenn sie zur Verfolgung weise gewählter Ziele eingesetzt wird. Künstliche Intelligenz zieht riesige Investitionen von Zeit und Anstrengung an, hauptsächlich wegen ihrer versprochenen Vorteile. Also sollten wir fragen: Zu welchem Grad würden wir immer noch die Vorteile der KI ernten, wenn wir ihren Kontrollverlust zur Superintelligenz eindämmen? Die Antwort: Wir könnten überraschend wenig verlieren.

Betrachten wir zunächst, dass aktuelle KI-Systeme bereits sehr mächtig sind, und wir haben wirklich nur an der Oberfläche dessen gekratzt, was mit ihnen getan werden kann.[^8] Sie sind durchaus imstande, "die Führung zu übernehmen" im Sinne des "Verstehens" einer ihnen vorgelegten Frage oder Aufgabe und dessen, was es bräuchte, um diese Frage zu beantworten oder jene Aufgabe zu erledigen.

Als Nächstes ist viel der Begeisterung für moderne KI-Systeme ihrer Allgemeinheit geschuldet; aber einige der fähigsten KI-Systeme – wie solche, die Sprache oder Bilder generieren oder erkennen, wissenschaftliche Vorhersagen und Modellierung betreiben, Spiele spielen usw. – sind viel enger und gut "innerhalb der Tore" in Bezug auf die Rechenleistung.[^9] Diese Systeme sind übermenschlich bei den speziellen Aufgaben, die sie erfüllen. Sie mögen Grenzfall-[^10] (oder [ausnutzbare](https://arxiv.org/abs/2211.00241)) Schwächen aufgrund ihrer Enge haben; jedoch sind *völlig* eng oder *völlig* allgemein nicht die einzigen verfügbaren Optionen: Es gibt viele Architekturen dazwischen.[^11]

Diese KI-Werkzeuge können den Fortschritt in anderen positiven Technologien erheblich beschleunigen, ohne AGI. Um bessere Kernphysik zu betreiben, brauchen wir nicht, dass KI ein Kernphysiker ist – wir haben welche! Wenn wir die Medizin vorantreiben wollen, geben wir den Biologen, Medizinforschern und Chemikern mächtige Werkzeuge. Sie wollen sie und werden sie zu enormem Gewinn nutzen. Wir brauchen keine Serverfarm voller einer Million digitaler Genies; wir haben Millionen von Menschen, deren Genialität KI helfen kann hervorzubringen. Ja, es wird länger dauern, Unsterblichkeit und die Heilung aller Krankheiten zu erreichen. Das ist ein echter Preis. Aber selbst die vielversprechendsten Gesundheitsinnovationen wären von geringem Nutzen, wenn KI-getriebene Instabilität zu globalem Konflikt oder gesellschaftlichem Kollaps führt. Wir schulden es uns selbst, KI-gestärkten Menschen zuerst eine Chance bei dem Problem zu geben.

Und angenommen, es gibt tatsächlich irgendeinen enormen Vorteil von AGI, der nicht von der Menschheit mit innerhalb-der-Tore-Werkzeugen erreicht werden kann. Verlieren wir diese, indem wir *niemals* AGI und Superintelligenz bauen? Bei der Abwägung von Risiken und Belohnungen hier gibt es einen enormen asymmetrischen Vorteil beim Warten versus Eilen: Wir können warten, bis es auf garantiert sichere und vorteilhafte Weise getan werden kann, und fast jeder wird immer noch die Früchte ernten können; wenn wir eilen, könnte es – in den Worten des OpenAI-CEOs Sam Altman – [Licht aus für *uns alle* bedeuten.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

Aber wenn Nicht-AGI-Werkzeuge potenziell so mächtig sind, können wir sie handhaben? Die Antwort ist ein klares... vielleicht.

## Werkzeug-KI-Systeme sind (wahrscheinlich, prinzipiell) handhabbar

Aber es wird nicht einfach sein. Aktuelle hochmoderne KI-Systeme können Menschen und Institutionen beim Erreichen ihrer Ziele erheblich stärken. Das ist im Allgemeinen eine gute Sache! Jedoch gibt es natürliche Dynamiken, solche Systeme zu unserer Verfügung zu haben – plötzlich und ohne viel Zeit für die Gesellschaft, sich anzupassen –, die ernste Risiken bieten, die bewältigt werden müssen. Es lohnt sich, einige große Klassen solcher Risiken zu diskutieren und wie sie verringert werden können, unter der Annahme einer Torschließung.

Eine Klasse von Risiken besteht darin, dass hochleistungsstarke Werkzeug-KI Zugang zu Wissen oder Fähigkeiten ermöglicht, die zuvor an eine Person oder Organisation gebunden waren, wodurch eine Kombination aus hoher Fähigkeit plus hoher Loyalität einer sehr breiten Palette von Akteuren verfügbar gemacht wird. Heute könnte eine Person mit bösen Absichten mit genügend Geld ein Team von Chemikern anheuern, um neue Chemiewaffen zu entwerfen und herzustellen – aber es ist nicht so einfach, dieses Geld zu haben oder das Team zu finden/zusammenzustellen und es zu überzeugen, etwas ziemlich eindeutig Illegales, Unethisches und Gefährliches zu tun. Um zu verhindern, dass KI-Systeme eine solche Rolle spielen, könnten Verbesserungen aktueller Methoden durchaus ausreichen,[^12] solange alle diese Systeme und der Zugang zu ihnen verantwortlich verwaltet werden. Andererseits, wenn mächtige Systeme für allgemeine Nutzung und Modifikation freigegeben werden, sind alle eingebauten Sicherheitsmaßnahmen wahrscheinlich entfernbar. Um also Risiken in dieser Klasse zu vermeiden, werden starke Beschränkungen dessen erforderlich sein, was öffentlich freigegeben werden kann – analog zu Beschränkungen bei Details von nuklearen, explosiven und anderen gefährlichen Technologien.[^13]

Eine zweite Klasse von Risiken entspringt der Skalierung von Maschinen, die sich wie Menschen verhalten oder Menschen imitieren. Auf der Ebene des Schadens für Einzelpersonen umfassen diese Risiken viel effektivere Betrug, Spam und Phishing sowie die Verbreitung nicht einvernehmlicher Deepfakes.[^14] Auf kollektiver Ebene umfassen sie die Störung grundlegender sozialer Prozesse wie öffentliche Diskussion und Debatte, unsere gesellschaftlichen Informations- und Wissenserwerbs-, -verarbeitungs- und -verbreitungssysteme sowie unsere politischen Wahlsysteme. Die Minderung dieses Risikos wird wahrscheinlich (a) Gesetze zur Beschränkung der Nachahmung von Personen durch KI-Systeme und die Haftbarmachung von KI-Entwicklern, die Systeme schaffen, die solche Nachahmungen generieren, (b) Wasserzeichen- und Herkunftssysteme, die (verantwortungsvoll) generierte KI-Inhalte identifizieren und klassifizieren, und (c) neue sozio-technische epistemische Systeme umfassen, die eine vertrauenswürdige Kette von Daten (z.B. Kameras und Aufzeichnungen) durch Fakten, Verständnis und gute Weltmodelle schaffen können.[^15] All das ist möglich, und KI kann bei einigen Teilen davon helfen.

Ein drittes allgemeines Risiko besteht darin, dass Menschen, die derzeit diese Aufgaben erledigen, in dem Maße, wie bestimmte Aufgaben automatisiert werden, weniger finanziellen Wert als Arbeitskraft haben können. Historisch gesehen hat die Automatisierung von Aufgaben Dinge, die durch diese Aufgaben ermöglicht werden, billiger und reichlicher gemacht, während sie die Menschen, die zuvor diese Aufgaben erledigten, in solche sortierte, die immer noch an der automatisierten Version beteiligt sind (im Allgemeinen mit höherer Qualifikation/Bezahlung), und solche, deren Arbeit weniger oder wenig wert ist. Im Saldo ist es schwierig vorherzusagen, in welchen Sektoren mehr versus weniger menschliche Arbeit im resultierenden größeren, aber effizienteren Sektor benötigt wird. Parallel dazu neigt die Automatisierungsdynamik dazu, Ungleichheit und allgemeine Produktivität zu erhöhen, die Kosten bestimmter Güter und Dienstleistungen zu senken (über Effizienzsteigerungen) und die Kosten anderer zu erhöhen (über [Kostenkrankheit](https://en.wikipedia.org/wiki/Baumol_effect)). Für diejenigen auf der benachteiligten Seite der Ungleichheitszunahme ist es zutiefst unklar, ob die Kostensenkung bei diesen bestimmten Gütern und Dienstleistungen die Zunahme bei anderen aufwiegt und zu insgesamt größerem Wohlbefinden führt. Wie wird das also mit KI ablaufen? Wegen der relativen Leichtigkeit, mit der menschliche intellektuelle Arbeit durch allgemeine KI ersetzt werden kann, können wir eine schnelle Version hiervon mit menschenwettbewerbsfähiger Allzweck-KI erwarten.[^16] Wenn wir das Tor zu AGI schließen, werden viele weniger Jobs vollständig durch KI-Agenten ersetzt; aber enorme Arbeitsplatzverdrängung ist dennoch über einen Zeitraum von Jahren wahrscheinlich.[^17] Um weit verbreitetes wirtschaftliches Leiden zu vermeiden, wird es wahrscheinlich notwendig sein, sowohl eine Form universeller Grundvermögen oder -einkommen zu implementieren als auch einen kulturellen Wandel hin zur Wertschätzung und Belohnung menschenzentrierter Arbeit zu bewirken, die schwerer zu automatisieren ist (anstatt zu sehen, wie Arbeitspreise aufgrund des Anstiegs verfügbarer Arbeit, die aus anderen Teilen der Wirtschaft gedrängt wird, fallen.) Andere Konstrukte, wie das der ["Datenwürde"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society) (in dem die menschlichen Produzenten von Trainingsdaten automatisch Lizenzgebühren für den durch diese Daten in KI geschaffenen Wert erhalten), können helfen. Automatisierung durch KI hat auch einen zweiten potenziell nachteiligen Effekt, nämlich *unangemessene* Automatisierung. Neben Anwendungen, bei denen KI einfach eine schlechtere Arbeit leistet, würde dies solche umfassen, bei denen KI-Systeme wahrscheinlich moralische, ethische oder rechtliche Prinzipien verletzen – zum Beispiel bei Leben-und-Tod-Entscheidungen und in juristischen Angelegenheiten. Diese müssen durch Anwendung und Erweiterung unserer aktuellen rechtlichen Rahmen behandelt werden.

Schließlich ist eine bedeutende Bedrohung von innerhalb-der-Tore-KI ihr Einsatz in personalisierter Überzeugung, Aufmerksamkeitsfang und Manipulation. Wir haben in sozialen Medien und anderen Online-Plattformen das Wachstum einer tief verwurzelten Aufmerksamkeitsökonomie (wo Online-Dienste heftig um Nutzeraufmerksamkeit kämpfen) und ["Überwachungskapitalismus"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism)-Systemen (in denen Nutzerinformationen und Profilerstellung zur Kommodifizierung der Aufmerksamkeit hinzugefügt werden) gesehen. Es ist so gut wie sicher, dass mehr KI in den Dienst beider gestellt wird. KI wird bereits stark in süchtig machenden Feed-Algorithmen verwendet, aber das wird sich zu süchtig machenden KI-generierten Inhalten entwickeln, die angepasst sind, um von einer einzelnen Person zwanghaft konsumiert zu werden. Und die Eingaben, Reaktionen und Daten dieser Person werden in die Aufmerksamkeits-/Werbemaschine eingespeist, um den Teufelskreis fortzusetzen. Außerdem werden, da KI-Helfer, die von Tech-Unternehmen bereitgestellt werden, zur Schnittstelle für mehr Online-Leben werden, sie wahrscheinlich Suchmaschinen und Feeds als Mechanismus ersetzen, über den Überzeugung und Monetarisierung von Kunden erfolgt. Das Versagen unserer Gesellschaft, diese Dynamiken bisher zu kontrollieren, verheißt nichts Gutes. Ein Teil dieser Dynamik kann über Vorschriften bezüglich Privatsphäre, Datenrechten und Manipulation verringert werden. Mehr an die Wurzel des Problems zu gehen, könnte andere Perspektiven erfordern, wie die loyaler KI-Assistenten (unten diskutiert.)

Die Schlussfolgerung dieser Diskussion ist hoffnungsvoll: innerhalb-der-Tore werkzeugbasierte Systeme – zumindest solange sie in Macht und Fähigkeit vergleichbar mit den heutigen modernsten Systemen bleiben – sind wahrscheinlich handhabbar, wenn der Wille und die Koordination dazu vorhanden sind. Anständige menschliche Institutionen, gestärkt durch KI-Werkzeuge,[^18] können es schaffen. Wir könnten auch dabei scheitern. Aber es ist schwer zu sehen, wie das Zulassen mächtigerer Systeme helfen würde – außer indem man sie in Charge setzt und auf das Beste hofft.

## Nationale Sicherheit

Wettkämpfe um KI-Vorherrschaft – angetrieben von nationaler Sicherheit oder anderen Motivationen – treiben uns zu unkontrollierten mächtigen KI-Systemen, die dazu neigen würden, Macht zu absorbieren, anstatt sie zu verleihen. Ein AGI-Wettlauf zwischen den USA und China ist ein Wettlauf zu bestimmen, welche Nation zuerst Superintelligenz bekommt.

Was sollten also die Verantwortlichen für nationale Sicherheit stattdessen tun? Regierungen haben starke Erfahrung im Aufbau kontrollierbarer und sicherer Systeme, und sie sollten sich darauf in der KI verdoppeln, indem sie die Art von Infrastrukturprojekten unterstützen, die am besten gelingen, wenn sie im großen Maßstab und mit staatlichem Imprimatur durchgeführt werden.

Anstatt eines rücksichtslosen "Manhattan-Projekts" in Richtung AGI[^19] könnte die US-Regierung ein Apollo-Projekt für kontrollierbare, sichere, vertrauenswürdige Systeme starten. Das könnte zum Beispiel umfassen:

- Ein großes Programm zur (a) Entwicklung der On-Chip-Hardware-Sicherheitsmechanismen und (b) der Infrastruktur zur Verwaltung der Rechenleistungsseite mächtiger KI. Diese könnten auf dem US [CHIPS Act](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) und [Exportkontrollregime](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion) aufbauen.
- Eine großangelegte Initiative zur Entwicklung formaler Verifikationstechniken, sodass bestimmte Eigenschaften von KI-Systemen (wie ein Ausschalter) *bewiesen* werden können, dass sie vorhanden oder abwesend sind. Dies kann KI selbst nutzen, um Beweise von Eigenschaften zu entwickeln.
- Eine nationale Anstrengung zur Schaffung von Software, die nachweislich sicher ist, angetrieben von KI-Werkzeugen, die bestehende Software in nachweislich sichere Frameworks umkodieren können.
- Ein nationales Investitionsprojekt im wissenschaftlichen Fortschritt unter Verwendung von KI,[^20] geführt als Partnerschaft zwischen DOE, NSF und NIH.

Im Allgemeinen gibt es eine enorme Angriffsfläche auf unsere Gesellschaft, die uns verwundbar für Risiken von KI und ihrem Missbrauch macht. Der Schutz vor einigen dieser Risiken wird regierungsgroße Investitionen und Standardisierung erfordern. Diese würden weitaus mehr Sicherheit bieten als Benzin ins Feuer von Wettläufen in Richtung AGI zu gießen. Und wenn KI in Waffen und Kommando-und-Kontrollsysteme eingebaut werden soll, ist es entscheidend, dass die KI vertrauenswürdig und sicher ist, was aktuelle KI einfach nicht ist.

## Machtkonzentration und ihre Abmilderung

Dieser Essay hat sich auf die Idee menschlicher Kontrolle über KI und ihr mögliches Scheitern konzentriert. Aber eine andere gültige Linse, durch die man die KI-Situation betrachten kann, ist die der *Machtkonzentration.* Die Entwicklung sehr mächtiger KI droht, Macht entweder in die sehr wenigen und sehr großen Unternehmen zu konzentrieren, die sie entwickelt haben und kontrollieren werden, oder in Regierungen, die KI als neues Mittel nutzen, um ihre eigene Macht und Kontrolle zu erhalten, oder in die KI-Systeme selbst. Oder eine unheilige Mischung aus dem Obigen. In jedem dieser Fälle verliert der Großteil der Menschheit Macht, Kontrolle und Handlungsfähigkeit. Wie könnten wir das bekämpfen?

Der allererste und wichtigste Schritt ist natürlich eine Torschließung zu intelligenter-als-menschlicher AGI und Superintelligenz. Diese können explizit Menschen und Menschengruppen direkt ersetzen. Wenn sie unter Unternehmens- oder Regierungskontrolle stehen, werden sie Macht in diesen Unternehmen oder Regierungen konzentrieren; wenn sie "frei" sind, werden sie Macht in sich selbst konzentrieren. Nehmen wir also an, die Tore sind geschlossen. Was dann?

Eine vorgeschlagene Lösung für Machtkonzentration ist "Open-Source"-KI, wo Modellgewichte frei oder weit verfügbar sind. Aber wie bereits erwähnt, können, sobald ein Modell offen ist, die meisten Sicherheitsmaßnahmen oder Leitplanken (und werden allgemein) entfernt werden. Es gibt also eine akute Spannung zwischen einerseits Dezentralisierung und andererseits Sicherheit, Schutz und menschlicher Kontrolle von KI-Systemen. Es gibt auch Gründe, skeptisch zu sein, dass offene Modelle von sich aus Machtkonzentration in KI sinnvoll bekämpfen werden, mehr als sie es bei Betriebssystemen getan haben (immer noch dominiert von Microsoft, Apple und Google trotz offener Alternativen).[^21]

Dennoch könnte es Wege geben, diesen Kreis zu quadrieren – Risiken zu zentralisieren und abzumildern, während Fähigkeit und wirtschaftliche Belohnung dezentralisiert werden. Dies erfordert ein Überdenken sowohl dessen, wie KI entwickelt wird, als auch wie ihre Vorteile verteilt werden.

Neue Modelle öffentlicher KI-Entwicklung und -Eigentümerschaft würden helfen. Das könnte mehrere Formen annehmen: regierungsentwickelte KI (unter demokratischer Aufsicht),[^22] gemeinnützige KI-Entwicklungsorganisationen (wie Mozilla für Browser) oder Strukturen, die sehr weit verbreitetes Eigentum und Governance ermöglichen. Schlüssel ist, dass diese Institutionen explizit beauftragt wären, dem öffentlichen Interesse zu dienen, während sie unter starken Sicherheitsbeschränkungen operieren.[^23] Wohlgestaltete Regulierungs- und Standards-/Zertifizierungsregime werden ebenfalls vital sein, damit KI-Produkte, die von einem lebendigen Markt angeboten werden, wirklich nützlich bleiben, anstatt gegenüber ihren Nutzern ausbeuterisch zu werden.

In Bezug auf wirtschaftliche Machtkonzentration können wir Herkunftsverfolgung und "Datenwürde" nutzen, um sicherzustellen, dass wirtschaftliche Vorteile weiter fließen. Insbesondere stammt die meiste KI-Macht jetzt (und in der Zukunft, wenn wir die Tore geschlossen halten) aus menschlich generierten Daten, seien es direkte Trainingsdaten oder menschliches Feedback. Wenn KI-Unternehmen verpflichtet wären, Datenanbieter fair zu entschädigen,[^24] könnte dies zumindest helfen, die wirtschaftlichen Belohnungen breiter zu verteilen. Darüber hinaus könnte ein anderes Modell öffentliches Eigentum an bedeutenden Anteilen großer KI-Unternehmen sein. Zum Beispiel könnten Regierungen, die KI-Unternehmen besteuern können, einen Bruchteil der Einnahmen in einen Staatsfonds investieren, der Aktien der Unternehmen hält und Dividenden an die Bevölkerung zahlt.[^25]

Entscheidend bei diesen Mechanismen ist es, die Macht der KI selbst zu nutzen, um Macht besser zu verteilen, anstatt einfach KI-getriebene Machtkonzentration mit Nicht-KI-Mitteln zu bekämpfen. Ein mächtiger Ansatz wäre durch wohlgestaltete KI-Assistenten, die mit echter treuhänderischer Pflicht gegenüber ihren Nutzern operieren – die Interessen der Nutzer an erste Stelle setzen, besonders über die der Unternehmensanbieter.[^26] Diese Assistenten müssen wirklich vertrauenswürdig, technisch kompetent aber angemessen begrenzt basierend auf Anwendungsfall und Risikoniveau und allen über öffentliche, gemeinnützige oder zertifizierte gewinnorientierte Kanäle weit verfügbar sein. Genauso wie wir niemals einen menschlichen Assistenten akzeptieren würden, der heimlich gegen unsere Interessen für eine andere Partei arbeitet, sollten wir keine KI-Assistenten akzeptieren, die ihre Nutzer für Unternehmensvorteile überwachen, manipulieren oder Wert aus ihnen ziehen.

Eine solche Transformation würde die aktuelle Dynamik grundlegend verändern, wo Individuen allein mit riesigen (KI-gestützten) Unternehmens- und Bürokratiemaschinen verhandeln müssen, die Wertextraktion über menschliches Wohlergehen priorisieren. Während es viele mögliche Ansätze gibt, KI-getriebene Macht breiter zu verteilen, wird keiner standardmäßig entstehen: Sie müssen bewusst entwickelt und regiert werden mit Mechanismen wie treuhänderischen Anforderungen, öffentlicher Bereitstellung und gestuftem Zugang basierend auf Risiko.

Ansätze zur Abmilderung von Machtkonzentration können bedeutendem Gegenwind von etablierten Mächten begegnen.[^27] Aber es gibt Wege zur KI-Entwicklung, die nicht erfordern, zwischen Sicherheit und konzentrierter Macht zu wählen. Indem wir jetzt die richtigen Institutionen aufbauen, könnten wir sicherstellen, dass die Vorteile der KI weit geteilt werden, während ihre Risiken sorgfältig gehandhabt werden.

## Neue Governance- und Gesellschaftsstrukturen

Unsere aktuellen Governance-Strukturen kämpfen: Sie reagieren langsam, sind oft von Sonderinteressen gefangen und [werden vom öffentlichen Vertrauen zunehmend schwer belastet.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) Dennoch ist das kein Grund, sie aufzugeben – ganz im Gegenteil. Einige Institutionen mögen Ersetzung brauchen, aber breiter brauchen wir neue Mechanismen, die unsere bestehenden Strukturen verstärken und ergänzen können und ihnen helfen, in unserer sich schnell entwickelnden Welt besser zu funktionieren.

Viel von unserer institutionellen Schwäche entspringt nicht formalen Regierungsstrukturen, sondern degradierten sozialen Institutionen: unseren Systemen zur Entwicklung gemeinsamen Verständnisses, zur Koordinierung von Handlungen und zur Führung sinnvoller Diskurse. Bisher hat KI diese Degradierung beschleunigt, unsere Informationskanäle mit generiertem Inhalt geflutet, uns zu den polarisierendsten und spaltendsten Inhalten hingewiesen und es schwerer gemacht, Wahrheit von Fiktion zu unterscheiden.

Aber KI könnte tatsächlich helfen, diese sozialen Institutionen wieder aufzubauen und zu stärken. Betrachten wir drei entscheidende Bereiche:

Erstens könnte KI dabei helfen, Vertrauen in unsere epistemischen Systeme wiederherzustellen – unsere Wege zu wissen, was wahr ist. Wir könnten KI-gestützte Systeme entwickeln, die die Herkunft von Informationen verfolgen und verifizieren, von Rohdaten durch Analyse zu Schlussfolgerungen. Diese Systeme könnten kryptographische Verifikation mit ausgeklügelter Analyse kombinieren, um Menschen zu helfen zu verstehen, nicht nur ob etwas wahr ist, sondern wie wir wissen, dass es wahr ist.[^28] Loyale KI-Assistenten könnten beauftragt werden, den Details zu folgen, um sicherzustellen, dass sie stimmen.

Zweitens könnte KI neue Formen großmaßstäblicher Koordination ermöglichen. Viele unserer drängendsten Probleme – vom Klimawandel bis zur Antibiotikaresistenz – sind grundlegend Koordinationsprobleme. Wir [stecken in Situationen fest, die schlechter sind, als sie für fast jeden sein könnten](https://equilibriabook.com/), weil sich kein Individuum oder Gruppe den ersten Schritt leisten kann. KI-Systeme könnten helfen, indem sie komplexe Anreizstrukturen modellieren, tragfähige Wege zu besseren Ergebnissen identifizieren und die Vertrauensbildungs- und Verpflichtungsmechanismen erleichtern, die nötig sind, um dorthin zu gelangen.

Vielleicht am faszinierendsten könnte KI völlig neue Formen sozialen Diskurses ermöglichen. Stellen Sie sich vor, "mit einer Stadt sprechen" zu können[^29] – nicht nur Statistiken zu sehen, sondern einen sinnvollen Dialog mit einem KI-System zu führen, das die Ansichten, Erfahrungen, Bedürfnisse und Aspirationen von Millionen von Bewohnern verarbeitet und synthetisiert. Oder betrachten Sie, wie KI echten Dialog zwischen Gruppen erleichtern könnte, die derzeit aneinander vorbeireden, indem sie jeder Seite hilft, die tatsächlichen Anliegen und Werte der anderen besser zu verstehen, anstatt ihre Karikaturen voneinander.[^30] Oder KI könnte geschickte, glaubwürdig neutrale Vermittlung von Streitigkeiten zwischen Menschen oder sogar großen Menschengruppen anbieten (die alle direkt und individuell mit ihr interagieren könnten!) Aktuelle KI ist völlig imstande, diese Arbeit zu tun, aber die Werkzeuge dazu werden nicht von selbst oder über Marktanreize entstehen.

Diese Möglichkeiten mögen utopisch klingen, besonders angesichts der aktuellen Rolle der KI bei der Degradierung von Diskurs und Vertrauen. Aber genau deshalb müssen wir diese positiven Anwendungen aktiv entwickeln. Indem wir die Tore zu unkontrollierbarer AGI schließen und KI priorisieren, die menschliche Handlungsfähigkeit verstärkt, können wir technologischen Fortschritt in Richtung einer Zukunft lenken, wo KI als Kraft für Ermächtigung, Widerstandsfähigkeit und kollektiven Fortschritt dient.

[^1]: Das gesagt, sich von der Drei-Schnittmenge fernzuhalten ist leider nicht so einfach, wie man vielleicht möchte. Fähigkeiten sehr stark in einem der drei Aspekte zu forcieren, neigt dazu, sie in den anderen zu erhöhen. Insbesondere könnte es schwer sein, eine extrem allgemeine und fähige Intelligenz zu schaffen, die nicht leicht autonom gemacht werden kann. Ein Ansatz ist es, Modelle ["myopisch"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) zu trainieren – Systeme mit verkrüppelter Planungsfähigkeit. Ein anderer wäre, sich auf die Entwicklung reiner ["Orakel"](https://arxiv.org/abs/1711.05541)-Systeme zu konzentrieren, die sich scheuen würden, handlungsorientierte Fragen zu beantworten.

[^2]: Viele Unternehmen scheitern daran zu erkennen, dass auch sie schließlich durch AGI ersetzt würden, auch wenn es länger dauert – wenn sie das täten, könnten sie etwas weniger an diesen Toren drücken!

[^3]: KI-Systeme könnten in effizienteren, aber weniger verständlichen Weisen kommunizieren, aber menschliches Verständnis zu bewahren sollte Priorität haben.

[^4]: Diese Idee modularer, interpretierbarer KI wurde von mehreren Forschern detailliert entwickelt; siehe z.B. das ["Comprehensive AI Services"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf)-Modell von Drexler, die ["Open Agency Architecture"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai) von Dalrymple und anderen. Während solche Systeme mehr Entwicklungsaufwand als monolithische neuronale Netzwerke mit massiver Rechenleistung erfordern könnten, ist das genau, wo Rechenleistungslimits helfen – indem sie den sichereren, transparenteren Pfad auch zum praktischeren machen.

[^5]: Zu Sicherheitsnachweisen im Allgemeinen siehe [dieses Handbuch](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). Speziell zu KI siehe [Wasil et al.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [Clymer et al.](https://arxiv.org/abs/2403.10462), [Buhl et al.](https://arxiv.org/abs/2410.21572) und [Balesni et al.](https://arxiv.org/abs/2411.03336).

[^6]: Wir sehen tatsächlich bereits diesen Trend, getrieben nur durch die hohen Kosten der Inferenz: kleinere und mehr spezialisierte Modelle, "destilliert" aus größeren und fähig, auf weniger teurer Hardware zu laufen.

[^7]: Mir ist verständlich, warum die vom KI-Tech-Ökosystem Begeisterten das, was sie als belastende Regulierung ihrer Industrie sehen, ablehnen. Aber es ist ehrlich verwirrend für mich, warum etwa ein Risikokapitalgeber einen Kontrollverlust zu AGI und Superintelligenz zulassen wollen würde. Diese Systeme (und Unternehmen, solange sie unter Unternehmenskontrolle bleiben) werden *alle Startups als Snack fressen*. Wahrscheinlich sogar *früher* als andere Industrien zu fressen. Jeder, der in ein blühendes KI-Ökosystem investiert, sollte priorisieren sicherzustellen, dass AGI-Entwicklung nicht zu Monopolisierung durch wenige dominante Akteure führt.

[^8]: Wie der Ökonom und ehemalige Deepmind-Forscher Michael Webb [es ausdrückte](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/): "Ich denke, wenn wir heute alle Entwicklung größerer Sprachmodelle stoppen würden, also GPT-4 und Claude und was auch immer, und sie sind die letzten Dinge, die wir in dieser Größe trainieren – also erlauben wir viel mehr Iteration bei Dingen dieser Größe und alle Arten von Feinabstimmung, aber nichts Größeres als das, keine größeren Fortschritte – nur was wir heute haben, denke ich, reicht aus, um 20 oder 30 Jahre unglaublichen Wirtschaftswachstums zu antreiben."

[^9]: Zum Beispiel nutzte DeepMinds AlphaFold-System nur ein Hunderttausendstel der FLOP-Zahl von GPT-4.

[^10]: Die Schwierigkeit selbstfahrender Autos ist hier wichtig zu bemerken: Während nominell eine enge Aufgabe und mit fairer Zuverlässigkeit mit relativ kleinen KI-Systemen erreichbar, ist umfangreiches reales Weltwissen und Verständnis notwendig, um Zuverlässigkeit auf das in solch einer sicherheitskritischen Aufgabe benötigte Niveau zu bekommen.

[^11]: Zum Beispiel würden wir bei einem gegebenen Rechenbudget wahrscheinlich Allzweck-KI-Modelle sehen, die bei (sagen wir) der Hälfte dieses Budgets vortrainiert werden, und die andere Hälfte wird verwendet, um sehr hohe Fähigkeit in einem engeren Aufgabenbereich zu trainieren. Das würde übermenschliche enge Fähigkeit geben, unterstützt von nahezu menschlicher allgemeiner Intelligenz.

[^12]: Die derzeit dominante Alignment-Technik ist "Verstärkungslernen durch menschliches Feedback" [(RLHF)](https://arxiv.org/abs/1706.03741) und nutzt menschliches Feedback, um ein Belohnungs-/Bestrafungssignal für das Verstärkungslernen des KI-Modells zu schaffen. Diese und verwandte Techniken wie [konstitutionelle KI](https://arxiv.org/abs/2212.08073) funktionieren überraschend gut (obwohl ihnen Robustheit fehlt und sie mit bescheidener Anstrengung umgangen werden können.) Zusätzlich sind aktuelle Sprachmodelle im Allgemeinen kompetent genug bei gesundem Menschenverstand-Denken, dass sie keine törichten moralischen Fehler machen werden. Das ist etwas von einem Sweet Spot: klug genug, um zu verstehen, was Menschen wollen (soweit es definiert werden kann), aber nicht klug genug, um ausgeklügelte Täuschungen zu planen oder enormen Schaden anzurichten, wenn sie es falsch verstehen.

[^13]: Auf lange Sicht wird jedes Niveau von KI-Fähigkeit, das entwickelt wird, wahrscheinlich proliferieren, da es letztendlich Software ist und nützlich. Wir werden robuste Mechanismen brauchen, um uns gegen die Risiken zu verteidigen, die solche Systeme darstellen. Aber wir *haben das jetzt nicht*, also müssen wir sehr besonnen sein, wie viel mächtige KI-Modelle proliferieren dürfen.

[^14]: Die große Mehrheit davon sind nicht einvernehmliche pornographische Deepfakes, einschließlich von Minderjährigen.

[^15]: Viele Zutaten für solche Lösungen existieren in Form von "Bot-oder-nicht"-Gesetzen (im EU AI Act unter anderen Orten), [industriellen Herkunftsverfolgungstechnologien](https://c2pa.org/), [innovativen Nachrichtenaggregatorren](https://www.improvethenews.org/), Vorhersage-[aggregatoren](https://metaculus.com/) und Märkten, etc.

[^16]: Die Automatisierungswelle folgt möglicherweise nicht früheren Mustern, insofern relativ *hochqualifizierte* Aufgaben wie Qualitätsschreibung, Gesetzesinterpretation oder medizinische Beratung genauso oder sogar verwundbarer für Automatisierung sein können als geringqualifizierte Aufgaben.

[^17]: Für sorgfältige Modellierung des Effekts von AGI auf Löhne siehe den Bericht [hier](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek) und grausige Details [hier](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0) von Anton Korinek und Kollaborateuren. Sie finden, dass, während mehr Teile von Jobs automatisiert werden, Produktivität und Löhne steigen – bis zu einem Punkt. Sobald *zu* viel automatisiert ist, steigt die Produktivität weiter, aber die Löhne stürzen ab, weil Menschen vollständig durch effiziente KI ersetzt werden. Deshalb ist das Schließen der Tore so nützlich: Wir bekommen die Produktivität ohne die verschwundenen menschlichen Löhne.

[^18]: Es gibt viele Wege, wie KI als und zum Aufbau "defensiver" Technologien genutzt werden kann, um Schutz und Management robuster zu machen. Siehe [diesen](https://vitalik.eth.limo/general/2025/01/05/dacc2.html) einflussreichen Post, der diese "D/acc"-Agenda beschreibt.

[^19]: Etwas ironisch würde ein US-Manhattan-Projekt wahrscheinlich wenig tun, um Zeitlinien in Richtung AGI zu beschleunigen – der Zeiger menschlicher und finanzieller Investition in KI-Fortschritt ist bereits bei 11 angepinnt. Die primären Ergebnisse wären, ein ähnliches Projekt in China zu inspirieren (das bei nationalen Infrastrukturprojekten glänzt), internationale Abkommen zur Begrenzung der Risiken von KI viel schwerer zu machen und andere geopolitische Gegner der USA wie Russland zu alarmieren.

[^20]: Das ["National AI Research Resource"](https://nairrpilot.org/)-Programm ist ein guter aktueller Schritt in diese Richtung und sollte erweitert werden.

[^21]: Siehe [diese Analyse](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807) der verschiedenen Bedeutungen und Implikationen von "offen" in Tech-Produkten und wie einige zu mehr, anstatt weniger Verankerung von Dominanz geführt haben.

[^22]: Pläne in den USA für eine [National AI Research Resource](https://nairratdoe.ornl.gov/) und der kürzliche Start einer [europäischen KI-Stiftung](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/) sind interessante Schritte in diese Richtung.

[^23]: Die Herausforderung hier ist nicht technisch, sondern institutionell – wir brauchen dringend reale Beispiele und Experimente darin, wie KI-Entwicklung im öffentlichen Interesse aussehen könnte.

[^24]: Das geht gegen aktuelle Big-Tech-Geschäftsmodelle und würde sowohl rechtliche Schritte als auch neue Normen erfordern.

[^25]: Nur einige Regierungen werden dazu imstande sein. Eine radikalere Idee ist [ein universeller Fonds dieses Typs, unter gemeinsamem Eigentum aller Menschen.](https://futureoflife.org/project/the-windfall-trust/)

[^26]: Für eine ausführliche Darlegung dieses Falls siehe [dieses Paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338) über KI-Loyalität. Leider ist die Standardtrajektorie von KI-Assistenten wahrscheinlich eine, wo sie zunehmend illoyal sind.

[^27]: Etwas ironisch sind viele etablierte Mächte auch dem Risiko KI-gestützter Entmachtung ausgesetzt; aber es könnte schwierig für sie sein, das zu erkennen, bis und außer der Prozess ziemlich weit fortgeschritten ist.

[^28]: Einige interessante Bemühungen in diese Richtung sind repräsentiert durch [die c2pa-Koalition](https://c2pa.org/) zur kryptographischen Verifikation; [Verity](https://www.improvethenews.org/) und [Ground news](https://ground.news/) zu besserer Nachrichten-Epistemik; und [Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com) und Vorhersagemärkte zur Erdung von Diskurs in falsifizierbaren Vorhersagen.

[^29]: Siehe [dieses](https://talktothecity.org/) faszinierende Pilotprojekt.

[^30]: Siehe [Kialo](https://www.kialo-edu.com/) und Bemühungen des [Collective Intelligence Project](https://www.cip.org/) für einige Beispiele.