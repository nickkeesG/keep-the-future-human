# Κεφάλαιο 3 - Βασικές πτυχές του τρόπου κατασκευής των σύγχρονων συστημάτων γενικής τεχνητής νοημοσύνης

Τα πιο προηγμένα συστήματα τεχνητής νοημοσύνης στον κόσμο κατασκευάζονται χρησιμοποιώντας εκπληκτικά παρόμοιες μεθόδους. Ακολουθούν τα βασικά στοιχεία.

Για να κατανοήσει κανείς πραγματικά έναν άνθρωπο χρειάζεται να γνωρίζει κάτι για τη βιολογία, την εξέλιξη, την ανατροφή, και άλλα· για να κατανοήσει την τεχνητή νοημοσύνη πρέπει επίσης να γνωρίζει τον τρόπο κατασκευής της. Τα τελευταία πέντε χρόνια, τα συστήματα τεχνητής νοημοσύνης έχουν εξελιχθεί τεράστια τόσο σε δυνατότητες όσο και σε πολυπλοκότητα. Καθοριστικός παράγοντας αυτής της εξέλιξης ήταν η διαθεσιμότητα πολύ μεγάλων ποσοτήτων υπολογισμών (ή στη γλώσσα του χώρου «υπολογιστική ισχύς» όταν αναφερόμαστε στην τεχνητή νοημοσύνη).

Οι αριθμοί είναι εντυπωσιακοί. Περίπου 10<sup>25</sup>-10<sup>26</sup> «πράξεις κινητής υποδιαστολής» (FLOP)[^1] χρησιμοποιούνται στην εκπαίδευση μοντέλων όπως η σειρά GPT, το Claude, το Gemini, κ.ά.[^2] (Για σύγκριση, εάν κάθε άνθρωπος στη Γη εργαζόταν αδιάκοπα κάνοντας έναν υπολογισμό κάθε πέντε δευτερόλεπτα, θα χρειαζόταν περίπου ένα δισεκατομμύριο χρόνια για να το επιτύχει.) Αυτή η τεράστια ποσότητα υπολογισμών επιτρέπει την εκπαίδευση μοντέλων με έως τρισεκατομμύρια βάρη σε terabytes δεδομένων – ένα μεγάλο μέρος από όλο το ποιοτικό κείμενο που έχει γραφτεί ποτέ μαζί με εκτεταμένες βιβλιοθήκες ήχων, εικόνων και βίντεο. Συμπληρώνοντας αυτή την εκπαίδευση με επιπλέον εκτεταμένη εκπαίδευση που ενισχύει τις ανθρώπινες προτιμήσεις και την καλή εκτέλεση εργασιών, τα μοντέλα που εκπαιδεύονται με αυτόν τον τρόπο επιδεικνύουν απόδοση συγκρίσιμη με τους ανθρώπους σε ένα σημαντικό φάσμα βασικών διανοητικών εργασιών, συμπεριλαμβανομένων της συλλογιστικής και της επίλυσης προβλημάτων.

Γνωρίζουμε επίσης (πολύ, πολύ κατά προσέγγιση) πόση ταχύτητα υπολογισμών, σε πράξεις ανά δευτερόλεπτο, είναι αρκετή ώστε η ταχύτητα *συμπερασμού*[^3] ενός τέτοιου συστήματος να ταιριάζει με την *ταχύτητα* της ανθρώπινης επεξεργασίας κειμένου. Είναι περίπου 10<sup>15</sup>-10<sup>16</sup> FLOP ανά δευτερόλεπτο.[^4]

Παρότι ισχυρά, αυτά τα μοντέλα εξ ορισμού περιορίζονται με βασικούς τρόπους, αρκετά ανάλογα με το πώς θα περιοριζόταν ένας επιμέρους άνθρωπος εάν αναγκαζόταν απλώς να παράγει κείμενο με σταθερό ρυθμό λέξεων ανά λεπτό, χωρίς να σταματά να σκέφτεται ή να χρησιμοποιεί επιπλέον εργαλεία. Τα πιο πρόσφατα συστήματα τεχνητής νοημοσύνης αντιμετωπίζουν αυτούς τους περιορισμούς μέσω μιας πιο πολύπλοκης διαδικασίας και αρχιτεκτονικής που συνδυάζει αρκετά βασικά στοιχεία:

- Ένα ή περισσότερα νευρωνικά δίκτυα, με ένα μοντέλο να παρέχει την κεντρική γνωστική ικανότητα, και έως αρκετά άλλα να εκτελούν άλλες πιο εξειδικευμένες εργασίες·
- *Εργαλεία* που παρέχονται και χρησιμοποιούνται από το μοντέλο – για παράδειγμα ικανότητα αναζήτησης στο διαδίκτυο, δημιουργίας ή επεξεργασίας εγγράφων, εκτέλεσης προγραμμάτων, κ.ά.
- *Υποστηρικτική αρχιτεκτονική* που συνδέει εισόδους και εξόδους των νευρωνικών δικτύων. Μια πολύ απλή υποστηρικτική αρχιτεκτονική μπορεί απλώς να επιτρέπει σε δύο «περιπτώσεις» ενός μοντέλου τεχνητής νοημοσύνης να συνομιλούν μεταξύ τους, ή σε μία να ελέγχει το έργο της άλλης.[^5]
- Οι τεχνικές *αλυσίδας σκέψης* και συναφείς τεχνικές προτροπής κάνουν κάτι παρόμοιο, προκαλώντας ένα μοντέλο να παραγάγει για παράδειγμα πολλές προσεγγίσεις σε ένα πρόβλημα, και στη συνέχεια να επεξεργαστεί αυτές τις προσεγγίσεις για μια συνολική απάντηση.
- *Επανεκπαίδευση* μοντέλων για καλύτερη χρήση εργαλείων, υποστηρικτικής αρχιτεκτονικής, και αλυσίδας σκέψης.

Επειδή αυτές οι επεκτάσεις μπορεί να είναι πολύ ισχυρές (και περιλαμβάνουν τα ίδια τα συστήματα τεχνητής νοημοσύνης), αυτά τα σύνθετα συστήματα μπορεί να είναι αρκετά εξελιγμένα και να ενισχύουν δραματικά τις δυνατότητες της τεχνητής νοημοσύνης.[^6] Και πρόσφατα, τεχνικές στην υποστηρικτική αρχιτεκτονική και ειδικά στην προτροπή αλυσίδας σκέψης (και ενσωμάτωση των αποτελεσμάτων πίσω στην επανεκπαίδευση μοντέλων για καλύτερη χρήση αυτών) έχουν αναπτυχθεί και χρησιμοποιηθεί στο [o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/), και [DeepSeek R1](https://api-docs.deepseek.com/news/news250120) για να πραγματοποιούν πολλές περάσεις συμπερασμού ως απάντηση σε ένα δεδομένο ερώτημα.[^7] Αυτό στην ουσία επιτρέπει στο μοντέλο να «σκεφτεί» την απάντησή του και ενισχύει δραματικά την ικανότητα αυτών των μοντέλων να κάνουν υψηλού επιπέδου συλλογισμούς σε εργασίες επιστήμης, μαθηματικών και προγραμματισμού.[^8]

Για μια δεδομένη αρχιτεκτονική τεχνητής νοημοσύνης, οι αυξήσεις στην υπολογιστική ισχύς εκπαίδευσης [μπορούν να μετατραπούν αξιόπιστα](https://arxiv.org/abs/2405.10938) σε βελτιώσεις σε ένα σύνολο σαφώς καθορισμένων μετρικών. Για λιγότερο σαφώς καθορισμένες γενικές δυνατότητες (όπως αυτές που συζητούνται παρακάτω), η μετατροπή είναι λιγότερο σαφής και προβλέψιμη, αλλά είναι σχεδόν βέβαιο ότι μεγαλύτερα μοντέλα με περισσότερη υπολογιστική ισχύ εκπαίδευσης θα έχουν νέες και καλύτερες δυνατότητες, ακόμη και αν είναι δύσκολο να προβλέψουμε ποιες θα είναι αυτές.

Παρομοίως, τα σύνθετα συστήματα και ειδικά οι εξελίξεις στην «αλυσίδα σκέψης» (και την εκπαίδευση μοντέλων που λειτουργούν καλά με αυτήν) έχουν ξεκλειδώσει την κλιμάκωση στην υπολογιστική ισχύ *συμπερασμού*: για ένα δεδομένο εκπαιδευμένο κεντρικό μοντέλο, τουλάχιστον ορισμένες δυνατότητες συστημάτων τεχνητής νοημοσύνης αυξάνονται καθώς εφαρμόζεται περισσότερη υπολογιστική ισχύς που τους επιτρέπει να «σκέφτονται πιο σκληρά και πιο πολύ» για πολύπλοκα προβλήματα. Αυτό έχει το κόστος μεγάλης ταχύτητας υπολογισμών, απαιτώντας εκατοντάδες ή χιλιάδες περισσότερα FLOP/s για να ταιριάξει την ανθρώπινη απόδοση.[^9]

Ενώ αποτελεί μόνο μέρος αυτού που οδηγεί στη ταχεία πρόοδο της τεχνητής νοημοσύνης,[^10] ο ρόλος της υπολογιστικής ισχύος και η δυνατότητα σύνθετων συστημάτων θα αποδειχθούν καθοριστικοί τόσο για την πρόληψη μη ελέγξιμης ΤΓΝ όσο και για την ανάπτυξη ασφαλέστερων εναλλακτικών.

[^1]: 10<sup>27</sup> σημαίνει 1 ακολουθούμενο από 25 μηδενικά, ή δέκα τρισεκατομμύρια τρισεκατομμύρια. Ένα FLOP είναι απλώς μια αριθμητική πρόσθεση ή πολλαπλασιασμός αριθμών με κάποια ακρίβεια. Σημειώστε ότι η απόδοση του υλικού τεχνητής νοημοσύνης μπορεί να διαφέρει κατά παράγοντα δέκα ανάλογα με την ακρίβεια της αριθμητικής και την αρχιτεκτονική του υπολογιστή. Η καταμέτρηση λογικών πράξεων πυλών (ANDs, ORS, AND NOTs) θα ήταν θεμελιώδης αλλά αυτές δεν είναι κοινώς διαθέσιμες ή αξιολογημένες· για τους παρόντες σκοπούς είναι χρήσιμο να τυποποιήσουμε σε πράξεις 16-bit (FP16), αν και θα πρέπει να καθοριστούν κατάλληλοι συντελεστές μετατροπής.

[^2]: Μια συλλογή εκτιμήσεων και σκληρών δεδομένων είναι διαθέσιμη από την [Epoch AI](https://epochai.org/data/large-scale-ai-models) και δείχνει περίπου 2×10<sup>25</sup> 16-bit FLOP για το GPT-4· αυτό ταιριάζει περίπου με [αριθμούς που διέρρευσαν](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) για το GPT-4. Οι εκτιμήσεις για άλλα μοντέλα μέσων του 2024 είναι όλες εντός παράγοντα λίγων του GPT-4.

[^3]: Ο συμπερασμός είναι απλώς η διαδικασία παραγωγής μιας εξόδου από ένα νευρωνικό δίκτυο. Η εκπαίδευση μπορεί να θεωρηθεί ως διαδοχή πολλών συμπερασμών και προσαρμογών βαρών μοντέλου.

[^4]: Για παραγωγή κειμένου, το αρχικό GPT-4 απαιτούσε 560 TFLOP ανά token που παραγόταν. Περίπου 7 tokens/s χρειάζονται για να συμβαδίσουν με την ανθρώπινη σκέψη, άρα αυτό δίνει ≈3×10<sup>15</sup> FLOP/s. Αλλά οι βελτιώσεις στην αποδοτικότητα έχουν μειώσει αυτό· [αυτό το φυλλάδιο της NVIDIA](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/) για παράδειγμα δείχνει μόλις 3×10<sup>14</sup> FLOP/s για ένα συγκρίσιμης απόδοσης μοντέλο Llama 405B.

[^5]: Ως ένα ελαφρώς πιο πολύπλοκο παράδειγμα, ένα σύστημα τεχνητής νοημοσύνης θα μπορούσε πρώτα να παραγάγει αρκετές πιθανές λύσεις σε ένα μαθηματικό πρόβλημα, έπειτα να χρησιμοποιήσει μια άλλη περίπτωση για να ελέγξει κάθε λύση, και τέλος να χρησιμοποιήσει μια τρίτη για να συνθέσει τα αποτελέσματα σε μια σαφή εξήγηση. Αυτό επιτρέπει πιο διεξοδική και αξιόπιστη επίλυση προβλημάτων από μια μονή πέρασμα.

[^6]: Δείτε για παράδειγμα λεπτομέρειες για τον [«Operator» της OpenAI](https://openai.com/index/introducing-operator/), τις [δυνατότητες εργαλείων του Claude](https://docs.anthropic.com/en/docs/build-with-claude/computer-use), και το [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). Το [Deep Research](https://openai.com/index/introducing-deep-research/) της OpenAI πιθανότατα έχει μια αρκετά εξελιγμένη αρχιτεκτονική αλλά οι λεπτομέρειες δεν είναι διαθέσιμες.

[^7]: Το Deepseek R1 βασίζεται στην επαναληπτική εκπαίδευση και προτροπή του μοντέλου έτσι ώστε το τελικό εκπαιδευμένο μοντέλο να δημιουργεί εκτεταμένη συλλογιστική αλυσίδας σκέψης. Αρχιτεκτονικές λεπτομέρειες δεν είναι διαθέσιμες για το o1 ή o3, ωστόσο το Deepseek έχει αποκαλύψει ότι δεν υπάρχει ιδιαίτερη «μαγική συνταγή» που απαιτείται για να ξεκλειδώσει την κλιμάκωση δυνατοτήτων με συμπερασμό. Αλλά παρά το ότι έλαβε πολλή προσοχή από τα μέσα ως ανατροπή του «status quo» στην τεχνητή νοημοσύνη, δεν επηρεάζει τους κεντρικούς ισχυρισμούς αυτού του δοκιμίου.

[^8]: Αυτά τα μοντέλα ξεπερνούν σημαντικά τα τυπικά μοντέλα σε benchmarks συλλογιστικής. Για παράδειγμα, στο GPQA Diamond Benchmark—μια αυστηρή δοκιμασία ερωτήσεων επιστήμης σε επίπεδο διδακτορικού—το GPT-4o [σκόραρε](https://openai.com/index/learning-to-reason-with-llms/) 56%, ενώ τα o1 και o3 πέτυχαν 78% και 88%, αντίστοιχα, ξεπερνώντας κατά πολύ τον μέσο όρο 70% των ανθρώπων ειδικών.

[^9]: Το O3 της OpenAI πιθανότατα δαπάνησε ∼10<sup>21</sup>-10<sup>22</sup> FLOP [για να ολοκληρώσει κάθε μια από τις ερωτήσεις της πρόκλησης ARC-AGI](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai), τις οποίες ικανοί άνθρωποι μπορούν να κάνουν σε (ας πούμε) 10-100 δευτερόλεπτα, δίνοντας έναν αριθμό πιο κοντά στα ∼10<sup>20</sup> FLOP/s.

[^10]: Ενώ η υπολογιστική ισχύς είναι βασικό μέτρο της ικανότητας συστημάτων τεχνητής νοημοσύνης, αλληλεπιδρά τόσο με την ποιότητα δεδομένων όσο και με τις αλγοριθμικές βελτιώσεις. Καλύτερα δεδομένα ή αλγόριθμοι μπορούν να μειώσουν τις υπολογιστικές απαιτήσεις, ενώ περισσότερη υπολογιστική ισχύς μπορεί μερικές φορές να αντισταθμίσει αδύναμα δεδομένα ή αλγορίθμους.