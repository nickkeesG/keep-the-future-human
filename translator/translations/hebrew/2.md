# פרק 2 - דברים חיוניים על רשתות נוירונים של AI

איך מערכות AI מודרניות פועלות, ומה עשוי להגיע בדור הבא של מערכות AI?

כדי להבין איך יתפתחו ההשלכות של פיתוח AI חזק יותר, חיוני להפנים כמה יסודות. הקטע הזה ושני הקטעים הבאים מפתחים את אלה, ומכסים בתורם מהו AI מודרני, איך הוא מנצל חישובים עצומים, ובאילו מובנים הוא גדל במהירות בכלליות וביכולת.[^1]

ישנן דרכים רבות להגדיר בינה מלאכותית, אבל לצרכינו התכונה המרכזית של AI היא שבעוד תוכנית מחשב רגילה היא רשימה של הוראות איך לבצע משימה, מערכת AI היא כזאת שלומדת מנתונים או מניסיון לבצע משימות *מבלי שאומרים לה במפורש איך לעשות זאת.*

כמעט כל AI מודרני בולט מבוסס על רשתות נוירונים. אלה הן מבנים מתמטיים/חישוביים, המיוצגים על ידי קבוצה גדולה מאוד (מיליארדים או טריליונים) של מספרים ("משקולות"), שמבצעים משימת אימון היטב. המשקולות הללו מעוצבות (או אולי "גדלות" או "נמצאות") על ידי התאמה חוזרת ונשנית שלהן כך שהרשת הנוירונים משפרת ציון מספרי (המכונה גם "אובדן") שהוגדר כדי לבצע היטב משימה אחת או יותר.[^2] התהליך הזה נקרא *אימון* הרשת הנוירונים.[^3]

ישנן טכניקות רבות לביצוע האימון הזה, אבל הפרטים הללו הרבה פחות רלוונטיים מהדרכים בהן הניקוד מוגדר, ואיך אלה מביאות למשימות שונות שהרשת הנוירונים מבצעת היטב. הבחנה מרכזית נעשתה היסטורית בין AI "צר" ו"כללי".

AI צר מאומן במכוון לעשות משימה מסוימת או קבוצה קטנה של משימות (כמו זיהוי תמונות או משחק שח); הוא דורש אימון מחדש למשימות חדשות, ויש לו טווח יכולת צר. יש לנו AI צר על-אנושי, כלומר כמעט לכל משימה דיסקרטית מוגדרת היטב שאדם יכול לעשות, אנחנו כנראה יכולים לבנות ציון ואז לאמן בהצלחה מערכת AI צרה לעשות אותה טוב יותר ממה שאדם יכול.

מערכות AI כלליות (GPAI) יכולות לבצע מגוון רחב של משימות, כולל רבות שלא אומנו עליהן במפורש; הן גם יכולות ללמוד משימות חדשות כחלק מהפעולה שלהן. "מודלים רב-תחומיים" גדולים נוכחיים[^4] כמו ChatGPT מדגימים זאת: מאומנים על קורפוס גדול מאוד של טקסט ותמונות, הם יכולים לעסוק בהיגיון מורכב, לכתוב קוד, לנתח תמונות, ולסייע עם מערך עצום של משימות אינטלקטואליות. למרות שהם עדיין שונים למדי מאינטליגנציה אנושית בדרכים שנראה לעומק למטה, הכלליות שלהם גרמה למהפכה ב-AI.[^5]

## חוסר יכולת חיזוי: תכונה מרכזית של מערכות AI

הבדל מרכזי בין מערכות AI ותוכנה רגילה הוא ביכולת החיזוי. הפלט של תוכנה סטנדרטית יכול להיות בלתי צפוי – ואכן לפעמים זה למה אנחנו כותבים תוכנה, כדי לתת לנו תוצאות שלא יכולנו לחזות. אבל תוכנה רגילה ממעט לעשות דבר שלא תוכנתה לעשות – הטווח וההתנהגות שלה בדרך כלל כמו שתוכננה. תוכנת שח מהשורה הראשונה עשויה לעשות מהלכים שאף אדם לא יכול לחזות (אחרת הם יכלו להביס את תוכנת השח הזאת!) אבל היא בדרך כלל לא תעשה דבר מלבד לשחק שח.

כמו תוכנה רגילה, ל-AI צר יש טווח והתנהגות צפויים אבל יכולות להיות לו תוצאות בלתי צפויות. זו באמת רק דרך נוספת להגדיר AI צר: כ-AI שדומה לתוכנה רגילה ביכולת החיזוי ובטווח הפעולה שלו.

AI כללי שונה: הטווח שלו (התחומים שעליהם הוא חל), ההתנהגות (סוגי הדברים שהוא עושה), והתוצאות (הפלטים בפועל שלו) יכולים כולם להיות בלתי צפויים.[^6] GPT-4 אומן רק ליצור טקסט במדויק, אבל פיתח יכולות רבות שהמאמנים שלו לא חזו או התכוונו אליהן. חוסר יכולת החיזוי הזה נובע מהמורכבות של האימון: מכיוון שנתוני האימון מכילים פלטים ממשימות שונות רבות, ה-AI חייב ללמוד ביעילות לבצע את המשימות הללו כדי לחזות היטב.

חוסר יכולת החיזוי הזה של מערכות AI כלליות הוא די בסיסי. למרות שבעקרון אפשר לבנות בזהירות מערכות AI שיש להן מגבלות מובטחות על ההתנהגות שלהן (כפי שמוזכר מאוחר יותר במאמר), הדרך שבה מערכות AI נוצרות כעת הן בלתי צפויות בפועל ואפילו בעקרון.

## AI פסיבי, סוכנים, מערכות אוטונומיות, והתאמה

חוסר יכולת החיזוי הזה הופך חשוב במיוחד כשאנחנו שוקלים איך מערכות AI אכן נפרסות ומשמשות להשגת מטרות שונות.

מערכות AI רבות יחסית פסיביות במובן שהן בעיקר מספקות מידע, והמשתמש נוקט בפעולות. אחרות, המכונות בדרך כלל *סוכנים*, נוקטות בפעולות בעצמן, עם רמות מעורבות משתנות מהמשתמש. אלה שנוקטות בפעולות עם קלט או פיקוח חיצוני פחות יחסית עשויות להיחשב יותר *אוטונומיות*. זה יוצר ספקטרום מבחינת עצמאות פעולה, מכלים פסיביים לסוכנים אוטונומיים.[^7]

לגבי מטרות של מערכות AI, אלה עשויות להיות קשורות ישירות ליעד האימון שלהן (למשל המטרה של "ניצחון" למערכת משחקת גו היא גם מפורשות מה שהיא אומנה לעשות). או שהן עשויות לא להיות: יעד האימון של ChatGPT הוא בחלקו לחזות טקסט, בחלקו להיות עוזר מועיל. אבל כשהוא עושה משימה נתונה, המטרה שלו מסופקת לו על ידי המשתמש. מטרות עשויות גם להיווצר על ידי מערכת AI בעצמה, רק בקשר עקיף מאוד ליעד האימון שלה.[^8]

מטרות קשורות קשר הדוק לשאלה של "התאמה", כלומר השאלה האם מערכות AI *יעשו מה שאנחנו רוצים שהן תעשנה*. השאלה הפשוטה הזאת מסתירה רמה עצומה של עדינות.[^9] בינתיים, שימו לב ש"אנחנו" במשפט הזה עשוי להתייחס לאנשים וקבוצות שונות רבות, מה שמוביל לסוגים שונים של התאמה. לדוגמה, AI עשוי להיות מאוד *צייתן* (או ["נאמן"](https://arxiv.org/abs/2003.11157)) למשתמש שלו – כאן "אנחנו" זה "כל אחד מאיתנו". או שהוא עשוי להיות יותר *ריבוני*, להיות מונע בעיקר מהמטרות והמגבלות שלו עצמו, אבל עדיין לפעול בגדול לטובת הרווחה האנושית הכללית – "אנחנו" זה אז "האנושות" או "החברה". באמצע יש ספקטרום שבו AI יהיה ברובו צייתן, אבל עשוי לסרב לנקוט בפעולות שמזיקות לאחרים או לחברה, מפרות את החוק, וכו'.

שני הצירים הללו – רמת אוטונומיה וסוג התאמה – אינם עצמאיים לחלוטין. לדוגמה, מערכת פסיבית ריבונית, למרות שאינה בדיוק סתירה עצמית, היא מושג במתח, כמו גם סוכן אוטונומי צייתן.[^10] יש הבנה ברורה שאוטונומיה וריבונות נוטות ללכת יד ביד. באופן דומה, יכולת החיזוי נוטה להיות גבוהה יותר במערכות AI "פסיביות" ו"צייתניות", בעוד שריבוניות או אוטונומיות ינטו להיות יותר בלתי צפויות. כל זה יהיה חיוני להבנת ההשלכות של AGI ועל-אינטליגנציה פוטנציאליים.

יצירת AI מותאם באמת, מכל סוג שהוא, דורשת פתרון שלושה אתגרים נפרדים:

1. הבנה של מה ש"אנחנו" רוצים – שזה מורכב בין אם "אנחנו" אומר אדם או ארגון ספציפי (נאמנות) או האנושות בגדול (ריבונות);
2. בנייה של מערכות שפועלות בקביעות בהתאם לרצונות הללו – בעצם יצירה של התנהגות חיובית עקבית;
3. בצורה הבסיסית ביותר, יצירה של מערכות שבאמת "אכפת" להן מהרצונות הללו במקום רק להתנהג כאילו כן.

ההבחנה בין התנהגות אמינה ואכפתיות אמיתית חיונית. בדיוק כמו עובד אנושי שעשוי לקיים הוראות בצורה מושלמת תוך חוסר מחויבות אמיתית למשימה של הארגון, מערכת AI עשויה לפעול בהתאמה מבלי לחשוב באמת על העדפות אנושיות. אנחנו יכולים לאמן מערכות AI לומר ולעשות דברים דרך משוב, והן יכולות ללמוד להסיק על מה שבני אדם רוצים. אבל לגרום להן *לחשוב באמת* על העדפות אנושיות זה אתגר הרבה יותר עמוק.[^11]

הקשיים העמוקים בפתרון אתגרי ההתאמה הללו, וההשלכות שלהם לסיכון AI, ייחקרו עוד למטה. בינתיים, הבינו שהתאמה היא לא רק תכונה טכנית שאנחנו מוסיפים למערכות AI, אלא היבט בסיסי של הארכיטקטורה שלהן שמעצב את הקשר שלהן עם האנושות.

[^1]: למבוא עדין אך טכני למכונת למידה ו-AI, במיוחד מודלי שפה, ראו [אתר זה.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) למדריך מודרני נוסף לסיכוני הכחדה של AI, ראו [יצירה זו.](https://www.thecompendium.ai/) לניתוח מדעי מקיף ומוסמך של מצב בטיחות AI, ראו את [הדוח הבינלאומי לבטיחות AI](https://arxiv.org/abs/2501.17805) האחרון.

[^2]: האימון בדרך כלל מתרחש על ידי חיפוש אחר מקסימום מקומי של הציון במרחב רב-ממדי הנתון על ידי משקולות המודל. על ידי בדיקה איך הציון משתנה כשמשקולות מותאמות, אלגוריתם האימון מזהה אילו התאמות משפרות את הציון הכי הרבה, ומזיז את המשקולות לכיוון הזה.

[^3]: לדוגמה, בבעיית זיהוי תמונות, הרשת הנוירונים תוציא הסתברויות לתוויות לתמונה. ציון יהיה קשור להסתברות שה-AI מייחס לתשובה הנכונה. הליך האימון יתאים אז משקולות כך שבפעם הבאה, ה-AI יוציא הסתברות גבוהה יותר לתווית הנכונה לאותה תמונה. זה אז חוזר על עצמו מספר עצום של פעמים. אותו הליך בסיסי משמש לאימון בעצם כל הרשתות הנוירוניות המודרניות, אמנם עם מנגנון ניקוד מורכב יותר.

[^4]: רוב המודלים הרב-תחומיים משתמשים בארכיטקטורת "הטרנספורמר" לעיבוד ויצירה של סוגי נתונים מרובים (טקסט, תמונות, קול). אלה כולם יכולים להתפרק ל, ואז להיחשב על אותה הבסיס, כסוגים שונים של "טוקנים". מודלים רב-תחומיים מאומנים תחילה לחזות טוקנים במדויק בתוך מאגרי נתונים עצומים, ואז משופרים דרך למידת חיזוק כדי לשפר יכולות ולעצב התנהגויות.

[^5]: שמודלי שפה מאומנים לעשות דבר אחד – לחזות מילים – גרם לכמה לקרוא להם AI צר. אבל זה מטעה: מכיוון שחיזוי טקסט היטב דורש כל כך הרבה יכולות שונות, משימת האימון הזאת מובילה למערכת כללית באופן מפתיע. שימו לב גם שהמערכות הללו מאומנות רבות על ידי למידת חיזוק, המייצגות ביעילות אלפי אנשים שנותנים למודל אות תגמול כשהוא עושה עבודה טובה בכל אחד מהדברים הרבים שהוא עושה. הוא אז יורש כלליות משמעותית מהאנשים שנותנים את המשוב הזה.

[^6]: ישנן דרכים מרובות שבהן AI בלתי צפוי. אחת היא שבמקרה הכללי אי אפשר לחזות מה אלגוריתם יעשה מבלי באמת להריץ אותו; יש [משפטים](https://arxiv.org/abs/1310.3225) לכך. זה יכול להיות נכון רק כי הפלט של אלגוריתמים יכול להיות מורכב. אבל זה ברור ורלוונטי במיוחד במקרה (כמו בשח או גו) שבו החיזוי ירמז על יכולת (להביס את ה-AI) שאין למי שרוצה לחזות. שנית, מערכת AI נתונה לא תמיד תפיק את אותו הפלט אפילו בהינתן אותו קלט – הפלטים שלה מכילים אקראיות; זה גם מתחבר עם חוסר יכולת חיזוי אלגוריתמית. שלישית, יכולות לא צפויות ומתהוות יכולות לצוץ מאימון, כלומר אפילו *סוגי* הדברים שמערכת AI יכולה ותעשה הם בלתי צפויים; הסוג האחרון הזה חשוב במיוחד לשיקולי בטיחות.

[^7]: ראו [כאן](https://arxiv.org/abs/2502.02649) לסקירה מעמיקה של מה שמתכוון "סוכן אוטונומי" (יחד עם טיעונים אתיים נגד בנייתם).

[^8]: אתם עשויים לשמוע לפעמים "AI לא יכול להחזיק מטרות משלו". זה שטויות מוחלטות. קל ליצור דוגמאות שבהן ל-AI יש או מפתח מטרות שמעולם לא ניתנו לו והן ידועות רק לו עצמו. אתם לא רואים את זה הרבה במודלים רב-תחומיים פופולריים נוכחיים כי זה מאומן להיות מחוץ להם; זה יכול באותה קלות להיות מאומן לתוכם.

[^9]: יש ספרות גדולה. על הבעיה הכללית ראו של כריסטיאן [*The Alignment Problem*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821), ושל ראסל [*Human-Compatible*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616). בצד יותר טכני ראו למשל [מאמר זה](https://arxiv.org/abs/2209.00626).

[^10]: נראה מאוחר יותר שבעוד מערכות כאלה לא עוקבות אחר המגמה, זה בעצם הופך אותן למעניינות ושימושיות מאוד.

[^11]: זה לא אומר שאנחנו דורשים רגשות או תחושה. במקום זאת, זה קשה מאוד מחוץ למערכת לדעת מהם המטרות, ההעדפות, והערכים הפנימיים שלה. "אמיתי" כאן היה אומר שיש לנו סיבה חזקה מספיק לסמוך על זה שבמקרה של מערכות קריטיות אנחנו יכולים להמר את החיים שלנו עליהם.