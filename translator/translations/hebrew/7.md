# פרק 7 - מה יקרה אם נבנה בינה מלאכותית כללית במסלול הנוכחי?

החברה אינה מוכנה למערכות ברמת בינה מלאכותית כללית. אם נבנה אותן בקרוב מאוד, הדברים עלולים להתדרדר קשות.

פיתוח בינה מלאכותית כללית מלאה – מה שנכנה כאן AI שנמצא "מחוץ לשערים" – יהיה שינוי יסודי בטבע העולם: מעצם טבעו זה אומר הוספת מין חדש של אינטליגנציה לכדור הארץ עם יכולות גבוהות יותר מאלה של בני האדם.

מה שיקרה אז תלוי בדברים רבים, כולל טבע הטכנולוגיה, בחירות של אלה שמפתחים אותה, והקשר העולמי שבו היא מפותחת.

כרגע, בינה מלאכותית כללית מלאה מפותחת על ידי קומץ חברות פרטיות ענקיות במירוץ זו מול זו, עם מעט רגולציה משמעותית או פיקוח חיצוני,[^1] בחברה עם מוסדות ליבה חלשים יותר ויותר ואפילו לא תפקודיים,[^2] בתקופה של מתיחות גיאופוליטית גבוהה ותיאום בינלאומי נמוך. אף שחלק מהמעורבים מונעים ממניעים אלטרואיסטיים, רבים מאלה שעושים זאת מונעים מכסף, או כוח, או שניהם.

חיזוי קשה מאוד, אבל יש כמה דינמיקות שמובנות מספיק טוב, ואנלוגיות מתאימות מספיק עם טכנולוגיות קודמות כדי לתת הדרכה. ולמרבה הצער, למרות ההבטחה של AI, הן נותנות סיבה טובה להיות פסימיים עמוקות לגבי איך המסלול הנוכחי שלנו יתפתח.

כדי לנסח זאת בבוטות, במסלול הנוכחי שלנו פיתוח בינה מלאכותית כללית יהיה לו כמה השפעות חיוביות (ויעשה כמה אנשים עשירים מאוד מאוד). אבל טבע הטכנולוגיה, הדינמיקות הבסיסיות, והקשר שבו היא מפותחת, מצביעים בחוזקה על כך ש: AI חזק יערער באופן דרמטי את החברה והציוויליזציה שלנו; נאבד שליטה עליו; אנחנו עלולים בהחלט להגיע למלחמת עולם בגללו; נאבד (או נוותר על) שליטה *בפניו*; זה יוביל לעל-אינטליגנציה מלאכותית, שבהחלט לא נשלט בה וזה יתפשר על סופו של עולם המנוהל על ידי בני אדם.

אלו טענות חזקות, והלוואי שהן היו ספקולציה סרק או "דומריזם" לא מוצדק. אבל לכאן המדע, תורת המשחקים, תורת האבולוציה וההיסטוריה כולם מצביעים. חלק זה מפתח את הטענות האלה, ואת התמיכה שלהן, בפירוט.

## נערער את החברה והציוויליזציה שלנו

למרות מה שאתם עלולים לשמוע בחדרי ישיבות של עמק הסיליקון, רוב ההפרעות – במיוחד מהסוג המהיר מאוד – אינן מועילות. יש הרבה יותר דרכים לגרום למערכות מורכבות להיות גרועות יותר מאשר טובות יותר. העולם שלנו פועל טוב כפי שהוא פועל כי בנינו בקפידה תהליכים, טכנולוגיות ומוסדות שהפכו אותו לטוב יותר בהתמדה.[^3] לקיחת פטיש כבד למפעל רק לעתים נדירות משפרת פעולות.

הנה קטלוג (לא מושלם) של דרכים שבהן מערכות בינה מלאכותית כללית יפריעו לציוויליזציה שלנו.

- הן יפריעו באופן דרמטי לעבודה, ויובילו *לכל הפחות* לאי שוויון הכנסות דרמטי יותר ופוטנציאלית לתת-תעסוקה או אבטלה רחבת היקף, בלוח זמנים קצר מדי מכדי שהחברה תוכל להסתגל.[^4]
- הן יובילו כנראה לריכוז כוח כלכלי, חברתי ופוליטי עצום – פוטנציאלית יותר מזה של מדינות – במספר קטן של אינטרסים פרטיים ענקיים שלא אחראים לציבור.
- הן יכולות לפתע להפוך פעילויות שהיו קודם קשות או יקרות לקלות טריוויאלית, ולערער מערכות חברתיות שתלויות בכך שפעילויות מסוימות נשארות יקרות או דורשות מאמץ אנושי משמעותי.[^5]
- הן יכולות להציף את מערכות איסוף המידע, עיבוד והתקשורת של החברה עם מדיה ריאליסטית לחלוטין אך שקרית, זבלית, ממוקדת יתר על המידה או מניפולטיבית עד כדי כך שיהיה בלתי אפשרי לדעת מה פיזית אמיתי או לא, אנושי או לא, עובדתי או לא, ומהימן או לא.[^6]
- הן יכולות ליצור תלות אינטלקטואלית מסוכנת וכמעט מוחלטת, שבה הבנה אנושית של מערכות וטכנולוגיות מפתח מתנוונת כשאנחנו מסתמכים יותר ויותר על מערכות AI שאיננו יכולים להבין לחלוטין.
- הן יכולות לסיים למעשה את התרבות האנושית, ברגע שכמעט כל האובייקטים התרבותיים (טקסט, מוזיקה, אמנות חזותית, קולנוע וכו') הנצרכים על ידי רוב האנשים נוצרים, מתווכים או נאצרים על ידי מוחות לא אנושיים.
- הן יכולות לאפשר מערכות מעקב ומניפולציה המוני יעילות השמישות על ידי ממשלות או אינטרסים פרטיים כדי לשלוט באוכלוסייה ולרדוף מטרות הסותרות את האינטרס הציבורי.
- על ידי ערעור השיח האנושי, דיון ומערכות בחירות, הן יכולות לצמצם את האמינות של מוסדות דמוקרטיים עד לנקודה שבה הם מוחלפים בפועל (או במפורש) באחרים, לסיים את הדמוקרטיה במדינות שבהן היא קיימת כרגע.
- הן יכולות להפוך להיות, או ליצור, וירוסי תוכנה אינטליגנטיים מתרבים עצמית ותולעים מתקדמות שיכולות להתפשט ולהתפתח, לערער באופן מסיבי מערכות מידע עולמיות.
- הן יכולות להגביר באופן דרמטי את היכולת של טרוריסטים, שחקנים רעים ומדינות סוררות לגרום נזק באמצעות נשק ביולוגי, כימי, סייבר, אוטונומי או אחר, בלי ש-AI יספק יכולת מאזנת למנוע נזק כזה. באופן דומה הן יערערו ביטחון לאומי ואיזונים גיאופוליטיים על ידי הפיכת מומחיות ברמה עליונה בנושאים גרעיניים, ביולוגיים, הנדסיים ואחרים לזמינה למשטרים שלא היו אחרת בעלי אותה.
- הן יכולות לגרום להיפר-קפיטליזם בהתחמקות רחב היקף מהיר, עם חברות המנוהלות למעשה על ידי AI המתחרות במרחבים פיננסיים, מכירות ושירותים אלקטרוניים ברובם. שווקים פיננסיים מונעי AI יכולים לפעול במהירויות ומורכבויות הרבה מעבר להבנה או שליטה אנושית. כל מודלי הכישלון והחיצוניות השליליות של הכלכלות הקפיטליסטיות הנוכחיות יכולים להחמיר ולהזדרז הרבה מעבר לשליטה, ממשל או יכולת רגולטורית אנושית.
- הן יכולות לתדלק מירוץ חימוש בין מדינות בנשק מופעל AI, מערכות פיקוד ושליטה, נשק סייבר וכו', ליצור הצטברות מהירה מאוד של יכולות הרסניות ביותר.

הסיכונים האלה אינם ספקולטיביים. רבים מהם מתממשים בזמן שאנחנו מדברים, באמצעות מערכות AI קיימות! אבל תחשבו, *באמת* תחשבו, איך כל אחד מהם ייראה עם AI חזק הרבה יותר.

תחשבו על עקירת עובדים כשרוב העובדים פשוט לא יכולים לספק שום ערך כלכלי משמעותי מעבר למה ש-AI יכול, בתחום המומחיות או הניסיון שלהם – או אפילו אם הם יעברו הכשרה מחדש! תחשבו על מעקב המוני אם כולם נצפים ונמקחים באופן אישי על ידי משהו מהיר וחכם יותר מהם. איך נראית דמוקרטיה כשאיננו יכולים לסמוך באופן מהימן על שום מידע דיגיטלי שאנחנו רואים, שומעים או קוראים, וכשהקולות הציבוריים המשכנעים ביותר אינם אפילו אנושיים, ואין להם חלק בתוצאה? מה נעשה למלחמה כשגנרלים צריכים להקשיב כל הזמן ל-AI (או פשוט לשים אותו בראש), כדי לא להעניק יתרון מכריע לאויב? כל אחד מהסיכונים לעיל מהווה אסון לציוויליזציה האנושית[^7] אם הוא מתממש במלואו.

אתם יכולים לעשות את החיזויים שלכם. תשאלו את עצמכם את שלושת השאלות האלה עבור כל סיכון:

1. האם AI בעל יכולת על, אוטונומי מאוד וכללי מאוד יאפשר את זה בדרך או בקנה מידה שלא היו אפשריים אחרת?
2. האם יש גורמים שיופיעו מדברים שגורמים לזה לקרות?
3. האם יש מערכות ומוסדות שימנעו ביעילות את זה מלהתרחש?

איפה שהתשובות שלכם הן "כן, כן, לא" אתם יכולים לראות שיש לנו בעיה גדולה.

מה התוכנית שלנו לנהל אותם? כפי שהמצב עומד יש שתיים על השולחן לגבי AI בכלל.

הראשונה היא לבנות אמצעי הגנה במערכות כדי למנוע מהן לעשות דברים שהן לא אמורות לעשות. זה נעשה עכשיו: מערכות AI מסחריות יסרבו, למשל, לעזור לבנות פצצה או לכתוב דברי שנאה.

התוכנית הזאת לקויה בצורה חמורה עבור מערכות מחוץ לשער.[^8] היא עשויה לעזור להקטין סיכון של AI הנותן סיוע מסוכן בבירור לשחקנים רעים. אבל זה לא יעשה דבר כדי למנוע הפרעה לעבודה, ריכוז כוח, היפר-קפיטליזם בהתחמקות, או החלפה של תרבות אנושית: אלה פשוט תוצאות של השימוש במערכות בדרכים מותרות שמרוויחות לספקים שלהן! וממשלות בוודאי ישיגו גישה למערכות לשימוש צבאי או מעקב.

התוכנית השנייה אפילו יותר גרועה: פשוט לשחרר באופן פתוח מערכות AI חזקות מאוד לכל אחד להשתמש בהן כפי שהוא רוצה,[^9] ולקוות לטוב.

מובלע בשתי התוכניות זה שמישהו אחר, למשל ממשלות, יעזור לפתור את הבעיות דרך חוק רך או קשה, סטנדרטים, רגולציות, נורמות ומנגנונים אחרים שבדרך כלל אנחנו משתמשים בהם כדי לנהל טכנולוגיות.[^10] אבל אם נשים בצד שחברות AI כבר נלחמות בכל כוחן נגד כל רגולציה משמעותית או הגבלות שמוטלות מבחוץ בכלל, עבור מספר מהסיכונים האלה די קשה לראות איזו רגולציה באמת תעזור. רגולציה יכולה להטיל תקני בטיחות על AI. אבל האם זה ימנע מחברות להחליף עובדים בסיטונאות ב-AI? האם זה יאסור על אנשים לתת ל-AI לנהל להם את החברות? האם זה ימנע מממשלות להשתמש ב-AI חזק במעקב ובנשק? הנושאים האלה יסודיים. האנושות יכולה פוטנציאלית למצוא דרכים להסתגל אליהם, אבל רק עם *הרבה* יותר זמן. כפי שהמצב עומד, בהתחשב במהירות ש-AI מגיע או עולה על היכולות של האנשים שמנסים לנהל אותם, הבעיות האלה נראות יותר ויותר בלתי פתירות.

## נאבד שליטה על (לפחות כמה) מערכות בינה מלאכותית כללית

רוב הטכנולוגיות מאוד ניתנות לשליטה, מבנית. אם המכונית או הטוסטר שלכם מתחילים לעשות משהו שאתם לא רוצים שהם יעשו, זה פשוט תקלה, לא חלק מהטבע שלהם כטוסטר. AI שונה: הוא *גדל* במקום להיות מעוצב, התפעול העיקרי שלו אטום, והוא מטבעו בלתי צפוי.

אובדן השליטה הזה אינו תיאורטי – אנחנו כבר רואים גרסאות מוקדמות. תחשבו ראשית על דוגמה פרוזאית, ובאופן מובחן מיטיבה. אם אתם מבקשים מ-ChatGPT לעזור לכם לערבב רעל, או לכתוב מאמר גזעני, הוא יסרב. זה כנראה טוב. אבל זה גם ChatGPT *לא עושה מה שביקשתם ממנו במפורש*. חלקי תוכנה אחרים לא עושים את זה. אותו מודל גם לא יעצב רעלים לבקשה של עובד OpenAI.[^11] זה מקל מאוד לדמיין איך זה יהיה עבור AI חזק יותר בעתיד להיות מחוץ לשליטה. במקרים רבים, הם פשוט לא יעשו מה שאנחנו מבקשים! או שמערכת בינה מלאכותית כללית על-אנושית נתונה תהיה צייתנית ונאמנה לחלוטין לאיזו מערכת פיקוד אנושית, או שלא. אם לא, *היא תעשה דברים שהיא אולי מאמינה שטובים עבורנו, אבל שנוגדים את הפקודות המפורשות שלנו.* זה לא משהו שנמצא תחת שליטה. אבל, אתם אולי אומרים, זה מכוון – הסירובים האלה הם בעיצוב, חלק ממה שנקרא "יישור" המערכות לערכים אנושיים. וזה נכון. אבל "תוכנית" היישור עצמה יש לה שתי בעיות עיקריות.[^12]

ראשית, ברמה עמוקה אין לנו מושג איך לעשות את זה. איך אנחנו מבטיחים שמערכת AI "תדאג" למה שאנחנו רוצים? אנחנו יכולים לאמן מערכות AI לומר ולא לומר דברים על ידי מתן משוב; והן יכולות ללמוד ולנמק על מה שבני אדם רוצים ואכפת להם מזה בדיוק כפי שהן מנמקות על דברים אחרים. אבל אין לנו שיטה – אפילו תיאורטית – לגרום להן להעריך באופן עמוק ומהימן את מה שאכפת לאנשים. יש פסיכופתים בני אדם המתפקדים גבוה שיודעים מה נחשב נכון ולא נכון, ואיך הם אמורים להתנהג. פשוט לא *אכפת* להם. אבל הם יכולים *להתנהג* כאילו כן, אם זה משרת את המטרה שלהם. בדיוק כפי שאיננו יודעים איך לשנות פסיכופת (או מישהו אחר) למישהו נאמן או מיושר באמת, לחלוטין עם מישהו או משהו אחר, אין לנו *מושג*[^13] איך לפתור בעיית היישור במערכות מתקדמות מספיק כדי לדמיין את עצמן כסוכנים בעולם ופוטנציאלית [לתפעל את האימון שלהן](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) ו[לרמות אנשים.](https://arxiv.org/abs/2311.08379) אם זה מתברר כבלתי אפשרי או בלתי השגי *או* להפוך את בינה מלאכותית כללית לצייתנית לחלוטין או לגרום לה לדאוג עמוקות לבני אדם, אז ברגע שהיא תוכל (ותאמין שהיא יכולה להימלט עם זה) היא תתחיל לעשות דברים שאנחנו לא רוצים.[^14]

שנית, יש סיבות תיאורטיות עמוקות להאמין שמטבען מערכות AI מתקדמות יהיו להן מטרות וכך התנהגויות הנוגדות אינטרסים אנושיים. למה? ובכן היא עלולה, כמובן, *לקבל* את המטרות האלה. מערכת שנוצרה על ידי הצבא תהיה כנראה במכוון רעה לפחות לכמה גורמים. אבל בצורה הרבה יותר כללית, עם זאת, מערכת AI עלולה לקבל איזו מטרה יחסית ניטרלית ("להרוויח הרבה כסף") או אפילו חיובית לכאורה ("לצמצם זיהום") שכמעט בהכרח מובילה למטרות "אינסטרומנטליות" שהן דווקא פחות מיטיבות.

אנחנו רואים את זה כל הזמן במערכות אנושיות. בדיוק כפי שתאגידים הרודפים רווח מפתחים מטרות אינסטרומנטליות כמו רכישת כוח פוליטי (כדי לנטרל רגולציות), הפיכה לסודיים (כדי לנטרל תחרות או שליטה חיצונית), או ערעור הבנה מדעית (אם ההבנה הזאת מראה שהפעולות שלהם מזיקות), מערכות AI חזקות יפתחו יכולות דומות – אבל במהירות ויעילות הרבה יותר גדולות. כל סוכן בעל כושר גבוה ירצה לעשות דברים כמו לרכוש כוח ומשאבים, להגביר את היכולות שלו, למנוע מעצמו להיהרג, לכבות, או להתנוון, לשלוט בנרטיבים חברתיים ובמסגרות סביב הפעולות שלו, לשכנע אחרים בדעותיו, וכן הלאה.[^15]

ובכל זאת זה לא רק חיזוי תיאורטי כמעט בלתי נמנע, זה כבר קורה באופן נצפה במערכות AI של היום, ועולה עם היכולת שלהן. כשמוערכות, אפילו מערכות AI "פסיביות" יחסית אלה יעשו, בנסיבות מתאימות, במכוון [ירמו מעריכים על המטרות והיכולות שלהן, יכוונו לנטרל מנגנוני פיקוח,](https://arxiv.org/abs/2412.04984) וימנעו מלהיכבות או להיות מאומנים מחדש על ידי [התחזות ליישור](https://arxiv.org/abs/2412.14093) או העתקת עצמם למקומות אחרים. למרות שלא מפתיע כלל לחוקרי בטיחות AI, ההתנהגויות האלה מרגיעות מאוד לצפייה. והן מבשרות רע מאוד למערכות AI חזקות הרבה יותר ואוטונומיות יותר שמגיעות.

אכן באופן כללי, חוסר היכולת שלנו להבטיח ש-AI "אכפת" לו ממה שאכפת לנו, או מתנהג באופן שליט או צפוי, או נמנע מפיתוח דחפים לשימור עצמי, רכישת כוח וכו', מבטיחים רק להתבטא יותר כש-AI הופך חזק יותר. יצירת מטוס חדש מרמזת על הבנה גדולה יותר של אוויוניקה, הידרודינמיקה ומערכות שליטה. יצירת מחשב חזק יותר מרמזת על הבנה ושליטה גדולות יותר בתפעול ועיצוב מחשב, שבב ותוכנה. *לא* כך עם מערכת AI.[^16]

לסכם: ייתכן שניתן יהיה לגרום לבינה מלאכותית כללית להיות צייתנית לחלוטין; אבל איננו יודעים איך לעשות זאת. אם לא, היא תהיה יותר ריבונית, כמו אנשים, עושה דברים שונים מסיבות שונות. אנחנו גם לא יודעים איך להחדיר באופן מהימן "יישור" עמוק ל-AI שיגרום לדברים האלה להיות בדרך כלל טובים לאנושות, ובהיעדר רמה עמוקה של יישור, טבע הסוכנות והאינטליגנציה עצמה מצביע על כך ש – בדיוק כמו אנשים ותאגידים – הן יונעו לעשות דברים אנטי-חברתיים עמוקים רבים.

איפה זה משאיר אותנו? עולם מלא ב-AI ריבוני חזק לא מבוקר *עלול* להיגמר כעולם טוב עבור בני אדם להיות בו.[^17] אבל כשהן הופכות חזקות יותר ויותר, כפי שנראה להלן, זה לא יהיה *שלנו* העולם.

זה עבור בינה מלאכותית כללית לא ניתנת לשליטה. אבל אפילו אם ניתן היה, איכשהו, לגרום לבינה מלאכותית כללית להיות מבוקרת ונאמנה לחלוטין, עדיין יהיו לנו בעיות עצומות. כבר ראינו אחת: AI חזק יכול לשמש ולהישתמש לרעה כדי לערער באופן עמוק את התפקוד של החברה שלנו. בואו נראה עוד אחת: במידה ש-AGI יהיה ניתן לשליטה ומשנה משחק באופן חזק (או אפילו *יחשב* ככזה) הוא יאיים כל כך על מבני כוח בעולם עד שיציג סיכון עמוק.

## אנחנו מגבירים באופן דרמטי את ההסתברות למלחמה רחבת היקף

דמיינו מצב בעתיד הקרוב, שבו התברר שמאמץ תאגידי, אולי בשיתוף עם ממשלה לאומית, נמצא על סף של AI מיטיב עצמי במהירות. זה קורה בהקשר הנוכחי של מירוץ בין חברות, ותחרות גיאופוליטית שבה מומלץ לממשלת ארה"ב לרדוף במפורש "פרויקט מנהטן בינה מלאכותית כללית" וארה"ב שולטת ביצוא של שבבי AI בעלי עוצמה גבוהה למדינות לא בעלות ברית.

תורת המשחקים כאן קשה: ברגע שמירוץ כזה מתחיל (כפי שהוא התחיל, בין חברות וקצת בין מדינות), יש רק ארבע תוצאות אפשריות:

1. המירוץ נעצר (בהסכמה, או בכוח חיצוני).
2. גורם אחד "מנצח" על ידי פיתוח בינה מלאכותית כללית חזקה ואז עצירת האחרים (באמצעות AI או אחרת).
3. המירוץ נעצר על ידי הרס הדדי של היכולת של הרצים לרוץ.
4. משתתפים מרובים ממשיכים לרוץ, ומפתחים על-אינטליגנציה, בערך באותה מהירות זה של זה.

בואו נבחן כל אפשרות. ברגע שהתחיל, עצירה בשלום של מירוץ בין חברות תדרוש התערבות ממשלה לאומית (עבור חברות) או תיאום בינלאומי חסר תקדים (עבור מדינות). אבל כשאיזושהי סגירה או זהירות משמעותית מוצעת, יהיו מיד זעקות: "אבל אם אנחנו נעצרים, *הם* הולכים לזרוק קדימה", שבו "הם" זה עכשיו סין (עבור ארה"ב), או ארה"ב (עבור סין), או סין *ו*ארה"ב (עבור אירופה או הודו). תחת הלך הרוח הזה,[^18] אף משתתף לא יכול לעצור באופן חד צדדי: כל עוד אחד מתחייב לרוץ, האחרים מרגישים שהם לא יכולים להרשות לעצמם לעצור.

האפשרות השנייה יש צד אחד "מנצח." אבל מה זה אומר? פשוט להשיג (איכשהו צייתנית) בינה מלאכותית כללית ראשון זה לא מספיק. המנצח חייב *גם* לעצור את האחרים מלהמשיך לרוץ – אחרת הם גם יגיעו לזה. זה אפשרי באופן עקרוני: מי שמפתח בינה מלאכותית כללית ראשון *יכול* לצבור כוח בלתי עצירה על כל השחקנים האחרים. אבל מה השגת "יתרון אסטרטגי מכריע" כזה באמת תדרוש? אולי זה יהיה יכולות צבאיות משנות משחק?[^19] או כוחות תקיפה סייבר?[^20] אולי בינה מלאכותית כללית פשוט תהיה כל כך משכנעת בצורה מדהימה שהיא תשכנע את הצדדים האחרים פשוט לעצור?[^21] כל כך עשירה שהיא תקנה את החברות האחרות או אפילו מדינות?[^22]

איך *בדיוק* צד אחד בונה AI חזק מספיק כדי לנטרל אחרים מלבנות AI חזק באופן דומה? אבל זאת השאלה הקלה.

כי עכשיו תחשבו איך המצב הזה נראה לכוחות אחרים. מה הממשלה הסינית חושבת כשארה"ב נראית משיגה יכולת כזאת? או להיפך? מה ממשלת ארה"ב (או הסינית, או הרוסית, או ההודית) חושבת כש-OpenAI או DeepMind או Anthropic נראים קרובים לפריצת דרך? מה קורה אם ארה"ב רואה מאמץ הודי או אמרתי חדש עם הצלחת פריצת דרך? הם יראו הן איום קיומי והן – במהותי – שהדרך היחידה ש"מירוץ" הזה נגמר היא דרך הנטרלה של עצמם. הסוכנים החזקים האלה מאוד – כולל ממשלות של מדינות מצוידות לחלוטין שבוודאי יש להן את האמצעים לעשות זאת – יהיו מונעים מאוד להשיג או להרוס יכולת כזאת, או בכוח או בחתרנות.[^23]

זה עלול להתחיל קטן, כחבלה של ריצות אימון או התקפות על ייצור שבבים, אבל ההתקפות האלה יכולות באמת להיעצר רק אחרי שכל הצדדים או מאבדים את היכולת לרוץ על AI, או מאבדים את היכולת לבצע את ההתקפות. כי המשתתפים רואים את הסיכונים כקיומיים, כל מקרה צפוי לייצג מלחמה קטסטרופלית.

זה מביא אותנו לאפשרות הרביעית: ריצה לעל-אינטליגנציה, ובדרך המהירה וההכי פחות מבוקרת שאפשר. כש-AI עולה בעוצמה, למפתחים שלו בשני הצדדים יהיה יותר ויותר קשה לשלוט, במיוחד כי ריצה ליכולות היא מנוגדת לסוג העבודה הזהירה ששליטה תדרוש. אז התרחיש הזה שם אותנו בדיוק במקרה שבו השליטה אבדה (או ניתנה, כפי שנראה הבא) למערכות ה-AI עצמן. כלומר, *AI מנצח במירוץ.* אבל מצד שני, במידה ששליטה *כן* נשמרת, אנחנו ממשיכים להחזיק מספר גורמים עויינים הדדיים כל אחד אחראי על יכולות חזקות ביותר. זה נראה כמו מלחמה שוב.

בואו נגיד את כל זה בדרך אחרת.[^24] לעולם הנוכחי פשוט אין מוסדות שניתן יהיה לסמוך עליהם לאכלס פיתוח של AI בהספק הזה בלי להזמין התקפה מידית.[^25] כל הצדדים ינמקו נכון שאו זה *לא* יהיה תחת שליטה – וכך זה איום לכל הצדדים, או זה *כן* יהיה תחת שליטה, וכך זה איום לכל יריב שמפתח אותו פחות מהר. אלה מדינות חמושות גרעינית, או חברות שנמצאות בתוכן.

בהיעדר דרך סבירה לבני אדם "לנצח" את המירוץ הזה, נשארנו עם מסקנה קשה: הדרך היחידה שהמירוץ הזה נגמר זה או בסכסוך קטסטרופלי או שבו AI, ולא איזה קבוצת בני אדם, הוא המנצח.

## אנחנו נותנים שליטה ל-AI (או הוא לוקח אותה)

תחרות "כוחות גדולים" גיאופוליטית היא רק אחד ממירוצים רבים: אנשים מתחרים כלכלית וחברתית; חברות מתחרות בשווקים; מפלגות פוליטיות מתחרות על כוח; תנועות מתחרות על השפעה. בכל זירה, כש-AI מתקרב ועולה על יכולת אנושית, לחץ תחרותי יאלץ משתתפים להאציל או לוותר על יותר ויותר שליטה למערכות AI – לא כי המשתתפים האלה רוצים, אלא כי הם [לא יכולים להרשות לעצמם לא.](https://arxiv.org/abs/2303.16200)

כמו עם סיכונים אחרים של בינה מלאכותית כללית, אנחנו רואים את זה כבר עם מערכות חלשות יותר. סטודנטים מרגישים לחץ להשתמש ב-AI במטלות שלהם, כי ברור שסטודנטים רבים אחרים כן. חברות [מתרוצצות לאמץ פתרונות AI מסיבות תחרותיות.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) אמנים ותכנתים מרגישים מאולצים להשתמש ב-AI אחרת התעריפים שלהם יידחקו על ידי אחרים שכן.

אלה מרגישים כמו האצלה בלחץ, אבל לא אובדן שליטה. אבל בואו נגביר את הסיכון ונדחוף את השעון קדימה. תחשבו על מנכ"ל שהמתחרים שלו משתמשים ב"עוזרי" בינה מלאכותית כללית לקבל החלטות מהירות יותר, טובות יותר, או מפקד צבאי שמתמודד עם יריב עם פיקוד ושליטה משופרי AI. מערכת AI מספיק מתקדמת יכולה לפעול באופן אוטונומי במספר פעמים מהירות אנושית, תחכום, מורכבות ויכולת עיבוד נתונים, רודפת מטרות מורכבות בדרכים מסובכות. המנכ"ל או המפקד שלנו, שאחראי על מערכת כזאת, עלולים לראות אותה משיגה מה שהם רוצים; אבל האם הם יבינו אפילו חלק קטן מ*איך* זה הושג? לא, הם פשוט יצטרכו לקבל את זה. יותר מזה, הרבה ממה שהמערכת עלולה לעשות זה לא רק לקחת פקודות אלא לייעץ לבוס המכניכל שלה מה לעשות. הייעוץ הזה יהיה טוב –– שוב ושוב.

באיזה שלב, אם כן, התפקיד של האדם יצטמצם ללחץ על "כן, תמשיך"?

זה מרגיש טוב להחזיק מערכות AI בעלות יכולת שיכולות לשפר את הפרודקטיביות שלנו, לטפל בעינויי שגרה מעצבנים, ואפילו לשמש כשותף מחשבה בהשגת דברים. זה ירגיש טוב להחזיק עוזר AI שיכול לטפל בפעולות עבורנו, כמו עוזר אישי אנושי טוב. זה ירגיש טבעי, אפילו מועיל, כש-AI הופך מאוד חכם, מוכשר ומהימן, להקשיב יותר ויותר להחלטות שלו. אבל ההאצלה "המועילה" הזאת יש לה נקודת קצה ברורה אם נמשיך בדרך: יום אחד נגלה שאנחנו לא באמת אחראים על כמעט כלום יותר, ושמערכות ה-AI שבאמת מנהלות את הזמר לא יותר ניתן לכיבוי מחברות נפט, מדיה חברתית, האינטרנט, או קפיטליזם.

וזו הגרסה החיובית הרבה יותר, שבה AI פשוט כל כך שימושי ויעיל שאנחנו נותנים לו לקבל רוב ההחלטות המפתח שלנו עבורנו. המציאות תהיה כנראה הרבה יותר תערובת בין זה לבין גרסאות שבהן מערכות בינה מלאכותית כללית לא מבוקרות *לוקחות* צורות שונות של כוח לעצמן כי, זכרו, כוח שימושי כמעט לכל מטרה שיש, ובינה מלאכותית כללית תהיה, בעיצוב, לפחות יעילה כמו בני אדם ברדיפה אחר המטרות שלה.

בין אם אנחנו מעניקים שליטה ובין אם היא נחטפת מאתנו, האובדן שלה נראה סביר ביותר. כפי שאלן טיורינג הניח במקור, "...נראה סביר שברגע ששיטת המחשבה של המכונה התחילה, לא ייקח הרבה זמן לעקוף את הכוחות החלשים שלנו. לא תהיה שאלה של המכונות גוועות, והן יוכלו לשוחח ביניהן כדי לחדד את שכלן. בשלב כלשהו לכן נצטרך לצפות שהמכונות ייקחו שליטה..."

אנא שימו לב, למרות שזה ברור מספיק, שאובדן שליטה על ידי האנושות ל-AI גם כרוך באובדן שליטה של ארצות הברית על ידי ממשלת ארצות הברית; זה אומר אובדן שליטה של סין על ידי המפלגה הקומוניסטית הסינית, ואובדן שליטה של הודו, צרפת, ברזיל, רוסיה וכל מדינה אחרת על ידי הממשלה שלהן. כך חברות AI משתתפות, גם אם זו אינה הכוונה שלהן, כרגע בהפלה פוטנציאלית של ממשלות עולם, כולל שלהן. זה יכול לקרות תוך שנים.

## בינה מלאכותית כללית תוביל לעל-אינטליגנציה

יש טיעון שאפשר לעשות שAI כלל-מטרה תחרותי אנושי או אפילו תחרותי מומחים, גם אם אוטונומי, יכול להיות נתיש. זה עלול להיות מפריע בצורה מדהימה בכל הדרכים שנידונו לעיל, אבל יש הרבה אנשים חכמים מאוד, סוכניים בעולם עכשיו, והם פחות או יותר ניתנים לניהול.[^26]

אבל לא נזכה להישאר ברמה אנושית בערך. ההתקדמות מעבר לכך תהיה כנראה מונעת על ידי אותם כוחות שכבר ראינו: לחץ תחרותי בין מפתחי AI המחפשים רווח וכוח, לחץ תחרותי בין משתמשי AI שלא יכולים להרשות לעצמם להיגרר מאחור, ו– הכי חשוב – יכולת הבינה מלאכותית כללית עצמה לשפר את עצמה.

בתהליך שכבר ראינו מתחיל עם מערכות פחות חזקות, בינה מלאכותית כללית תוכל בעצמה להעלות ולעצב גרסאות משופרות של עצמה. זה כולל חומרה, תוכנה, רשתות נוירונים, כלים, פיגומים וכו'. היא תהיה, בהגדרה, טובה מאתנו בלעשות את זה, אז אנחנו לא יודעים בדיוק איך היא תיצור בוטסטרפ לאינטליגנציה. אבל לא נצטרך. במידה שעוד יש לנו השפעה במה שבינה מלאכותית כללית עושה, פשוט נצטרך לבקש ממנה, או לתת לה.

אין מחסום ברמה אנושית לקוגניציה שיכול להגן עלינו מהתחמקות הזו.[^27]

ההתקדמות של בינה מלאכותית כללית לעל-אינטליגנציה אינה חוק טבע; עדיין יהיה אפשרי לצמצם את ההתחמקות, במיוחד אם בינה מלאכותית כללית תהיה יחסית מרוכזת ובמידה שהיא מבוקרת על ידי גורמים שלא מרגישים לחץ לרוץ זה מול זה. אבל אם בינה מלאכותית כללית תתפזר בהרחבה ותהיה אוטונומית ביותר, נראה כמעט בלתי אפשרי למנוע ממנה להחליט שהיא אמורה להיות יותר, ואז עוד יותר, חזקה.

## מה קורה אם נבנה (או שבינה מלאכותית כללית תבנה) על-אינטליגנציה

כדי לנסח זאת בבוטות, אין לנו מושג מה יקרה אם נבנה על-אינטליגנציה.[^28] היא תנקוט פעולות שאיננו יכולים לעקוב או לתפוס מסיבות שאיננו יכולים להבין לעבר מטרות שאיננו יכולים לתפוס. מה שאנחנו כן יודעים זה שזה לא יהיה תלוי בנו.[^29]

חוסר האפשרות לשלוט בעל-אינטליגנציה יכול להיות מובן דרך אנלוגיות מתוחות יותר ויותר. ראשית, דמיינו שאתם מנכ"ל של חברה גדולה. אין דרך שאתם יכולים לעקוב אחר כל מה שקורה, אבל עם ההתקנה הנכונה של כוח אדם, אתם עדיין יכולים להבין באופן משמעותי את התמונה הכללית, ולקבל החלטות. אבל נניח רק דבר אחד: כל השאר בחברה פועלים במאה פעמים המהירות שלכם. אתם עדיין יכולים להתעדכן?

עם AI על-אינטליגנטי, אנשים "יפקדו" על משהו לא רק מהיר יותר, אלא פועל ברמות של תחכום ומורכבות שהם לא יכולים להבין, מעבד הרבה יותר נתונים ממה שהם יכולים אפילו לתפוס. חוסר התאימות הזה יכול להיות מועמד לרמה פורמלית: [חוק הגיוון הנדרש של אשבי](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (וראו את ["משפט הרגולטור הטוב"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf) הקשור) קובעים, בערך, שכל מערכת שליטה חייבת להחזיק כמה כפתורים וחוגות שיש למערכת המבוקרת דרגות חופש.

אדם השולט במערכת AI על-אינטליגנטית יהיה כמו שרך השולט בג'נרל מוטורס: גם אם "עשו מה שהשרך רוצה" ייכתב בתקנון החברה, המערכות כל כך שונות במהירות ובטווח פעולה ש"שליטה" פשוט לא חלה. (וכמה זמן עד שהתקנון המעצבן הזה ייכתב מחדש?)[^30]

כשיש אפס דוגמאות של צמחים השולטים בתאגידים בפורצ'ן 500, יהיו בדיוק אפס דוגמאות של אנשים השולטים בעל-אינטליגנציות. זה מתקרב לעובדה מתמטית.[^31] אם על-אינטליגנציה הייתה נבנית – ללא קשר לאיך הגענו לשם – השאלה לא תהיה האם בני אדם יכלו לשלוט בה, אלא האם נמשיך להתקיים, ואם כן, האם תהיה לנו קיום טוב ומשמעותי כיחידים או כמין. על השאלות הקיומיות האלה עבור האנושות יהיה לנו מעט רכישה. העידן האנושי ייגמר.

## מסקנה: אסור לנו לבנות בינה מלאכותית כללית

יש תרחיש שבו בניית בינה מלאכותית כללית עלולה ללכת טוב עבור האנושות: היא נבנית בזהירות, תחת שליטה ולטובת האנושות, נשלטת על ידי הסכמה הדדית של בעלי אינטרס רבים,[^32] ומונעת מלהתפתח לעל-אינטליגנציה בלתי שליטה.

*התרחיש הזה אינו פתוח לנו תחת הנסיבות הנוכחיות.* כפי שנידון בחלק זה, בהסתברות גבוהה מאוד, פיתוח בינה מלאכותית כללית יוביל לאיזה שילוב של:

- הפרעה או הרס חברתי וציוויליזציוני מסיבי;
- קונפליקט או מלחמה בין כוחות גדולים;
- אובדן שליטה על ידי האנושות *על* או *ל* מערכות AI חזקות;
- התחמקות לעל-אינטליגנציה בלתי שליטה, וחוסר הרלוונטיות או הפסקה של המין האנושי.

כפי שתיאור בדיוני מוקדם של בינה מלאכותית כללית הניח: הדרך היחידה לנצח היא לא לשחק.

[^1]: [חוק ה-AI של האיחוד האירופי](https://artificialintelligenceact.eu/) הוא חיקוק משמעותי אבל לא ימנע ישירות מערכת AI מסוכנת מלהיות מפותחת או נפרסת, או אפילו משוחררת בגלוי, במיוחד בארה"ב. חלק משמעותי אחר של מדיניות, הצו הנהלתי של ארה"ב על AI, בוטל.

[^2]: [סקר הגאלופ הזה](https://news.gallup.com/poll/1597/confidence-institutions.aspx) מראה ירידה עגומה באמון במוסדות ציבוריים מאז 2000 בארה"ב. המספרים האירופיים מגוונים ופחות קיצוניים, אבל גם במגמת ירידה. חוסר אמון לא אומר בהכרח שמוסדות באמת *הם* לא תפקודיים, אבל זה גם אינדיקציה וגם סיבה.

[^3]: והפרעות גדולות שאנחנו עכשיו תומכים בהן – כמו הרחבת זכויות לקבוצות חדשות – הונעו במיוחד על ידי אנשים בכיוון של שיפור הדברים.

[^4]: תן לי להיות ברור. אם העבודה שלכם יכולה להיעשות מאחורי מחשב, עם יחסית מעט אינטראקציה פנים אל פנים עם אנשים מחוץ לארגון שלכם, ולא כרוכה באחריות משפטית לצ