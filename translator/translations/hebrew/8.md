# פרק 8 - איך לא לבנות AGI

AGI אינה בלתי נמנעת – היום אנו עומדים בפרשת דרכים. פרק זה מציג הצעה לאופן שבו נוכל למנוע את בנייתה.

אם הדרך שבה אנו צועדים כיום מובילה לסיום האפשרי של הציוויליזציה שלנו, כיצד נחליף מסלול?

נניח שהרצון להפסיק לפתח בינה מלאכותית כללית (AGI) ועל-אינטליגנציה היה נפוץ וחזק,[^1] משום שהופכת להבנה מקובלת שAGI תהיה בולעת כוח במקום מעניקת כוח, ותהווה סכנה עמוקה לחברה ולאנושות. איך נסגור את השערים?

נכון להיום אנו יודעים על דרך אחת בלבד *ליצור* AI חזק וכללי, והיא באמצעות חישובים ענקיים באמת של רשתות נוירונים עמוקות. מכיוון שאלה דברים קשים ויקרים להפליא לביצוע, יש משמעות מסוימת שבה *לא* לעשות אותם זה קל.[^2] אבל כבר ראינו את הכוחות שמניעים לכיוון AGI, ואת הדינמיקה התאורטית-משחקית שמקשה מאוד על כל צד להפסיק באופן חד-צדדי. לכן יידרש שילוב של התערבות מבחוץ (כלומר ממשלות) כדי לעצור תאגידים, והסכמים בין ממשלות כדי לעצור את עצמן.[^3] איך זה יכול להיראות?

שימושי קודם כל להבחין בין פיתוחי AI שחייבים להיות *נמנעים* או *נאסרים*, לבין אלה שחייבים להיות *מנוהלים*. הראשונים יהיו בעיקר התחמקות לעל-אינטליגנציה.[^4] לגבי פיתוח אסור, ההגדרות צריכות להיות חדות ככל האפשר, וגם האימות וגם האכיפה צריכים להיות מעשיים. מה שחייב להיות *מנוהל* הם מערכות AI כלליות וחזקות – שכבר יש לנו, ושיהיו להן הרבה אזורים אפורים, ניואנסים ומורכבות. לגביהן, מוסדות חזקים ויעילים חיוניים.

אנו גם יכולים להבחין באופן שימושי בין סוגיות שחייבות להיות מטופלות ברמה הבינלאומית (כולל בין יריבים או יריבות גיאופוליטיות) [^5] לבין אלה שתחומי שיפוט, מדינות או אוספי מדינות יכולים לנהל. פיתוח אסור נופל בעיקר לקטגוריה "הבינלאומית", משום שאיסור מקומי על פיתוח של טכנולוגיה בדרך כלל ניתן לעקיפה על ידי החלפת מיקום.[^6]

לבסוף, אנו יכולים לבחון כלים בארגז הכלים. יש הרבה, כולל כלים טכניים, חוק רך (תקנים, נורמות וכו'), חוק קשה (רגולציות ודרישות), אחריות, תמריצי שוק וכן הלאה. בואו נשים תשומת לב מיוחדת לאחד שמיוחד ל-AI.

## אבטחת כוח חישוב וממשל

כלי מרכזי בממשל AI עתיר-כוח יהיה החומרה שהוא דורש. תוכנה מתפשטת בקלות, עם עלות ייצור שולית כמעט אפס, חוצה גבולות בקלילות, וניתן לשנותה מיידית; אף אחד מאלה לא נכון לגבי חומרה. עם זאת, כפי שדנו, כמויות עצומות של "כוח חישוב" זה נחוצות הן במהלך אימון מערכות AI והן במהלך הסקה כדי להשיג את המערכות המסוגלות ביותר. כוח חישוב ניתן לכימות, לחישוב ולביקורת בקלות, עם מעט עמימות יחסית ברגע שמפתחים כללים טובים לעשות זאת. והכי חשוב, כמויות גדולות של חישוב הן, כמו אורניום מועשר, משאב נדיר, יקר וקשה לייצור מאוד. למרות שצ'יפים ממוחשבים הם בכל מקום, החומרה הנדרשת ל-AI יקרה וקשה לייצור במיוחד.[^7]

מה שהופך צ'יפים המתמחים ב-AI *הרבה יותר* ניתנים לניהול כמשאב נדיר מאשר אורניום הוא שהם יכולים לכלול מנגנוני אבטחה מבוססי חומרה. רוב הטלפונים החכמים המודרניים, וחלק מהמחשבים הניידים, בעלי תכונות חומרה מיוחדות על-גבי-הצ'יפ שמאפשרות להם להבטיח שהם מתקינים רק תוכנת מערכת הפעלה ועדכונים מאושרים, שהם שומרים ומגנים על נתונים ביומטריים רגישים במכשיר, ושניתן להפוך אותם לחסרי תועלת לכל אחד מלבד בעליהם אם הם אבדו או נגנבו. במהלך השנים האחרונות אמצעי אבטחת חומרה כאלה הפכו מבוססים ונפוצים, ובאופן כללי הוכיחו עצמם כבטוחים למדי.

החידוש המרכזי של התכונות האלה הוא שהן קושרות חומרה ותוכנה יחד באמצעות קריפטוגרפיה.[^8] כלומר, עצם הבעלות על חתיכת חומרת מחשב מסוימת לא אומרת שמשתמש יכול לעשות איתה כל מה שהוא רוצה על ידי הפעלת תוכנה שונה. והקישור הזה גם מספק אבטחה חזקה משום שהתקפות רבות ידרשו פריצה של אבטחת *חומרה* ולא רק *תוכנה*.

כמה דוחות עדכניים (למשל מ[GovAI ושותפים](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips), ו[RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) הצביעו על כך שתכונות חומרה דומות המוטמעות בחומרת חישוב חדישה הרלוונטית ל-AI יכולות למלא תפקיד שימושי מאוד באבטחה ובממשל של AI. הן מאפשרות מספר פונקציות זמינות ל"מפקח" [^9] שאדם אולי לא יניח שזמינות או בכלל אפשריות. כמה דוגמאות מרכזיות:

- *גיאולוקציה*: מערכות יכולות להיות מוקמות כך שלצ'יפים יש מיקום ידוע, ויכולות לפעול בצורה שונה (או להיות כבות לגמרי) בהתבסס על מיקום.[^10]
- *חיבורים מורשים*: כל צ'יפ יכול להיות מוגדר עם רשימת היתרים מבוססת חומרה של צ'יפים אחרים מסוימים איתם הוא יכול לתקשר ברשת, ולא יוכל להתחבר עם צ'יפים שלא נמצאים ברשימה הזו.[^11] זה יכול להגביל את גודל האשכולות התקשורתיים של צ'יפים.[^12]
- *הסקה או אימון מדודים (ומתג כיבוי אוטומטי)*: מפקח יכול להעניק רישיון רק לכמות מסוימת של אימון או הסקה (בזמן, או FLOPs, או אולי טוקנים) להתבצע על ידי משתמש, ואחר כך נדרש רישיון חדש. אם הגדלים קטנים, אז נדרש רישוי מחדש יחסית רציף של מודל. המודל יכול אז להיות "כבוי" פשוט על ידי מניעת איתות רישיון זה.[^13]
- *הגבלת מהירות*: מודל נמנע מריצה במהירות הסקה גבוהה יותר ממגבלה מסוימת שנקבעת על ידי מפקח או בדרך אחרת. זה יכול להיות מיושם באמצעות סט מוגבל של חיבורים מורשים, או באמצעים מתוחכמים יותר.
- *אימון מאומת*: הליך אימון יכול להניב הוכחה קריפטוגרפית בטוחה שסט מסוים של קודים, נתונים וכמות שימוש בכוח חישוב הופעלו ביצירת המודל.

## איך לא לבנות על-אינטליגנציה: מגבלות גלובליות על כוח חישוב לאימון ולהסקה

עם השיקולים האלה – במיוחד לגבי חישוב – במקום, אנו יכולים לדון איך לסגור את השערים לעל-אינטליגנציה מלאכותית; אז נפנה למניעת AGI מלא, ולניהול מודלי AI כשהם מתקרבים לרמה האנושית ועוברים אותה בהיבטים שונים.

המרכיב הראשון הוא, כמובן, ההבנה שעל-אינטליגנציה לא תהיה ניתנת לשליטה, ושההשלכות שלה בלתי צפויות ביסודן. לפחות סין וארה"ב חייבות להחליט באופן עצמאי, למטרה זו או אחרת, לא לבנות על-אינטליגנציה.[^14] אז נדרשת הסכמה בינלאומית ביניהן ואחרות, עם מנגנון אימות ואכיפה חזק, כדי להבטיח לכל הצדדים שהיריבים שלהם לא בוגדים ומחליטים לשחק בקוביות.

כדי להיות ניתנות לאימות ולאכיפה המגבלות צריכות להיות מגבלות קשות, וחד-משמעיות ככל האפשר. זה נראה כמו בעיה כמעט בלתי אפשרית: הגבלת יכולות של תוכנה מורכבת עם תכונות בלתי צפויות, ברחבי העולם. למרבה המזל המצב הרבה יותר טוב מזה, משום שהדבר שעשה AI מתקדם אפשרי – כמות ענקית של כוח חישוב – הרבה, הרבה יותר קל לשליטה. למרות שזה עדיין יכול לאפשר מערכות חזקות ומסוכנות, *התחמקות על-אינטליגנציה* כנראה יכולה להיות נמנעת על ידי תקרה קשה על כמות החישוב שנכנסת לרשת נוירונים, יחד עם הגבלת קצב על כמות ההסקה שמערכת AI (של רשתות נוירונים מחוברות ותוכנה אחרת) יכולה לבצע. גרסה ספציפית של זה מוצעת להלן.

אולי נראה שהצבת מגבלות גלובליות קשות על חישוב AI תדרוש רמות ענקיות של תיאום בינלאומי ופיקוח חודרני ומרסק פרטיות. למרבה המזל, זה לא יידרש. [שרשרת האספקה ההדוקה ובעלת צוואר הבקבוק](https://arxiv.org/abs/2402.08797) מספקת שברגע שמגבלה נקבעת חוקית (בין אם בחוק או בצו ביצועי), אימות עמידה במגבלה זו ידרוש רק מעורבות ושיתוף פעולה של קומץ חברות גדולות.[^15]

תוכנית כזו בעלת מספר תכונות רצויות מאוד. היא פולשנית במינימום במובן שרק כמה חברות גדולות יש להן דרישות מוטלות עליהן, ורק אשכולות חישוב די משמעותיים יהיו מנוהלים. הצ'יפים הרלוונטיים כבר מכילים את יכולות החומרה הנדרשות לגרסה ראשונה.[^16] גם יישום וגם אכיפה נשענים על הגבלות חוקיות סטנדרטיות. אבל אלה מגובות על ידי תנאי שימוש של החומרה ועל ידי בקרות חומרה, מה שמפשט מאוד את האכיפה ומונע רמאות של חברות, קבוצות פרטיות, או אפילו מדינות. יש תקדים רב לחברות חומרה שמטילות הגבלות מרחוק על שימוש בחומרה שלהן, ונועלות/פותחות יכולות מסוימות חיצונית,[^17] כולל אפילו ב-CPUים עתירי ביצועים במרכזי נתונים.[^18] אפילו לגבי החלק הקטן יחסית של החומרה והארגונים המושפעים, הפיקוח יכול להיות מוגבל לטלמטריה, ללא גישה ישירה לנתונים או מודלים עצמם; והתוכנה לכך יכולה להיות פתוחה לבדיקה כדי להראות שלא מתועדים נתונים נוספים. התכנית היא בינלאומית ושיתופית, ודי גמישה וניתנת להרחבה. מכיוון שהמגבלה בעיקר על חומרה ולא על תוכנה, היא יחסית אגנוסטית לגבי איך פיתוח ופריסה של תוכנת AI מתרחשים, ומתאימה למגוון פרדיגמות כולל AI "מבוזר" או "ציבורי" יותר שמכוון למאבק בריכוזיות כוח מונעת AI.

סגירת שערים מבוססת חישוב כן יש לה חסרונות גם כן. ראשית, היא רחוקה מלהיות פתרון מלא לבעיית ממשל AI בכלל. שנית, ככל שחומרת מחשב נהיית מהירה יותר, המערכת "תתפוס" יותר ויותר חומרה באשכולות קטנים יותר ויותר (או אפילו GPUים בודדים).[^19] אפשר גם שבגלל שיפורים אלגוריתמיים מגבלת חישוב נמוכה עוד יותר תהיה נחוצה בזמן,[^20] או שכמות החישוב תהפוך לרובה לא רלוונטית וסגירת השער תדרוש במקום זה משטר ממשל מפורט יותר מבוסס סיכונים או יכולות עבור AI. שלישית, לא משנה הערבויות ומספר הישויות הקטן המושפעות, מערכת כזו בהכרח תיצור התנגדות בנוגע לפרטיות ולפיקוח, בין חששות אחרים.[^21]

כמובן, פיתוח ויישום של תכנית ממשל מגבילת חישוב בפרק זמן קצר יהיו די מאתגרים. אבל זה בהחלט בר ביצוע.

## A-G-I: החיתוך המשולש כבסיס הסיכון ושל המדיניות

בואו נפנה עכשיו ל-AGI. קווים קשים והגדרות כאן קשים יותר, משום שבוודאי יש לנו אינטליגנציה שהיא מלאכותית וכללית, ולפי שום הגדרה קיימת לא כולם יסכימו אם או מתי היא קיימת. יותר מזה, מגבלת כוח חישוב או הסקה היא כלי די קהה (כוח חישוב הוא פרוקסי ליכולת, שהיא אז פרוקסי לסיכון) ש– אלא אם היא די נמוכה – לא צפויה למנוע AGI שהוא חזק מספיק לגרום לשיבוש חברתי או ציוויליזציוני או לסיכונים חריפים.

טענתי שהסיכונים החריפים ביותר נוצרים מהחיתוך המשולש של יכולת גבוהה מאוד, אוטונומיה גבוהה וכלליות רבה. אלה המערכות ש– אם הן מפותחות בכלל – חייבות להיות מנוהלות בזהירות עצומה. על ידי יצירת תקנים מחמירים (באמצעות אחריות ורגולציה) למערכות המשלבות את שלושת התכונות האלה, אנו יכולים להכוון פיתוח AI לכיוון אלטרנטיבות בטוחות יותר.

כמו עם תעשיות ומוצרים אחרים שיכולים להזיק לצרכנים או לציבור, מערכות AI דורשות רגולציה זהירה של סוכנויות ממשלתיות יעילות ומוסמכות. הרגולציה הזו צריכה לזהות את הסיכונים הטבועים של AGI, ולמנוע מערכות AI עתירות כוח מסוכנות באופן בלתי מקובל מלהיות מפותחות.[^22]

עם זאת, רגולציה בקנה מידה גדול, במיוחד עם שיניים אמיתיות שבטוח יתנגדו להן בתעשייה,[^23] לוקחת זמן [^24] וגם הרשעה פוליטית שזה נחוץ.[^25] בהתחשב בקצב ההתקדמות, זה יכול לקחת יותר זמן ממה שעומד לרשותנו.

בלוח זמנים הרבה יותר מהיר וכשאמצעים רגולטוריים מתפתחים, אנו יכולים לתת לחברות את התמריצים הנחוצים ל(א) להימנע מפעילויות מסוכנות מאוד ו(ב) לפתח מערכות מקיפות להערכת סיכונים ולהפחתתם, על ידי הבהרה והגדלה של רמות האחריות למערכות המסוכנות ביותר. הרעיון יהיה להטיל את רמות האחריות הגבוהות ביותר – חמורה ובמקרים מסוימים פלילית אישית – למערכות בחיתוך המשולש של אוטונומיה-כלליות-אינטליגנציה גבוהות, אבל לספק "מקלטים בטוחים" לאחריות מבוססת תקלות טיפוסית יותר למערכות שבהן אחת מהתכונות האלה חסרה או מובטח שתהיה ניתנת לניהול. כלומר, למשל, מערכת "חלשה" שהיא כללית ואוטונומית (כמו עוזר אישי מסוגל ואמין אבל מוגבל) תהיה כפופה לרמות אחריות נמוכות יותר. כמו כן מערכת צרה ואוטונומית כמו מכונית נהיגה עצמית עדיין תהיה כפופה לרגולציה המשמעותית שהיא כבר, אבל לא אחריות מוגברת. באופן דומה למערכת מסוגלת וכללית מאוד שהיא "פאסיבית" ובעיקר לא מסוגלת לפעולה עצמאית. מערכות שחסרות לן *שתיים* משלוש התכונות הן עוד יותר ניתנות לניהול ומקלטים בטוחים יהיו עוד יותר קלים לתביעה. הגישה הזו משקפת איך אנו מטפלים בטכנולוגיות מסוכנות אחרות:[^26] אחריות גבוהה יותר לתצורות מסוכנות יותר יוצרת תמריצים טבעיים לאלטרנטיבות בטוחות יותר.

התוצאה המוגדרת כברירת מחדל של רמות אחריות כה גבוהות, שפועלות *להפנים* סיכון AGI לחברות במקום להעביר אותו לציבור, היא כנראה (ובתקווה!) שחברות פשוט לא יפתחו AGI מלא עד ואלא אם הן יכולות באמת להפוך אותו לאמין, בטוח וניתן לשליטה בהתחשב ש*המנהיגות שלהן עצמה* הן הצדדים בסיכון. (במקרה שזה לא מספיק, החקיקה המבהירה אחריות צריכה גם לאפשר במפורש הקלה עוצרת, כלומר שופט שמצווה על עצירה, לפעילויות שבבירור באזור הסכנה ולכאורה מציבות סיכון ציבורי.) כשרגולציה נכנסת למקום, מציתות לרגולציה יכולה להפוך למקלט הבטוח, והמקלטים הבטוחים מאוטונומיה נמוכה, צרות, או חולשה של מערכות AI יכולים להמיר למשטרים רגולטוריים קלים יחסית.

## הוראות מפתח של סגירת שערים

עם הדיון לעיל בראש, חלק זה מספק הצעות להוראות מפתח שיישמו וישמרו איסור על AGI מלא ועל-אינטליגנציה, וניהול של AI תחרותי-אנושי או תחרותי-מומחים לשימוש כללי ליד סף ה-AGI המלא.[^27] יש לו ארבעה חלקים מרכזיים: 1) חשבונאות ופיקוח על כוח חישוב, 2) תקרות כוח חישוב באימון ובתפעול של AI, 3) מסגרת אחריות, ו-4) תקני בטיחות ואבטחה מדורגים שכוללים דרישות רגולטוריות קשות. אלה מתוארים בקצרה בהמשך, עם פרטים נוספים או דוגמאות יישום שניתנות בשלושה טבלאות נלוות. חשוב לציין שאלה רחוקים מלהיות כל מה שיהיה נחוץ לממשל מערכות AI מתקדמות; למרות שיהיו להם יתרונות אבטחה ובטיחות נוספים, הן מכוונות לסגירת השער לבריחה של אינטליגנציה, ולהפניית פיתוח AI לכיוון טוב יותר.

### 1\. חשבונאות כוח חישוב, ושקיפות

- ארגון תקנים (למשל NIST בארה"ב ואחר כך ISO/IEEE בינלאומית) צריך להגדיר תקן טכני מפורט לכוח החישוב הכולל שמשמש באימון ותפעול של מודלי AI, ב-FLOP, והמהירות ב-FLOP/s שבה הם פועלים. פרטים לאיך זה יכול להיראות ניתנים בנספח א.[^28]
- דרישה – בין אם בחקיקה חדשה או תחת הסמכות הקיימת [^29] – צריכה להיות מוטלת על ידי תחומי שיפוט שבהם מתרחש אימון AI בקנה מידה גדול לחשב ולדווח לגוף רגולטורי או סוכנות אחרת את סך ה-FLOP שמשמש באימון ובתפעול של כל המודלים מעל סף של 10<sup>25</sup> FLOP או 10<sup>18</sup> FLOP/s.[^30]
- הדרישות האלה צריכות להיות מופעלות בהדרגה, בהתחלה לדרוש הערכות מתועדות היטב בתום לב על בסיס רבעוני, עם שלבים מאוחרים יותר שדורשים תקנים גבוהים יותר בהדרגה, עד ל-FLOP כולל ו-FLOP/s מאומתים קריפטוגרפית המחוברים לכל *פלט* מודל.
- הדוחות האלה צריכים להיות משולבים עם הערכות מתועדות היטב של עלות אנרגיה ופיננסית שולית שמשמשת ביצירת כל פלט AI.

הסבר: המספרים האלה המחושבים היטב ומדווחים בשקיפות יספקו הבסיס לתקרות אימון ותפעול, וגם מקלט בטוח מאמצעי אחריות גבוהים יותר (ראו נספחים C ו-D).

### 2\. תקרות כוח חישוב לאימון ולתפעול

- תחומי שיפוט שמארחים מערכות AI צריכים להטיל מגבלה קשה על כוח החישוב הכולל שנכנס לכל פלט מודל AI, מתחיל ב-10<sup>27</sup> FLOP [^31] וניתן להתאמה לפי הצורך.
- תחומי שיפוט שמארחים מערכות AI צריכים להטיל מגבלה קשה על קצב כוח החישוב של פלטי מודל AI, מתחיל ב-10<sup>20</sup> FLOP/s וניתן להתאמה לפי הצורך.

הסבר: חישוב כולל, למרות שהוא מאוד לא מושלם, הוא פרוקסי ליכולת AI (ולסיכון) שניתן למדידה ולאימות באופן קונקרטי, אז מספק עצירה קשה להגבלת יכולות. הצעת יישום קונקרטית ניתנת בנספח ב.

### 3\. אחריות מוגברת למערכות מסוכנות

- יצירה ותפעול [^32] של מערכת AI מתקדמת שהיא כללית, מסוגלת ואוטונומית מאוד, צריכים להיות מובהרים חוקית באמצעות חקיקה להיות כפופים לאחריות חמורה, משותפת-וכמה, ולא מבוססת תקלה של צד יחיד.[^33]
- הליך משפטי צריך להיות זמין להצגת טענות בטיחות חיוביות, שיעניקו מקלט בטוח מאחריות חמורה למערכות שהן קטנות (במונחי כוח חישוב), חלשות, צרות, פאסיביות, או שיש להן ערבויות בטיחות, אבטחה ושליטה מספיקות.
- מסלול מפורש ומערכת תנאים להקלה עוצרת לעצור פעילויות אימון והסקה של AI שמהווים סכנה ציבורית צריכים להיות מתוארים.

הסבר: מערכות AI לא יכולות להיות אחראיות, אז אנו חייבים להחזיק אנשים וארגונים אחראים לנזק שהם גורמים (אחריות).[^34] AGI בלתי שליט הוא איום על החברה והציוויליזציה ובהיעדר טענת בטיחות צריך להיחשב מסוכן באופן חריג. הטלת נטל האחריות על מפתחים להראות שמודלים חזקים בטוחים מספיק כדי לא להיחשב "מסוכנים באופן חריג" מתמרצת פיתוח בטוח, יחד עם שקיפות ושמירת רישומים כדי לתבוע את המקלטים הבטוחים האלה. רגולציה יכולה אז למנוע נזק איפה שהרתעה מאחריות לא מספיקה. לבסוף, מפתחי AI כבר אחראים לנזקים שהם גורמים, אז הבהרה חוקית של אחריות למערכות הכי מסוכנות יכולה להיות עשויה מיידית, בלי שתקנים מפורטים מאוד יפותחו; אלה יכולים אז להתפתח עם הזמן. פרטים ניתנים בנספח ג.

### 4\. רגולציית בטיחות עבור AI

מערכת רגולטורית שמטפלת בסיכונים חריפים בקנה מידה גדול של AI תדרוש לכל הפחות:

- זיהוי או יצירה של סט מתאים של גופים רגולטוריים, כנראה סוכנות חדשה;
- מסגרת הערכת סיכונים מקיפה;[^35]
- מסגרת לטענות בטיחות חיוביות, מבוססת בחלקה על מסגרת הערכת הסיכונים, להיות מוגשות על ידי מפתחים, ולביקורת על ידי קבוצות וסוכנויות *עצמאיות*;
- מערכת רישוי מדורגת, עם דרגות שעוקבות אחר רמות יכולת.[^36] רישיונות יינתנו על בסיס טענות בטיחות וביקורות, לפיתוח ופריסה של מערכות. הדרישות ינועו מהודעה בקצה התחתון, לערבויות כמותיות של בטיחות, אבטחה ושליטה לפני פיתוח, בקצה העליון. אלה ימנעו שחרור של מערכות עד שיוכח שהן בטוחות, ויאסרו פיתוח של מערכות מסוכנות מטבעטבעו. נספח ד מספק הצעה לאיך תקני בטיחות ואבטחה כאלה יכולים להיראות.
- הסכמות להביא אמצעים כאלה לרמה הבינלאומית, כולל גופים בינלאומיים להרמוניה של נורמות ותקנים, ואולי סוכנויות בינלאומיות לבחינת טענות בטיחות.

הסבר: בסופו של דבר, אחריות אינה המנגנון הנכון למניעת סיכון בקנה מידה גדול לציבור מטכנולוgiה חדשה. רגולציה מקיפה, עם גופים רגולטוריים מוסמכים, תהיה נחוצה עבור AI בדיוק כמו לכל תעשייה גדולה אחרת שמציבה סיכון לציבור.[^37]

רגולציה לכיוון מניעת סיכונים נפוצים אחרים אבל פחות חריפים צפויה להשתנות בצורתה מתחום שיפוט לתחום שיפוט. הדבר החשוב הוא להימנע מפיתוח מערכות AI שהן כל כך מסוכנות שהסיכונים האלה בלתי ניתנים לניהול.

## מה אז?

במהלך העשור הבא, כש-AI הופך נפוץ יותר וטכנולוגיית הליבה מתקדמת, שני דברים מרכזיים צפויים לקרות. ראשית, רגולציה של מערכות AI חזקות קיימות תהפוך קשה יותר, אך אפילו יותר נחוצה. סביר שלפחות כמה אמצעים המטפלים בסיכוני בטיחות בקנה מידה גדול ידרשו הסכמה ברמה הבינלאומית, עם תחומי שיפוט בודדים שיאכפו כללים מבוססים על הסכמות בינלאומיות.

שנית, תקרות כוח חישוב לאימון ולתפעול יהפכו קשות יותר לשמירה כשחומרה הופכת זולה יותר ויעילה יותר במחיר; הן גם יכולות להפוך פחות רלוונטיות (או שיהיה צורך שיהיו אפילו יותר הדוקות) עם התקדמויות באלגוריתמים ובארכיטקטורות.

זה ששליטה על AI תהפוך קשה יותר לא אומר שאנחנו צריכים לוותר! יישום התוכנית שמתוארת במאמר הזה ייתן לנו גם זמן יקר וגם שליטה מכרעת על התהליך שתשים אותנו במצב הרבה, הרבה יותר טוב להימנע מהסיכון הקיומי של AI לחברה, לציוויליזציה ולמין שלנו.

בטווח הארוך יותר, יהיו בחירות לעשות לגבי מה אנו מאפשרים. אנחנו עשויים לבחור עדיין ליצור צורה כלשהי של AGI שניתן לשליטה באמת, במידה שזה יתגלה כאפשרי. או שאנחנו עלולים להחליט שניהול העולם עדיף שיישאר למכונות, אם נוכל לשכנע את עצמנו שהן יעשו עבודה טובה יותר בזה, ויתייחסו אלינו טוב. אבל אלה צריכות להיות החלטות שנעשות עם הבנה מדעית עמוקה של AI בטכן, ואחרי דיון כלל-עולמי כולל משמעותי, לא במירוץ בין איל טכנולוגיה עם רוב האנושות לגמרי לא מעורבת ולא מודעת.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) סיכום ממשל A-G-I ועל-אינטליגנציה באמצעות אחריות ורגולציה. האחריות הכי גבוהה, והרגולציה הכי חזקה, בחיתוך המשולש של אוטונומיה, כלליות, ואינטליגנציה. מקלטים בטוחים מאחריות חמורה ורגולציה חזקה יכולים להיות מושגים באמצעות טענות בטיחות חיוביות המדגימות שמערכת היא חלשה ו/או צרה ו/או פאסיבית. תקרות על כוח חישוב כולל לאימון וקצב כוח חישוב להסקה, מאומתות ונאכפות חוקית ובאמצעות אמצעי אבטחה של חומרה וקריפטוגרפיים, תומכות בבטיחות על ידי הימנעות מAGI מלא ואיסור יעיל של על-אינטליגנציה.

[^1]: ככל הנראה, התפשטות ההכרה הזו תדרוש מאמץ אינטנסיבי של קבוצות חינוך והסברה שמציגות את הטענה הזו, או אסון די משמעותי שנגרם על ידי AI. אנו יכולים לקוות שזה יהיה הראשון.

[^2]: באופן פרדוקסלי, אנו רגילים שהטבע מגביל את הטכנולוגיה שלנו על ידי כך שהיא קשה מאוד לפיתוח, במיוחד מבחינה מדעית. אבל זה כבר לא המקרה עבור AI: הבעיות המדעיות המרכזיות מתגלות כקלות יותר מהצפוי. אנחנו לא יכולים לסמוך על הטבע שיציל אותנו מעצמנו כאן – נצטרך לעשות זאת בעצמנו.

[^3]: איפה, בדיוק, אנחנו עוצרים בפיתוח מערכות חדשות? כאן, עלינו לאמץ עקרון זהירות. ברגע שמערכת פרוסה, ובמיוחד ברגע שרמת יכולת מערכת כזו מתפשטת, קשה ביותר לחזור אחורה. ואם מערכת *מפותחת* (במיוחד בעלות ומאמץ גדולים), יהיה לחץ עצום להשתמש או לפרוס אותה, ופיתוי שהיא תודלף או תיגנב. פיתוח מערכות ו*אז* החלטה אם הן לא בטוחות עמוקות זו דרך מסוכנת.

[^4]: יהיה גם חכם לאסור פיתוח AI שמסוכן מטבעו, כמו מערכות שכפולות ומתפתחות, אלה שמתוכננות לברוח מהגבלה, אלה שיכולות לשפר את עצמן באופן אוטונומי, AI מטעה וזדוני במכוון, וכו'.

[^5]: שימו לב זה לא בהכרח אומר *נאכף* ברמה הבינלאומית על ידי איזה סוג של גוף עולמי: במקום זה מדינות ריבוניות יכולות לאכוף כללים מוסכמים, כמו בהרבה אמנות.

[^6]: כפי שנראה להלן, טיב חישוב ה-AI יאפשר משהו כמו היברידי; אבל שיתוף פעולה בינלאומי עדיין יידרש.

[^7]: לדוגמה, המכונות הנדרשות לחריטת צ'יפים הרלוונטיים ל-AI מיוצרות על ידי חברה אחת בלבד, ASML (למרות נסיונות רבים אחרים לעשות זאת), הרוב הגדול של הצ'יפים הרלוונטיים מיוצרים על ידי חברה אחת, TSMC (למרות שאחרות מנסות להתחרות), והעיצוב והבנייה של חומרה מהצ'יפים האלה נעשים על ידי רק כמה כולל NVIDIA, AMD, וגוגל.

[^8]: הכי חשוב, כל צ'יפ מחזיק מפתח פרטי קריפטוגרפי יחיד ובלתי נגיש שהוא יכול להשתמש בו כדי "לחתום" על דברים.

[^9]: כברירת מחדל זו תהיה החברה שמוכרת את הצ'יפים, אבל מודלים אחרים אפשריים ואולי שימושיים.

[^10]: מפקח יכול לברר מיקום צ'יפ על ידי תזמון חילופי הודעות חתומות איתו: מהירות האור הסופית מחייבת את הצ'יפ להיות בתוך רדיוס נתון *r* של "תחנה" אם הוא יכול להחזיר הודעה חתומה בזמן פחות מ-*r* / *c*, כאשר *c* היא מהירות האור. בשימוש במספר תחנות, ובהבנה כלשהי של מאפייני רשת, ניתן לקבוע את מיקום הצ'יפ. היופי של השיטה הזו הוא שרוב האבטחה שלה מסופקת על ידי חוקי הפיזיקה. שיטות אחרות יכולות להשתמש ב-GPS, מעקב אינרציאלי וטכנולוגיות דומות.

[^11]: לחילופין, זוגות צ'יפים יוכלו לתקשר זה עם זה רק באמצעות רשות מפורשת של מפקח.

[^12]: זה חשוב משום שלפחות כרגע, חיבור ברוחב פס גבוה מאוד בין צ'יפים נחוץ לאימון מודלי AI גדולים עליהם.

[^13]: זה גם יכול להיות מוקם לדרוש הודעות חתומות מ-*N* של *M* מפקחים שונים, מה שמאפשר לכמה צדדים לחלוק ממשל.

[^14]: זה רחוק מלהיות חסר תקדים – לדוגמה צבאות לא פיתחו צבאות של חיילים על משובטים או מהונדסים גנטית, למרות שזה כנראה אפשרי טכנולוגית. אבל הם *בחרו* לא לעשות זאת, במקום להיות נמנעים על ידי אחרים. המאזן לא נהדר לגבי מעצמות עולמיות גדולות שנמנעות מפיתוח טכנולוגיה שהן מאוד רוצות לפתח.

[^15]: פרט לכמה חריגים ראויים לציון (במיוחד NVIDIA) החומרה המתמחה ב-AI היא חלק קטן יחסית של המודל העסקי והכנסות של החברות האלה. יותר מזה, הפער בין חומרה שמשמשת ב-AI מתקדם ולחומרה "לצרכן" משמעותי, אז רוב צרכני חומרת מחשב יהיו מושפעים בעיקר מעט.

[^16]: לניתוח מפורט יותר, ראו את הדוחות העדכניים מ[RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) ו[CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). אלה מתמקדים בכדאיות טכנית, במיוחד בהקשר של בקרות יצוא אמריקאיות השואפות להגביל את הקיבולת של מדינות אחרות בחישוב ברמה גבוהה; אבל לזה יש חפיפה ברורה עם האילוץ הגלובלי שנצפה כאן.

[^17]: מכשירי אפל, לדוגמה, ננעלים מרחוק ובטוח כשמדווח שאבדו או נגנבו, ויכולים להיות מופעלים מחדש מרחוק. זה נשען על אותן תכונות אבטחת חומרה שנדונות כאן.

[^18]: ראו למשל הצעת [capacity on demand](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) של IBM, [Intel on demand.](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html) של אינטל, ו[private cloud compute](https://security.apple.com/blog/private-cloud-compute/) של אפל.

[^19]: [המחקר הזה](https://epochai.org/trends#hardware-trends-section) מראה שבהיסטוריה אותו ביצועים הושגו באמצעות כ-30% פחות דולרים בשנה. אם המגמה הזו תמשיך, יכולה להיות חפיפה משמעותית בין שימוש בצ'יפים של AI ו"צרכניים", ובאופן כללי כמות החומרה הנדרשת למערכות AI עתירות כוח יכולה להפוך קטנה באופן לא נוח.

[^20] לפי [אותו מחקר](https://epochai.org/trends#hardware-trends-section), ביצועים נתונים בזיהוי תמונות דרש פי 2.5 פחות חישוב כל שנה. אם זה ימשיך לחזור גם על המערכות הכי מסוגלות של AI, מגבלת חישוב לא תהיה שימושית במשך זמן ארוך.

[^21]: במיוחד, ברמת המדינה זה נראה הרבה כמו הלאמת חישוב, בכך שלממשלה יהיה הרבה שליטה על איך כוח חישובי ישמש. עם זאת, לאלה המודאגים ממעורבות ממשלתית, זה נראה הרבה יותר בטוח ועדיף מאשר תוכנת ה-AI החזקה ביותר *עצמה* מולאמת באמצעות מיזוג כלשהו בין חברות AI גדולות וממשלות לאומיות, כפי שחלקים מתחילים לתמוך.

[^22]: צעד רגולטורי גדול באירופה ננקט עם מעבר ב2024 של [חוק ה-AI של האיחוד האירופי.](https://artificialintelligenceact.eu/) הוא מסווג AI לפי סיכון: אוסר מערכות בלתי מקובלות, מווסת מערכות ברמת סיכון גבוהה, ומטיל כללי שקיפות, או שום אמצעים בכלל, על מערכות ברמת סיכון נמוכה. הוא יפחית משמעותית כמה סיכוני AI, ויחזק שקיפות AI אפילו לחברות אמריקאיות, אבל יש לו שני פגמים מרכזיים. ראשית, טווח הגעה מוגבל: למרות שהוא חל על כל חברה שמספקת AI באיחוד האירופי, אכיפה על חברות שבסיסן בארה"ב חלשה, ו-AI צבאי פטור. שנית, למרות שהוא מכסה GPAI, הוא נכשל בלהכיר ב-AGI או בעל-אינטליגנציה כסיכונים בלתי מקובלים או למנוע את פיתוחם—רק את הפריסה שלהם באיחוד האירופי. כתוצאה, הוא עושה מעט כדי לחסם את הסיכונים של AGI או על-אינטליגנציה.

[^23]: חברות לעיתים קרובות מציגות שהן בעד רגולציה סבירה. אבל איכשהו הן כמעט תמיד נראות מתנגדות לכל רגולציה *ספציפית*; עדים למאבק על ה-SB1047 הדי מעורב, ש[רוב חברות ה-AI התנגדו לו בפומבי או בפרטי.](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/)

[^24]: זה היה כ-3.5 שנים מהזמן שחוק ה-AI של האיחוד האירופי הוצע עד שהוא נכנס לתוקף.

[^25]: זה לפעמים מבוטא שזה "מוקדם מדי" להתחיל לווסת AI. בהתחשב בהערה האחרונה, זה בקושי נראה סביר. חשש נוסף שמובע הוא שרגולציה תזיק ל"חדשנות." אבל רגולציה טובה פשוט משנה את הכיוון, לא את הכמות, של חדשנות.

[^26]: תקדים מעניין הוא בהובלת חומרים מסוכנים, שיכולים לברוח ולגרום לנזק. כאן, [רגולציה](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) ו[פסיקה](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) קבעו אחריות חמורה לחומרים מסוכנים מאוד כמו חומרי נפץ, בנזין, רעלים, סוכנים זיהומיים ופסולת רדיואקטיבית. דוגמאות אחרות כוללות [אזהרות על תרופות](https://www.medicalnewstoday.com/articles/boxed-warnings), [סוגי מכשירים רפואיים,](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification) וכו'.

[^27]: הצעה מקיפה אחרת עם מטרות דומות שהוצגת ב["A Narrow Path"](https://www.narrowpath.co/) תומכת בגישה מרוכזת יותר, מבוססת איסור שמכווה את כל פיתוח ה-AI החדשני דרך ישות בינלאומית יחידה, בפיקוח של מוסדות בינלאומיים חזקים, עם איסורים קטגוריאליים ברורים ולא הגבלות מדורגות. גם אני הייתי תומך בתוכנית הזו; עם זאת היא תדרוש עוד יותר רצון פוליטי ותיאום מהזה שמוצע כאן.

[^28]: כמה הנחיות לתקן כזה [פורסמו](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/) על ידי פורום מודל החזית. יחסית להצעה כאן, אלה מתרכזות בצד של פחות די