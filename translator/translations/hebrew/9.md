# פרק 9 - הנדסת העתיד — מה עלינו לעשות במקום זאת

בינה מלאכותית יכולה לעשות טוב מדהים בעולם. כדי לקבל את כל היתרונות ללא הסיכונים, עלינו לוודא שהבינה המלאכותית תישאר כלי אנושי.

אם נצליח לבחור לא להחליף את האנושות במכונות - לפחות לזמן מה! - מה נוכל לעשות במקום זאת? האם נוותר על ההבטחה העצומה של הבינה המלאכותית כטכנולוגיה? ברמה מסוימת התשובה היא *לא* פשוטה: נסגור את השערים לבינה מלאכותית כללית בלתי נשלטת ולעל-אינטליגנציה, אבל *כן* נבנה צורות רבות אחרות של בינה מלאכותית, כמו גם את מבני הממשל והמוסדות שנצטרך כדי לנהל אותן.

אבל עדיין יש הרבה מה לומר; להגשים זאת יהיה עיסוק מרכזי של האנושות. החלק הזה חוקר כמה נושאים מרכזיים:

- איך נוכל לאפיין בינה מלאכותית "כלית" ואת הצורות שהיא יכולה לקחת.
- שנוכל לקבל (כמעט) כל מה שהאנושות רוצה בלי AGI, עם AI כלי.
- שמערכות AI כלי הן (כנראה, עקרונית) ניתנות לניהול.
- שהתרחקות מ-AGI לא אומרת ויתור על ביטחון לאומי - להיפך.
- שריכוז כוח הוא דאגה אמיתית. האם נוכל למתן אותו מבלי לפגוע בבטיחות ובביטחון?
- שנרצה - ונצטרך - מבני ממשל וחברה חדשים, ובינה מלאכותית יכולה דווקא לעזור.

## בינה מלאכותית בתוך השערים: AI כלי

דיאגרמת החיתוך המשולש נותנת דרך טובה להגדיר את מה שנוכל לקרוא "AI כלי": בינה מלאכותית שהיא כלי שליט לשימוש אנושי, במקום יריב או תחליף בלתי שליט. המערכות הפחות בעייתיות הן אלה שהן אוטונומיות אבל לא כלליות או בעלות יכולת-על (כמו בוט מכרזים), או כלליות אבל לא אוטונומיות או מסוגלות (כמו מודל שפה קטן), או מסוגלות אבל צרות ומאוד שליטות (כמו AlphaGo).[^1] אלה עם שתי תכונות מצטלבות יש להן יישום רחב יותר אבל סיכון גבוה יותר ויידרשו מאמצים גדולים לניהול. (זה שמערכת בינה מלאכותית היא יותר כלי לא אומר שהיא בטוחה מטבעה, רק שהיא לא בלתי בטוחה מטבעה - חשבו על מסור חשמלי, לעומת נמר מחמד.) השער חייב להישאר סגור ל-(מלא) AGI ועל-אינטליגנציה בחיתוך המשולש, ויש לנקוט זהירות עצומה במערכות בינה מלאכותית המתקרבות לסף הזה.

אבל זה משאיר הרבה בינה מלאכותית חזקה! נוכל להפיק תועלת עצומה מ"אורקולים" פסיביים חכמים וכלליים ומערכות צרות, מערכות כלליות ברמה אנושית אבל לא על-אנושית, וכן הלאה. חברות טכנולוגיה רבות ומפתחים בונים באופן פעיל כלים מהסוג הזה וצריכים להמשיך; כמו רוב האנשים הם מניחים במובלע שהשערים ל-AGI ועל-אינטליגנציה יסגרו.[^2]

כמו כן, מערכות בינה מלאכותית יכולות להשתלב בצורה יעילה למערכות מורכבות השומרות על פיקוח אנושי תוך הגברת היכולת. במקום להסתמך על קופסאות שחורות בלתי מובנות, נוכל לבנות מערכות שבהן רכיבים מרובים - כולל בינה מלאכותית ותוכנה מסורתית - עובדים יחד בדרכים שבני אדם יכולים לפקח ולהבין.[^3] אמנם חלק מהרכיבים עשויים להיות קופסאות שחורות, אבל אף אחד לא יהיה קרוב ל-AGI - רק המערכת המורכבת בכללותה תהיה גם כללית מאוד וגם מסוגלת מאוד, ובאופן שליט לחלוטין.[^4]

### שליטה אנושית משמעותית ומובטחת

מה אומר "שליט לחלוטין"? רעיון מרכזי במסגרת ה"כלי" הוא לאפשר מערכות - גם אם כלליות וחזקות למדי - המובטחות להיות תחת שליטה אנושית משמעותית. מה זה אומר? זה כולל שני היבטים. הראשון הוא שיקול עיצוב: בני אדם צריכים להיות מעורבים בעמקותובאופן מרכזי במה שהמערכת עושה, *בלי* להאציל החלטות חשובות מרכזיות לבינה המלאכותית. זה האופי של רוב מערכות הבינה המלאכותית הנוכחיות. שנית, ככל שמערכות בינה מלאכותית אוטונומיות, חייבות להיות להן ערבויות המגבילות את היקף פעולתן. ערבות צריכה להיות *מספר* המאפיין את ההסתברות שמשהו יקרה, וסיבה להאמין במספר הזה. זה מה שאנו דורשים בתחומים ביקורתיים אחרים לבטיחות, שבהם מספרים כמו "זמן ממוצע בין כשלים" ומספרים צפויים של תאונות מחושבים, נתמכים ומפורסמים בתיקי בטיחות.[^5] המספר האידיאלי לכשלים הוא אפס, כמובן. והחדשות הטובות הן שאולי נגיע קרוב למדי, אם כי תוך שימוש בארכיטקטורות בינה מלאכותית שונות למדי, תוך שימוש ברעיונות של תכונות *מאומתות פורמלית* של תוכניות (כולל בינה מלאכותית). הרעיון, שנחקר בהרחבה על ידי אומוהונדרו, טגמארק, בנג'יו, דלרימפל ואחרים (ראו [כאן](https://arxiv.org/abs/2309.01933) ו[כאן](https://arxiv.org/abs/2405.06624)) הוא לבנות תוכנית עם תכונות מסוימות (למשל: שאדם יכול לכבות אותה) ו*להוכיח* פורמלית שהתכונות הללו קיימות. זה ניתן לעשות עכשיו לתוכניות קצרות למדי ותכונות פשוטות, אבל הכוח (הקרב) של תוכנת הוכחה מופעלת בינה מלאכותית יכול לאפשר זאת לתוכניות מורכבות הרבה יותר (למשל עטפות) ואפילו לבינה מלאכותית עצמה. זוהי תוכנית שאפתנית מאוד, אבל כשהלחץ גובר על השערים, אנחנו נזדקק לחומרים חזקים שיחזקו אותם. הוכחה מתמטית עשויה להיות אחד מהמעטים שחזקים דיים.

### לאן תעשיית הבינה המלאכותית

עם הפניית התקדמות הבינה המלאכותית, AI כלי עדיין תהיה תעשיה ענקית. במונחי חומרה, גם עם הגבלות כוח חישוב למניעת על-אינטליגנציה, אימון והסקה במודלים קטנים יותר עדיין יידרשו כמויות ענקיות של רכיבים מיוחדים. בצד התוכנה, ביטול הפיצוץ בגודל מודל ובחישוב של בינה מלאכותית צריך פשוט להוביל לחברות להפנות משאבים לשיפור המערכות הקטנות יותר, להפוך אותן למגוונות ומיוחדות יותר, במקום פשוט להגדיל אותן.[^6] יהיה הרבה מקום - יותר כנראה - לכל הסטארט-אפים המייצרי כסף של עמק הסיליקון.[^7]

## AI כלי יכול להניב (כמעט) כל מה שהאנושות רוצה, בלי AGI

אינטליגנציה, בין אם ביולוגית או מכונה, יכולה להיחשב באופן רחב כיכולת לתכנן ולבצע פעילויות המביאות לעתידים המתיישרים יותר עם קבוצת מטרות. ככזו, אינטליגנציה מועילה מאוד כשמשתמשים בה במרדף אחרי מטרות נבחרות בחכמה. בינה מלאכותית מושכת השקעות עצומות של זמן ומאמץ בעיקר בגלל היתרונות המובטחים שלה. אז עלינו לשאול: באיזה מידה עדיין נקצור את היתרונות של בינה מלאכותית אם נבלום את התחמקותה לעל-אינטליגנציה? התשובה: אנחנו עשויים להפסיד מעט באופן מפתיע.

חשבו תחילה שמערכות בינה מלאכותית נוכחיות כבר חזקות מאוד, ואנחנו באמת רק גירדנו את פני השטח של מה שאפשר לעשות איתן.[^8] הן מסוגלות סבירות "לנהל את המופע" במונחי "הבנה" של שאלה או משימה המוצגת להן, ומה יידרש כדי לענות על השאלה הזו או לעשות את המשימה הזו.

הלאה, הרבה מההתרגשות מסביב למערכות בינה מלאכותית מודרניות נובע מהכלליות שלהן; אבל חלק ממערכות הבינה המלאכותית המסוגלות ביותר - כמו כאלה שיוצרות או מזהות דיבור או תמונות, עושות חיזוי ומידול מדעי, משחקות משחקים וכו' - הן הרבה יותר צרות ובטוח "בתוך השערים" במונחי חישוב.[^9] המערכות הללו הן על-אנושיות במשימות הספציפיות שהן עושות. יכול להיות להן חולשות בקצה הקצה[^10] (או [ניתנות לניצול](https://arxiv.org/abs/2211.00241)) בגלל הצרות שלהן; אבל צר *לחלוטין* או כללי *לחלוטין* אינם האפשרויות היחידות הזמינות: יש ארכיטקטורות רבות ביניהן.[^11]

כלי בינה מלאכותית אלה יכולים להאיץ מאוד התקדמות בטכנולוגיות חיוביות אחרות, בלי AGI. כדי לעשות פיזיקה גרעינית טובה יותר, אנחנו לא צריכים שהבינה המלאכותית תהיה פיזיקאי גרעיני - יש לנו כאלה! אם אנחנו רוצים להאיץ רפואה, ניתן לביולוגים, לחוקרי רפואה ולכימאים כלים חזקים. הם רוצים אותם וישתמשו בהם לתועלת עצומה. אנחנו לא צריכים חוות שרתים מלאה במיליון גאונים דיגיטליים; יש לנו מיליוני בני אדם שהבינה המלאכותית יכולה לעזור להוציא את הגאונות שלהם. כן, ייקח יותר זמן לקבל אלמוות ותרופה לכל המחלות. זוהי עלות אמיתית. אבל אפילו חידושי הבריאות המבטיחים ביותר יהיו חסרי תועלת אם אי-יציבות מונעת בינה מלאכותית תוביל לעימות גלובלי או התמוטטות חברתית. אנחנו חייבים לעצמנו לתת לבני אדם מחוזקי בינה מלאכותית הזדמנות לפתור את הבעיה קודם.

ונניח שיש, למעשה, איזה תועלת עצומה ל-AGI שלא ניתן להשיגה על ידי האנושות באמצעות כלים בתוך השערים. האם נאבד אותם על ידי *אי*-בנייה של AGI ועל-אינטליגנציה? בשקלול הסיכונים והתגמולים כאן, יש יתרון אסימטרי עצום בהמתנה מול חיפזון: נוכל לחכות עד שאפשר יהיה לעשות זאת בדרך מובטחת בטוחה ומועילה, וכמעט כולם עדיין יוכלו לקצור את התגמולים; אם נמהר, זה יכול להיות - במילותיו של מנכ"ל OpenAI סם אלטמן - [כיבוי האורות לכולנו.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

אבל אם כלים שאינם AGI הם כל כך חזקים באופן פוטנציאלי, האם נוכל לנהל אותם? התשובה היא ברורה...אולי.

## מערכות AI כלי הן (כנראה, עקרונית) ניתנות לניהול

אבל זה לא יהיה קל. מערכות בינה מלאכותית חדישות יכולות להעצים מאוד אנשים ומוסדות בהשגת המטרות שלהם. זה, באופן כללי, דבר טוב! אבל, יש דינמיקות טבעיות של הימצאות מערכות כאלה לרשותנו - בפתאומיות ובלי הרבה זמן לחברה להסתגל - שמציעות סיכונים רציניים שצריך לנהל. כדאי לדון בכמה סוגים עיקריים של סיכונים כאלה, ואיך אפשר להקטין אותם, בהנחה של סגירת שער.

סוג אחד של סיכונים הוא של AI כלי בעל כוח גבוה שמאפשר גישה לידע או יכולת שבעבר היו קשורים לאדם או ארגון, מה שהופך שילוב של יכולת גבוהה פלוס נאמנות גבוהה זמינים למגוון רחב מאוד של שחקנים. היום, עם מספיק כסף אדם עם כוונות רעות יכול לשכור צוות כימאים לתכנן ולייצר נשק כימי חדש - אבל זה לא כל כך קל להשיג את הכסף הזה או למצוא/להרכיב את הצוות ולשכנע אותם לעשות משהו די ברור שהוא לא חוקי, לא אתי ומסוכן. כדי למנוע ממערכות בינה מלאכותית למלא תפקיד כזה, שיפורים על השיטות הנוכחיות עשויים להספיק,[^12] כל עוד כל המערכות הללו והגישה אליהן מנוהלות באחריות. מצד שני, אם מערכות חזקות משוחררות לשימוש כללי ושינוי, כל אמצעי הבטיחות המובנים כנראה ניתנים להסרה. אז כדי למנוע סיכונים מהסוג הזה, יידרשו הגבלות חזקות על מה שאפשר לשחרר לציבור - באנלוגיה להגבלות על פרטים של טכנולוגיות גרעיניות, חבלניות ומסוכנות אחרות.[^13]

סוג שני של סיכונים נובע מהרחבת קנה מידה של מכונות שמתנהגות כמו או מתחזות לאנשים. ברמת הפגיעה לאנשים בודדים, הסיכונים הללו כוללים הונאות, ספאם ופישינג יעילים הרבה יותר, והפצה של דיפ-פיקים לא הסכמיים.[^14] ברמה קולקטיבית, הם כוללים שיבוש של תהליכים חברתיים מרכזיים כמו דיון וויכוח ציבורי, מערכות איסוף, עיבוד והפצת מידע וידע חברתיות, ומערכות הבחירה הפוליטיות שלנו. הפחתת הסיכון הזה כנראה תכלול (א) חוקים המגבילים התחזות לאנשים על ידי מערכות בינה מלאכותית, והטלת אחריות על מפתחי בינה מלאכותית שיוצרים מערכות המייצרות התחזויות כאלה, (ב) מערכות סימון מים ומקור שמזהות ומסווגות (באחריות) תוכן שנוצר בינה מלאכותית, ו(ג) מערכות אפיסטמיות חברתיות-טכניות חדשות שיכולות ליצור שרשרת מהימנה מנתונים (למשל מצלמות והקלטות) דרך עובדות, הבנה ומודלי עולם טובים.[^15] כל זה אפשרי, והבינה המלאכותית יכולה לעזור עם חלקים ממנו.

סיכון כללי שלישי הוא שככל שמשימות מסוימות מתבצעות באופן אוטומטי, לבני האדם שעושים כרגע את המשימות הללו יכול להיות פחות ערך פיננסי כעבודה. היסטורית, הפיכת משימות לאוטומטיות הפכה דברים המתאפשרים על ידי המשימות הללו לזולים ושופעים יותר, תוך מיון האנשים שעשו בעבר את המשימות הללו לאלה שעדיין מעורבים בגרסה האוטומטית (בדרך כלל במיומנות/שכר גבוהים יותר), ואלה שעבודתם שווה פחות או מעט. בסך הכל קשה לחזות באילו סקטורים יידרש יותר לעומת פחות עבודה אנושית בסקטור הגדול יותר אבל יעיל יותר שנוצר. במקביל, הדינמיקה האוטומטית נוטה להגביר אי-שוויון ופרודוקטיביות כללית, להפחית את עלותם של סחורות ושירותים מסוימים (דרך הגברת יעילות), ולהגביר את העלות של אחרים (דרך [מחלת עלויות](https://en.wikipedia.org/wiki/Baumol_effect)). לגבי אלה שבצד הלא נוח של עליית אי-השוויון, לגמרי לא ברור האם הירידה בעלות באותן סחורות ושירותים מסוימים עולה על העלייה באחרים, ומובילה לרווחה כללית גדולה יותר. אז איך זה יהיה עם בינה מלאכותית? בגלל הקלות היחסית שבה אפשר להחליף עבודה אנושית אינטלקטואלית בבינה מלאכותית כללית, נוכל לצפות לגרסה מהירה של זה עם בינה מלאכותית כללית מתחרה באדם.[^16] אם נסגור את השער ל-AGI, הרבה פחות עבודות יוחלפו בסיטונות על ידי סוכני בינה מלאכותית; אבל עקירת עבודה ענקית עדיין סבירה על פני תקופה של שנים.[^17] כדי למנוע סבל כלכלי נרחב, כנראה יהיה צריך ליישם גם צורה כלשהי של נכסים או הכנסה בסיסיים אוניברסליים, וגם להנדס שינוי תרבותי לכיוון הערכה ותגמול של עבודה אנושית-מרכזית שקשה יותר לבצע אוטומטית (במקום לראות מחירי עבודה יורדים בגלל העלייה בעבודה זמינה שנדחקה מחלקים אחרים של הכלכלה.) מבנים אחרים, כמו זה של ["כבוד נתונים"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society) (שבו היצרנים האנושיים של נתוני אימון מקבלים באופן אוטומטי תמלוגים לערך שנוצר על ידי הנתונים הללו בבינה מלאכותית) עשויים לעזור. לאוטומציה על ידי בינה מלאכותית יש גם השפעה שלילית פוטנציאלית שנייה, שהיא של אוטומציה *לא מתאימה*. יחד עם יישומים שבהם בינה מלאכותית פשוט עושה עבודה גרועה יותר, זה יכלול אלה שבהם מערכות בינה מלאכותית צפויות להפר עקרונות מוסריים, אתיים או חוקיים - למשל בהחלטות חיים ומוות, ובעניינים שיפוטיים. אלה חייבים להיטפל על ידי יישום והרחבה של המסגרות החוקיות הנוכחיות שלנו.

לבסוף, איום משמעותי של בינה מלאכותית בתוך השערים הוא השימוש בה לשכנוע מותאם אישית, לכידת קשב ומניפולציה. ראינו ברשתות חברתיות ובפלטפורמות מקוונות אחרות את הצמיחה של כלכלת קשב מושרשת עמוקות (שבה שירותים מקוונים נלחמים בצורה אכזרית על קשב משתמשים) ומערכות ["קפיטליזם מעקב"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism) (שבהן מידע המשתמש ויצירת פרופיל נוספים לסחרור הקשב). כמעט ודאי שעוד בינה מלאכותית תוכנס לשירות שניהם. בינה מלאכותית כבר נמצאת בשימוש נרחב באלגוריתמי פיד ממכרים, אבל זה יתפתח לתוכן שנוצר בבינה מלאכותית ממכר, מותאם להתכלות כפייתית על ידי אדם יחיד. והקלט, התגובות והנתונים של האדם הזה יוזנו למכונת הקשב/פרסום כדי להמשיך את המעגל הרעיל. כמו כן, כשעוזרי בינה מלאכותית שמסופקים על ידי חברות טכנולוגיה הופכים לממשק ליותר חיים מקוונים, הם כנראה יחליפו מנועי חיפוש ופידים כמנגנון שבו מתרחשים שכנוע והשתכרות על לקוחות. הכישלון של החברה שלנו לשלוט בדינמיקות הללו עד כה לא מבשר טובות. חלק מהדינמיקה הזו עשוי להיות מופחת דרך תקנות הנוגעות לפרטיות, זכויות נתונים ומניפולציה. להגיע יותר לשורש הבעיה עשוי לדרוש פרספקטיבות שונות, כמו זו של עוזרי בינה מלאכותית נאמנים (שנדונה להלן.)

המסקנה מהדיון הזה היא של תקווה: מערכות מבוססות-כלי בתוך השערים - לפחות כל עוד הן נשארות דומות בכוח וביכולת למערכות החדישות ביותר של היום - כנראה ניתנות לניהול אם יש רצון ותיאום לעשות זאת. מוסדות אנושיים הגונים, המוגברים על ידי כלי בינה מלאכותית,[^18] יכולים לעשות זאת. נוכל גם לכשל בעשיית זה. אבל קשה לראות איך לאפשר מערכות חזקות יותר יעזור - מלבד לשים אותן בפיקוד ולקוות לטוב.

## ביטחון לאומי

מרוצים לעליונות בינה מלאכותית - המונעים על ידי ביטחון לאומי או מוטיבציות אחרות - דוחפים אותנו לכיוון מערכות בינה מלאכותית חזקות ובלתי נשלטות שינטו לקלוט, במקום להעניק, כוח. מרוץ AGI בין ארה"ב וסין הוא מרוץ לקבוע איזה מדינה מקבלת על-אינטליגנציה ראשונה.

אז מה צריכים לעשות אלה שאחראים על הביטחון הלאומי במקום זאת? לממשלות יש ניסיון חזק בבניית מערכות שליטות ובטוחות, והן צריכות להכפיל את המאמצים לעשות זאת בבינה מלאכותית, ולתמוך בסוג של פרויקטי תשתית שמצליחים הכי טוב כשנעשים בקנה מידה ועם אישור ממשלתי.

במקום "פרויקט מנהטן" פזיז לכיוון AGI,[^19] הממשלה האמריקנית יכולה לשגר פרויקט אפולו למערכות שליטות, בטוחות ואמינות. זה יכול לכלול למשל:

- תוכנית גדולה ל(א) פיתוח מנגנוני אבטחת חומרה על-שבבית ו(ב) התשתית, לנהל את צד כוח החישוב של בינה מלאכותית חזקה. אלה יכולים להיבנות על [חוק השבבים](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) האמריקני ו[משטר בקרת יצוא](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion).
- יוזמה גדולה לפיתוח טכניקות אימות פורמלי כך שתכונות מסוימות של מערכות בינה מלאכותית (כמו מתג כיבוי) יכולות *להוכח* שהן נוכחות או נעדרות. זה יכול למנף את הבינה המלאכותית עצמה לפיתוח הוכחות של תכונות.
- מאמץ בקנה מידה לאומי ליצור תוכנה שהיא אמינה בטוחה באופן מוכח, מופעלת על ידי כלי בינה מלאכותית שיכולים לקודד מחדש תוכנה קיימת למסגרות בטוחות באופן מוכח.
- פרויקט השקעה לאומי בהתקדמות מדעית באמצעות בינה מלאכותית,[^20] שרץ כשותפות בין המשרד לאנרגיה, קרן המדע הלאומית ומכון הבריאות הלאומי.

באופן כללי, יש משטח תקיפה עצום על החברה שלנו שהופך אותנו לפגיעים לסיכונים מבינה מלאכותית ומשימוש לרעה בה. הגנה מחלק מהסיכונים הללו תדרוש השקעה וסטנדרטיזציה בגודל ממשלתי. אלה יספקו הרבה יותר ביטחון משפיכת בנזין על האש של מרוצים לכיוון AGI. ואם בינה מלאכותית הולכת להיבנות בנשק ומערכות פיקוד ושליטה, חיוני שהבינה המלאכותית תהיה אמינה ובטוחה, מה שהבינה המלאכותית הנוכחית פשוט לא.

## ריכוז כוח והקלות שלו

המאמר הזה התמקד ברעיון של שליטה אנושית בבינה מלאכותית וכישלונה הפוטנציאלי. אבל עדשה תקפה אחרת שדרכה לראות את מצב הבינה המלאכותית היא דרך *ריכוז כוח.* הפיתוח של בינה מלאכותית חזקה מאוד מאיים לרכז כוח או לידיים המועטות והגדולות מאוד של החברות שפיתחו ושישלטו בה, או לידי ממשלות המשתמשות בבינה מלאכותית כאמצעי חדש לשמור על הכוח והשליטה שלהן, או למערכות הבינה המלאכותית עצמן. או איזה תערובת לא קדושה מהאמור לעיל. בכל המקרים הללו רוב האנושות מאבדת כוח, שליטה וכוח פעולה. איך נוכל להילחם בזה?

הצעד הראשון והחשוב ביותר, כמובן, הוא סגירת שער ל-AGI ועל-אינטליגנציה חכמות מאדם. אלה יכולות במפורש להחליף ישירות בני אדם וקבוצות של בני אדם. אם הן תחת שליטה ארגונית או ממשלתית הן ירכזו כוח בארגונים או ממשלות הללו; אם הן "חופשיות" הן ירכזו כוח לעצמן. אז בואו נניח שהשערים סגורים. ואז מה?

פתרון מוצע אחד לריכוז כוח הוא בינה מלאכותית "קוד פתוח", שבה משקולות מודל זמינות חינם או באופן נרחב. אבל כפי שהוזכר קודם, ברגע שמודל פתוח, רוב אמצעי הבטיחות או המעקות יכולים להיות (ובדרך כלל הם) מוסרים. אז יש מתח חד בין מצד אחד ביזור, ומצד שני בטיחות, ביטחון ושליטה אנושית במערכות בינה מלאכותית. יש גם סיבות להיות ספקניים שמודלים פתוחים יילחמו בעצמם בצורה משמעותית בריכוז כוח בבינה מלאכותית יותר ממה שהם עשו במערכות הפעלה (עדיין נשלטות על ידי מיקרוסופט, אפל וגוגל למרות אלטרנטיבות פתוחות).[^21]

ובכל זאת עשויות להיות דרכים ליישר את העיגול הזה - לרכז ולהקל סיכונים תוך ביזור יכולת ותגמול כלכלי. זה דורש חשיבה מחדש גם איך בינה מלאכותית מפותחת וגם איך היתרונות שלה מחולקים.

מודלים חדשים של פיתוח ובעלות ציבורית על בינה מלאכותית יעזרו. זה יכול לקחת צורות שונות: בינה מלאכותית שפותחה על ידי הממשלה (כפופה לפיקוח דמוקרטי),[^22] ארגוני פיתוח בינה מלאכותית ללא רווח (כמו Mozilla לדפדפנים), או מבנים המאפשרים בעלות וממשל רחבים מאוד. המפתח הוא שהמוסדות הללו יוסמכו במפורש לשרת את האינטרס הציבורי תוך פעולה תחת אילוצי בטיחות חזקים.[^23] משטרי רגולציה וסטנדרטים/אישורים מעוצבים היטב יהיו גם חיוניים, כך שמוצרי בינה מלאכותית המוצעים על ידי שוק תוסס יישארו שימושיים באמת במקום ניצוליים כלפי המשתמשים שלהם.

במונחי ריכוז כוח כלכלי, נוכל להשתמש במעקב מקור ו"כבוד נתונים" כדי להבטיח שיתרונות כלכליים זורמים באופן רחב יותר. בפרט, רוב כוח הבינה המלאכותית עכשיו (ובעתיד אם נשמור את השערים סגורים) נובע מנתונים שנוצרו על ידי אדם, בין אם נתוני אימון ישירים או משוב אנושי. אם חברות בינה מלאכותית יידרשו לפצות ספקי נתונים בהגינות,[^24] זה יכול לפחות לעזור להפיץ את התגמולים הכלכליים באופן רחב יותר. מעבר לכך, מודל אחר יכול להיות בעלות ציבורית על חלקים משמעותיים של חברות בינה מלאכותית גדולות. למשל, ממשלות המסוגלות להטיל מס על חברות בינה מלאכותית יכולות להשקיע חלק מהתקבולים בקרן עושר ריבוני שמחזיקה מניות בחברות, ומשלמת דיבידנדים לאוכלוסייה.[^25]

חיוני במנגנונים הללו הוא להשתמש בכוחה של הבינה המלאכותית עצמה כדי לעזור להפיץ כוח טוב יותר, במקום פשוט להילחם בריכוז כוח מונע בינה מלאכותית באמצעים שאינם בינה מלאכותית. גישה חזקה אחת תהיה דרך עוזרי בינה מלאכותית מעוצבים היטב שפועלים עם חובת אמון אמיתית כלפי המשתמשים שלהם - שמים את האינטרסים של המשתמשים ראשונים, במיוחד מעל לאלה של ספקים ארגוניים.[^26] העוזרים הללו חייבים להיות אמינים באמת, מוכשרים טכנית אבל מוגבלים בצורה מתאימה על בסיס מקרה שימוש ורמת סיכון, וזמינים לכולם דרך ערוצים ציבוריים, ללא רווח, או מוסמכים לרווח. בדיוק כמו שלעולם לא נקבל עוזר אנושי שעובד בסתר נגד האינטרסים שלנו לטובת צד אחר, אנחנו לא צריכים לקבל עוזרי בינה מלאכותית שמעקבים, מניפולים או מפיקים ערך מהמשתמשים שלהם לטובה ארגונית.

שינוי כזה יחליף בצורה יסודית את הדינמיקה הנוכחית שבה יחידים נשארים לנהל משא ומתן לבדם עם מכונות ארגוניות ובירוקרטיות עצומות (מופעלות בינה מלאכותית) שנותנות עדיפות לחילוץ ערך על פני רווחת אדם. בעוד יש גישות רבות אפשריות להפצה מחדש של כוח מונע בינה מלאכותית באופן רחב יותר, אף אחת לא תופיע כברירת מחדל: הן חייבות להיות מהונדסות ומנוהלות בכוונה עם מנגנונים כמו דרישות אמון, אספקה ציבורית, וגישה מדורגת על בסיס סיכון.

גישות להקלת ריכוז כוח יכולות להתמודד עם רוחות נגד משמעותיות מכוחות קיימים.[^27] אבל יש נתיבים לכיוון פיתוח בינה מלאכותית שלא דורש בחירה בין בטיחות וכוח מרוכז. על ידי בניית המוסדות הנכונים עכשיו, נוכל להבטיח שהיתרונות של בינה מלאכותית יחולקו באופן נרחב בעוד הסיכונים שלה מנוהלים בזהירות.

## מבני ממשל וחברה חדשים

מבני הממשל הנוכחיים שלנו נאבקים: הם איטיים להגיב, לעתים קרובות נלכדים על ידי אינטרסים מיוחדים, ו[זוכים לאמון הולך ופוחת מהציבור.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) ובכל זאת זו לא סיבה לנטוש אותם - להיפך. חלק מהמוסדות עשויים לזקוק להחלפה, אבל באופן רחב יותר אנחנו צריכים מנגנונים חדשים שיכולים לשפר ולהשלים את המבנים הקיימים שלנו, לעזור להם לתפקד טוב יותר בעולם המתפתח במהירות שלנו.

הרבה מהחולשה המוסדית שלנו נובעת לא ממבני ממשל פורמליים, אלא ממוסדות חברתיים מדורדרים: המערכות שלנו לפיתוח הבנה משותפת, תיאום פעולה, וניהול שיח משמעותי. עד כה, בינה מלאכותית האיצה את ההידרדרות הזו, הציפה את ערוצי המידע שלנו בתוכן מיוצר, הפנתה אותנו לתוכן המקטב והמפלג ביותר, והקשתה על הבחנה בין אמת לבדיה.

אבל בינה מלאכותית יכולה דווקא לעזור לבנות ולחזק את המוסדות החברתיים הללו. חשבו על שלושה תחומים חיוניים:

ראשית, בינה מלאכותית יכולה לעזור לשחזר אמון במערכות האפיסטמיות שלנו - הדרכים שלנו לדעת מה אמיתי. נוכל לפתח מערכות מופעלות בינה מלאכותית שעוקבות ומאמתות את מקור המידע, מנתונים גולמיים דרך ניתוח למסקנות. המערכות הללו יכולות לשלב אימות קריפטוגרפי עם ניתוח מתוחכם כדי לעזור לאנשים להבין לא רק האם משהו אמיתי, אלא איך אנחנו יודעים שהוא אמיתי.[^28] עוזרי בינה מלאכותית נאמנים יכולים להיות מופקדים על מעקב אחר הפרטים כדי להבטיח שהם מתיישבים.

שנית, בינה מלאכותית יכולה לאפשר צורות חדשות של תיאום בקנה מידה גדול. הרבה מהבעיות הדחופות ביותר שלנו - משינוי אקלים לעמידות לאנטיביוטיקה - הן בעיקר בעיות תיאום. אנחנו [תקועים במצבים שהם גרועים ממה שהם יכולים להיות כמעט לכולם](https://equilibriabook.com/), כי שום יחיד או קבוצה לא יכול להרשות לעצמו לעשות את המהלך הראשון. מערכות בינה מלאכותית יכולות לעזור על ידי מידול של מבני תמריצים מורכבים, זיהוי נתיבים בני קיימא לתוצאות טובות יותר, והקלה על מנגנוני בניית אמון והתחייבות הנדרשים כדי להגיע לשם.

אולי הכי מעניין, בינה מלאכותית יכולה לאפשר צורות חדשות לחלוטין של שיח חברתי. דמיינו יכולת "לדבר עם עיר"[^29] - לא רק צפייה בסטטיסטיקות, אלא ניהול דיאלוג משמעותי עם מערכת בינה מלאכותית שמעבדת ומסנתזת את הדעות, החוויות, הצרכים והשאיפות של מיליוני תושבים. או חשבו איך בינה מלאכותית יכולה להקל על דיאלוג אמיתי בין קבוצות שכרגע מדברות זו על פני זו, על ידי עזרה לכל צד להבין טוב יותר את הדאגות והערכים האמיתיים של הצד השני במקום הקריקטורות שלהם זו על זו.[^30] או בינה מלאכותית יכולה להציע תווך מיומן וניטרלי באופן אמין של מחלוקות בין אנשים או אפילו קבוצות גדולות של אנשים (שכולן יכולות לאתר איתה ישירות ובאופן אינדיבידואלי!) הבינה המלאכותית הנוכחית לחלוטין מסוגלת לעשות את העבודה הזו, אבל הכלים לעשות זאת לא יבואו לעולם מעצמם, או דרך תמריצי שוק.

האפשרויות הללו עשויות להישמע אוטופיות, במיוחד בהתחשב בתפקיד הנוכחי של בינה מלאכותית בהידרדרות השיח והאמון. אבל זה בדיוק למה עלינו לפתח באופן פעיל את היישומים החיוביים הללו. על ידי סגירת השערים לבינה מלאכותית כללית בלתי שליטה ונתינת עדיפות לבינה מלאכותית שמגביר כוח פעולה אנושי, נוכל לכוון התקדמות טכנולוגית לכיוון עתיד שבו בינה מלאכותית משמשת ככוח להעצמה, עמידות והתקדמות קולקטיבית.

[^1]: עם זאת, הישארות הרחק מהחיתוך המשולש היא לרוע המזל לא פשוטה כמו שאפשר להאמין. דחיפה קשה של יכולת בכל אחד משלושת ההיבטים נוטה להגביר אותה באחרים. בפרט, יכול להיות קשה ליצור אינטליגנציה כללית ומסוגלת מאוד שלא ניתן בקלות להפוך אוטונומית. גישה אחת היא לאמן מודלים ["קצרי ראייה"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) עם יכולת תכנון פגומה. אחרת תהיה להתמקד בהנדסה של מערכות ["אורקול"](https://arxiv.org/abs/1711.05541) טהורות שיתרחקו מלענות על שאלות מוכוונות פעולה.

[^2]: חברות רבות לא מצליחות להבין שגם הן יוחלפו בסופו של דבר על ידי AGI, גם אם ייקח יותר זמן - אם כן, הן עשויות לדחוף על השערים האלה קצת פחות!

[^3]: מערכות בינה מלאכותית יכולות לתקשר בדרכים יעילות יותר אבל פחות מובנות, אבל שמירה על הבנה אנושית צריכה לקבל עדיפות.

[^4]: הרעיון הזה של בינה מלאכותית מודולרית וניתנת לפרשנות פותח בפירוט על ידי כמה חוקרים; ראו למשל המודל ["שירותי בינה מלאכותית מקיפים"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) של דרקסלר, ["ארכיטקטורת סוכנות פתוחה"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai) של דלרימפל ואחרים. בעוד מערכות כאלה עשויות לדרוש יותר מאמץ הנדסי מאשר רשתות נוירונים מונוליטיות שאומנו עם חישוב עצום, זה בדיוק איפה הגבלות חישוב עוזרות - על ידי הפיכת הנתיב הבטוח והשקוף יותר לגם הנתיב המעשי יותר.

[^5]: על תיקי בטיחות באופן כללי ראו [המדריך הזה](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). הנוגע לבינה מלאכותית בפרט, ראו [ווסיל ואחרים](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [קליימר ואחרים](https://arxiv.org/abs/2403.10462), [בול ואחרים](https://arxiv.org/abs/2410.21572), ו[באלסני ואחרים](https://arxiv.org/abs/2411.03336)

[^6]: אנחנו למעשה כבר רואים את המגמה הזו מונעת רק על ידי העלות הגבוהה של הסקה: מודלים קטנים ומיוחדים יותר "מזוקקים" מגדולים יותר ומסוגלים לרוץ על חומרה פחות יקרה.

[^7]: אני מבין למה אלה שמתרגשים מהמערכת האקולוגית של טכנולוגיית בינה מלאכותית עשויים להתנגד למה שהם רואים כרגולציה מכבידה על התעשייה שלהם. אבל זה ממש מבלבל אותי למה, נאמר, משקיע הון סיכון ירצה לאפשר התחמקות ל-AGI ועל-אינטליגנציה. המערכות הללו (והחברות, כל עוד הן נשארות תחת שליטת החברה) *יאכלו את כל הסטארט-אפים כחטיף*. כנראה אפילו *יותר מוקדם* מאשר לאכול תעשיות אחרות. כל מי שמשקיע במערכת אקולוגית בינה מלאכותית משגשגת צריך לתת עדיפות להבטחה שפיתוח AGI לא מוביל למונופוליזציה על ידי כמה שחקנים דומיננטיים.

[^8]: כמו שכלכלן וחוקר דיפמיינד לשעבר מייקל ווב [ביטא זאת](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/), "אני חושב שאם נעצור את כל הפיתוח של מודלי שפה גדולים יותר היום, אז GPT-4 וקלוד ומה שלא יהיה, והם הדברים האחרונים שאנחנו מאמנים בגודל הזה - אז אנחנו מאפשרים הרבה יותר איטרציה על דברים בגודל הזה וכל מיני כיוונון עדין, אבל שום דבר גדול מזה, שום התקדמויות גדולות יותר - רק מה שיש לנו היום אני חושב מספיק כדי להניע 20 או 30 שנים של צמיחה כלכלית מדהימה."

[^9]: למשל, מערכת alphafold של דיפמיינד השתמשה רק ב-100,000 מספר ה-FLOP של GPT-4.

[^10]: הקושי של מכוניות נוהגות עצמן חשוב לציין כאן: בעוד זו באופן נומינלי משימה צרה, וניתנת להשגה עם מהימנות הגונה עם מערכות בינה מלאכותית קטנות יחסית, ידע והבנה נרחבים של העולם האמיתי נדרשים כדי להגיע למהימנות לרמה הנדרשת במ