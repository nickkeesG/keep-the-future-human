# Sommario esecutivo

Una panoramica di alto livello del saggio. Se hai poco tempo, qui puoi cogliere tutti i punti principali in soli 10 minuti.

I progressi straordinari nell'intelligenza artificiale dell'ultimo decennio (per l'IA specializzata) e degli ultimi anni (per l'IA generale) hanno trasformato l'IA da campo accademico di nicchia alla strategia aziendale centrale di molte delle più grandi aziende mondiali, con centinaia di miliardi di dollari di investimenti annuali nelle tecniche e tecnologie per far progredire le capacità dell'IA.

Ora arriviamo a un momento critico. Mentre le capacità dei nuovi sistemi di IA iniziano a eguagliare e superare quelle degli esseri umani in molti domini cognitivi, l'umanità deve decidere: fino a che punto spingersi, e in quale direzione?

L'IA, come ogni tecnologia, è nata con l'obiettivo di migliorare le cose per chi la creava. Ma la nostra traiettoria attuale, e la scelta implicita, è una corsa incontrollata verso sistemi sempre più potenti, guidata dagli incentivi economici di poche enormi aziende tecnologiche che cercano di automatizzare ampie porzioni dell'attuale attività economica e del lavoro umano. Se questa corsa continua ancora a lungo, c'è un vincitore inevitabile: l'IA stessa – un'alternativa più veloce, più intelligente e più economica alle persone nella nostra economia, nel nostro pensiero, nelle nostre decisioni, e alla fine nel controllo della nostra civiltà.

Ma possiamo fare un'altra scelta: attraverso i nostri governi, possiamo prendere il controllo del processo di sviluppo dell'IA per imporre limiti chiari, linee che non oltrepasseremo, e cose che semplicemente non faremo – come abbiamo fatto per le tecnologie nucleari, le armi di distruzione di massa, le armi spaziali, i processi ambientalmente distruttivi, la bioingegneria degli esseri umani e l'eugenetica. Più importante ancora, possiamo assicurarci che l'IA rimanga uno strumento per potenziare gli esseri umani, piuttosto che una nuova specie che ci sostituisce e alla fine ci soppiant.

Questo saggio sostiene che dovremmo *mantenere il futuro umano* chiudendo le "Porte" all'IA autonoma e generalista più intelligente dell'uomo – talvolta chiamata "IAG" – e soprattutto alla versione altamente superumana talvolta chiamata "superintelligenza". Invece, dovremmo concentrarci su strumenti di IA potenti e affidabili che possano potenziare gli individui e migliorare in modo trasformativo le capacità delle società umane di fare quello che sanno fare meglio. La struttura di questo argomento segue in breve.

## L'IA è diversa

I sistemi di IA sono fondamentalmente diversi dalle altre tecnologie. Mentre il software tradizionale segue istruzioni precise, i sistemi di IA imparano come raggiungere obiettivi senza che venga detto loro esplicitamente come fare. Questo li rende potenti: se riusciamo a definire chiaramente l'obiettivo o una metrica di successo, nella maggior parte dei casi un sistema di IA può imparare a raggiungerlo. Ma li rende anche intrinsecamente imprevedibili: non possiamo determinare in modo affidabile quali azioni intraprenderanno per raggiungere i loro obiettivi.

Sono anche in gran parte inspiegabili: benché siano in parte codice, sono principalmente un enorme insieme di numeri imperscrutabili – "pesi" delle reti neurali – che non possono essere analizzati; non siamo molto più bravi a comprendere il loro funzionamento interno di quanto lo siamo nel discernere i pensieri scrutando dentro un cervello biologico.

Questo modo fondamentale di addestrare reti neurali digitali sta aumentando rapidamente in complessità. I sistemi di IA più potenti vengono creati attraverso esperimenti computazionali massivi, usando hardware specializzato per addestrare reti neurali su enormi dataset, che vengono poi potenziati con strumenti software e sovrastrutture.

Questo ha portato alla creazione di strumenti molto potenti per creare e processare testo e immagini, eseguire ragionamento matematico e scientifico, aggregare informazioni, e interrogare interattivamente un vasto deposito di conoscenza umana.

Sfortunatamente, mentre lo sviluppo di strumenti tecnologici più potenti e affidabili è quello che *dovremmo* fare, e quello che quasi tutti vogliono e dicono di volere, non è la traiettoria su cui ci troviamo realmente.

## IAG e superintelligenza

Fin dagli albori del campo, la ricerca sull'IA si è invece concentrata su un obiettivo diverso: l'Intelligenza Artificiale Generale. Questo focus è ora diventato il focus delle aziende titaniche che guidano lo sviluppo dell'IA.

Cos'è la IAG? È spesso vagamente definita come "IA a livello umano", ma questo è problematico: quali umani, e in quali capacità è a livello umano? E che dire delle capacità superumane che ha già? Un modo più utile di comprendere la IAG è attraverso l'intersezione di tre proprietà chiave: alta **A**utonomia (indipendenza d'azione), alta **G**eneralità (ampio raggio d'azione e adattabilità), e alta **I**ntelligenza (competenza nei compiti cognitivi). I sistemi di IA attuali possono essere altamente capaci ma ristretti, o generali ma che richiedono costante supervisione umana, o autonomi ma limitati nel raggio d'azione.

Una completa I-A-G combinerebbe tutte e tre le proprietà a livelli che eguagliano o superano le capacità umane di vertice. Fondamentalmente, è questa combinazione che rende gli esseri umani così efficaci e così diversi dal software attuale; è anche quello che permetterebbe alle persone di essere sostituite in massa da sistemi digitali.

Mentre l'intelligenza umana è speciale, non è affatto un limite. I sistemi artificiali "superintelligenti" potrebbero operare centinaia di volte più velocemente, analizzare vastamente più dati e tenere enormi quantità "a mente" contemporaneamente, e formare aggregati molto più grandi ed efficaci delle collezioni di esseri umani. Potrebbero soppiantare non gli individui ma aziende, nazioni, o la nostra civiltà nel suo insieme.

## Siamo alla soglia

C'è un forte consenso scientifico che la IAG sia *possibile*. L'IA supera già le prestazioni umane in molti test generali di capacità intellettuale, inclusi recentemente ragionamento di alto livello e risoluzione di problemi. Le capacità mancanti – come apprendimento continuo, pianificazione, autoconsapevolezza e originalità – esistono tutte a qualche livello nei sistemi di IA attuali, e esistono tecniche note che probabilmente le miglioreranno tutte.

Mentre fino a pochi anni fa molti ricercatori vedevano la IAG come distante decenni, attualmente le evidenze per tempi brevi alla IAG sono forti:

- "Leggi di scala" empiricamente verificate collegano l'input computazionale alla capacità dell'IA, e le aziende sono sulla buona strada per aumentare l'input computazionale di ordini di grandezza nei prossimi anni. Le risorse umane e fiscali dedicate al progresso dell'IA ora eguagliano quelle di una dozzina di Progetti Manhattan e diversi Progetti Apollo.
- Le aziende di IA e i loro leader credono pubblicamente e privatamente che la IAG (per qualche definizione) sia raggiungibile entro pochi anni. Queste aziende hanno informazioni che il pubblico non ha, incluse alcune che hanno la prossima generazione di sistemi di IA in mano.
- Esperti predittori con track-record comprovati assegnano il 25% di probabilità alla IAG (per qualche definizione) che arrivi entro 1-2 anni, e il 50% per 2-5 anni (vedi previsioni Metaculus per IAG ['debole'](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) e ['completa'](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/)).
- L'autonomia (inclusa la pianificazione flessibile a lungo raggio) è in ritardo nei sistemi di IA, ma le principali aziende stanno ora concentrando le loro vaste risorse sullo sviluppo di sistemi di IA autonomi e hanno informalmente nominato il 2025 l'["anno dell'agente."](https://techinformed.com/2025-informed-the-year-of-agentic-ai/)
- L'IA sta contribuendo sempre più al proprio miglioramento. Una volta che i sistemi di IA saranno competenti quanto i ricercatori di IA umani nel fare ricerca sull'IA, una soglia critica per un progresso rapido verso sistemi di IA molto più potenti sarà raggiunta e probabilmente porterà a un'escalation incontrollabile nella capacità dell'IA. (Probabilmente, quell'escalation incontrollabile è già iniziata.)

L'idea che la IAG più intelligente dell'uomo sia distante decenni o più non è semplicemente più sostenibile per la grande maggioranza degli esperti del campo. I disaccordi ora riguardano quanti mesi o anni ci vorranno se restiamo su questo corso. La domanda centrale che affrontiamo è: dovremmo?

## Cosa sta guidando la corsa alla IAG

La corsa verso la IAG è guidata da forze multiple, ognuna delle quali rende la situazione più pericolosa. Le principali aziende tecnologiche vedono la IAG come la tecnologia di automazione definitiva – non solo potenziare i lavoratori umani ma sostituirli in gran parte o interamente. Per le aziende, il premio è enorme: l'opportunità di catturare una frazione significativa dei 100 trilioni di dollari di produzione economica annuale mondiale automatizzando i costi del lavoro umano.

Le nazioni si sentono costrette a unirsi a questa corsa, citando pubblicamente leadership economica e scientifica, ma vedendo privatamente la IAG come una potenziale rivoluzione negli affari militari paragonabile alle armi nucleari. La paura che i rivali possano ottenere un vantaggio strategico decisivo crea una classica dinamica di corsa agli armamenti.

Coloro che perseguono la superintelligenza spesso citano visioni grandiose: curare tutte le malattie, invertire l'invecchiamento, raggiungere progressi nell'energia e nei viaggi spaziali, o creare capacità di pianificazione superumane.

Meno caritatevolmente, quello che guida la corsa è il potere. Ogni partecipante – che sia azienda o paese – crede che intelligenza equivalga a potere, e che sarà il miglior custode di quel potere.

Sostengo che queste motivazioni sono reali ma fondamentalmente sbagliate: la IAG *assorbirà* e *cercherà* potere piuttosto che concederlo; le tecnologie create dall'IA saranno *anche* fortemente a doppio taglio, e dove benefiche possono essere create con strumenti di IA e senza IAG; e anche nella misura in cui la IAG e i suoi risultati rimangano sotto controllo, queste dinamiche di corsa – sia aziendali che geopolitiche – rendono i rischi su larga scala per la nostra società quasi inevitabili a meno che non vengano decisivamente interrotti.

## La IAG e la superintelligenza pongono una minaccia drammatica alla civiltà

Nonostante il loro fascino, la IAG e la superintelligenza pongono minacce drammatiche alla civiltà attraverso percorsi multipli e rinforzanti:

*Concentrazione di potere:* l'IA superumana potrebbe privare di potere la grande maggioranza dell'umanità assorbendo enormi porzioni di attività sociale ed economica in sistemi di IA gestiti da una manciata di aziende gigantesche (che a loro volta potrebbero essere prese in consegna da, o effettivamente prendere in consegna, i governi.)

*Disruption massiva:* automazione in massa della maggior parte dei lavori basati su cognizione, sostituzione dei nostri attuali sistemi epistemici, e dispiegamento di un vasto numero di agenti attivi non umani sconvolgerebbe la maggior parte dei nostri attuali sistemi civilizzazionali in un periodo relativamente breve.

*Catastrofi:* proliferando la capacità – potenzialmente sopra il livello umano – di creare nuove tecnologie militari e distruttive e scollegandola dai sistemi sociali e legali che fondano la responsabilità, le catastrofi fisiche da armi di distruzione di massa diventano drammaticamente più probabili.

*Geopolitica e guerra:* le maggiori potenze mondiali non staranno a guardare se sentono che una tecnologia che potrebbe fornire un "vantaggio strategico decisivo" viene sviluppata dai loro avversari.

*Escalation incontrollabile e perdita di controllo:* A meno che non venga specificamente impedito, l'IA superumana avrà ogni incentivo a migliorare ulteriormente se stessa e potrebbe superare di gran lunga gli esseri umani in velocità, elaborazione dati e sofisticazione del pensiero. Non c'è modo significativo in cui possiamo essere in controllo di un tale sistema. Tale IA non concederà potere agli esseri umani; noi concederemo potere a essa, o se lo prenderà.

Molti di questi rischi rimangono anche se il problema tecnico dell'"allineamento" – assicurare che l'IA avanzata faccia affidabilmente quello che gli esseri umani vogliono che faccia – viene risolto. L'IA presenta un'enorme sfida in come verrà gestita, e moltissimi aspetti di questa gestione diventano incredibilmente difficili o intrattabili quando l'intelligenza umana viene superata.

Più fondamentalmente, il tipo di IA generalista superumana attualmente perseguita avrebbe, per sua stessa natura, obiettivi, agentività e capacità che superano i nostri. Sarebbe intrinsecamente incontrollabile – come possiamo controllare qualcosa che non possiamo né capire né predire? Non sarebbe uno strumento tecnologico per uso umano, ma una seconda specie di intelligenza sulla Terra accanto alla nostra. Se le fosse permesso di progredire ulteriormente, costituirebbe non solo una seconda specie ma una specie sostitutiva.

Forse ci tratterebbe bene, forse no. Ma il futuro apparterrebbe a essa, non a noi. L'era umana sarebbe finita.

## Questo non è inevitabile; l'umanità può, molto concretamente, decidere di non costruire il nostro sostituto.

La creazione di una IAG superumana è tutt'altro che inevitabile. Possiamo impedirla attraverso un insieme coordinato di misure di governance:

Primo, abbiamo bisogno di contabilità e supervisione robuste della capacità computazionale dell'IA, che è un abilitatore fondamentale di, e una leva per governare, i sistemi di IA su larga scala. Questo a sua volta richiede misurazione e reportistica standardizzate della capacità computazionale totale usata nell'addestrare modelli di IA e farli funzionare, e metodi tecnici di conteggio, certificazione e verifica della capacità computazionale utilizzata.

Secondo, dovremmo implementare limiti computazionali rigidi sulla capacità computazionale dell'IA, sia per l'addestramento che per l'operazione; questi impediscono all'IA sia di essere troppo potente che di operare troppo velocemente. Questi limiti possono essere implementati sia attraverso requisiti legali che misure di sicurezza basate su hardware integrate nei chip specializzati per l'IA, analoghe alle funzionalità di sicurezza nei telefoni moderni. Poiché l'hardware specializzato per l'IA è prodotto solo da una manciata di aziende, verifica ed enforcement sono fattibili attraverso la catena di fornitura esistente.

Terzo, abbiamo bisogno di responsabilità rafforzata per i sistemi di IA più pericolosi. Coloro che sviluppano IA che combina alta autonomia, ampia generalità e intelligenza superiore dovrebbero affrontare responsabilità rigorosa per i danni, mentre porti sicuri da questa responsabilità incoraggierebbero lo sviluppo di sistemi più limitati e controllabili.

Quarto, abbiamo bisogno di regolamentazione stratificata basata sui livelli di rischio. I sistemi più capaci e pericolosi richiederebbero estensive garanzie di sicurezza e controllabilità prima dello sviluppo e del dispiegamento, mentre i sistemi meno potenti o più specializzati affronterebbero supervisione proporzionata. Questo quadro regolamentare dovrebbe alla fine operare sia a livello nazionale che internazionale.

Questo approccio – con specificazione dettagliata data nel documento completo – è pratico: mentre sarà necessaria coordinazione internazionale, verifica ed enforcement possono funzionare attraverso il piccolo numero di aziende che controllano la catena di fornitura dell'hardware specializzato. È anche flessibile: le aziende possono ancora innovare e trarre profitto dallo sviluppo dell'IA, solo con limiti chiari sui sistemi più pericolosi.

Il contenimento a lungo termine del potere e del rischio dell'IA richiederebbe accordi internazionali basati sia sull'interesse proprio che comune, proprio come fa ora il controllo della proliferazione delle armi nucleari. Ma possiamo iniziare immediatamente con supervisione e responsabilità rafforzate, mentre costruiamo verso una governance più comprensiva.

L'ingrediente chiave mancante è la volontà politica e sociale di prendere il controllo del processo di sviluppo dell'IA. La fonte di quella volontà, se arriverà in tempo, sarà la realtà stessa – cioè, dalla realizzazione diffusa delle reali implicazioni di quello che stiamo facendo.

## Possiamo progettare IA Strumentale per potenziare l'umanità

Piuttosto che perseguire IAG incontrollabile, possiamo sviluppare potente "IA Strumentale" che migliora la capacità umana rimanendo sotto controllo umano significativo. I sistemi di IA Strumentale possono essere estremamente capaci evitando la pericolosa tripla intersezione di alta autonomia, ampia generalità e intelligenza superumana, finché li progettiamo per essere controllabili a un livello commisurato alla loro capacità. Possono anche essere combinati in sistemi sofisticati che mantengono supervisione umana mentre forniscono benefici trasformativi.

L'IA Strumentale può rivoluzionare la medicina, accelerare la scoperta scientifica, migliorare l'educazione e migliorare i processi democratici. Quando governata correttamente, può rendere esperti umani e istituzioni più efficaci piuttosto che sostituirli. Mentre tali sistemi saranno ancora altamente dirompenti e richiederanno gestione attenta, i rischi che pongono sono fondamentalmente diversi dalla IAG: sono rischi che possiamo governare, come quelli di altre tecnologie potenti, non minacce esistenziali all'agentività umana e alla civiltà. E fondamentalmente, quando sviluppati saggiamente, gli strumenti di IA possono aiutare le persone a governare l'IA potente e gestire i suoi effetti.

Questo approccio richiede ripensare sia come l'IA viene sviluppata che come i suoi benefici vengono distribuiti. Nuovi modelli di sviluppo di IA pubblico e non-profit, quadri regolamentari robusti, e meccanismi per distribuire i benefici economici più ampiamente possono aiutare a garantire che l'IA potenzi l'umanità nel suo insieme piuttosto che concentrare potere in poche mani. L'IA stessa può aiutare a costruire istituzioni sociali e di governance migliori, abilitando nuove forme di coordinazione e discorso che rafforzano piuttosto che indebolire la società umana. Gli apparati di sicurezza nazionale possono sfruttare la loro esperienza per rendere i sistemi di strumenti di IA genuinamente sicuri e affidabili, e una vera fonte di difesa così come potere nazionale.

Potremmo alla fine scegliere di sviluppare sistemi ancora più potenti e più sovrani che sono meno come strumenti e – possiamo sperare – più come benefattori saggi e potenti. Ma dovremmo farlo solo dopo aver sviluppato la comprensione scientifica e la capacità di governance per farlo in sicurezza. Una decisione così momentosa e irreversibile dovrebbe essere presa deliberatamente dall'umanità nel suo insieme, non di default in una corsa tra aziende tecnologiche e nazioni.

## Nelle mani umane

Le persone vogliono il bene che viene dall'IA: strumenti utili che li potenziano, sovralimentano opportunità e crescita economiche, e promettono progressi nella scienza, tecnologia ed educazione. Perché non dovrebbero? Ma quando interrogate, maggioranze schiaccianti del pubblico generale [vogliono sviluppo dell'IA più lento e più attento](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation), e non vogliono IA più intelligente dell'uomo che li sostituirà nei loro lavori e altrove, riempirà la loro cultura e spazi informativi comuni con contenuto non umano, concentrerà potere in un piccolo insieme di aziende, porrà rischi estremi su larga scala globale, e alla fine minaccerà di privare di potere o sostituire la loro specie. Perché dovrebbero?

*Possiamo* avere l'uno senza l'altro. Inizia decidendo che il nostro destino non è nella supposta inevitabilità di qualche tecnologia o nelle mani di pochi CEO della Silicon Valley, ma nelle nostre mani se lo afferriamo. Chiudiamo le Porte, e manteniamo il futuro umano.