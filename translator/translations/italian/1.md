# Capitolo 1 - Introduzione

Il modo in cui risponderemo alla prospettiva di un'IA più intelligente degli esseri umani è la questione più urgente del nostro tempo. Questo saggio fornisce una strada da seguire.

Potremmo trovarci alla fine dell'era umana.

Negli ultimi dieci anni è iniziato qualcosa di unico nella storia della nostra specie. Le sue conseguenze determineranno, in larga misura, il futuro dell'umanità. A partire dal 2015 circa, i ricercatori sono riusciti a sviluppare intelligenza artificiale (IA) *specializzata* – sistemi in grado di vincere a giochi come il Go, riconoscere immagini e linguaggio parlato, e altro ancora, meglio di qualsiasi essere umano.[^1]

Si tratta di un successo straordinario, che sta producendo sistemi e prodotti estremamente utili che potenzieranno l'umanità. Ma l'intelligenza artificiale specializzata non è mai stata il vero obiettivo del settore. Piuttosto, l'obiettivo è stato quello di creare sistemi di IA *generali*, in particolare quelli spesso chiamati "intelligenza artificiale generale" (IAG) o "superintelligenza" che sono contemporaneamente buoni o migliori degli umani in quasi *tutte* le attività, proprio come l'IA è ora sovrumana nel Go, negli scacchi, nel poker, nelle corse di droni, ecc. Questo è l'obiettivo dichiarato di molte grandi aziende di IA.[^2]

*Questi sforzi stanno anche avendo successo.* I sistemi di IA generali come ChatGPT, Gemini, Llama, Grok, Claude e Deepseek, basati su calcoli massicci e montagne di dati, hanno raggiunto la parità con gli esseri umani comuni in un'ampia varietà di compiti, e persino eguagliato esperti umani in alcuni domini. Ora gli ingegneri di IA di alcune delle più grandi aziende tecnologiche stanno correndo per spingere questi giganteschi esperimenti di intelligenza artificiale ai livelli successivi, in cui eguagliano e poi superano l'intera gamma delle capacità, competenze e autonomia umane.

*Questo è imminente.* Negli ultimi dieci anni, le stime degli esperti su quanto tempo ci vorrà – se continuiamo il nostro corso attuale – sono scese da decenni (o secoli) a pochi anni.

È anche di importanza epocale e di rischio trascendente. I sostenitori della IAG la vedono come una trasformazione positiva che risolverà problemi scientifici, curerà malattie, svilupperà nuove tecnologie e automatizzerà il lavoro faticoso. E l'IA potrebbe certamente aiutare a raggiungere tutte queste cose – infatti lo sta già facendo. Ma nel corso dei decenni, molti pensatori attenti, da Alan Turing a Stephen Hawking fino agli odierni Geoffrey Hinton e Yoshua Bengio [^3] hanno lanciato un monito severo: costruire un'IA davvero più intelligente degli umani, generale e autonoma, come minimo sconvolgerà completamente e irrevocabilmente la società, e al massimo comporterà l'estinzione umana.[^4]

L'IA superintelligente si sta rapidamente avvicinando sulla nostra strada attuale, ma è tutt'altro che inevitabile. Questo saggio è un'argomentazione estesa sul perché e su come dovremmo *chiudere le Porte* a questo futuro inumano che si avvicina, e cosa dovremmo fare invece.


[^1]: Questo [grafico](https://time.com/6300942/ai-progress-charts/) mostra una serie di compiti; molte curve simili potrebbero essere aggiunte a questo grafico. Questo rapido progresso nell'IA specializzata ha sorpreso anche gli esperti del settore, con benchmark superati anni prima delle previsioni.

[^2]: Deepmind, OpenAI, Anthropic e X.ai sono state tutte fondate con l'obiettivo specifico di sviluppare la IAG. Ad esempio, lo statuto di OpenAI dichiara esplicitamente il suo obiettivo come sviluppare "intelligenza artificiale generale che benefici tutta l'umanità", mentre la missione di DeepMind è "risolvere l'intelligenza, e poi usarla per risolvere tutto il resto". Meta, Microsoft e altri stanno ora perseguendo percorsi sostanzialmente simili. Meta ha dichiarato di [pianificare lo sviluppo della IAG e di rilasciarla apertamente.](https://www.forbes.com/sites/johnkoetsier/2024/01/18/zuckerberg-on-ai-meta-building-agi-for-everyone-and-open-sourcing-it/)

[^3]: Hinton e Bengio sono due dei ricercatori di IA più citati, hanno entrambi vinto il Nobel del settore IA, il Premio Turing, e Hinton ha vinto anche un premio Nobel (in fisica).

[^4]: Costruire qualcosa di questo rischio, sotto incentivi commerciali e con una supervisione governativa quasi nulla, è assolutamente senza precedenti. Non c'è nemmeno controversia sul rischio tra coloro che lo stanno costruendo! I leader di Deepmind, OpenAI e Anthropic, tra molti altri esperti, hanno tutti letteralmente firmato una [dichiarazione](https://www.safe.ai/work/statement-on-ai-risk) che l'IA avanzata pone un *rischio di estinzione per l'umanità.* I campanelli d'allarme non potrebbero suonare più forte, e si può solo concludere che coloro che li ignorano semplicemente non stanno prendendo sul serio la IAG e la superintelligenza. Uno degli obiettivi di questo saggio è aiutarli a capire perché dovrebbero.