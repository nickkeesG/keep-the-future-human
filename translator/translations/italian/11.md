# Appendici

Informazioni supplementari, che includono - Dettagli tecnici sulla contabilizzazione computazionale, un esempio di implementazione di una 'chiusura delle porte', dettagli per un regime rigoroso di responsabilità per la IAG, e un approccio stratificato agli standard di sicurezza per la IAG.

## Appendice A: Dettagli tecnici della contabilizzazione computazionale

Per controlli significativi basati sulla capacità computazionale è necessario un metodo dettagliato sia per la "verità oggettiva" che per buone approssimazioni della capacità computazionale totale utilizzata nell'addestramento e nell'inferenza. Ecco un esempio di come la "verità oggettiva" potrebbe essere calcolata a livello tecnico.

**Definizioni:**

*Grafo causale computazionale:* Per un dato output O di un modello di IA, esiste un insieme di calcoli digitali per i quali modificare il risultato di tale calcolo potrebbe potenzialmente cambiare O. (Questo dovrebbe essere assunto in modo conservativo, ovvero dovrebbe esistere una ragione chiara per credere che un calcolo sia indipendente da un precursore che avviene prima nel tempo e ha un potenziale percorso causale fisico di effetto.) Questo include i calcoli eseguiti dal modello di IA durante l'inferenza, così come i calcoli che sono entrati nell'input, nella preparazione dei dati e nell'addestramento del modello. Poiché ognuno di questi può essere a sua volta l'output di un modello di IA, questo viene calcolato ricorsivamente, interrotto dove un umano ha fornito un cambiamento significativo all'input.

*Capacità computazionale di addestramento:* La capacità computazionale totale, in FLOP o altre unità, implicata dal grafo causale computazionale di una rete neurale (inclusi preparazione dei dati, addestramento, fine-tuning e qualsiasi altro calcolo.)

*Capacità computazionale di output:* La capacità computazionale totale nel grafo causale computazionale di un dato output di IA, incluse tutte le reti neurali (e inclusa la loro Capacità computazionale di addestramento) e altri calcoli che contribuiscono a quell'output.

*Tasso di capacità computazionale di inferenza:* In una serie di output, il tasso di cambiamento (in FLOP/s o altre unità) della Capacità computazionale di output tra gli output, ovvero la capacità computazionale utilizzata per produrre l'output successivo, divisa per l'intervallo di tempo tra gli output.

**Esempi e approssimazioni:**

- Per una singola rete neurale addestrata su dati creati da umani, la Capacità computazionale di addestramento è semplicemente la capacità computazionale totale di addestramento come riportato abitualmente.
- Per una tale rete neurale che esegue inferenza a ritmo costante, il Tasso di capacità computazionale di inferenza è approssimativamente la velocità totale del cluster computazionale che esegue l'inferenza in FLOP/s.
- Per il fine-tuning di modelli, la Capacità computazionale di addestramento del modello completo è data dalla Capacità computazionale di addestramento del modello non sottoposto a fine-tuning più il calcolo eseguito durante il fine-tuning e per preparare qualsiasi dato utilizzato nel fine-tuning.
- Per un modello distillato, la Capacità computazionale di addestramento del modello completo include l'addestramento sia del modello distillato che del modello più grande utilizzato per fornire dati sintetici o altro input di addestramento.
- Se vengono addestrati diversi modelli, ma molti "tentativi" vengono scartati sulla base del giudizio umano, questi non contano verso la Capacità computazionale di addestramento o di output del modello conservato.

## Appendice B: Esempio di implementazione di una chiusura delle porte

**Esempio di implementazione:** Ecco un esempio di come potrebbe funzionare una chiusura delle porte, dato un limite di 10 <sup>27</sup> FLOP per l'addestramento e 10 <sup>20</sup> FLOP/s per l'inferenza (esecuzione dell'IA):

**1\. Pausa:** Per ragioni di sicurezza nazionale, il ramo esecutivo statunitense chiede a tutte le aziende con sede negli Stati Uniti, che operano negli Stati Uniti o che utilizzano chip prodotti negli Stati Uniti, di cessare qualsiasi nuovo ciclo di addestramento di IA che potrebbe superare il limite di Capacità computazionale di addestramento di 10 <sup>27</sup> FLOP. Gli Stati Uniti dovrebbero avviare discussioni con altri paesi che ospitano sviluppo di IA, incoraggiandoli fortemente ad adottare misure simili e indicando che la pausa statunitense potrebbe essere revocata qualora scegliessero di non conformarsi.

**2\. Supervisione e licenze statunitensi:** Tramite ordine esecutivo o azione di un'agenzia regolatoria esistente, gli Stati Uniti richiedono che entro (diciamo) un anno:

- Tutti i cicli di addestramento di IA stimati sopra i 10 <sup>25</sup> FLOP eseguiti da aziende che operano negli Stati Uniti siano registrati in un database mantenuto da un'agenzia regolatoria statunitense. (Nota: Una versione leggermente più debole di questo era già stata inclusa nell'ordine esecutivo statunitense del 2023 sull'IA, ora revocato, che richiedeva la registrazione per modelli sopra i 10 <sup>26</sup> FLOP.)
- Tutti i produttori di hardware rilevante per l'IA che operano negli Stati Uniti o fanno affari con il governo statunitense aderiscano a una serie di requisiti sul loro hardware specializzato e sul software che lo gestisce. (Molti di questi requisiti potrebbero essere incorporati in aggiornamenti software e firmware dell'hardware esistente, ma soluzioni a lungo termine e robuste richiederebbero modifiche alle generazioni successive di hardware.) Tra questi c'è il requisito che se l'hardware fa parte di un cluster interconnesso ad alta velocità capace di eseguire 10 <sup>18</sup> FLOP/s di calcolo, è richiesto un livello più alto di verifica, che include autorizzazione regolare da parte di un "governatore" remoto che riceve sia telemetria che richieste per eseguire calcoli aggiuntivi.
- Il custode riporta il calcolo totale eseguito sul suo hardware all'agenzia che mantiene il database statunitense.
- Requisiti più stringenti vengono introdotti gradualmente per consentire una supervisione e autorizzazione più sicura e flessibile.

**3\. Supervisione internazionale:**

- Gli Stati Uniti, la Cina e qualsiasi altro paese che ospita capacità avanzate di produzione di chip negoziano un accordo internazionale.
- Questo accordo crea una nuova agenzia internazionale, analoga all'Agenzia Internazionale per l'Energia Atomica, incaricata di supervisionare l'addestramento e l'esecuzione dell'IA.
- I paesi firmatari devono richiedere ai loro produttori nazionali di hardware per IA di conformarsi a una serie di requisiti almeno tanto stringenti quanto quelli imposti negli Stati Uniti.
- I custodi sono ora tenuti a riportare i numeri di calcolo dell'IA sia alle agenzie nei loro paesi di origine che a un nuovo ufficio all'interno dell'agenzia internazionale.
- Altri paesi sono fortemente incoraggiati ad aderire all'accordo internazionale esistente: i controlli sull'esportazione da parte dei paesi firmatari limitano l'accesso all'hardware di fascia alta da parte dei non firmatari mentre i firmatari possono ricevere supporto tecnico nella gestione dei loro sistemi di IA.

**4\. Verifica e applicazione internazionale:**

- Il sistema di verifica hardware viene aggiornato in modo che riporti l'utilizzo di calcolo sia al custode originale che anche direttamente all'ufficio dell'agenzia internazionale.
- L'agenzia, tramite discussione con i firmatari dell'accordo internazionale, concorda su limitazioni computazionali che poi assumono forza legale nei paesi firmatari.
- In parallelo, può essere sviluppata una serie di standard internazionali in modo che l'addestramento e l'esecuzione di IA sopra una soglia di calcolo (ma sotto il limite) siano tenuti ad aderire a quegli standard.
- L'agenzia può, se necessario per compensare algoritmi migliori ecc., abbassare il limite computazionale. O, se ritenuto sicuro e consigliabile (al livello di garanzie di sicurezza dimostrabili), aumentare il limite computazionale.

## Appendice C: Dettagli per un regime rigoroso di responsabilità per la IAG

**Dettagli per un regime rigoroso di responsabilità per la IAG**

- La creazione e il funzionamento di un sistema di IA avanzato che sia altamente generale, capace e autonomo, è considerata un'attività "anormalmente pericolosa".
- Come tale, la responsabilità predefinita per l'addestramento e il funzionamento di tali sistemi è la responsabilità oggettiva, solidale (o il suo equivalente non statunitense) per qualsiasi danno causato dal modello o dai suoi output/azioni.
- La responsabilità personale sarà imposta per dirigenti e membri del consiglio di amministrazione in casi di grave negligenza o cattiva condotta intenzionale. Questo dovrebbe includere sanzioni penali per i casi più gravi.
- Esistono numerose clausole di salvaguardia sotto le quali la responsabilità ritorna a quella predefinita (basata sulla colpa, negli Stati Uniti) a cui persone e aziende sarebbero normalmente soggette.
	- Modelli addestrati e operati sotto una certa soglia computazionale (che sarebbe almeno 10 volte inferiore ai limiti descritti sopra.)
	- IA che è "debole" (approssimativamente, sotto il livello di esperto umano nei compiti per cui è intesa) e/o
	- IA che è "ristretta" (con un ambito fisso e piuttosto limitato di compiti e operazioni per cui è specificamente progettata e addestrata) e/o
	- IA che è "passiva" (molto limitata nella sua capacità – anche sotto modeste modifiche – di intraprendere azioni o eseguire compiti complessi multi-fase senza coinvolgimento e controllo umano diretto.)
	- Un'IA che è garantita essere sicura, protetta e controllabile (dimostrabilmente sicura, o un'analisi del rischio indica un livello trascurabile di danno previsto.)
- Le clausole di salvaguardia possono essere rivendicate sulla base di un'[analisi di sicurezza](https://arxiv.org/abs/2410.21572) preparata dallo sviluppatore dell'IA e approvata da un'agenzia o auditor accreditato da un'agenzia. Per rivendicare una clausola di salvaguardia basata sul calcolo, lo sviluppatore deve solo fornire stime credibili della Capacità computazionale di addestramento totale e del Tasso di inferenza massimo
- La legislazione delineerebbe esplicitamente situazioni in cui il risarcimento ingiuntivo dallo sviluppo di sistemi di IA con alto rischio di danno pubblico sarebbe appropriato.
- Consorzi di aziende, lavorando con ONG e agenzie governative, dovrebbero sviluppare standard e norme che definiscano questi termini, come i regolatori dovrebbero concedere clausole di salvaguardia, come gli sviluppatori di IA dovrebbero sviluppare analisi di sicurezza, e come i tribunali dovrebbero interpretare la responsabilità dove le clausole di salvaguardia non sono rivendicate proattivamente.

## Appendice D: Un approccio stratificato agli standard di sicurezza per la IAG

**Un approccio stratificato agli standard di sicurezza per la IAG**

| Livello di Rischio | Innesco/i | Requisiti per l'addestramento | Requisito per il deployment |
| --- | --- | --- | --- |
| LR-0 | IA debole in autonomia, generalità e intelligenza | nessuno | nessuno |
| LR-1 | IA forte in una di autonomia, generalità e intelligenza | nessuno | Basato su rischio e uso, potenzialmente analisi di sicurezza approvate dalle autorità nazionali ovunque il modello possa essere utilizzato |
| LR-2 | IA forte in due di autonomia, generalità e intelligenza | Registrazione presso l'autorità nazionale con giurisdizione sullo sviluppatore | Analisi di sicurezza che delimita il rischio di danno grave sotto i livelli autorizzati più audit di sicurezza indipendenti (inclusi red-teaming black-box e white-box) approvati dalle autorità nazionali ovunque il modello possa essere utilizzato |
| LR-3 | IAG forte in autonomia, generalità e intelligenza | Pre-approvazione del piano di sicurezza dall'autorità nazionale con giurisdizione sullo sviluppatore | Analisi di sicurezza che garantisce rischio delimitato di danno grave sotto i livelli autorizzati così come specifiche richieste, inclusi cybersicurezza, controllabilità, un interruttore di emergenza non rimovibile, allineamento con i valori umani, e robustezza all'uso malevolo. |
| LR-4 | Qualsiasi modello che superi anche 10 <sup>27</sup> FLOP di Addestramento o 10 <sup>20</sup> FLOP/s di Inferenza | Proibito in attesa della revoca concordata internazionalmente del limite computazionale | Proibito in attesa della revoca concordata internazionalmente del limite computazionale |

Classificazioni di rischio e standard di sicurezza, con livelli basati su soglie computazionali così come combinazioni di alta autonomia, generalità e intelligenza:

- *Forte autonomia* si applica se il sistema è in grado di eseguire, o può essere facilmente fatto eseguire, compiti multi-fase e/o intraprendere azioni complesse che sono rilevanti nel mondo reale, senza supervisione o intervento umano significativo. Esempi: veicoli autonomi e robot; bot di trading finanziario. Non-esempi: GPT-4; classificatori di immagini
- *Forte generalità* indica un ampio ambito di applicazione, esecuzione di compiti per cui il modello non è stato deliberatamente e specificamente addestrato, e capacità significativa di apprendere nuovi compiti. Esempi: GPT-4; mu-zero. Non-esempi: AlphaFold; veicoli autonomi; generatori di immagini
- *Forte intelligenza* corrisponde al raggiungimento di prestazioni a livello di esperto umano sui compiti per cui il modello performa meglio (e per un modello generale, attraverso una vasta gamma di compiti.) Esempi: AlphaFold; mu-zero; o3. Non-esempi: GPT-4; Siri