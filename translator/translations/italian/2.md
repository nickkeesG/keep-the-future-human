# Capitolo 2 - Nozioni essenziali sulle reti neurali dell'IA

Come funzionano i sistemi di IA moderni, e cosa potrebbe arrivare nella prossima generazione di IA?

Per comprendere come si svilupperanno le conseguenze dello sviluppo di un'IA più potente, è essenziale interiorizzare alcuni concetti fondamentali. Questa e le prossime due sezioni li sviluppano, coprendo a turno cos'è l'IA moderna, come sfrutta calcoli massicci, e i modi in cui sta crescendo rapidamente in generalità e capacità.[^1]

Ci sono molti modi per definire l'intelligenza artificiale, ma per i nostri scopi la proprietà chiave dell'IA è che mentre un programma informatico standard è una lista di istruzioni su come eseguire un compito, un sistema di IA è uno che impara da dati o esperienza per eseguire compiti *senza che gli venga esplicitamente detto come farlo.*

Quasi tutta l'IA moderna rilevante è basata su reti neurali. Queste sono strutture matematiche/computazionali, rappresentate da un insieme molto grande (miliardi o trilioni) di numeri ("pesi"), che eseguono bene un compito di addestramento. Questi pesi vengono elaborati (o forse "coltivati" o "trovati") aggiustandoli iterativamente in modo che la rete neurale migliori un punteggio numerico (noto anche come "perdita") definito per eseguire bene uno o più compiti.[^2] Questo processo è noto come *addestramento* della rete neurale.[^3]

Ci sono molte tecniche per fare questo addestramento, ma quei dettagli sono molto meno rilevanti dei modi in cui viene definito il punteggio, e di come questi risultino in diversi compiti che la rete neurale esegue bene. Una distinzione chiave è stata storicamente tracciata tra IA "ristretta" e "generale".

L'IA ristretta è deliberatamente addestrata per fare un compito particolare o un piccolo insieme di compiti (come riconoscere immagini o giocare a scacchi); richiede un nuovo addestramento per nuovi compiti, e ha un ambito ristretto di capacità. Abbiamo IA ristretta sovrumana, nel senso che per quasi qualsiasi compito discreto e ben definito che una persona può fare, probabilmente possiamo costruire un punteggio e poi addestrare con successo un sistema di IA ristretta per farlo meglio di quanto potrebbe fare un umano.

I sistemi di IA per uso generale (GPAI) possono eseguire un'ampia gamma di compiti, inclusi molti per cui non sono stati esplicitamente addestrati; possono anche imparare nuovi compiti come parte della loro operazione. Gli attuali "modelli multimodali" di grandi dimensioni [^4] come ChatGPT esemplificano questo: addestrati su un corpus molto grande di testo e immagini, possono impegnarsi in ragionamenti complessi, scrivere codice, analizzare immagini, e assistere con una vasta gamma di compiti intellettuali. Pur essendo ancora abbastanza diversi dall'intelligenza umana in modi che vedremo in profondità più avanti, la loro generalità ha causato una rivoluzione nell'IA.[^5]

## Imprevedibilità: una caratteristica chiave dei sistemi di IA

Una differenza chiave tra i sistemi di IA e il software convenzionale è nella prevedibilità. L'output del software standard può essere imprevedibile – infatti a volte è proprio per questo che scriviamo software, per darci risultati che non avremmo potuto prevedere. Ma il software convenzionale raramente fa qualcosa per cui non è stato programmato – il suo ambito e comportamento sono generalmente come progettati. Un programma di scacchi di alto livello può fare mosse che nessun umano potrebbe prevedere (altrimenti potrebbero battere quel programma di scacchi!) ma generalmente non farà altro che giocare a scacchi.

Come il software convenzionale, l'IA ristretta ha ambito e comportamento prevedibili ma può avere risultati imprevedibili. Questo è davvero solo un altro modo per definire l'IA ristretta: come IA che è simile al software convenzionale nella sua prevedibilità e gamma di operazioni.

L'IA per uso generale è diversa: il suo ambito (i domini su cui si applica), comportamento (i tipi di cose che fa), e risultati (i suoi output effettivi) possono tutti essere imprevedibili.[^6] GPT-4 è stato addestrato solo per generare testo accuratamente, ma ha sviluppato molte capacità che i suoi addestratori non avevano previsto o inteso. Questa imprevedibilità deriva dalla complessità dell'addestramento: poiché i dati di addestramento contengono output da molti compiti diversi, l'IA deve effettivamente imparare a eseguire questi compiti per predire bene.

Questa imprevedibilità dei sistemi di IA generale è abbastanza fondamentale. Anche se in principio è possibile costruire attentamente sistemi di IA che abbiano limiti garantiti sul loro comportamento (come menzionato più avanti nel saggio), il modo in cui i sistemi di IA vengono creati ora li rende imprevedibili nella pratica e persino in principio.

## IA passiva, agenti, sistemi autonomi, e allineamento

Questa imprevedibilità diventa particolarmente importante quando consideriamo come i sistemi di IA vengono effettivamente distribuiti e usati per raggiungere vari obiettivi.

Molti sistemi di IA sono relativamente passivi nel senso che forniscono principalmente informazioni, e l'utente intraprende azioni. Altri, comunemente chiamati *agenti*, intraprendono azioni da soli, con diversi livelli di coinvolgimento da parte di un utente. Quelli che intraprendono azioni con relativamente meno input o supervisione esterni possono essere chiamati più *autonomi*. Questo forma uno spettro in termini di indipendenza d'azione, da strumenti passivi ad agenti autonomi.[^7]

Per quanto riguarda gli obiettivi dei sistemi di IA, questi possono essere direttamente legati al loro obiettivo di addestramento (ad esempio l'obiettivo di "vincere" per un sistema che gioca a Go è anche esplicitamente ciò per cui è stato addestrato). O potrebbero non esserlo: l'obiettivo di addestramento di ChatGPT è in parte predire il testo, in parte essere un assistente utile. Ma quando fa un compito dato, il suo obiettivo gli viene fornito dall'utente. Gli obiettivi possono anche essere creati da un sistema di IA stesso, solo indirettamente correlati al suo obiettivo di addestramento.[^8]

Gli obiettivi sono strettamente legati alla questione dell'"allineamento", cioè la questione se i sistemi di IA *faranno quello che vogliamo che facciano*. Questa semplice domanda nasconde un enorme livello di sottigliezza.[^9] Per ora, nota che "noi" in questa frase potrebbe riferirsi a molte persone e gruppi diversi, portando a diversi tipi di allineamento. Per esempio, un'IA potrebbe essere altamente *obbediente* (o ["leale"](https://arxiv.org/abs/2003.11157)) al suo utente – qui "noi" è "ciascuno di noi." O potrebbe essere più *sovrana*, essendo principalmente guidata dai propri obiettivi e vincoli, ma agendo comunque ampiamente nell'interesse comune del benessere umano – "noi" è allora "l'umanità" o "la società." Nel mezzo c'è uno spettro dove un'IA sarebbe largamente obbediente, ma potrebbe rifiutarsi di intraprendere azioni che danneggino altri o la società, violino la legge, ecc.

Questi due assi – livello di autonomia e tipo di allineamento – non sono del tutto indipendenti. Per esempio, un sistema passivo sovrano, pur non essendo del tutto auto-contraddittorio, è un concetto in tensione, come lo è un agente autonomo obbediente.[^10] C'è un senso chiaro in cui autonomia e sovranità tendono ad andare di pari passo. In modo simile, la prevedibilità tende ad essere più alta nei sistemi di IA "passivi" e "obbedienti", mentre quelli sovrani o autonomi tenderanno ad essere più imprevedibili. Tutto questo sarà cruciale per comprendere le ramificazioni della potenziale IAG e superintelligenza.

Creare un'IA veramente allineata, di qualsiasi tipo, richiede la risoluzione di tre sfide distinte:

1. Capire cosa "noi" vogliamo – che è complesso sia che "noi" significhi una persona o organizzazione specifica (lealtà) o l'umanità in generale (sovranità);
2. Costruire sistemi che agiscano regolarmente in accordo con quei desideri – essenzialmente creare comportamento positivo consistente;
3. Più fondamentalmente, rendere sistemi che genuinamente "si preoccupano" di quei desideri piuttosto che agire meramente come se lo facessero.

La distinzione tra comportamento affidabile e genuina cura è cruciale. Proprio come un impiegato umano potrebbe seguire gli ordini perfettamente pur mancando di qualsiasi reale impegno verso la missione dell'organizzazione, un sistema di IA potrebbe agire allineato senza valorizzare veramente le preferenze umane. Possiamo addestrare i sistemi di IA a dire e fare cose attraverso feedback, e possono imparare a ragionare su cosa vogliono gli umani. Ma fargli *genuinamente* valorizzare le preferenze umane è una sfida molto più profonda.[^11]

Le profonde difficoltà nel risolvere queste sfide di allineamento, e le loro implicazioni per il rischio dell'IA, saranno esplorate ulteriormente più avanti. Per ora, capisci che l'allineamento non è solo una caratteristica tecnica che attacchiamo ai sistemi di IA, ma un aspetto fondamentale della loro architettura che modella la loro relazione con l'umanità.

[^1]: Per un'introduzione gentile ma tecnica all'apprendimento automatico e all'IA, particolarmente ai modelli linguistici, vedi [questo sito.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) Per un altro primer moderno sui rischi di estinzione dell'IA, vedi [questo pezzo.](https://www.thecompendium.ai/) Per un'analisi scientifica completa e autorevole sullo stato della sicurezza dell'IA, vedi il recente [International AI Safety Report.](https://arxiv.org/abs/2501.17805)

[^2]: L'addestramento tipicamente avviene cercando un massimo locale del punteggio in uno spazio ad alta dimensionalità dato dai pesi del modello. Controllando come cambia il punteggio quando i pesi vengono modificati, l'algoritmo di addestramento identifica quali modifiche migliorano il punteggio di più, e muove i pesi in quella direzione.

[^3]: Per esempio, in un problema di riconoscimento immagini, la rete neurale produrrebbe probabilità per etichette per l'immagine. Un punteggio sarebbe correlato alla probabilità che l'IA accorda alla risposta corretta. La procedura di addestramento modificherebbe quindi i pesi in modo che la prossima volta, l'IA produrrebbe una probabilità più alta per l'etichetta corretta per quell'immagine. Questo viene poi ripetuto un enorme numero di volte. La stessa procedura di base viene usata nell'addestramento di essenzialmente tutte le reti neurali moderne, anche se con meccanismi di punteggio più complessi.

[^4]: La maggior parte dei modelli multimodali usa l'architettura "transformer" per elaborare e generare tipi multipli di dati (testo, immagini, suono). Questi possono tutti essere decomposti in, e poi trattati sulla stessa base, come diversi tipi di "token." I modelli multimodali sono addestrati prima per predire accuratamente token all'interno di dataset massicci, poi perfezionati attraverso apprendimento per rinforzo per migliorare le capacità e modellare i comportamenti.

[^5]: Il fatto che i modelli linguistici siano addestrati per fare una cosa – predire parole – ha portato alcuni a chiamarli IA ristretta. Ma questo è fuorviante: poiché predire il testo bene richiede così tante capacità diverse, questo compito di addestramento porta a un sistema sorprendentemente generale. Nota anche che questi sistemi sono estensivamente addestrati dall'apprendimento per rinforzo, rappresentando effettivamente migliaia di persone che danno al modello un segnale di ricompensa quando fa un buon lavoro in qualsiasi delle molte cose che fa. Eredita quindi significativa generalità dalle persone che danno questo feedback.

[^6]: Ci sono modi multipli in cui l'IA è imprevedibile. Uno è che nel caso generale non si può predire cosa farà un algoritmo senza effettivamente eseguirlo; ci sono [teoremi](https://arxiv.org/abs/1310.3225) a questo effetto. Questo può essere vero solo perché l'output degli algoritmi può essere complesso. Ma è particolarmente chiaro e rilevante nel caso (come negli scacchi o nel Go) dove la predizione implicherebbe una capacità (battere l'IA) che il potenziale predittore non ha. Secondo, un dato sistema di IA non produrrà sempre lo stesso output anche dato lo stesso input – i suoi output contengono casualità; questo si accoppia anche con l'imprevedibilità algoritmica. Terzo, capacità inaspettate ed emergenti possono sorgere dall'addestramento, significando che persino i *tipi* di cose che un sistema di IA può e farà sono imprevedibili; Quest'ultimo tipo è particolarmente importante per considerazioni di sicurezza.

[^7]: Vedi [qui](https://arxiv.org/abs/2502.02649) per una revisione approfondita di cosa si intende per "agente autonomo" (insieme ad argomenti etici contro la loro costruzione).

[^8]: Potresti a volte sentire "l'IA non può avere i propri obiettivi." Questo è un assoluto nonsense. È facile generare esempi dove l'IA ha o sviluppa obiettivi che non le sono mai stati dati e sono noti solo a se stessa. Non vedi questo molto nei modelli multimodali popolari attuali perché viene addestrato fuori di loro; potrebbe altrettanto facilmente essere addestrato dentro di loro.

[^9]: C'è una vasta letteratura. Sul problema generale vedi *The Alignment Problem* di Christian, e *Human-Compatible* di Russell. Su un lato più tecnico vedi ad es. [questo paper](https://arxiv.org/abs/2209.00626).

[^10]: Vedremo più tardi che mentre tali sistemi vanno contro la tendenza, questo li rende effettivamente molto interessanti e utili.

[^11]: Questo non vuol dire che richiediamo emozioni o senzienza. Piuttosto, è enormemente difficile dall'esterno di un sistema sapere quali sono i suoi obiettivi interni, preferenze, e valori. "Genuino" qui significherebbe che abbiamo ragioni abbastanza forti per affidarci ad esso che nel caso di sistemi critici possiamo scommetterci le nostre vite.