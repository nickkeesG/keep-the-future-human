# Capitolo 3 - Aspetti chiave della costruzione dei moderni sistemi di IA generale

La maggior parte dei sistemi di IA più avanzati al mondo viene creata utilizzando metodi sorprendentemente simili. Ecco le basi.

Per comprendere davvero un essere umano bisogna conoscere qualcosa di biologia, evoluzione, educazione infantile e altro ancora; per comprendere l'IA bisogna anche sapere come viene creata. Negli ultimi cinque anni, i sistemi di IA si sono evoluti enormemente sia in termini di capacità che di complessità. Un fattore abilitante chiave è stata la disponibilità di quantità molto ampie di capacità computazionale (o colloquialmente "compute" quando applicata all'IA).

I numeri sono impressionanti. Circa 10<sup>25</sup>-10<sup>26</sup> "operazioni in virgola mobile" (FLOP)[^1] vengono utilizzate nell'addestramento di modelli come la serie GPT, Claude, Gemini, ecc.[^2] (Per confronto, se ogni essere umano sulla Terra lavorasse ininterrottamente facendo un calcolo ogni cinque secondi, ci vorrebbero circa un miliardo di anni per realizzare questo.) Questa enorme quantità di calcolo consente l'addestramento di modelli con fino a trilioni di pesi del modello su terabyte di dati – una grande frazione di tutto il testo di qualità che sia mai stato scritto insieme a ampie librerie di suoni, immagini e video. Completando questo addestramento con ulteriore addestramento estensivo che rinforza le preferenze umane e le buone prestazioni nei compiti, i modelli addestrati in questo modo mostrano prestazioni competitive con quelle umane attraverso una gamma significativa di compiti intellettuali di base, inclusi ragionamento e risoluzione di problemi.

Sappiamo anche (molto, molto approssimativamente) quanta velocità computazionale, in operazioni al secondo, è sufficiente perché la velocità di *inferenza*[^3] di un tale sistema corrisponda alla *velocità* dell'elaborazione del testo umano. È circa 10<sup>15</sup>-10<sup>16</sup> FLOP al secondo.[^4]

Pur essendo potenti, questi modelli sono per loro natura limitati in modi chiave, abbastanza analoghi a come un singolo essere umano sarebbe limitato se fosse costretto a produrre semplicemente testo a un tasso fisso di parole al minuto, senza fermarsi a pensare o utilizzare strumenti aggiuntivi. I sistemi di IA più recenti affrontano queste limitazioni attraverso un processo e un'architettura più complessi che combinano diversi elementi chiave:

- Una o più reti neurali, con un modello che fornisce la capacità cognitiva principale, e fino a diversi altri che svolgono altri compiti più specifici;
- *Strumenti* forniti e utilizzabili dal modello – ad esempio la capacità di cercare sul web, creare o modificare documenti, eseguire programmi, ecc.
- *Architettura di supporto* che collega input e output delle reti neurali. Un'architettura molto semplice potrebbe semplicemente permettere a due "istanze" di un modello di IA di conversare tra loro, o a una di controllare il lavoro di un'altra.[^5]
- Tecniche di *catena di ragionamento* e prompting correlate fanno qualcosa di simile, causando un modello a generare ad esempio molti approcci a un problema, poi elaborare quegli approcci per una risposta aggregata.
- *Riaddestramento* dei modelli per fare un uso migliore di strumenti, architettura di supporto e catena di ragionamento.

Poiché queste estensioni possono essere molto potenti (e includere i sistemi di IA stessi), questi sistemi compositi possono essere piuttosto sofisticati e migliorare drasticamente le capacità dell'IA.[^6] E recentemente, tecniche nell'architettura di supporto e specialmente nel prompting a catena di ragionamento (e nel reintegrare i risultati nell'riaddestramento dei modelli per usare questi meglio) sono state sviluppate e impiegate in [o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/), e [DeepSeek R1](https://api-docs.deepseek.com/news/news250120) per fare molti passaggi di inferenza in risposta a una data query.[^7] Questo in effetti permette al modello di "riflettere" sulla sua risposta e aumenta drasticamente la capacità di questi modelli di fare ragionamenti di alto livello in compiti di scienza, matematica e programmazione.[^8]

Per una data architettura di IA, gli aumenti nella capacità computazionale di addestramento [possono essere tradotti in modo affidabile](https://arxiv.org/abs/2405.10938) in miglioramenti in un insieme di metriche chiaramente definite. Per capacità generali meno definite con precisione (come quelle discusse di seguito), la traduzione è meno chiara e predittiva, ma è quasi certo che modelli più grandi con più capacità computazionale di addestramento avranno capacità nuove e migliori, anche se è difficile predire quali saranno.

Allo stesso modo, i sistemi compositi e specialmente i progressi nella "catena di ragionamento" (e l'addestramento di modelli che funzionano bene con essa) hanno sbloccato la scalabilità nella capacità computazionale di *inferenza*: per un dato modello principale addestrato, almeno alcune capacità del sistema di IA aumentano quando viene applicato più calcolo che permette loro di "pensare più intensamente e a lungo" sui problemi complessi. Questo comporta un costo ripido in termini di velocità di calcolo, richiedendo centinaia o migliaia di FLOP/s in più per eguagliare le prestazioni umane.[^9]

Pur essendo solo una parte di ciò che sta portando a rapidi progressi nell'IA,[^10] il ruolo del calcolo e la possibilità di sistemi compositi si riveleranno cruciali sia per prevenire l'IAG incontrollabile che per sviluppare alternative più sicure.

[^1]: 10<sup>27</sup> significa 1 seguito da 25 zeri, o dieci trilioni di trilioni. Un FLOP è semplicemente un'addizione o moltiplicazione aritmetica di numeri con una certa precisione. Si noti che le prestazioni dell'hardware di IA possono variare di un fattore di dieci in più a seconda della precisione dell'aritmetica e dell'architettura del computer. Contare le operazioni di porte logiche (AND, OR, AND NOT) sarebbe fondamentale ma queste non sono comunemente disponibili o benchmarkate; per i presenti scopi è utile standardizzare sulle operazioni a 16 bit (FP16), anche se dovrebbero essere stabiliti fattori di conversione appropriati.

[^2]: Una raccolta di stime e dati concreti è disponibile da [Epoch AI](https://epochai.org/data/large-scale-ai-models) e indica circa 2×10<sup>25</sup> FLOP a 16 bit per GPT-4; questo corrisponde approssimativamente ai [numeri che sono trapelati](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) per GPT-4. Le stime per altri modelli di metà 2024 sono tutte entro un fattore di pochi rispetto a GPT-4.

[^3]: L'inferenza è semplicemente il processo di generare un output da una rete neurale. L'addestramento può essere considerato una successione di molte inferenze e modifiche dei pesi del modello.

[^4]: Per la produzione di testo, il GPT-4 originale richiedeva 560 TFLOP per token generato. Circa 7 token/s sono necessari per stare al passo con il pensiero umano, quindi questo dà ≈3×10<sup>15</sup> FLOP/s. Ma le efficienze hanno ridotto questo; [questa brochure NVIDIA](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/) per esempio indica appena 3×10<sup>14</sup> FLOP/s per un modello Llama 405B con prestazioni comparabili.

[^5]: Come esempio leggermente più complesso, un sistema di IA potrebbe prima generare diverse possibili soluzioni a un problema matematico, poi usare un'altra istanza per controllare ogni soluzione, e infine usare una terza per sintetizzare i risultati in una spiegazione chiara. Questo permette una risoluzione dei problemi più accurata e affidabile rispetto a un singolo passaggio.

[^6]: Vedere ad esempio dettagli su ["Operator" di OpenAI](https://openai.com/index/introducing-operator/), [le capacità di strumenti di Claude](https://docs.anthropic.com/en/docs/build-with-claude/computer-use), e [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). [Deep Research](https://openai.com/index/introducing-deep-research/) di OpenAI probabilmente ha un'architettura piuttosto sofisticata ma i dettagli non sono disponibili.

[^7]: Deepseek R1 si basa sull'addestrare e fare prompting del modello iterativamente in modo che il modello finale addestrato crei un ragionamento estensivo a catena di ragionamento. I dettagli architetturali non sono disponibili per o1 o o3, tuttavia Deepseek ha rivelato che non è richiesta alcuna particolare "salsa segreta" per sbloccare la scalabilità delle capacità con l'inferenza. Ma nonostante abbia ricevuto molta attenzione dalla stampa come qualcosa che sconvolge lo "status quo" nell'IA, non impatta le affermazioni centrali di questo saggio.

[^8]: Questi modelli superano significativamente i modelli standard sui benchmark di ragionamento. Per esempio, nel GPQA Diamond Benchmark—un test rigoroso di domande scientifiche di livello PhD—GPT-4o [ha ottenuto](https://openai.com/index/learning-to-reason-with-llms/) il 56%, mentre o1 e o3 hanno raggiunto il 78% e l'88%, rispettivamente, superando di gran lunga il punteggio medio del 70% degli esperti umani.

[^9]: L'O3 di OpenAI probabilmente ha speso ∼10<sup>21</sup>-10<sup>22</sup> FLOP [per completare ognuna delle domande della sfida ARC-AGI](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai), che gli esseri umani competenti possono fare in (diciamo) 10-100 secondi, dando una cifra più simile a ∼10<sup>20</sup> FLOP/s.

[^10]: Mentre il calcolo è una misura chiave della capacità del sistema di IA, interagisce sia con la qualità dei dati che con i miglioramenti algoritmici. Dati o algoritmi migliori possono ridurre i requisiti computazionali, mentre più calcolo può talvolta compensare dati o algoritmi più deboli.