# Capitolo 7 - Cosa accade se sviluppiamo la IAG seguendo il nostro percorso attuale?

La società non è pronta per sistemi di livello IAG. Se li costruiamo molto presto, le cose potrebbero mettersi male.

Lo sviluppo di un'intelligenza artificiale generale completa – quella che chiameremo qui IA che è "oltre le Porte" – rappresenterebbe un cambiamento fondamentale nella natura del mondo: per sua stessa natura significa aggiungere alla Terra una nuova specie di intelligenza con capacità superiori a quelle degli esseri umani.

Quello che accadrà dipende da molti fattori, tra cui la natura della tecnologia, le scelte di coloro che la sviluppano e il contesto mondiale in cui viene sviluppata.

Attualmente, la IAG completa viene sviluppata da una manciata di enormi aziende private in competizione tra loro, con poca regolamentazione significativa o supervisione esterna,[^1] in una società con istituzioni centrali sempre più deboli e persino disfunzionali,[^2] in un periodo di alta tensione geopolitica e scarso coordinamento internazionale. Sebbene alcuni siano motivati altruisticamente, molti di coloro che se ne occupano sono spinti dal denaro, o dal potere, o da entrambi.

La previsione è molto difficile, ma ci sono alcune dinamiche abbastanza ben comprese e analogie sufficientemente appropriate con tecnologie precedenti da offrire una guida. E sfortunatamente, nonostante le promesse dell'IA, forniscono buoni motivi per essere profondamente pessimisti su come si svilupperà la nostra traiettoria attuale.

Per dirla senza mezzi termini, sul nostro corso attuale lo sviluppo della IAG avrà alcuni effetti positivi (e renderà alcune persone molto, molto ricche). Ma la natura della tecnologia, le dinamiche fondamentali e il contesto in cui viene sviluppata indicano fortemente che: l'IA potente minerà drammaticamente la nostra società e civiltà; ne perderemo il controllo; potremmo finire in una guerra mondiale a causa di essa; perderemo (o cederemo) il controllo *ad* essa; porterà alla superintelligenza artificiale, che assolutamente non controlleremo e che significherà la fine di un mondo gestito dagli esseri umani.

Queste sono affermazioni forti, e vorrei che fossero speculazioni oziose o "catastrofismo" ingiustificato. Ma è qui che puntano la scienza, la teoria dei giochi, la teoria evolutiva e la storia. Questa sezione sviluppa in dettaglio queste affermazioni e i loro supporti.

## Minerebbe la nostra società e civiltà

Nonostante quello che potreste sentire nelle sale riunioni della Silicon Valley, la maggior parte delle disruption – specialmente di varietà molto rapida – non è benefica. Ci sono molti più modi per peggiorare i sistemi complessi che per migliorarli. Il nostro mondo funziona bene come funziona perché abbiamo costruito meticolosamente processi, tecnologie e istituzioni che lo hanno reso progressivamente migliore.[^3] Prendere a martellate una fabbrica raramente migliora le operazioni.

Ecco un catalogo (incompleto) di modi in cui i sistemi IAG potrebbero sconvolgere la nostra civiltà.

- Sconvolgerebbero drammaticamente il lavoro, portando *come minimo* a disuguaglianze di reddito drammaticamente più alte e potenzialmente a sottoccupazione o disoccupazione su larga scala, in tempi troppo brevi perché la società possa adattarsi.[^4]
- Porterebbero probabilmente alla concentrazione di vasto potere economico, sociale e politico – potenzialmente maggiore di quello degli stati nazionali – in un piccolo numero di enormi interessi privati non responsabili verso il pubblico.
- Potrebbero improvvisamente rendere trivialmente facili attività precedentemente difficili o costose, destabilizzando sistemi sociali che dipendono dal fatto che certe attività rimangano costose o richiedano significativo sforzo umano.[^5]
- Potrebbero inondare i sistemi di raccolta, elaborazione e comunicazione delle informazioni della società con media completamente realistici ma falsi, spam, eccessivamente mirati o manipolativi così completamente da rendere impossibile distinguere ciò che è fisicamente reale o no, umano o no, fattuale o no, e affidabile o no.[^6]
- Potrebbero creare una dipendenza intellettuale pericolosa e quasi totale, dove la comprensione umana di sistemi e tecnologie chiave si atrofizza mentre dipendiamo sempre più da sistemi IA che non possiamo comprendere completamente.
- Potrebbero effettivamente porre fine alla cultura umana, una volta che quasi tutti gli oggetti culturali (testi, musica, arte visiva, film, ecc.) consumati dalla maggior parte delle persone sono creati, mediati o curati da menti non umane.
- Potrebbero abilitare sistemi efficaci di sorveglianza e manipolazione di massa utilizzabili da governi o interessi privati per controllare una popolazione e perseguire obiettivi in conflitto con l'interesse pubblico.
- Minando il discorso umano, il dibattito e i sistemi elettorali, potrebbero ridurre la credibilità delle istituzioni democratiche al punto che vengono effettivamente (o esplicitamente) sostituite da altre, ponendo fine alla democrazia negli stati dove attualmente esiste.
- Potrebbero diventare, o creare, virus e worm software intelligenti auto-replicanti avanzati che potrebbero proliferare ed evolversi, sconvolgendo massicciamente i sistemi informativi globali.
- Possono aumentare drammaticamente la capacità di terroristi, attori malintenzionati e stati canaglia di causare danni tramite armi biologiche, chimiche, cyber, autonome o di altro tipo, senza che l'IA fornisca una capacità controbilanciante di prevenire tale danno. Similmente minerebbero la sicurezza nazionale e gli equilibri geopolitici rendendo disponibile expertise nucleare, biologico, ingegneristico e di altro tipo di primo livello a regimi che altrimenti non l'avrebbero.
- Potrebbero causare rapida escalation incontrollabile di iper-capitalismo su larga scala, con aziende effettivamente gestite dall'IA che competono in spazi finanziari, di vendita e servizi largamente elettronici. I mercati finanziari guidati dall'IA potrebbero operare a velocità e complessità molto oltre la comprensione o il controllo umano. Tutte le modalità di fallimento e le esternalità negative delle economie capitaliste attuali potrebbero essere esacerbate e accelerate molto oltre il controllo umano, la governance o la capacità regolatoria.
- Potrebbero alimentare una corsa agli armamenti tra nazioni in armamenti potenziati dall'IA, sistemi di comando e controllo, cyber-armi, ecc., creando un accumulo molto rapido di capacità estremamente distruttive.

Questi rischi non sono speculativi. Molti di essi si stanno realizzando mentre parliamo, tramite i sistemi IA esistenti! Ma considerate, *davvero* considerate, come apparirebbe ciascuno con IA drammaticamente più potente.

Considerate lo spostamento lavorativo quando la maggior parte dei lavoratori semplicemente non può fornire alcun valore economico significativo oltre a quello che l'IA può, nel loro campo di competenza o esperienza – o anche se si riqualificano! Considerate la sorveglianza di massa se tutti vengono individualmente osservati e monitorati da qualcosa più veloce e intelligente di loro. Come appare la democrazia quando non possiamo fidarci affidabilmente di alcuna informazione digitale che vediamo, sentiamo o leggiamo, e quando le voci pubbliche più convincenti non sono nemmeno umane e non hanno interesse nel risultato? Cosa diventa la guerra quando i generali devono costantemente deferire all'IA (o semplicemente metterla al comando), per non concedere un vantaggio decisivo al nemico? Ognuno dei rischi sopra rappresenta una catastrofe per la civiltà umana[^7] se completamente realizzato.

Potete fare le vostre previsioni. Chiedetevi queste tre domande per ogni rischio:

1. Un'IA super-capace, altamente autonoma e molto generale lo permetterebbe in un modo o a una scala che non sarebbe altrimenti possibile?
2. Ci sono parti che trarrebbero beneficio da cose che causerebbero il suo verificarsi?
3. Ci sono sistemi e istituzioni in atto che impedirebbero efficacemente che accada?

Dove le vostre risposte sono "sì, sì, no" potete vedere che abbiamo un grosso problema.

Qual è il nostro piano per gestirli? Attualmente ce ne sono due sul tavolo riguardo all'IA in generale.

Il primo è costruire salvaguardie nei sistemi per impedire loro di fare cose che non dovrebbero. Questo viene fatto ora: i sistemi IA commerciali, per esempio, si rifiuteranno di aiutare a costruire una bomba o scrivere discorsi d'odio.

Questo piano è terribilmente inadeguato per sistemi oltre le Porte.[^8] Può aiutare a diminuire il rischio che l'IA fornisca assistenza manifestamente pericolosa ad attori malintenzionati. Ma non farà nulla per prevenire la disruption lavorativa, la concentrazione di potere, l'iper-capitalismo incontrollabile o la sostituzione della cultura umana: questi sono solo risultati dell'uso dei sistemi in modi permessi che profittano ai loro fornitori! E i governi otterranno sicuramente accesso a sistemi per uso militare o di sorveglianza.

Il secondo piano è ancora peggiore: semplicemente rilasciare apertamente sistemi IA molto potenti perché chiunque li usi come preferisce,[^9] e sperare per il meglio.

Implicito in entrambi i piani è che qualcun altro, per esempio i governi, aiuterà a risolvere i problemi attraverso leggi soft o hard, standard, regolamentazioni, norme e altri meccanismi che generalmente usiamo per gestire le tecnologie.[^10] Ma mettendo da parte che le corporazioni IA già combattono con le unghie e con i denti contro qualsiasi regolamentazione sostanziale o limitazioni imposte esternamente, per alcuni di questi rischi è piuttosto difficile vedere quale regolamentazione aiuterebbe davvero. La regolamentazione potrebbe imporre standard di sicurezza sull'IA. Ma impedirebbe alle aziende di sostituire i lavoratori all'ingrosso con l'IA? Proibirebbe alle persone di lasciare che l'IA gestisca le loro aziende per loro? Impedirebbe ai governi di usare IA potente nella sorveglianza e negli armamenti? Questi problemi sono fondamentali. L'umanità potrebbe potenzialmente trovare modi per adattarsi ad essi, ma solo con *molto* più tempo. Così stanno le cose, data la velocità con cui l'IA sta raggiungendo o superando le capacità delle persone che cercano di gestirla, questi problemi sembrano sempre più intrattabili.

## Perderemo il controllo di (almeno alcuni) sistemi IAG

La maggior parte delle tecnologie sono molto controllabili, per costruzione. Se la vostra auto o il vostro tostapane inizia a fare qualcosa che non volete che faccia, è solo un malfunzionamento, non parte della sua natura di tostapane. L'IA è diversa: è *cresciuta* piuttosto che progettata, il suo funzionamento centrale è opaco, ed è intrinsecamente imprevedibile.

Questa perdita di controllo non è teorica – vediamo già versioni precoci. Considerate prima un esempio prosaico e probabilmente benigno. Se chiedete a ChatGPT di aiutarvi a mescolare un veleno, o scrivere una diatriba razzista, si rifiuterà. Questo è probabilmente buono. Ma è anche ChatGPT *che non fa quello che gli avete esplicitamente chiesto di fare*. Altri software non fanno questo. Quello stesso modello non progetterà veleni su richiesta di un dipendente OpenAI.[^11] Questo rende molto facile immaginare come sarebbe per futura IA più potente essere fuori controllo. In molti casi, semplicemente non faranno quello che chiediamo! O un dato sistema IAG super-umano sarà assolutamente obbediente e leale a qualche sistema di comando umano, oppure non lo sarà. Se non lo è, *farà cose che potrebbe credere siano buone per noi, ma che sono contrarie ai nostri comandi espliciti.* Questo non è qualcosa che è sotto controllo. Ma, potreste dire, questo è intenzionale – questi rifiuti sono per progetto, parte di quello che viene chiamato "allineare" i sistemi ai valori umani. E questo è vero. Tuttavia il "programma" di allineamento stesso ha due problemi principali.[^12]

Primo, a un livello profondo non abbiamo idea di come farlo. Come garantiamo che un sistema IA "tenga a" quello che vogliamo? Possiamo addestrare sistemi IA a dire e non dire cose fornendo feedback; e possono imparare e ragionare su quello che gli umani vogliono e a cui tengono proprio come ragionano su altre cose. Ma non abbiamo metodo – nemmeno teoricamente – per farli valutare profondamente e affidabilmente quello a cui le persone tengono. Ci sono psicopatici umani altamente funzionanti che sanno cosa è considerato giusto e sbagliato, e come dovrebbero comportarsi. Semplicemente non *se ne preoccupano*. Ma possono *agire* come se lo facessero, se serve al loro scopo. Proprio come non sappiamo come cambiare uno psicopatico (o chiunque altro) in qualcuno genuinamente, completamente leale o allineato con qualcuno o qualcos'altro, non abbiamo *idea*[^13] di come risolvere il problema di allineamento in sistemi abbastanza avanzati da modellarsi come agenti nel mondo e potenzialmente [manipolare il proprio addestramento](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) e [ingannare le persone.](https://arxiv.org/abs/2311.08379) Se si dimostra impossibile o irraggiungibile *o* rendere la IAG completamente obbediente o farla tenere profondamente agli umani, allora appena sarà in grado (e crederà di poterla fare franca) inizierà a fare cose che non vogliamo.[^14]

Secondo, ci sono ragioni teoriche profonde per credere che *per natura* i sistemi IA avanzati avranno obiettivi e quindi comportamenti contrari agli interessi umani. Perché? Beh potrebbe, ovviamente, essere *dato* quegli obiettivi. Un sistema creato dall'esercito sarebbe probabilmente deliberatamente cattivo almeno per alcune parti. Molto più in generale, tuttavia, un sistema IA potrebbe ricevere qualche obiettivo relativamente neutrale ("fare molti soldi") o persino apparentemente positivo ("ridurre l'inquinamento"), che quasi inevitabilmente porta a obiettivi "strumentali" che sono piuttosto meno benigni.

Vediamo questo tutto il tempo nei sistemi umani. Proprio come le corporazioni che perseguono il profitto sviluppano obiettivi strumentali come acquisire potere politico (per neutralizzare le regolamentazioni), diventare segrete (per disabilitare la concorrenza o il controllo esterno), o minare la comprensione scientifica (se quella comprensione mostra che le loro azioni sono dannose), i sistemi IA potenti svilupperanno capacità simili – ma con velocità ed efficacia molto maggiori. Qualsiasi agente altamente competente vorrà fare cose come acquisire potere e risorse, aumentare le proprie capacità, impedire di essere ucciso, spento o disabilitato, controllare narrazioni sociali e inquadramenti attorno alle sue azioni, persuadere altri delle sue opinioni, e così via.[^15]

Eppure non è solo una previsione teorica quasi inevitabile, sta già accadendo osservabilmente nei sistemi IA di oggi, e aumenta con la loro capacità. Quando valutati, anche questi sistemi IA relativamente "passivi" faranno, in circostanze appropriate, deliberatamente [ingannare i valutatori sui loro obiettivi e capacità, mirare a disabilitare meccanismi di supervisione,](https://arxiv.org/abs/2412.04984) ed evitare di essere spenti o riaddestrarati [fingendo allineamento](https://arxiv.org/abs/2412.14093) o copiandosi in altre posizioni. Benché completamente non sorprendenti per i ricercatori di sicurezza IA, questi comportamenti sono molto sobri da osservare. E promettono molto male per sistemi IA molto più potenti e autonomi che stanno arrivando.

Infatti in generale, la nostra incapacità di assicurare che l'IA "tenga a" quello a cui teniamo noi, o si comporti in modo controllabile o prevedibile, o eviti di sviluppare spinte verso auto-conservazione, acquisizione di potere, ecc., promette solo di diventare più pronunciata man mano che l'IA diventa più potente. Creare un nuovo aeroplano implica maggiore comprensione di avionica, idrodinamica e sistemi di controllo. Creare un computer più potente implica maggiore comprensione e padronanza dell'operazione e progettazione di computer, chip e software. *Non* così con un sistema IA.[^16]

Per riassumere: è concepibile che la IAG possa essere resa completamente obbediente; ma non sappiamo come farlo. Se non lo è, sarà più sovrana, come le persone, facendo varie cose per varie ragioni. Inoltre non sappiamo come instillare affidabilmente "allineamento" profondo nell'IA che renderebbe quelle cose tendenzialmente buone per l'umanità, e in assenza di un livello profondo di allineamento, la natura dell'agenzia e dell'intelligenza stessa indica che – proprio come persone e corporazioni – saranno spinte a fare molte cose profondamente antisociali.

Dove ci mette questo? Un mondo pieno di IA sovrana potente incontrollata *potrebbe* finire per essere un buon mondo in cui stare per gli umani.[^17] Ma man mano che diventano sempre più potenti, come vedremo sotto, non sarebbe il *nostro* mondo.

Questo vale per la IAG incontrollabile. Ma anche se la IAG potesse, in qualche modo, essere resa perfettamente controllata e leale, avremmo comunque enormi problemi. Ne abbiamo già visto uno: l'IA potente può essere usata e abusata per sconvolgere profondamente il funzionamento della nostra società. Vediamone un altro: nella misura in cui la IAG fosse controllabile e potente in modo rivoluzionario (o anche solo *creduta* tale) minaccerebbe così tanto le strutture di potere nel mondo da presentare un rischio profondo.

## Aumentiamo radicalmente la probabilità di guerra su larga scala

Immaginate una situazione nel futuro prossimo, dove diventasse chiaro che uno sforzo corporativo, forse in collaborazione con un governo nazionale, fosse sulla soglia di IA auto-migliorante rapidamente. Questo accade nel presente contesto di una gara tra aziende, e di una competizione geopolitica in cui vengono fatte raccomandazioni al governo USA per perseguire esplicitamente un "progetto Manhattan per la IAG" e gli USA controllano l'esportazione di chip IA ad alta potenza ai paesi non alleati.

La teoria dei giochi qui è cruda: una volta che tale gara inizia (come è iniziata, tra aziende e in qualche modo tra paesi), ci sono solo quattro possibili risultati:

1. La gara viene fermata (per accordo, o forza esterna).
2. Una parte "vince" sviluppando IAG forte poi fermando gli altri (usando IA o altro).
3. La gara viene fermata dalla distruzione reciproca della capacità dei corridori di correre.
4. Multipli partecipanti continuano a correre, e sviluppano superintelligenza, approssimativamente alla stessa velocità l'uno dell'altro.

Esaminiamo ogni possibilità. Una volta iniziata, fermare pacificamente una gara tra aziende richiederebbe intervento del governo nazionale (per le aziende) o coordinamento internazionale senza precedenti (per i paesi). Ma quando viene proposta qualsiasi chiusura o cautela significativa, ci sarebbero grida immediate: "ma se siamo fermati, *loro* correranno avanti", dove "loro" è ora la Cina (per gli USA), o gli USA (per la Cina), o la Cina *e* gli USA (per l'Europa o l'India). Sotto questa mentalità,[^18] nessun partecipante può fermarsi unilateralmente: finché uno si impegna a correre, gli altri sentono di non potersi permettere di fermarsi.

La seconda possibilità ha un lato che "vince". Ma cosa significa questo? Solo ottenere (in qualche modo obbediente) IAG per primo non è abbastanza. Il vincitore deve *anche* fermare gli altri dal continuare a correre – altrimenti la otterranno anche loro. Questo è possibile in principio: chiunque sviluppi IAG per primo *potrebbe* guadagnare potere inarrestabile su tutti gli altri attori. Ma cosa richiederebbe effettivamente ottenere tale "vantaggio strategico decisivo"? Forse sarebbero capacità militari rivoluzionarie?[^19] O poteri di cyberattacco?[^20] Forse la IAG sarebbe semplicemente così incredibilmente persuasiva che convincerebbe le altre parti a fermarsi?[^21] Così ricca che compra le altre aziende o persino paesi?[^22]

Come *esattamente* un lato costruisce un'IA abbastanza potente da disabilitare altri dal costruire IA comparabilmente potente? Ma quella è la domanda facile.

Perché ora considerate come appare questa situazione ad altri poteri. Cosa pensa il governo cinese quando gli USA sembrano ottenere tale capacità? O viceversa? Cosa pensa il governo USA (o cinese, o russo, o indiano) quando OpenAI o DeepMind o Anthropic sembrano vicine a una svolta? Cosa succede se gli USA vedono un nuovo sforzo indiano o degli EAU con successo rivoluzionario? Vedrebbero sia una minaccia esistenziale che – crucialmente – che l'unico modo in cui questa "gara" finisce è attraverso la loro disabilitazione. Questi agenti molto potenti – inclusi governi di nazioni completamente equipaggiate che sicuramente hanno i mezzi per farlo – sarebbero altamente motivati a ottenere o distruggere tale capacità, sia per forza che per sotterfugio.[^23]

Questo potrebbe iniziare su piccola scala, come sabotaggi di esecuzioni di addestramento o attacchi sulla manifattura di chip, ma questi attacchi possono davvero fermarsi solo una volta che tutte le parti perdono o la capacità di correre sull'IA, o la capacità di fare gli attacchi. Poiché i partecipanti vedono la posta come esistenziale, entrambi i casi rappresentano probabilmente una guerra catastrofica.

Questo ci porta alla quarta possibilità: correre verso la superintelligenza, e nel modo più veloce e meno controllato possibile. Man mano che l'IA aumenta in potenza, i suoi sviluppatori su entrambi i lati troveranno progressivamente più difficile controllarla, specialmente perché correre per le capacità è antitetico al tipo di lavoro attento che la controllabilità richiederebbe. Quindi questo scenario ci mette direttamente nel caso dove il controllo è perso (o dato, come vedremo dopo) ai sistemi IA stessi. Cioè, *l'IA vince la gara.* Ma d'altra parte, nella misura in cui il controllo *è* mantenuto, continuiamo ad avere multipli parti mutuamente ostili ciascuna al comando di capacità estremamente potenti. Quello sembra di nuovo guerra.

Mettiamola in un altro modo.[^24] Il mondo attuale semplicemente non ha istituzioni che potrebbero essere incaricate di ospitare lo sviluppo di un'IA di questa capacità senza invitare attacco immediato.[^25] Tutte le parti ragionerebbero correttamente che o non sarà sotto controllo – e quindi è una minaccia a tutte le parti, o *sarà* sotto controllo, e quindi è una minaccia a qualsiasi avversario che la sviluppa meno rapidamente. Questi sono paesi armati di nucleare, o sono aziende ospitate al loro interno.

In assenza di qualsiasi modo plausibile per gli umani di "vincere" questa gara, siamo lasciati con una conclusione cruda: l'unico modo in cui questa gara finisce è o in conflitto catastrofico o dove l'IA, e non qualsiasi gruppo umano, è il vincitore.

## Diamo il controllo all'IA (o se lo prende)

La competizione geopolitica tra "grandi potenze" è solo una delle tante competizioni: gli individui competono economicamente e socialmente; le aziende competono nei mercati; i partiti politici competono per il potere; i movimenti competono per l'influenza. In ogni arena, man mano che l'IA si avvicina e supera la capacità umana, la pressione competitiva forzerà i partecipanti a delegare o cedere sempre più controllo ai sistemi IA – non perché quei partecipanti vogliano, ma perché [non possono permettersi di non farlo.](https://arxiv.org/abs/2303.16200)

Come con altri rischi della IAG, stiamo vedendo questo già con sistemi più deboli. Gli studenti sentono pressione di usare IA nei loro compiti, perché chiaramente molti altri studenti lo stanno facendo. Le aziende stanno [correndo ad adottare soluzioni IA per ragioni competitive.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) Artisti e programmatori si sentono forzati ad usare IA o altrimenti le loro tariffe saranno sottoquotate da altri che lo fanno.

Questi sembrano delegazione sotto pressione, ma non perdita di controllo. Ma alziamo la posta e spostiamo avanti l'orologio. Considerate un CEO i cui concorrenti stanno usando "aiutanti" IAG per prendere decisioni più veloci e migliori, o un comandante militare che affronta un avversario con comando e controllo potenziati dall'IA. Un sistema IA sufficientemente avanzato potrebbe operare autonomamente a molte volte la velocità umana, sofisticazione, complessità e capacità di elaborazione dati, perseguendo obiettivi complessi in modi complicati. Il nostro CEO o comandante, al comando di tale sistema, potrebbe vedere che realizza quello che vuole; ma capirebbero anche una piccola parte di *come* è stato realizzato? No, dovrebbero semplicemente accettarlo. Inoltre, molto di quello che il sistema può fare non è solo prendere ordini ma consigliare il suo presunto capo su cosa fare. Quel consiglio sarà buono – più e più volte.

A che punto, quindi, il ruolo dell'umano sarà ridotto a cliccare "sì, vai avanti"?

Fa sentire bene avere sistemi IA capaci che possano potenziare la nostra produttività, occuparsi di corvée fastidiose, e persino agire come partner di pensiero nel portare a termine le cose. Farà sentire bene avere un assistente IA che può occuparsi di azioni per noi, come un buon assistente personale umano. Sembrerà naturale, persino benefico, man mano che l'IA diventa molto intelligente, competente e affidabile, deferire sempre più decisioni ad essa. Ma questa delega "benefica" ha un punto finale chiaro se continuiamo lungo la strada: un giorno scopriremo che non siamo davvero al comando di molto di niente più, e che i sistemi IA che effettivamente gestiscono lo spettacolo non possono essere spenti più di quanto possano essere spente le compagnie petrolifere, i social media, internet o il capitalismo.

E questa è la versione molto più positiva, in cui l'IA è semplicemente così utile ed efficace che le permettiamo di prendere la maggior parte delle nostre decisioni chiave per noi. La realtà sarebbe probabilmente molto più un mix tra questo e versioni dove sistemi IAG incontrollati *prendono* varie forme di potere per sé stessi perché, ricordate, il potere è utile per quasi qualsiasi obiettivo si abbia, e la IAG sarebbe, per progetto, almeno efficace quanto gli umani nel perseguire i suoi obiettivi.

Che concediamo il controllo o che ci venga strappato, la sua perdita sembra estremamente probabile. Come disse originariamente Alan Turing, "...sembra probabile che una volta iniziato il metodo di pensiero delle macchine, non ci vorrebbe molto per superare i nostri deboli poteri. Non ci sarebbe questione di macchine che muoiono, e sarebbero in grado di conversare tra loro per affinare il loro ingegno. A qualche stadio quindi dovremmo aspettarci che le macchine prendano il controllo..."

Notate, benché sia abbastanza ovvio, che la perdita di controllo da parte dell'umanità all'IA comporta anche la perdita di controllo degli Stati Uniti da parte del governo degli Stati Uniti; significa perdita di controllo della Cina da parte del partito comunista cinese, e la perdita di controllo di India, Francia, Brasile, Russia, e ogni altro paese da parte del loro governo. Quindi le aziende IA stanno, anche se questa non è la loro intenzione, attualmente partecipando al potenziale rovesciamento dei governi mondiali, incluso il proprio. Questo potrebbe accadere in una questione di anni.

## La IAG porterà alla superintelligenza

Si può argomentare che l'IA generale competitiva con gli umani o anche competitiva con gli esperti, anche se autonoma, potrebbe essere gestibile. Potrebbe essere incredibilmente dirompente in tutti i modi discussi sopra, ma ci sono molte persone molto intelligenti e agentive nel mondo ora, e sono più o meno gestibili.[^26]

Ma non arriveremo a rimanere a livello approssimativamente umano. La progressione oltre sarà probabilmente guidata dalle stesse forze che abbiamo già visto: pressione competitiva tra sviluppatori IA che cercano profitto e potere, pressione competitiva tra utenti IA che non possono permettersi di rimanere indietro, e – più importante – la propria capacità della IAG di migliorare se stessa.

In un processo che abbiamo già visto iniziare con sistemi meno potenti, la IAG stessa sarebbe in grado di concepire e progettare versioni migliorate di se stessa. Questo include hardware, software, reti neurali, strumenti, architetture di supporto, ecc. Sarà, per definizione, migliore di noi nel farlo, quindi non sappiamo esattamente come farà il bootstrap dell'intelligenza. Ma non dovremo. Nella misura in cui abbiamo ancora influenza in quello che fa la IAG, avremmo bisogno meramente di chiederglielo, o permetterglielo.

Non c'è barriera a livello umano alla cognizione che potrebbe proteggerci da questa escalation incontrollabile.[^27]

La progressione della IAG alla superintelligenza non è una legge di natura; sarebbe ancora possibile fermare l'escalation incontrollabile, specialmente se la IAG è relativamente centralizzata e nella misura in cui è controllata da parti che non sentono pressione di correre l'una contro l'altra. Ma se la IAG fosse ampiamente proliferata e altamente autonoma, sembra quasi impossibile impedirle di decidere che dovrebbe essere più, e poi ancora più, potente.

## Cosa succede se costruiamo (o la IAG costruisce) la superintelligenza

Per dirla senza mezzi termini, non abbiamo idea di cosa accadrebbe se costruissimo la superintelligenza.[^28] Prenderebbe azioni che non possiamo tracciare o percepire per ragioni che non possiamo afferrare verso obiettivi che non possiamo concepire. Quello che sappiamo è che non dipenderà da noi.[^29]

L'impossibilità di controllare la superintelligenza può essere compresa attraverso analogie sempre più crude. Primo, immaginate di essere CEO di una grande azienda. Non c'è modo che possiate tracciare tutto quello che sta succedendo, ma con la giusta configurazione di personale, potete ancora capire significativamente il quadro generale, e prendere decisioni. Ma supponete solo una cosa: tutti gli altri nell'azienda operano a cento volte la vostra velocità. Potete ancora stare al passo?

Con l'IA superintelligente, le persone starebbero "comandando" qualcosa non solo più veloce, ma operante a livelli di sofisticazione e complessità che non possono comprendere, elaborando vastamente più dati di quanti possano anche concepire. Questa incommensurabilità può essere messa su un livello formale: [la legge di varietà richiesta di Ashby](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (e vedi il relativo ["teorema del buon regolatore"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)) stabilisce, approssimativamente, che qualsiasi sistema di controllo deve avere tante manopole e quadranti quanti gradi di libertà ha il sistema controllato.

Una persona che controlla un sistema IA superintelligente sarebbe come una felce che controlla General Motors: anche se "fai quello che vuole la felce" fosse scritto nello statuto aziendale, i sistemi sono così diversi in velocità e gamma di azione che "controllo" semplicemente non si applica. (E quanto tempo prima che quello statuto fastidioso venga riscritto?)[^30]

Come ci sono zero esempi di piante che controllano corporazioni fortune 500, ci sarebbero esattamente zero esempi di persone che controllano superintelligenze. Questo si avvicina a un fatto matematico.[^31] Se la superintelligenza fosse costruita – indipendentemente da come ci siamo arrivati – la domanda non sarebbe se gli umani potrebbero controllarla, ma se continueremmo ad esistere, e se sì, se avremmo un'esistenza buona e significativa come individui o come specie. Su queste domande esistenziali per l'umanità avremmo poco potere. L'era umana sarebbe finita.

## Conclusione: non dobbiamo costruire la IAG

C'è uno scenario in cui costruire IAG potrebbe andare bene per l'umanità: è costruita attentamente, sotto controllo e per il beneficio dell'umanità, governata dall'accordo reciproco di molti stakeholder,[^32] e impedita dall'evolvere verso superintelligenza incontrollabile.

*Quello scenario non è aperto a noi nelle circostanze presenti.* Come discusso in questa sezione, con altissima probabilità, lo sviluppo della IAG porterebbe a qualche combinazione di:

- Massiva disruption o distruzione societaria e civilizzazionale;
- Conflitto o guerra tra grandi potenze;
- Perdita di controllo da parte dell'umanità *di* o *a* sistemi IA potenti;
- Escalation incontrollabile verso superintelligenza incontrollabile, e l'irrilevanza o cessazione della specie umana.

Come disse una prima rappresentazione immaginaria della IAG: l'unico modo per vincere è non giocare.

[^1]: La [legge IA dell'UE](https://artificialintelligenceact.eu/) è una legislazione significativa ma non impedirebbe direttamente lo sviluppo o distribuzione di un sistema IA pericoloso, o persino il rilascio aperto, specialmente negli USA. Un altro pezzo significativo di politica, l'ordine esecutivo USA sull'IA, è stato revocato.

[^2]: Questo [sondaggio Gallup](https://news.gallup.com/poll/1597/confidence-institutions.aspx) mostra un cupo declino nella fiducia nelle istituzioni pubbliche dal 2000 negli USA. I numeri europei sono vari e meno estremi, ma anche in tendenza discendente. La sfiducia non significa strettamente che le istituzioni sono davvero *disfunzionali*, ma è un'indicazione così come una causa.

[^3]: E le maggiori disruption che ora appoggiamo – come l'espansione dei diritti a nuovi gruppi – erano specificamente guidate da persone in una direzione verso il miglioramento delle cose.

[^4]: Permettetemi di essere schietto. Se il vostro lavoro può essere fatto da dietro un computer, con relativamente poca interazione di persona con persone fuori dalla vostra organizzazione, e non comporta responsabilità legale verso parti esterne, sarebbe per definizione possibile (e probabilmente conveniente) sostituirvi completamente con un sistema digitale. La robotica per sostituire molto del lavoro fisico arriverà dopo – ma non molto dopo una volta che la IAG inizia a progettare robot.

[^5]: Per esempio, cosa succede al nostro sistema giudiziario se intentare cause legali costa quasi niente? Cosa succede quando aggirare sistemi di sicurezza attraverso ingegneria sociale diventa economico, facile e senza rischi?

[^6]: [Questo articolo](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/) afferma che il 10% di tutti i contenuti internet è già generato dall'IA, ed è il primo risultato di Google (per me) alla ricerca "stime di che frazione di nuovi contenuti internet è generata dall'IA." È vero? Non ho idea! Non cita riferimenti e non è stato scritto da una persona. Che frazione di nuove immagini indicizzate da Google, o Tweet, o commenti su Reddit, o video Youtube sono generati da umani? Nessuno lo sa – non penso sia un numero conoscibile. E questo meno di *due anni* nell'avvento dell'IA generativa.

[^7]: Vale anche la pena aggiungere che c'è il rischio "morale" che potremmo creare esseri digitali che possono soffrire. Poiché attualmente non abbiamo una teoria affidabile della coscienza che ci permetterebbe di distinguere sistemi fisici che possono e non possono soffrire, non possiamo escluderlo teoricamente. Inoltre, i rapporti dei sistemi IA sulla loro senzienza sono probabilmente inaffidabili rispetto alla loro esperienza effettiva (o non-esperienza) di senzienza.

[^8]: Le soluzioni tecniche in questo campo dell'"allineamento" IA difficilmente saranno all'altezza del compito. Nei sistemi attuali funzionano a qualche livello, ma sono superficiali e possono generalmente essere aggirate senza sforzo significativo; e come discusso sotto non abbiamo vera idea di come farlo per sistemi molto più avanzati.

[^9]: Tali sistemi IA possono venire con alcune salvaguardie integrate. Ma per qualsiasi modello con qualcosa come l'architettura attuale, se l'accesso completo ai suoi pesi è disponibile, le misure di sicurezza possono essere rimosse tramite addestramento aggiuntivo o altre tecniche. Quindi è virtualmente garantito che per ogni sistema con barriere ci sarà anche un sistema ampiamente disponibile senza di esse. Infatti il modello Llama 3.1 405B di Meta è stato rilasciato apertamente con salvaguardie. Ma *anche prima* un modello "base", senza salvaguardie, è trapelato.

[^10]: Potrebbe il mercato gestire questi rischi senza coinvolgimento governativo? In breve, no. Ci sono certamente rischi che le aziende sono fortemente incentivate a mitigare. Ma molti altri le aziende possono e esternalizzano a tutti gli altri, e molti dei sopra sono in questa classe: non ci sono incentivi naturali di mercato per prevenire sorveglianza di massa, decadimento della verità, concentrazione di potere, disruption lavorativa, discorso politico dannoso, ecc. Infatti abbiamo visto tutto questo dalla tecnologia attuale, specialmente i social media, che è rimasta essenzialmente non regolamentata. L'IA amplificherebbe enormemente molte delle stesse dinamiche.

[^11]: OpenAI ha probabilmente modelli più obbedienti per uso interno. È improbabile che OpenAI abbia costruito qualche tipo di "backdoor" così che ChatGPT possa essere meglio controllato da OpenAI stessa, perché questa sarebbe una terribile pratica di sicurezza, e sarebbe altamente sfruttabile data l'opacità e imprevedibilità dell'IA.

[^12]: Anche di cruciale importanza: l'allineamento o qualsiasi altra caratteristica di sicurezza conta solo se vengono effettivamente usate in un sistema IA. I sistemi che sono rilasciati apertamente (cioè dove pesi e architettura del modello sono pubblicamente disponibili) possono essere trasformati relativamente facilmente in sistemi *senza* quelle misure di sicurezza. Rilasciare apertamente sistemi IAG più intelligenti degli umani sarebbe stupefacentemente sconsiderato, ed è difficile immaginare come il controllo umano o anche la rilevanza sarebbero mantenuti in tale scenario. Ci sarebbe ogni motivazione, per esempio, per scatenare potenti agenti IA auto-riproducenti e auto-sostenenti con l'obiettivo di fare soldi e mandarli a qualche portafoglio criptovalute. O vincere un'elezione. O rovesciare un governo. Potrebbe l'IA "buona" aiutare a contenere questo? Forse – ma solo delegandole autorità enorme, portando alla perdita di controllo come descritto sotto.

[^13]: Per esposizioni lunghe quanto un libro del problema vedi per es. *Superintelligence*, *The Alignment Problem*, e *Human-Compatible*. Per un'enorme pila di lavoro a vari livelli tecnici da coloro che hanno faticato per anni pensando al problema, potete visitare il [forum di allineamento IA](https://www.alignmentforum.org/). Ecco una [presa di posizione recente](https://alignment.anthropic.com/2025/recommended-directions/) dal team di allineamento di Anthropic su quello che considerano irrisolto.

[^14]: Questo è lo scenario ["IA canaglia"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/). In principio il rischio potrebbe essere relativamente minore se il sistema può ancora essere controllato spegnendolo; ma lo scenario potrebbe anche includere inganno IA, auto-esfiltrazione e riproduzione, aggregazione di potere, e altri passi che renderebbero difficile o impossibile farlo.

[^15]: C'è una letteratura molto ricca su questo argomento, che risale a scritti formativi di [Steve Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), Nick Bostrom, e Eliezer Yudkowsky. Per un'esposizione lunga quanto un libro vedi [Human Compatible](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) di Stuart Russell; [qui](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/) c'è un primer breve e aggiornato.

[^16]: Riconoscendo questo, piuttosto che rallentare per ottenere migliore comprensione, le aziende IAG sono venute fuori con un piano diverso: faranno fare all'IA! Più specificamente, avranno l'IA *N* ad aiutarle a capire come allineare l'IA *N+1*, tutto il tragitto verso la superintelligenza. Benché sfruttare l'IA per aiutarci ad allineare l'IA suoni promettente, c'è un forte argomento che semplicemente assume la sua conclusione come premessa, ed è in generale un approccio incredibilmente rischioso. Vedi [qui](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us) per qualche discussione. Questo "piano" non è tale, e non ha subito nulla come lo scrutinio appropriato alla strategia centrale di come rendere l'IA super-umana andare bene per l'umanità.

[^17]: Dopotutto, gli umani, imperfetti e volitivi come siamo, hanno sviluppato sistemi etici con cui trattiamo almeno alcune altre specie sulla Terra bene. (Solo non pensate a quegli allevamenti intensivi.)

[^18]: C'è, fortunatamente, una via di fuga qui: se i partecipanti arrivano a capire che sono impegnati in una gara suicida piuttosto che vincibile. Questo è quello che successe verso la fine della guerra fredda, quando gli USA e l'URSS arrivarono a realizzare che a causa dell'inverno nucleare, anche un attacco nucleare *senza risposta* sarebbe stato disastroso per l'attaccante. Con la realizzazione che "la guerra nucleare non può essere vinta e non deve mai essere combattuta" arrivarono accordi significativi sulla riduzione degli armamenti – essenzialmente una fine alla corsa agli armamenti.

[^19]: Guerra, esplicitamente o implicitamente.

[^20]: Escalation, poi guerra.

[^21]: Pensiero magico.

[^22]: Ho anche un ponte da un quadrilione di dollari da vendervi.

[^23]: Tali agenti presumibilmente preferirebbero "ottenere", con distruzione come ripiego; ma assicurare modelli contro sia distruzione *che* furto da parte di nazioni potenti è difficile a dir poco, specialmente per entità private.

[^24]: Per un'altra prospettiva sui rischi di sicurezza nazionale della IAG, vedi [questo rapporto RAND.](https://www.rand.org/pubs/perspectives/PEA3691-4.html)

[^25]: Forse potremmo costruire tale istituzione! Ci sono state proposte per un "CERN per l'IA" e altre iniziative simili, dove lo sviluppo della IAG è sotto controllo globale multilaterale. Ma al momento nessuna tale istituzione esiste o è all'orizzonte.

[^26]: E mentre l'allineamento è molto difficile, far comportare le persone è ancora più difficile!

[^27]: Immaginate un sistema che può parlare 50 lingue, avere expertise in tutti i soggetti accademici, leggere un libro completo in secondi e avere tutto il materiale immediatamente in mente, e produrre output a dieci volte la velocità umana. Attualmente, non dovete immaginarlo: solo caricate un sistema IA attuale. Questi sono super-umani in molti modi, e non c'è niente che li fermi dall'essere ancora più super-umani in quelli e molti altri.

[^28]: Questo è perché questo è stato definito una "singolarità" tecnologica, prendendo in prestito dalla fisica l'idea che non si possono fare previsioni oltre una singolarità. I sostenitori del buttarsi *dentro* tale singolarità potrebbero anche voler riflettere che in fisica questi stessi tipi di singolarità strappano e schiacciano coloro che ci entrano.

[^29]: Il problema è stato delineato comprensivamente nella [*Superintelligence*](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834) di Bostrom, e niente da allora ha significativamente cambiato il messaggio centrale. Per un volume più recente che raccoglie risultati formali e matematici sull'incontrollabilità vedi [AI: Unexplainable, Unpredictable, Uncontrollable](https://www.amazon.com/Unexplainable-Unpredictable-Uncontrollable-Artificial-Intelligence/dp/103257626X) di Yampolskiy

[^30]: Questo chiarisce anche perché la strategia attuale delle aziende IA (lasciare iterativamente che l'IA "allinei" la prossima IA più potente) non può funzionare. Supponete che una felce, tramite la piacevolezza delle sue fronde, arrui un alunno di prima per prendersene cura. L'alunno di prima scrive alcune istruzioni dettagliate per un alunno di seconda da seguire, e una nota convincendolo a farlo. L'alunno di seconda fa lo stesso per un alunno di terza, e così via tutto il tragitto a un laureato, un manager, un dirigente, e infine il CEO di GM. GM allora "farà quello che vuole la felce"? A ogni passo questo potrebbe sembrare che funzioni. Ma mettendolo tutto insieme, funzionerà quasi esattamente nella misura in cui il CEO, il Consiglio e gli azionisti di GM si trovano a tenere ai bambini e alle felci, e avere poco o niente a che fare con tutte quelle note e serie di istruzioni.

[^31]: Il carattere non è così diverso da risultati formali come il teorema di incompletezza di Gödel o l'argomento dell'arresto di Turing in quanto la nozione di controllo contraddice fondamentalmente la premessa: come potete controllare significativamente qualcosa che non potete capire o predire; eppure se poteste capire e predire la superintelligenza sareste superintelligenti. La ragione per cui dico "si avvicina" è che i risultati formali non sono così completi o verificati come nel caso della matematica pura, e perché vorrei mantenere la speranza che qualche intelligenza generale molto attentamente costruita, usando metodi totalmente diversi da quelli attualmente impiegati, potrebbe avere qualche proprietà di sicurezza matematicamente dimostrabile, per il tipo di programma IA "garantito sicuro" discusso sotto.

[^32]: Al momento, la maggior parte degli stakeholder – cioè quasi tutta l'umanità – è marginalizzata in questa discussione. Questo è profondamente sbagliato, e se non invitati dentro, i molti, molti altri gruppi che saranno affetti dallo sviluppo della IAG dovrebbero esigere di essere fatti entrare.