# Capitolo 8 - Come non costruire l'IAG

L'IAG non è inevitabile – oggi ci troviamo a un bivio. Questo capitolo presenta una proposta su come potremmo impedire che venga costruita.

Se la strada che stiamo percorrendo attualmente conduce alla probabile fine della nostra civiltà, come cambiamo direzione?

Supponiamo che il desiderio di smettere di sviluppare IAG e superintelligenza fosse diffuso e potente,[^1] perché diventa comprensione comune che l'IAG assorbirebbe potere anziché concederlo, e rappresenterebbe un pericolo profondo per la società e l'umanità. Come chiuderemmo le Porte?

Al momento conosciamo solo un modo per *creare* IA potente e generale, ovvero attraverso calcoli davvero massicci di reti neurali profonde. Poiché si tratta di operazioni incredibilmente difficili e costose da fare, c'è un senso in cui *non* farle è facile.[^2] Ma abbiamo già visto le forze che spingono verso l'IAG, e le dinamiche teorico-ludiche che rendono molto difficile per qualsiasi parte fermarsi unilateralmente. Quindi ci vorrebbe una combinazione di interventi dall'esterno (cioè i governi) per fermare le corporazioni, e accordi tra governi per fermare se stessi.[^3] Come potrebbe essere?

È utile prima distinguere tra sviluppi dell'IA che devono essere *impediti* o *proibiti*, e quelli che devono essere *gestiti*. I primi sarebbero principalmente l'escalation incontrollabile verso la superintelligenza.[^4] Per lo sviluppo proibito, le definizioni dovrebbero essere il più precise possibile, e sia la verifica che l'applicazione dovrebbero essere pratiche. Ciò che deve essere *gestito* sarebbero i sistemi di IA generali e potenti – che abbiamo già, e che avranno molte aree grigie, sfumature e complessità. Per questi, istituzioni forti ed efficaci sono cruciali.

Possiamo anche delineare utilmente questioni che devono essere affrontate a livello internazionale (incluso tra rivali o avversari geopolitici)[^5] da quelle che singole giurisdizioni, paesi, o gruppi di paesi possono gestire. Lo sviluppo proibito rientra largamente nella categoria "internazionale", perché un divieto locale sullo sviluppo di una tecnologia può generalmente essere aggirato cambiando ubicazione.[^6]

Infine, possiamo considerare gli strumenti nella cassetta degli attrezzi. Ce ne sono molti, inclusi strumenti tecnici, soft law (standard, norme, ecc.), hard law (regolamenti e requisiti), responsabilità civile, incentivi di mercato, e così via. Prestiamo attenzione speciale a uno che è particolare all'IA.

## Sicurezza e governance della capacità computazionale

Uno strumento centrale nel governare l'IA ad alta potenza sarà l'hardware che richiede. Il software prolifera facilmente, ha costi marginali di produzione quasi nulli, attraversa i confini banalmente, e può essere modificato istantaneamente; niente di questo è vero per l'hardware. Tuttavia, come abbiamo discusso, enormi quantità di questa "capacità computazionale" sono necessarie sia durante l'addestramento dei sistemi di IA che durante l'inferenza per ottenere i sistemi più capaci. La capacità computazionale può essere facilmente quantificata, contabilizzata e verificata, con relativamente poca ambiguità una volta sviluppate buone regole per farlo. Più crucialmente, grandi quantità di calcolo sono, come l'uranio arricchito, una risorsa molto scarsa, costosa e difficile da produrre. Sebbene i chip per computer siano onnipresenti, l'hardware richiesto per l'IA è costoso ed enormemente difficile da produrre.[^7]

Ciò che rende i chip specializzati per l'IA *molto più* gestibili come risorsa scarsa rispetto all'uranio è che possono includere meccanismi di sicurezza basati su hardware. La maggior parte dei telefoni cellulari moderni, e alcuni laptop, hanno caratteristiche hardware specializzate on-chip che consentono loro di assicurarsi di installare solo software e aggiornamenti del sistema operativo approvati, di conservare e proteggere dati biometrici sensibili sul dispositivo, e di poter essere resi inutili a chiunque tranne al loro proprietario se persi o rubati. Negli ultimi anni tali misure di sicurezza hardware sono diventate ben consolidate e ampiamente adottate, e generalmente si sono dimostrate abbastanza sicure.

La novità chiave di queste caratteristiche è che legano hardware e software insieme usando la crittografia.[^8] Cioè, avere semplicemente un particolare pezzo di hardware per computer non significa che un utente possa fare tutto quello che vuole con esso applicando software diverso. E questo legame fornisce anche sicurezza potente perché molti attacchi richiederebbero una violazione della sicurezza *hardware* piuttosto che solo *software*.

Diversi rapporti recenti (ad esempio da [GovAI e collaboratori](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips), e [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) hanno sottolineato che caratteristiche hardware simili incorporate nell'hardware computazionale all'avanguardia rilevante per l'IA potrebbero svolgere un ruolo estremamente utile nella sicurezza e governance dell'IA. Esse abilitano una serie di funzioni disponibili a un "governatore"[^9] che uno potrebbe non immaginare fossero disponibili o anche possibili. Come alcuni esempi chiave:

- *Geolocalizzazione*: I sistemi possono essere configurati in modo che i chip abbiano una ubicazione nota, e possano agire diversamente (o essere spenti del tutto) basandosi sull'ubicazione.[^10]
- *Connessioni autorizzate*: ogni chip può essere configurato con una lista di autorizzazione applicata via hardware di particolari altri chip con cui può collegarsi in rete, ed essere incapace di connettersi con chip non su questa lista.[^11] Questo può limitare la dimensione dei cluster comunicanti di chip.[^12]
- *Inferenza o addestramento misurati (e auto-spegnimento)*: Un governatore può concedere in licenza solo una certa quantità di addestramento o inferenza (in tempo, o FLOP, o possibilmente token) da eseguire da un utente, dopo di che è richiesta nuova autorizzazione. Se gli incrementi sono piccoli, allora è richiesta ri-autorizzazione relativamente continua di un modello. Il modello può quindi essere "spento" semplicemente negando questo segnale di licenza.[^13]
- *Limite di velocità*: Un modello è impedito dall'eseguire a velocità di inferenza superiore a un certo limite che è determinato da un governatore o altrimenti. Questo potrebbe essere implementato tramite un set limitato di connessioni autorizzate, o con mezzi più sofisticati.
- *Addestramento attestato*: Una procedura di addestramento può fornire prova crittograficamente sicura che un particolare set di codici, dati, e quantità di uso di capacità computazionale furono impiegati nella generazione del modello.

## Come non costruire superintelligenza: limiti globali sulla capacità computazionale per addestramento e inferenza

Con queste considerazioni – specialmente riguardo al calcolo – in atto, possiamo discutere come chiudere le Porte alla superintelligenza artificiale; ci rivolgeremo poi a prevenire l'IAG completa, e gestire i modelli di IA mentre si avvicinano e superano la capacità umana in aspetti diversi.

Il primo ingrediente è, naturalmente, la comprensione che la superintelligenza non sarebbe controllabile, e che le sue conseguenze sono fondamentalmente imprevedibili. Almeno la Cina e gli USA devono decidere indipendentemente, per questo o altri scopi, di non costruire superintelligenza.[^14] Poi è necessario un accordo internazionale tra loro e altri, con un forte meccanismo di verifica e applicazione, per assicurare a tutte le parti che i loro rivali non stanno defezionando e decidendo di tentare la fortuna.

Per essere verificabili e applicabili i limiti dovrebbero essere limiti fermi, e il più chiari possibile. Questo sembra un problema virtualmente impossibile: limitare le capacità di software complesso con proprietà imprevedibili, in tutto il mondo. Fortunatamente la situazione è molto migliore di così, perché la stessa cosa che ha reso possibile l'IA avanzata – un'enorme quantità di calcolo – è molto, molto più facile da controllare. Sebbene potrebbe ancora consentire alcuni sistemi potenti e pericolosi, l'*escalation incontrollabile di superintelligenza* può probabilmente essere prevenuta da un limite massimo sulla quantità di calcolo che va in una rete neurale, insieme a un limite di velocità sulla quantità di inferenza che un sistema di IA (di reti neurali connesse e altro software) può eseguire. Una versione specifica di questo è proposta sotto.

Potrebbe sembrare che porre limiti globali fermi sul calcolo dell'IA richiederebbe livelli enormi di coordinamento internazionale e sorveglianza intrusiva che distrugge la privacy. Fortunatamente, non sarebbe così. La [catena di approvvigionamento estremamente ristretta e con colli di bottiglia](https://arxiv.org/abs/2402.08797) fa sì che una volta che un limite è posto legalmente (sia per legge che per ordine esecutivo), la verifica della conformità a quel limite richiederebbe solo il coinvolgimento e la cooperazione di una manciata di grandi aziende.[^15]

Un piano come questo ha una serie di caratteristiche altamente desiderabili. È minimamente invasivo nel senso che solo poche aziende principali hanno requisiti posti su di loro, e solo cluster di calcolo abbastanza significativi sarebbero governati. I chip rilevanti contengono già le capacità hardware necessarie per una prima versione.[^16] Sia l'implementazione che l'applicazione si basano su restrizioni legali standard. Ma queste sono supportate da termini d'uso dell'hardware e da controlli hardware, semplificando vastamente l'applicazione e prevenendo l'inganno da parte di aziende, gruppi privati, o anche paesi. C'è ampio precedente per aziende hardware che pongono restrizioni remote sull'uso del loro hardware, e bloccano/sbloccano particolari capacità esternamente,[^17] incluso anche in CPU ad alta potenza nei data center.[^18] Anche per la frazione piuttosto piccola di hardware e organizzazioni coinvolte, la supervisione potrebbe essere limitata alla telemetria, senza accesso diretto ai dati o modelli stessi; e il software per questo potrebbe essere aperto all'ispezione per dimostrare che non vengono registrati dati aggiuntivi. Lo schema è internazionale e cooperativo, e abbastanza flessibile ed estensibile. Poiché il limite è principalmente sull'hardware piuttosto che sul software, è relativamente agnostico riguardo a come avviene lo sviluppo e il deployment del software di IA, ed è compatibile con una varietà di paradigmi inclusa l'IA più "decentralizzata" o "pubblica" mirata a combattere la concentrazione di potere guidata dall'IA.

Una chiusura delle Porte basata su calcolo ha anche svantaggi. Primo, è lontana dall'essere una soluzione completa al problema della governance dell'IA in generale. Secondo, man mano che l'hardware per computer diventa più veloce, il sistema "catturerebbe" sempre più hardware in cluster sempre più piccoli (o anche singole GPU).[^19] È anche possibile che a causa di miglioramenti algoritmici un limite di calcolo anche più basso sarebbe nel tempo necessario,[^20] o che la quantità di calcolo diventi largamente irrilevante e chiudere la Porta richiederebbe invece un regime di governance più dettagliato basato sul rischio o sulla capacità per l'IA. Terzo, non importa le garanzie e il piccolo numero di entità coinvolte, tale sistema è destinato a creare resistenza riguardo privacy e sorveglianza, tra altre preoccupazioni.[^21]

Naturalmente, sviluppare e implementare uno schema di governance che limita il calcolo in un periodo di tempo breve sarà abbastanza sfidante. Ma è assolutamente fattibile.

## I-A-G: La tripla-intersezione come base del rischio, e della politica

Rivolgiamoci ora all'IAG. Linee ferme e definizioni qui sono più difficili, perché certamente abbiamo intelligenza che è artificiale e generale, e per nessuna definizione esistente tutti saranno d'accordo se o quando esiste. Inoltre, un limite di calcolo o inferenza è uno strumento alquanto grezzo (il calcolo essendo un proxy per la capacità, che è poi un proxy per il rischio) che – a meno che non sia abbastanza basso – è improbabile che prevenga l'IAG abbastanza potente da causare disruzione sociale o civilizzazionale o rischi acuti.

Ho argomentato che i rischi più acuti emergono dalla tripla-intersezione di capacità molto alta, alta autonomia, e grande generalità. Questi sono i sistemi che – se vengono sviluppati affatto – devono essere gestiti con enorme cura. Creando standard rigorosi (attraverso responsabilità civile e regolamentazione) per sistemi che combinano tutte e tre le proprietà, possiamo incanalare lo sviluppo dell'IA verso alternative più sicure.

Come con altre industrie e prodotti che potrebbero potenzialmente danneggiare i consumatori o il pubblico, i sistemi di IA richiedono regolamentazione attenta da parte di agenzie governative efficaci e autorizzate. Questa regolamentazione dovrebbe riconoscere i rischi inerenti dell'IAG, e prevenire che vengano sviluppati sistemi di IA ad alta potenza inaccettabilmente rischiosi.[^22]

Tuttavia, regolamentazione su larga scala, specialmente con denti veri che sicuramente saranno opposti dall'industria,[^23] prende tempo[^24] così come convinzione politica che sia necessaria.[^25] Dato il ritmo del progresso, questo potrebbe prendere più tempo di quello che abbiamo disponibile.

Su una scala temporale molto più veloce e mentre le misure regolamentari vengono sviluppate, possiamo dare alle aziende gli incentivi necessari per (a) desistere da attività ad altissimo rischio e (b) sviluppare sistemi comprensivi per valutare e mitigare il rischio, chiarificando e aumentando i livelli di responsabilità civile per i sistemi più pericolosi. L'idea sarebbe di imporre i livelli più alti di responsabilità – stretta e in alcuni casi penale personale – per sistemi nella tripla-intersezione di alta autonomia-generalità-intelligenza, ma fornire "porti sicuri" a responsabilità più tipica basata su colpa per sistemi in cui una di quelle proprietà manca o è garantita essere gestibile. Cioè, per esempio, un sistema "debole" che è generale e autonomo (come un assistente personale capace e affidabile ma limitato) sarebbe soggetto a livelli di responsabilità più bassi. Allo stesso modo un sistema ristretto e autonomo come un'auto a guida autonoma sarebbe comunque soggetto alla regolamentazione significativa che già ha, ma non responsabilità aumentata. Similmente per un sistema altamente capace e generale che è "passivo" e largamente incapace di azione indipendente. I sistemi che mancano di *due* delle tre proprietà sono ancora più gestibili e i porti sicuri sarebbero ancora più facili da rivendicare. Questo approccio rispecchia come gestiamo altre tecnologie potenzialmente pericolose:[^26] responsabilità più alta per configurazioni più pericolose crea incentivi naturali per alternative più sicure.

Il risultato predefinito di tali alti livelli di responsabilità, che agiscono per *internalizzare* il rischio IAG alle aziende piuttosto che scaricarlo sul pubblico, è probabilmente (e sperabilmente!) per le aziende di semplicemente non sviluppare IAG completa fino a e a meno che non possano genuinamente renderla affidabile, sicura, e controllabile dato che la *loro propria leadership* sono le parti a rischio. (Nel caso questo non sia sufficiente, la legislazione che chiarifica la responsabilità dovrebbe anche permettere esplicitamente il risarcimento ingiuntivo, cioè un giudice che ordina una fermata, per attività che sono chiaramente nella zona di pericolo e discutibilmente pongono un rischio pubblico.) Man mano che la regolamentazione entra in atto, rispettare la regolamentazione può diventare il porto sicuro, e i porti sicuri da bassa autonomia, ristrettezza, o debolezza dei sistemi di IA possono convertirsi in regimi regolamentari relativamente più leggeri.

## Disposizioni chiave di una chiusura delle Porte

Con la discussione sopra in mente, questa sezione fornisce proposte per disposizioni chiave che implementerebbero e manterrebbero il divieto su IAG completa e superintelligenza, e gestione di IA competitiva a livello umano o esperto per scopi generali vicino alla soglia dell'IAG completa.[^27] Ha quattro pezzi chiave: 1) contabilità e supervisione della capacità computazionale, 2) limiti computazionali nell'addestramento e operazione dell'IA, 3) un quadro di responsabilità, e 4) standard di sicurezza e protezione a livelli definiti che includono requisiti regolamentari fermi. Questi sono descritti succintamente di seguito, con ulteriori dettagli o esempi di implementazione dati in tre tabelle accompagnatorie. Importante, notare che questi sono lontani da tutto ciò che sarà necessario per governare sistemi di IA avanzati; mentre avranno benefici aggiuntivi di sicurezza e protezione, sono mirati a chiudere la Porta all'escalation incontrollabile dell'intelligenza, e reindirizzare lo sviluppo dell'IA in una direzione migliore.

### 1\. Contabilità della capacità computazionale, e trasparenza

- Un'organizzazione di standard (ad esempio NIST negli USA seguito da ISO/IEEE internazionalmente) dovrebbe codificare uno standard tecnico dettagliato per la capacità computazionale totale usata nell'addestrare e operare modelli di IA, in FLOP, e la velocità in FLOP/s a cui operano. Dettagli per come questo potrebbe apparire sono dati nell'Appendice A.[^28]
- Un requisito – sia per nuova legislazione che sotto autorità esistente[^29] – dovrebbe essere imposto dalle giurisdizioni in cui avviene addestramento di IA su larga scala per calcolare e riportare a un corpo regolamentario o altra agenzia i FLOP totali usati nell'addestrare e operare tutti i modelli sopra una soglia di 10<sup>25</sup> FLOP o 10<sup>18</sup> FLOP/s.[^30]
- Questi requisiti dovrebbero essere introdotti a fasi, inizialmente richiedendo stime di buona fede ben documentate su base trimestrale, con fasi successive che richiedono progressivamente standard più alti, fino a FLOP totali e FLOP/s crittograficamente attestati allegati a ogni *output* del modello.
- Questi rapporti dovrebbero essere complementati da stime ben documentate del costo energetico e finanziario marginale usato nel generare ogni output di IA.

Razionale: Questi numeri ben calcolati e riportati trasparentemente fornirebbero la base per limiti di addestramento e operazione, così come un porto sicuro da misure di responsabilità più alte (vedi Appendici C e D).

### 2\. Limiti di capacità computazionale per addestramento e operazione

- Le giurisdizioni che ospitano sistemi di IA dovrebbero imporre un limite fermo sulla capacità computazionale totale che va in qualsiasi output di modello di IA, iniziando a 10<sup>27</sup> FLOP[^31] e aggiustabile come appropriato.
- Le giurisdizioni che ospitano sistemi di IA dovrebbero imporre un limite fermo sulla velocità di calcolo degli output di modelli di IA, iniziando a 10<sup>20</sup> FLOP/s e aggiustabile come appropriato.

Razionale: La capacità computazionale totale, mentre molto imperfetta, è un proxy per la capacità dell'IA (e il rischio) che è concretamente misurabile e verificabile, quindi fornisce una barriera ferma per limitare le capacità. Una proposta di implementazione concreta è data nell'Appendice B.

### 3\. Responsabilità aumentata per sistemi pericolosi

- La creazione e operazione[^32] di un sistema di IA avanzato che è altamente generale, capace, e autonomo, dovrebbe essere chiarita legalmente via legislazione per essere soggetta a responsabilità stretta, solidale, piuttosto che basata su colpa di singola parte.[^33]
- Un processo legale dovrebbe essere disponibile per fare casi di sicurezza affermativi, che concederebbero porto sicuro dalla responsabilità stretta per sistemi che sono piccoli (in termini di calcolo), deboli, ristretti, passivi, o che hanno sufficienti garanzie di sicurezza, protezione, e controllabilità.
- Un percorso esplicito e set di condizioni per il risarcimento ingiuntivo per fermare attività di addestramento e inferenza dell'IA che costituiscono un pericolo pubblico dovrebbe essere delineato.

Razionale: I sistemi di IA non possono essere ritenuti responsabili, quindi dobbiamo ritenere individui umani e organizzazioni responsabili per il danno che causano (responsabilità).[^34] L'IAG incontrollabile è una minaccia per la società e la civiltà e in assenza di un caso di sicurezza dovrebbe essere considerata anormalmente pericolosa. Mettere il carico di responsabilità sugli sviluppatori di mostrare che i modelli potenti sono abbastanza sicuri da non essere considerati "anormalmente pericolosi" incentiva lo sviluppo sicuro, insieme a trasparenza e tenuta di registri per rivendicare quei porti sicuri. La regolamentazione può poi prevenire danno dove la deterrenza dalla responsabilità è insufficiente. Infine, gli sviluppatori di IA sono già responsabili per i danni che causano, quindi chiarire legalmente la responsabilità per i sistemi più rischiosi può essere fatto immediatamente, senza che standard altamente dettagliati vengano sviluppati; questi possono poi svilupparsi nel tempo. I dettagli sono dati nell'Appendice C.

### 4\. Regolamentazione di sicurezza per l'IA

Un sistema regolamentario che affronta rischi acuti su larga scala dell'IA richiederà al minimo:

- L'identificazione o creazione di un set appropriato di corpi regolamentari, probabilmente una nuova agenzia;
- Un quadro comprensivo di valutazione del rischio;[^35]
- Un quadro per casi di sicurezza affermativi, basato in parte sul quadro di valutazione del rischio, da essere fatti dagli sviluppatori, e per controllo da parte di gruppi e agenzie *indipendenti*;
- Un sistema di licenze a livelli, con livelli che tracciano livelli di capacità.[^36] Le licenze sarebbero concesse sulla base di casi di sicurezza e controlli, per sviluppo e deployment di sistemi. I requisiti andrebbero dalla notifica al livello basso, a garanzie quantitative di sicurezza, protezione, e controllabilità prima dello sviluppo, al livello più alto. Questi preverebbero il rilascio di sistemi fino a che non sono dimostrati sicuri, e proibirebbero lo sviluppo di sistemi intrinsecamente non sicuri. L'Appendice D fornisce una proposta per ciò che tali standard di sicurezza e protezione potrebbero comportare.
- Accordi per portare tali misure a livello internazionale, inclusi corpi internazionali per armonizzare norme e standard, e potenzialmente agenzie internazionali per rivedere casi di sicurezza.

Razionale: Alla fine, la responsabilità non è il meccanismo giusto per prevenire rischio su larga scala al pubblico da una nuova tecnologia. Regolamentazione comprensiva, con corpi regolamentari autorizzati, sarà necessaria per l'IA proprio come per ogni altra industria principale che pone un rischio al pubblico.[^37]

La regolamentazione verso la prevenzione di altri rischi pervasivi ma meno acuti è probabile che vari nella sua forma da giurisdizione a giurisdizione. La cosa cruciale è evitare di sviluppare i sistemi di IA che sono così rischiosi che questi rischi sono ingestibili.

## E poi?

Nel prossimo decennio, man mano che l'IA diventa più pervasiva e la tecnologia core avanza, due cose chiave sono probabilmente destinate ad accadere. Primo, la regolamentazione dei sistemi di IA potenti esistenti diventerà più difficile, tuttavia ancora più necessaria. È probabile che almeno alcune misure che affrontano rischi di sicurezza su larga scala richiederanno accordo a livello internazionale, con giurisdizioni individuali che applicano regole basate su accordi internazionali.

Secondo, i limiti di capacità computazionale per addestramento e operazione diventeranno più difficili da mantenere man mano che l'hardware diventa più economico e più efficiente in termini di costi; potrebbero anche diventare meno rilevanti (o aver bisogno di essere anche più stretti) con avanzamenti in algoritmi e architetture.

Che controllare l'IA diventerà più difficile non significa che dovremmo arrenderci! Implementare il piano delineato in questo saggio ci darebbe sia tempo prezioso che controllo cruciale sul processo che ci metterebbe in una posizione molto, molto migliore per evitare il rischio esistenziale dell'IA per la nostra società, civiltà, e specie.

Nel termine ancora più lungo, ci saranno scelte da fare riguardo a cosa permettiamo. Possiamo scegliere ancora di creare qualche forma di IAG genuinamente controllabile, nel grado in cui questo si dimostri possibile. O possiamo decidere che gestire il mondo è meglio lasciarlo alle macchine, se possiamo convincerci che faranno un lavoro migliore, e ci tratteranno bene. Ma queste dovrebbero essere decisioni prese con profonda comprensione scientifica dell'IA in mano, e dopo discussione globale inclusiva significativa, non in una corsa tra magnati della tecnologia con la maggior parte dell'umanità completamente non coinvolta e inconsapevole.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) Riassunto della governance di I-A-G e superintelligenza tramite responsabilità e regolamentazione. La responsabilità è più alta, e la regolamentazione più forte, alla tripla-intersezione di Autonomia, Generalità, e Intelligenza. Porti sicuri dalla responsabilità stretta e regolamentazione forte possono essere ottenuti tramite casi di sicurezza affermativi che dimostrano che un sistema è debole e/o ristretto e/o passivo. Limiti sulla Capacità Computazionale Totale di Addestramento e sulla velocità di Capacità Computazionale di Inferenza, verificati e applicati legalmente e usando misure di sicurezza hardware e crittografiche, sostengono la sicurezza evitando l'IAG completa e proibendo efficacemente la superintelligenza.

[^1]: Molto probabilmente, la diffusione di questa realizzazione prenderà o sforzo intenso da parte di gruppi di educazione e advocacy che fanno questo caso, o un disastro causato dall'IA abbastanza significativo. Possiamo sperare che sia il primo.

[^2]: Paradossalmente, siamo abituati che la Natura limiti la nostra tecnologia rendendola molto difficile da sviluppare, specialmente scientificamente. Ma questo non è più il caso per l'IA: i problemi scientifici chiave stanno risultando più facili del previsto. Non possiamo contare sulla Natura che ci salvi da noi stessi qui – dovremo farlo noi.

[^3]: Dove, esattamente, ci fermiamo nello sviluppare nuovi sistemi? Qui, dovremmo adottare un principio precauzionale. Una volta che un sistema è deployato, e specialmente una volta che quel livello di capacità del sistema prolifera, è estremamente difficile fare marcia indietro. E se un sistema è *sviluppato* (specialmente a grande costo e sforzo), ci sarà enorme pressione per usarlo o deployarlo, e tentazione per esso di essere fatto trapelare o rubato. Sviluppare sistemi e *poi* decidere se sono profondamente non sicuri è una strada pericolosa.

[^4]: Sarebbe anche saggio proibire sviluppo di IA che è intrinsecamente pericoloso, come sistemi auto-replicanti ed evolventi, quelli progettati per sfuggire alla reclusione, quelli che possono auto-migliorarsi autonomamente, IA deliberatamente ingannevole e maliziosa, ecc.

[^5]: Notare questo non significa necessariamente *applicato* a livello internazionale da qualche tipo di corpo globale: invece nazioni sovrane potrebbero applicare regole concordate, come in molti trattati.

[^6]: Come vedremo sotto, la natura del calcolo dell'IA permetterebbe qualcosa di un ibrido; ma la cooperazione internazionale sarà comunque necessaria.

[^7]: Per esempio, le macchine richieste per incidere chip rilevanti per l'IA sono fatte solo da una ditta, ASML (nonostante molti altri tentativi di farlo), la vasta maggioranza dei chip rilevanti sono prodotti da una ditta, TSMC (nonostante altri tentino di competere), e il design e costruzione di hardware da quei chip fatto solo da pochi inclusi NVIDIA, AMD, e Google.

[^8]: Più importante, ogni chip tiene una chiave privata crittografica unica e inaccessibile che può usare per "firmare" cose.

[^9]: Per impostazione predefinita questo sarebbe l'azienda che vende i chip, ma altri modelli sono possibili e potenzialmente utili.

[^10]: Un governatore può accertare l'ubicazione di un chip cronometrando lo scambio di messaggi firmati con esso: la velocità finita della luce richiede che il chip sia entro un dato raggio *r* di una "stazione" se può restituire un messaggio firmato in un tempo meno di *r* / *c*, dove *c* è la velocità della luce. Usando stazioni multiple, e qualche comprensione delle caratteristiche di rete, l'ubicazione del chip può essere determinata. La bellezza di questo metodo è che la maggior parte della sua sicurezza è fornita dalle leggi della fisica. Altri metodi potrebbero usare GPS, tracciamento inerziale, e tecnologie simili.

[^11]: Alternativamente, coppie di chip potrebbero essere permesse di comunicare tra loro solo via permesso esplicito di un governatore.

[^12]: Questo è cruciale perché almeno attualmente, connessione ad alta larghezza di banda tra chip è necessaria per addestrare grandi modelli di IA su di essi.

[^13]: Questo potrebbe anche essere configurato per richiedere messaggi firmati da *N* di *M* governatori diversi, permettendo a parti multiple di condividere la governance.

[^14]: Questo è tutt'altro che senza precedenti – per esempio i militari non hanno sviluppato eserciti di supersoldati clonati o geneticamente modificati, sebbene questo sia probabilmente tecnologicamente possibile. Ma hanno *scelto* di non farlo, piuttosto che essere prevenuti da altri. Il record non è eccellente per potenze mondiali principali che vengono prevenute dallo sviluppare una tecnologia che desiderano fortemente sviluppare.

[^15]: Con un paio di eccezioni notevoli (in particolare NVIDIA) l'hardware specializzato per l'IA è una parte relativamente piccola del modello di business e ricavi complessivi di queste aziende. Inoltre, il divario tra hardware usato nell'IA avanzata e hardware "grado consumatore" è significativo, quindi la maggior parte dei consumatori di hardware per computer sarebbero largamente non influenzati.

[^16]: Per analisi più dettagliata, vedere i rapporti recenti da [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) e [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). Questi si concentrano sulla fattibilità tecnica, specialmente nel contesto dei controlli di esportazione USA che cercano di limitare la capacità di altri paesi nel calcolo di alta gamma; ma questo ha sovrapposizione ovvia con il vincolo globale immaginato qui.

[^17]: I dispositivi Apple, per esempio, sono bloccati remotamente e sicuramente quando riportati persi o rubati, e possono essere ri-attivati remotamente. Questo si basa sulle stesse caratteristiche di sicurezza hardware discusse qui.

[^18]: Vedere ad esempio l'offerta [capacity on demand](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) di IBM, [Intel on demand](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html) di Intel, e [private cloud compute](https://security.apple.com/blog/private-cloud-compute/) di Apple.

[^19]: [Questo studio](https://epochai.org/trends#hardware-trends-section) mostra che storicamente la stessa performance è stata ottenuta usando circa 30% meno dollari per anno. Se questa tendenza continua, ci potrebbe essere sovrapposizione significativa tra uso di chip IA e "consumatore", e in generale la quantità di hardware necessario per sistemi di IA ad alta potenza potrebbe diventare scomodamente piccola.

[^20]: Per lo [stesso studio](https://epochai.org/trends#hardware-trends-section), data performance sul riconoscimento di immagini ha richiesto 2.5x meno calcolo ogni anno. Se questo dovesse anche valere per i sistemi di IA più capaci, un limite di calcolo non sarebbe utile per molto.

[^21]: In particolare, a livello di paese questo assomiglia molto a una nazionalizzazione del calcolo, in quanto il governo avrebbe molto controllo su come viene usata la potenza computazionale. Tuttavia, per quelli preoccupati del coinvolgimento del governo, questo sembra molto più sicuro e preferibile al software di IA più potente *stesso* che viene nazionalizzato tramite qualche fusione tra aziende di IA principali e governi nazionali, come alcuni stanno iniziando ad advocare.

[^22]: Un passo regolamentario principale in Europa è stato preso con il passaggio nel 2024 dell'[EU AI Act](https://artificialintelligenceact.eu/). Classifica l'IA per rischio: proibendo sistemi inaccettabili, regolamentando quelli ad alto rischio, e imponendo regole di trasparenza, o nessuna misura affatto, su sistemi a basso rischio. Ridurrà significativamente alcuni rischi dell'IA, e aumenterà la trasparenza dell'IA anche per ditte USA, ma ha due difetti chiave. Primo, portata limitata: mentre si applica a qualsiasi azienda che fornisce IA nell'UE, l'applicazione su ditte con base USA è debole, e l'IA militare è esente. Secondo, mentre copre GPAI, fallisce nel riconoscere IAG o superintelligenza come rischi inaccettabili o prevenire il loro sviluppo—solo il loro deployment nell'UE. Di conseguenza, fa poco per frenare i rischi di IAG o superintelligenza.

[^23]: Le aziende spesso rappresentano che sono a favore di regolamentazione ragionevole. Ma in qualche modo sembrano quasi sempre opporsi a qualsiasi regolamentazione *particolare*; testimone la lotta sulla SB1047 abbastanza leggera, che [la maggior parte delle aziende di IA si sono opposte pubblicamente o privatamente](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/).

[^24]: Sono stati circa 3 anni e mezzo dal momento in cui l'EU AI act è stato proposto fino a quando è entrato in vigore.

[^25]: È a volte espresso che è "troppo presto" per iniziare a regolamentare l'IA. Data la nota precedente, questo sembra difficilmente probabile. Un'altra preoccupazione espressa è che la regolamentazione "danneggerebbe l'innovazione". Ma la buona regolamentazione cambia solo la direzione, non la quantità, di innovazione.

[^26]: Un precedente interessante è nel trasporto di materiali pericolosi, che potrebbero sfuggire e causare danno. Qui, [regolamentazione](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) e [giurisprudenza](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) hanno stabilito responsabilità stretta per materiali molto pericolosi come esplosivi, benzina, veleni, agenti infettivi, e rifiuti radioattivi. Altri esempi includono [avvertenze sui farmaci](https://www.medicalnewstoday.com/articles/boxed-warnings), [classi di dispositivi medici](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification), ecc.

[^27]: Un'altra proposta comprensiva con obiettivi simili messa avanti in ["A Narrow Path"](https://www.narrowpath.co/) advocata per un approccio più centralizzato, basato su proibizione che incanala tutto lo sviluppo di IA di frontiera attraverso una singola entità internazionale, supervisionata da forti istituzioni internazionali, con proibizioni categoriche chiare piuttosto che restrizioni graduate. Approverei anche quel piano; tuttavia prenderà ancora più volontà politica e coordinamento di quello proposto qui.

[^28]: Alcune linee guida per tale standard sono state [pubblicate](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/) dal Frontier Model Forum. Relative alla proposta qui, quelle errano sul lato di meno precisione e meno calcolo incluso nel conteggio.

[^29]: L'ordine esecutivo AI USA del 2023 (ora rescisso) richiedeva rapporto simile ma meno dettagliato. Questo dovrebbe essere rafforzato da un ordine sostitutivo.

[^30]: Molto approssimativamente, per i chip H100 ora comuni questo corrisponde a cluster di circa 1000 che fanno inferenza; sono circa 100 (circa 5 milioni di USD di valore) dei chip NVIDIA B200 nuovissimi top di gamma che fanno inferenza. In entrambi i casi il numero di addestramento corrisponde a quel cluster che calcola per diversi mesi.

[^31]: Questa quantità è più grande di qualsiasi sistema di IA attualmente addestrato; un numero più grande o più piccolo potrebbe essere giustificato man mano che capiamo meglio come la capacità dell'IA scala con il calcolo.

[^32]: Questo si applica a quelli che creano e forniscono/ospitano i modelli, non agli utenti finali.

[^33]: Approssimativamente, responsabilità "stretta" significa che gli sviluppatori sono ritenuti responsabili per danni fatti da un prodotto *per impostazione predefinita* ed è uno standard usato per prodotti "anormalmente pericolosi", e (alquanto divertente ma appropriato) animali selvatici. Responsabilità "solidale" significa che la responsabilità è assegnata a tutte le parti responsabili per un prodotto, e quelle parti devono sistemare tra loro chi porta quale responsabilità. Questo è importante per sistemi come l'IA con una catena del valore lunga e complessa.

[^34]: La responsabilità standard basata su colpa di singola parte non è abbastanza: la colpa sarà sia difficile da tracciare e assegnare perché i sistemi di IA sono complessi, la loro operazione non è compresa, e molte parti potrebbero essere coinvolte nella creazione di un sistema o output pericoloso. In aggiunta, le cause legali prenderanno anni per essere giudicate e probabilmente risulteranno meramente in multe che sono conseguenti per queste aziende, quindi la responsabilità personale per i dirigenti è importante anche.

[^35]: Non dovrebbe esserci esenzione dai criteri di sicurezza per modelli open-weight. Inoltre, nel valutare il rischio si dovrebbe assumere che protezioni che possono essere rimosse saranno rimosse da modelli ampiamente disponibili, e che anche modelli chiusi prolifereranno a meno che non ci sia un'assicurazione molto alta che rimarranno sicuri.

[^36]: Lo schema proposto qui ha scrutinio regolamentario innescato sulla capacità generale; tuttavia ha senso per alcuni casi d'uso specialmente rischiosi innescare più scrutinio – per esempio un sistema di IA esperto in virologia, anche se ristretto e passivo, dovrebbe probabilmente andare in un livello più alto. Il precedente ordine esecutivo USA aveva qualche di questa struttura per capacità biologiche.

[^37]: Due esempi chiari sono aviazione e medicine, regolamentate dalla FAA e FDA, e agenzie simili in altri paesi. Queste agenzie sono imperfette, ma sono state assolutamente vitali per il funzionamento e successo di quelle industrie.