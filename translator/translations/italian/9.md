# Capitolo 9 - Ingegnerizzare il futuro — cosa dovremmo fare invece

L'IA può fare un bene incredibile nel mondo. Per ottenere tutti i benefici senza i rischi, dobbiamo assicurarci che l'IA rimanga uno strumento umano.

Se riusciamo a scegliere di non sostituire l'umanità con le macchine – almeno per un po'! – cosa possiamo fare invece? Rinunciamo all'enorme promessa dell'IA come tecnologia? A un certo livello la risposta è un semplice *no:* chiudiamo le Porte all'IAG incontrollabile e alla superintelligenza, ma *costruiamo* molte altre forme di IA, insieme alle strutture di governance e alle istituzioni di cui avremo bisogno per gestirle.

Ma c'è ancora molto da dire; realizzare tutto questo sarebbe un'occupazione centrale dell'umanità. Questa sezione esplora diversi temi chiave:

- Come possiamo caratterizzare l'IA "Strumentale" e le forme che può assumere.
- Che possiamo ottenere (quasi) tutto quello che l'umanità desidera senza IAG, con l'IA Strumentale.
- Che i sistemi di IA Strumentale sono (probabilmente, in linea di principio) gestibili.
- Che allontanarsi dall'IAG non significa compromettere la sicurezza nazionale – anzi, tutto il contrario.
- Che la concentrazione del potere è una preoccupazione reale. Possiamo mitigarla senza compromettere sicurezza e protezione?
- Che vorremo – e avremo bisogno di – nuove strutture di governance e sociali, e l'IA può effettivamente aiutare.

## IA dentro le Porte: IA Strumentale

Il diagramma della tripla-intersezione offre un buon modo per delineare quello che possiamo chiamare "IA Strumentale": IA che è uno strumento controllabile per l'uso umano, piuttosto che un rivale o un sostituto incontrollabile. I sistemi di IA meno problematici sono quelli che sono autonomi ma non generali o super capaci (come un bot per aste automatiche), o generali ma non autonomi o capaci (come un modello linguistico piccolo), o capaci ma specifici e molto controllabili (come AlphaGo).[^1] Quelli con due caratteristiche che si intersecano hanno un'applicazione più ampia ma un rischio più alto e richiederanno sforzi considerevoli per essere gestiti. (Il fatto che un sistema di IA sia più strumentale non significa che sia intrinsecamente sicuro, semplicemente che non è intrinsecamente *non sicuro* – si pensi a una motosega, contro una tigre domestica.) La Porta deve rimanere chiusa alla IAG (completa) e alla superintelligenza alla tripla intersezione, e bisogna prestare enorme attenzione ai sistemi di IA che si avvicinano a quella soglia.

Ma questo lascia spazio a molta IA potente! Possiamo ottenere un'enorme utilità da "oracoli" passivi intelligenti e generali e da sistemi specifici, sistemi generali a livello umano ma non sovrumano, e così via. Molte aziende tecnologiche e sviluppatori stanno attivamente costruendo questi tipi di strumenti e dovrebbero continuare; come la maggior parte delle persone, stanno implicitamente *assumendo* che le Porte alla IAG e alla superintelligenza rimarranno chiuse.[^2]

Inoltre, i sistemi di IA possono essere efficacemente combinati in sistemi compositi che mantengono la supervisione umana mentre potenziano le capacità. Piuttosto che affidarsi a scatole nere imperscrutabili, possiamo costruire sistemi dove più componenti – inclusi sia IA che software tradizionali – lavorano insieme in modi che gli umani possono monitorare e comprendere.[^3] Mentre alcuni componenti potrebbero essere scatole nere, nessuno sarebbe vicino all'IAG – solo il sistema composito nel suo insieme sarebbe sia altamente generale che altamente capace, e in modo rigorosamente controllabile.[^4]

### Controllo umano significativo e garantito

Cosa significa "rigorosamente controllabile"? Un'idea chiave del framework "Strumentale" è permettere sistemi – anche se abbastanza generali e potenti – che sono garantiti essere sotto controllo umano significativo. Cosa significa questo? Comporta due aspetti. Il primo è una considerazione progettuale: gli umani dovrebbero essere profondamente e centralmente coinvolti in quello che il sistema sta facendo, *senza* delegare decisioni chiave importanti all'IA. Questo è il carattere della maggior parte dei sistemi di IA attuali. Secondo, nella misura in cui i sistemi di IA sono autonomi, devono avere garanzie che limitano la loro portata d'azione. Una garanzia dovrebbe essere un *numero* che caratterizza la probabilità che qualcosa accada, e una ragione per credere a quel numero. Questo è quello che richiediamo in altri campi critici per la sicurezza, dove numeri come "tempo medio tra i guasti" e numeri attesi di incidenti vengono calcolati, supportati e pubblicati nelle analisi di sicurezza.[^5] Il numero ideale per i guasti è zero, ovviamente. E la buona notizia è che potremmo avvicinarci parecchio, anche se usando architetture di IA abbastanza diverse, usando idee di proprietà *formalmente verificate* dei programmi (inclusa l'IA). L'idea, esplorata a lungo da Omohundro, Tegmark, Bengio, Dalrymple e altri (vedi [qui](https://arxiv.org/abs/2309.01933) e [qui](https://arxiv.org/abs/2405.06624)) è costruire un programma con certe proprietà (per esempio: che un umano può spegnerlo) e *dimostrare* formalmente che quelle proprietà valgono. Questo può essere fatto ora per programmi abbastanza brevi e proprietà semplici, ma il (futuro) potere del software di dimostrazione potenziato dall'IA potrebbe permetterlo per programmi molto più complessi (ad esempio wrapper) e persino l'IA stessa. Questo è un programma molto ambizioso, ma mentre cresce la pressione sulle Porte, avremo bisogno di alcuni materiali potenti per rinforzarle. La dimostrazione matematica potrebbe essere uno dei pochi abbastanza forte.

### Il destino dell'industria dell'IA

Con il progresso dell'IA reindirizzato, l'IA Strumentale sarebbe ancora un'industria enorme. In termini di hardware, anche con limiti computazionali per prevenire la superintelligenza, l'addestramento e l'inferenza in modelli più piccoli richiederanno ancora enormi quantità di componenti specializzati. Sul lato software, disinnescare l'esplosione nelle dimensioni dei modelli di IA e della computazione dovrebbe semplicemente portare le aziende a reindirizzare le risorse verso il miglioramento dei sistemi più piccoli, rendendoli migliori, più diversi e più specializzati, piuttosto che semplicemente renderli più grandi.[^6] Ci sarebbe molto spazio – probabilmente di più – per tutte quelle startup della Silicon Valley che fanno soldi.[^7]

## L'IA Strumentale può produrre (quasi) tutto quello che l'umanità desidera, senza IAG

L'intelligenza, sia biologica che artificiale, può essere considerata in senso lato come la capacità di pianificare ed eseguire attività che portano a futuri più in linea con una serie di obiettivi. Come tale, l'intelligenza è di enorme beneficio quando usata per perseguire obiettivi saggiamente scelti. L'intelligenza artificiale sta attraendo enormi investimenti di tempo e sforzo principalmente a causa dei suoi benefici promessi. Quindi dovremmo chiederci: in che misura otterremmo ancora i benefici dell'IA se conteniamo la sua escalation incontrollabile verso la superintelligenza? La risposta: potremmo perdere sorprendentemente poco.

Consideriamo innanzitutto che i sistemi di IA attuali sono già molto potenti, e abbiamo davvero solo scalfito la superficie di quello che si può fare con essi.[^8] Sono ragionevolmente capaci di "gestire la situazione" in termini di "comprensione" di una domanda o compito presentato loro, e quello che servirebbe per rispondere a questa domanda o fare quel compito.

Inoltre, gran parte dell'entusiasmo sui sistemi di IA moderni è dovuto alla loro generalità; ma alcuni dei sistemi di IA più capaci – come quelli che generano o riconoscono voce o immagini, fanno previsioni e modellazione scientifica, giocano, ecc. – sono molto più specifici e ben "dentro le Porte" in termini di computazione.[^9] Questi sistemi sono sovrumani nei compiti particolari che fanno. Potrebbero avere debolezze nei casi limite[^10] (o [sfruttabili](https://arxiv.org/abs/2211.00241)) dovute alla loro specificità; tuttavia *totalmente* specifici o *completamente* generali non sono le uniche opzioni disponibili: ci sono molte architetture intermedie.[^11]

Questi strumenti di IA possono accelerare notevolmente l'avanzamento in altre tecnologie positive, senza IAG. Per fare meglio la fisica nucleare, non abbiamo bisogno che l'IA sia un fisico nucleare – ne abbiamo! Se vogliamo accelerare la medicina, diamo ai biologi, ai ricercatori medici e ai chimici strumenti potenti. Li vogliono e li useranno con enorme vantaggio. Non abbiamo bisogno di una server farm piena di un milione di geni digitali; abbiamo milioni di umani il cui genio l'IA può aiutare a far emergere. Sì, ci vorrà più tempo per ottenere l'immortalità e la cura per tutte le malattie. Questo è un costo reale. Ma anche le innovazioni sanitarie più promettenti sarebbero di poca utilità se l'instabilità guidata dall'IA portasse a conflitti globali o al collasso sociale. Dobbiamo dare agli umani potenziati dall'IA una possibilità di affrontare prima il problema.

E supponiamo che ci sia, di fatto, qualche enorme vantaggio dell'IAG che non può essere ottenuto dall'umanità usando strumenti dentro-Porta. Li perdiamo *non* costruendo mai IAG e superintelligenza? Nel valutare rischi e benefici qui, c'è un enorme beneficio asimmetrico nell'aspettare piuttosto che affrettarsi: possiamo aspettare finché non può essere fatto in modo garantito sicuro e benefico, e quasi tutti potranno ancora raccogliere i frutti; se ci affrettiamo, potrebbe essere – nelle parole del CEO di OpenAI Sam Altman – [luci spente per *tutti* noi.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

Ma se gli strumenti non-IAG sono potenzialmente così potenti, possiamo gestirli? La risposta è un chiaro...forse.

## I sistemi di IA Strumentale sono (probabilmente, in linea di principio) gestibili

Ma non sarà facile. I sistemi di IA all'avanguardia attuali possono potenziare notevolmente persone e istituzioni nel raggiungere i loro obiettivi. Questo è, in generale, una cosa buona! Tuttavia, ci sono dinamiche naturali nell'avere tali sistemi a nostra disposizione – improvvisamente e senza molto tempo per la società di adattarsi – che offrono seri rischi che devono essere gestiti. Vale la pena discutere alcune classi principali di tali rischi, e come possano essere diminuiti, assumendo una chiusura delle Porte.

Una classe di rischi è dell'IA Strumentale ad alta potenza che permette l'accesso a conoscenza o capacità che erano precedentemente legate a una persona o organizzazione, rendendo disponibile una combinazione di alta capacità più alta lealtà a una gamma molto ampia di attori. Oggi, con abbastanza soldi una persona con cattive intenzioni potrebbe assumere un team di chimici per progettare e produrre nuove armi chimiche – ma non è così facile avere quei soldi o trovare/assemblare il team e convincerli a fare qualcosa chiaramente illegale, non etico e pericoloso. Per prevenire che i sistemi di IA giochino un tale ruolo, miglioramenti sui metodi attuali potrebbero bastare,[^12] purché tutti quei sistemi e l'accesso ad essi siano gestiti responsabilmente. D'altra parte, se sistemi potenti vengono rilasciati per uso generale e modifiche, qualsiasi misura di sicurezza incorporata è probabilmente rimovibile. Quindi per evitare rischi in questa classe, saranno richieste forti restrizioni su quello che può essere rilasciato pubblicamente – analoghe alle restrizioni sui dettagli delle tecnologie nucleari, esplosive e altre pericolose.[^13]

Una seconda classe di rischi deriva dal potenziamento di macchine che agiscono come o impersonano persone. A livello di danno alle persone individuali, questi rischi includono truffe, spam e phishing molto più efficaci, e la proliferazione di deepfake non consensuali.[^14] A livello collettivo, includono la disruzione di processi sociali fondamentali come la discussione e il dibattito pubblico, i nostri sistemi sociali di raccolta, elaborazione e diffusione di informazioni e conoscenza, e i nostri sistemi di scelta politica. Mitigare questo rischio probabilmente comporterà (a) leggi che limitano l'impersonificazione di persone da parte di sistemi di IA, e che ritengono responsabili gli sviluppatori di IA che creano sistemi che generano tali impersonificazioni, (b) sistemi di watermarking e provenienza che identificano e classificano (responsabilmente) il contenuto generato dall'IA, e (c) nuovi sistemi epistemici socio-tecnici che possono creare una catena fidata dai dati (ad esempio telecamere e registrazioni) attraverso fatti, comprensione e buoni modelli del mondo.[^15] Tutto questo è possibile, e l'IA può aiutare con alcune parti di esso.

Un terzo rischio generale è che nella misura in cui alcuni compiti sono automatizzati, gli umani che attualmente fanno quei compiti possono avere meno valore finanziario come forza lavoro. Storicamente, automatizzare i compiti ha reso le cose abilitate da quei compiti più economiche e abbondanti, mentre ha diviso le persone che precedentemente facevano quei compiti in quelli ancora coinvolti nella versione automatizzata (generalmente a skill/paga più alta), e quelli il cui lavoro vale meno o poco. Complessivamente è difficile predire in quali settori sarà richiesto più versus meno lavoro umano nel settore risultante più grande ma più efficiente. In parallelo, la dinamica dell'automazione tende ad aumentare la disuguaglianza e la produttività generale, diminuire il costo di certi beni e servizi (via aumenti di efficienza), e aumentare il costo di altri (via [malattia dei costi](https://en.wikipedia.org/wiki/Baumol_effect)). Per quelli sul lato sfavorito dell'aumento della disuguaglianza, è profondamente poco chiaro se la diminuzione di costo in certi beni e servizi superi l'aumento negli altri, e porti a maggiore benessere complessivo. Quindi come andrà per l'IA? A causa della relativa facilità con cui il lavoro intellettuale umano può essere sostituito dall'IA generale, possiamo aspettarci una versione rapida di questo con IA generale competitiva con l'umano.[^16] Se chiudiamo la Porta all'IAG, molti meno lavori saranno sostituiti all'ingrosso da agenti di IA; ma un enorme spostamento lavorativo è ancora probabile in un periodo di anni.[^17] Per evitare sofferenza economica diffusa, sarà probabilmente necessario implementare sia qualche forma di beni di base universali o reddito, sia anche ingegnerizzare uno spostamento culturale verso il valorizzare e ricompensare il lavoro umano-centrico che è più difficile da automatizzare (piuttosto che vedere i prezzi del lavoro scendere a causa dell'aumento del lavoro disponibile spinto fuori da altre parti dell'economia.) Altri costrutti, come quello della ["dignità dei dati"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society) (in cui i produttori umani di dati di addestramento ricevono automaticamente royalty per il valore creato da quei dati nell'IA) possono aiutare. L'automazione tramite IA ha anche un secondo potenziale effetto avverso, che è dell'automazione *inappropriata*. Insieme alle applicazioni dove l'IA semplicemente fa un lavoro peggiore, questo includerebbe quelle dove i sistemi di IA probabilmente violerebbero precetti morali, etici o legali – per esempio nelle decisioni di vita e morte, e in questioni giudiziarie. Questi devono essere trattati applicando ed estendendo i nostri framework legali attuali.

Infine, una minaccia significativa dell'IA dentro-porta è il suo uso nella persuasione personalizzata, cattura dell'attenzione e manipolazione. Abbiamo visto nei social media e altre piattaforme online la crescita di un'economia dell'attenzione profondamente radicata (dove i servizi online combattono ferocemente per l'attenzione degli utenti) e sistemi di ["capitalismo di sorveglianza"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism) (in cui informazioni e profilazione degli utenti si aggiungono alla mercificazione dell'attenzione.) È quasi certo che più IA sarà messa al servizio di entrambi. L'IA è già pesantemente usata negli algoritmi di feed che creano dipendenza, ma questo evolverà in contenuto generato dall'IA che crea dipendenza, personalizzato per essere consumato compulsivamente da una singola persona. E l'input, le risposte e i dati di quella persona saranno alimentati nella macchina dell'attenzione/pubblicità per continuare il circolo vizioso. Inoltre, quando gli assistenti IA forniti dalle aziende tecnologiche diventano l'interfaccia per più vita online, probabilmente sostituiranno i motori di ricerca e i feed come meccanismo attraverso cui avviene la persuasione e la monetizzazione dei clienti. Il fallimento della nostra società nel controllare queste dinamiche finora non è di buon auspicio. Parte di questa dinamica potrebbe essere diminuita via regolamenti riguardo privacy, diritti sui dati e manipolazione. Arrivare più alla radice del problema potrebbe richiedere prospettive diverse, come quella degli assistenti IA leali (discussa sotto.)

Il risultato di questa discussione è di speranza: i sistemi basati su strumenti dentro-Porta – almeno finché restano comparabili in potenza e capacità ai sistemi all'avanguardia di oggi – sono probabilmente gestibili se c'è volontà e coordinamento per farlo. Istituzioni umane decenti, potenziate da strumenti di IA,[^18] possono farlo. Potremmo anche fallire nel farlo. Ma è difficile vedere come permettere sistemi più potenti aiuterebbe – altro che metterli al comando e sperare per il meglio.

## Sicurezza nazionale

Le corse per la supremazia dell'IA – guidate dalla sicurezza nazionale o altre motivazioni – ci spingono verso sistemi di IA potenti e incontrollati che tenderebbero ad assorbire, piuttosto che conferire, potere. Una corsa all'IAG tra USA e Cina è una corsa per determinare quale nazione ottiene prima la superintelligenza.

Quindi cosa dovrebbero fare invece quelli incaricati della sicurezza nazionale? I governi hanno forte esperienza nel costruire sistemi controllabili e sicuri, e dovrebbero raddoppiare nel farlo nell'IA, supportando il tipo di progetti infrastrutturali che riescono meglio quando fatti su scala e con imprimatur governativo.

Invece di un "progetto Manhattan" sconsiderato verso l'IAG,[^19] il governo USA potrebbe lanciare un progetto Apollo per sistemi controllabili, sicuri, affidabili. Questo potrebbe includere per esempio:

- Un programma importante per (a) sviluppare i meccanismi di sicurezza hardware on-chip e (b) l'infrastruttura, per gestire il lato computazionale dell'IA potente. Questi potrebbero costruire sull'[atto CHIPS](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) USA e il [regime di controlli all'esportazione](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion).
- Un'iniziativa su larga scala per sviluppare tecniche di verifica formale in modo che caratteristiche particolari dei sistemi di IA (come un interruttore di spegnimento) possano essere *dimostrate* essere presenti o assenti. Questo può sfruttare l'IA stessa per sviluppare dimostrazioni di proprietà.
- Uno sforzo su scala nazionale per creare software che sia verificabilmente sicuro, alimentato da strumenti di IA che possono ricodificare il software esistente in framework verificabilmente sicuri.
- Un progetto di investimento nazionale nell'avanzamento scientifico usando l'IA,[^20] gestito come partnership tra DOE, NSF e NIH.

In generale, c'è un'enorme superficie di attacco sulla nostra società che ci rende vulnerabili ai rischi dall'IA e dal suo uso improprio. Proteggere da alcuni di questi rischi richiederà investimento e standardizzazione a dimensione governativa. Questi fornirebbero vastamente più sicurezza che versare benzina sul fuoco delle corse verso l'IAG. E se l'IA deve essere costruita nelle armi e nei sistemi di comando e controllo, è cruciale che l'IA sia affidabile e sicura, cosa che l'IA attuale semplicemente non è.

## Concentrazione del potere e le sue mitigazioni

Questo saggio si è concentrato sull'idea del controllo umano dell'IA e il suo potenziale fallimento. Ma un'altra lente valida attraverso cui vedere la situazione dell'IA è attraverso la *concentrazione del potere.* Lo sviluppo di IA molto potente minaccia di concentrare il potere o nelle pochissime e grandissime mani aziendali che l'hanno sviluppata e la controlleranno, o nei governi che usano l'IA come nuovo mezzo per mantenere il proprio potere e controllo, o nei sistemi di IA stessi. O qualche miscela empia dei suddetti. In ognuno di questi casi la maggior parte dell'umanità perde potere, controllo e agenzia. Come potremmo combattere questo?

Il primissimo e più importante passo, ovviamente, è una chiusura delle Porte all'IAG e superintelligenza più intelligenti dell'umano. Queste esplicitamente possono sostituire direttamente umani e gruppi di umani. Se sono sotto controllo aziendale o governativo concentreranno il potere in quelle aziende o governi; se sono "libere" concentreranno il potere in se stesse. Quindi assumiamo che le Porte siano chiuse. E poi?

Una soluzione proposta alla concentrazione del potere è l'IA "open-source", dove i pesi del modello sono disponibili liberamente o ampiamente. Ma come menzionato prima, una volta che un modello è aperto, la maggior parte delle misure di sicurezza o protezioni possono essere (e generalmente sono) rimosse. Quindi c'è una tensione acuta tra da una parte la decentralizzazione, e dall'altra sicurezza, protezione e controllo umano dei sistemi di IA. Ci sono anche ragioni per essere scettici che i modelli aperti combatteranno da soli significativamente la concentrazione del potere nell'IA più di quanto abbiano fatto nei sistemi operativi (ancora dominati da Microsoft, Apple e Google nonostante alternative aperte).[^21]

Eppure potrebbero esserci modi per quadrare questo cerchio – centralizzare e mitigare i rischi mentre si decentralizza capacità e ricompensa economica. Questo richiede ripensare sia come l'IA è sviluppata sia come i suoi benefici sono distribuiti.

Nuovi modelli di sviluppo e proprietà pubblica dell'IA aiuterebbero. Questo potrebbe prendere diverse forme: IA sviluppata dal governo (soggetta a supervisione democratica),[^22] organizzazioni di sviluppo IA no-profit (come Mozilla per i browser), o strutture che abilitano proprietà e governance molto diffuse. La chiave è che queste istituzioni sarebbero esplicitamente incaricate di servire l'interesse pubblico mentre operano sotto forti vincoli di sicurezza.[^23] Regimi regulativi e di standard/certificazioni ben realizzati saranno anche vitali, in modo che i prodotti di IA offerti da un mercato vibrante restino genuinamente utili piuttosto che sfruttatori verso i loro utenti.

In termini di concentrazione del potere economico, possiamo usare tracciamento della provenienza e "dignità dei dati" per assicurare che i benefici economici fluiscano più ampiamente. In particolare, la maggior parte del potere dell'IA ora (e in futuro se teniamo le Porte chiuse) deriva da dati generati dall'umano, sia dati di addestramento diretto che feedback umano. Se le aziende di IA fossero richieste di compensare i fornitori di dati equamente,[^24] questo potrebbe almeno aiutare a distribuire le ricompense economiche più ampiamente. Oltre a questo, un altro modello potrebbe essere la proprietà pubblica di frazioni significative di grandi aziende di IA. Per esempio, governi capaci di tassare le aziende di IA potrebbero investire una frazione di entrate in un fondo sovrano che detiene azioni nelle aziende, e paga dividendi alla popolazione.[^25]

Cruciale in questi meccanismi è usare il potere dell'IA stessa per aiutare a distribuire meglio il potere, piuttosto che semplicemente combattere la concentrazione del potere guidata dall'IA usando mezzi non-IA. Un approccio potente sarebbe attraverso assistenti IA ben progettati che operano con genuino dovere fiduciario verso i loro utenti – mettendo gli interessi degli utenti al primo posto, specialmente sopra quelli dei fornitori aziendali.[^26] Questi assistenti devono essere veramente affidabili, tecnicamente competenti eppure appropriatamente limitati basandosi sul caso d'uso e livello di rischio, e ampiamente disponibili a tutti attraverso canali pubblici, no-profit, o certificati for-profit. Proprio come non accetteremmo mai un assistente umano che segretamente lavora contro i nostri interessi per un'altra parte, non dovremmo accettare assistenti IA che sorvegliano, manipolano, o estraggono valore dai loro utenti per beneficio aziendale.

Una tale trasformazione altererebbe fondamentalmente la dinamica attuale dove gli individui sono lasciati a negoziare da soli con vaste macchine aziendali e burocratiche (potenziate dall'IA) che prioritizzano l'estrazione di valore sopra il welfare umano. Mentre ci sono molti possibili approcci per ridistribuire più ampiamente il potere guidato dall'IA, nessuno emergerà di default: devono essere deliberatamente ingegnerizzati e governati con meccanismi come requisiti fiduciari, fornitura pubblica, e accesso a livelli basato sul rischio.

Gli approcci per mitigare la concentrazione del potere possono affrontare venti contrari significativi dai poteri in carica.[^27] Ma ci sono percorsi verso lo sviluppo dell'IA che non richiedono di scegliere tra sicurezza e potere concentrato. Costruendo le istituzioni giuste ora, potremmo assicurare che i benefici dell'IA siano ampiamente condivisi mentre i suoi rischi sono gestiti attentamente.

## Nuove strutture di governance e sociali

Le nostre strutture di governance attuali stanno lottando: sono lente a rispondere, spesso catturate da interessi speciali, e [sempre più non fidate dal pubblico.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) Eppure questa non è una ragione per abbandonarle – anzi, tutto il contrario. Alcune istituzioni potrebbero aver bisogno di essere sostituite, ma più ampiamente abbiamo bisogno di nuovi meccanismi che possano potenziare e supplementare le nostre strutture esistenti, aiutandole a funzionare meglio nel nostro mondo in rapida evoluzione.

Gran parte della nostra debolezza istituzionale deriva non dalle strutture governative formali, ma dalle istituzioni sociali degradate: i nostri sistemi per sviluppare comprensione condivisa, coordinare l'azione, e condurre discorso significativo. Finora, l'IA ha accelerato questa degradazione, inondando i nostri canali informativi con contenuto generato, indicandoci verso il contenuto più polarizzante e divisivo, e rendendo più difficile distinguere la verità dalla finzione.

Ma l'IA potrebbe effettivamente aiutare a ricostruire e rafforzare queste istituzioni sociali. Consideriamo tre aree cruciali:

Primo, l'IA potrebbe aiutare a restaurare la fiducia nei nostri sistemi epistemici – i nostri modi di sapere cosa è vero. Potremmo sviluppare sistemi potenziati dall'IA che tracciano e verificano la provenienza dell'informazione, dai dati grezzi attraverso l'analisi alle conclusioni. Questi sistemi potrebbero combinare verifica crittografica con analisi sofisticata per aiutare le persone a capire non solo se qualcosa è vero, ma come sappiamo che è vero.[^28] Assistenti IA leali potrebbero essere incaricati di seguire i dettagli per assicurare che reggano.

Secondo, l'IA potrebbe abilitare nuove forme di coordinamento su larga scala. Molti dei nostri problemi più pressanti – dal cambiamento climatico alla resistenza antibiotica – sono fondamentalmente problemi di coordinamento. Siamo [bloccati in situazioni che sono peggiori di quello che potrebbero essere per quasi tutti](https://equilibriabook.com/), perché nessun individuo o gruppo può permettersi di fare la prima mossa. I sistemi di IA potrebbero aiutare modellando strutture di incentivi complesse, identificando percorsi praticabili verso risultati migliori, e facilitando la costruzione di fiducia e meccanismi di impegno necessari per arrivarci.

Forse più intrigante, l'IA potrebbe abilitare forme completamente nuove di discorso sociale. Immaginate di poter "parlare con una città"[^29] – non solo visualizzare statistiche, ma avere un dialogo significativo con un sistema di IA che processa e sintetizza le opinioni, esperienze, bisogni e aspirazioni di milioni di residenti. O considerate come l'IA potrebbe facilitare dialogo genuino tra gruppi che attualmente si parlano senza ascoltarsi, aiutando ogni lato a capire meglio le preoccupazioni e valori effettivi dell'altro piuttosto che le loro caricature degli altri.[^30] O l'IA potrebbe offrire intermediazione esperta e credibilmente neutrale di dispute tra persone o persino grandi gruppi di persone (che potrebbero tutti interagire con essa direttamente e individualmente!) L'IA attuale è totalmente capace di fare questo lavoro, ma gli strumenti per farlo non nascono da soli, o via incentivi di mercato.

Queste possibilità potrebbero suonare utopiche, specialmente dato il ruolo attuale dell'IA nel degradare discorso e fiducia. Ma è precisamente per questo che dobbiamo sviluppare attivamente queste applicazioni positive. Chiudendo le Porte all'IAG incontrollabile e prioritizzando l'IA che potenzia l'agenzia umana, possiamo dirigere il progresso tecnologico verso un futuro dove l'IA serve come forza per empowerment, resilienza e avanzamento collettivo.


[^1]: Detto questo, stare lontano dalla tripla-intersezione è sfortunatamente non così facile come si potrebbe desiderare. Spingere molto forte la capacità in uno qualsiasi dei tre aspetti tende ad aumentarla negli altri. In particolare, potrebbe essere difficile creare un'intelligenza estremamente generale e capace che non possa essere facilmente resa autonoma. Un approccio è addestrare modelli ["miopi"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) sistemi con capacità di pianificazione limitata. Un altro sarebbe concentrarsi sull'ingegnerizzare sistemi ["oracolo"](https://arxiv.org/abs/1711.05541) puri che eviterebbero di rispondere a domande orientate all'azione.

[^2]: Molte aziende falliscono nel realizzare che anche loro sarebbero eventualmente rimpiazzate dall'IAG, anche se ci volesse più tempo – se lo facessero, potrebbero spingere su quelle Porte un po' meno!

[^3]: I sistemi di IA potrebbero comunicare in modi più efficienti ma meno intelligibili, ma mantenere la comprensione umana dovrebbe avere priorità.

[^4]: Questa idea di IA modulare, interpretabile è stata sviluppata in dettaglio da diversi ricercatori; vedi ad es. il modello ["Comprehensive AI Services"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) di Drexler, l'["Open Agency Architecture"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai) di Dalrymple e altri. Mentre tali sistemi potrebbero richiedere più sforzo ingegneristico rispetto a reti neurali monolitiche addestrate con computazione massiva, questo è precisamente dove i limiti computazionali aiutano – rendendo il percorso più sicuro, più trasparente anche quello più pratico.

[^5]: Sulle analisi di sicurezza in generale vedi [questo manuale](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). Pertinente all'IA in particolare, vedi [Wasil et al.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [Clymer et al.](https://arxiv.org/abs/2403.10462), [Buhl et al.](https://arxiv.org/abs/2410.21572), e [Balesni et al.](https://arxiv.org/abs/2411.03336)

[^6]: Stiamo di fatto già vedendo questa tendenza guidata solo dal costo alto dell'inferenza: modelli più piccoli e più specializzati "distillati" da quelli più grandi e capaci di girare su hardware meno costoso.

[^7]: Capisco perché quelli eccitati sull'ecosistema tech dell'IA possano opporsi a quella che vedono come regolamentazione onerosa sulla loro industria. Ma è francamente sconcertante per me perché, diciamo, un venture capitalist vorrebbe permettere escalation incontrollabile all'IAG e superintelligenza. Quei sistemi (e aziende, mentre rimangono sotto controllo aziendale) *mangeranno tutte le startup come spuntino*. Probabilmente anche *prima* di mangiare altre industrie. Chiunque sia investito in un ecosistema IA fiorente dovrebbe prioritizzare l'assicurare che lo sviluppo dell'IAG non porti a monopolizzazione da parte di pochi giocatori dominanti.

[^8]: Come l'economista ed ex ricercatore Deepmind Michael Webb [ha detto](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/), "Penso che se fermassimo tutto lo sviluppo di modelli linguistici più grandi oggi, quindi GPT-4 e Claude e qualunque, e fossero le ultime cose che addestriamo di quella dimensione – quindi permettiamo molte più iterazioni su cose di quella dimensione e tutti i tipi di fine-tuning, ma niente di più grande di quello, nessun avanzamento più grande – solo quello che abbiamo oggi penso sia abbastanza per alimentare 20 o 30 anni di incredibile crescita economica."

[^9]: Per esempio, il sistema alphafold di DeepMind ha usato solo 100,000esimi del numero FLOP di GPT-4.

[^10]: La difficoltà delle auto a guida autonoma è importante notare qui: mentre nominalmente un compito specifico, e raggiungibile con affidabilità equa con sistemi di IA relativamente piccoli, è necessaria conoscenza e comprensione del mondo reale estensiva per ottenere l'affidabilità al livello necessario in un compito così critico per la sicurezza.

[^11]: Per esempio, dato un budget computazionale, vedremmo probabilmente modelli GPAI pre-addestrati a (diciamo) metà di quel budget, e l'altra metà usata per addestrare capacità molto alta in una gamma più ristretta di compiti. Questo darebbe capacità specifica sovrumana supportata da intelligenza generale quasi-umana.

[^12]: La tecnica dominante di allineamento attuale è "reinforcement learning by human feedback" [(RLHF)](https://arxiv.org/abs/1706.03741) e usa feedback umano per creare un segnale di ricompensa/punizione per l'apprendimento per rinforzo del modello di IA. Questa e tecniche correlate come [constitutional AI](https://arxiv.org/abs/2212.08073) stanno funzionando sorprendentemente bene (anche se mancano di robustezza e possono essere aggirate con sforzo modesto.) Inoltre, i modelli linguistici attuali sono generalmente abbastanza competenti nel ragionamento di senso comune che non faranno errori morali sciocchi. Questo è qualcosa come un punto ottimale: abbastanza intelligenti per capire quello che le persone vogliono (nella misura in cui può essere definito), ma non abbastanza intelligenti per pianificare inganni elaborati o causare danni enormi quando sbagliano.

[^13]: Nel lungo termine, qualsiasi livello di capacità IA che viene sviluppato probabilmente prolifererà, dato che ultimamente è software, ed utile. Avremo bisogno di avere meccanismi robusti per difendere contro i rischi che tali sistemi pongono. Ma *non li abbiamo ora* quindi dobbiamo essere molto misurati in quanto modelli di IA potenti possono proliferare.

[^14]: La vasta maggioranza di questi sono deepfake pornografici non consensuali, inclusi di minori.

[^15]: Molti ingredienti per tali soluzioni esistono, nella forma di leggi "bot-o-no" (nell'atto AI dell'UE tra altri posti), [tecnologie di tracciamento provenienza dell'industria](https://c2pa.org/), [aggregatori di notizie innovativi](https://www.improvethenews.org/), [aggregatori](https://metaculus.com/) di previsioni e mercati, ecc.

[^16]: L'ondata di automazione potrebbe non seguire pattern precedenti, in quanto compiti relativamente *ad alta* competenza come scrittura di qualità, interpretazione della legge, o dare consigli medici, potrebbero essere altrettanto o persino più vulnerabili all'automazione dei compiti a competenza più bassa.

[^17]: Per modellazione attenta dell'effetto dell'IAG sui salari, vedi il report [qui](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek), e dettagli cruenti [qui](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0), da Anton Korinek e collaboratori. Trovano che mentre più pezzi di lavori sono automatizzati, produttività e salari salgono – fino a un punto. Una volta che *troppo* è automatizzato, la produttività continua ad aumentare, ma i salari crollano perché le persone sono sostituite all'ingrosso da IA efficiente. Questo è perché chiudere le Porte è così utile: otteniamo la produttività senza i salari umani svaniti.

[^18]: Ci sono molti modi in cui l'IA può essere usata come, e per aiutare a costruire, tecnologie "difensive" per rendere più robuste protezioni e gestione. Vedi [questo](https://vitalik.eth.limo/general/2025/01/05/dacc2.html) post influente che descrive questa agenda "D/acc".

[^19]: Qualcosa di ironico, un progetto Manhattan USA farebbe probabilmente poco per accelerare le tempistiche verso l'IAG – il quadrante dell'investimento umano e fiscale nel progresso IA è già puntato all'11. I risultati primari sarebbero ispirare un progetto simile in Cina (che eccelle in progetti infrastrutturali a livello nazionale), rendere molto più difficili accordi internazionali che limitano il rischio dell'IA, e allarmare altri avversari geopolitici degli USA come la Russia.

[^20]: Il programma ["National AI Research Resource"](https://nairrpilot.org/) è un buon passo attuale in questa direzione e dovrebbe essere espanso.

[^21]: Vedi [questa analisi](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807) dei vari significati e implicazioni di "aperto" nei prodotti tech e come alcuni abbiano portato a più, piuttosto che meno, radicamento del dominio.

[^22]: Piani negli USA per una [National AI Research Resource](https://nairratdoe.ornl.gov/) e il recente lancio di una [European AI Foundation](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/) sono passi interessanti in questa direzione.

[^23]: La sfida qui non è tecnica ma istituzionale – abbiamo urgentemente bisogno di esempi ed esperimenti del mondo reale su come potrebbe essere lo sviluppo IA nell'interesse pubblico.

[^24]: Questo va contro i modelli di business attuali delle grandi aziende tech e richiederebbe sia azione legale che nuove norme.

[^25]: Solo alcuni governi saranno capaci di farlo. Un'idea più radicale è [un fondo universale di questo tipo, sotto proprietà congiunta di tutti gli umani.](https://futureoflife.org/project/the-windfall-trust/)

[^26]: Per un'esposizione lunga di questo caso vedi [questo paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338) sulla lealtà dell'IA. Sfortunatamente la traiettoria di default degli assistenti IA è probabilmente una dove sono sempre più sleali.

[^27]: Qualcosa di ironico, molti poteri in carica sono anche a rischio di disempowerment supportato dall'IA; ma può essere difficile per loro percepire questo finché e a meno che il processo non arrivi abbastanza lontano.

[^28]: Alcuni sforzi interessanti in questa direzione sono rappresentati dalla [coalizione c2pa](https://c2pa.org/) sulla verifica crittografica; [Verity](https://www.improvethenews.org/) e [Ground news](https://ground.news/) su migliore epistemica delle notizie; e [Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com) e mercati di previsione su radicare il discorso in previsioni falsificabili.

[^29]: Vedi [questo](https://talktothecity.org/) progetto pilota affascinante.

[^30]: Vedi [Kialo](https://www.kialo-edu.com/), e sforzi del [Collective Intelligence Project](https://www.cip.org/) per alcuni esempi.