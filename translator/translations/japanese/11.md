# 付録

補足情報。計算資源の会計処理に関する技術的詳細、「ゲートクローズ」の実装例、厳格なAGI責任制度の詳細、AGIの安全性・セキュリティ基準の段階的アプローチを含む。

## 付録A：計算資源の会計処理に関する技術的詳細

計算資源を基盤とした有意義な制御を実現するためには、訓練と推論で使用される総計算量について「正確な値」と適切な近似値の両方を詳細に算出する手法が必要である。以下に、技術レベルで「正確な値」を集計する方法の例を示す。

**定義：**

*計算因果グラフ：* AIモデルの特定の出力Oについて、その計算結果を変更することでOを潜在的に変化させる可能性のあるデジタル計算の集合。（これは保守的に想定すべきである。すなわち、ある計算が、時間的に先行し物理的な因果効果の経路を持つ前駆要素から独立していると信じる明確な理由がなければならない。）これには、推論中にAIモデルが実行する計算に加え、入力、データ準備、モデルの訓練に関わる計算も含まれる。これらはそれ自体がAIモデルの出力である可能性があるため、人間が入力に重要な変更を加えた箇所で打ち切って、再帰的に計算される。

*訓練計算量：* ニューラルネットワークの計算因果グラフに含まれる総計算量（FLOP等の単位）（データ準備、訓練、ファインチューニング、その他の計算を含む）。

*出力計算量：* 特定のAI出力の計算因果グラフにおける総計算量。すべてのニューラルネットワーク（その訓練計算量を含む）とその出力に関わるその他の計算を含む。

*推論計算量レート：* 一連の出力において、出力間の出力計算量の変化率（FLOP/s等の単位）。すなわち、次の出力を生成するために使用された計算量を出力間の時間間隔で除した値。

**例と近似：**

- 人間が作成したデータで訓練された単一のニューラルネットワークの場合、訓練計算量は従来報告されている総訓練計算量と同じである。
- そのようなニューラルネットワークが一定の速度で推論を行う場合、推論計算量レートは推論を実行する計算クラスターの総計算速度（FLOP/s）とほぼ等しい。
- モデルのファインチューニングについては、完全なモデルの訓練計算量は、ファインチューニング前のモデルの訓練計算量に、ファインチューニング中に実行された計算量とファインチューニングで使用されたデータの準備計算量を加えたものとなる。
- 蒸留モデルの場合、完全なモデルの訓練計算量には、蒸留モデルと合成データやその他の訓練入力を提供するために使用されたより大きなモデルの両方の訓練が含まれる。
- 複数のモデルが訓練されても、多くの「試行」が人間の判断により破棄される場合、これらは保持されたモデルの訓練計算量や出力計算量にはカウントされない。

## 付録B：ゲートクローズの実装例

**実装例：** 訓練で10<sup>27</sup>FLOP、推論（AIの実行）で10<sup>20</sup>FLOP/sという制限を前提とした、ゲートクローズの実施方法の一例を示す：

**1. 一時停止：** 国家安全保障上の理由により、米国行政府は米国に拠点を置く、米国で事業を行う、または米国製チップを使用するすべての企業に対し、10<sup>27</sup>FLOP訓練計算量制限を超える可能性のある新たなAI訓練実行の中止を求める。米国はAI開発を行う他国との協議を開始し、同様の措置を講じるよう強く奨励し、他国が従わない場合は米国の一時停止が解除される可能性があることを示すべきである。

**2. 米国の監督とライセンス：** 大統領令または既存の規制機関の措置により、米国は（例えば）1年以内に以下を要求する：

- 米国で事業を行う企業による10<sup>25</sup>FLOPを超えると推定されるすべてのAI訓練実行を、米国規制機関が管理するデータベースに登録する。（注：これよりやや弱いバージョンが、すでに撤回された2023年の米国AI大統領令に含まれており、10<sup>26</sup>FLOP以上のモデルの登録を要求していた。）
- 米国で事業を行う、または米国政府と取引するすべてのAI関連ハードウェア製造業者は、その専用ハードウェアとそれを駆動するソフトウェアに関する一連の要件を遵守する。（これらの要件の多くは既存ハードウェアのソフトウェアやファームウェアの更新で実装可能だが、長期的で堅牢なソリューションには後世代のハードウェアの変更が必要である。）これには、ハードウェアが10<sup>18</sup>FLOP/sの計算を実行可能な高速相互接続クラスターの一部である場合、テレメトリーと追加計算実行要求の両方を受信する遠隔「ガバナー」による定期的な許可を含む、より高レベルの検証が必要であるという要件が含まれる。
- 保管責任者は自らのハードウェアで実行された総計算量を、米国データベースを管理する機関に報告する。
- より安全で柔軟な監督と許可を可能にするため、より強力な要件が段階的に導入される。

**3. 国際監督：**

- 米国、中国、および先進チップ製造能力を有するその他の国が国際協定を交渉する。
- この協定により、国際原子力機関に類似した新たな国際機関が創設され、AI訓練と実行の監督を担当する。
- 署名国は、国内のAIハードウェア製造業者に対し、米国で課されたものと少なくとも同等以上の強度を持つ要件の遵守を求めなければならない。
- 保管責任者は現在、自国の機関と国際機関内の新設事務所の両方にAI計算数値を報告する必要がある。
- 既存の国際協定への追加国の参加が強く奨励される：署名国による輸出規制により非署名国の高性能ハードウェアへのアクセスが制限される一方、署名国はAIシステム管理において技術支援を受けることができる。

**4. 国際検証と執行：**

- ハードウェア検証システムが更新され、計算使用量を元の保管責任者と国際機関事務所の両方に直接報告する。
- 同機関は、国際協定署名国との協議を通じて計算制限に合意し、それが署名国で法的効力を持つ。
- 並行して、国際基準が策定され、ある計算閾値を超える（ただし制限値以下の）AIの訓練と実行がこれらの基準への準拠を義務付けられる可能性がある。
- 同機関は、必要に応じてより良いアルゴリズム等を補償するため計算制限を下げることができる。または、安全で適切と判断される場合（例：証明可能な安全保証のレベルで）、計算制限を上げることができる。

## 付録C：厳格なAGI責任制度の詳細

**厳格なAGI責任制度の詳細**

- 高度に汎用的で、能力が高く、自律的な先進AIシステムの作成と運用は「異常に危険な」活動とみなされる。
- そのため、そのようなシステムの訓練と運用に対するデフォルトの責任レベルは、モデルまたはその出力・行動によって生じるあらゆる損害に対する厳格な連帯責任（または米国外での同等の責任）である。
- 重大な過失や故意の不正行為の場合、経営陣と取締役会メンバーに個人責任が課される。これには最も悪質なケースに対する刑事罰も含まれるべきである。
- 責任が通常人々や企業が負うべきデフォルト（米国では過失に基づく）責任に戻る多数のセーフハーバーがある。
	- ある計算閾値以下で訓練・運用されるモデル（上記で説明した上限の少なくとも10分の1以下となる）。
	- 「弱い」AI（おおまかに、意図されたタスクにおいて人間の専門家レベル以下）および/または
	- 「狭い」AI（設計・訓練された特定の固定され、かなり限定的な範囲のタスクと操作を持つ）および/または
	- 「受動的な」AI（わずかな修正でも、直接的な人間の関与と制御なしに行動を取る、または複雑な多段階タスクを実行する能力が非常に限定的）。
	- 安全、セキュア、制御可能であることが保証されているAI（証明可能に安全、またはリスク分析で想定される損害レベルが無視できる）。
- セーフハーバーは、AI開発者が作成し機関または機関が認定した監査人が承認した[安全ケース](https://arxiv.org/abs/2410.21572)に基づいて主張される可能性がある。計算量に基づくセーフハーバーを主張するために、開発者は総訓練計算量と最大推論率の信頼できる推定値を提供するだけでよい。
- 法律は、公衆に害をもたらすリスクが高いAIシステムの開発に対して差止救済が適切となる状況を明示的に概説する。
- 企業コンソーシアムは、NGOや政府機関と協力して、これらの用語の定義、規制当局がセーフハーバーを付与すべき方法、AI開発者が安全ケースを開発すべき方法、セーフハーバーが積極的に主張されない場合に裁判所が責任をどう解釈すべきかに関する基準と規範を策定すべきである。

## 付録D：AGIの安全性・セキュリティ基準への段階的アプローチ

**AGIの安全性・セキュリティ基準への段階的アプローチ**

| リスク段階 | トリガー | 訓練要件 | 配備要件 |
| --- | --- | --- | --- |
| RT-0 | 自律性、汎用性、知能において弱いAI | なし | なし |
| RT-1 | 自律性、汎用性、知能のうち一つにおいて強いAI | なし | リスクと用途に基づき、モデルが使用可能なあらゆる場所での国家当局による安全ケース承認の可能性 |
| RT-2 | 自律性、汎用性、知能のうち二つにおいて強いAI | 開発者を管轄する国家当局への登録 | 重大な損害リスクを認可レベル以下に制限する安全ケースに加え、モデルが使用可能なあらゆる場所での国家当局が承認した独立安全監査（ブラックボックスおよびホワイトボックス・レッドチーミングを含む） |
| RT-3 | 自律性、汎用性、知能において強いAGI | 開発者を管轄する国家当局による安全・セキュリティ計画の事前承認 | 重大な損害の限定リスクを認可レベル以下に保証する安全ケースと、サイバーセキュリティ、制御可能性、取り外し不可能なキルスイッチ、人間の価値観とのアライメント、悪意ある使用への頑健性を含む必要仕様 |
| RT-4 | 10<sup>27</sup>FLOP訓練または10<sup>20</sup>FLOP/s推論のいずれかを超えるモデル | 国際的に合意された計算資源上限の解除まで禁止 | 国際的に合意された計算資源上限の解除まで禁止 |

計算閾値と高い自律性、汎用性、知能の組み合わせに基づく段階による、リスク分類と安全・セキュリティ基準：

- *強い自律性*は、システムが重要な人間の監督や介入なしに、多段階タスクを実行する、および/または現実世界に関連する複雑な行動を取ることができる、または容易にそのようにすることができる場合に適用される。例：自動運転車とロボット；金融取引ボット。非該当例：GPT-4；画像分類器
- *強い汎用性*は、幅広い応用範囲、意図的かつ具体的に訓練されていないタスクの実行、新しいタスクを学習する重要な能力を示す。例：GPT-4；mu-zero。非該当例：AlphaFold；自動運転車；画像生成器
- *強い知能*は、モデルが最も優れた性能を発揮するタスク（汎用モデルの場合は幅広いタスクにわたって）において、人間の専門家レベルの性能と同等であることに相当する。例：AlphaFold；mu-zero；o3。非該当例：GPT-4；Siri