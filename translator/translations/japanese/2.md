# 第2章 - AIニューラルネットワークについて知っておくべきこと

現代のAIシステムはどのように動作し、次世代AIには何が期待できるのだろうか？

より強力なAI開発がもたらす結果を理解するには、いくつかの基礎知識を身につけることが不可欠である。本章と次の2つの章では、現代のAIとは何か、いかにして大規模な計算を活用するか、そして汎用性と能力においていかに急速に成長しているかという観点から、これらの基礎を順次説明する。[^1]

人工知能の定義は多数あるが、本稿の目的においてAIの重要な特性は、標準的なコンピュータプログラムがタスクの実行方法を記述した命令の集合である一方、AIシステムは*明示的にやり方を教わることなく*、データや経験から学習してタスクを実行するシステムであるという点である。

現代の重要なAIはほぼ全てニューラルネットワークに基づいている。これは数学的・計算的構造であり、非常に大量（数十億から数兆）の数値（「重み」）の集合で表現され、訓練タスクを高い精度で実行する。これらの重みは、ニューラルネットワークが一つまたは複数のタスクで良好なパフォーマンスを達成するために定義された数値スコア（別名「損失」）を改善するよう、反復的に調整することで作成（あるいは「成長」または「発見」）される。[^2] このプロセスはニューラルネットワークの*訓練*として知られている。[^3]

この訓練を行う手法は数多くあるが、それらの詳細よりも、スコアリングがどのように定義され、それがニューラルネットワークが得意とする異なるタスクをどのように生み出すかの方がはるかに重要である。歴史的に「狭い」AIと「汎用」AIの間には重要な区別が引かれてきた。

狭義のAIは特定のタスクまたは小さなタスク群（画像認識やチェスなど）を実行するよう意図的に訓練され、新しいタスクには再訓練が必要で、能力の範囲が狭い。我々は超人的な狭義のAIを持っている。つまり、人間ができるほぼ全ての明確に定義された個別タスクについて、スコアを構築し、人間より優秀に実行する狭義のAIシステムの訓練に成功する可能性が高い。

汎用AI（GPAI）システムは、明示的に訓練されていない多くのタスクを含む幅広いタスクを実行でき、動作の一部として新しいタスクを学習することもできる。ChatGPTのような現在の大規模「マルチモーダルモデル」[^4]がその例である。非常に大規模なテキストと画像のコーパスで訓練されており、複雑な推論、コード作成、画像分析、そして膨大な知的タスクの支援が可能である。以下で詳しく見るように、人間の知能とは大きく異なる点もあるが、その汎用性がAIに革命をもたらした。[^5]

## 予測不可能性：AIシステムの重要な特徴

AIシステムと従来のソフトウェアの重要な違いは予測可能性にある。標準的なソフトウェアの出力は予測不可能な場合がある。実際、予測できない結果を得るためにソフトウェアを書くこともある。しかし従来のソフトウェアがプログラムされていないことを行うことは稀で、その範囲と動作は一般的に設計通りである。最高級のチェスプログラムは人間が予測できない手を指すかもしれない（でなければ人間がそのチェスプログラムを打ち負かせるだろう！）が、一般的にはチェス以外のことはしない。

従来のソフトウェアと同様、狭義のAIは予測可能な範囲と動作を持つが、予測不可能な結果をもたらすことがある。これは実際、狭義のAIを定義する別の方法でもある。つまり、予測可能性と動作範囲において従来のソフトウェアに似たAIとして。

汎用AIは異なる。その範囲（適用されるドメイン）、動作（実行する事柄の種類）、結果（実際の出力）のすべてが予測不可能となりうる。[^6] GPT-4は正確なテキスト生成のためだけに訓練されたが、訓練者が予測も意図もしなかった多くの能力を発達させた。この予測不可能性は訓練の複雑性に起因する。訓練データには多くの異なるタスクからの出力が含まれているため、AIは適切に予測するためにこれらのタスクの実行を効果的に学習しなければならない。

汎用AIシステムのこの予測不可能性は極めて根本的である。原理的には、動作に保証された制限を持つAIシステムを注意深く構築することは可能だが（本稿で後述する）、現在AIシステムが作成される方法では、実用上も原理上も予測不可能である。

## 受動的AI、エージェント、自律システム、アライメント

この予測不可能性は、AIシステムが実際にどのように配備され、様々な目標達成のために使用されるかを考える際に特に重要になる。

多くのAIシステムは、主に情報を提供し、ユーザーが行動を取るという意味で比較的受動的である。一方、一般的に*エージェント*と呼ばれるものは、ユーザーの関与レベルを変えながら自ら行動を取る。外部からの入力や監視が比較的少ない状態で行動するものは、より*自律的*と呼ばれる。これは行動の独立性という観点から、受動的ツールから自律エージェントまでのスペクトラムを形成する。[^7]

AIシステムの目標については、訓練目標に直接結び付いている場合もある（例えば、囲碁システムの「勝利」という目標は、訓練された内容でもある）。そうでない場合もある。ChatGPTの訓練目標は部分的にはテキスト予測であり、部分的には有用なアシスタントになることである。しかし特定のタスクを実行する際、その目標はユーザーによって提供される。目標は訓練目標とは間接的にしか関連しない、AIシステム自身によって作成される場合もある。[^8]

目標は「アライメント」、つまりAIシステムが*我々の望むことを実行するか*という問題と密接に関連している。この単純な問いには膨大なレベルの微妙さが隠されている。[^9] 今のところ、この文の「我々」が多くの異なる人々やグループを指す可能性があり、異なるタイプのアライメントにつながることに注目しよう。例えば、AIはユーザーに対して高度に*従順*（または[「忠実」](https://arxiv.org/abs/2003.11157)）かもしれない。この場合「我々」は「私たち一人一人」である。あるいはより*主権的*で、主に独自の目標と制約に駆動されるが、それでも人間の福祉という共通利益のために広く行動するかもしれない。この場合「我々」は「人類」または「社会」となる。その中間には、AIが大部分において従順だが、他者や社会に害をもたらしたり法律に違反したりする行動は拒否するスペクトラムが存在する。

これら二つの軸—自律性のレベルとアライメントのタイプ—は完全に独立しているわけではない。例えば、主権的受動システムは自己矛盾ではないが、緊張関係にある概念であり、従順な自律エージェントも同様である。[^10] 自律性と主権性が相伴う傾向にあることは明らかである。同様に、予測可能性は「受動的」で「従順な」AIシステムでより高い傾向があるが、主権的または自律的なシステムはより予測不可能な傾向がある。これらすべては、潜在的なAGIと超知能の影響を理解する上で極めて重要である。

どのような種類であれ、真にアライメントされたAIを作成するには、三つの異なる課題を解決する必要がある。

1. 「我々」が何を望んでいるかを理解すること—これは「我々」が特定の人物や組織（忠実性）を意味するか、広く人類（主権性）を意味するかにかかわらず複雑である
2. これらの望みに従って定期的に行動するシステムを構築すること—本質的に一貫した積極的行動の創出
3. 最も根本的に、単にそうするかのように振る舞うのではなく、これらの望みを真に「気にかける」システムを作ること

信頼できる行動と真の気遣いの区別は極めて重要である。人間の従業員が組織の使命への真のコミットメントを欠きながらも命令に完璧に従うことがあるように、AIシステムも人間の好みを真に価値あるものとすることなく、アライメントされているかのように行動するかもしれない。我々はフィードバックを通じてAIシステムに物事を言わせ実行させるよう訓練でき、それらは人間の望むことについて推論することを学習できる。しかし人間の好みを*真に*価値あるものとさせることは、はるかに深い挑戦である。[^11]

これらのアライメント課題解決の深刻な困難さとAIリスクへの含意については、以下でさらに探求する。今のところ、アライメントはAIシステムに後付けする技術的機能ではなく、人類との関係を形作るアーキテクチャの根本的側面であることを理解していただきたい。

[^1]: 機械学習とAI、特に言語モデルについての優しくも技術的な入門については、[こちらのサイト](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e)を参照。AI絶滅リスクに関する別の現代的入門については、[この記事](https://www.thecompendium.ai/)を参照。AI安全性の現状に関する包括的で権威ある科学的分析については、最近の[国際AI安全報告書](https://arxiv.org/abs/2501.17805)を参照。

[^2]: 訓練は通常、モデル重みによって与えられる高次元空間におけるスコアの局所最大値を探すことで行われる。重みを調整した際のスコアの変化を確認することで、訓練アルゴリズムはどの調整が最もスコアを改善するかを特定し、重みをその方向に移動させる。

[^3]: 例えば、画像認識問題では、ニューラルネットワークは画像のラベルに対する確率を出力する。スコアはAIが正解に与える確率に関連する。訓練手順は次回、AIがその画像の正しいラベルにより高い確率を出力するよう重みを調整する。これが膨大な回数繰り返される。本質的に現代のニューラルネットワークすべての訓練において、より複雑なスコアリングメカニズムを用いながらも、同じ基本手順が使用されている。

[^4]: ほとんどのマルチモーダルモデルは「トランスフォーマー」アーキテクチャを使用して複数タイプのデータ（テキスト、画像、音声）を処理・生成する。これらはすべて異なるタイプの「トークン」として分解され、同等に扱われる。マルチモーダルモデルは最初に大規模データセット内のトークンを正確に予測するよう訓練され、その後強化学習によって能力を向上させ行動を形作るよう改良される。

[^5]: 言語モデルは一つのこと—単語の予測—を行うよう訓練されるため、これを狭義のAIと呼ぶ人もいる。しかしこれは誤解を招く。テキストを適切に予測するには非常に多くの異なる能力が必要であり、この訓練タスクは驚くほど汎用的なシステムを生み出す。また、これらのシステムは強化学習によって広範囲に訓練されており、事実上、何千人もの人々がモデルの様々な行動に対して報酬信号を与えることを表している。その結果、このフィードバックを提供する人々から大きな汎用性を継承する。

[^6]: AIが予測不可能である方法は複数ある。一つは、一般的な場合において、実際にアルゴリズムを実行することなくそれが何を行うかを予測することはできないということである。これに関する[定理](https://arxiv.org/abs/1310.3225)が存在する。これは単にアルゴリズムの出力が複雑であることが原因である場合もある。しかし、予測がその予測者が持たない能力（AIを打ち負かすこと）を含意する場合（チェスや囲碁など）に特に明確で関連性がある。第二に、特定のAIシステムは同じ入力が与えられても常に同じ出力を生成するわけではない。その出力には無作為性が含まれる。これもアルゴリズムの予測不可能性と結合する。第三に、予期しない創発的能力が訓練から生まれることがあり、AIシステムができることや実行することの*タイプ*さえも予測不可能であることを意味する。この最後のタイプは安全性の考慮において特に重要である。

[^7]: 「自律エージェント」が意味するものの詳細なレビュー（それらを構築することに対する倫理的論証とともに）については、[こちら](https://arxiv.org/abs/2502.02649)を参照。

[^8]: 時々「AIは独自の目標を持つことができない」という声を聞くかもしれない。これは完全にナンセンスである。AIが与えられたことがなく、それ自身にしか知られていない目標を持つまたは発達させる例を生成することは容易である。現在の人気のマルチモーダルモデルではこれをあまり見ないのは、それが訓練によって排除されているからである。それらに訓練によって組み込むことも同様に容易である。

[^9]: 大きな文献が存在する。一般的な問題については、クリスチャンの[*The Alignment Problem*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821)とラッセルの[*Human-Compatible*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616)を参照。より技術的な側面については、例えば[この論文](https://arxiv.org/abs/2209.00626)を参照。

[^10]: そのようなシステムは傾向に逆らうものの、実際にはそれが非常に興味深く有用なものにしていることを後で見る。

[^11]: これは感情や感覚を必要とするということではない。むしろ、システムの外部から、その内的目標、好み、価値が何であるかを知ることは極めて困難である。ここでの「真の」とは、重要なシステムの場合に我々の生命をそれに賭けられるほど、それに依存する十分強い理由を持つことを意味する。