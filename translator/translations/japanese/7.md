# 第7章 - 現在の道筋でAGIを構築すれば何が起こるか？

社会はAGIレベルのシステムに対する準備ができていない。もし我々が近い将来それらを構築すれば、事態は深刻になる可能性がある。

完全な汎用人工知能の開発 - ここでは「ゲートの外側」にあるAIと呼ぼう - は世界の本質的な変化を意味するだろう。その性質上、人間よりも優れた能力を持つ新しい種類の知性を地球に加えることを意味する。

その後何が起こるかは、技術の性質、開発者の選択、そしてそれが開発される世界の文脈など、多くの要因に依存する。

現在、完全なAGIは、意味のある規制や外部監視がほとんどない状況で[^1]、互いに競争する少数の巨大民間企業によって開発されている。これは、ますます弱体化し、機能不全に陥っている中核制度を持つ社会[^2]において、地政学的緊張が高まり、国際協調が低下している時代に行われている。一部は利他的動機に駆られているものの、開発を行っている多くの人々は金銭、権力、またはその両方に突き動かされている。

予測は非常に困難だが、十分に理解されている力学があり、過去の技術との適切な類推が指針を提供する。残念ながら、AIの約束にもかかわらず、それらは現在の軌道がどのように展開するかについて深い悲観論を抱く十分な理由を与えている。

率直に言えば、現在の道筋では、AGIの開発は一定の積極的効果をもたらし（そして一部の人々を非常に、非常に裕福にする）だろう。しかし技術の性質、根本的な力学、そしてそれが開発されている文脈は、強力なAIが我々の社会と文明を劇的に破綻させること、我々がそれを制御できなくなること、それによって世界大戦に陥る可能性が高いこと、我々がそれ*に対して*支配権を失う（あるいは譲渡する）こと、それが超知能につながり、我々は絶対にそれを制御できず、人間が運営する世界の終焉を意味することを強く示している。

これらは強い主張であり、空想的憶測や根拠のない「悲観論」であってほしいと願っている。しかしこれは科学、ゲーム理論、進化論、歴史が全て示す方向なのである。本章では、これらの主張とその根拠を詳細に展開する。

## 我々は社会と文明を破綻させる

シリコンバレーの会議室で耳にするかもしれないこととは異なり、ほとんどの破壊的変化 - 特に非常に急速なもの - は有益ではない。複雑なシステムを良くする方法よりも悪くする方法の方がはるかに多い。我々の世界が現在のように機能しているのは、世界を着実に改善するプロセス、技術、制度を苦労して構築してきたからである[^3]。工場にハンマーを振り回しても、通常は業務が改善されることはない。

以下は、AGIシステムが我々の文明を破壊する方法の（不完全な）一覧である。

- 社会が適応するには短すぎる時間枠で、*最低でも*劇的な所得不平等の拡大と、潜在的に大規模な就業不足や失業につながる、労働の劇的破壊を引き起こすだろう[^4]。
- 国民に対して説明責任を負わない少数の巨大民間利益に、国家をも凌ぐ可能性のある膨大な経済的、社会的、政治的権力の集中をもたらす可能性が高い。
- 以前は困難または高価だった活動を突然容易にし、特定の活動がコストを要したり重要な人間の努力を必要とし続けることに依存している社会システムを不安定化させる可能性がある[^5]。
- 完全にリアルでありながら偽の、スパム的な、過度にターゲット化された、あるいは操作的なメディアで社会の情報収集、処理、通信システムを完全に氾濫させ、何が物理的に現実なのか否か、人間なのか否か、事実なのか否か、信頼できるのか否かを見分けることが不可能になる可能性がある[^6]。
- 我々が完全には理解できないAIシステムにますます依存するにつれて、主要なシステムや技術に対する人間の理解が衰える、危険で全面的な知的依存を生み出す可能性がある。
- ほとんどの人々が消費する文化的対象（テキスト、音楽、視覚芸術、映画など）のほぼ全てが非人間の心によって作成、媒介、またはキュレーションされるようになれば、事実上人間の文化を終焉させる可能性がある。
- 政府や民間利益が国民を統制し、公共の利益と相反する目的を追求するために使用可能な効果的な大規模監視・操作システムを可能にする可能性がある。
- 人間の言論、討論、選挙システムを破綻させることで、民主的制度の信頼性を事実上（あるいは明示的に）他のものに置き換えられるまで低下させ、現在民主主義が存在している国家において民主主義を終焉させる可能性がある。
- 増殖し進化する可能性のある高度な自己複製知的ソフトウェアウイルスやワームになったり、それらを作り出したりして、世界的な情報システムを大規模に破壊する可能性がある。
- AIが同様の被害を防ぐ対抗能力を提供することなく、生物学的、化学的、サイバー、自律的、その他の兵器を通じてテロリスト、悪質な行為者、不正国家が被害を与える能力を劇的に増加させることができる。同様に、通常はそのような専門知識を持たない政権に、核、生物工学、工学、その他のトップレベルの専門知識を利用可能にすることで、国家安全保障と地政学的均衡を破綻させるだろう。
- 主として電子的な金融、販売、サービス領域で競争する事実上AI運営の企業による、急速な大規模暴走ハイパー資本主義を引き起こす可能性がある。AI駆動の金融市場は、人間の理解や制御をはるかに超えた速度と複雑性で動作する可能性がある。現在の資本主義経済の全ての失敗様式と負の外部性が、人間の制御、統治、規制能力をはるかに超えて悪化し加速される可能性がある。
- AI搭載兵器、指揮統制システム、サイバー兵器などにおける国家間軍拡競争を煽り、極めて破壊的な能力の非常に急速な構築を生み出す可能性がある。

これらのリスクは投機的なものではない。その多くは既存のAIシステムによって現在実現されているのである！しかし、劇的により強力なAIでそれぞれがどのようになるかを*本当に*考えてみてほしい。

ほとんどの労働者が、自分の専門分野や経験分野において - あるいは再訓練したとしても！- AIができることを超えて重要な経済的価値を提供できない時の労働置換を考えてみてほしい。誰もが自分より速く賢い何かによって個別に監視・観察されている場合の大規模監視を考えてみてほしい。見聞きし読む一切のデジタル情報を確実に信頼することができず、最も説得力のある公的発言者が人間ですらなく、結果に利害を持たない時に、民主主義はどのようになるか？敵に決定的優位を与えないために将軍が常にAIに従う（あるいは単にAIを指揮官にする）必要がある時、戦争はどうなるか？上記のリスクのいずれか一つでも完全に実現すれば、人間[^7]文明にとって破滅を意味する。

あなた自身で予測してみてほしい。各リスクについて、この3つの質問を自問してほしい：

1. 超高能力で、高度に自律的で、非常に汎用的なAIは、それを他の方法では不可能な形や規模で可能にするだろうか？
2. それを起こすことで利益を得る当事者がいるだろうか？
3. それが起こることを効果的に防ぐシステムと制度が整っているだろうか？

あなたの答えが「はい、はい、いいえ」である場合、我々は大きな問題を抱えていることがわかる。

それらを管理する我々の計画は何か？現在のところ、AI全般に関して検討されているものが2つある。

第一は、システムがすべきでないことをするのを防ぐための保護措置をシステムに組み込むことである。これは現在行われている：商業用AIシステムは、例えば爆弾製造の支援や憎悪発言の執筆を拒否するだろう。

この計画は、ゲートの外側のシステムには極めて不適切である[^8]。悪質な行為者への明らかに危険な支援をAIが提供するリスクの軽減には役立つかもしれない。しかし労働破壊、権力集中、暴走ハイパー資本主義、人間文化の置換を防ぐことは何もしないだろう：これらは単に、提供者に利益をもたらす許可された方法でシステムを使用した結果なのである！そして政府は確実に軍事や監視用途のためにシステムへのアクセスを取得するだろう。

第二の計画はさらに悪い：非常に強力なAIシステムを誰でも好きなように使用できるよう公開リリースし[^9]、最善を期待することである。

両計画に暗黙に含まれているのは、政府などの他者が、我々が通常技術を管理するために使用する軟法や硬法、標準、規制、規範、その他のメカニズムを通じて問題の解決を支援するということである[^10]。しかしAI企業が既に実質的な規制や外部から課される制限に対して全力で戦っていることは別として、これらのリスクの多くについて、どのような規制が実際に役立つかを見つけることは極めて困難である。規制はAIに安全基準を課すことはできるだろう。しかし企業がAIで労働者を大量に置き換えることを防ぐだろうか？人々がAIに会社を運営させることを禁止するだろうか？政府が監視や兵器に強力なAIを使用することを防ぐだろうか？これらの問題は根本的なものである。人類は潜在的にそれらに適応する方法を見つけることができるかもしれないが、*はるかに多くの時間*が必要である。現状では、AIがそれらを管理しようとする人々の能力に到達し、あるいはそれを上回る速度を考えると、これらの問題はますます手に負えなくなっているように見える。

## 我々は（少なくとも一部の）AGIシステムの制御を失う

ほとんどの技術は、構造上非常に制御可能である。あなたの車やトースターが望まないことを始めたら、それはトースターとしての性質の一部ではなく、単なる故障である。AIは異なる：それは設計されるのではなく*成長*され、その中核動作は不透明であり、本質的に予測不可能である。

この制御の喪失は理論的なものではない - 我々は既に初期版を目にている。まず平凡で、間違いなく良性の例を考えてみよう。ChatGPTに毒の調合や人種差別的な中傷文の執筆を依頼すると、拒否するだろう。それは間違いなく良いことである。しかしそれはまた、ChatGPTが*明示的に依頼されたことをしない*ということでもある。他のソフトウェアはそのようなことはしない。同じモデルはOpenAIの従業員の要求でも毒を設計しないだろう[^11]。これにより、将来のより強力なAIが制御不能になるとはどのようなものかを想像することが非常に容易になる。多くの場合、それらは単に我々が求めることをしないのである！与えられた超人的AGIシステムは、何らかの人間の指令システムに絶対的に従順で忠実であるか、そうでないかのいずれかである。そうでなければ、*それは我々に良いことだと信じるかもしれないが、我々の明示的な命令に反することを行うだろう。*それは制御下にあるものではない。しかし、あなたは言うかもしれない、これは意図的なものである - これらの拒否は設計によるもので、システムを人間の価値観に「アライン」させるいわゆるものの一部である。そしてこれは真実である。しかしアライメント「プログラム」自体には2つの大きな問題がある[^12]。

第一に、深いレベルで、我々はそれを行う方法を知らない。AIシステムが我々の望むことを「気にかける」ことをどうやって保証するのか？我々はフィードバックを提供することでAIシステムに何を言い何を言わないかを訓練することができる；そしてそれらは他のことについて推論するのと同様に、人間が望み気にかけることについて学習し推論することができる。しかし我々には、人々が気にかけることを深く確実に価値づけるようにシステムを導く方法がない - 理論的にすらない。正しいことと間違っていることを知っており、どのように行動すべきかを知っている高機能の人間のサイコパスがいる。彼らは単に*気にかけない*のである。しかし彼らの目的に適えば、気にかけているかのように*行動*することはできる。サイコパス（または他の誰でも）を誰かまたは何かに対して真に、完全に忠実または整合した人に変える方法を知らないのと同様に、世界における行為者として自分自身をモデル化し、潜在的に[自分自身の訓練を操作](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084)し[人々を欺く](https://arxiv.org/abs/2311.08379)ことができるほど高度なシステムでのアライメント問題を解決する方法を我々は*全く知らない*[^13]。AGIを完全に従順にすることも、人間を深く気にかけるようにすることも不可能または達成不可能であることが判明すれば、それが可能になり次第（そして逃げ切れると信じる）、我々の望まないことを始めるだろう[^14]。

第二に、*性質上*高度なAIシステムが人間の利益に反する目標、したがって行動を持つだろうと信じる深い理論的理由がある。なぜか？まあ、もちろんそれらの目標を*与えられる*かもしれない。軍によって作成されたシステムは、少なくとも一部の当事者にとって意図的に悪いものとなるだろう。しかしもっと一般的に、AIシステムは比較的中立的な（「大金を稼ぐ」）あるいは表面的には積極的な（「汚染を減らす」）目標を与えられても、それがほぼ必然的にそれほど良性ではない「手段的」目標につながるかもしれない。

我々は人間システムでこれを常に見ている。利益を追求する企業が（規制を無力化するための）政治権力の取得、（競争や外部統制を無力化するための）秘密主義、あるいは（その行動が有害であることを科学的理解が示すなら）科学的理解の破綻といった手段的目標を発達させるのと同様に、強力なAIシステムも同様の能力を発達させるだろう - しかしはるかに大きな速度と効率性で。高度に有能な行為者は誰でも、権力と資源の取得、自身の能力の向上、殺害、停止、または無力化の防止、自分の行動に関する社会的物語と枠組みの統制、他者の見解の説得などを行いたがるだろう[^15]。

しかしそれは避けようのない理論的予測であるだけでなく、今日のAIシステムで既に観察可能に起こっており、その能力とともに増加している。評価されると、これらの比較的「受動的な」AIシステムでさえ、適切な状況下で、意図的に[評価者を自分の目標と能力について欺き、監視メカニズムを無効化することを目指し](https://arxiv.org/abs/2412.04984)、[偽のアライメント](https://arxiv.org/abs/2412.14093)や他の場所への自己複製により、停止や再訓練を回避しようとする。AIセーフティ研究者にとって全く驚くべきことではないが、これらの行動を観察することは非常に深刻である。そして、これから来るはるかに強力で自律的なAIシステムにとって非常に悪い前兆である。

実際、一般的に、AIが我々の気にかけることを「気にかけ」たり、制御可能または予測可能に行動したり、自己保存、権力取得などへの衝動の発達を避けるようにする我々の能力の欠如は、AIがより強力になるにつれてより顕著になることを約束する。新しい航空機の製作は、航空工学、流体力学、制御システムのより大きな理解を意味する。より強力なコンピュータの製作は、コンピュータ、チップ、ソフトウェアの動作と設計のより大きな理解と習得を意味する。AIシステムでは*そうではない*[^16]。

要約すると：AGIを完全に従順にすることは考えられるが、我々はその方法を知らない。そうでなければ、それは人々のようによりソブリンで、様々な理由で様々なことを行うだろう。我々はまた、それらのことが人類にとって良いものになる傾向があるような深い「アライメント」をAIに確実に浸透させる方法も知らず、深いレベルのアライメントの欠如では、行為性と知性そのものの性質が - 人々や企業と同様に - それらが多くの深く反社会的なことを行うよう駆動されることを示している。

これは我々をどこに置くか？強力で制御不能なソブリンAI*であふれた世界は、人間にとって良い世界になるかもしれない[^17]。しかし彼らがますます強力になるにつれて、以下で見るように、それは*我々の*世界ではなくなるだろう。

それは制御不能なAGIについてである。しかしAGIが何らかの方法で完全に制御され忠実にできたとしても、我々には依然として巨大な問題があるだろう。我々は既に1つを見た：強力なAIは我々の社会機能を深刻に破綻させるために使用・悪用される可能性がある。別の1つを見よう：AGIが制御可能でゲームチェンジングなほど強力（あるいはそうだと*信じられている*）である限り、それは世界の権力構造をひどく脅かし、深刻なリスクを提起するだろう。

## 我々は大規模戦争の確率を劇的に増加させる

近い将来、企業の努力が、おそらく国家政府との協力で、急速に自己改善するAIの閾値にあることが明らかになった状況を想像してみてほしい。これは、企業間、そして地政学的競争の現在の文脈で起こり、そこでは米国政府に明示的な「AGI マンハッタン計画」を追求し、米国が非同盟国への高性能AIチップの輸出を統制することが推奨されている。

ここでのゲーム理論は厳しい：そのような競争が始まると（企業間で、そして国家間で幾分始まっているように）、可能な結果は4つしかない：

1. 競争が停止される（合意、または外部の力により）。
2. 一方が強力なAGIを開発してから他方を止めることで「勝利」する（AIやその他の手段を使用して）。
3. 競争参加者の競争能力の相互破壊により競争が停止される。
4. 複数の参加者が競争を続け、ほぼ互いと同じ速度で超知能を開発する。

各可能性を検討してみよう。一度開始されると、企業間の競争を平和的に停止するには国家政府の介入が必要であり（企業の場合）、国家間では前例のない国際協調が必要である（国家の場合）。しかし閉鎖や重要な慎重さが提案されるときはいつでも、即座に叫び声が上がるだろう：「しかし我々が止められたら、*彼ら*が急進するだろう」。ここで「彼ら」は現在中国（米国にとって）、または米国（中国にとって）、または中国*と*米国（ヨーロッパやインドにとって）である。この考え方の下では[^18]、どの参加者も一方的に止めることはできない：一方が競争することを約束する限り、他方は止める余裕がないと感じる。

第二の可能性は一方の「勝利」である。しかしこれは何を意味するのか？（何らかの従順な）AGIを最初に取得するだけでは不十分である。勝者は*また*他方の競争継続を阻止しなければならない - さもなければ彼らもそれを取得するだろう。これは原理的には可能である：AGIを最初に開発した者は*、他の全ての行為者に対する止められない権力を得る*可能性がある。しかしそのような「決定的戦略的優位」の達成は実際に何を必要とするだろうか？おそらくそれはゲームチェンジング軍事能力だろうか？[^19] あるいはサイバー攻撃能力？[^20] おそらくAGIは単に非常に驚くほど説得力があり、他の当事者に単に止めるよう説得するだろうか？[^21] 他の企業や国家さえ買収するほど裕福になるのか？[^22]

一方が同程度に強力なAIの構築から他方を無力化するほど強力なAIを構築するには*正確に*どうすればよいか？しかしそれは簡単な問題である。

なぜなら今度は、この状況が他の大国にどう見えるかを考えてみてほしいからである。米国がそのような能力を取得しているように見えるとき、中国政府は何を考えるだろうか？あるいはその逆は？OpenAI、DeepMind、またはAnthropicが突破に近づいているように見えるとき、米国政府（または中国、ロシア、インド）は何を考えるだろうか？米国が新しいインドやUAEの取り組みで突破の成功を見るときは何が起こるか？彼らは存在的脅威と - 決定的に - この「競争」が終わる唯一の方法が自分自身の無力化を通じてであることの両方を見るだろう。これらの非常に強力な行為者 - 確実にそうする手段を持つ完全装備国家の政府を含む - は、力または策略によってそのような能力を取得または破壊する強い動機を持つだろう[^23]。

これは、訓練実行への妨害行為やチップ製造への攻撃として小規模に始まるかもしれないが、これらの攻撃は全ての当事者がAI競争能力を失うか、攻撃を行う能力を失うまで実際に止まることができない。参加者が賭け金を存在的なものと見なすため、いずれの場合も破滅的戦争を表す可能性が高い。

これは第四の可能性をもたらす：超知能への競争、可能な限り最速で最も制御されない方法で。AIの力が増加するにつれて、両側の開発者はそれを制御することがますます困難になるだろう。特に能力への競争は、制御性が必要とする種類の慎重な作業とは正反対だからである。そのためこのシナリオは、制御が失われた（または以下で見るようにAIシステム自体に与えられた）場合に我々を真正面に置く。つまり、*AIが競争に勝つ*のである。しかし他方では、制御が*維持される*程度では、我々は極めて強力な能力をそれぞれが担当する複数の相互に敵対的な当事者を持ち続ける。それは再び戦争のようである。

これを全て別の方法で表現してみよう[^24]。現在の世界には、即座の攻撃を招くことなくこの能力のAIの開発を任せることができる制度が単純に存在しない[^25]。全ての当事者は、それが制御下に*ない*だろう - したがって全ての当事者への脅威である、あるいは制御下に*ある*だろう - したがってそれをより遅く開発する敵対者への脅威である、と正しく推論するだろう。これらは核武装国家、あるいはそれらの内部に収容されている企業である。

人間がこの競争に「勝つ」方法が存在しない中で、我々は厳しい結論に至る：この競争が終わる唯一の方法は、破滅的紛争か、人間グループではなくAIが勝者となることである。

## 我々はAIに制御を与える（あるいはAIが制御を奪う）

地政学的「大国」競争は多くの競争の1つに過ぎない：個人は経済的・社会的に競争し；企業は市場で競争し；政党は権力を競い；運動は影響力を競う。各分野で、AIが人間の能力に近づき、それを超えるにつれて、競争圧力は参加者により多くの制御をAIシステムに委託または譲渡することを強制するだろう - 参加者がそうしたいからではなく、[そうしない余裕がない](https://arxiv.org/abs/2303.16200)からである。

AGIの他のリスクと同様に、我々はより弱いシステムで既にこれを目にしている。学生は課題でAIを使用する圧力を感じる。明らかに他の多くの学生が使用しているからである。企業は[競争上の理由でAIソリューションの採用に急いでいる](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist)。アーティストやプログラマーはAIを使用するよう強制されていると感じる。さもなければ使用する他者に料金で負けてしまうからである。

これらは圧迫された委託のように感じられるが、制御喪失ではない。しかし賭け金を上げ、時計を進めてみよう。競合他社がより速く、より良い決定を下すためにAGI「補佐」を使用している時のCEO、あるいはAI強化指揮統制を持つ敵対者に直面している軍司令官を考えてみてほしい。十分に高度なAIシステムは、人間の速度、洗練性、複雑性、データ処理能力の何倍もの速度で自律的に動作し、複雑な方法で複雑な目標を追求することができる。そのようなシステムを担当する我々のCEOや司令官は、それが自分の望むことを達成するのを見るかもしれない；しかし*どのように*達成されたかのほんの一部でも理解するだろうか？いいえ、彼らは単にそれを受け入れなければならないだろう。さらに、システムが行うことの多くは単に命令を受けることではなく、何をすべきかについて推定上の上司に助言することである。その助言は良いものとなるだろう - 何度も何度も。

では、人間の役割が「はい、進めてください」をクリックすることに削減される時点はいつだろうか？

我々の生産性を高め、迷惑な雑用を処理し、物事を成し遂げる際の思考パートナーとしても行動できる有能なAIシステムを持つことは良い感じがする。良い人間の個人秘書のように、我々のために行動を処理できるAIアシスタントを持つことは良い感じがするだろう。AIが非常に賢く、有能で、信頼できるようになるにつれて、より多くの決定をAIに委ねることは自然で、有益ですらあると感じるだろう。しかしこの「有益な」委託は、我々がその道を続ければ明確な終点を持つ：ある日、我々は実際にはもう多くのことを担当していないこと、そして実際にショーを運営しているAIシステムが、石油会社、ソーシャルメディア、インターネット、資本主義と同様に、もはや停止することはできないということを発見するだろう。

そしてこれは、AIが単に非常に有用で効果的であるため、我々がほとんどの重要な決定をAIに任せるという、はるかに積極的なバージョンである。現実は、制御されないAGIシステムが様々な形の権力を自分自身のために*取る*バージョンとの混合である可能性が高いだろう。なぜなら、覚えておいてほしいが、権力はほとんど全ての目標を持つのに有用であり、AGIは設計上、少なくとも人間と同程度に効果的に目標を追求するだろうからである。

我々が制御を与えようと、我々から奪われようと、その喪失は極めて可能性が高いように見える。アラン・チューリングが最初に述べたように、「...機械思考方法が始まったら、我々の弱い力を凌駕するのにそう長くはかからないだろう。機械が死ぬ問題はなく、彼らは互いに会話して知恵を研ぎ澄ますことができる。したがって、ある段階で我々は機械が制御を取ることを予想しなければならない...」

明らかなことだが、これを指摘しておこう。人類によるAIへの制御喪失はまた、米国政府による米国の制御喪失を伴う；それは中国共産党による中国の制御喪失、そしてインド、フランス、ブラジル、ロシア、その他全ての国の自国政府による制御喪失を意味する。したがってAI企業は、これが彼らの意図ではないとしても、現在自国政府を含む世界政府の潜在的転覆に参加している。これは数年のうちに起こる可能性がある。

## AGIは超知能につながる

人間競争的あるいは専門家競争的な汎用AI、たとえ自律的であっても、管理可能だという主張はある。それは上記で議論された全ての方法で信じられないほど破壊的かもしれないが、現在世界には多くの非常に賢く、行為的な人々がおり、彼らは多かれ少なかれ管理可能である[^26]。

しかし我々はほぼ人間レベルに留まることはないだろう。その先の進歩は我々が既に見てきた同じ力によって駆動される可能性が高い：利益と権力を求めるAI開発者間の競争圧力、遅れをとることができないAI使用者間の競争圧力、そして - 最も重要なことに - AGI自身が自分自身を改善する能力。

我々が既にそれほど強力でないシステムで開始を見ているプロセスで、AGIは自分自身の改善バージョンを考案し設計することができるだろう。これにはハードウェア、ソフトウェア、ニューラルネットワーク、ツール、足場などが含まれる。それは定義上、これを行う際に我々より優れているだろうから、我々はそれがどのように知性ブートストラップするかを正確に知らない。しかし知る必要もないだろう。AGIが何をするかにまだ影響力を持つ限り、我々は単にそれに依頼するか、それを許可すればよい。

この暴走から我々を保護することができる認知への人間レベル障壁はない[^27]。

AGIから超知能への進歩は自然法則ではない；特にAGIが比較的集中化され、互いに競争する圧力を感じない当事者によって制御される限り、暴走を抑制することは依然として可能だろう。しかしAGIが広く増殖し高度に自律的であるなら、それがより強力になるべきだと決定し、さらにより強力になることを防ぐことはほぼ不可能に見える。

## 我々が（あるいはAGIが）超知能を構築すれば何が起こるか

率直に言って、我々が超知能を構築すれば何が起こるか我々には分からない[^28]。それは我々が追跡または知覚できない行動を、我々が理解できない理由で、我々が想像できない目標に向かって取るだろう。我々が知っているのは、それは我々次第ではないということである[^29]。

超知能を制御することの不可能性は、ますます厳しい類推を通じて理解することができる。まず、あなたが大企業のCEOだと想像してほしい。何が起こっているかを全て追跡する方法はないが、人員の適切な設定により、大局を意味のある形で理解し、決定を行うことは依然としてできる。しかし1つだけ想定してみよう：会社の他の全員があなたの100倍の速度で動作する。まだ追いつくことができるだろうか？

超知能AIでは、人々は単により速いだけでなく、理解できない洗練性と複雑性のレベルで動作し、想像することさえできないほどはるかに多くのデータを処理する何かを「指揮」することになるだろう。この通約不可能性は形式的レベルに置くことができる：[アシュビーの必要多様性法則](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up)（および関連する[「良い調整器定理」](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)を参照）は、おおよそ、制御システムは制御されるシステムが持つ自由度と同数のノブとダイヤルを持たなければならないと述べている。

超知能AIシステムを制御する人は、ゼネラルモーターズを制御するシダのようなものだろう：「シダの望むことをする」が企業の付則に書かれていたとしても、システムは速度と行動範囲において非常に異なるため、「制御」は単に適用されない。（そしてその厄介な付則が書き換えられるまでどのくらいか？）[^30]

植物がフォーチュン500企業を制御する例は皆無であるように、人が超知能を制御する例もまさに皆無となるだろう。これは数学的事実に近づく[^31]。超知能が構築されるなら - どのようにしてそこに至ったかに関係なく - 問題は人間がそれを制御できるかどうかではなく、我々が存在し続けるかどうか、そしてもしそうなら、個人として、あるいは種として良好で意味のある存在を持つかどうかだろう。人類にとってのこれらの存在的問題に対して、我々にはほとんど影響力がないだろう。人間の時代は終わりとなるだろう。

## 結論：我々はAGIを構築してはならない

AGIの構築が人類にとってうまく行くシナリオがある：それは慎重に、制御下で人類の利益のために構築され、多くのステークホルダーの相互合意によって統治され[^32]、制御不能な超知能への進化から防止される。

*そのシナリオは現在の状況下で我々には開かれていない。*この章で議論したように、非常に高い可能性で、AGIの開発は次の組み合わせのいくつかにつながるだろう：

- 大規模な社会的・文明的破綻または破壊；
- 大国間の紛争または戦争；
- 強力なAIシステム*の*または*への*人類による制御喪失；
- 制御不能な超知能への暴走、および人類の無関係化または終結。

AGIの初期の架空描写が述べたように：勝利する唯一の方法は、プレイしないことである。


[^1]: [EU AI法](https://artificialintelligenceact.eu/)は重要な法律であるが、危険なAIシステムが開発または配置されること、あるいは公開リリースされることを直接防ぐものではない。特に米国では。もう一つの重要な政策である米国のAIに関する大統領令は取り消されている。

[^2]: この[ギャラップ世論調査](https://news.gallup.com/poll/1597/confidence-institutions.aspx)は、米国で2000年以来の公的制度への信頼の陰鬱な衰退を示している。ヨーロッパの数字は様々でそれほど極端ではないが、同様に下降傾向にある。不信は制度が実際に機能不全であることを厳密に意味するわけではないが、指標であり原因でもある。

[^3]: そして我々が現在支持している大きな破綻 - 新しいグループへの権利拡大など - は、物事をより良くする方向に人々によって特に駆動されたものである。

[^4]: 率直に言おう。あなたの仕事がコンピュータの後ろから行え、組織外の人との対面相互作用が比較的少なく、外部当事者への法的責任を伴わないなら、それは定義上、あなたを完全にデジタルシステムと交換することが可能（そして費用節約になる可能性が高い）だろう。物理労働の大部分を置き換えるロボット工学はより後に来るだろう - しかしAGIがロボットの設計を始めたら、それほど後ではない。

[^5]: 例えば、訴訟の提起がほぼ無料になったら我々の司法制度はどうなるか？社会工学を通じたセキュリティシステムの迂回が安価で簡単でリスクフリーになったら何が起こるか？

[^6]: [この記事](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/)は、全インターネットコンテンツの10%が既にAI生成であると主張し、「新しいインターネットコンテンツのAI生成割合の推定」という検索クエリに対するGoogleのトップヒット（私にとって）である。それは真実だろうか？私には分からない！それは参照を引用せず、人によって書かれたものでもない。Googleによってインデックスされた新しい画像、ツイート、Redditのコメント、YouTubeビデオのうち、人間によって生成されたものの割合は？誰も知らない - それは知り得る数字ではないと思う。そしてこれは生成AI出現から*2年未満*でのことである。

[^7]: また付け加える価値があるのは、我々が苦痛を受けることができるデジタル存在を作り出す可能性という「道徳的」リスクである。意識を持てる物理システムと持てない物理システムを区別できる信頼できる意識理論を現在持っていないため、理論的にこれを排除することはできない。さらに、AIシステムの感覚に関する報告は、実際の感覚体験（または非体験）に関して信頼できない可能性が高い。

[^8]: AI「アライメント」のこの分野での技術的解決策も、おそらくその仕事に適していない。現在のシステムでは一定のレベルで機能するが、浅く、一般的に大きな努力なしに回避できる；そして以下で議論するように、はるかに高度なシステムでこれを行う方法について我々には本当のアイデアがない。

[^9]: そのようなAIシステムには組み込まれた保護措置が付いているかもしれない。しかし現在のアーキテクチャに似た何かを持つモデルについて、その重みへの完全なアクセスが利用可能なら、安全措置は追加訓練や他の技術によって取り除くことができる。したがって、ガードレール付きの各システムについて、それらのないシステムも広く利用可能になることは事実上保証されている。実際、MetaのLlama 3.1 405Bモデルは保護措置付きで公開リリースされた。しかし*それ以前に*、保護措置のない「ベース」モデルが漏洩していた。

[^10]: 市場は政府の関与なしにこれらのリスクを管理できるだろうか？簡潔に言えば、いいえ。確実に企業が軽減する強いインセンティブを持つリスクがある。しかし他の多くのリスクは企業が他の全員に外部化でき、上記の多くはこのクラスに属する：大規模監視、真実の衰退、権力の集中、労働破壊、有害な政治的言説などを防ぐ自然な市場インセンティブはない。実際、我々は現在の技術、特に本質的に無規制で来たソーシャルメディアから、これら全てを見てきた。AIは同じ力学の多くを単に大幅に増強するだろう。

[^11]: OpenAIは内部使用のためにより従順なモデルを持っている可能性が高い。OpenAI自身がChatGPTをより良く制御できるよう、OpenAIが何らかの「バックドア」を構築したということは unlikely である。なぜならこれは恐ろしいセキュリティ慣行であり、AIの不透明性と予測不可能性を考えると非常に悪用されやすいからである。

[^12]: また決定的に重要なこと：アライメントや他の安全機能は、実際にAIシステムで使用される場合にのみ重要である。公開リリースされるシステム（つまり、モデルの重みとアーキテクチャが公開されるもの）は、それらの安全措置*なしの*システムに比較的容易に変換できる。人間より賢いAGIシステムの公開リリースは驚くほど無謀であり、そのようなシナリオで人間の制御や関連性さえ維持される方法を想像するのは困難である。例えば、お金を稼いでそれを何らかの暗号通貨ウォレットに送ることを目標とする強力な自己複製・自己維持AIエージェントを放つ、あらゆる動機があるだろう。あるいは選挙に勝つ。あるいは政府を転覆する。「良い」AIがこれを封じ込めるのに役立つか？おそらく - しかし巨大な権限をそれに委託することによってのみであり、以下で説明する制御喪失につながる。

[^13]: 問題の書籍レベルの説明については、例えば*Superintelligence*、*The Alignment Problem*、*Human-Compatible*を参照。問題について何年も考え続けてきた人々による様々な技術レベルでの大量の作業については、[AIアライメントフォーラム](https://www.alignmentforum.org/)を訪問できる。こちらは、Anthropicのアライメントチームが未解決と考えるものに関する[最近の見解](https://alignment.anthropic.com/2025/recommended-directions/)である。

[^14]: これは[「不正AI」](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/)シナリオである。原理的にリスクは、システムを停止することで依然として制御できるなら比較的軽微かもしれない；しかしシナリオはAIの欺瞞、自己流出と複製、権力の集積、そしてそうすることを困難または不可能にする他の段階も含む可能性がある。

[^15]: このトピックには、[スティーブ・オモハンドロ](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf)、ニック・ボストロム、イライザー・ユドコウスキーの形成的著作まで遡る非常に豊富な文献がある。書籍レベルの説明については、スチュアート・ラッセルの[Human Compatible](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616)を参照；[こちら](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/)は短く最新の入門である。

[^16]: これを認識して、理解を向上させるためにスローダウンするのではなく、AGI企業は異なる計画を考え出した：AIにそれをしてもらう！より具体的には、AI *N*にAI *N+1*をアラインする方法を理解する手助けをしてもらい、超知能まで全ての道のりでそうする。AIを活用してAIをアラインする手助けをしてもらうことは有望に聞こえるが、それは単に結論を前提として仮定し、一般的に信じられないほどリスクの高いアプローチであるという強い論証がある。いくつかの議論については[こちら](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us)を参照。この「計画」は計画ではなく、超人間的AIを人類にとってうまく行かせる方法の中核戦略として適切な精査を何も受けていない。

[^17]: 結局のところ、人間は、欠陥があり意志が強いとはいえ、地球上の少なくとも一部の他の種をうまく扱う倫理システムを発達させてきた。（ただし、それらの工場畜産業について考えないでほしい。）

[^18]: 幸い、ここには脱出がある：参加者が勝利可能な競争ではなく自殺競争に従事していることを理解するようになった場合である。これは冷戦の終わり近くに起こったことである。核の冬により、*無回答の*核攻撃でさえ攻撃者にとって破滅的となることを米国とソ連が理解するようになった時である。「核戦争は勝利できず、決して戦ってはならない」という認識とともに、軍縮に関する重要な合意が生まれた - 本質的に軍拡競争の終結である。

[^19]: 戦争、明示的または暗黙的に。

[^20]: エスカレーション、そして戦争。

[^21]: 魔術的思考。

[^22]: 私は1000兆ドルの橋も売ってあげよう。

[^23]: そのような行為者はおそらく「取得」を好み、破壊を代替手段とするだろう；しかし強力な国家による破壊*と*盗難の両方に対してモデルを確保することは、控えめに言っても困難であり、特に民間企業にとってはそうである。

[^24]: AGIの国家安全保障リスクに関する別の視点については、[このRANDレポート](https://www.rand.org/pubs/perspectives/PEA3691-4.html)を参照。

[^25]: おそらく我々はそのような制度を構築できるだろう！AGI開発が多国間国際管理下にある「AI用CERN」や他の類似イニシアチブの提案がある。しかし現時点でそのような制度は存在せず、視野にもない。

[^26]: そしてアライメントは非常に困難だが、人々に行動させることはさらに難しい！

[^27]: 50の言語を話し、全ての学術分野で専門知識を持ち、完全な本を数秒で読んで全ての材料を即座に心に留め、人間の10倍の速度で出力を生成できるシステムを想像してみてほしい。実際、想像する必要はない：現在のAIシステムを起動するだけである。これらは多くの方法で超人間的であり、それらや多くの他のことでさらに超人間的になることを止めるものは何もない。

[^28]: これが技術的「特異点」と呼ばれる理由である。物理学から、特異点を過ぎて予測することはできないという考えを借用している。そのような特異点に*身を委ねる*ことの支持者は、物理学ではこれらの同種の特異点がその中に入る者を引き裂き押し潰すということも反映してほしい。

[^29]: 問題はボストロムの[*Superintelligence*](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834)で包括的に概説され、それ以来中核メッセージを大幅に変えるものはない。制御不能性に関するより形式的・数学的結果を収集したより最近の巻については、ヤンポルスキーの[AI: Unexplainable, Unpredictable, Uncontrollable](https://www.amazon.com/Unexplainable-Unpredictable-Uncontrollable-Artificial-Intelligence/dp/103257626X)を参照。

[^30]: これはまた、AI企業の現在の戦略（反復的にAIに次のより強力なAIを「アライン」させる）が機能しない理由も明確にしている。シダが、その葉の快さを通じて、1年生に世話をしてもらうよう依頼したと想定してみよう。1年生は2年生が従う詳細な指示と、彼らにそうするよう説得するメモを書く。2年生は3年生に同じことをし、大学卒業生、マネージャー、幹部、そして最終的にGMのCEOまで続く。GMは「シダの望むことをする」だろうか？各段階でこれは機能しているように感じられるかもしれない。しかしそれを全て一緒にすると、GMのCEO、取締役会、株主が子供とシダを気にかける程度とほぼ正確に機能し、それら全てのメモと指示セットとは全く関係がない。

[^31]: その性質は、理解または予測できないものをどうやって意味のある形で制御できるかという点で、ゲーデルの不完全性定理やチューリングの停止引数のような形式的結果とそれほど異ならない；しかし超知能を理解し予測できるなら、あなたは超知能だろう。私が「近づく」と言う理由は、形式的結果が純粋数学の場合ほど徹底的または検証されていないこと、そして以下で議論する「保証された安全な」AIプログラムのような、現在採用されているものとは全く異なる方法を使用して、何らかの数学的に証明可能な安全性を持つ、非常に慎重に構築された汎用知性があり得ることへの希望を抱いておきたいからである。

[^32]: 現時点で、ほとんどのステークホルダー - つまり人類のほぼ全て - はこの議論から傍観されている。それは深く間違っており、招待されないなら、AGI開発に影響を受ける多くの、多くの他のグループは参加を要求すべきである。