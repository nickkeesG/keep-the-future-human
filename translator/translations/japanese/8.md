# 第8章 - AGIを構築しない方法

AGIは必然ではない――今日、私たちは分岐点に立っている。本章では、AGIの構築を阻止する方法についての提案を示す。

現在進んでいる道が文明の終焉につながる可能性が高いなら、どうすれば道を変えることができるのか？

AGIと超知能の開発を停止したいという願いが広く共有され、強力になったとしよう[^1]。AGIが権力を付与するのではなく吸収し、社会と人類にとって深刻な危険をもたらすという理解が一般的になったからだ。どのようにしてゲートを閉じるのか？

現在、私たちが強力で汎用的なAIを*作る*唯一の方法は、深層ニューラルネットワークの真に大規模な計算を通じてである。これらは非常に困難で高価なことなので、それらを*しない*ことは、ある意味では簡単だ[^2]。しかし、私たちはすでにAGIへと駆り立てる力と、いずれかの当事者が一方的に止めることを非常に困難にするゲーム理論的な力学を見てきた。そのため、外部からの介入（すなわち政府）による企業の阻止と、政府間の合意による自制の組み合わせが必要となる[^3]。これはどのようなものになるだろうか？

まず、*防止*または*禁止*すべきAI開発と、*管理*すべきものを区別することが有用である。前者は主に超知能への暴走である[^4]。禁止される開発については、定義は可能な限り明確であるべきで、検証と執行の両方が実用的でなければならない。*管理*すべきものは、汎用的で強力なAIシステムである――これはすでに存在し、多くのグレーゾーン、ニュアンス、複雑さを持つ。これらには、強力で効果的な機関が極めて重要である。

また、国際レベル（地政学的ライバルや敵対者間を含む）で対処すべき問題[^5]と、個々の管轄区域、国、または国の集団が管理できるものを区別することも有用である。禁止される開発は主に「国際的」カテゴリーに属する。なぜなら、技術開発の局所的な禁止は一般的に場所を変えることで回避できるからだ[^6]。

最後に、ツールボックス内の道具を検討できる。技術的ツール、ソフト・ロー（標準、規範など）、ハード・ロー（規制と要件）、責任制度、市場インセンティブなど、多くがある。AIに特有のものに特別な注意を向けよう。

## 計算資源セキュリティとガバナンス

高性能AIの統治における中核的なツールは、それが必要とするハードウェアである。ソフトウェアは容易に普及し、限界生産費用がほぼゼロで、国境を簡単に越え、瞬時に修正できる。これらのどれもハードウェアには当てはまらない。しかし、これまで議論してきたように、AIシステムの訓練と推論の両方で最も高性能なシステムを実現するには、この「計算資源」の膨大な量が必要である。計算資源は容易に定量化、説明、監査でき、良いルールが開発されれば曖昧さは比較的少ない。最も重要なことに、大量の計算は、濃縮ウランのように、非常に希少で高価で製造困難な資源である。コンピューターチップはどこにでもあるが、AI用ハードウェアは高価で製造が非常に困難だ[^7]。

AI専用チップをウランよりもはるかに*管理しやすい*希少資源にするのは、ハードウェアベースのセキュリティメカニズムを含められることだ。最新の携帯電話の多くとノートパソコンの一部は、承認されたオペレーティングシステムソフトウェアとアップデートのみをインストールすること、機密な生体認証データをデバイス上で保持・保護すること、紛失や盗難時に所有者以外には役立たないよう無力化できることを保証する、専用のオンチップハードウェア機能を持つ。過去数年間で、このようなハードウェアセキュリティ対策は確立され広く採用され、一般的に非常に安全であることが証明されている。

これらの機能の重要な新規性は、暗号化を使ってハードウェアとソフトウェアを結び付けることである[^8]。つまり、特定のコンピューターハードウェアを持っているからといって、ユーザーが異なるソフトウェアを適用して何でもできるわけではない。そして、この結び付きは、多くの攻撃が*ソフトウェア*セキュリティだけでなく*ハードウェア*セキュリティの侵害を必要とするため、強力なセキュリティも提供する。

最近のいくつかの報告書（例えば[GovAIと共同研究者](https://www.governance.ai/post/computing-power-and-the-governance-of-ai)、[CNAS](https://www.cnas.org/publications/reports/secure-governable-chips)、[RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)）は、最先端のAI関連計算ハードウェアに組み込まれた同様のハードウェア機能が、AIセキュリティとガバナンスにおいて極めて有用な役割を果たすことができると指摘している。これらは「ガバナー」[^9]が利用可能であると推測できない、または可能とさえ思えない多くの機能を可能にする。主な例として：

- *地理位置特定*：チップが既知の位置を持ち、位置に基づいて異なる動作をする（または完全にシャットオフされる）ようシステムを設定できる[^10]。
- *許可リスト接続*：各チップは、ネットワーク接続可能な特定の他のチップのハードウェア強制許可リストで構成でき、このリストにないチップとは接続できない[^11]。これにより、チップの通信クラスターのサイズを制限できる[^12]。
- *従量制推論または訓練（および自動オフスイッチ）*：ガバナーは、ユーザーが実行できる一定量の訓練または推論（時間、FLOP、またはトークンで）のみをライセンスでき、その後新しい許可が必要になる。増分が小さい場合、モデルの比較的継続的な再ライセンスが必要になる。このライセンス信号を保留するだけで、モデルを「オフにする」ことができる[^13]。
- *速度制限*：モデルが、ガバナーまたは他の方法で決定される制限を超える推論速度で実行することを防ぐ。これは、限定された許可リスト接続のセット、またはより洗練された手段を通じて実装できる。
- *証明された訓練*：訓練手順は、特定のコード、データ、および計算使用量のセットがモデルの生成に使用されたことの暗号学的に安全な証明を生成できる。

## 超知能を構築しない方法：訓練および推論計算量に関するグローバル制限

これらの考慮事項――特に計算に関して――を踏まえて、人工超知能へのゲートを閉じる方法について議論できる。その後、完全なAGIの防止と、異なる側面で人間の能力に近づき、それを超えるAIモデルの管理について論じる。

最初の要素は、もちろん、超知能は制御不可能であり、その結果は根本的に予測不可能だという理解である。少なくとも中国と米国は、この目的またはその他の目的のために、超知能を構築しないことを独立して決定しなければならない[^14]。その後、強力な検証と執行メカニズムを持つ両国およびその他の国々の間の国際合意が必要で、すべての当事者にライバルが裏切ってサイコロを振ることを決めていないことを保証する必要がある。

検証可能で執行可能にするため、制限はハード制限であり、可能な限り明確でなければならない。これは事実上不可能な問題のようだ：予測不可能な特性を持つ複雑なソフトウェアの能力を世界規模で制限することだ。幸い、状況ははるかに良い。高度なAIを可能にしたまさにそのもの――膨大な量の計算資源――の方がはるかに制御しやすいからだ。いくつかの強力で危険なシステムを許容する可能性はあるが、*超知能の暴走*は、ニューラルネットワークに投入される計算量のハード上限と、AIシステム（接続されたニューラルネットワークと他のソフトウェア）が実行できる推論量の制限によって阻止できる可能性が高い。この具体版を以下に提案する。

AI計算にハードなグローバル制限を課すには、膨大なレベルの国際協調と、プライバシーを破壊する侵入的な監視が必要のように思える。幸い、そうではない。極めて[緊密でボトルネックのあるサプライチェーン](https://arxiv.org/abs/2402.08797)により、制限が法的に設定されれば（法律または大統領令によって）、その制限への準拠の検証は、少数の大企業の関与と協力のみを必要とする[^15]。

このような計画には多くの非常に望ましい特徴がある。少数の大企業のみに要件が課され、かなり大規模な計算クラスターのみが統治されるという意味で、最小限の侵入的である。関連チップは、最初のバージョンに必要なハードウェア機能をすでに含んでいる[^16]。実装と執行の両方が標準的な法的制限に依存している。しかし、これらはハードウェアの利用規約とハードウェア制御によって裏付けられ、執行を大幅に簡素化し、企業、民間グループ、さらには国による不正行為を阻止する。ハードウェア企業がハードウェア使用にリモート制限を課し、特定の機能を外部でロック/アンロックする豊富な前例がある[^17]。データセンターの高性能CPUでさえそうだ[^18]。影響を受けるハードウェアと組織のかなり小さな割合であっても、監督はテレメトリに限定でき、データやモデル自体への直接アクセスは不要である。そのためのソフトウェアは検査のために公開でき、追加データが記録されていないことを示すことができる。このスキーマは国際的で協力的であり、非常に柔軟で拡張可能である。制限は主にソフトウェアではなくハードウェアにあるため、AIソフトウェアの開発と展開の方法に比較的不可知論的であり、AI駆動の権力集中に対抗することを目的とした、より「分散化」または「公共」AIを含む様々なパラダイムと互換性がある。

計算ベースのゲートクローズには欠点もある。第一に、AI統治全般の問題の完全な解決策には程遠い。第二に、コンピューターハードウェアが高速化するにつれ、システムはより小さなクラスター（または個々のGPU）でさえ、より多くのハードウェアを「捕捉」するようになる[^19]。また、アルゴリズムの改良により、より低い計算制限が必要になる可能性もあり[^20]、または計算量が大部分無関係になり、ゲートを閉じるにはより詳細なリスクベースまたは能力ベースのAI統治体制が必要になる可能性もある。第三に、保証や影響を受ける事業体数の少なさに関わらず、このようなシステムはプライバシーと監視に関する懸念などから反発を生むに違いない[^21]。

もちろん、短期間で計算制限ガバナンススキームを開発・実装することは非常に困難である。しかし、絶対に実行可能だ。

## A-G-I：リスクと政策の基盤としての三重交差

次にAGIに目を向けよう。ここではハードラインと定義がより困難である。なぜなら、人工的で汎用的な知能は確実に存在し、現存するどの定義でも、それが存在するかどうか、いつ存在するかについて全員が合意することはないからだ。さらに、計算または推論制限は（計算が能力の代理指標であり、それがリスクの代理指標である）やや鈍い道具であり――かなり低くない限り――社会または文明の破綻や急性リスクを引き起こすほど強力なAGIを阻止する可能性は低い。

私は、最も深刻なリスクが非常に高い能力、高い自律性、大きな汎用性の三重交差から生じると論じてきた。これらは――もし開発されるなら――極めて慎重に管理されなければならないシステムである。3つの特性すべてを組み合わせたシステムに対して厳格な標準（責任制度と規制を通じて）を作ることで、AI開発をより安全な代替案に向けることができる。

消費者や公衆に潜在的に害を与える可能性のある他の産業や製品と同様に、AIシステムには効果的で権限を与えられた政府機関による慎重な規制が必要である。この規制はAGIの固有のリスクを認識し、受け入れがたくリスクの高い高性能AIシステムの開発を阻止すべきである[^22]。

しかし、産業界からの反対が確実な真の歯を持つ大規模な規制[^23]には、政治的確信と同様に時間がかかる[^24] [^25]。進歩のペースを考えると、これは利用可能な時間を超える可能性がある。

規制措置が開発される間、はるかに高速なタイムスケールで、企業に(a)非常にリスクの高い活動からの離脱と(b)リスクの評価と軽減のための包括的システムの開発に必要なインセンティブを与えることができる。最も危険なシステムの責任レベルを明確化し、増大させることによってだ。アイデアは、高い自律性-汎用性-知能の三重交差にあるシステムに対して最高レベルの責任――厳格で場合によっては個人的な刑事責任――を課すが、これらの特性の一つが欠如している、または管理可能であることが保証されているシステムに対しては、より典型的な過失ベースの責任への「セーフハーバー」を提供することである。つまり、例えば、汎用的で自律的だが「弱い」システム（有能で信頼できるが限定的なパーソナルアシスタントのような）は、より低い責任レベルの対象となる。同様に、自動運転車のような狭く自律的なシステムは、すでに対象となっている重要な規制の対象であり続けるが、強化された責任ではない。高度に有能で汎用的だが「受動的」で独立した行動がほぼ不可能なシステムも同様である。3つの特性のうち*2つ*が欠如しているシステムはさらに管理しやすく、セーフハーバーを主張するのはさらに容易になる。このアプローチは、他の潜在的に危険な技術の扱い方を反映している[^26]：より危険な構成に対するより高い責任は、より安全な代替案への自然なインセンティブを作り出す。

公衆ではなく企業にAGIリスクを*内在化*させるこのような高レベルの責任のデフォルト結果は、企業が*自らの指導者*がリスクを負う当事者であることを考慮して、真に信頼でき、安全で制御可能にできない限り、完全なAGIを単に開発しないことである（そして願わくば！）。（これが不十分な場合、責任を明確化する法律は、明らかに危険ゾーンにあり、公共リスクをもたらすと論じられる活動に対する差し止め救済、すなわち裁判官による停止命令も明示的に認めるべきである。）規制が整備されるにつれ、規制の遵守がセーフハーバーとなり、AIシステムの低い自律性、狭さ、または弱さからのセーフハーバーは、比較的軽い規制体制に変換できる。

## ゲートクローズの主要規定

上記の議論を念頭に置いて、本節では完全なAGIと超知能の禁止を実装・維持し、完全なAGI閾値近くの人間競争レベルまたは専門家競争レベルの汎用AIを管理する主要規定の提案を示す[^27]。4つの主要要素がある：1）計算資源の会計と監督、2）AIの訓練と運用における計算上限、3）責任制度、4）ハード規制要件を含む階層化された安全・セキュリティ基準の定義。これらは次に簡潔に記述され、さらなる詳細または実装例が3つの付属表で提供される。重要なことに、これらは高度なAIシステムを統治するために必要なすべてからは程遠い。追加のセキュリティと安全上の利益をもたらすが、知能の暴走へのゲートを閉じ、AI開発をより良い方向に向け直すことを目的としている。

### 1. 計算資源会計と透明性

- 標準化機関（例：米国のNIST、続いて国際的にISO/IEEE）は、AIモデルの訓練と運用に使用される総計算量をFLOPで、およびFLOP/sでの動作速度について詳細な技術標準を成文化すべきである。これがどのようなものになり得るかの詳細は付録Aに示されている[^28]。
- 大規模AI訓練が行われる管轄区域では、新しい法律または既存の権限の下で[^29]、10<sup>25</sup> FLOPまたは10<sup>18</sup> FLOP/sの閾値を超えるすべてのモデルの訓練と運用に使用される総FLOPを規制機関またはその他の機関に計算・報告する要件を課すべきである[^30]。
- これらの要件は段階的に導入され、最初は四半期ベースでよく文書化された誠実な推定を要求し、後の段階では各モデル*出力*に添付される暗号学的に証明された総FLOPとFLOP/sまでのより高い標準を段階的に要求する。
- これらの報告は、各AI出力の生成に使用される限界エネルギーと金銭コストのよく文書化された推定で補完されるべきである。

根拠：これらのよく計算され透明に報告された数値は、訓練と運用の上限、およびより高い責任制度からのセーフハーバーの基盤を提供する（付録CとDを参照）。

### 2. 訓練および運用計算上限

- AIシステムをホストする管轄区域は、任意のAIモデル出力に投入される総計算量に、10<sup>27</sup> FLOPから始まり[^31]、適宜調整可能なハード制限を課すべきである。
- AIシステムをホストする管轄区域は、AIモデル出力の計算率に、10<sup>20</sup> FLOP/sから始まり、適宜調整可能なハード制限を課すべきである。

根拠：総計算量は非常に不完全だが、具体的に測定・検証可能なAI能力（およびリスク）の代理指標であり、能力制限のハードなバックストップを提供する。具体的な実装提案は付録Bに示されている。

### 3. 危険システムに対する強化された責任制度

- 高度に汎用的、有能、かつ自律的な高度なAIシステムの作成と運用[^32]は、単一当事者の過失ベースではなく、厳格で連帯責任制に法的に明確化されるべきである[^33]。
- 小規模（計算資源の観点で）、弱い、狭い、受動的、または十分な安全性、セキュリティ、制御可能性の保証を持つシステムに対して、厳格責任からのセーフハーバーを付与する積極的安全ケースを作成する法的プロセスが利用可能であるべきである。
- 公共の危険を構成するAI訓練と推論活動を停止するための差し止め救済への明示的な道筋と条件のセットが概説されるべきである。

根拠：AIシステムは責任を負えないため、それらが引き起こす害について人間個人と組織を責任者としなければならない（責任制度）[^34]。制御不可能なAGIは社会と文明への脅威であり、安全ケースがない場合は「異常に危険」と見なされるべきである。強力なモデルが「異常に危険」と見なされないほど十分に安全であることを示す責任を開発者に負わせることは、これらのセーフハーバーを主張するための透明性と記録保持とともに、安全な開発にインセンティブを与える。規制は責任制度からの抑止が不十分な場合に害を防ぐことができる。最後に、AI開発者は彼らが引き起こす損害についてすでに責任を負っているため、最もリスクの高いシステムの責任を法的に明確化することは、非常に詳細な標準が開発されることなく即座に実行でき、これらは時間をかけて開発できる。詳細は付録Cに示されている。

### 4. AI安全規制

AIの大規模急性リスクに対処する規制システムには、最低限以下が必要である：

- 適切な規制機関のセット、おそらく新機関の特定または創設；
- 包括的リスク評価枠組み[^35]；
- 部分的にリスク評価枠組みに基づいた、開発者による積極的安全ケースの枠組みと、*独立*グループと機関による監査；
- 能力レベルを追跡する階層を持つ階層化ライセンスシステム[^36]。ライセンスは安全ケースと監査に基づいてシステムの開発と展開に付与される。要件は低い段階での通知から、最上位段階での開発前の定量的安全性、セキュリティ、制御可能性保証まで範囲に及ぶ。これらはシステムが安全であることが実証されるまでリリースを防ぎ、本質的に安全でないシステムの開発を禁止する。付録Dは、そのような安全・セキュリティ基準が何を含み得るかの提案を提供する。
- そのような措置を国際レベルに持ち込む合意、規範と基準を調和させる国際機関、潜在的に安全ケースを審査する国際機関を含む。

根拠：最終的に、責任制度は新しい技術からの公衆への大規模リスクを防ぐ適切なメカニズムではない。権限を与えられた規制機関を持つ包括的な規制が、他のすべての公衆にリスクをもたらす主要産業と同様に、AIにも必要になる[^37]。

他の広範だがより急性度の低いリスクを防ぐ方向の規制は、管轄区域によってその形式が異なる可能性が高い。重要なことは、これらのリスクが管理不可能になるほどリスクの高いAIシステムの開発を避けることである。

## その後どうするか？

今後10年間で、AIがより広範囲に浸透し、中核技術が進歩するにつれて、2つの重要なことが起こる可能性が高い。第一に、既存の強力なAIシステムの規制がより困難になるが、さらに必要になる。大規模安全リスクに対処する少なくともいくつかの措置は国際レベルでの合意を必要とし、個別管轄区域が国際合意に基づくルールを執行する可能性が高い。

第二に、ハードウェアがより安価で費用対効果が高くなるにつれて、訓練および運用計算上限の維持がより困難になる。また、アルゴリズムとアーキテクチャの進歩により、それらの関連性が低下する（またはさらに厳しくする必要がある）可能性もある。

AIの制御がより困難になるからといって諦めるべきではない！この論文で概説された計画を実装することは、貴重な時間と、社会、文明、種に対するAIの存在リスクを回避するはるかに優れた立場に立つプロセスの重要な制御の両方を与えてくれる。

さらに長期的には、何を許可するかについて選択を行うことになる。真に制御可能なAGIのいくつかの形式を、これが可能であると証明される程度まで、依然として創造することを選択するかもしれない。または、機械により良い仕事をし、私たちを良く扱うことを自分自身に納得させることができれば、世界の運営を機械に任せることを決定するかもしれない。しかし、これらはAIの深い科学的理解を手にし、意味のあるグローバルで包括的な議論の後に行われるべき決定であり、人類の大部分が完全に関与せず無自覚なまま技術界の大物たちの間でレースで決定されるべきではない。

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) 責任制度と規制を通じたA-G-Iと超知能ガバナンスの要約。責任は自律性、汎用性、知能の三重交差で最も高く、規制が最も強い。システムが弱い、および/または狭い、および/または受動的であることを実証する積極的安全ケースを通じて、厳格責任と強い規制からのセーフハーバーを得ることができる。法的および暗号学的セキュリティ措置を用いて検証・執行される総訓練計算量と推論計算率の上限は、完全なAGIを避け、超知能を効果的に禁止することで安全をバックストップする。

[^1]: 最も可能性が高いのは、この認識の拡散が、この主張を行う教育・擁護グループによる激しい努力、またはかなり重大なAI起因の災害のいずれかを要することだ。前者であることを願う。

[^2]: 逆説的に、私たちは自然が技術開発を非常に困難にすることで、特に科学的に、技術を制限することに慣れている。しかし、それはもはやAIには当てはまらない：主要な科学的問題は予想より簡単であることが判明している。ここで自然が私たち自身から救ってくれることは期待できない――私たち自身がそうしなければならない。

[^3]: では、新システムの開発をどこで正確に止めるのか？ここでは、予防原則を採用すべきだ。システムがいったん展開され、特にそのレベルのシステム能力が拡散すると、それを後退させることは極めて困難である。そして、システムが（特に大きなコストと労力をかけて）*開発*されれば、それを使用または展開する巨大な圧力と、それがリークまたは盗難される誘惑がある。システムを開発し、*その後*それらが深刻に安全でないかどうかを決定するのは危険な道である。

[^4]: 本質的に危険なAI開発、例えば自己複製し進化するシステム、囲い込みからの脱出を意図したもの、自律的に自己改良できるもの、故意に欺瞞的で悪意のあるAIなどを禁止することも賢明であろう。

[^5]: これは、何らかのグローバル機関による国際レベルでの*執行*を必ずしも意味しない：代わりに主権国家が多くの条約のように合意されたルールを執行できる。

[^6]: 以下で見るように、AI計算の性質は何らかのハイブリッドを可能にするが、国際協力は依然として必要である。

[^7]: 例えば、AI関連チップのエッチングに必要な機械は（他の多くの企業の試みにもかかわらず）1社のASMLのみが製造し、関連チップの大部分は（他社が競争を試みているにもかかわらず）1社のTSMCが製造し、それらのチップからのハードウェア設計と構築はNVIDIA、AMD、Googleを含むわずか数社が行っている。

[^8]: 最も重要なことに、各チップは物事に「署名」するために使用できる固有でアクセス不可能な暗号化秘密鍵を保持している。

[^9]: デフォルトでは、これはチップを販売する企業だが、他のモデルも可能で潜在的に有用である。

[^10]: ガバナーは署名されたメッセージの交換のタイミングによってチップの位置を確認できる：光の有限速度により、署名されたメッセージを*r* / *c*未満の時間で返すことができる場合、チップは「ステーション」の所与の半径*r*内になければならない（*c*は光速）。複数のステーションとネットワーク特性のある程度の理解を使って、チップの位置を決定できる。この方法の美しさは、そのセキュリティの大部分が物理法則によって供給されることだ。他の方法はGPS、慣性追跡、類似技術を使用できる。

[^11]: または、チップのペアはガバナーの明示的な許可によってのみ相互通信を許可される。

[^12]: これは、少なくとも現在、大規模AIモデルをその上で訓練するためにチップ間の非常に高い帯域幅接続が必要なため重要である。

[^13]: これは*M*の異なるガバナーの*N*からの署名されたメッセージを要求するよう設定することもでき、複数の当事者にガバナンスを共有させることができる。

[^14]: これは前例がないわけではない――例えば軍隊は、これがおそらく技術的に可能であるにもかかわらず、クローンまたは遺伝子工学的超兵士の軍隊を開発していない。しかし、他者に阻止されるのではなく、これを行わないことを*選択*している。主要世界大国が強く開発を望む技術の開発を阻止される実績はあまり良くない。

[^15]: いくつかの注目すべき例外（特にNVIDIA）を除いて、AI専用ハードウェアは、これらの企業の全体的なビジネスと収益モデルの比較的小さな部分である。さらに、高度なAIで使用されるハードウェアと「消費者グレード」ハードウェアの間のギャップは大きく、コンピューターハードウェアの大多数の消費者は大部分影響を受けない。

[^16]: より詳細な分析については、[RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html)と[CNAS](https://www.cnas.org/publications/reports/secure-governable-chips)の最近の報告書を参照。これらは技術的実現可能性、特に他国の高性能計算能力を制約しようとする米国の輸出管理の文脈に焦点を当てている。しかし、これはここで想定されているグローバル制約と明らかに重複している。

[^17]: 例えば、Appleデバイスは紛失または盗難として報告されたとき遠隔で安全にロックされ、遠隔で再アクティブ化できる。これはここで議論されている同じハードウェアセキュリティ機能に依存している。

[^18]: 例えば、IBMの[容量オンデマンド](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand)オファリング、Intelの[Intel on demand](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html)、Appleの[プライベートクラウドコンピュート](https://security.apple.com/blog/private-cloud-compute/)を参照。

[^19]: [この研究](https://epochai.org/trends#hardware-trends-section)は、歴史的に同じ性能が年間約30%少ないドルで達成されてきたことを示している。この傾向が続けば、AIと「消費者」チップ使用の間にかなりの重複がある可能性があり、一般的に高性能AIシステムに必要なハードウェア量は不快なほど小さくなる可能性がある。

[^20]: [同じ研究](https://epochai.org/trends#hardware-trends-section)によると、画像認識における所与の性能は毎年2.5倍少ない計算を必要としている。これが最も能力の高いAIシステムにも当てはまるなら、計算制限は長い間有用ではないだろう。

[^21]: 特に、国レベルでは、これは政府が計算能力の使用方法について多くの制御を持つという意味で、計算の国有化のように見える。しかし、政府の関与を懸念する人々にとって、これは最も強力なAIソフトウェア*自体*が主要AI企業と各国政府間の何らかの合併を通じて国有化されることよりもはるかに安全で望ましいように思える。一部の人々が提唱し始めているように。

[^22]: ヨーロッパでの主要な規制ステップは、2024年の[EU AI法](https://artificialintelligenceact.eu/)の成立であった。これはリスクによってAIを分類している：受け入れ難いシステムを禁止し、高リスクなものを規制し、低リスクシステムには透明性ルールまたは措置なしを課している。これはいくつかのAIリスクを大幅に削減し、米国企業に対してもAIの透明性を向上させるが、2つの主要な欠陥がある。第一に、限定的な適用範囲：EUでAIを提供するあらゆる企業に適用されるが、米国ベース企業への執行は弱く、軍事AIは免除されている。第二に、GPAIをカバーしているが、AGIまたは超知能を受け入れ難いリスクとして認識したり、その開発を防ぐことに失敗している――その EU展開のみ。結果として、AGIまたは超知能のリスクを抑制することはほとんどない。

[^23]: 企業は合理的な規制に賛成していると表明することが多い。しかし、どういうわけか彼らはほぼ常にあらゆる*特定の*規制に反対するようだ。非常に軽いタッチのSB1047をめぐる戦いを見よ。[大多数のAI企業が公然とまたは私的に反対した](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/)。

[^24]: EU AI法が提案されてから発効するまで約3年半かかった。

[^25]: AIの規制を始めるのは「まだ早すぎる」と表明されることがある。最後の注を考えると、それはほぼあり得ないようだ。表明されるもう一つの懸念は、規制が「イノベーションを害する」というものだ。しかし、良い規制は方向を変えるだけで、イノベーションの量は変わらない。

[^26]: 興味深い前例は、漏出して損害を引き起こす可能性のある有害物質の輸送にある。ここで、[規制](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442)と[判例法](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf)が爆発物、ガソリン、毒物、感染性物質、放射性廃棄物のような非常に危険な物質に対して厳格責任を確立している。他の例には[医薬品の警告](https://www.medicalnewstoday.com/articles/boxed-warnings)、[医療機器のクラス](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification)などが含まれる。

[^27]: 類似の目的を持つ別の包括的提案である["A Narrow Path"](https://www.narrowpath.co/)では、段階的制限ではなく明確な分類的禁止を伴い、すべてのフロンティアAI開発を強力な国際機関によって監督される単一の国際団体を通じて誘導する、より中央集権的で禁止ベースのアプローチを提唱している。私もその計画を支持するが、ここで提案されたものよりもさらに多くの政治的意志と協調を要する。

[^28]: そのような標準のいくつかのガイドラインが[Frontier Model Forum](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/)によって公表された。ここでの提案に対して、それらは精度の低さと集計に含まれる計算の少なさの側に偏っている。

[^29]: 2023年の米国AI大統領令（現在撤回）は類似しているがより粗い粒度の報告を要求していた。これは代替命令によって強化されるべきである。

[^30]: 非常に大まかに言えば、現在一般的なH100チップの場合、これは推論を行う約1000のクラスターに相当する。最新の最高級NVIDIA B200チップ（約100台、約500万米ドル相当）が推論を行う場合である。両方の場合、訓練数値はそのクラスターが数ヶ月計算することに相当する。

[^31]: この量は現在訓練されているどのAIシステムよりも大きい。AI能力が計算とどのようにスケールするかをより良く理解するにつれて、より大きいまたはより小さい数値が正当化される可能性がある。

[^32]: これはモデルを作成・提供/ホストする人々に適用され、エンドユーザーには適用されない。

[^33]: 大まかに言えば、「厳格」責任は開発者が製品によって引き起こされた害について*デフォルトで*責任を負うことを意味し、「異常に危険」な製品に使用される基準である（やや面白いことに適切だが、野生動物にも）。「連帯」責任は、製品に責任のあるすべての当事者に責任が割り当てられ、これらの当事者は誰が何の責任を負うかを互いの間で整理しなければならないことを意味する。これは複雑で長い価値連鎖を持つAIのようなシステムにとって重要である。

[^34]: 標準的な過失ベースの単一当事者責任では不十分である：AIシステムは複雑で、その動作は理解されておらず、危険なシステムまたは出力の作成に多くの当事者が関与する可能性があるため、過失の追跡と割り当ての両方が困難になる。さらに、訴訟は判決に何年もかかり、おそらく単にこれらの企業には取るに足らない罰金をもたらすだけなので、幹部への個人責任も重要である。

[^35]: オープンウェイトモデルに安全基準からの免除があってはならない。さらに、リスクを評価する際、除去可能なガードレールは広く利用可能なモデルから除去され、クローズドモデルでさえ非常に高い保証がない限り拡散すると想定すべきである。

[^36]: ここで提案されたスキームは一般的能力で規制精査をトリガーしている。しかし、いくつかの特にリスクの高い使用例がより多くの精査をトリガーすることは理にかなっている――例えば、狭く受動的であっても専門ウイルス学AIシステムはおそらくより高い階層に入るべきである。元米国大統領令は生物学的能力についてこの構造の一部を持っていた。

[^37]: 2つの明確な例は、FAA と FDA、および他国の類似機関によって規制される航空と医薬品である。これらの機関は不完全だが、これらの産業の機能と成功にとって絶対に重要であった。