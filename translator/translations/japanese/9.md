# 第9章 未来を設計する — 代わりに何をすべきか

AIは世界に素晴らしい恩恵をもたらすことができます。リスクなしにすべての利益を得るためには、AIが確実に人間のツールであり続けるようにしなければなりません。

もし機械による人類の置き換えを選択しないことに成功したなら — 少なくとも当面は！ — 代わりに何ができるでしょうか？技術としてのAIの巨大な可能性を諦めることになるのでしょうか？ある意味で答えは単純な*いいえ*です：制御不能なAGIと超知能へのゲートを閉じるが、他の多くの形態のAIと、それらを管理するために必要なガバナンス構造や制度は構築する*のです*。

しかし、語るべきことは依然として多くあります。これを実現することは人類の中心的な仕事となるでしょう。本節ではいくつかの重要なテーマを探ります：

- 「ツール」AIをどのように特徴づけ、どのような形態を取ることができるか。
- ツールAIによって、AGIなしに人類が望む（ほぼ）すべてを得られること。
- ツールAIシステムは（おそらく、原理的には）管理可能であること。
- AGIから方向転換することは国家安全保障において妥協することを意味しない — むしろその逆であること。
- 権力の集中は現実的な懸念であること。安全性とセキュリティを損なうことなくそれを軽減できるか？
- 新しいガバナンスと社会構造が欲しく — そして必要になり — AIが実際にそれを支援できること。

## ゲート内部のAI：ツールAI

三重交差図は、「ツールAI」と呼べるものを描写する良い方法を提供します：制御不能な競合相手や代替物ではなく、人間の使用のための制御可能なツールであるAI。最も問題の少ないAIシステムは、自律的だが汎用的でも超能力的でもないもの（オークション入札ボットのような）、または汎用的だが自律的でも能力的でもないもの（小さな言語モデルのような）、または能力的だが狭く非常に制御可能なもの（AlphaGoのような）です。[^1] 二つの交差する特徴を持つものはより広い応用を持ちますが、より高いリスクがあり、管理に大きな努力が必要になります。（AIシステムがよりツール的であることは、それが本質的に安全であることを意味するのではなく、単に本質的に*危険でない*ことを意味します — チェーンソーとペットのトラを比べてみてください。）ゲートは三重交差での（完全な）AGIと超知能に対して閉じられたままでなければならず、その閾値に近づくAIシステムには細心の注意を払わなければなりません。

しかし、これでも多くの強力なAIが残されます！賢く汎用的な受動的「オラクル」や狭いシステム、超人的ではなく人間レベルの汎用システムなどから巨大な有用性を得ることができます。多くのテック企業や開発者がこのような種類のツールを積極的に構築しており、続けるべきです。多くの人々と同様、彼らはAGIと超知能へのゲートが閉じられることを暗黙的に*想定している*のです。[^2]

また、AIシステムは、人間の監視を維持しながら能力を向上させる複合システムに効果的に組み合わせることができます。不可解なブラックボックスに依存するのではなく、複数のコンポーネント — AIと従来のソフトウェアの両方を含む — が人間が監視し理解できる方法で連携するシステムを構築できます。[^3] 一部のコンポーネントはブラックボックスかもしれませんが、どれもAGIには近くないでしょう — 複合システム全体のみが高度に汎用的で高度に能力的であり、それも厳密に制御可能な方法でです。[^4]

### 意味のある保証された人間の制御

「厳密に制御可能」とは何を意味するのでしょうか？「ツール」フレームワークの重要なアイデアは、かなり汎用的で強力であっても、意味のある人間の制御下にあることが保証されたシステムを可能にすることです。これは何を意味するでしょうか？二つの側面があります。第一は設計上の考慮事項です：人間は、重要な決定をAIに委ねることなく、システムが行っていることに深く中心的に関与すべきです。これが現在のほとんどのAIシステムの特徴です。第二に、AIシステムが自律的である程度において、それらの行動範囲を制限する保証がなければなりません。保証とは何かが起こる確率を特徴づける*数値*と、その数値を信じる理由であるべきです。これは「故障間平均時間」や予想される事故数などの数値が計算され、支持され、安全ケースで公表される他の安全重要分野で我々が要求するものです。[^5] 故障の理想的な数値はもちろんゼロです。そして良いニュースは、プログラム（AIを含む）の*形式的に検証された*特性のアイデアを使用して、全く異なるAIアーキテクチャを使用してかなり近づくかもしれないということです。Omohundro、Tegmark、Bengio、Dalrymple、その他（[ここ](https://arxiv.org/abs/2309.01933)と[ここ](https://arxiv.org/abs/2405.06624)を参照）によって詳細に探求されているアイデアは、特定の特性（例：人間がそれをシャットダウンできる）を持つプログラムを構築し、それらの特性が成り立つことを形式的に*証明する*ことです。これは現在、かなり短いプログラムと単純な特性については実行できますが、AI支援の証明ソフトウェアの（来るべき）力は、はるかに複雑なプログラム（例：ラッパー）やAI自体に対してもそれを可能にするかもしれません。これは非常に野心的なプログラムですが、ゲートへの圧力が高まるにつれて、それらを補強する強力な材料が必要になります。数学的証明は十分に強いもののひとつかもしれません。

### AI業界の行方

AI進歩が方向転換されても、ツールAIは依然として巨大な産業であるでしょう。ハードウェアの面では、超知能を防ぐための計算資源キャップがあっても、より小さなモデルでの訓練と推論には依然として大量の特殊コンポーネントが必要です。ソフトウェア面では、AIモデルと計算サイズの爆発を無力化することで、企業は単にそれらを大きくするのではなく、より小さなシステムをより良く、より多様で、より専門化することにリソースを向け直すべきです。[^6] 金儲けのシリコンバレースタートアップにとって十分な — おそらくもっと多くの — 余地があるでしょう。[^7]

## ツールAIはAGIなしに人類が望む（ほぼ）すべてを産出できる

知能は、生物学的であれ機械的であれ、一連の目標により合致した未来をもたらす活動を計画し実行する能力として広く考えることができます。そのため、賢明に選択された目標の追求に使用される場合、知能は非常に有益です。人工知能が時間と努力の巨大な投資を引き付けているのは、その約束された利益のためです。そこで問うべきです：超知能への暴走を抑制した場合、AIの利益をどの程度まだ得られるでしょうか？答え：驚くほど少ししか失わないかもしれません。

まず、現在のAIシステムはすでに非常に強力であり、それらでできることの表面をかろうじて削っただけであることを考えてください。[^8] それらは、提示された質問やタスクを「理解」し、その質問に答えたりそのタスクを実行したりするために何が必要かという点で、「ショーを運営する」ことが合理的に可能です。

次に、現代のAIシステムに関する興奮の多くはそれらの汎用性によるものですが、最も能力の高いAIシステムの一部 — 音声や画像を生成・認識し、科学的予測とモデリングを行い、ゲームをプレイするなど — ははるかに狭く、計算の面で十分に「ゲート内」にあります。[^9] これらのシステムは、それらが行う特定のタスクにおいて超人的です。それらの狭さのために、エッジケース[^10]（または[悪用可能](https://arxiv.org/abs/2211.00241)）な弱点があるかもしれません。しかし、*完全に*狭いか*完全に*汎用的かが利用可能な唯一の選択肢ではありません：その間には多くのアーキテクチャがあります。[^11]

これらのAIツールは、AGIなしに他の肯定的技術の進歩を大幅に加速できます。より良い核物理学を行うために、AIが核物理学者である必要はありません — 我々にはそれらがいるのです！医学を加速したいなら、生物学者、医学研究者、化学者に強力なツールを与えましょう。彼らはそれらを望んでおり、巨大な利益のために使用するでしょう。サーバーファームにいる百万人のデジタル天才は必要ありません。我々には、AIが才能を引き出すのを助けることができる何百万人もの人間がいます。はい、不死や全ての病気の治療法を得るのに時間がかかるでしょう。これは現実的なコストです。しかし、最も有望な健康革新でさえ、AI駆動の不安定が世界的紛争や社会崩壊につながった場合には、ほとんど役に立たないでしょう。ゲート内ツールを使用するAI支援の人間に問題への取り組みを最初に試してもらうのは、我々自身に対する義務です。

そして、実際にゲート内ツールでは得られないAGIの巨大な利点があると仮定しましょう。AGIと超知能を*決して*構築しないことでそれらを失うのでしょうか？ここでリスクと報酬を比較検討する際、急ぐのと待つのには非常に非対称的な利益があります：保証された安全で有益な方法でそれが実行できるまで待つことができ、ほぼ全員が報酬を享受することができます。急げば、OpenAIのCEOサム・アルトマンの言葉で、[*全員*にとって電気が消える](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)ことになりかねません。

しかし、非AGIツールが潜在的にそれほど強力なら、それらを管理できるでしょうか？答えは明確な...たぶんです。

## ツールAIシステムは（おそらく、原理的には）管理可能である

しかし、それは簡単ではないでしょう。現在の最先端のAIシステムは、人々や機関が目標を達成することを大幅に支援できます。これは一般的には良いことです！しかし、そのようなシステムを — 社会が適応する時間をあまり持たずに突然 — 自由に使えるようになることの自然な動態には、ゲートクローズを想定して管理される必要がある深刻なリスクを提供します。そのようなリスクのいくつかの主要なクラスと、それらがどのように減じられるかについて議論する価値があります。

一つのリスクのクラスは、高性能ツールAIが以前は人や組織に結び付いていた知識や能力へのアクセスを可能にし、高い能力と高い忠誠の組み合わせを非常に広範囲の行為者に利用可能にすることです。今日、悪意を持つ人が十分なお金があれば、化学者のチームを雇って新しい化学兵器を設計・製造させることができるでしょう — しかし、そのお金を持つことやチーム見つけて集め、明らかに違法で非倫理的で危険なことを行うよう説得することはそれほど簡単ではありません。AIシステムがそのような役割を果たすことを防ぐために、現在の方法の改善で十分かもしれません。[^12] ただし、それらのシステムすべてとそれらへのアクセスが責任を持って管理される限り。一方、強力なシステムが一般使用と修正のためにリリースされた場合、組み込まれた安全対策は除去可能である可能性があります。そのため、このクラスのリスクを回避するために、一般に公開できるものに対する強い制限 — 核、爆発物、その他の危険技術の詳細に対する制限に類似した — が必要になります。[^13]

第二のリスクのクラスは、人のように行動したり人になりすましたりする機械のスケールアップから生じます。個人への害のレベルでは、これらのリスクには、はるかに効果的な詐欺、スパム、フィッシング、非合意ディープフェイクの拡散が含まれます。[^14] 集合的レベルでは、公的議論と討論、我々の社会の情報と知識の収集、処理、普及システム、政治的選択システムなどの核となる社会プロセスの破綻が含まれます。このリスクを軽減するには、おそらく（a）AIシステムによる人の偽装を制限し、そのような偽装を生成するシステムを作成するAI開発者に責任を負わせる法律、（b）AI生成コンテンツを（責任を持って）識別し分類する透かしと出所システム、（c）データ（カメラや録音など）から事実、理解、良い世界モデルまでの信頼できる連鎖を作成できる新しい社会技術認識論システムが関与するでしょう。[^15] これらすべては可能であり、AIがその一部を支援できます。

第三の一般的リスクは、いくつかのタスクが自動化される程度において、現在それらのタスクを行っている人間の労働としての経済的価値が低下する可能性があることです。歴史的に、タスクの自動化はそれらのタスクによって可能になることをより安価で豊富にする一方で、以前にそれらのタスクを行っていた人々を自動化版に依然として関与している人々（一般的にはより高いスキル/給与）と、労働価値が低いか小さい人々に仕分けしてきました。全体として、より多いかより少ない人間の労働が結果として生じるより大きくより効率的な分野で必要になるかをどの分野で予測するのは困難です。並行して、自動化の動態は不平等と一般的な生産性を増加させ、特定の商品とサービスのコスト（効率向上による）を減少させ、他のもののコスト（[費用病](https://en.wikipedia.org/wiki/Baumol_effect)による）を増加させる傾向があります。不平等増加の不利な側にいる人々にとって、それらの特定の商品とサービスのコスト減少が他のもののの増加を上回り、全体的により大きな幸福につながるかは非常に不明確です。では、AIではこれはどうなるでしょうか？人間の知的労働が汎用AIによって置き換えられる相対的な容易さのため、人間競争力のある汎用AIで、これの急速版を予想できます。[^16] AGIへのゲートを閉じた場合、AIエージェントによって丸ごと置き換えられる仕事ははるかに少なくなるでしょう。しかし、巨大な労働移転は数年の期間で依然として起こりそうです。[^17] 広範囲な経済的苦痛を避けるために、おそらくある種のユニバーサル基本資産や所得と、自動化がより困難な人間中心労働の価値と報酬を高める文化的シフトの工学（経済の他の部分から押し出された労働の増加により労働価格が下がることを見るのではなく）の両方を実施することが必要になるでしょう。「[データ尊厳](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society)」のような他の構造（AIでのそのデータによって作成された価値に対してトレーニングデータの人間の生産者が自動的にロイヤリティを与えられる）は助けになるかもしれません。AIによる自動化には、*不適切*な自動化という第二の潜在的な悪影響もあります。AIが単により悪い仕事をする応用とともに、これにはAIシステムが道徳的、倫理的、または法的戒律に違反する可能性が高いもの — 例えば生死の決定や司法問題において — が含まれるでしょう。これらは現在の法的枠組みを適用し拡張することで扱わなければなりません。

最後に、ゲート内AIの重要な脅威は、個人化された説得、注意捕獲、操作での使用です。ソーシャルメディアと他のオンラインプラットフォームで、深く根ざした注意経済（オンラインサービスがユーザーの注意を激しく争う）と[「監視資本主義」](https://en.wikipedia.org/wiki/The_Age_of_surveillance_Capitalism)システム（ユーザー情報とプロファイリングが注意の商品化に追加される）の成長を見てきました。より多くのAIが両方の奉仕に投入されることはほぼ確実です。AIは既に中毒性のフィードアルゴリズムで重く使用されていますが、これは単一の人によって強迫的に消費されるようカスタマイズされた中毒性のAI生成コンテンツに進化するでしょう。そしてその人の入力、応答、データは、悪循環を続けるために注意/広告マシンに送り込まれるでしょう。また、テック企業によって提供されるAIヘルパーがより多くのオンライン生活のインターフェイスになるにつれ、それらはおそらく顧客の説得と収益化が発生するメカニズムとして検索エンジンとフィードを置き換えるでしょう。これまでのこれらの動態を制御することにおける我々の社会の失敗は良い前兆ではありません。この動態の一部は、プライバシー、データ権、操作に関する規制を通じて軽減されるかもしれません。問題の根にもっと到達することは、忠実なAIアシスタント（以下で議論）のような異なる視点を必要とするかもしれません。

この議論の要点は希望のそれです：ゲート内ツールベースシステム — 少なくとも今日の最先端システムと同等の力と能力にとどまる限り — は、そうする意志と協調があればおそらく管理可能です。AIツールによって力を与えられたまともな人間の制度は[^18]、それを実行できます。我々はそれを実行することに失敗することもできます。しかし、より強力なシステムを許可することがどのように助けになるかを見るのは困難です — それらを担当に据え、最善を期待すること以外では。

## 国家安全保障

AI覇権のためのレース — 国家安全保障またはその他の動機によって駆動される — は我々を、権力を授与するのではなく吸収する傾向がある制御されない強力なAIシステムに向かわせます。米国と中国の間のAGIレースは、どちらの国が最初に超知能を得るかを決定するレースです。

では、国家安全保障を担当する人々は代わりに何をすべきでしょうか？政府は制御可能で安全なシステムを構築することに強い経験があり、規模で、政府の権威で最もよく成功するような種類のインフラプロジェクトを支援して、AIでそうすることを倍化すべきです。

AGIに向けた無謀な「マンハッタンプロジェクト」[^19]の代わりに、米国政府は制御可能で、安全で、信頼できるシステムのためのアポロプロジェクトを開始できるでしょう。これには例えば以下が含まれる可能性があります：

- 強力なAIの計算面を管理するために（a）オンチップハードウェアセキュリティメカニズムと（b）インフラを開発する主要プログラム。これらは米国の[CHIPS法](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local)と[輸出管理レジーム](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion)を基盤にすることができるでしょう。
- AIシステムの特定の機能（オフスイッチのような）が存在するか存在しないかを*証明*できるような形式検証技術を開発する大規模イニシアチブ。これは証明の特性を開発するためにAI自体を活用できます。
- 検証可能に安全なソフトウェアを作成する国家規模の取り組みで、既存のソフトウェアを検証可能に安全なフレームワークに再コード化できるAIツールを動力とするもの。
- DOE、NSF、NIHの間のパートナーシップとして実行される、AIを使用した科学的進歩への国家投資プロジェクト。[^20]

一般的に、我々をAIとその誤用からのリスクに脆弱にする我々の社会への巨大な攻撃面があります。これらのリスクの一部から保護することは、政府規模の投資と標準化を必要とするでしょう。これらは、AGIへのレースの炎にガソリンを注ぐよりもはるかに多くの安全性を提供するでしょう。そして、AIが兵器や指揮統制システムに組み込まれるなら、AIが信頼できて安全であることが重要であり、現在のAIは単にそうではありません。

## 権力集中とその軽減

この論文は、AIの人間制御とその潜在的失敗のアイデアに焦点を当ててきました。しかし、AI状況を見る別の有効なレンズは*権力の集中*を通したものです。非常に強力なAIの開発は、それを開発し制御する非常に少数で非常に大きな企業の手に、または自身の権力と制御を維持する新しい手段としてAIを使用する政府に、またはAIシステム自体に権力を集中させる恐れがあります。あるいは上記の不浄な混合に。これらのいずれの場合でも、人類の大部分が権力、制御、主体性を失います。これにどのように対抗できるでしょうか？

最初で最も重要なステップは、もちろん、人間より賢いAGIと超知能へのゲートクローズです。これらは明示的に人間と人間のグループを直接置き換えることができます。それらが企業や政府の制御下にあれば、それらの企業や政府に権力を集中させるでしょう。それらが「自由」であれば、自分自身に権力を集中させるでしょう。そこで、ゲートが閉じられていると仮定しましょう。では何を？

権力集中に対する一つの提案された解決策は「オープンソース」AIで、モデルの重みが自由にまたは広く利用可能なものです。しかし、先ほど述べたように、モデルがオープンになれば、ほとんどの安全対策やガードレールは取り除かれ得る（そして一般的に取り除かれます）。そのため、一方で分散化と、他方で安全性、セキュリティ、AIシステムの人間制御との間に急激な緊張があります。オープンモデルがオペレーティングシステム（オープンな代替があるにもかかわらず、依然としてMicrosoft、Apple、Googleが支配）以上に、それ自体でAIにおける権力集中に意味のある対抗をするだろうと懐疑的になる理由もあります。[^21]

しかし、この円を二乗する方法があるかもしれません — リスクを中央化し軽減しながら、能力と経済的報酬を分散化する。これはAIがどのように開発されるかと、その利益がどのように分配されるかの両方を再考することを必要とします。

公的AI開発と所有権の新しいモデルが助けになるでしょう。これはいくつかの形態を取ることができます：政府開発AI（民主的監視の対象）、[^22] 非営利AI開発組織（ブラウザ用のMozillaのような）、または非常に広範囲な所有権とガバナンスを可能にする構造。重要なのは、これらの制度が強い安全制約の下で運営されながら公共の利益に奉仕することを明示的に認可されることです。[^23] よく作られた規制と標準/認証制度も極めて重要で、活気のある市場によって提供されるAI製品がユーザーに対して搾取的ではなく真に有用であり続けるようにします。

経済的権力集中に関しては、出所追跡と「データ尊厳」を使用して、経済的利益がより広く流れるよう確保できます。特に、現在のほとんどのAI力（そして我々がゲートを閉じ続ければ将来も）は、直接トレーニングデータか人間フィードバックかにかかわらず、人間生成データから生じます。AI企業がデータ提供者に公正に補償することを要求されれば、[^24]これは少なくとも経済的報酬をより広く分配することに役立つかもしれません。これを超えて、別のモデルは大きなAI企業の重要な部分の公的所有権かもしれません。例えば、AI企業に課税できる政府は、収入の一部を企業の株式を保有する国富ファンドに投資し、住民に配当を支払うことができるでしょう。[^25]

これらのメカニズムで重要なのは、非AIの手段を使ってAI駆動の権力集中と単に戦うのではなく、AI自体の力を使って権力をよりよく分配することを支援することです。一つの強力なアプローチは、ユーザーに対する真の受託義務 — ユーザーの利益を企業提供者の利益より優先する — で運営するよく設計されたAIアシスタントを通じたものでしょう。[^26]これらのアシスタントは本当に信頼でき、技術的に有能でありながら使用事例とリスクレベルに基づいて適切に制限され、公的、非営利、または認定された営利チャネルを通じてすべてに広く利用可能でなければなりません。密かに他の当事者のために我々の利益に反して働く人間のアシスタントを決して受け入れないように、企業利益のためにユーザーを監視し、操作し、価値を抽出するAIアシスタントを受け入れるべきではありません。

そのような変革は、個人が企業利益よりも価値抽出を優先する広大な（AI支援）企業・官僚機械と単独で交渉を強いられる現在の動態を根本的に変えるでしょう。AI駆動の権力をより広く再分配する多くの可能なアプローチがありますが、どれもデフォルトでは現れません：それらは受託要件、公的提供、リスクに基づく階層アクセスなどのメカニズムで意図的に工学され統治されなければなりません。

権力集中を軽減するアプローチは、既存権力からの重要な逆風に直面する可能性があります。[^27]しかし、安全性と集中権力のどちらかを選ぶことを必要としないAI開発への道筋があります。今適切な制度を構築することで、AIのリスクが注意深く管理されながら、その利益が広く共有されることを確保できるでしょう。

## 新しいガバナンスと社会構造

我々の現在のガバナンス構造は苦戦しています：反応が遅く、しばしば特別利益によって掌握され、[公衆による信頼がますます低下しています。](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx)しかし、これはそれらを放棄する理由ではありません — むしろその逆です。一部の制度は置き換えが必要かもしれませんが、より広くは我々の既存構造を向上させ補完し、急速に進化する我々の世界でより良く機能するのを支援できる新しいメカニズムが必要です。

我々の制度的弱さの多くは、正式な政府構造からではなく、劣化した社会制度から生じています：共有された理解を発達させ、行動を調整し、意味のある談話を行う我々のシステム。これまで、AIはこの劣化を加速し、我々の情報チャネルを生成コンテンツで氾濫させ、最も偏向的で分裂的なコンテンツに我々を向かわせ、真実と虚構を区別することをより困難にしています。

しかしAIは実際にこれらの社会制度を再構築し強化することを支援できるでしょう。三つの重要な領域を考えてみてください：

第一に、AIは我々の認識論システム — 何が真実かを知る方法 — への信頼を回復することを支援できるでしょう。生データから分析を通じて結論まで、情報の出所を追跡し検証するAI支援システムを開発できるでしょう。これらのシステムは、暗号学的検証と高度な分析を組み合わせて、何かが真実かどうかだけでなく、それが真実だとどのようにして知るかを人々が理解することを支援できるでしょう。[^28]忠実なAIアシスタントは、詳細を追跡してそれらが正しいことを確保することを担当できるでしょう。

第二に、AIは新しい形態の大規模調整を可能にすることができるでしょう。我々の最も差し迫った問題の多く — 気候変動から抗生物質耐性まで — は基本的に調整問題です。我々は、どの個人やグループも最初の動きをする余裕がないため、[ほぼ全員にとってあり得るよりも悪い状況に行き詰まっています](https://equilibriabook.com/)。AIシステムは、複雑なインセンティブ構造をモデリングし、より良い結果への実行可能な道筋を特定し、そこに到達するために必要な信頼構築とコミットメントメカニズムを促進することによって支援できるでしょう。

おそらく最も興味深いのは、AIが全く新しい形態の社会的談話を可能にすることができることです。「都市と話す」[^29]ことを想像してみてください — 統計を見るだけでなく、何百万の住民の見解、経験、ニーズ、願望を処理し合成するAIシステムと意味のある対話を持つこと。または、AIが現在お互いのことを話し違えているグループの間で、お互いの戯画ではなく相手の実際の関心と価値をより良く理解することを各側が支援することによって真の対話をどのように促進できるかを考えてみてください。[^30]またはAIが人々や大きなグループの人々の間の論争の熟練した、信頼できる中立的な仲裁を提供することができるかもしれません（それらすべてが直接個別にそれと相互作用できます！）現在のAIはこの仕事を行うことを完全に可能にしますが、そうするツールは市場インセンティブによって、またはそれ自体で存在するようになることはありません。

これらの可能性は、特に談話と信頼を劣化させるAIの現在の役割を考えると、ユートピア的に聞こえるかもしれません。しかし、だからこそ我々はこれらの肯定的な応用を積極的に開発しなければならないのです。制御不能なAGIへのゲートを閉じ、人間の主体性を向上させるAIを優先することで、AIが力の付与、回復力、集合的進歩の力として機能する未来に向けて技術進歩を導くことができます。


[^1]: そうは言っても、三重交差から離れていることは残念ながら思うほど簡単ではありません。三つの側面のどれか一つで能力を非常に強く押すことは、他の側面でもそれを増加させる傾向があります。特に、極めて汎用的で能力的な知能を作ることは、簡単に自律的に変えられないようにするのが困難かもしれません。一つのアプローチは、計画能力を削がれた[「近視眼的」](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia)システムを訓練することです。もう一つは、行動指向の質問に答えることを避ける純粋な[「オラクル」](https://arxiv.org/abs/1711.05541)システムの工学に焦点を当てることでしょう。

[^2]: 多くの企業は、時間がかかっても、彼らもまた最終的にはAGIによって置き換えられることを理解していません — もしそうなら、彼らもそれらのゲートをもう少し強く押さないかもしれません！

[^3]: AIシステムはより効率的だがより理解しにくい方法でコミュニケーションすることができるでしょうが、人間の理解を維持することが優先されるべきです。

[^4]: このモジュラーで解釈可能なAIのアイデアは、数人の研究者によって詳細に開発されています。例えば、DrexlerによるComprehensive AI Services」](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf)モデル、Dalrymopleその他による[「オープンエージェンシーアーキテクチャ」](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai)を参照。そのようなシステムは、巨大な計算で訓練されたモノリシックなニューラルネットワークよりも多くの工学努力を必要とするかもしれませんが、それはまさに計算制限が役立つところです — より安全で透明な道筋をより実用的なものにもすることによって。

[^5]: 一般的な安全ケースについては[このハンドブック](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16)を参照。特にAIに関しては、[Wasilら](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274)、[Clymerら](https://arxiv.org/abs/2403.10462)、[Buhlら](https://arxiv.org/abs/2410.21572)、[Balesniら](https://arxiv.org/abs/2411.03336)を参照

[^6]: 我々は実際に、推論の高コストによって駆動された、より大きなものから「蒸留」され、より安価なハードウェアで実行可能なより小さくより専門化されたモデルの傾向を既に見ています。

[^7]: AIテック・エコシステムについて興奮している人々が、彼らの産業への煩わしい規制と見なすものに反対する理由は理解できます。しかし、例えばベンチャーキャピタリストが、AGIと超知能への暴走を許可したいと思う理由は率直に言って私には当惑します。それらのシステム（そして企業が会社の制御下にある間は企業）は*すべてのスタートアップをおやつとして食べる*でしょう。おそらく他の産業を食べるより*さらに早く*。繁栄するAIエコシステムに投資している誰もが、AGI開発が少数の支配的プレーヤーによる独占につながらないことを確保することを優先すべきです。

[^8]: 経済学者で元Deepmind研究者のマイケル・ウェッブが[述べた](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/)ように、「もし今日すべてのより大きな言語モデルの開発を停止したら、つまりGPT-4とClaudeとその他何でも、そしてそれらがそのサイズで我々が訓練する最後のものだとしたら — つまりそのサイズのもののはるかに多くの反復とあらゆる種類のファインチューニングを許可するが、それより大きなもの、より大きな進歩はなし — 今日我々が持っているもので、それは20年または30年の信じられない経済成長を動力とするのに十分だと思います。」

[^9]: 例えば、DeepMindのalphafoldシステムはGPT-4のFLOP数のわずか10万分の1しか使用していません。

[^10]: 自動運転車の困難はここで注意することが重要です：名目上は狭いタスクであり、比較的小さなAIシステムでかなりの信頼性で達成可能ですが、そのような安全重要タスクで必要なレベルまで信頼性を得るには、広範囲な現実世界の知識と理解が必要です。

[^11]: 例えば、計算予算が与えられれば、（例えば）その予算の半分で事前訓練されたGPAIモデルと、もう半分がより狭い範囲のタスクで非常に高い能力を訓練するために使用されることを見るでしょう。これは人間近くの一般知能に支えられた超人的な狭い能力を与えるでしょう。

[^12]: 現在の支配的アライメント技術は「人間フィードバックによる強化学習」[(RLHF)](https://arxiv.org/abs/1706.03741)で、AIモデルの強化学習のための報酬/罰信号を作成するために人間フィードバックを使用します。これと[constitutional AI](https://arxiv.org/abs/2212.08073)のような関連技術は驚くほどうまく機能しています（ただし、堅牢性に欠け、適度な努力で回避できます。）加えて、現在の言語モデルは一般的に常識的推論において、愚かな道徳的間違いを犯さない程度に有能です。これはある種のスウィートスポットです：人々が望むもの（それが定義できる程度において）を理解するのに十分賢いが、精巧な欺瞞を計画したりそれを間違えたときに巨大な害を引き起こすほど賢くない。

[^13]: 長期的には、開発されるAIの能力のどのようなレベルも拡散する可能性があります。最終的にそれはソフトウェアで有用だからです。そのようなシステムが引き起こすリスクから守るための堅牢なメカニズムを持つ必要があります。しかし、我々は*今それを持っていない*ので、どれだけの強力なAIモデルの拡散が許可されるかについて非常に慎重でなければなりません。

[^14]: これらの大部分は、未成年者を含む非合意ポルノのディープフェイクです。

[^15]: そのような解決策の多くの要素が存在します。「ボットかそうでないか」法（EU AI法など）、[業界出所追跡技術](https://c2pa.org/)、[革新的ニュースアグリゲーター](https://www.improvethenews.org/)、予測[アグリゲーター](https://metaculus.com/)と市場などの形で。

[^16]: 自動化の波は以前のパターンに従わないかもしれません。比較的*高い*スキルのタスク、質の高い執筆、法律の解釈、医学的アドバイスの提供などが、低スキルタスクと同じか、あるいはより多く自動化に脆弱かもしれません。

[^17]: AGIの賃金への影響の注意深いモデリングについては、[ここ](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek)のレポートと血なまぐさい詳細は[ここ](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0)、Anton Korinekと共同研究者から参照。彼らは、仕事のより多くの部分が自動化されるにつれて、生産性と賃金が上がることを発見しました — ある点まで。*あまりに*多くが自動化されると、生産性は増加し続けますが、人々が効率的なAIによって全面的に置き換えられるため賃金は急落します。これがゲートクローズがとても有用な理由です：消失した人間の賃金なしに生産性を得るのです。

[^18]: AIを「防御的」技術として使用し、構築を支援して保護と管理をより堅牢にする多くの方法があります。この「D/acc」アジェンダを記述するこの[影響力のある投稿](https://vitalik.eth.limo/general/2025/01/05/dacc2.html)を参照。

[^19]: やや皮肉なことに、米国のマンハッタンプロジェクトはAGIへのタイムラインを速めることをほとんどしないでしょう — AI進歩への人的・財政投資のダイアルは既に11でピン留めされています。主要な結果は、中国で類似のプロジェクト（国家レベルのインフラプロジェクトに優秀）を促し、AIのリスクを制限する国際協定をはるかに困難にし、ロシアなどの米国の他の地政学的敵対者を警戒させることでしょう。

[^20]: [「国家AI研究資源」](https://nairrpilot.org/)プログラムは、この方向への良い現在のステップで拡大されるべきです。

[^21]: テック製品における「オープン」の様々な意味と含意の[この分析](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807)と、一部がどのように支配の定着をより少なくではなくより多くもたらしたかを参照。

[^22]: 米国での[国家AI研究資源](https://nairratdoe.ornl.gov/)の計画と最近の[欧州AI財団](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/)の開始は、この方向への興味深いステップです。

[^23]: ここでの課題は技術的ではなく制度的です — 公共利益AI開発がどのようなものかの現実世界の例と実験が緊急に必要です。

[^24]: これは現在のビッグテックのビジネスモデルに反し、法的行動と新しい規範の両方を必要とするでしょう。

[^25]: 一部の政府のみがそうすることができるでしょう。より急進的なアイデアは、[全人類の共同所有の下でのこの種の普遍的基金](https://futureoflife.org/project/the-windfall-trust/)です。

[^26]: この事例の長い解説については、AI忠誠に関する[この論文](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338)を参照。残念ながら、AIアシスタントのデフォルトの軌道は、ますます不忠実なもになる可能性が高いです。

[^27]: やや皮肉なことに、多くの既存権力もAI支援の権力剥奪のリスクにありますが、プロセスがかなり進行するまで、そしてしない限り、彼らがこれを知覚することは困難かもしれません。

[^28]: この方向でのいくつかの興味深い取り組みが暗号学的検証に関する[c2paコアリション](https://c2pa.org/)；より良いニュース認識論に関する[Verity](https://www.improvethenews.org/)と[Ground news](https://ground.news/)；偽証可能な予測に談話を基盤にする[Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com)と予測市場によって代表されています。

[^29]: この[魅力的なパイロットプロジェクト](https://talktothecity.org/)を参照。

[^30]: いくつかの例については[Kialo](https://www.kialo-edu.com/)と[集合知プロジェクト](https://www.cip.org/)の取り組みを参照。