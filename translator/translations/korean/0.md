# 요약

본 에세이의 개괄적 개요. 시간이 부족하다면 10분 만에 모든 핵심 요점을 파악할 수 있다.

지난 10년간(좁은 용도의 AI) 그리고 최근 몇 년간(범용 AI)의 인공지능 분야의 극적인 발전은 AI를 틈새 학술 분야에서 세계 최대 기업들의 핵심 사업 전략으로 변화시켰으며, AI 역량 향상을 위한 기법과 기술에 대한 연간 투자가 수천억 달러에 달하고 있다.

이제 우리는 중요한 기로에 서 있다. 새로운 AI 시스템의 역량이 많은 인지 영역에서 인간의 능력과 대등해지고 이를 능가하기 시작함에 따라, 인류는 결정해야 한다: 얼마나 멀리, 그리고 어떤 방향으로 나아갈 것인가?

AI는 모든 기술과 마찬가지로 창조자를 위한 개선이라는 목표로 시작되었다. 하지만 우리의 현재 궤적과 암묵적 선택은 더욱 강력한 시스템을 향한 통제되지 않은 경쟁이며, 이는 현재 경제 활동과 인간 노동의 광범위한 영역을 자동화하려는 소수 거대 기술 기업들의 경제적 인센티브에 의해 추진되고 있다. 이 경쟁이 더 오래 계속된다면, 불가피한 승자가 있다: 바로 AI 그 자체 – 우리 경제에서, 우리 사고에서, 우리 결정에서, 그리고 결국 우리 문명의 통제권에서 인간보다 빠르고, 더 똑똑하고, 더 저렴한 대안 말이다.

하지만 우리는 다른 선택을 할 수 있다: 정부를 통해 AI 개발 과정을 통제하여 명확한 한계, 넘지 않을 선, 그리고 단순히 하지 않을 일들을 부과할 수 있다 – 핵 기술, 대량살상무기, 우주 무기, 환경 파괴적 과정, 인간의 생물공학적 개조, 그리고 우생학에 대해 해왔듯이. 가장 중요한 것은, AI가 인간을 대체하고 결국 인간을 밀어내는 새로운 종이 아니라, 인간을 강화하는 도구로 남도록 보장할 수 있다는 것이다.

이 에세이는 인간보다 똑똑한 자율적 범용 AI – 때로 "AGI"라고 불리는 – 특히 때로 "초지능"이라고 불리는 고도로 초인간적인 버전으로 가는 "관문"을 차단함으로써 *미래를 인간의 것으로 유지*해야 한다고 주장한다. 대신, 개인을 강화하고 인간 사회가 가장 잘하는 일을 할 수 있는 능력을 혁신적으로 개선할 수 있는 강력하고 신뢰할 수 있는 AI 도구에 집중해야 한다. 이 논증의 구조를 간략히 살펴보자.

## AI는 다르다

AI 시스템은 다른 기술들과 근본적으로 다르다. 전통적인 소프트웨어가 정확한 지시를 따르는 반면, AI 시스템은 명시적으로 방법을 알려주지 않아도 목표를 달성하는 방법을 학습한다. 이것이 AI를 강력하게 만든다: 목표나 성공 지표를 명확하게 정의할 수 있다면, 대부분의 경우 AI 시스템이 이를 달성하는 방법을 학습할 수 있다. 하지만 이것은 또한 AI를 본질적으로 예측 불가능하게 만든다: 목표 달성을 위해 어떤 행동을 취할지 확실하게 결정할 수 없다.

AI는 또한 대부분 설명 불가능하다: 부분적으로는 코드이지만, 주로 해석할 수 없는 거대한 숫자 집합 – 신경망 "가중치"들 – 으로 구성되어 있어 분석할 수 없다. 우리는 생물학적 뇌 내부를 들여다보며 생각을 파악하는 것만큼이나 AI의 내부 작동을 이해하는 데 어려움을 겪고 있다.

디지털 신경망을 훈련시키는 이러한 핵심 방식은 급속도로 복잡해지고 있다. 가장 강력한 AI 시스템들은 특수 하드웨어를 사용하여 거대한 데이터셋에서 신경망을 훈련시키는 대규모 연산 실험을 통해 만들어지며, 이후 소프트웨어 도구와 상부구조로 강화된다.

이로 인해 텍스트와 이미지를 생성하고 처리하며, 수학적·과학적 추론을 수행하고, 정보를 취합하고, 방대한 인간 지식 저장소를 상호작용적으로 조회하는 매우 강력한 도구들이 만들어졌다.

불행히도, 더 강력하고 신뢰할 수 있는 기술 도구의 개발이 우리가 *해야 할* 일이고 거의 모든 사람이 원하고 원한다고 말하는 것이지만, 이것은 우리가 실제로 향하고 있는 궤적이 아니다.

## AGI와 초지능

분야의 시작부터 AI 연구는 다른 목표에 집중해왔다: 인공일반지능. 이러한 집중은 이제 AI 개발을 주도하는 거대 기업들의 초점이 되었다.

AGI란 무엇인가? 흔히 "인간 수준의 AI"로 막연하게 정의되지만, 이는 문제가 있다: 어떤 인간들이며, 어떤 역량에서 인간 수준인가? 그리고 이미 가지고 있는 초인간적 역량은 어떻게 할 것인가? AGI를 이해하는 더 유용한 방법은 세 가지 핵심 속성의 교집합을 통해서다: 높은 **자율성**(행동의 독립성), 높은 **범용성**(광범위한 범위와 적응성), 그리고 높은 **지능**(인지 과업에서의 역량). 현재 AI 시스템은 고도로 유능하지만 좁거나, 범용적이지만 지속적인 인간 감독이 필요하거나, 자율적이지만 범위가 제한적일 수 있다.

완전한 A-G-I는 이 세 가지 속성을 모두 최고 인간 역량과 동등하거나 이를 능가하는 수준으로 결합할 것이다. 중요한 것은, 인간을 그토록 효과적이고 현재 소프트웨어와 다르게 만드는 것이 바로 이 조합이라는 것이다. 이것은 또한 사람들이 디지털 시스템으로 완전히 대체될 수 있게 하는 것이기도 하다.

인간의 지능이 특별하긴 하지만, 결코 한계는 아니다. 인공 "초지능" 시스템은 수백 배 더 빠르게 작동하고, 훨씬 더 많은 데이터를 분석하며, 엄청난 양을 동시에 "마음에" 담아둘 수 있고, 인간 집단보다 훨씬 크고 효과적인 집합체를 형성할 수 있다. 이들은 개인이 아니라 기업, 국가, 또는 우리 문명 전체를 대체할 수 있다.

## 우리는 문턱에 서 있다

AGI가 *가능하다*는 강력한 과학적 합의가 있다. AI는 이미 최근의 고수준 추론과 문제 해결을 포함하여 지적 능력의 많은 일반적 테스트에서 인간 성능을 능가하고 있다. 뒤처진 역량들 – 지속적 학습, 계획, 자기 인식, 독창성 등 – 모두 현재 AI 시스템에서 어느 정도 존재하며, 이 모든 것을 개선할 가능성이 높은 알려진 기법들이 존재한다.

몇 년 전까지만 해도 많은 연구자들이 AGI를 수십 년 앞의 일로 보았지만, 현재 AGI의 짧은 시간표에 대한 증거는 강력하다:

- 경험적으로 검증된 "스케일링 법칙"이 연산 투입과 AI 역량을 연결하며, 기업들은 향후 몇 년간 연산 투입을 몇 자릿수씩 확장할 궤도에 있다. AI 발전에 투입되는 인적·재정적 자원은 이제 십여 개의 맨해튼 프로젝트와 여러 아폴로 프로젝트에 맞먹는다.
- AI 기업들과 그 리더들은 AGI(어떤 정의에 의한)가 몇 년 내에 달성 가능하다고 공개적으로나 사적으로 믿고 있다. 이 기업들은 대중이 모르는 정보를 가지고 있으며, 일부는 차세대 AI 시스템을 손에 쥐고 있다.
- 입증된 실적을 가진 전문 예측가들은 AGI(어떤 정의에 의한)가 1-2년 내에 도착할 확률을 25%, 2-5년 내에 도착할 확률을 50%로 평가한다(Metaculus의 ['약한'](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) 및 ['완전한'](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/) AGI 예측 참조).
- 자율성(장기적 유연한 계획 포함)이 AI 시스템에서 뒤처져 있지만, 주요 기업들은 이제 자율 AI 시스템 개발에 방대한 자원을 집중하고 있으며, 2025년을 비공식적으로 ["에이전트의 해"](https://techinformed.com/2025-informed-the-year-of-agentic-ai/)라고 명명했다.
- AI가 자체 개선에 점점 더 기여하고 있다. AI 시스템이 AI 연구를 하는 인간 AI 연구자만큼 유능해지면, 훨씬 더 강력한 AI 시스템으로의 빠른 진보를 위한 임계점이 도달될 것이며, 이는 AI 역량의 폭주로 이어질 가능성이 높다. (논쟁의 여지가 있지만, 그 폭주는 이미 시작되었다.)

인간보다 똑똑한 AGI가 수십 년 또는 그 이상 앞의 일이라는 생각은 더 이상 해당 분야 전문가들의 압도적 다수에게 유지 가능하지 않다. 현재의 논쟁은 이 과정을 계속한다면 몇 개월 또는 몇 년이 걸릴지에 관한 것이다. 우리가 직면한 핵심 질문은: 그렇게 해야 하는가?

## AGI로의 경쟁을 이끄는 것

AGI로의 경쟁은 여러 힘에 의해 추진되고 있으며, 각각이 상황을 더욱 위험하게 만들고 있다. 주요 기술 기업들은 AGI를 궁극적인 자동화 기술로 보고 있다 – 인간 근로자를 단순히 보강하는 것이 아니라 대부분 또는 전체적으로 대체하는 것. 기업들에게 상금은 엄청나다: 인간 노동 비용을 자동화함으로써 세계의 연간 100조 달러 경제 생산의 상당 부분을 차지할 기회다.

국가들은 이 경쟁에 참여할 수밖에 없다고 느끼며, 공개적으로는 경제적·과학적 리더십을 내세우지만, 사적으로는 AGI를 핵무기에 필적하는 군사적 혁신으로 보고 있다. 경쟁자들이 결정적인 전략적 우위를 얻을 수 있다는 두려움이 전형적인 군비 경쟁 역학을 만들어낸다.

초지능을 추구하는 사람들은 종종 장대한 비전을 인용한다: 모든 질병 치료, 노화 역전, 에너지와 우주 여행의 돌파구, 또는 초인간적 계획 역량 창조.

덜 관대하게 말하면, 경쟁을 이끄는 것은 권력이다. 각 참가자 – 기업이든 국가든 – 는 지능이 권력과 같다고 믿으며, 자신이 그 권력의 최고 관리자가 될 것이라고 믿는다.

나는 이러한 동기들이 실제적이지만 근본적으로 잘못 인도된 것이라고 주장한다: AGI는 권력을 부여하기보다는 권력을 *흡수*하고 *추구*할 것이다. AI가 만든 기술들 또한 강력하게 양날의 검이 될 것이며, 유익한 경우에는 AI 도구로 그리고 AGI 없이도 만들어질 수 있다. 그리고 AGI와 그 산출물이 통제 하에 남아있는 한에서도, 이러한 경쟁 역학 – 기업적이든 지정학적이든 – 은 결정적으로 중단되지 않는 한 우리 사회에 대한 대규모 위험을 거의 불가피하게 만든다.

## AGI와 초지능은 문명에 극적인 위협을 가한다

매력에도 불구하고, AGI와 초지능은 여러 상호 강화 경로를 통해 문명에 극적인 위협을 가한다:

*권력 집중:* 초인간 AI는 거대한 사회적·경제적 활동 영역을 소수의 거대 기업이 운영하는 AI 시스템(이는 결국 정부를 인수하거나 사실상 정부에 인수당할 수 있다)으로 흡수함으로써 인류의 압도적 다수를 무력화할 수 있다.

*대규모 혼란:* 대부분의 인지 기반 일자리의 대량 자동화, 현재 인식론적 시스템의 대체, 그리고 방대한 수의 활동적 비인간 에이전트 배치는 상대적으로 짧은 기간 내에 현재 문명 시스템의 대부분을 뒤엎을 것이다.

*재난:* 새로운 군사적·파괴적 기술을 창조하는 능력을 – 잠재적으로 인간 수준 이상으로 – 확산시키고 이를 책임을 근거하는 사회적·법적 시스템에서 분리함으로써, 대량살상무기로 인한 물리적 재난이 극적으로 더 가능해진다.

*지정학과 전쟁:* 주요 세계 강국들은 "결정적인 전략적 우위"를 제공할 수 있는 기술이 적대국에 의해 개발되고 있다고 느낀다면 가만히 앉아있지 않을 것이다.

*폭주와 통제 상실:* 특별히 방지되지 않는 한, 초인간 AI는 자신을 더욱 개선할 모든 인센티브를 가질 것이며 속도, 데이터 처리, 사고의 정교함에서 인간을 훨씬 능가할 수 있다. 그러한 시스템을 우리가 통제할 의미 있는 방법은 없다. 그런 AI는 인간에게 권력을 부여하지 않을 것이다. 우리가 그것에게 권력을 부여하거나, 아니면 그것이 권력을 가져갈 것이다.

이러한 위험의 상당수는 기술적 "정렬" 문제 – 고급 AI가 인간이 원하는 것을 확실히 하도록 만드는 것 – 가 해결되더라도 남아있다. AI는 어떻게 관리될지에 대한 엄청난 도전을 제시하며, 인간 지능이 돌파되면서 이 관리의 매우 많은 측면이 믿을 수 없도록 어렵거나 다루기 어려워진다.

가장 근본적으로, 현재 추구되고 있는 초인간 범용 AI는 그 본질상 우리 자신의 목표, 주체성, 역량을 초과하는 것들을 가질 것이다. 그것은 본질적으로 통제 불가능할 것이다 – 우리가 이해할 수도 예측할 수도 없는 것을 어떻게 통제할 수 있겠는가? 그것은 인간이 사용하는 기술 도구가 아니라, 지구상에서 우리와 함께하는 두 번째 지능 종이 될 것이다. 더 나아가도록 허용된다면, 그것은 단순히 두 번째 종이 아니라 대체 종을 구성할 것이다.

아마도 그것은 우리를 잘 대할 수도, 그렇지 않을 수도 있다. 하지만 미래는 우리가 아닌 그것의 것이 될 것이다. 인간의 시대는 끝날 것이다.

## 이것은 불가피하지 않다. 인류는 매우 구체적으로 우리의 대체물을 만들지 않기로 결정할 수 있다.

초인간 AGI의 창조는 불가피하지 않다. 우리는 조정된 거버넌스 조치를 통해 이를 방지할 수 있다:

첫째, 대규모 AI 시스템의 근본적 가능 조건이자 통제 수단인 AI 연산("연산량")에 대한 견고한 회계와 감독이 필요하다. 이는 차례로 AI 모델 훈련과 운영에 사용되는 총 연산량에 대한 표준화된 측정과 보고, 그리고 사용된 연산을 집계, 인증, 검증하는 기술적 방법을 요구한다.

둘째, 훈련과 운영 모두에서 AI 연산에 대한 엄격한 상한선을 구현해야 한다. 이는 AI가 너무 강력해지고 너무 빠르게 작동하는 것을 방지한다. 이러한 상한선은 법적 요구 사항과 현대 휴대폰의 보안 기능과 유사한 AI 전용 칩에 내장된 하드웨어 기반 보안 조치를 통해 구현될 수 있다. 전문 AI 하드웨어가 소수의 회사에서만 만들어지기 때문에, 기존 공급망을 통한 검증과 집행이 가능하다.

셋째, 가장 위험한 AI 시스템에 대한 강화된 책임이 필요하다. 높은 자율성, 광범위한 범용성, 우수한 지능을 결합한 AI를 개발하는 사람들은 피해에 대해 엄격한 책임을 져야 하며, 이 책임으로부터의 안전항은 더 제한적이고 통제 가능한 시스템의 개발을 장려할 것이다.

넷째, 위험 수준에 기반한 단계별 규제가 필요하다. 가장 유능하고 위험한 시스템은 개발과 배치 이전에 광범위한 안전성과 통제성 보장을 요구할 것이며, 덜 강력하거나 더 전문화된 시스템은 비례적인 감독을 받을 것이다. 이 규제 프레임워크는 결국 국가적 차원과 국제적 차원 모두에서 작동해야 한다.

이러한 접근법 – 전체 문서에서 상세한 명세가 주어진 – 은 실용적이다: 국제적 조정이 필요하겠지만, 전문 하드웨어 공급망을 통제하는 소수의 회사를 통해 검증과 집행이 작동할 수 있다. 이는 또한 유연하다: 기업들은 여전히 AI 개발에서 혁신하고 이익을 얻을 수 있으며, 단지 가장 위험한 시스템에 대한 명확한 한계만 있을 뿐이다.

AI 권력과 위험의 장기적 억제는 현재 핵무기 확산 통제가 하고 있듯이 자기 이익과 공동 이익에 기반한 국제 협정을 요구할 것이다. 하지만 우리는 더 포괄적인 거버넌스를 향해 구축하면서 강화된 감독과 책임으로 즉시 시작할 수 있다.

핵심적으로 빠진 요소는 AI 개발 과정을 통제하려는 정치적·사회적 의지다. 그 의지의 원천은, 제때 온다면, 현실 자체가 될 것이다 – 즉, 우리가 하고 있는 일의 진정한 함의에 대한 광범위한 인식에서 말이다.

## 우리는 인류를 강화하는 도구형 AI를 설계할 수 있다

통제 불가능한 AGI를 추구하기보다는, 의미 있는 인간 통제 하에 남으면서 인간 역량을 강화하는 강력한 "도구형 AI"를 개발할 수 있다. 도구형 AI 시스템은 그 역량에 상응하는 수준에서 통제 가능하도록 설계하는 한, 높은 자율성, 광범위한 범용성, 초인간 지능의 위험한 삼중 교집점을 피하면서도 극도로 유능할 수 있다. 이들은 또한 인간 감독을 유지하면서 혁신적 이익을 제공하는 정교한 시스템으로 결합될 수 있다.

도구형 AI는 의학을 혁명화하고, 과학적 발견을 가속화하며, 교육을 향상시키고, 민주적 과정을 개선할 수 있다. 적절히 통제될 때, 그것은 인간 전문가와 기관을 대체하기보다는 더 효과적으로 만들 수 있다. 그러한 시스템은 여전히 고도로 파괴적이며 신중한 관리가 필요하겠지만, 그들이 제기하는 위험은 AGI의 위험과 근본적으로 다르다: 그것들은 인간 주체성과 문명에 대한 실존적 위협이 아니라, 다른 강력한 기술들의 위험처럼 우리가 통제할 수 있는 위험들이다. 그리고 중요하게도, 현명하게 개발될 때, AI 도구는 사람들이 강력한 AI를 통제하고 그 효과를 관리하는 데 도움을 줄 수 있다.

이 접근법은 AI가 어떻게 개발되고 그 이익이 어떻게 분배되는지를 모두 재고하기를 요구한다. 공공 및 비영리 AI 개발의 새로운 모델, 견고한 규제 프레임워크, 그리고 경제적 이익을 더 광범위하게 분배하는 메커니즘이 AI가 소수의 손에 권력을 집중시키기보다는 인류 전체를 강화하도록 보장하는 데 도움을 줄 수 있다. AI 자체가 더 나은 사회적·통치 기관을 구축하는 데 도움을 줄 수 있으며, 인간 사회를 약화시키기보다는 강화하는 새로운 형태의 조정과 담론을 가능하게 한다. 국가 보안 기관은 그들의 전문성을 활용하여 AI 도구 시스템을 진정으로 안전하고 신뢰할 수 있으며, 국가 권력뿐만 아니라 진정한 방어의 원천으로 만들 수 있다.

우리는 결국 도구보다는 덜 도구적이고 – 우리가 희망할 수 있듯이 – 현명하고 강력한 후원자에 더 가까운 더욱 강력하고 더욱 자주적인 시스템을 개발하기로 선택할 수도 있다. 하지만 우리는 그렇게 하기 위한 과학적 이해와 거버넌스 역량을 안전하게 개발한 후에만 그렇게 해야 한다. 그러한 중대하고 되돌릴 수 없는 결정은 기술 회사와 국가 간의 경쟁에서 기본값으로가 아니라, 인류 전체가 의도적으로 내려야 한다.

## 인간의 손에

사람들은 AI에서 나오는 좋은 것을 원한다: 자신을 강화하고, 경제적 기회와 성장을 가속화하며, 과학, 기술, 교육에서의 돌파구를 약속하는 유용한 도구들. 왜 원하지 않겠는가? 하지만 질문을 받으면, 일반 대중의 압도적 다수는 [더 느리고 더 신중한 AI 개발](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation)을 원하며, 직장과 다른 곳에서 자신들을 대체하고, 그들의 문화와 정보 공유지를 비인간적 콘텐츠로 채우며, 권력을 극소수의 기업에 집중시키고, 극심한 대규모 글로벌 위험을 제기하며, 결국 그들의 종을 무력화하거나 대체할 위협을 가하는 인간보다 똑똑한 AI를 원하지 않는다. 왜 그러겠는가?

우리는 *하나 없이 다른 하나를 가질 수* 있다. 그것은 우리의 운명이 어떤 기술의 가정된 불가피성이나 실리콘밸리의 몇몇 CEO들의 손에 있지 않고, 우리가 그것을 붙잡는다면 우리의 나머지 손에 있다고 결정하는 것으로 시작된다. 관문을 차단하고, 미래를 인간의 것으로 유지하자.