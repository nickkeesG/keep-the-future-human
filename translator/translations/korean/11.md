# 부록

연산량 회계처리, '관문 차단'의 구현 사례, 엄격한 AGI 책임 체계의 세부사항, 그리고 AGI 안전성 및 보안 표준의 단계적 접근법을 포함한 보충 정보.

## 부록 A: 연산량 회계처리 기술적 세부사항

의미 있는 연산량 기반 통제를 위해서는 훈련과 추론에 사용된 총 연산량에 대한 "정확한 값"과 적절한 근삿값 모두에 대한 상세한 방법론이 필요하다. 다음은 기술적 차원에서 "정확한 값"을 집계하는 방법의 한 예이다.

**정의:**

*연산 인과 그래프:* AI 모델의 특정 출력 O에 대해, 해당 연산의 결과를 변경하면 잠재적으로 O를 변경할 수 있는 디지털 연산의 집합이다. (이는 보수적으로 가정되어야 하며, 즉 한 연산이 시간상 더 이른 시점에 발생하고 물리적으로 잠재적인 인과 경로를 갖는 선행 요소와 독립적이라고 믿을 명확한 이유가 있어야 한다.) 여기에는 추론 중 AI 모델이 수행한 연산뿐만 아니라 입력, 데이터 준비, 모델 훈련에 사용된 연산도 포함된다. 이들 각각이 AI 모델의 출력일 수 있으므로, 인간이 입력에 상당한 변경을 가한 지점에서 차단하여 재귀적으로 계산된다.

*훈련 연산량:* 신경망의 연산 인과 그래프에 수반되는 총 연산량(FLOP 또는 기타 단위로, 데이터 준비, 훈련, 미세조정 및 기타 모든 연산 포함).

*출력 연산량:* 특정 AI 출력의 연산 인과 그래프에 포함된 총 연산량으로, 해당 출력에 사용된 모든 신경망(및 이들의 훈련 연산량)과 기타 연산을 포함한다.

*추론 연산량 비율:* 일련의 출력에서 출력 간 출력 연산량의 변화율(FLOP/s 또는 기타 단위)로, 즉 다음 출력을 생성하는 데 사용된 연산량을 출력 간 시간 간격으로 나눈 값이다.

**예시 및 근삿값:**

- 인간이 생성한 데이터로 훈련된 단일 신경망의 경우, 훈련 연산량은 관례적으로 보고되는 총 훈련 연산량과 같다.
- 이러한 신경망이 일정한 속도로 추론을 수행하는 경우, 추론 연산량 비율은 대략 추론을 수행하는 연산 클러스터의 총 연산 속도(FLOP/s)이다.
- 모델 미세조정의 경우, 완전한 모델의 훈련 연산량은 미세조정되지 않은 모델의 훈련 연산량에 미세조정 중 수행된 연산과 미세조정에 사용된 데이터 준비에 사용된 연산을 더한 값이다.
- 지식 증류 모델의 경우, 완전한 모델의 훈련 연산량에는 증류된 모델과 합성 데이터나 기타 훈련 입력을 제공하는 데 사용된 더 큰 모델 모두의 훈련이 포함된다.
- 여러 모델이 훈련되었지만 많은 "시도"가 인간의 판단에 따라 폐기된 경우, 이들은 보존된 모델의 훈련 또는 출력 연산량에 포함되지 않는다.

## 부록 B: 관문 차단의 구현 사례

**구현 사례:** 다음은 훈련에 10<sup>27</sup> FLOP, 추론(AI 실행)에 10<sup>20</sup> FLOP/s의 제한을 설정한 상황에서 관문 차단이 어떻게 작동할 수 있는지를 보여주는 한 가지 예이다:

**1\. 중단:** 국가 안보상의 이유로, 미국 행정부는 미국에 본사를 둔 모든 회사, 미국에서 사업을 하는 회사, 또는 미국에서 제조된 칩을 사용하는 회사에 10<sup>27</sup> FLOP 훈련 연산량 한계를 초과할 수 있는 새로운 AI 훈련 실행을 중단할 것을 요구한다. 미국은 AI 개발을 주도하는 다른 국가들과 논의를 시작하여 유사한 조치를 취하도록 강력히 권장하고, 이들이 이를 따르지 않을 경우 미국의 중단이 해제될 수 있음을 시사해야 한다.

**2\. 미국 감독 및 허가:** 행정명령이나 기존 규제기관의 조치를 통해, 미국은 (예를 들어) 1년 내에 다음을 요구한다:

- 미국에서 운영되는 회사가 수행하는 10<sup>25</sup> FLOP 이상의 모든 AI 훈련 실행을 미국 규제기관이 유지하는 데이터베이스에 등록. (참고: 이보다 다소 약한 버전이 2023년 미국 AI 행정명령에 이미 포함되어 있었으나 현재는 철회되었으며, 10<sup>26</sup> FLOP 이상의 모델에 대한 등록을 요구했다.)
- 미국에서 운영되거나 미국 정부와 사업을 하는 모든 AI 관련 하드웨어 제조업체는 특수 하드웨어와 이를 구동하는 소프트웨어에 대한 일련의 요구사항을 준수. (이러한 요구사항 중 많은 부분은 기존 하드웨어에 대한 소프트웨어 및 펌웨어 업데이트를 통해 구현될 수 있지만, 장기적이고 견고한 해결책을 위해서는 차세대 하드웨어의 변경이 필요하다.) 이 중 하나는 하드웨어가 10<sup>18</sup> FLOP/s의 연산을 실행할 수 있는 고속 상호연결 클러스터의 일부인 경우, 원격 "거버너"의 정기적인 허가를 포함하는 더 높은 수준의 검증이 필요하다는 요구사항으로, 거버너는 원격측정과 추가 연산 수행 요청을 모두 받는다.
- 관리자는 자신의 하드웨어에서 수행된 총 연산을 미국 데이터베이스를 유지하는 기관에 보고.
- 더 안전하고 유연한 감독 및 허가를 가능하게 하는 더 강력한 요구사항이 단계적으로 도입.

**3\. 국제 감독:**

- 미국, 중국, 그리고 고급 칩 제조 능력을 보유한 다른 국가들이 국제 협정을 협상.
- 이 협정은 국제원자력기구와 유사한 새로운 국제기구를 창설하여 AI 훈련과 실행을 감독.
- 협정 서명국들은 자국의 AI 하드웨어 제조업체가 적어도 미국에서 부과된 것만큼 강력한 요구사항을 준수하도록 요구해야 함.
- 관리자들은 이제 본국 기관과 국제기구 내 새로운 사무소 모두에 AI 연산 수치를 보고해야 함.
- 추가 국가들이 기존 국제 협정에 참여하도록 강력히 권장: 서명국의 수출 통제는 비서명국의 고성능 하드웨어 접근을 제한하는 반면, 서명국은 AI 시스템 관리에 대한 기술 지원을 받을 수 있음.

**4\. 국제 검증 및 집행:**

- 하드웨어 검증 시스템이 업데이트되어 원래 관리자와 국제기구 사무소 모두에 직접 연산 사용량을 보고.
- 국제기구는 국제 협정 서명국들과의 논의를 통해 연산 제한에 합의하고, 이는 서명국에서 법적 효력을 갖게 됨.
- 이와 병행하여, 일련의 국제 표준이 개발되어 (한계 미만이지만) 연산 임계값 이상의 AI 훈련 및 실행이 해당 표준을 준수하도록 요구될 수 있음.
- 기관은 필요한 경우 더 나은 알고리즘 등을 보상하기 위해 연산 한계를 낮출 수 있음. 또는 (예를 들어 증명 가능한 안전 보장 수준에서) 안전하고 바람직하다고 판단되면 연산 한계를 높일 수 있음.

## 부록 C: 엄격한 AGI 책임 체계의 세부사항

**엄격한 AGI 책임 체계의 세부사항**

- 고도로 일반적이고 능력이 뛰어나며 자율적인 고급 AI 시스템의 생성과 운영은 "비정상적으로 위험한" 활동으로 간주.
- 따라서 이러한 시스템의 훈련과 운영에 대한 기본 책임 수준은 모델이나 그 출력/행동으로 인한 모든 피해에 대한 엄격하고 연대적인 책임(또는 미국 외 지역의 이에 상응하는 책임).
- 중대한 과실이나 고의적 위법행위의 경우 경영진과 이사회 구성원에게 개인 책임이 부과됨. 가장 악질적인 사례에 대해서는 형사 처벌이 포함되어야 함.
- 책임이 사람과 회사가 일반적으로 적용받는 기본(미국에서는 과실 기반) 책임으로 되돌아가는 수많은 안전 피난처가 있음.
	- 일정한 연산 임계값(위에서 설명한 상한선보다 최소 10배 이상 낮은) 미만에서 훈련되고 운영되는 모델.
	- "약한" AI(대략적으로, 의도된 작업에서 인간 전문가 수준 이하) 그리고/또는
	- "좁은" AI(구체적으로 설계되고 훈련된 고정되고 상당히 제한된 범위의 작업과 운영을 갖는) 그리고/또는
	- "수동적" AI(직접적인 인간 개입과 통제 없이 행동을 취하거나 복잡한 다단계 작업을 수행할 수 있는 능력이 매우 제한적인 - 약간의 수정을 통해서도).
	- 안전하고 보안이 확보되며 통제 가능함이 보장된 AI(증명 가능하게 안전하거나, 위험 분석에서 무시할 만한 수준의 예상 피해를 나타내는).
- 안전 피난처는 AI 개발자가 준비하고 기관이나 기관에서 인증한 감사자가 승인한 [안전 사례](https://arxiv.org/abs/2410.21572)를 기반으로 주장될 수 있음. 연산량을 기반으로 안전 피난처를 주장하기 위해서는 개발자가 총 훈련 연산량과 최대 추론 비율에 대한 신뢰할 만한 추정치만 제공하면 됨.
- 법률은 공공 피해의 높은 위험이 있는 AI 시스템 개발에 대한 금지명령이 적절한 상황을 명시적으로 규정해야 함.
- 회사 컨소시엄이 NGO 및 정부 기관과 협력하여 이러한 용어들을 정의하고, 규제기관이 안전 피난처를 부여하는 방법, AI 개발자가 안전 사례를 개발하는 방법, 안전 피난처가 사전에 주장되지 않은 경우 법원이 책임을 해석하는 방법에 대한 표준과 규범을 개발해야 함.

## 부록 D: AGI 안전성 및 보안 표준의 단계적 접근법

**AGI 안전성 및 보안 표준의 단계적 접근법**

| 위험 등급 | 유발 조건 | 훈련 요구사항 | 배포 요구사항 |
| --- | --- | --- | --- |
| RT-0 | 자율성, 일반성, 지능이 모두 약한 AI | 없음 | 없음 |
| RT-1 | 자율성, 일반성, 지능 중 하나가 강한 AI | 없음 | 위험과 용도에 따라, 모델을 사용할 수 있는 모든 곳의 국가 당국이 승인한 안전 사례가 잠재적으로 필요 |
| RT-2 | 자율성, 일반성, 지능 중 두 가지가 강한 AI | 개발자를 관할하는 국가 당국에 등록 | 주요 피해 위험을 승인된 수준 이하로 제한하는 안전 사례와 모델을 사용할 수 있는 모든 곳의 국가 당국이 승인한 독립적인 안전 감사(블랙박스 및 화이트박스 레드팀 포함) |
| RT-3 | 자율성, 일반성, 지능이 모두 강한 AGI | 개발자를 관할하는 국가 당국의 안전성 및 보안 계획 사전 승인 | 주요 피해의 제한된 위험을 승인된 수준 이하로 보장하는 안전 사례와 사이버 보안, 통제 가능성, 제거 불가능한 킬 스위치, 인간 가치와의 정렬, 악의적 사용에 대한 견고성을 포함한 필수 사양 |
| RT-4 | 10<sup>27</sup> FLOP 훈련 또는 10<sup>20</sup> FLOP/s 추론 중 하나를 초과하는 모든 모델 | 국제적으로 합의된 연산량 상한선 해제 시까지 금지 | 국제적으로 합의된 연산량 상한선 해제 시까지 금지 |

높은 자율성, 일반성, 지능의 조합과 연산량 임계값을 기반으로 한 위험 분류 및 안전성/보안 표준:

- *강한 자율성*은 시스템이 상당한 인간 감독이나 개입 없이 여러 단계의 작업을 수행하거나 실제 세계와 관련된 복잡한 행동을 취할 수 있거나 쉽게 그렇게 만들 수 있는 경우에 적용됨. 예시: 자율주행차와 로봇; 금융 거래 봇. 비해당 예시: GPT-4; 이미지 분류기
- *강한 일반성*은 광범위한 적용 범위, 모델이 의도적이고 구체적으로 훈련받지 않은 작업의 수행, 그리고 새로운 작업을 학습하는 상당한 능력을 나타냄. 예시: GPT-4; 뮤제로. 비해당 예시: 알파폴드; 자율주행차; 이미지 생성기
- *강한 지능*은 모델이 최고 성능을 발휘하는 작업에서 (그리고 일반 모델의 경우 광범위한 작업에 걸쳐) 인간 전문가 수준의 성능과 일치하는 것에 해당함. 예시: 알파폴드; 뮤제로; o3. 비해당 예시: GPT-4; 시리