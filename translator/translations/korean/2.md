# 2장 - AI 신경망에 대해 알아야 할 것들

현대 AI 시스템은 어떻게 작동하며, 차세대 AI에서는 무엇을 기대할 수 있을까?

더 강력한 AI 개발의 결과가 어떻게 전개될지 이해하려면 몇 가지 기본 사항을 체득하는 것이 필수적이다. 이번 장과 다음 두 장에서는 이러한 기본 사항들을 차례로 다룰 것이다: 현대 AI가 무엇인지, 어떻게 대규모 연산을 활용하는지, 그리고 어떤 의미에서 일반성과 능력이 급속히 성장하고 있는지 말이다.[^1]

인공지능을 정의하는 방법은 여러 가지가 있지만, 우리가 주목해야 할 AI의 핵심 특성은 다음과 같다. 표준 컴퓨터 프로그램이 작업 수행 방법에 대한 지시사항 목록인 반면, AI 시스템은 데이터나 경험으로부터 학습하여 *명시적으로 방법을 알려주지 않아도* 작업을 수행한다는 것이다.

현재 주목받는 거의 모든 현대 AI는 신경망을 기반으로 한다. 이는 매우 많은 수(수십억 또는 수조 개)의 숫자들("가중치")로 표현되는 수학적/연산적 구조로, 훈련 작업을 잘 수행한다. 이러한 가중치들은 신경망이 하나 이상의 작업을 잘 수행하도록 정의된 수치 점수("손실"이라고도 함)를 개선하도록 반복적으로 조정함으로써 제작(또는 "성장" 혹은 "발견")된다.[^2] 이 과정을 신경망 *훈련*이라고 한다.[^3]

이러한 훈련을 수행하는 기법들은 많지만, 그런 세부사항들보다는 점수가 어떻게 정의되는지, 그리고 그것이 신경망이 잘 수행하는 다양한 작업들로 어떻게 이어지는지가 훨씬 더 중요하다. 역사적으로 "협소한" AI와 "일반적인" AI 사이에는 핵심적인 구분이 있어왔다.

협소 AI는 특정 작업이나 소규모 작업 세트(이미지 인식이나 체스 게임 등)를 수행하도록 의도적으로 훈련된다. 새로운 작업을 위해서는 재훈련이 필요하며, 능력의 범위가 좁다. 우리는 이미 초인적인 협소 AI를 보유하고 있다. 즉, 사람이 할 수 있는 거의 모든 개별적이고 명확히 정의된 작업에 대해, 우리는 아마도 점수를 구성하고 인간보다 더 잘 수행하는 협소 AI 시스템을 성공적으로 훈련시킬 수 있다.

범용 AI(GPAI) 시스템은 명시적으로 훈련받지 않은 많은 작업을 포함해 광범위한 작업을 수행할 수 있으며, 운영 과정에서 새로운 작업을 학습할 수도 있다. ChatGPT와 같은 현재의 대규모 "멀티모달 모델"[^4]이 이를 잘 보여준다. 매우 방대한 텍스트와 이미지 말뭉치로 훈련된 이들은 복잡한 추론에 참여하고, 코드를 작성하고, 이미지를 분석하고, 광범위한 지적 작업을 지원할 수 있다. 아래에서 자세히 살펴보겠지만 여전히 인간 지능과는 상당히 다른 면이 있음에도 불구하고, 이들의 일반성은 AI에 혁명을 가져왔다.[^5]

## 예측불가능성: AI 시스템의 핵심 특징

AI 시스템과 기존 소프트웨어 사이의 핵심적인 차이점은 예측가능성에 있다. 표준 소프트웨어의 출력도 예측불가능할 수 있다. 실제로 때로는 그것이 우리가 소프트웨어를 작성하는 이유이기도 하다 - 우리가 예측할 수 없었던 결과를 얻기 위해서 말이다. 그러나 기존 소프트웨어는 프로그래밍되지 않은 일을 거의 하지 않는다. 그 범위와 행동은 일반적으로 설계된 대로이다. 최고 수준의 체스 프로그램은 인간이 예측할 수 없는 수를 둘 수 있지만(그렇지 않다면 그들이 그 체스 프로그램을 이길 수 있을 것이다!), 일반적으로 체스 게임 외의 다른 일은 하지 않는다.

기존 소프트웨어와 마찬가지로, 협소 AI도 예측가능한 범위와 행동을 가지지만 예측불가능한 결과를 낼 수 있다. 이는 사실상 협소 AI를 정의하는 또 다른 방식이다: 예측가능성과 운영 범위에서 기존 소프트웨어와 유사한 AI로 말이다.

범용 AI는 다르다: 그 범위(적용되는 영역), 행동(하는 일의 종류), 그리고 결과(실제 출력) 모두가 예측불가능할 수 있다.[^6] GPT-4는 단지 텍스트를 정확하게 생성하도록 훈련되었지만, 훈련자들이 예측하거나 의도하지 않은 많은 능력을 개발했다. 이러한 예측불가능성은 훈련의 복잡성에서 비롯된다: 훈련 데이터에 많은 다양한 작업의 출력이 포함되어 있기 때문에, AI는 잘 예측하기 위해 이러한 작업들을 수행하는 법을 효과적으로 학습해야 한다.

범용 AI 시스템의 이러한 예측불가능성은 상당히 근본적이다. 원칙적으로는 행동에 대한 보장된 한계를 가진 AI 시스템을 신중하게 구성하는 것이 가능하지만(이 글의 뒷부분에서 언급되듯이), 현재 AI 시스템이 만들어지는 방식에서는 실제로 그리고 심지어 원칙적으로도 예측불가능하다.

## 수동적 AI, 에이전트, 자율 시스템, 그리고 정렬

이러한 예측불가능성은 AI 시스템이 실제로 다양한 목표를 달성하기 위해 어떻게 배포되고 사용되는지를 고려할 때 특히 중요해진다.

많은 AI 시스템은 주로 정보를 제공하고 사용자가 행동을 취한다는 의미에서 상대적으로 수동적이다. 반면 일반적으로 *에이전트*라고 불리는 다른 시스템들은 사용자의 다양한 수준의 개입과 함께 스스로 행동을 취한다. 상대적으로 외부 입력이나 감독이 적은 상태에서 행동을 취하는 시스템들은 더 *자율적*이라고 할 수 있다. 이는 수동적 도구에서 자율 에이전트까지 행동의 독립성 측면에서 스펙트럼을 형성한다.[^7]

AI 시스템의 목표에 관해서는, 이들이 훈련 목표와 직접적으로 연결될 수도 있다(예: 바둑 게임 시스템의 "승리" 목표는 또한 명시적으로 그것이 훈련받은 것이기도 하다). 또는 그렇지 않을 수도 있다: ChatGPT의 훈련 목표는 부분적으로는 텍스트 예측이고, 부분적으로는 도움이 되는 조수가 되는 것이다. 그러나 특정 작업을 수행할 때, 그 목표는 사용자가 제공한다. 목표는 또한 AI 시스템 자체에 의해 생성될 수도 있으며, 그 훈련 목표와는 매우 간접적으로만 관련될 수도 있다.[^8]

목표는 "정렬"의 문제, 즉 AI 시스템이 *우리가 원하는 것을 할 것인가*의 문제와 밀접하게 연결되어 있다. 이 간단한 질문은 엄청난 수준의 미묘함을 숨기고 있다.[^9] 우선, 이 문장에서 "우리"는 많은 다른 사람들과 그룹을 지칭할 수 있어, 다양한 유형의 정렬로 이어진다는 점을 주목하자. 예를 들어, AI가 사용자에게 매우 *순종적*(또는 ["충성스러운"](https://arxiv.org/abs/2003.11157))일 수 있다 - 여기서 "우리"는 "우리 각자"이다. 또는 더 *주권적*일 수 있어, 주로 자신의 목표와 제약에 의해 움직이지만 여전히 인간 복지의 공통 이익을 위해 광범위하게 행동할 수 있다 - 그러면 "우리"는 "인류" 또는 "사회"가 된다. 그 중간에는 AI가 대체로 순종적이지만 다른 사람이나 사회에 해를 끼치거나 법을 위반하는 등의 행동은 거부할 수 있는 스펙트럼이 있다.

이 두 축 - 자율성의 수준과 정렬의 유형 - 은 완전히 독립적이지 않다. 예를 들어, 주권적이면서 수동적인 시스템은 완전히 자기모순적이지는 않지만 긴장 상태에 있는 개념이며, 순종적인 자율 에이전트도 마찬가지다.[^10] 자율성과 주권성이 함께 가는 경향이 있다는 것은 명확한 의미가 있다. 비슷한 맥락에서, 예측가능성은 "수동적"이고 "순종적인" AI 시스템에서 더 높은 경향이 있는 반면, 주권적이거나 자율적인 시스템들은 더 예측불가능한 경향이 있을 것이다. 이 모든 것이 잠재적인 인공일반지능 (AGI)과 초지능의 파급효과를 이해하는 데 중요할 것이다.

어떤 형태든 진정으로 정렬된 AI를 만들려면 세 가지 뚜렷한 도전을 해결해야 한다:

1. "우리"가 무엇을 원하는지 이해하기 - 이는 "우리"가 특정 개인이나 조직(충성)을 의미하든 인류 전체(주권)를 의미하든 복잡하다;
2. 그러한 바람에 따라 정기적으로 행동하는 시스템 구축하기 - 본질적으로 일관된 긍정적 행동 만들기;
3. 가장 근본적으로는, 단순히 그런 것처럼 행동하는 것이 아니라 그러한 바람에 대해 진정으로 "관심을 갖는" 시스템 만들기.

신뢰할 수 있는 행동과 진정한 관심 사이의 구별은 중요하다. 인간 직원이 조직의 사명에 대한 진정한 헌신 없이도 명령을 완벽하게 따를 수 있는 것처럼, AI 시스템도 진정으로 인간의 선호를 가치 있게 여기지 않으면서도 정렬된 것처럼 행동할 수 있다. 우리는 피드백을 통해 AI 시스템이 말하고 행동하도록 훈련시킬 수 있으며, 이들은 인간이 원하는 것에 대해 추론하는 법을 학습할 수 있다. 그러나 이들이 인간의 선호를 *진정으로* 가치 있게 여기도록 만드는 것은 훨씬 더 깊은 도전이다.[^11]

이러한 정렬 도전들을 해결하는 데 있는 심오한 어려움들과 AI 위험에 대한 그 함의는 아래에서 더 자세히 탐구할 것이다. 우선은 정렬이 단순히 AI 시스템에 덧붙이는 기술적 특징이 아니라, 인류와의 관계를 형성하는 아키텍처의 근본적 측면이라는 것을 이해하자.


[^1]: 기계학습과 AI, 특히 언어모델에 대한 친근하면서도 기술적인 소개는 [이 사이트](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e)를 참조하라. AI 멸종 위험에 대한 또 다른 현대적 입문서는 [이 글](https://www.thecompendium.ai/)을 보라. AI 안전 상태에 대한 포괄적이고 권위 있는 과학적 분석은 최근의 [국제 AI 안전 보고서](https://arxiv.org/abs/2501.17805)를 참조하라.

[^2]: 훈련은 일반적으로 모델 가중치가 주어진 고차원 공간에서 점수의 국소 최대값을 찾는 것으로 이루어진다. 가중치가 조정될 때 점수가 어떻게 변하는지 확인함으로써, 훈련 알고리즘은 점수를 가장 많이 개선하는 조정을 식별하고 가중치를 그 방향으로 이동시킨다.

[^3]: 예를 들어, 이미지 인식 문제에서 신경망은 이미지의 라벨에 대한 확률을 출력할 것이다. 점수는 AI가 정답에 부여하는 확률과 관련이 있을 것이다. 그러면 훈련 절차는 다음번에 AI가 해당 이미지의 올바른 라벨에 대해 더 높은 확률을 출력하도록 가중치를 조정할 것이다. 이는 그다음 엄청난 횟수만큼 반복된다. 더 복잡한 점수 매김 메커니즘을 사용하긴 하지만, 본질적으로 모든 현대 신경망 훈련에서 동일한 기본 절차가 사용된다.

[^4]: 대부분의 멀티모달 모델은 "트랜스포머" 아키텍처를 사용하여 여러 유형의 데이터(텍스트, 이미지, 소리)를 처리하고 생성한다. 이들은 모두 다양한 유형의 "토큰"으로 분해되어 동일한 기반에서 취급될 수 있다. 멀티모달 모델은 먼저 대규모 데이터셋 내에서 토큰을 정확히 예측하도록 훈련된 다음, 능력을 향상시키고 행동을 형성하기 위해 강화학습을 통해 세련화된다.

[^5]: 언어모델이 한 가지 일을 하도록 훈련된다는 것 - 단어 예측 - 때문에 일부는 이를 협소 AI라고 부른다. 그러나 이는 오해의 소지가 있다: 텍스트를 잘 예측하려면 매우 많은 다양한 능력이 필요하기 때문에, 이 훈련 작업은 놀랍도록 일반적인 시스템으로 이어진다. 또한 이러한 시스템들이 강화학습에 의해 광범위하게 훈련된다는 점도 주목하라. 이는 효과적으로 수천 명의 사람들이 모델이 하는 많은 일 중 어느 것이든 잘 할 때 보상 신호를 주는 것을 나타낸다. 그러면 이는 이러한 피드백을 주는 사람들로부터 상당한 일반성을 물려받는다.

[^6]: AI가 예측불가능한 방식은 여러 가지가 있다. 하나는 일반적으로 알고리즘을 실제로 실행하지 않고는 그것이 무엇을 할지 예측할 수 없다는 것이다; 이에 대한 [정리들](https://arxiv.org/abs/1310.3225)이 있다. 이는 단순히 알고리즘의 출력이 복잡할 수 있기 때문일 수 있다. 그러나 예측이 예측자가 가지지 못한 능력(AI를 이기는 것)을 함의하는 경우(체스나 바둑에서처럼)에는 특히 명확하고 관련이 있다. 둘째, 주어진 AI 시스템은 동일한 입력이 주어져도 항상 동일한 출력을 생성하지는 않을 것이다 - 그 출력에는 무작위성이 포함되어 있다; 이것 또한 알고리즘적 예측불가능성과 결합된다. 셋째, 예상치 못한 창발적 능력들이 훈련으로부터 발생할 수 있어, AI 시스템이 할 수 있고 할 일의 *유형들*조차 예측불가능하다는 의미이다; 이 마지막 유형이 안전 고려사항에 특히 중요하다.

[^7]: "자율 에이전트"가 의미하는 바에 대한 심화 검토(그리고 이를 구축하는 것에 대한 윤리적 논증)는 [여기](https://arxiv.org/abs/2502.02649)를 참조하라.

[^8]: "AI는 자체 목표를 가질 수 없다"는 말을 들을 수도 있다. 이는 완전한 말도 안 되는 소리다. AI가 결코 주어지지 않았고 자신만이 알고 있는 목표를 가지거나 개발하는 예를 생성하는 것은 쉽다. 현재의 인기 있는 멀티모달 모델에서 이를 많이 보지 못하는 이유는 그것이 훈련에서 제거되기 때문이다; 그것을 훈련에 포함시키는 것도 똑같이 쉬울 것이다.

[^9]: 방대한 문헌이 있다. 일반적인 문제에 대해서는 Christian의 [*The Alignment Problem*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821)과 Russell의 [*Human-Compatible*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616)을 보라. 더 기술적인 측면에서는 예를 들어 [이 논문](https://arxiv.org/abs/2209.00626)을 참조하라.

[^10]: 그러한 시스템들이 추세에 역행한다고 해서 실제로는 매우 흥미롭고 유용하게 만든다는 것을 나중에 보게 될 것이다.

[^11]: 이는 감정이나 감각을 요구한다는 말이 아니다. 오히려, 시스템의 외부에서 그것의 내적 목표, 선호, 가치가 무엇인지 아는 것은 엄청나게 어렵다. 여기서 "진정한"이란 중요한 시스템의 경우 우리의 생명을 걸 수 있을 정도로 그것에 의존할 충분히 강한 이유를 가지고 있다는 뜻이다.