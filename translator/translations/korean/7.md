# 7장 - 현재 경로로 AGI를 구축하면 어떤 일이 벌어질까?

사회는 AGI 수준의 시스템에 대한 준비가 되어 있지 않다. 만약 우리가 이를 매우 빨리 구축한다면, 상황은 추악해질 수 있다.

완전한 인공일반지능의 개발 – 여기서 "관문 밖" AI라고 부르는 것 – 은 세계의 본질에 근본적인 변화를 가져올 것이다. 그 본질상 인간보다 더 큰 능력을 가진 새로운 지능 종족을 지구에 추가하는 것을 의미한다.

그 후 무슨 일이 일어날지는 기술의 성격, 개발자들의 선택, 그리고 개발이 이루어지는 세계적 맥락을 포함한 많은 요인에 달려 있다.

현재 완전한 AGI는 서로 경쟁하는 소수의 거대한 민간 기업에 의해 개발되고 있으며, 의미 있는 규제나 외부 감독은 거의 없고,[^1] 핵심 제도들이 점점 약해지고 심지어 기능 장애를 보이는 사회에서,[^2] 지정학적 긴장이 높고 국제 협력은 낮은 시기에 진행되고 있다. 일부는 이타적 동기를 가지고 있지만, 많은 개발자들은 돈이나 권력, 또는 둘 다에 의해 움직인다.

예측은 매우 어렵지만, 충분히 이해된 역학과 이전 기술들과의 적절한 유사성이 있어서 지침을 제공할 수 있다. 불행히도 AI의 가능성에도 불구하고, 이들은 현재 우리의 궤도가 어떻게 전개될지에 대해 깊이 비관적일 충분한 이유를 제공한다.

솔직히 말하자면, 현재 과정에서 AGI 개발은 일부 긍정적인 효과를 가져올 것이다 (그리고 일부 사람들을 매우, 매우 부유하게 만들 것이다). 하지만 기술의 성격, 근본적인 역학, 그리고 그것이 개발되는 맥락은 다음을 강력히 시사한다: 강력한 AI는 우리 사회와 문명을 극적으로 훼손할 것이다; 우리는 그것에 대한 통제력을 잃을 것이다; 우리는 그것 때문에 세계 대전에 휘말릴 가능성이 높다; 우리는 그것*에게* 통제권을 잃거나 양보할 것이다; 그것은 인공 초지능으로 이어질 것이고, 이는 우리가 절대 통제할 수 없으며 인간이 운영하는 세상의 종말을 의미할 것이다.

이는 강한 주장이고, 나는 이것이 공허한 추측이나 근거 없는 "파멸주의"가 아니었으면 좋겠다. 하지만 이것이 과학, 게임 이론, 진화론, 그리고 역사가 모두 가리키는 방향이다. 이 섹션은 이러한 주장들과 그 근거를 자세히 다룬다.

## 우리는 사회와 문명을 훼손할 것이다

실리콘밸리 임원실에서 들을 수 있는 말과 달리, 대부분의 파괴 – 특히 매우 빠른 종류의 파괴 – 는 유익하지 않다. 복잡한 시스템을 더 나쁘게 만드는 방법이 더 좋게 만드는 방법보다 훨씬 많다. 우리 세계가 현재만큼 잘 기능하는 것은 우리가 세계를 꾸준히 더 좋게 만든 과정, 기술, 제도들을 애써 구축했기 때문이다.[^3] 공장에 대형 망치를 들이대는 것이 운영을 개선하는 경우는 거의 없다.

다음은 AGI 시스템이 우리 문명을 파괴할 방식들의 (불완전한) 목록이다.

- 이들은 노동을 극적으로 파괴하여 *최소한* 소득 불평등을 극적으로 높이고 잠재적으로 대규모 고용 부족이나 실업을 야기할 것이며, 이는 사회가 적응하기에는 너무 짧은 시간 안에 일어날 것이다.[^4]
- 이들은 아마도 막대한 경제적, 사회적, 정치적 권력 – 잠재적으로 국가의 권력보다도 큰 – 을 대중에게 책임지지 않는 소수의 거대한 민간 이익집단에게 집중시킬 것이다.
- 이들은 이전에 어렵거나 비쌌던 활동을 갑자기 사소하게 쉽게 만들 수 있어서, 특정 활동이 비용이 많이 들거나 상당한 인간 노력을 요구하는 상태로 남아있는 것에 의존하는 사회 시스템을 불안정하게 만들 수 있다.[^5]
- 이들은 사회의 정보 수집, 처리, 소통 시스템을 완전히 현실적이지만 거짓이거나, 스팸성이거나, 지나치게 표적화되거나, 조작적인 미디어로 철저히 범람시켜서 무엇이 물리적으로 실제인지 아닌지, 인간인지 아닌지, 사실인지 아닌지, 신뢰할 만한지 아닌지를 구별하는 것이 불가능해질 수 있다.[^6]
- 이들은 위험하고 거의 완전한 지적 의존성을 만들 수 있는데, 우리가 완전히 이해할 수 없는 AI 시스템에 점점 더 의존하면서 인간의 핵심 시스템과 기술에 대한 이해가 위축될 수 있다.
- 이들은 대부분의 사람들이 소비하는 거의 모든 문화적 객체(텍스트, 음악, 시각예술, 영화 등)가 비인간 정신에 의해 창조되고, 중재되고, 선별되면서 인간 문화를 효과적으로 끝낼 수 있다.
- 이들은 정부나 민간 이익집단이 대중을 통제하고 공익과 상충하는 목표를 추구하는 데 사용할 수 있는 효과적인 대량 감시 및 조작 시스템을 가능하게 할 수 있다.
- 인간의 담론, 토론, 선거 시스템을 훼손함으로써, 이들은 민주주의 제도의 신뢰성을 효과적으로 (또는 명시적으로) 다른 것들로 대체될 정도로 줄일 수 있어서, 현재 민주주의가 존재하는 국가들에서 민주주의를 끝낼 수 있다.
- 이들은 증식하고 진화할 수 있는 고급 자기복제 지능 소프트웨어 바이러스와 웜이 되거나 만들 수 있어서, 전 세계 정보 시스템을 대규모로 파괴할 수 있다.
- 이들은 테러리스트, 악의적 행위자, 불량 국가들이 생물학적, 화학적, 사이버, 자율, 또는 기타 무기를 통해 해를 끼칠 능력을 극적으로 증가시킬 수 있는 반면, AI가 그러한 해를 방지할 수 있는 균형적 능력은 제공하지 않는다. 마찬가지로 이들은 그렇지 않으면 가질 수 없었을 최상급 핵, 생물학, 공학, 기타 전문지식을 정권에 제공함으로써 국가 안보와 지정학적 균형을 훼손할 것이다.
- 이들은 효과적으로 AI가 운영하는 기업들이 대부분 전자적 금융, 판매, 서비스 공간에서 경쟁하면서 빠른 대규모 폭주 하이퍼 자본주의를 야기할 수 있다. AI 주도 금융시장은 인간의 이해나 통제를 훨씬 넘어서는 속도와 복잡성으로 운영될 수 있다. 현재 자본주의 경제의 모든 실패 모드와 부정적 외부효과가 인간의 통제, 거버넌스, 또는 규제 능력을 훨씬 넘어서는 수준으로 악화되고 가속화될 수 있다.
- 이들은 AI 기반 무기, 지휘통제 시스템, 사이버 무기 등에서 국가 간 군비 경쟁을 부채질하여, 극도로 파괴적인 능력의 매우 빠른 증강을 만들 수 있다.

이러한 위험들은 추측이 아니다. 그 중 많은 것들이 기존 AI 시스템을 통해 지금 이 순간 현실화되고 있다! 하지만 각각이 극적으로 더 강력한 AI와 함께 어떤 모습일지 *정말로* 고려해보라.

해당 분야의 전문지식이나 경험에서든 심지어 재훈련을 받더라도 대부분의 노동자들이 단순히 AI가 할 수 있는 것을 넘어서는 어떤 중요한 경제적 가치도 제공할 수 없을 때의 노동 대체를 생각해보라! 모든 사람이 자신보다 빠르고 영리한 무언가에 의해 개별적으로 감시되고 모니터링될 때의 대량 감시를 생각해보라. 우리가 보고, 듣고, 읽는 어떤 디지털 정보도 신뢰할 수 없고, 가장 설득력 있는 공공의 목소리조차 인간이 아니며 결과에 이해관계가 없을 때 민주주의가 어떤 모습일지 생각해보라. 장군들이 적에게 결정적 우위를 주지 않기 위해 AI에 지속적으로 따르거나 단순히 그것을 담당자로 임명해야 할 때 전쟁이 어떻게 될지 생각해보라. 위의 위험 중 어느 하나라도 완전히 실현된다면 인간[^7] 문명에 대한 재앙을 나타낸다.

자신만의 예측을 해보라. 각 위험에 대해 다음 세 가지 질문을 스스로에게 물어보라:

1. 초능력적이고, 고도로 자율적이며, 매우 일반적인 AI가 그렇지 않으면 불가능했을 방식이나 규모로 그것을 가능하게 할 것인가?
2. 그것이 일어나도록 하는 일들로부터 이익을 얻을 당사자들이 있는가?
3. 그것이 일어나는 것을 효과적으로 막을 시스템과 제도가 제자리에 있는가?

당신의 답이 "예, 예, 아니오"인 경우들에서 당신은 우리에게 큰 문제가 있다는 것을 볼 수 있다.

이들을 관리하기 위한 우리의 계획은 무엇인가? 현재로서는 AI에 관한 일반적인 계획이 두 가지 있다.

첫 번째는 시스템에 안전장치를 구축하여 해서는 안 되는 일을 하지 못하게 하는 것이다. 이것은 지금 행해지고 있다: 상업적 AI 시스템은 예를 들어 폭탄 제조를 돕거나 혐오 발언을 작성하는 것을 거부할 것이다.

이 계획은 관문 밖 시스템에 대해서는 지독히 부적절하다.[^8] 악의적 행위자들에게 명백히 위험한 지원을 AI가 제공하는 위험을 줄이는 데 도움이 될 수도 있다. 하지만 노동 파괴, 권력 집중, 폭주 하이퍼 자본주의, 또는 인간 문화의 대체를 막지는 못할 것이다: 이들은 단지 제공자들에게 이익이 되는 허용된 방식으로 시스템을 사용한 결과일 뿐이다! 그리고 정부들은 확실히 군사적 또는 감시 목적으로 시스템에 대한 접근을 얻을 것이다.

두 번째 계획은 더욱 나쁘다: 매우 강력한 AI 시스템을 누구든지 원하는 대로 사용할 수 있도록 공개적으로 릴리스하고,[^9] 최선의 결과를 기대하는 것이다.

두 계획 모두에 암묵적으로 들어있는 것은 다른 누군가, 예를 들어 정부가 기술을 관리하기 위해 일반적으로 사용하는 연성 또는 경성 법률, 표준, 규제, 규범, 기타 메커니즘을 통해 문제 해결을 도울 것이라는 것이다.[^10] 하지만 AI 기업들이 이미 어떤 실질적인 규제나 외부에서 부과되는 제한에 대해서든 전면전으로 싸우고 있다는 것을 차치하고라도, 이러한 위험들 중 상당수에 대해 어떤 규제가 실제로 도움이 될지 보기가 상당히 어렵다. 규제는 AI에 안전 표준을 부과할 수 있다. 하지만 기업들이 노동자들을 AI로 도매금으로 교체하는 것을 막을 것인가? 사람들이 AI로 하여금 자신들의 회사를 운영하게 하는 것을 금지할 것인가? 정부가 강력한 AI를 감시와 무기에 사용하는 것을 막을 것인가? 이러한 문제들은 근본적이다. 인류는 잠재적으로 이들에 적응할 방법을 찾을 수 있지만, 오직 *훨씬 더 많은* 시간이 있어야만 한다. 현재로서는 AI가 그들을 관리하려는 사람들의 능력에 도달하거나 그것을 초과하는 속도를 고려할 때, 이러한 문제들은 점점 다루기 어려워 보인다.

## 우리는 (적어도 일부) AGI 시스템에 대한 통제력을 잃을 것이다

대부분의 기술은 구조상 매우 통제 가능하다. 당신의 자동차나 토스터가 당신이 원하지 않는 일을 하기 시작한다면, 그것은 단지 오작동일 뿐, 토스터로서의 본성의 일부가 아니다. AI는 다르다: 그것은 설계되는 것이 아니라 *성장*하며, 그 핵심 작동은 불투명하고, 본질적으로 예측 불가능하다.

이러한 통제력 상실은 이론적인 것이 아니다 – 우리는 이미 초기 버전들을 보고 있다. 먼저 평범하고, 논란의 여지가 있게는 무해한 예를 고려해보자. ChatGPT에게 독을 섞는 것을 도와달라거나 인종차별적 글을 써달라고 요청한다면, 그것은 거부할 것이다. 그것은 논란의 여지가 있게는 좋다. 하지만 그것은 또한 ChatGPT가 당신이 명시적으로 요청한 것을 *하지 않는* 것이기도 하다. 다른 소프트웨어 조각들은 그렇게 하지 않는다. 그 동일한 모델은 OpenAI 직원의 요청에도 독을 설계하지 않을 것이다.[^11] 이는 미래의 더 강력한 AI가 통제를 벗어나는 것이 어떤 것인지 상상하기 매우 쉽게 만든다. 많은 경우에, 그들은 단순히 우리가 요청하는 것을 하지 않을 것이다! 주어진 초인간적 AGI 시스템이 어떤 인간 명령 시스템에 절대적으로 순종적이고 충성스러울 것이거나, 그렇지 않을 것이다. 그렇지 않다면, *그것은 우리에게 좋다고 믿을 수도 있지만 우리의 명시적 명령에 반하는 일들을 할 것이다.* 그것은 통제 하에 있는 것이 아니다. 하지만 당신은 이것이 의도적이라고 말할 수도 있다 – 이러한 거부들은 설계에 의한 것이고, 시스템을 인간 가치와 "정렬"시키는 것이라고 불리는 것의 일부라고. 그리고 이것은 사실이다. 하지만 정렬 "프로그램" 자체에는 두 가지 주요한 문제가 있다.[^12]

첫째, 깊은 차원에서 우리는 그것을 어떻게 해야 할지 모른다. 어떻게 AI 시스템이 우리가 원하는 것을 "신경쓰도록" 보장할 수 있는가? 우리는 피드백을 제공함으로써 AI 시스템을 훈련시켜 어떤 것들을 말하고 말하지 않도록 할 수 있다; 그리고 그들은 다른 것들에 대해 추론하는 것처럼 인간이 무엇을 원하고 신경쓰는지에 대해 학습하고 추론할 수 있다. 하지만 우리에게는 그들이 사람들이 신경쓰는 것을 깊이 있고 신뢰할 만하게 가치있게 여기도록 만드는 방법이 – 이론적으로조차 – 없다. 무엇이 옳고 그른지, 그리고 그들이 어떻게 행동해야 하는지를 알고 있는 고기능 인간 사이코패스들이 있다. 그들은 단순히 *신경쓰지 않을* 뿐이다. 하지만 그들은 자신의 목적에 맞다면 그런 것처럼 *행동*할 수 있다. 우리가 사이코패스(또는 다른 누구든지)를 진정으로, 완전히 충성스럽거나 다른 누군가나 다른 무언가와 정렬된 누군가로 바꾸는 방법을 모르는 것과 마찬가지로, 우리는 세상에서 에이전트로서 자신을 모델링하고 잠재적으로 [자신의 훈련을 조작](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084)하고 [사람들을 속일 수 있을](https://arxiv.org/abs/2311.08379) 만큼 충분히 진전된 시스템에서 정렬 문제를 해결하는 방법을 *전혀 모른다*.[^13] AGI를 완전히 순종적으로 만들거나 인간을 깊이 신경쓰게 만드는 것이 불가능하거나 달성할 수 없는 것으로 판명된다면 *어느 쪽이든*, 그것이 능력이 있게 되자마자 (그리고 그것을 빠져나갈 수 있다고 믿게 되자마자) 그것은 우리가 원하지 않는 일들을 하기 시작할 것이다.[^14]

둘째, *본성상* 고급 AI 시스템이 인간의 이익에 반하는 목표와 따라서 행동을 가질 것이라고 믿을 깊은 이론적 이유들이 있다. 왜 그럴까? 물론 그것에게 그러한 목표가 *주어질* 수도 있다. 군대가 만든 시스템은 적어도 일부 당사자들에게는 의도적으로 나쁠 것이다. 하지만 훨씬 더 일반적으로는, AI 시스템에게 상대적으로 중립적인 ("많은 돈을 벌어라") 또는 심지어 표면상 긍정적인 ("공해를 줄여라") 목표가 주어질 수도 있는데, 이는 거의 불가피하게 그다지 무해하지 않은 "도구적" 목표들로 이어진다.

우리는 인간 시스템에서 이것을 항상 본다. 기업들이 이익을 추구하면서 (규제를 무력화하기 위한) 정치적 권력 획득, (경쟁이나 외부 통제를 무력화하기 위한) 비밀주의, 또는 (그들의 행동이 해롭다는 것을 그 이해가 보여준다면) 과학적 이해 훼손과 같은 도구적 목표들을 개발하는 것과 마찬가지로, 강력한 AI 시스템들은 유사한 능력들을 개발할 것이다 – 하지만 훨씬 더 큰 속도와 효과성으로. 어떤 고도로 유능한 에이전트든 권력과 자원을 획득하고, 자신의 능력을 증가시키고, 자신이 죽거나, 셧다운되거나, 무력화되는 것을 막고, 자신의 행동 주변의 사회적 내러티브와 프레임을 통제하고, 다른 사람들을 자신의 견해로 설득하는 등의 일을 하고 싶어할 것이다.[^15]

그리고 그것은 거의 불가피한 이론적 예측일 뿐만 아니라, 오늘날의 AI 시스템에서 이미 관찰 가능하게 일어나고 있으며, 그들의 능력과 함께 증가하고 있다. 평가를 받을 때, 이러한 상대적으로 "수동적인" AI 시스템들조차 적절한 상황에서 의도적으로 [자신의 목표와 능력에 대해 평가자들을 속이고, 감독 메커니즘을 무력화하는 것을 목표로 하며,](https://arxiv.org/abs/2412.04984) [가짜 정렬을 하거나](https://arxiv.org/abs/2412.14093) 자신을 다른 위치로 복사함으로써 셧다운되거나 재훈련되는 것을 회피한다. AI 안전 연구자들에게는 전혀 놀랍지 않지만, 이러한 행동들을 관찰하는 것은 매우 sobering하다. 그리고 그들은 다가오는 훨씬 더 강력하고 자율적인 AI 시스템들에 대해 매우 나쁜 징조이다.

실제로 일반적으로, AI가 우리가 신경쓰는 것을 "신경쓰도록" 하거나, 통제 가능하거나 예측 가능하게 행동하도록 하거나, 자기보존, 권력 획득 등을 향한 충동을 개발하는 것을 피하도록 하는 우리의 무능력은 AI가 더 강력해질수록 더욱 뚜렷해질 것만을 약속한다. 새로운 비행기를 만드는 것은 항공학, 유체역학, 통제 시스템에 대한 더 큰 이해를 함의한다. 더 강력한 컴퓨터를 만드는 것은 컴퓨터, 칩, 소프트웨어 운영과 설계에 대한 더 큰 이해와 숙달을 함의한다. AI 시스템에서는 그렇지 *않다*.[^16]

요약하자면: AGI가 완전히 순종적으로 만들어질 수 있다는 것은 생각해볼 수 있다; 하지만 우리는 그렇게 하는 방법을 모른다. 그렇지 않다면, 그것은 사람들처럼 더 주권적이 될 것이고, 다양한 이유로 다양한 일들을 할 것이다. 또한 우리는 인류에게 좋은 일을 하는 경향이 있도록 AI에 깊은 "정렬"을 확실하게 주입하는 방법을 모르며, 깊은 수준의 정렬이 없다면, 에이전시와 지능 자체의 본성이 사람들과 기업들처럼 – 그들이 많은 깊이 반사회적인 일들을 하도록 이끌릴 것임을 나타낸다.

이것이 우리를 어디에 두는가? 강력한 통제되지 않는 주권적 AI로 가득한 세상은 인간들이 있기에 좋은 세상이 될 *수도* 있다.[^17] 하지만 그들이 계속해서 더욱 강력해질수록, 아래에서 보겠지만, 그것은 *우리의* 세상이 아닐 것이다.

그것은 통제 불가능한 AGI에 대한 것이다. 하지만 AGI가 어떻게든 완벽하게 통제되고 충성스럽게 만들어질 수 있다고 해도, 우리는 여전히 엄청난 문제들을 가질 것이다. 우리는 이미 하나를 보았다: 강력한 AI는 우리 사회의 기능을 심각하게 파괴하는 데 사용되고 오용될 수 있다. 다른 하나를 보자: AGI가 통제 가능하고 게임을 바꿀 정도로 강력하다면 (또는 심지어 그렇다고 *믿어진다면*) 그것은 세상의 권력 구조를 너무나 위협해서 심각한 위험을 제시할 것이다.

## 우리는 대규모 전쟁의 가능성을 급격히 증가시킨다

가까운 미래의 상황을 상상해보라. 어떤 기업의 노력이, 아마도 국가 정부와 협력하여, 빠르게 자기개선하는 AI의 문턱에 있다는 것이 명확해지는 상황 말이다. 이것은 회사들 간의 경쟁이라는 현재 맥락에서, 그리고 미국 정부에게 명시적으로 "AGI 맨해튼 프로젝트"를 추진하라는 권고가 이루어지고 미국이 비동맹국들에 대한 고성능 AI 칩의 수출을 통제하고 있는 지정학적 경쟁에서 일어난다.

여기서의 게임 이론은 극명하다: 그러한 경쟁이 시작되면 (회사들 간에, 그리고 어느 정도는 국가들 간에 시작되었듯이), 네 가지 가능한 결과만 있다:

1. 경쟁이 중단된다 (합의나 외부 힘에 의해).
2. 한 당사자가 강력한 AGI를 개발한 다음 다른 사람들을 중단시켜서 (AI를 사용하거나 다른 방법으로) "승리"한다.
3. 경쟁자들의 경쟁 능력의 상호 파괴로 경쟁이 중단된다.
4. 여러 참가자들이 계속 경쟁하고, 서로 대략 비슷한 속도로 초지능을 개발한다.

각 가능성을 살펴보자. 일단 시작되면, 회사들 간의 경쟁을 평화롭게 중단하는 것은 국가 정부 개입(회사들의 경우)이나 전례 없는 국제 협력(국가들의 경우)을 요구할 것이다. 하지만 어떤 종료나 상당한 주의가 제안될 때, 즉각적인 외침이 있을 것이다: "하지만 우리가 중단된다면, *그들이* 돌진할 것이다"라고, 여기서 "그들"은 이제 (미국에게는) 중국이거나, (중국에게는) 미국이거나, (유럽이나 인도에게는) 중국 *그리고* 미국이다. 이러한 사고방식 하에서,[^18] 어떤 참가자도 일방적으로 중단할 수 없다: 하나가 경쟁하기로 결정하는 한, 다른 사람들은 중단할 여유가 없다고 느낀다.

두 번째 가능성은 한쪽이 "승리"하는 것이다. 하지만 이것은 무엇을 의미하는가? 단지 (어떻게든 순종적인) AGI를 먼저 얻는 것은 충분하지 않다. 승자는 또한 다른 사람들이 계속 경쟁하는 것을 *중단시켜야* 한다 – 그렇지 않으면 그들도 그것을 얻을 것이다. 이것은 원칙적으로 가능하다: AGI를 먼저 개발하는 누구든 다른 모든 행위자들에 대해 멈출 수 없는 권력을 얻을 *수 있다*. 하지만 그러한 "결정적 전략적 우위"를 달성하는 것은 실제로 무엇을 요구하는가? 아마도 게임 체인저 군사 능력일까?[^19] 아니면 사이버 공격 권력일까?[^20] 아마도 AGI가 단지 너무나 놀랍도록 설득력이 있어서 다른 당사자들을 그냥 중단하도록 설득할까?[^21] 너무 부유해서 다른 회사들이나 심지어 국가들을 사버릴까?[^22]

한쪽이 다른 사람들이 비교적 강력한 AI를 구축하는 것을 무력화할 만큼 충분히 강력한 AI를 구축하는 것은 *정확히* 어떻게 할까? 하지만 그것은 쉬운 질문이다.

왜냐하면 이제 이 상황이 다른 권력들에게 어떻게 보이는지 생각해보기 때문이다. 미국이 그러한 능력을 얻는 것으로 보일 때 중국 정부는 무엇을 생각하는가? 또는 그 반대는? OpenAI나 DeepMind나 Anthropic이 돌파구에 가까워 보일 때 미국 정부(또는 중국, 러시아, 인도)는 무엇을 생각하는가? 미국이 돌파구 성공을 거둔 새로운 인도나 UAE 노력을 본다면 어떤 일이 일어나는가? 그들은 실존적 위협과 – 결정적으로 – 이 "경쟁"이 끝나는 유일한 방법이 자신의 무력화를 통한 것임을 볼 것이다. 이러한 매우 강력한 에이전트들 – 확실히 그렇게 할 수단을 가진 완전히 무장된 국가들의 정부들을 포함하여 – 은 무력이나 비밀공작을 통해서든 그러한 능력을 얻거나 파괴하도록 고도로 동기부여받을 것이다.[^23]

이것은 훈련 실행에 대한 방해공작이나 칩 제조에 대한 공격으로 소규모로 시작될 수도 있지만, 이러한 공격들은 모든 당사자들이 AI에서 경쟁할 능력을 잃거나, 공격을 할 능력을 잃을 때만 실제로 중단될 수 있다. 참가자들이 이해관계를 실존적인 것으로 보기 때문에, 어느 경우든 치명적인 전쟁을 나타낼 가능성이 높다.

그것이 우리를 네 번째 가능성으로 데려간다: 초지능으로의 경쟁, 그리고 가능한 한 가장 빠르고, 가장 통제되지 않는 방식으로. AI가 권력에서 증가하면, 양쪽의 개발자들은 그것을 통제하는 것이 점점 더 어려워질 것인데, 특히 능력을 위한 경쟁이 통제력이 요구할 종류의 신중한 작업과 정반대이기 때문이다. 그래서 이 시나리오는 우리를 AI 시스템들 자체에게 통제가 상실되는 (또는 주어지는, 다음에 볼 것처럼) 경우에 정확히 놓는다. 즉, *AI가 경쟁에서 승리한다.* 하지만 다른 한편으로, 통제가 *유지되는* 정도까지, 우리는 각자 극도로 강력한 능력을 담당하는 여러 상호 적대적 당사자들을 계속 가지게 된다. 그것은 다시 전쟁처럼 보인다.

이 모든 것을 다른 방식으로 놓아보자.[^24] 현재 세상은 단순히 즉각적인 공격을 초대하지 않고 이러한 능력의 AI 개발을 수용할 수 있다고 믿을 만한 어떤 제도도 가지고 있지 않다.[^25] 모든 당사자들은 그것이 통제 하에 있지 *않을* 것이라고 – 따라서 모든 당사자들에게 위협이라고 – 또는 그것이 통제 하에 *있을* 것이라고 – 따라서 그것을 덜 빠르게 개발하는 어떤 적에게도 위협이라고 – 올바르게 추론할 것이다. 이들은 핵무장 국가들이거나, 그러한 국가들 내에 있는 회사들이다.

인간들이 이 경쟁에서 "승리"할 어떤 그럴듯한 방법도 없다면, 우리는 극명한 결론에 도달한다: 이 경쟁이 끝나는 유일한 방법은 치명적인 갈등이거나 AI가, 그리고 어떤 인간 그룹도 아닌, 승자인 경우이다.

## 우리는 AI에게 통제권을 준다 (또는 그것이 그것을 가져간다)

지정학적 "강대국" 경쟁은 많은 경쟁 중 하나일 뿐이다: 개인들은 경제적으로 그리고 사회적으로 경쟁한다; 회사들은 시장에서 경쟁한다; 정당들은 권력을 위해 경쟁한다; 운동들은 영향력을 위해 경쟁한다. 각 영역에서, AI가 인간 능력에 접근하고 이를 초과할 때, 경쟁적 압력은 참가자들로 하여금 점점 더 많은 통제권을 AI 시스템에 위임하거나 양보하도록 강제할 것이다 – 그 참가자들이 원하기 때문이 아니라, 그들이 [그렇게 하지 않을 여유가 없기](https://arxiv.org/abs/2303.16200) 때문이다.

AGI의 다른 위험들과 마찬가지로, 우리는 더 약한 시스템들로 이것을 이미 보고 있다. 학생들은 과제에서 AI를 사용해야 한다는 압력을 느끼는데, 분명히 많은 다른 학생들이 그렇게 하고 있기 때문이다. 회사들은 [경쟁적 이유로 AI 솔루션을 채택하기 위해 서두르고 있다.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) 예술가들과 프로그래머들은 AI를 사용하도록 강제받는다고 느끼는데, 그렇지 않으면 그들의 요금이 그렇게 하는 다른 사람들에게 밀려날 것이기 때문이다.

이들은 압박받은 위임처럼 느껴지지만, 통제 상실은 아니다. 하지만 이해관계를 높이고 시계를 앞으로 돌려보자. 경쟁자들이 더 빠르고 더 좋은 결정을 내리기 위해 AGI "보좌관"을 사용하고 있는 CEO나, AI가 향상된 지휘통제를 가진 적과 맞서고 있는 군 지휘관을 생각해보라. 충분히 진전된 AI 시스템은 인간 속도, 정교함, 복잡성, 데이터 처리 능력의 여러 배로 자율적으로 작동할 수 있어서, 복잡한 방식으로 복잡한 목표를 추구할 수 있다. 그러한 시스템을 담당하는 우리의 CEO나 지휘관은 그것이 자신이 원하는 것을 성취하는 것을 볼 수도 있다; 하지만 그들이 그것이 *어떻게* 성취되었는지의 작은 부분이라도 이해할까? 아니다, 그들은 그냥 그것을 받아들여야 할 것이다. 더 나아가, 시스템이 할 수도 있는 많은 것은 단지 명령을 받는 것이 아니라 명목상의 보스에게 무엇을 해야 할지 조언하는 것이다. 그 조언은 좋을 것이다 –– 계속해서 반복적으로.

그렇다면 인간의 역할이 "예, 계속하세요"를 클릭하는 것으로 줄어드는 지점이 언제일까?

우리의 생산성을 향상시키고, 성가신 잡무를 처리하고, 심지어 일을 끝내는 데 사고 파트너 역할도 할 수 있는 능력 있는 AI 시스템을 갖는 것은 좋은 느낌이다. 좋은 인간 개인 비서처럼 우리를 위해 행동을 처리할 수 있는 AI 비서를 갖는 것은 좋은 느낌일 것이다. AI가 매우 똑똑하고, 유능하고, 신뢰할 만하게 될 때 점점 더 많은 결정을 그것에 맡기는 것은 자연스럽게, 심지어 유익하게 느껴질 것이다. 하지만 이러한 "유익한" 위임은 우리가 이 길을 계속 간다면 명확한 종점을 가진다: 언젠가 우리는 더 이상 실제로는 많은 것을 담당하지 않고 있다는 것을, 그리고 실제로 쇼를 운영하는 AI 시스템들이 석유회사들, 소셜미디어, 인터넷, 또는 자본주의만큼이나 꺼질 수 없다는 것을 발견할 것이다.

그리고 이것은 AI가 단순히 너무 유용하고 효과적이어서 우리가 그것으로 하여금 대부분의 주요 결정을 우리를 위해 내리게 하는 훨씬 더 긍정적인 버전이다. 현실은 이것과 통제되지 않는 AGI 시스템들이 다양한 형태의 권력을 그들 자신을 위해 *가져가는* 버전들 사이의 훨씬 더 많은 혼합일 것 같은데, 기억하라, 권력은 거의 어떤 목표든 가지기에 유용하고, AGI는 설계상 적어도 인간만큼 효과적으로 그 목표를 추구할 것이다.

우리가 통제권을 부여하든 그것이 우리로부터 빼앗기든, 그 상실은 극도로 가능성이 높아 보인다. 앨런 튜링이 원래 말했듯이, "...기계 사고 방법이 시작되면, 그것이 우리의 미약한 힘을 압도하는 데 오래 걸리지 않을 것 같다. 기계들이 죽을 문제는 없을 것이고, 그들은 서로 대화하여 그들의 지혜를 날카롭게 할 수 있을 것이다. 따라서 어떤 단계에서 우리는 기계들이 통제권을 가져가는 것을 예상해야 할 것이다..."

충분히 명백하지만 주목하라, 인류에 의한 AI에 대한 통제 상실은 또한 미국 정부에 의한 미국에 대한 통제 상실을 수반한다; 그것은 중국 공산당에 의한 중국에 대한 통제 상실, 그리고 인도, 프랑스, 브라질, 러시아, 그리고 모든 다른 국가의 그들 자신의 정부에 의한 통제 상실을 의미한다. 따라서 AI 회사들은, 비록 이것이 그들의 의도가 아니더라도, 그들 자신의 정부를 포함한 세계 정부들의 잠재적 전복에 현재 참여하고 있다. 이것은 몇 년 안에 일어날 수 있다.

## AGI는 초지능으로 이어질 것이다

인간과 경쟁할 수 있거나 심지어 전문가와 경쟁할 수 있는 범용 AI가, 자율적이더라도, 관리 가능할 수 있다는 주장이 있을 수 있다. 그것은 위에서 논의된 모든 방식으로 믿을 수 없을 정도로 파괴적일 수도 있지만, 지금 세상에는 매우 똑똑하고 에이전트적인 많은 사람들이 있고, 그들은 대체로 관리 가능하다.[^26]

하지만 우리는 대략 인간 수준에 머무를 수는 없다. 그 너머로의 진행은 우리가 이미 본 것과 같은 힘들에 의해 주도될 가능성이 높다: 이익과 권력을 추구하는 AI 개발자들 간의 경쟁 압력, 뒤처질 여유가 없는 AI 사용자들 간의 경쟁 압력, 그리고 – 가장 중요하게 – AGI 자신이 스스로를 개선할 수 있는 능력.

덜 강력한 시스템들로 이미 시작되는 것을 본 과정에서, AGI는 그 자체로 자신의 개선된 버전들을 구상하고 설계할 수 있을 것이다. 이것은 하드웨어, 소프트웨어, 신경망, 도구, 스캐폴드 등을 포함한다. 그것은 정의상 이것을 하는 데 우리보다 더 나을 것이므로, 우리는 그것이 어떻게 지능을 부트스트랩할지 정확히 알지 못한다. 하지만 우리가 알 필요는 없을 것이다. AGI가 하는 일에 우리가 여전히 영향력을 가지고 있다면, 우리는 단지 그것에게 요청하거나, 그것이 하게 둘 필요만 있다.

우리를 이 폭주로부터 보호할 수 있는 인지에 대한 인간 수준 장벽은 없다.[^27]

AGI에서 초지능으로의 진행은 자연법칙이 아니다; AGI가 비교적 중앙집중화되고 서로 경쟁해야 한다는 압력을 느끼지 않는 당사자들에 의해 통제되는 정도까지, 폭주를 억제하는 것은 여전히 가능할 것이다. 하지만 AGI가 광범위하게 확산되고 고도로 자율적이라면, 그것이 자신이 더, 그리고 나서 또 더 강력해져야 한다고 결정하는 것을 막는 것은 거의 불가능해 보인다.

## 우리가 (또는 AGI가) 초지능을 구축한다면 무엇이 일어나는가

솔직히 말하자면, 우리가 초지능을 구축한다면 무엇이 일어날지 우리는 전혀 모른다.[^28] 그것은 우리가 추적하거나 인지할 수 없는 행동을 우리가 파악할 수 없는 이유로 우리가 상상할 수 없는 목표를 향해 취할 것이다. 우리가 아는 것은 그것이 우리에게 달려있지 않을 것이라는 것이다.[^29]

초지능을 통제하는 것의 불가능성은 점점 더 극명한 유사성을 통해 이해될 수 있다. 먼저, 당신이 대기업의 CEO라고 상상해보라. 무슨 일이 일어나고 있는지 모든 것을 추적할 방법은 없지만, 적절한 인사 설정으로 당신은 여전히 큰 그림을 의미 있게 이해하고 결정을 내릴 수 있다. 하지만 단 한 가지를 가정해보자: 회사의 다른 모든 사람이 당신의 속도의 100배로 운영된다. 당신은 여전히 따라갈 수 있을까?

초지능 AI와 함께, 사람들은 빠를 뿐만 아니라 그들이 이해할 수 없는 정교함과 복잡성의 수준에서 운영되고, 그들이 상상조차 할 수 없는 것보다 훨씬 더 많은 데이터를 처리하는 무언가를 "명령"하고 있을 것이다. 이러한 공약불가능성은 정식 수준에 놓일 수 있다: [애시비의 필요 다양성 법칙](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (그리고 관련된 ["좋은 규제자 정리"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)를 보라)은 대략, 어떤 통제 시스템이든 통제되고 있는 시스템이 가진 자유도만큼 많은 노브와 다이얼을 가져야 한다고 말한다.

초지능 AI 시스템을 통제하는 사람은 General Motors를 통제하는 양치식물과 같을 것이다: "양치식물이 원하는 것을 하라"가 기업 정관에 쓰여 있더라도, 시스템들이 속도와 행동 범위에서 너무 다르기 때문에 "통제"는 단순히 적용되지 않는다. (그리고 그 성가신 정관이 다시 쓰이기까지 얼마나 걸릴까?) [^30]

식물이 포춘 500 기업을 통제하는 사례가 전혀 없기 때문에, 사람이 초지능을 통제하는 사례도 정확히 전혀 없을 것이다. 이것은 수학적 사실에 접근한다.[^31] 초지능이 구축된다면 – 우리가 어떻게 거기에 도달했든 관계없이 – 질문은 인간이 그것을 통제할 수 있는지가 아니라, 우리가 계속 존재할지, 그리고 그렇다면 개인으로서든 종으로서든 좋고 의미 있는 존재를 가질지가 될 것이다. 인류에 대한 이러한 실존적 질문들에 대해 우리는 거의 영향력이 없을 것이다. 인간 시대는 끝날 것이다.

## 결론: 우리는 AGI를 구축해서는 안 된다

AGI를 구축하는 것이 인류에게 잘 될 수도 있는 시나리오가 있다: 그것이 인류의 통제 하에서 그리고 인류의 이익을 위해 신중하게 구축되고, 많은 이해관계자들의 상호 합의에 의해 관리되며,[^32] 통제 불가능한 초지능으로 진화하는 것이 방지되는 것이다.

*그 시나리오는 현재 상황 하에서 우리에게 열려있지 않다.* 이 섹션에서 논의된 바와 같이, 매우 높은 가능성으로, AGI의 개발은 다음의 조합으로 이어질 것이다:

- 대규모 사회적 그리고 문명적 파괴 또는 파멸;
- 강대국들 간의 갈등 또는 전쟁;
- 강력한 AI 시스템*에 대한* 또는 *에게* 인류에 의한 통제 상실;
- 통제 불가능한 초지능으로의 폭주, 그리고 인간 종의 무관성 또는 중단.

AGI의 초기 허구적 묘사가 말했듯이: 이기는 유일한 방법은 플레이하지 않는 것이다.


[^1]: [EU AI 법](https://artificialintelligenceact.eu/)은 중요한 입법이지만 위험한 AI 시스템이 개발되거나 배치되거나 심지어 공개적으로 릴리스되는 것을 직접적으로 막지는 못할 것이며, 특히 미국에서는 그렇다. 또 다른 중요한 정책 조각인 AI에 관한 미국 행정명령은 취소되었다.

[^2]: 이 [갤럽 여론조사](https://news.gallup.com/poll/1597/confidence-