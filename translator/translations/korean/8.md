# 8장 - AGI를 만들지 않는 방법

AGI는 불가피한 것이 아니다. 오늘날 우리는 갈림길에 서 있다. 이번 장에서는 AGI 개발을 어떻게 막을 수 있는지에 대한 방안을 제시한다.

현재 우리가 걷고 있는 길이 문명의 종말로 이어질 가능성이 높다면, 어떻게 방향을 바꿀 수 있을까?

AGI와 초지능 개발을 중단하려는 열망이 널리 퍼지고 강력해진다고 가정해보자.[^1] 왜냐하면 AGI가 권력을 부여하는 것이 아니라 권력을 흡수하는 존재이며, 사회와 인류에 심각한 위험이 된다는 것이 일반적인 이해가 되었기 때문이다. 어떻게 관문을 차단할 것인가?

현재 우리가 아는 강력하고 범용적인 AI를 *만드는* 유일한 방법은 심층 신경망의 정말로 대규모 연산을 통해서다. 이는 매우 어렵고 비용이 많이 드는 작업이기 때문에, 이를 *하지 않는* 것은 어떤 의미에서는 쉽다.[^2] 하지만 우리는 이미 AGI를 향해 나아가게 하는 힘들과, 어떤 당사자든 일방적으로 중단하기 매우 어렵게 만드는 게임이론적 역학을 살펴보았다. 따라서 기업들을 막기 위한 외부(즉, 정부)의 개입과 정부들 자신을 막기 위한 정부 간 합의의 조합이 필요할 것이다.[^3] 이것이 어떤 모습일 수 있을까?

먼저 *방지되거나* *금지되어야* 하는 AI 개발과 *관리되어야* 하는 개발을 구분하는 것이 유용하다. 전자는 주로 초지능으로의 폭주일 것이다.[^4] 금지된 개발의 경우, 정의는 가능한 한 명확해야 하고, 검증과 집행 모두 실용적이어야 한다. *관리되어야* 할 것은 범용적이고 강력한 AI 시스템들일 것이다. 이는 우리가 이미 보유하고 있으며, 많은 회색지대와 미묘함, 복잡성을 가질 것이다. 이를 위해서는 강력하고 효과적인 제도가 중요하다.

또한 국제적 차원에서 (지정학적 경쟁국이나 적대국 간을 포함하여) 다뤄져야 하는 문제들[^5]과 개별 관할권, 국가, 또는 국가 연합이 관리할 수 있는 문제들을 구분하는 것도 유용하다. 금지된 개발은 대부분 "국제적" 범주에 속한다. 왜냐하면 기술 개발에 대한 지역적 금지는 일반적으로 장소를 바꿈으로써 우회될 수 있기 때문이다.[^6]

마지막으로, 도구상자에 있는 도구들을 고려해볼 수 있다. 기술적 도구, 연성법(표준, 규범 등), 경성법(규제와 요구사항), 책임, 시장 인센티브 등을 포함해 많은 도구가 있다. AI에 특별한 하나의 도구에 특히 주목해보자.

## 연산량 보안과 거버넌스

고성능 AI를 통제하는 핵심 도구는 그것이 필요로 하는 하드웨어가 될 것이다. 소프트웨어는 쉽게 확산되고, 한계생산비용이 거의 0에 가까우며, 국경을 쉽게 넘나들고, 즉시 수정될 수 있다. 하드웨어는 이 중 어느 것도 해당되지 않는다. 그러나 우리가 논의했듯이, 가장 뛰어난 시스템을 달성하기 위해서는 AI 시스템 훈련과 추론 모두에서 엄청난 양의 이 "연산량"이 필요하다. 연산량은 쉽게 정량화되고, 계산되며, 감사될 수 있고, 이를 위한 좋은 규칙이 개발되면 비교적 모호함이 적다. 가장 중요한 것은, 대량의 연산이 농축 우라늄처럼 매우 희귀하고 비싸며 생산하기 어려운 자원이라는 점이다. 컴퓨터 칩은 도처에 있지만, AI에 필요한 하드웨어는 비싸고 제조하기가 극도로 어렵다.[^7]

AI 전용 칩을 우라늄보다 희소 자원으로서 *훨씬 더* 관리하기 쉽게 만드는 것은 하드웨어 기반 보안 메커니즘을 포함할 수 있다는 점이다. 대부분의 현대 휴대폰과 일부 노트북은 승인된 운영체제 소프트웨어와 업데이트만 설치하고, 민감한 생체인식 데이터를 기기 내에서 보관하고 보호하며, 분실이나 도난 시 소유자가 아닌 사람에게는 쓸모없게 만들 수 있도록 하는 전용 온칩 하드웨어 기능을 가지고 있다. 지난 몇 년간 이러한 하드웨어 보안 조치들은 잘 정립되고 널리 채택되었으며, 일반적으로 상당히 안전함이 입증되었다.

이러한 기능의 핵심 혁신은 암호화를 사용해 하드웨어와 소프트웨어를 결합한다는 점이다.[^8] 즉, 특정 컴퓨터 하드웨어를 가지고 있다고 해서 사용자가 다른 소프트웨어를 적용해 원하는 모든 것을 할 수 있는 것은 아니다. 그리고 이러한 결합은 많은 공격이 단순히 *소프트웨어* 보안이 아닌 *하드웨어* 보안의 침해를 필요로 하게 만들기 때문에 강력한 보안을 제공한다.

최근 몇몇 보고서([GovAI와 협력기관](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips), [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf) 등)는 최첨단 AI 관련 컴퓨팅 하드웨어에 내장된 유사한 하드웨어 기능들이 AI 보안과 거버넌스에서 극도로 유용한 역할을 할 수 있다고 지적했다. 이들은 "통제자"[^9]가 사용할 수 있는 여러 기능을 가능하게 하는데, 이는 사람들이 가능하거나 심지어 실현 가능하다고 추측하지 못했을 수도 있는 것들이다. 몇 가지 주요 예시들:

- *지리적 위치*: 칩들이 알려진 위치를 가지고, 위치에 따라 다르게 작동하거나 완전히 차단되도록 시스템을 설정할 수 있다.[^10]
- *허용 목록 연결*: 각 칩은 네트워킹할 수 있는 특정한 다른 칩들의 하드웨어로 강제되는 허용 목록으로 구성될 수 있으며, 이 목록에 없는 칩과는 연결할 수 없다.[^11] 이는 연결 가능한 칩 클러스터의 크기를 제한할 수 있다.[^12]
- *계량화된 추론 또는 훈련 (그리고 자동 차단 스위치)*: 통제자는 사용자가 수행할 수 있는 특정량의 훈련이나 추론(시간, FLOP, 또는 토큰 단위)만 허가할 수 있으며, 그 후에는 새로운 허가가 필요하다. 증분이 작다면, 모델의 비교적 지속적인 재허가가 필요하다. 그러면 이 허가 신호를 보류하는 것만으로도 모델을 "차단"할 수 있다.[^13]
- *속도 제한*: 모델이 통제자나 다른 방식으로 결정된 어떤 제한보다 빠른 추론 속도로 실행되는 것을 방지한다. 이는 제한된 허용 목록 연결이나 더 정교한 수단을 통해 구현될 수 있다.
- *증명된 훈련*: 훈련 절차는 특정 코드 집합, 데이터, 연산량 사용이 모델 생성에 사용되었다는 암호학적으로 안전한 증명을 제공할 수 있다.

## 초지능을 만들지 않는 방법: 훈련과 추론 연산량에 대한 글로벌 제한

이러한 고려사항들, 특히 연산에 관한 것들을 염두에 두고, 인공 초지능으로의 관문을 차단하는 방법을 논의할 수 있다. 그다음 완전한 AGI 방지와 다양한 측면에서 인간 능력에 접근하고 이를 초과하는 AI 모델 관리로 전환할 것이다.

첫 번째 요소는 물론 초지능이 통제 불가능하며, 그 결과가 근본적으로 예측 불가능하다는 이해다. 최소한 중국과 미국은 이런 목적이나 다른 목적을 위해 초지능을 만들지 않기로 독립적으로 결정해야 한다.[^14] 그다음 강력한 검증과 집행 메커니즘을 갖춘 이들과 다른 국가들 간의 국제적 합의가 필요하다. 이는 모든 당사자들이 경쟁국들이 배신하여 주사위를 던지지 않는다는 것을 보장하기 위해서다.

검증 가능하고 집행 가능하려면 제한은 엄격한 제한이어야 하고, 가능한 한 명확해야 한다. 이는 거의 불가능한 문제처럼 보인다: 예측 불가능한 속성을 가진 복잡한 소프트웨어의 능력을 전 세계적으로 제한하는 것. 다행히 상황은 이보다 훨씬 낫다. 왜냐하면 바로 고급 AI를 가능하게 만든 것, 즉 엄청난 양의 연산량이 통제하기 훨씬, 훨씬 쉽기 때문이다. 여전히 일부 강력하고 위험한 시스템을 허용할 수 있지만, *폭주하는 초지능*은 신경망에 들어가는 연산량에 대한 엄격한 상한선과 AI 시스템(연결된 신경망과 기타 소프트웨어)이 수행할 수 있는 추론량에 대한 비율 제한을 통해 방지될 가능성이 높다. 이에 대한 구체적인 버전을 아래에서 제안한다.

AI 연산에 엄격한 글로벌 제한을 가하는 것은 막대한 수준의 국제 협력과 침입적이고 프라이버시를 파괴하는 감시를 필요로 할 것처럼 보일 수 있다. 다행히 그렇지 않다. 극도로 [긴밀하고 병목이 된 공급망](https://arxiv.org/abs/2402.08797)으로 인해 일단 제한이 법적으로 (법률이든 행정명령이든) 설정되면, 그 제한에 대한 준수 검증은 소수의 대기업들의 참여와 협력만 필요로 할 것이다.[^15]

이런 계획은 여러 매우 바람직한 특징을 가지고 있다. 소수의 주요 기업들만이 요구사항을 부과받고, 상당히 큰 연산 클러스터만 통제된다는 점에서 최소 침입적이다. 관련 칩들은 이미 첫 번째 버전에 필요한 하드웨어 능력을 포함하고 있다.[^16] 구현과 집행 모두 표준 법적 제한에 의존한다. 하지만 이들은 하드웨어의 사용 약관과 하드웨어 통제에 의해 뒷받침되어, 집행을 크게 단순화하고 기업, 사적 집단, 심지어 국가들의 부정행위를 방지한다. 하드웨어 회사들이 자신들의 하드웨어 사용에 원격 제한을 가하고, 특정 능력을 외부에서 잠그거나 해제하는 것은 데이터센터의 고성능 CPU에서도[^18] 포함해 충분한 선례가 있다.[^17] 영향을 받는 하드웨어와 조직의 상당히 작은 부분에 대해서도, 감독은 원격 측정에 한정될 수 있고, 데이터나 모델 자체에 직접 접근할 필요가 없다. 이를 위한 소프트웨어는 추가적인 데이터가 기록되지 않음을 보여주기 위해 검사에 공개될 수 있다. 이 체계는 국제적이고 협력적이며, 상당히 유연하고 확장 가능하다. 제한이 주로 소프트웨어보다는 하드웨어에 있기 때문에, AI 소프트웨어 개발과 배치가 어떻게 이루어지는지에 상대적으로 구애받지 않으며, AI 주도의 권력 집중에 맞서는 더 "탈중앙화된" 또는 "공공" AI를 포함한 다양한 패러다임과 호환된다.

연산 기반 관문 차단에도 단점이 있다. 첫째, AI 거버넌스 전반 문제에 대한 완전한 해결책과는 거리가 멀다. 둘째, 컴퓨터 하드웨어가 빨라질수록, 시스템은 더 작고 작은 클러스터(또는 심지어 개별 GPU)에서 점점 더 많은 하드웨어를 "포착"할 것이다.[^19] 또한 알고리즘 개선으로 인해 훨씬 낮은 연산 제한이 필요해질 수 있거나,[^20] 연산량이 대부분 무관해져서 관문 차단이 대신 AI에 대한 더 자세한 위험 기반 또는 능력 기반 거버넌스 체제를 필요로 할 수도 있다. 셋째, 보장과 영향을 받는 주체의 작은 수에도 불구하고, 이러한 시스템은 프라이버시와 감시를 비롯한 다른 우려사항들에 대한 반발을 불러일으킬 수밖에 없다.[^21]

물론, 짧은 기간 내에 연산 제한 거버넌스 체계를 개발하고 구현하는 것은 상당히 도전적일 것이다. 하지만 절대적으로 실행 가능하다.

## A-G-I: 위험의 기반이자 정책의 기반으로서의 삼중 교집합

이제 AGI로 전환해보자. 여기서 엄격한 선과 정의는 더 어렵다. 왜냐하면 우리는 분명히 인공적이고 범용적인 지능을 가지고 있으며, 기존의 어떤 정의로도 모든 사람이 그것이 존재하는지 또는 언제 존재하는지에 동의하지 않을 것이기 때문이다. 더욱이, 연산량이나 추론 제한은 다소 둔탁한 도구(연산량이 능력의 대리변수이고, 그것이 다시 위험의 대리변수)여서, 상당히 낮지 않다면 사회적 또는 문명적 혼란이나 급성 위험을 일으킬 만큼 강력한 AGI를 방지할 가능성은 낮다.

나는 가장 급성 위험이 매우 높은 능력, 높은 자율성, 그리고 큰 범용성의 삼중 교집합에서 나타난다고 주장했다. 이들은 개발된다면 극도의 주의를 기울여 관리되어야 하는 시스템들이다. 세 가지 속성을 모두 결합한 시스템에 대해 엄격한 표준을 (책임과 규제를 통해) 만들어냄으로써, 우리는 AI 개발을 더 안전한 대안으로 향하게 할 수 있다.

잠재적으로 소비자나 대중에게 해를 끼칠 수 있는 다른 산업과 제품들과 마찬가지로, AI 시스템들은 효과적이고 권한을 부여받은 정부 기관들의 신중한 규제를 필요로 한다. 이 규제는 AGI의 내재적 위험을 인식하고, 용인할 수 없을 정도로 위험한 고성능 AI 시스템의 개발을 방지해야 한다.[^22]

하지만, 업계의 반대를 받을 것이 확실한 진짜 효력을 가진 대규모 규제는[^23] 시간이[^24] 걸릴 뿐만 아니라 그것이 필요하다는 정치적 확신도 필요하다.[^25] 진전 속도를 고려할 때, 이는 우리가 가진 시간보다 더 오래 걸릴 수 있다.

규제 조치가 개발되는 동안 훨씬 빠른 시간 척도에서, 우리는 기업들에게 (a) 매우 고위험 활동을 중단하고 (b) 위험을 평가하고 완화하기 위한 포괄적 시스템을 개발하기 위한 필요한 인센티브를 가장 위험한 시스템들에 대한 책임 수준을 명확히 하고 증가시킴으로써 제공할 수 있다. 아이디어는 높은 자율성-범용성-지능의 삼중 교집합에 있는 시스템에 대해서는 가장 높은 수준의 책임(엄격하고 경우에 따라서는 개인적 형사책임)을 부과하되, 이러한 속성 중 하나가 부족하거나 관리 가능하다고 보장되는 시스템에 대해서는 더 일반적인 과실 기반 책임에 대한 "안전 항구"를 제공하는 것이다. 즉, 예를 들어 범용적이고 자율적이지만 "약한" 시스템(유능하고 신뢰할 만하지만 제한된 개인 비서 같은)은 더 낮은 책임 수준을 적용받을 것이다. 마찬가지로 자율주행차 같은 협소하고 자율적인 시스템은 이미 적용받고 있는 상당한 규제를 받겠지만, 강화된 책임은 받지 않을 것이다. 고도로 유능하고 범용적이지만 "수동적"이고 독립적 행동이 대부분 불가능한 시스템도 마찬가지다. 세 가지 속성 중 *둘*이 부족한 시스템들은 더욱 관리하기 쉽고 안전 항구를 주장하기가 더 쉬울 것이다. 이 접근법은 우리가 다른 잠재적으로 위험한 기술들을 다루는 방식을 반영한다:[^26] 더 위험한 구성에 대한 더 높은 책임은 더 안전한 대안에 대한 자연스러운 인센티브를 만든다.

대중에게 AGI 위험을 전가하지 않고 기업에 *내재화*시키는 역할을 하는 이러한 높은 수준의 책임의 기본 결과는 *그들 자신의 리더십*이 위험에 처한 당사자라는 점을 고려할 때, 기업들이 진정으로 신뢰할 만하고 안전하며 통제 가능하게 만들 수 있을 때까지 완전한 AGI를 개발하지 않는 것일 가능성이 높다(그리고 바라건대!). (이것이 충분하지 않을 경우를 대비해, 책임을 명확히 하는 법률은 명백히 위험 지대에 있고 공공 위험을 제기한다고 볼 수 있는 활동에 대해 금지명령 구제, 즉 판사가 중단을 명령하는 것을 명시적으로 허용해야 한다.) 규제가 시행되면서, 규제 준수가 안전 항구가 될 수 있고, AI 시스템의 낮은 자율성, 협소함, 또는 약함으로부터의 안전 항구는 상대적으로 가벼운 규제 체제로 전환될 수 있다.

## 관문 차단의 핵심 조항

위의 논의를 염두에 두고, 이 섹션은 완전한 AGI와 초지능에 대한 금지를 구현하고 유지하며, 완전한 AGI 임계점에 가까운 인간 경쟁력 또는 전문가 경쟁력의 범용 AI를 관리하기 위한 핵심 조항들에 대한 제안을 제공한다.[^27] 네 가지 핵심 요소가 있다: 1) 연산량 계산과 감독, 2) AI 훈련과 운영에서의 연산량 상한선, 3) 책임 프레임워크, 4) 엄격한 규제 요구사항을 포함하는 단계별 안전 및 보안 표준. 이들은 다음에 간결하게 설명되고, 추가 세부사항이나 구현 예시는 세 개의 첨부 표에 제공된다. 중요한 것은, 이들이 고급 AI 시스템을 통제하는 데 필요한 전부와는 거리가 멀다는 점이다. 추가적인 보안과 안전 이익을 가질 것이지만, 이들은 지능 폭주로의 관문을 차단하고 AI 개발을 더 나은 방향으로 전환하는 것을 목표로 한다.

### 1\. 연산량 계산 및 투명성

- 표준 기관(미국의 NIST, 이어서 국제적으로 ISO/IEEE)은 AI 모델 훈련과 운영에 사용된 총 연산량을 FLOP 단위로, 그리고 FLOP/s 단위의 운영 속도에 대한 상세한 기술 표준을 성문화해야 한다. 이것이 어떤 모습일 수 있는지에 대한 세부사항은 부록 A에 제시되어 있다.[^28]
- 대규모 AI 훈련이 이루어지는 관할권은 새로운 법률이나 기존 권한[^29] 하에서 10<sup>25</sup> FLOP 또는 10<sup>18</sup> FLOP/s 임계값을 넘는 모든 모델의 훈련과 운영에 사용된 총 FLOP를 규제 기관이나 다른 기관에 계산하고 보고하도록 하는 요구사항을 부과해야 한다.[^30]
- 이러한 요구사항들은 단계적으로 도입되어야 하는데, 처음에는 분기별로 잘 문서화된 선의의 추정치를 요구하고, 이후 단계에서는 각 모델 *출력*에 첨부된 암호학적으로 증명된 총 FLOP와 FLOP/s까지 점진적으로 더 높은 표준을 요구한다.
- 이러한 보고서들은 각 AI 출력 생성에 사용된 한계 에너지와 재정 비용에 대한 잘 문서화된 추정치로 보완되어야 한다.

근거: 이러한 잘 계산되고 투명하게 보고된 수치들은 훈련과 운영 상한선의 기반을 제공할 뿐만 아니라 더 높은 책임 조치로부터의 안전 항구 역할을 할 것이다(부록 C와 D 참조).

### 2\. 훈련과 운영 연산량 상한선

- AI 시스템을 호스팅하는 관할권들은 모든 AI 모델 출력에 들어가는 총 연산량에 대해 10<sup>27</sup> FLOP[^31]에서 시작하여 적절하게 조정 가능한 엄격한 제한을 부과해야 한다.
- AI 시스템을 호스팅하는 관할권들은 AI 모델 출력의 연산 속도에 대해 10<sup>20</sup> FLOP/s에서 시작하여 적절하게 조정 가능한 엄격한 제한을 부과해야 한다.

근거: 총 연산량은 매우 불완전하지만, 구체적으로 측정 가능하고 검증 가능한 AI 능력(과 위험)의 대리변수이므로, 능력을 제한하기 위한 엄격한 백스톱을 제공한다. 구체적인 구현 제안은 부록 B에 제시되어 있다.

### 3\. 위험한 시스템에 대한 강화된 책임

- 고도로 범용적이고 유능하며 자율적인 고급 AI 시스템의 생성과 운영[^32]은 단일 당사자 과실 기반이 아닌 엄격하고 연대책임에 적용된다는 것을 법률을 통해 법적으로 명확히 해야 한다.[^33]
- 작은(연산량 측면에서), 약한, 협소한, 수동적이거나 충분한 안전, 보안, 통제 가능성 보장을 가진 시스템에 대해 엄격한 책임으로부터 안전 항구를 부여할 적극적 안전 사례를 만드는 법적 절차가 있어야 한다.
- 공공 위험을 구성하는 AI 훈련과 추론 활동을 중단시키기 위한 명시적인 경로와 금지명령 구제 조건들의 집합이 개괄되어야 한다.

근거: AI 시스템들은 책임을 질 수 없으므로, 우리는 그들이 야기한 피해에 대해 인간 개인과 조직을 책임지게 해야 한다(책임).[^34] 통제 불가능한 AGI는 사회와 문명에 위협이 되며, 안전 사례가 없는 한 "비정상적으로 위험한" 것으로 간주되어야 한다. 강력한 모델이 "비정상적으로 위험한" 것으로 간주되지 않을 만큼 충분히 안전하다는 것을 보여줄 책임을 개발자에게 두는 것은 투명성과 기록 보관과 함께 안전한 개발을 장려해 그러한 안전 항구를 주장한다. 그러면 규제가 책임으로부터의 억제력이 불충분한 곳에서 피해를 방지할 수 있다. 마지막으로, AI 개발자들은 이미 그들이 야기한 손해에 대해 책임을 지고 있으므로, 가장 위험한 시스템들에 대한 책임을 법적으로 명확히 하는 것은 매우 자세한 표준이 개발되지 않고도 즉시 이루어질 수 있다. 그러면 이들은 시간이 지나면서 발전할 수 있다. 세부사항은 부록 C에 제시되어 있다.

### 4\. AI에 대한 안전 규제

AI의 대규모 급성 위험을 다루는 규제 시스템은 최소한 다음을 요구할 것이다:

- 적절한 규제 기관들의 식별 또는 창설, 아마도 새로운 기관;
- 포괄적인 위험 평가 프레임워크;[^35]
- 부분적으로 위험 평가 프레임워크에 기반한 개발자들의 적극적 안전 사례와 *독립적인* 집단과 기관들의 감사를 위한 프레임워크;
- 능력 수준을 추적하는 단계별 허가 시스템.[^36] 허가는 시스템의 개발과 배치에 대해 안전 사례와 감사를 기반으로 부여될 것이다. 요구사항은 하위 단계의 통지부터 상위 단계의 개발 전 정량적 안전, 보안, 통제 가능성 보장까지 다양할 것이다. 이들은 안전함이 입증될 때까지 시스템의 출시를 방지하고, 본질적으로 안전하지 않은 시스템의 개발을 금지할 것이다. 부록 D는 그러한 안전과 보안 표준이 무엇을 수반할 수 있는지에 대한 제안을 제공한다.
- 그러한 조치들을 국제 차원으로 가져갈 합의들, 규범과 표준을 조화시킬 국제 기구들, 그리고 잠재적으로 안전 사례를 검토할 국제 기관들을 포함하여.

근거: 궁극적으로, 책임은 새로운 기술로부터 대중에 대한 대규모 위험을 방지하기 위한 올바른 메커니즘이 아니다. 권한을 부여받은 규제 기관들을 갖춘 포괄적인 규제가, 대중에 위험을 제기하는 다른 모든 주요 산업과 마찬가지로 AI에도 필요할 것이다.[^37]

다른 광범위하지만 덜 급성인 위험을 방지하기 위한 규제는 관할권마다 형태가 다를 가능성이 높다. 중요한 것은 이러한 위험들이 관리 불가능할 정도로 위험한 AI 시스템들의 개발을 피하는 것이다.

## 그다음에는 무엇인가?

향후 10년간 AI가 더욱 광범위해지고 핵심 기술이 발전하면서, 두 가지 주요한 일이 일어날 가능성이 높다. 첫째, 기존의 강력한 AI 시스템에 대한 규제가 더 어려워지지만, 훨씬 더 필요해질 것이다. 최소한 일부 대규모 안전 위험을 다루는 조치들은 개별 관할권이 국제 합의에 기반한 규칙을 집행하는 형태로 국제 차원의 합의를 필요로 할 가능성이 높다.

둘째, 하드웨어가 더 저렴해지고 비용 효율성이 높아지면서 훈련과 운영 연산량 상한선을 유지하기가 더 어려워질 것이다. 또한 알고리즘과 아키텍처의 발전으로 관련성이 낮아지거나(또는 더 엄격해져야 하거나) 할 수도 있다.

AI 통제가 더 어려워진다고 해서 포기해야 하는 것은 아니다! 이 에세이에 개괄된 계획을 실행하는 것은 우리에게 귀중한 시간과 과정에 대한 중요한 통제권을 모두 제공해, 우리 사회, 문명, 종족에 대한 AI의 실존적 위험을 피하는 훨씬, 훨씬 더 나은 위치에 놓아줄 것이다.

더 먼 장기적으로는, 우리가 무엇을 허용할 것인가에 대한 선택들이 있을 것이다. 우리는 여전히 이것이 가능한 정도까지 진정으로 통제 가능한 AGI의 어떤 형태를 만들기로 선택할 수 있다. 또는 우리가 기계들이 더 나은 일을 할 것이고 우리를 잘 대우할 것이라고 확신할 수 있다면, 세상을 운영하는 것을 기계에 맡기는 것이 낫다고 결정할 수도 있다. 하지만 이러한 것들은 AI에 대한 깊은 과학적 이해를 손에 넣고, 의미 있는 글로벌하고 포괄적인 논의 후에 내려져야 할 결정들이어야 한다. 인류의 대부분이 완전히 배제되고 인식하지 못하는 가운데 기술 거물들 간의 경쟁에서 내려지는 결정이 아니라.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) 책임과 규제를 통한 A-G-I와 초지능 거버넌스 요약. 자율성, 범용성, 지능의 삼중 교집합에서 책임이 가장 높고 규제가 가장 강하다. 시스템이 약하고/또는 협소하고/또는 수동적임을 입증하는 적극적 안전 사례를 통해 엄격한 책임과 강한 규제로부터 안전 항구를 얻을 수 있다. 하드웨어와 암호화 보안 조치를 사용해 검증되고 집행되는 총 훈련 연산량과 추론 연산량 비율에 대한 상한선은 완전한 AGI를 피하고 초지능을 효과적으로 금지함으로써 안전을 뒷받침한다.

[^1]: 이 깨달음의 확산은 이런 주장을 하는 교육과 옹호 집단들의 강력한 노력이나, 꽤 심각한 AI로 인한 재해 중 하나를 통해 일어날 가능성이 높다. 전자이기를 바랄 수 있다.

[^2]: 역설적으로, 우리는 자연이 우리의 기술을 개발하기 매우 어렵게, 특히 과학적으로 만들어서 제한하는 데 익숙하다. 하지만 그것은 더 이상 AI의 경우가 아니다: 핵심 과학적 문제들이 예상보다 쉬운 것으로 판명되고 있다. 우리는 자연이 여기서 우리를 우리 자신으로부터 구해주는 것에 의존할 수 없다. 우리가 스스로 해야 할 것이다.

[^3]: 정확히 어디서 새로운 시스템 개발을 중단할 것인가? 여기서 우리는 예방 원칙을 채택해야 한다. 시스템이 일단 배치되고, 특히 그 수준의 시스템 능력이 확산되면, 되돌리기가 극도로 어렵다. 그리고 시스템이 *개발되면* (특히 큰 비용과 노력으로), 그것을 사용하거나 배치하라는 엄청난 압력이 있을 것이고, 그것이 유출되거나 도난당할 유혹도 있을 것이다. 시스템을 개발한 *다음에* 그것들이 심각하게 안전하지 않은지를 결정하는 것은 위험한 길이다.

[^4]: 또한 본질적으로 위험한 AI 개발, 예를 들어 자기복제하고 진화하는 시스템, 격리에서 탈출하도록 설계된 것들, 자율적으로 자기개선할 수 있는 것들, 의도적으로 기만적이고 악의적인 AI 등을 금지하는 것이 현명할 것이다.

[^5]: 이것이 반드시 어떤 종류의 글로벌 기구에 의한 국제 차원의 *집행*을 의미하는 것은 아니다: 대신 주권 국가들이 많은 조약에서처럼 합의된 규칙을 집행할 수 있다.

[^6]: 아래에서 보겠지만, AI 연산의 특성상 어느 정도 하이브리드를 허용할 것이다. 하지만 여전히 국제 협력이 필요할 것이다.

[^7]: 예를 들어, AI 관련 칩을 식각하는 데 필요한 기계는 (다른 많은 시도에도 불구하고) 단 하나의 회사인 ASML만이 만들고, 관련 칩의 대부분은 (다른 회사들의 경쟁 시도에도 불구하고) 한 회사인 TSMC가 제조하며, 그 칩들로부터 하드웨어의 설계와 제작은 NVIDIA, AMD, Google을 포함한 소수만이 하고 있다.

[^8]: 가장 중요한 것은, 각 칩이 사물을 "서명"하는 데 사용할 수 있는 고유하고 접근 불가능한 암호화 개인키를 보유한다는 것이다.

[^9]: 기본적으로 이것은 칩을 판매하는 회사가 될 것이지만, 다른 모델들도 가능하고 잠재적으로 유용하다.

[^10]: 통제자는 칩과 서명된 메시지 교환의 타이밍을 통해 칩의 위치를 확인할 수 있다: 유한한 빛의 속도는 칩이 *r* / *c*보다 짧은 시간에 서명된 메시지를 반환할 수 있다면 "스테이션"의 주어진 반경 *r* 내에 있어야 한다는 것을 요구한다. 여기서 *c*는 빛의 속도이다. 여러 스테이션과 네트워크 특성에 대한 일부 이해를 사용하여 칩의 위치를 결정할 수 있다. 이 방법의 아름다움은 대부분의 보안이 물리 법칙에 의해 제공된다는 것이다. 다른 방법들은 GPS, 관성 추적, 그리고 유사한 기술들을 사용할 수 있다.

[^11]: 또는, 칩 쌍들이 통제자의 명시적 허가에 의해서만 서로 소통할 수 있도록 허용될 수 있다.

[^12]: 이는 현재 최소한 칩 간의 매우 높은 대역폭 연결이 대규모 AI 모델을 훈련하는 데 필요하기 때문에 중요하다.

[^13]: 이것은 또한 *N*개의 서로 다른 통제자 중 *M*명으로부터 서명된 메시지를 요구하도록 설정되어, 여러 당사자가 거버넌스를 공유할 수 있게 한다.

[^14]: 이는 전혀 전례가 없는 것이 아니다. 예를 들어 군대들은 복제되거나 유전적으로 조작된 초병사들의 군대를 개발하지 않았는데, 이것이 아마도 기술적으로 가능함에도 불구하고 그렇다. 하지만 그들은 다른 사람들에 의해 방지되기보다는 이것을 하지 않기로 *선택했다*. 주요 세계 강국들이 강하게 개발하기를 원하는 기술의 개발을 다른 사람들이 방지하는 것에 대한 실적은 그리 좋지 않다.

[^15]: 몇 가지 주목할 만한 예외(특히 NVIDIA)를 제외하고, AI 전용 하드웨어는 이러한 회사들의 전체 비즈니스와 수익 모델에서 상대적으로 작은 부분이다. 더욱이, 고급 AI에 사용되는 하드웨어와 "소비자급" 하드웨어 간의 격차는 상당하므로, 대부분의 컴퓨터 하드웨어 소비자들은 크게 영향을 받지 않을 것이다.

[^16]: 더 자세한 분석을 위해서는 [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html)와 [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips)의 최근 보고서를 참조하라. 이들은 특히 다른 국가들의 고급 연산 능력을 제한하려는 미국 수출 통제의 맥락에서 기술적 실현 가능성에 초점을 맞추고 있다. 하지만 이는 여기서 구상하는 글로벌 제약과 명백한 중복이 있다.

[^17]: 예를 들어, 애플 기기들은 분실이나 도난이 신고될 때 원격으로 안전하게 잠기고, 원격으로 재활성화될 수 있다. 이는 여기서 논의된 동일한 하드웨어 보안 기능에 의존한다.

[^18]: 예를 들어 IBM의 [용량 온디맨드](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) 제공, Intel의 [Intel 온디맨드](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html), 그리고 Apple의 [프라이빗 클라우드 컴퓨트](https://security.apple.com/blog/private-cloud-compute/)를 참조하라.

[^19]: [이 연구](https://epochai.org/trends#hardware-trends-section)는 역사적으로 동일한 성능이 연간 약 30% 적은 비용으로 달성되어 왔다는 것을 보여준다. 이 추세가 계속된다면, AI와 "소비자" 칩 사용 간에 상당한 중복이 있을 수 있고, 일반적으로 고성능 AI 시스템에 필요한 하드웨어의 양이 불편할 정도로 작아질 수 있다.

[^20]: [동일한 연구](https://epochai.org/trends#hardware-trends-section)에 따르면, 이미지 인식에서 주어진 성능은 매년 2.5배 적은 연산을 필요로 해왔다. 이것이 가장 뛰어난 AI 시스템에도 적용된다면, 연산 제한은 오래 유용하지 않을 것이다.

[^21]: 특히, 국가 차원에서 이것은 정부가 연산 능력이 어떻게 사용되는지에 대해 많은 통제권을 가진다는 점에서 연산의 국유화와 많이 비슷해 보인다. 하지만 정부 개입을 걱정하는 사람들에게는, 이것이 일부가 옹호하기 시작하는 주요 AI 회사들과 국가 정부 간의 어떤 합병을 통해 가장 강력한 AI 소프트웨어 *자체*가 국유화되는 것보다 훨씬 안전하고 선호할 만해 보인다.

[^22]: 유럽에서 주요한 규제 단계는 2024년 [EU AI 법](https://artificialintelligenceact.eu/) 통과와 함께 이루어졌다. 이는 AI를 위험에 따라 분류한다: 용인할 수 없는 시스템을 금지하고, 고위험 시스템을 규제하며, 저위험 시스템에 투명성 규칙을 부과하거나 전혀 조치를 취하지 않는다. 이는 일부 AI 위험을 상당히 줄이고, 미국 회사들에 대해서도 AI 투명성을 높일 것이지만, 두 가지 주요 결함이 있다. 첫째, 제한된 범위: EU에서 AI를 제공하는 모든 회사에 적용되지만, 미국 기반 회사들에 대한 집행은 약하고, 군사 AI는 면제된다. 둘째, GPAI를 다루지만, AGI나 초지능을 용인할 수 없는 위험으로 인식하거나 그들의 개발을 방지하지 못한다. 단지 EU 배치만 방지한다. 결과적으로, 그것은 AGI나 초지능의 위험을 억제하는 데 거의 도움이 되지 않는다.

[^23]: 기업들은 종종 합리적인 규제에 찬성한다고 표명한다. 하지만 어떻게든 그들은 거의 항상 모든 *특정한* 규제에 반대하는 것 같다. [대부분의 AI 회사들이 공개적으로 또는 비공개적으로 반대한](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/) 상당히 낮은 접촉의 SB1047에 대한 싸움을 보라.

[^24]: EU AI 법이 제안된 시점부터 효력이 발생할 때까지 약 3년 반이 걸렸다.

[^25]: AI 규제를 시작하기에는 "너무 이르다"는 표현을 하는 경우가 있다. 마지막 각주를 고려하면, 그럴 가능성은 거의 없어 보인다. 또 다른 표명된 우려는 규제가 "혁신을 해칠" 것이라는 것이다. 하지만 좋은 규제는 혁신의 양이 아니라 방향만 바꾼다.

[^26]: 흥미로운 선례는 탈출하여 손해를 일으킬 수 있는 위험물질의 운송에 있다. 여기서 [규제](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442)와 [판례법](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf)은 폭발물, 가솔린, 독, 감염 물질, 방사성 폐기물과 같은 매우 위험한 물질에 대해 엄격한 책임을 확립했다. 다른 예로는 [의약품 경고](https://www.medicalnewstoday.com/articles/boxed-warnings), [의료기기 분류](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification) 등이 있다.

[^27]: 유사한 목표를 가진 또 다른 포괄적 제안인 ["좁은 길"](https://www.narrowpath.co/)은 모든 첨단 AI 개발을 강력한 국제 제도에 의해 감독되는 단일 국제 주체를 통해 전달하는 더 중앙집권적이고 금지 기반 접근법을 옹호하며, 단계적 제한보다는 명확한 범주적 금지를 제시한다. 나도 그 계획을 지지한다. 하지만 그것은 여기서 제안하는 것보다 훨씬 더 많은 정치적 의지와 조정을 필요로 할 것이다.

[^28]: 그러한 표준에 대한 일부 가이드라인이 프론티어 모델 포럼에 의해 [발표되었다](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/). 여기서의 제안과 비교해, 그들은 덜 정밀하고 집계에 포함되는 연산량이 적은 쪽에서 오류가 난다.

[^29]: 2023년 미국 AI 행정명령(현재 취소됨)은 유사하지만 덜 세분화된 보고를 요구했다. 이는 대체 명령에 의해 강화되어야 한다.

[^30]: 매우 대략적으로, 현재 일반적인 H100 칩에 대해 이것은 추론을 하는 약 1000개의 클러스터에 해당한다. 이는 추론을 하는 최신 최고급 NVIDIA B200 칩의 약 100개 (약 500만 달러 상당)이다. 두 경우 모두 훈련 수는 그 클러스터가 몇 달간 연산하는 것에 해당한다.

[^31]: 이 양은 현재 훈련된 어떤 AI 시스템보다도 크다. AI 능력이 연산량과 어떻게 비례하는지 더 잘 이해하게 되면 더 크거나 작은 수가 정당화될 수 있다.

[^32]: 이는 모델을 생성하고 제공/호스팅하는 사람들에게 적용되며, 최종 사용자가 아니다.

[^33]: 대략, "엄격한" 책임은 개발자들이 제품에 의해 야기된 피해에 대해 *기본적으로* 책임을 진다는 의미이며, "비정상적으로 위험한" 제품에 사용되는 표준이고, (다소 재미있지만 적절하게도) 야생동물에도 사용된다. "연대" 책임은 책임이 제품에 책임이 있는 모든 당사자에게 할당되고, 그 당사자들이 그들 사이에서 누가 어떤 책임을 지는지 정리해야 한다는 의미다. 이는 긴 복잡한 가치 사슬을 가진 AI 같은 시스템에 중요하다.

[^34]: 표준 과실 기반 단일 당사자 책임으로는