# 9장 - 미래 설계 — 우리가 대신 해야 할 일

AI는 세상에 놀라운 선익을 가져다줄 수 있다. 위험 없이 모든 혜택을 얻으려면, AI가 인간의 도구로 남도록 해야 한다.

인류를 기계로 대체하지 않기로 성공적으로 선택한다면 — 적어도 당분간은! — 우리는 무엇을 할 수 있을까? AI 기술의 거대한 가능성을 포기해야 할까? 어떤 면에서는 답은 간단한 *아니오*다: 통제 불가능한 AGI와 초지능으로 향하는 관문을 차단하되, 다른 많은 형태의 AI는 *구축하고*, 이를 관리하는 데 필요한 거버넌스 구조와 제도도 함께 만들어야 한다.

하지만 여전히 할 말이 많다. 이를 실현하는 것은 인류의 핵심 과업이 될 것이다. 이 섹션에서는 몇 가지 핵심 주제를 탐구한다:

- "도구형" AI를 어떻게 특성화하고 어떤 형태를 취할 수 있는지
- AGI 없이도 도구형 AI로 인류가 원하는 (거의) 모든 것을 얻을 수 있다는 점
- 도구형 AI 시스템이 (원칙적으로, 아마도) 관리 가능하다는 점
- AGI를 포기하는 것이 국가안보를 저해하지 않는다는 점 — 오히려 그 반대
- 권력 집중이 실제 우려사항이라는 점. 안전과 보안을 훼손하지 않으면서 이를 완화할 수 있을까?
- 새로운 거버넌스와 사회 구조가 필요하다는 점, 그리고 AI가 실제로 도움이 될 수 있다는 점

## 관문 안의 AI: 도구형 AI

삼중 교집합 다이어그램은 "도구형 AI"라고 부를 수 있는 것을 구분하는 좋은 방법을 제공한다: 통제 불가능한 경쟁자나 대체재가 아닌, 인간이 사용할 수 있는 통제 가능한 도구인 AI다. 가장 문제가 적은 AI 시스템은 자율적이지만 일반적이지 않거나 초능력을 갖지 않는 것(경매 입찰 봇 같은), 일반적이지만 자율적이지 않거나 능력이 없는 것(소형 언어모델 같은), 또는 능력이 있지만 좁고 매우 통제 가능한 것(알파고 같은)이다.[^1] 두 가지 교차 특성을 가진 것들은 더 넓은 응용 가능성을 갖지만 위험이 높아 관리하려면 큰 노력이 필요하다. (AI 시스템이 더 도구적이라고 해서 본질적으로 안전한 것은 아니며, 단지 본질적으로 *위험하지* 않을 뿐이다 — 전기톱과 애완 호랑이를 생각해보라.) 삼중 교집합의 (완전한) AGI와 초지능으로 향하는 관문은 닫힌 상태를 유지해야 하며, 그 임계점에 접근하는 AI 시스템에는 막대한 주의를 기울여야 한다.

하지만 이것도 여전히 강력한 AI를 많이 남겨둔다! 우리는 똑똑하고 일반적인 수동적 "오라클"과 좁은 시스템들, 초인간적이 아닌 인간 수준의 일반 시스템 등으로부터 엄청난 효용을 얻을 수 있다. 많은 기술 회사와 개발자들이 적극적으로 이런 종류의 도구를 구축하고 있으며 계속해야 한다. 대부분의 사람들처럼 그들은 AGI와 초지능으로 향하는 관문이 닫힐 것이라고 암묵적으로 *가정하고* 있다.[^2]

또한 AI 시스템들은 인간의 감독을 유지하면서 능력을 향상시키는 복합 시스템으로 효과적으로 결합될 수 있다. 불가해한 블랙박스에 의존하기보다는, AI와 전통적인 소프트웨어를 모두 포함한 여러 구성 요소가 인간이 모니터링하고 이해할 수 있는 방식으로 함께 작동하는 시스템을 구축할 수 있다.[^3] 일부 구성 요소는 블랙박스일 수 있지만, 어느 것도 AGI에 근접하지 않을 것이다 — 복합 시스템 전체만이 매우 일반적이면서 매우 능력이 있되, 엄격하게 통제 가능한 방식으로 그럴 것이다.[^4]

### 의미 있고 보장된 인간 통제

"엄격하게 통제 가능하다"는 것은 무엇을 의미하는가? "도구" 프레임워크의 핵심 아이디어는 — 아주 일반적이고 강력하더라도 — 의미 있는 인간 통제 하에 있음이 보장되는 시스템을 허용하는 것이다. 이것은 무엇을 의미하는가? 두 가지 측면이 있다. 첫 번째는 설계 고려사항이다: 인간이 시스템이 하는 일에 깊숙이 그리고 중심적으로 관여해야 하며, 핵심적인 중요한 결정을 AI에 *위임하지* 않아야 한다. 이것이 현재 대부분 AI 시스템의 특성이다. 둘째, AI 시스템이 자율적인 정도까지, 행동 범위를 제한하는 보장이 있어야 한다. 보장은 어떤 일이 일어날 확률을 특성화하는 *수치*와 그 수치를 믿을 이유여야 한다. 이것이 우리가 다른 안전 중요 분야에서 요구하는 것으로, "고장 간 평균 시간"과 예상 사고 수 같은 수치가 계산되고, 뒷받침되며, 안전 사례에서 공개된다.[^5] 고장의 이상적인 수치는 물론 0이다. 그리고 좋은 소식은 상당히 다른 AI 아키텍처를 사용하여 프로그램(AI 포함)의 *형식적으로 검증된* 속성이라는 아이디어를 사용하면 꽤 가까워질 수 있다는 것이다. Omohundro, Tegmark, Bengio, Dalrymple 등이 자세히 탐구한 아이디어([여기](https://arxiv.org/abs/2309.01933)와 [여기](https://arxiv.org/abs/2405.06624) 참조)는 특정 속성(예: 인간이 시스템을 끌 수 있다는 것)을 가진 프로그램을 구성하고 그러한 속성이 성립함을 형식적으로 *증명하는* 것이다. 이것은 현재 아주 짧은 프로그램과 간단한 속성에 대해서는 가능하지만, AI 기반 증명 소프트웨어의 (다가오는) 힘은 훨씬 복잡한 프로그램(예: 래퍼)과 심지어 AI 자체에 대해서도 가능하게 할 수 있다. 이것은 매우 야심찬 프로그램이지만, 관문에 대한 압력이 커지면서 우리는 그것을 강화하는 강력한 재료들이 필요할 것이다. 수학적 증명이 충분히 강한 몇 안 되는 것 중 하나일 수 있다.

### AI 산업의 향방

AI 진전이 방향을 바꾸더라도, 도구형 AI는 여전히 엄청난 산업이 될 것이다. 하드웨어 측면에서는 초지능을 방지하는 연산량 상한선이 있더라도, 더 작은 모델의 훈련과 추론에는 여전히 엄청난 양의 특수 부품이 필요할 것이다. 소프트웨어 측면에서는, AI 모델과 연산량 크기의 폭발을 억제하는 것이 단순히 더 크게 만들기보다는 더 작은 시스템을 더 좋고, 더 다양하며, 더 전문화되게 만드는 쪽으로 회사들이 자원을 재투입하게 할 것이다.[^6] 돈벌이를 하는 실리콘 밸리 스타트업들을 위한 충분한 — 아마도 더 많은 — 여지가 있을 것이다.[^7]

## 도구형 AI는 AGI 없이도 인류가 원하는 (거의) 모든 것을 제공할 수 있다

생물학적이든 기계적이든 지능은 일반적으로 목표 집합과 더 일치하는 미래를 가져오는 활동을 계획하고 실행하는 능력으로 간주될 수 있다. 따라서 지능은 현명하게 선택된 목표를 추구하는 데 사용될 때 엄청난 이익이 된다. 인공지능이 약속된 혜택 때문에 시간과 노력의 막대한 투자를 끌어들이는 것도 주로 이 때문이다. 그래서 우리는 물어야 한다: 초지능으로의 폭주를 억제한다면 우리는 AI의 혜택을 어느 정도까지 여전히 얻을 수 있을까? 답은: 놀랍도록 적게 잃을 수도 있다는 것이다.

먼저 현재 AI 시스템이 이미 매우 강력하고, 우리는 정말로 그것들로 할 수 있는 일의 표면만 긁었다는 것을 생각해보자.[^8] 그것들은 제시된 질문이나 과제를 "이해"하고, 그 질문에 답하거나 그 과제를 수행하는 데 필요한 것이 무엇인지 파악하는 면에서 "쇼를 운영하는" 능력이 상당히 있다.

다음으로, 현대 AI 시스템에 대한 흥분의 대부분은 그것들의 일반성 때문이지만, 가장 능력 있는 AI 시스템들 중 일부 — 음성이나 이미지를 생성하거나 인식하고, 과학적 예측과 모델링을 하며, 게임을 하는 등의 시스템들 — 은 훨씬 더 좁고 연산량 면에서 "관문 안에" 잘 있다.[^9] 이러한 시스템들은 그들이 수행하는 특정 과제에서 초인간적이다. 그것들의 좁음 때문에 엣지 케이스[^10] (또는 [악용 가능한](https://arxiv.org/abs/2211.00241)) 약점이 있을 수 있다. 하지만 *완전히* 좁거나 *완전히* 일반적인 것만이 유일한 선택지는 아니다: 그 사이에는 많은 아키텍처가 있다.[^11]

이러한 AI 도구들은 AGI 없이도 다른 긍정적 기술의 발전을 크게 가속화할 수 있다. 더 나은 핵물리학을 하려고 해도, AI가 핵물리학자가 될 필요는 없다 — 우리에게는 그런 사람들이 있다! 의학을 발전시키려면, 생물학자, 의학 연구자, 화학자들에게 강력한 도구를 주면 된다. 그들은 그것을 원하고 엄청난 이득을 위해 사용할 것이다. 서버 팜을 가득 채운 백만 명의 디지털 천재가 필요한 것이 아니다. 우리에게는 AI가 그들의 천재성을 발휘하도록 도울 수 있는 수백만 명의 인간이 있다. 그렇다, 불멸과 모든 질병의 치료법을 얻는 데는 더 오래 걸릴 것이다. 이것은 실제 비용이다. 하지만 가장 유망한 건강 혁신조차 AI 주도의 불안정이 전 세계적 갈등이나 사회 붕괴로 이어진다면 거의 소용없을 것이다. 우리는 AI로 강화된 인간들이 먼저 문제에 도전할 기회를 주어야 할 의무가 있다.

그리고 실제로 관문 안 도구로는 얻을 수 없는 AGI의 엄청난 장점이 있다고 가정해보자. *결코* AGI와 초지능을 구축하지 않음으로써 그런 것들을 잃게 될까? 여기서 위험과 보상을 저울질할 때, 서두르는 것 대 기다리는 것에는 엄청난 비대칭적 이익이 있다: 보장된 안전하고 유익한 방식으로 할 수 있을 때까지 기다릴 수 있고, 거의 모든 사람이 여전히 보상을 거둘 수 있다. 서두르면 OpenAI CEO 샘 알트만의 말로는 [우리 *모두*에게 등불이 꺼질](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1) 수 있다.

하지만 비AGI 도구들이 잠재적으로 그렇게 강력하다면, 우리가 그것들을 관리할 수 있을까? 답은 명확한...아마도이다.

## 도구형 AI 시스템은 (아마도, 원칙적으로) 관리 가능하다

하지만 쉽지는 않을 것이다. 현재 최첨단 AI 시스템은 사람과 기관이 목표를 달성하는 데 크게 힘을 실어줄 수 있다. 이것은 일반적으로 좋은 일이다! 하지만 그런 시스템을 갑작스럽게 그리고 사회가 적응할 시간이 많지 않은 상태에서 우리가 활용하게 되는 자연스러운 역학이 있어, 관문 차단을 가정하고 관리해야 할 심각한 위험을 제공한다. 그런 위험의 몇 가지 주요 부류와 어떻게 줄일 수 있는지 논의할 가치가 있다.

한 부류의 위험은 고성능 도구형 AI가 이전에는 개인이나 조직에 묶여 있던 지식이나 능력에 대한 접근을 허용하여, 고능력과 고충성도의 조합을 매우 광범위한 행위자들이 사용할 수 있게 하는 것이다. 오늘날 충분한 돈이 있으면 악의적인 사람이 화학자 팀을 고용해 새로운 화학 무기를 설계하고 생산하게 할 수 있지만 — 그런 돈을 갖거나 팀을 찾아서/조립해서 명백히 불법적이고, 비윤리적이며, 위험한 일을 하도록 설득하는 것이 그리 쉽지는 않다. AI 시스템이 그런 역할을 하지 못하도록 하려면, 모든 시스템과 그에 대한 접근이 책임감 있게 관리되는 한, 현재 방법의 개선으로도 충분할 수 있다.[^12] 반면에 강력한 시스템이 일반 사용과 수정을 위해 공개된다면, 내장된 안전 조치들은 제거될 가능성이 높다. 따라서 이 부류의 위험을 피하려면, 공개될 수 있는 것에 대한 강력한 제한 — 핵, 폭발물 및 기타 위험한 기술의 세부사항에 대한 제한과 유사한 — 이 필요할 것이다.[^13]

두 번째 위험 부류는 사람처럼 행동하거나 사람을 사칭하는 기계의 확산에서 나온다. 개인에게 해를 끼치는 수준에서는, 이러한 위험에는 훨씬 더 효과적인 사기, 스팸, 피싱과 비동의 딥페이크의 확산이 포함된다.[^14] 집단적 수준에서는 공적 토론과 논쟁, 우리 사회의 정보와 지식 수집, 처리, 배포 시스템, 그리고 우리의 정치적 선택 시스템 같은 핵심 사회 과정의 파괴가 포함된다. 이 위험을 완화하려면 (a) AI 시스템의 사람 사칭을 제한하는 법률과 그런 사칭을 생성하는 시스템을 만드는 AI 개발자에게 책임을 묻는 것, (b) (책임감 있게) 생성된 AI 콘텐츠를 식별하고 분류하는 워터마킹과 출처 추적 시스템, (c) 데이터(예: 카메라와 녹음)에서 사실, 이해, 그리고 좋은 세계 모델까지 신뢰할 수 있는 연쇄를 만들 수 있는 새로운 사회-기술적 인식론적 시스템이 포함될 것 같다.[^15] 이 모든 것이 가능하고, AI가 그 일부를 도울 수 있다.

세 번째 일반적 위험은 일부 과제가 자동화되는 정도에서 현재 그러한 과제를 수행하는 인간의 노동으로서의 재정적 가치가 낮아질 수 있다는 것이다. 역사적으로 과제를 자동화하는 것은 그러한 과제로 가능해진 것들을 더 저렴하고 풍부하게 만들면서, 이전에 그러한 과제를 수행했던 사람들을 자동화된 버전에 여전히 관여하는 사람들(일반적으로 더 높은 기술/급여)과 노동의 가치가 적거나 거의 없는 사람들로 분류했다. 전체적으로는 결과적으로 더 크지만 더 효율적인 부문에서 더 많은 대 더 적은 인간 노동이 필요할 부문을 예측하기 어렵다. 동시에 자동화 역학은 불평등과 일반 생산성을 증가시키고, (효율성 증가를 통해) 특정 상품과 서비스의 비용을 낮추며, (비용 질병을 통해) 다른 것들의 비용을 증가시키는 경향이 있다. 불평등 증가의 불리한 쪽에 있는 사람들에게는 그 특정 상품과 서비스의 비용 감소가 다른 것들의 증가를 상쇄하고 전반적인 더 큰 복지로 이어지는지 매우 불분명하다. 그렇다면 AI에서는 이것이 어떻게 될까? 인간의 지적 노동이 일반 AI로 대체될 수 있는 상대적 용이함 때문에, 우리는 인간 경쟁력을 갖춘 일반 목적 AI와 함께 이것의 급속한 버전을 예상할 수 있다.[^16] AGI로의 관문을 닫으면 AI 에이전트로 도매로 대체될 일자리는 훨씬 적어질 것이지만, 여전히 수년에 걸쳐 대규모 노동 대체는 일어날 가능성이 높다.[^17] 광범위한 경제적 고통을 피하려면 어떤 형태의 보편적 기본 자산이나 소득을 실시하고, 자동화하기 어려운 인간 중심 노동을 가치 있게 여기고 보상하는 문화적 전환을 설계하는 것(경제의 다른 부분에서 밀려난 사용 가능한 노동의 증가로 인해 노동 가격이 하락하는 것을 보기보다는)이 필요할 것이다. ["데이터 존엄성"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society)(훈련 데이터의 인간 생산자들이 그 데이터가 AI에서 창출하는 가치에 대해 자동으로 로열티를 받는) 같은 다른 구성체들도 도움이 될 수 있다. AI에 의한 자동화는 또한 *부적절한* 자동화라는 두 번째 잠재적 역효과를 갖는다. AI가 단순히 더 나쁜 일을 하는 응용과 함께, 여기에는 AI 시스템이 도덕적, 윤리적, 또는 법적 규율을 위반할 가능성이 높은 것들 — 예를 들어 생사 결정과 사법 문제에서 — 이 포함될 것이다. 이것들은 현재의 법적 프레임워크를 적용하고 확장하여 다루어져야 한다.

마지막으로, 관문 안 AI의 중대한 위협은 개인화된 설득, 주의 끌기, 조작에서의 사용이다. 우리는 소셜 미디어와 기타 온라인 플랫폼에서 깊이 뿌리내린 관심 경제(온라인 서비스들이 사용자 관심을 위해 치열하게 경쟁하는)와 ["감시 자본주의"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism) 시스템(사용자 정보와 프로파일링이 관심의 상품화에 추가되는)의 성장을 보았다. 더 많은 AI가 둘 다의 서비스에 투입될 것은 거의 확실하다. AI는 이미 중독성 피드 알고리즘에 많이 사용되고 있지만, 이는 한 개인이 강박적으로 소비하도록 맞춤화된 중독성 AI 생성 콘텐츠로 진화할 것이다. 그리고 그 사람의 입력, 응답, 데이터는 악순환을 계속하기 위해 관심/광고 기계에 투입될 것이다. 또한 기술 회사들이 제공하는 AI 도우미들이 더 많은 온라인 생활의 인터페이스가 되면서, 그들은 아마도 설득과 고객 수익화가 발생하는 메커니즘으로서 검색 엔진과 피드를 대체할 것이다. 지금까지 이러한 역학을 통제하는 데 실패한 우리 사회는 좋은 징조가 아니다. 이 역학의 일부는 프라이버시, 데이터 권리, 조작에 관한 규정을 통해 줄어들 수 있다. 문제의 근원에 더 가까이 가려면 충성스러운 AI 어시스턴트(아래에서 논의) 같은 다른 관점이 필요할 수 있다.

이 논의의 요점은 희망이다: 관문 안 도구 기반 시스템들은 — 적어도 그들이 오늘날의 가장 최첨단 시스템들과 비교할 만한 힘과 능력에 머무르는 한 — 그렇게 할 의지와 조정이 있다면 아마도 관리 가능하다. AI 도구로 강화된 괜찮은 인간 제도들이[^18] 해낼 수 있다. 우리는 또한 그것을 하는 데 실패할 수도 있다. 하지만 더 강력한 시스템을 허용하는 것이 어떻게 도움이 될지 보기 어렵다 — 그들을 책임자로 만들고 최선을 희망하는 것 말고는.

## 국가안보

AI 우위를 위한 경쟁 — 국가안보나 기타 동기에 의해 주도되는 — 은 우리를 권력을 부여하기보다는 흡수하는 경향이 있는 통제되지 않은 강력한 AI 시스템으로 이끈다. 미국과 중국 간의 AGI 경쟁은 어느 국가가 초지능을 먼저 얻는지를 결정하는 경쟁이다.

그렇다면 국가안보를 담당하는 사람들은 대신 무엇을 해야 할까? 정부는 통제 가능하고 보안이 되는 시스템을 구축하는 강력한 경험을 가지고 있으며, AI에서 그렇게 하는 것을 두 배로 강화하고, 규모에서 그리고 정부의 권위로 할 때 가장 잘 성공하는 종류의 인프라 프로젝트를 지원해야 한다.

AGI를 향한 무모한 "맨해튼 프로젝트" 대신,[^19] 미국 정부는 통제 가능하고, 안전하며, 신뢰할 수 있는 시스템을 위한 아폴로 프로젝트를 시작할 수 있다. 여기에는 예를 들어:

- (a) 온칩 하드웨어 보안 메커니즘을 개발하고 (b) 강력한 AI의 연산량 측면을 관리할 인프라를 개발하는 주요 프로그램. 이들은 미국의 [CHIPS 법](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local)과 [수출 통제 체제](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion)를 기반으로 구축될 수 있다.
- AI 시스템의 특정 기능(끄기 스위치 같은)이 존재하거나 부재함을 *증명*할 수 있도록 형식적 검증 기법을 개발하는 대규모 이니셔티브. 이것은 AI 자체를 활용하여 속성의 증명을 개발할 수 있다.
- AI 도구를 활용하여 기존 소프트웨어를 검증 가능하게 보안이 되는 프레임워크로 다시 코딩할 수 있는 검증 가능하게 보안이 되는 소프트웨어를 만드는 국가 규모의 노력.
- DOE, NSF, NIH 간의 파트너십으로 운영되는 AI를 사용한 과학 발전의 국가 투자 프로젝트.[^20]

일반적으로, AI와 그 오용으로부터의 위험에 우리 사회를 취약하게 만드는 엄청난 공격 표면이 있다. 이러한 위험 중 일부로부터 보호하려면 정부 규모의 투자와 표준화가 필요할 것이다. 이들은 AGI를 향한 경쟁의 불에 기름을 붓는 것보다 훨씬 더 많은 보안을 제공할 것이다. 그리고 AI가 무기와 지휘통제 시스템에 구축될 것이라면, AI가 신뢰할 수 있고 보안이 되는 것이 중요한데, 현재의 AI는 그렇지 않다.

## 권력 집중과 그 완화책

이 글은 AI의 인간 통제와 그 잠재적 실패 아이디어에 초점을 맞췄다. 하지만 AI 상황을 보는 또 다른 유효한 렌즈는 *권력 집중*을 통해서다. 매우 강력한 AI의 개발은 권력을 그것을 개발하고 통제할 극소수의 매우 큰 기업 손이나, AI를 자신의 권력과 통제를 유지하는 새로운 수단으로 사용하는 정부들이나, AI 시스템 자체로 집중시킬 위협이 있다. 또는 위의 어떤 불경한 혼합체로. 이 중 어떤 경우든 대부분의 인류는 권력, 통제, 주체성을 잃는다. 어떻게 이에 맞설 수 있을까?

물론 첫 번째이자 가장 중요한 단계는 인간보다 똑똑한 AGI와 초지능으로의 관문 차단이다. 이것들은 명시적으로 인간과 인간 그룹을 직접 대체할 수 있다. 만약 그들이 기업이나 정부 통제 하에 있다면 그 기업이나 정부로 권력을 집중시킬 것이고, "자유롭다면" 자신들로 권력을 집중시킬 것이다. 그러니 관문이 닫혔다고 가정하자. 그럼 다음은?

권력 집중에 대한 하나의 제안된 해결책은 "오픈소스" AI로, 모델 가중치가 자유롭게 또는 널리 사용 가능한 것이다. 하지만 앞서 언급했듯이, 모델이 오픈되면 대부분의 안전 조치나 가드레일이 (그리고 일반적으로) 제거될 수 있다. 따라서 한편으로는 탈중앙화와 다른 한편으로는 안전, 보안, AI 시스템의 인간 통제 사이에 심각한 긴장이 있다. 오픈 모델이 운영 체제에서 그랬듯이 AI에서 권력 집중과 의미 있게 맞설 것이라는 이유도 회의적이다(오픈 대안이 있음에도 여전히 마이크로소프트, 애플, 구글이 지배한다).[^21]

그러나 이 원을 제곱하는 방법이 있을 수 있다 — 위험을 중앙집권화하고 완화하면서 능력과 경제적 보상을 탈중앙집권화하는. 이는 AI가 어떻게 개발되고 그 혜택이 어떻게 배분되는지를 재고하는 것을 요구한다.

AI 개발과 소유의 새로운 모델들이 도움이 될 것이다. 이것은 여러 형태를 취할 수 있다: 정부 개발 AI(민주적 감독 대상),[^22] 비영리 AI 개발 조직(브라우저의 Mozilla 같은), 또는 매우 광범위한 소유권과 거버넌스를 가능하게 하는 구조. 핵심은 이러한 기관들이 강력한 안전 제약 하에서 작동하면서 공익에 봉사하도록 명시적으로 인가되는 것이다.[^23] 잘 만들어진 규제와 표준/인증 체제도 중요할 것인데, AI 제품들이 활발한 시장에서 제공될 때 사용자에게 착취적이기보다는 진정으로 유용하게 남도록 하기 위해서다.

경제적 권력 집중 측면에서, 우리는 출처 추적과 "데이터 존엄성"을 사용하여 경제적 혜택이 더 널리 흐르도록 할 수 있다. 특히 현재와 (관문을 닫힌 상태로 유지한다면) 미래의 대부분 AI 힘은 인간이 생성한 데이터, 즉 직접적인 훈련 데이터든 인간 피드백이든에서 나온다. AI 회사들이 데이터 제공자에게 공정하게 보상하도록 요구된다면,[^24] 이것은 적어도 경제적 보상을 더 넓게 배분하는 데 도움이 될 수 있다. 이를 넘어서, 또 다른 모델은 대형 AI 회사들의 상당한 지분을 공적으로 소유하는 것일 수 있다. 예를 들어, AI 회사들에 세금을 부과할 수 있는 정부들이 수입의 일부를 회사들의 주식을 보유하고 국민에게 배당금을 지급하는 국부펀드에 투자할 수 있다.[^25]

이러한 메커니즘에서 중요한 것은 단순히 비AI 수단을 사용하여 AI 주도 권력 집중과 싸우기보다는, AI 자체의 힘을 사용하여 권력을 더 잘 배분하는 데 도움을 주는 것이다. 하나의 강력한 접근법은 사용자에게 진정한 수탁 의무를 가지고 작동하는 — 특히 기업 제공자들보다 사용자의 이익을 우선시하는 — 잘 설계된 AI 어시스턴트를 통해서일 것이다.[^26] 이러한 어시스턴트들은 진정으로 신뢰할 수 있고, 기술적으로 유능하면서도 사용 사례와 위험 수준에 따라 적절하게 제한되며, 공적, 비영리, 또는 인증된 영리 채널을 통해 모든 사람이 널리 사용할 수 있어야 한다. 우리가 다른 당사자를 위해 우리 이익에 반해 몰래 일하는 인간 어시스턴트를 결코 받아들이지 않을 것처럼, 기업 이익을 위해 사용자를 감시하고, 조작하거나, 가치를 추출하는 AI 어시스턴트를 받아들여서는 안 된다.

그런 변화는 개인들이 인간 복지보다 가치 추출을 우선시하는 거대한 (AI로 강화된) 기업 및 관료적 기계들과 혼자 협상하도록 남겨지는 현재의 역학을 근본적으로 바꿀 것이다. AI 주도 권력을 더 넓게 재배분하는 많은 가능한 접근법이 있지만, 어느 것도 기본적으로 나타나지 않을 것이다: 수탁 요구사항, 공적 제공, 위험에 기반한 계층화된 접근 같은 메커니즘으로 의도적으로 설계되고 관리되어야 한다.

권력 집중을 완화하는 접근법들은 기존 권력들로부터 상당한 역풍에 직면할 수 있다.[^27] 하지만 안전과 집중된 권력 사이에서 선택할 필요가 없는 AI 개발로의 경로들이 있다. 지금 올바른 제도를 구축함으로써, 우리는 AI의 위험이 신중하게 관리되는 동안 AI의 혜택이 널리 공유되도록 할 수 있다.

## 새로운 거버넌스와 사회 구조

우리의 현재 거버넌스 구조는 어려움을 겪고 있다: 대응이 느리고, 종종 특수 이익에 포획되며, [공중의 신뢰가 점점 줄어들고 있다.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) 그러나 이것이 그것들을 포기해야 할 이유는 아니다 — 오히려 그 반대다. 일부 제도는 교체가 필요할 수 있지만, 더 넓게는 우리의 기존 구조를 강화하고 보완할 수 있는 새로운 메커니즘이 필요하며, 빠르게 진화하는 세계에서 더 잘 기능하도록 도울 수 있다.

우리 제도적 약점의 대부분은 형식적 정부 구조가 아니라 퇴화된 사회 제도에서 나온다: 공유된 이해를 개발하고, 행동을 조정하며, 의미 있는 담론을 수행하는 우리의 시스템. 지금까지 AI는 이 퇴화를 가속화하여, 우리의 정보 채널을 생성된 콘텐츠로 범람시키고, 가장 양극화되고 분열적인 콘텐츠를 가리키며, 진실과 허구를 구별하기 더 어렵게 만들었다.

하지만 AI는 실제로 이러한 사회 제도를 재건하고 강화하는 데 도움이 될 수 있다. 세 가지 중요한 영역을 고려해보자:

첫째, AI는 우리의 인식론적 시스템 — 무엇이 참인지 아는 방법 — 에 대한 신뢰를 회복하는 데 도움이 될 수 있다. 원시 데이터에서 분석을 거쳐 결론까지 정보의 출처를 추적하고 검증하는 AI 기반 시스템을 개발할 수 있다. 이러한 시스템은 암호화 검증과 정교한 분석을 결합하여 사람들이 무언가가 참인지뿐만 아니라 그것이 참이라는 것을 어떻게 아는지 이해하도록 도울 수 있다.[^28] 충성스러운 AI 어시스턴트들은 세부사항을 따라가며 그것들이 확인되도록 하는 책임을 맡을 수 있다.

둘째, AI는 새로운 형태의 대규모 조정을 가능하게 할 수 있다. 우리의 가장 시급한 문제들 중 많은 것 — 기후 변화에서 항생제 내성까지 — 은 근본적으로 조정 문제다. 우리는 [거의 모든 사람에게 있을 수 있는 것보다 나쁜 상황에 갇혀 있는데](https://equilibriabook.com/), 어떤 개인이나 그룹도 첫 번째 움직임을 할 여유가 없기 때문이다. AI 시스템은 복잡한 인센티브 구조를 모델링하고, 더 나은 결과로의 실행 가능한 경로를 식별하며, 그곳에 도달하는 데 필요한 신뢰 구축과 약속 메커니즘을 촉진함으로써 도울 수 있다.

아마도 가장 흥미롭게는, AI가 완전히 새로운 형태의 사회적 담론을 가능하게 할 수 있다. "도시와 대화하는"[^29] 것을 상상해보라 — 단순히 통계를 보는 것이 아니라, 수백만 주민들의 견해, 경험, 필요, 열망을 처리하고 종합하는 AI 시스템과 의미 있는 대화를 갖는 것. 또는 AI가 현재 서로 빗나가며 이야기하는 그룹들 사이에 진정한 대화를 촉진할 수 있는 방법을 고려해보라, 각 편이 서로에 대한 캐리커처가 아닌 상대편의 실제 우려와 가치를 더 잘 이해하도록 도움으로써. 또는 AI가 사람들 또는 심지어 큰 그룹의 사람들(모두가 직접 그리고 개별적으로 그것과 상호작용할 수 있는!) 사이의 분쟁에 대해 숙련되고 신뢰할 수 있게 중립적인 중재를 제공할 수 있다. 현재의 AI는 이 일을 하는 데 완전히 능력이 있지만, 그렇게 하는 도구들은 저절로, 또는 시장 인센티브를 통해 생겨나지 않을 것이다.

이러한 가능성들은 특히 담론과 신뢰를 퇴화시키는 AI의 현재 역할을 고려할 때 유토피아적으로 들릴 수 있다. 하지만 그것이 바로 우리가 이러한 긍정적 응용을 적극적으로 개발해야 하는 이유다. 통제 불가능한 AGI로의 관문을 차단하고 인간의 주체성을 강화하는 AI를 우선시함으로써, 우리는 AI가 임파워먼트, 회복력, 집단적 발전의 힘으로 봉사하는 미래를 향해 기술적 진보를 조종할 수 있다.


[^1]: 그렇긴 하지만, 삼중 교집합에서 멀리 떨어져 있는 것이 불행히도 원하는 만큼 쉽지 않다. 세 가지 측면 중 어느 하나에서 능력을 매우 강하게 밀어붙이는 것은 다른 것들에서도 그것을 증가시키는 경향이 있다. 특히, 매우 일반적이고 능력 있는 지능을 만들면서 쉽게 자율적으로 바뀔 수 없게 하는 것은 어려울 수 있다. 한 가지 접근법은 계획 능력이 제한된 ["근시안적"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) 시스템을 훈련하는 것이다. 또 다른 것은 행동 지향적 질문에 대한 답변을 회피하는 순수한 ["오라클"](https://arxiv.org/abs/1711.05541) 시스템에 초점을 맞추는 것이다.

[^2]: 많은 회사들이 자신들도 시간이 더 걸리더라도 결국 AGI에 의해 대체될 것이라는 사실을 깨닫지 못한다 — 만약 깨달았다면, 그 관문들을 조금 덜 밀어붙일지도 모른다!

[^3]: AI 시스템들은 더 효율적이지만 덜 이해하기 쉬운 방식으로 소통할 수 있지만, 인간의 이해를 유지하는 것이 우선되어야 한다.

[^4]: 모듈적이고 해석 가능한 AI의 이 아이디어는 여러 연구자들에 의해 자세히 개발되었다. 예를 들어 Drexler의 ["포괄적 AI 서비스"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) 모델, Dalrymple과 다른 사람들의 ["오픈 에이전시 아키텍처"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai)를 참조하라. 그런 시스템들은 거대한 연산량으로 훈련된 단일체 신경망보다 더 많은 엔지니어링 노력이 필요할 수 있지만, 그것이 바로 연산량 제한이 도움이 되는 부분이다 — 더 안전하고 투명한 경로를 더 실용적인 경로로 만들어줌으로써.

[^5]: 안전 사례 일반에 대해서는 [이 핸드북](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16)을 참조하라. AI와 특별히 관련해서는 [Wasil 등](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [Clymer 등](https://arxiv.org/abs/2403.10462), [Buhl 등](https://arxiv.org/abs/2410.21572), [Balesni 등](https://arxiv.org/abs/2411.03336)을 참조하라.

[^6]: 우리는 실제로 단지 추론의 높은 비용에 의해 주도되는 이 추세를 이미 보고 있다: 더 큰 것들로부터 "증류된" 더 작고 전문화된 모델들로, 덜 비싼 하드웨어에서 실행할 수 있는.

[^7]: AI 기술 생태계에 대해 흥미를 갖는 사람들이 자신들의 산업에 대한 부담스러운 규제로 보는 것에 반대하는 이유를 이해한다. 하지만 솔직히 말해서 예를 들어 벤처 캐피털리스트가 AGI와 초지능으로의 폭주를 허용하려고 하는 것은 나에게 당황스럽다. 그러한 시스템들(그리고 회사들, 회사 통제 하에 있는 동안)은 *모든 스타트업을 간식으로 먹어버릴* 것이다. 아마도 다른 산업들을 먹어치우기보다 *더 빨리*. 번영하는 AI 생태계에 투자하는 누구든지 AGI 개발이 소수의 지배적 플레이어들에 의한 독점으로 이어지지 않도록 하는 것을 우선시해야 한다.

[^8]: 경제학자이자 전 딥마인드 연구원인 마이클 웹이 [말했듯이](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/), "만약 우리가 오늘 더 큰 언어모델의 모든 개발을 멈춘다면, 그래서 GPT-4와 클로드 등이 우리가 그 크기로 훈련하는 마지막 것들이라면 — 그래서 우리는 그 크기의 것들과 모든 종류의 파인튜닝에 대해 훨씬 더 많은 반복을 허용하되, 그것보다 큰 것은 없고, 더 큰 발전도 없다면 — 오늘 우리가 가진 것만으로도 20년 또는 30년의 놀라운 경제 성장을 이끌기에 충분하다고 생각한다."

[^9]: 예를 들어, 딥마인드의 알파폴드 시스템은 GPT-4의 FLOP 수의 10만분의 1만 사용했다.

[^10]: 자율주행차의 어려움은 여기서 주목할 중요한 점이다: 명목상 좁은 과제이고 상당한 신뢰성을 가지고 상대적으로 작은 AI 시스템으로 달성 가능하지만, 그런 안전 중요 과제에서 필요한 수준으로 신뢰성을 얻으려면 광범위한 실제 세계 지식과 이해가 필요하다.

[^11]: 예를 들어, 연산량 예산이 주어지면, 우리는 아마도 그 예산의 (예를 들어) 절반으로 사전 훈련된 GPAI 모델들을 보게 될 것이고, 나머지 절반은 더 좁은 범위의 과제들에서 매우 높은 능력을 훈련하는 데 사용될 것이다. 이것은 거의 인간 수준의 일반 지능에 의해 뒷받침되는 초인간적 좁은 능력을 제공할 것이다.

[^12]: 현재 지배적인 정렬 기법은 "인간 피드백에 의한 강화학습" [(RLHF)](https://arxiv.org/abs/1706.03741)이고 인간 피드백을 사용하여 AI 모델의 강화학습을 위한 보상/처벌 신호를 만든다. 이것과 [헌법적 AI](https://arxiv.org/abs/2212.08073) 같은 관련 기법들은 놀랍도록 잘 작동하고 있다(견고성이 부족하고 적당한 노력으로 우회될 수 있지만). 또한, 현재 언어모델들은 일반적으로 상식적 추론에 충분히 능숙해서 어리석은 도덕적 실수를 하지 않을 것이다. 이것은 어떤 면에서 스위트 스팟이다: 사람들이 원하는 것을 이해할 만큼 똑똑하지만(정의될 수 있는 정도까지), 정교한 기만을 계획하거나 잘못했을 때 큰 해를 끼칠 만큼 똑똑하지는 않다.

[^13]: 장기적으로, 개발되는 어떤 수준의 AI 능력이든 궁극적으로는 소프트웨어이고 유용하기 때문에 확산될 것 같다. 우리는 그런 시스템들이 제기하는 위험에 대해 방어하는 견고한 메커니즘을 가져야 할 것이다. 하지만 우리는 *지금 그것을 갖고 있지 않으므로* 얼마나 많은 강력한 AI 모델들이 확산되도록 허용되는지에 대해 매우 신중해야 한다.

[^14]: 이것들의 대다수는 미성년자를 포함한 비동의 음란물 딥페이크다.

[^15]: 그런 해결책의 많은 성분들이 "봇인가 아닌가" 법률(EU AI 법 등에서), [산업 출처 추적 기술](https://c2pa.org/), [혁신적 뉴스 애그리게이터](https://www.improvethenews.org/), 예측 [애그리게이터](https://metaculus.com/)와 시장 등의 형태로 존재한다.

[^16]: 자동화의 파도는 이전 패턴을 따르지 않을 수 있는데, 상대적으로 *높은* 기술 과제들인 양질의 글쓰기, 법 해석, 의학적 조언 제공 등이 낮은 기술 과제들만큼 또는 심지어 더 자동화에 취약할 수 있다.

[^17]: AGI가 임금에 미치는 영향의 신중한 모델링에 대해서는 [여기](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek) 보고서를