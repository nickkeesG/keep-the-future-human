# 2. nodaļa - Būtiskākais par AI neironu tīkliem

Kā darbojas mūsdienu AI sistēmas un kas mūs var sagaidīt nākamajā AI paaudzē?

Lai saprastu, kā attīstīsies spēcīgākas AI izveides sekas, ir būtiski apgūt dažus pamatus. Šī un nākamās divas sadaļas tos aplūko, pēc kārtas apskatot, kas ir mūsdienu AI, kā tas izmanto milzīgas skaitļošanas jaudas, un kādā mērā tas strauji kļūst vispārīgāks un spējīgāks.[^1]

Mākslīgo intelektu var definēt dažādi, bet mūsu nolūkiem galvenā AI īpašība ir tā, ka kamēr standarta datora programma ir instrukciju saraksts uzdevuma veikšanai, AI sistēma ir tāda, kas *mācās no datiem vai pieredzes veikt uzdevumus, netiekot skaidri pastāstīta, kā to darīt.*

Gandrīz viss nozīmīgais mūsdienu AI balstās uz neironu tīkliem. Tie ir matemātiski/skaitļošanas struktūras, ko reprezentē ļoti liels (miljardiem vai triljoniem) skaitļu kopums ("svari"), kas labi veic apmācības uzdevumu. Šos svarus rada (vai varbūt "izaudzē" vai "atrod"), tos iteratīvi pielāgojot, lai neironu tīkls uzlabotu skaitlisko rādītāju (arī saukts par "zudumu"), kas definēts, lai labi veiktu vienu vai vairākus uzdevumus.[^2] Šo procesu sauc par neironu tīkla *apmācību*.[^3]

Šādai apmācībai ir daudz paņēmienu, bet šīs detaļas ir daudz mazāk svarīgas nekā veids, kā tiek definēts vērtējums, un kā tas rezultējas dažādos uzdevumos, ko neironu tīkls veic labi. Vēsturiski ir tikts vilkts būtisks šķīrums starp "šauru" un "vispārējo" AI.

Šaurais AI ir apzināti apmācīts darīt konkrētu uzdevumu vai nelielu uzdevumu kopu (piemēram, atpazīt attēlus vai spēlēt šahu); tam nepieciešama pārapmācība jauniem uzdevumiem, un tam ir šaurs spēju diapazons. Mums ir pārcilvēcisks šaurais AI, kas nozīmē, ka gandrīz jebkuram diskrētam, labi definētam uzdevumam, ko cilvēks var darīt, mēs, iespējams, varam izveidot vērtējumu un tad veiksmīgi apmācīt šauru AI sistēmu, kas to darītu labāk nekā cilvēks.

Vispārējas nozīmes AI (GPAI) sistēmas var veikt plašu uzdevumu klāstu, tostarp daudzus, kam tās netika skaidri apmācītas; tās var arī apgūt jaunus uzdevumus kā daļu no savas darbības. Pašreizējie lielie "daudzveidīgie modeļi"[^4] kā ChatGPT to ilustrē: apmācīti uz ļoti liela tekstu un attēlu korpusa, tie var iesaistīties sarežģītā spriedumu veidošanā, rakstīt kodu, analizēt attēlus un palīdzēt plašā intelektuālo uzdevumu klāstā. Lai gan joprojām diezgan atšķiras no cilvēka intelekta veidos, ko redzēsim dziļi zemāk, to vispārīgums ir izraisījis revolūciju AI.[^5]

## Neparedzamība: galvenā AI sistēmu īpašība

Galvenā atšķirība starp AI sistēmām un parastajām programmām ir paredzamībā. Standarta programmatūras izvade var būt neparedzama - tiešām, dažkārt tāpēc mēs rakstām programmatūru, lai iegūtu rezultātus, ko nevarējām paredzēt. Bet parasta programmatūra reti dara kaut ko, kam tā nebija programmēta - tās darbības joma un uzvedība parasti atbilst plānotajam. Augstākā līmeņa šaha programma var veikt gājienus, ko neviens cilvēks nevarēja paredzēt (citādi viņi varētu uzvarēt šo šaha programmu!), bet tā parasti nedarīs neko citu kā vien spēlēs šahu.

Tāpat kā parastā programmatūra, šaurajam AI ir paredzama darbības joma un uzvedība, bet var būt neparedzami rezultāti. Tas patiešām ir tikai cits veids, kā definēt šauro AI: kā AI, kas līdzinās parastai programmatūrai savā paredzamībā un darbības diapazonā.

Vispārējas nozīmes AI ir citāds: tā darbības joma (domēni, kuros tas darbojas), uzvedība (veidi, kā tas rīkojas) un rezultāti (tā faktiskā izvade) visi var būt neparedzami.[^6] GPT-4 tika apmācīts tikai precīzi ģenerēt tekstu, bet attīstīja daudzas spējas, ko tā apmācītāji neparedzēja vai neieredzēja. Šī neparedzamība izriet no apmācības sarežģītības: tā kā apmācības dati satur daudzu dažādu uzdevumu izvadi, AI faktiski jāiemācās veikt šos uzdevumus, lai labi prognozētu.

Šī vispārējo AI sistēmu neparedzamība ir diezgan fundamentāla. Lai gan principā ir iespējams rūpīgi konstruēt AI sistēmas, kurām ir garantēti to uzvedības ierobežojumi (kā minēts vēlāk esejā), veids, kā AI sistēmas tagad tiek veidotas, tās ir neparedzamas gan praksē, gan principā.

## Pasīvais AI, aģenti, autonomās sistēmas un saskaņošana

Šī neparedzamība kļūst īpaši svarīga, kad apskatām, kā AI sistēmas faktiski tiek izvietotas un izmantotas dažādu mērķu sasniegšanai.

Daudzas AI sistēmas ir relatīvi pasīvas tādā ziņā, ka tās galvenokārt sniedz informāciju, un lietotājs veic darbības. Citas, ko parasti sauc par *aģentiem*, pašas veic darbības ar dažādu lietotāja iesaistes līmeni. Tās, kas veic darbības ar relatīvi mazāku ārēju ievadi vai uzraudzību, var saukt par vairāk *autonomām*. Tas veido spektru darbības neatkarības ziņā - no pasīviem rīkiem līdz autonomiem aģentiem.[^7]

Attiecībā uz AI sistēmu mērķiem, tie var būt tieši saistīti ar to apmācības mērķi (piemēram, "uzvaras" mērķis Go spēlējošai sistēmai ir arī tieši tas, kam tā tika apmācīta). Vai arī tie var nebūt: ChatGPT apmācības mērķis daļēji ir prognozēt tekstu, daļēji būt noderīgam palīgam. Bet, veicot konkrētu uzdevumu, tā mērķi piegādā lietotājs. Mērķus var izveidot arī pati AI sistēma, tikai ļoti netiešā saistībā ar tās apmācības mērķi.[^8]

Mērķi ir cieši saistīti ar "saskaņošanas" jautājumu, tas ir ar jautājumu, vai AI sistēmas *darīs to, ko mēs vēlamies, lai tās darītu*. Šis vienkāršais jautājums slēpj milzīgu subtilitāšu līmeni.[^9] Pagaidām ievērojiet, ka "mēs" šajā teikumā var attiekties uz daudziem dažādiem cilvēkiem un grupām, radot dažādus saskaņošanas veidus. Piemēram, AI varētu būt ļoti *paklausīgs* (vai ["uzticīgs"](https://arxiv.org/abs/2003.11157)) savam lietotājam - šeit "mēs" ir "katrs no mums." Vai tas varētu būt vairāk *suverēns*, galvenokārt vadoties no saviem mērķiem un ierobežojumiem, bet joprojām rīkojoties plaši cilvēku labklājības kopējās interesēs - "mēs" tad ir "cilvēce" vai "sabiedrība." Starpā ir spektrs, kur AI būtu lielākoties paklausīgs, bet varētu atteikties veikt darbības, kas kaitē citiem vai sabiedrībai, pārkāpj likumu utt.

Šie divi asi - autonomijas līmenis un saskaņošanas veids - nav pilnīgi neatkarīgi. Piemēram, suverēna pasīva sistēma, lai gan nav pilnīgi pašpretrunīga, ir koncepta spriedze, tāpat kā paklausīgs autonoms aģents.[^10] Ir skaidra izpratne, ka autonomija un suverenitāte mēdz iet roku rokā. Līdzīgā garā paredzamība mēdz būt augstāka "pasīvās" un "paklausīgās" AI sistēmās, turpretī suverēnās vai autonomās būs tieksme būt neparedzamākām. Viss tas būs izšķiroši svarīgi, lai saprastu potenciālā MVI un superintelekta sekas.

Patiesi saskaņota AI izveide, lai kāda tā būtu, prasa atrisināt trīs dažādus izaicinājumus:

1. Saprast, ko "mēs" vēlamies - kas ir sarežģīti neatkarīgi no tā, vai "mēs" nozīmē konkrētu personu vai organizāciju (lojalitāte) vai cilvēci plaši (suverenitāte);
2. Veidot sistēmas, kas regulāri rīkojas saskaņā ar šīm vēlmēm - būtībā radot konsekventu pozitīvu uzvedību;
3. Vispamatīgāk, veidot sistēmas, kas patiešām "rūpējas" par šīm vēlmēm, nevis tikai rīkojas tā, it kā tās to darītu.

Atšķirība starp uzticamu uzvedību un patieso rūpešanos ir būtiska. Tāpat kā cilvēks darbinieks varētu paklausīt rīkojumiem nevainojami, vienlaikus neizjūtot īstu apņēmību organizācijas misijai, AI sistēma varētu rīkoties saskaņoti, nepatiesībā novērtējot cilvēku preferences. Mēs varam apmācīt AI sistēmas teikt un darīt lietas, izmantojot atgriezenisko saiti, un tās var iemācīties spriest par to, ko cilvēki vēlas. Bet panākt, lai tās *patiešām* vērtē cilvēku preferences, ir daudz dziļāks izaicinājums.[^11]

Šo saskaņošanas izaicinājumu atrisināšanas pamatīgās grūtības un to sekas AI riskam tiks pētītas sīkāk zemāk. Pagaidām sapratiet, ka saskaņošana nav tikai tehniska funkcija, ko mēs pievienojam AI sistēmām, bet pamata aspekts to arhitektūrā, kas veido to attiecības ar cilvēci.


[^1]: For a gentle but technical introduction to machine learning and AI, particularly language models, see [this site.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) For another modern primer on AI extinction risks, see [this piece.](https://www.thecompendium.ai/) For a comprehensive and authoritative scientific analysis of the state of AI safety, see the recent [International AI Safety Report.](https://arxiv.org/abs/2501.17805)

[^2]: Training typically occurs by looking for a local maximum of the score in a high-dimensional space given by the model weights. By checking how the score changes as weights are tweaked, the training algorithm identifies which tweaks improve score the most, and moves the weights in that direction.

[^3]: For example, in an image recognition problem, the neural network would output probabilities for labels for the image. A score would be related to the probability the AI accords to the correct answer. The training procedure would then adjust weights so that next time, the AI would output a higher probability for the correct label for that image. This is then repeated a huge number of times. The same basic procedure is used in training essentially all modern neural networks, albeit with more complex scoring mechanism.

[^4]: Most multimodal models use the "transformer" architecture to process and generate multiple types of data (text, images, sound). These can all decomposed into, and then treated on the same footing, as different types of "tokens." Multimodal models are trained first to accurately predict tokens within massive datasets, then refined through reinforcement learning to enhance capabilities and shape behaviors.

[^5]: That language models are trained to do one thing – predict words – has caused some to call them narrow AI. But this is misleading: because predicting text well requires so many different capabilities, this training task leads to a surprisingly general system. Also note that these systems are extensively trained by reinforcement learning, effectively representing thousands of people giving the model a reward signal when it does a good job at any of the many things it does. It then inherits significant generality from the people giving this feedback.

[^6]: There are multiple ways in which AI is unpredictable. One is that in the general case one cannot predict what an algorithm will do without actually running it; there are [theorems](https://arxiv.org/abs/1310.3225) to this effect. This can be true just because the output of algorithms can be complex. But it is particularly clear and relevant in the case (such as in chess or Go) where the prediction would imply a capability (beating the AI) the would-be predictor does not have. Second, a given AI system will not always produce the same output even given the same input – its outputs contain randomness; this also couples with algorithmic unpredictability. Third, unexpected and emergent capabilities can arise from training, meaning even the *types* of things an AI system can and will do are unpredictable; This last type is particularly important for safety considerations.

[^7]: See [here](https://arxiv.org/abs/2502.02649) for an in-depth review of what is meant by an "autonomous agent" (along with ethical arguments against building them).

[^8]: You may sometimes hear "AI can't have its own goals." This is absolute nonsense. It is easy to generate examples where AI has or develops goals that were never given to it and are known only to itself. You don't see this much in current popular multimodal models because it is trained out of them; it could just as easily be trained into them.

[^9]: There's a large literature. On the general problem see Christian's [*The Alignment Problem*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821), and Russell's [*Human-Compatible*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616). On a more technical side see e.g. [this paper](https://arxiv.org/abs/2209.00626).

[^10]: We'll later see that while such systems buck the trend, that actually makes them very interesting and useful.

[^11]: This is not to say we require emotions or sentience. Rather, it is enormously difficult from the outside of a system to know what its inner goals, preferences, and values are. "Genuine" here would mean that we have strong enough reason to rely on it that in the case of critical systems we can bet our lives on it.