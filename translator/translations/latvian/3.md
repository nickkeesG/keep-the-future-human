# 3. nodaļa - Galvenie mūsdienu vispārējā AI sistēmu izstrādes aspekti

Lielākā daļa pasaules vismodernāko AI sistēmu tiek radītas, izmantojot pārsteidzoši līdzīgas metodes. Šeit ir pamati.

Lai patiesi izprastu cilvēku, nepieciešamas zināšanas par bioloģiju, evolūciju, bērnu audzināšanu un citām jomām; lai izprastu AI, jāzina arī par tā izstrādes procesu. Pēdējo piecu gadu laikā AI sistēmas ir ievērojami attīstījušās gan spēju, gan sarežģītības ziņā. Galvenais veicinošais faktors ir bijusi ļoti lielu skaitļošanas resursu (vai sarunvalodā "skaitļošanas jaudas", runājot par AI) pieejamība.

Skaitļi ir pārsteidzoši. Modeļu apmācībai, piemēram, GPT sērijai, Claude, Gemini utt., tiek izmantotas aptuveni 10 <sup>25</sup> -10 <sup>26</sup> "peldošā komata operācijas" (FLOP) [^1].[^2] (Salīdzinājumam: ja visi Zemes cilvēki nepārtraukti strādātu, veicot vienu aprēķinu ik pēc piecām sekundēm, būtu nepieciešams aptuveni miljards gadu, lai to paveiktu.) Šis milzīgais skaitļošanas apjoms ļauj apmācīt modeļus ar līdz pat triljoniem modeļa parametru, izmantojot terabaitus datu - lielu daļu no visa kvalitatīvā teksta, kas jebkad ir uzrakstīts, kopā ar plašām skaņu, attēlu un video bibliotēkām. Papildus šai apmācībai tiek veikta plaša papildu apmācība, kas pastiprina cilvēku preferences un labu uzdevumu izpildi. Šādā veidā apmācīti modeļi uzrāda ar cilvēkiem salīdzināmu sniegumu plašā pamata intelektuālo uzdevumu spektrā, ieskaitot spriedumu veidošanu un problēmu risināšanu.

Mēs arī zinām (ļoti, ļoti aptuveni), cik liela skaitļošanas ātruma (operāciju sekundē) ir pietiekami, lai šādas sistēmas *secinājumu izdarīšanas* ātrums [^3] atbilstu cilvēka teksta apstrādes *ātrumam*. Tas ir aptuveni 10 <sup>15</sup> -10 <sup>16</sup> FLOP sekundē.[^4]

Lai gan šie modeļi ir spēcīgi, to raksturam raksturīgi būtiski ierobežojumi, kas ir diezgan līdzīgi tam, kā būtu ierobežots atsevišķs cilvēks, ja viņš būtu spiests vienkārši izvadīt tekstu ar fiksētu vārdu ātrumu minūtē, neapstājoties domāt vai neizmantojot nekādus papildu rīkus. Jaunākās AI sistēmas novērš šos ierobežojumus, izmantojot sarežģītāku procesu un arhitektūru, kas apvieno vairākus galvenos elementus:

- Vienu vai vairākus neironu tīklus, kur viens modelis nodrošina galveno kognitīvo spēju, bet līdz pat vairāki citi veic citus, šaurākus uzdevumus;
- *Instrumentus*, kas tiek nodrošināti modelim un ko tas var izmantot - piemēram, spēja meklēt internetā, izveidot vai rediģēt dokumentus, izpildīt programmas utt.
- *Atbalsta struktūru*, kas savieno neironu tīklu ieejās un izejas. Ļoti vienkārša atbalsta struktūra varētu vienkārši ļaut diviem AI modeļa "gadījumiem" sarunāties savā starpā vai vienam pārbaudīt otra darbu.[^5]
- *Domāšanas ķēde* un saistītās uzvednes metodes dara kaut ko līdzīgu, liekot modelim, piemēram, izveidot daudzas problēmas risinājuma pieejas, pēc tam apstrādāt šīs pieejas kopēja atbilžu iegūšanai.
- Modeļu *pārapmācība*, lai tie labāk izmantotu instrumentus, atbalsta struktūru un domāšanas ķēdi.

Tā kā šie paplašinājumi var būt ļoti spēcīgi (un ietvert pašas AI sistēmas), šīs kompozītsistēmas var būt diezgan sarežģītas un dramatiski uzlabot AI spējas.[^6] Un nesen scaffolding un īpaši domāšanas ķēdes uzvedņu metodes (un rezultātu iekļaušana atpakaļ modeļu pārapmācībā, lai tie tos labāk izmantotu) ir izstrādātas un ieviests [o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/) un [DeepSeek R1](https://api-docs.deepseek.com/news/news250120), lai veiktu daudzus secinājumu ciklus atbildē uz konkrētu vaicājumu.[^7] Tas faktiski ļauj modelim "domāt par" savu atbildi un dramatiski pastiprina šo modeļu spēju veikt augsta līmeņa spriedumus zinātnes, matemātikas un programmēšanas uzdevumos.[^8]

Konkrētai AI arhitektūrai apmācības skaitļošanas palielināšana [var būt ticami pārvērsta](https://arxiv.org/abs/2405.10938) uzlabojumos skaidri definētu rādītāju kopā. Mazāk precīzi definētām vispārējām spējām (piemēram, tām, kas aplūkotas tālāk), šī pārvēršana ir mazāk skaidra un paredzama, bet ir gandrīz noteikti, ka lielākiem modeļiem ar vairāk apmācības skaitļošanas būs jaunas un labākas spējas, pat ja ir grūti paredzēt, kādas tās būs.

Līdzīgi kompozītsistēmas un īpaši "domāšanas ķēdes" sasniegumi (un modeļu apmācība, kas labi darbojas ar to) ir atklājuši mērogošanu *secinājumu izdarīšanas* skaitļošanā: konkrētam apmācītam pamatmodelim vismaz dažas AI sistēmas spējas palielinās, kad tiek pielietota vairāk skaitļošanas, kas ļauj tām "domāt smagāk un ilgāk" par sarežģītām problēmām. Tas nāk ar augstām skaitļošanas ātruma izmaksām, prasot simtiem vai tūkstošiem vairāk FLOP/s, lai atbilstu cilvēka sniegumam.[^9]

Lai gan tas ir tikai daļa no tā, kas izraisa strauju AI progresu,[^10] skaitļošanas loma un kompozītsistēmu iespējas izrādīsies būtiskas gan nekontrolējama MVI novēršanā, gan drošāku alternatīvu izstrādē.

[^1]: 10 <sup>27</sup> nozīmē 1, kam seko 25 nulles, jeb desmit triljoni triljonu. FLOP ir vienkārši aritmētiska skaitļu saskaitīšana vai reizināšana ar kādu precizitāti. Ņemiet vērā, ka AI aparatūras veiktspēja var atšķirties par desmit reižu faktoru atkarībā no aritmētikas precizitātes un datora arhitektūras. Loģisko vārtu operāciju (UN, VAI, UN NE) skaitīšana būtu fundamentāla, bet tās nav plaši pieejamas vai testētas; pašreizējiem mērķiem ir lietderīgi standartizēt 16 bitu operācijas (FP16), lai gan būtu jāizveido atbilstoši konversijas faktori.

[^2]: Novērtējumu un precīzu datu kolekcija ir pieejama no [Epoch AI](https://epochai.org/data/large-scale-ai-models) un norāda uz aptuveni 2×10 <sup>25</sup> 16 bitu FLOP GPT-4; tas aptuveni atbilst [numuriem, kas tika nopludināti](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) GPT-4. Novērtējumi citiem 2024. gada vidus modeļiem visi ir dažu faktoru robežās no GPT-4.

[^3]: Secinājumu izdarīšana ir vienkārši process, kā no neironu tīkla ģenerēt izvadi. Apmācību var uzskatīt par daudziem secinājumiem un modeļa parametru korekcijām.

[^4]: Teksta ražošanai sākotnējam GPT-4 bija nepieciešami 560 TFLOP uz katru ģenerēto žetonu. Aptuveni 7 žetoni/s ir nepieciešami, lai sekotu līdzi cilvēka domāšanai, tāpēc tas dod ≈3×10 <sup>15</sup> FLOP/s. Bet efektivitātes uzlabojumi to ir samazinājuši; [šajā NVIDIA brošūrā](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/), piemēram, norādīts uz tik maz kā 3×10 <sup>14</sup> FLOP/s salīdzināmi darboties spējīgam Llama 405B modelim.

[^5]: Kā nedaudz sarežģītāks piemērs, AI sistēma varētu vispirms ģenerēt vairākus iespējamos matemātikas problēmas risinājumus, pēc tam izmantot citu gadījumu, lai pārbaudītu katru risinājumu, un beigās izmantot trešo, lai sintezētu rezultātus skaidrā skaidrojumā. Tas ļauj rūpīgāk un uzticamāk risināt problēmas nekā viens cikls.

[^6]: Skatiet, piemēram, detaļas par [OpenAI "Operator"](https://openai.com/index/introducing-operator/), [Claude rīku iespējām](https://docs.anthropic.com/en/docs/build-with-claude/computer-use) un [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). OpenAI [Deep Research](https://openai.com/index/introducing-deep-research/) visticamāk ir diezgan sarežģīta arhitektūra, bet detaļas nav pieejamas.

[^7]: Deepseek R1 paļaujas uz iteratīvu modeļa apmācību un uzvednēm, lai gala apmācītais modelis radītu plašu domāšanas ķēdes spriedumu veidošanu. Arhitektūras detaļas nav pieejamas o1 vai o3, tomēr Deepseek ir atklājis, ka nav nepieciešama īpaša "maģiska sastāvdaļa", lai atbloķētu spēju mērogošanu ar secinājumu izdarīšanu. Bet, neskatoties uz to, ka saņēma lielu preses uzmanību kā "status quo" apgāzošana AI jomā, tas neietekmē šī raksta pamatapgalvojumus.

[^8]: Šie modeļi ievērojami pārspēj standarta modeļus spriedumu testēšanas etalonos. Piemēram, GPQA Diamond etalona testā - stingrā PhD līmeņa zinātnes jautājumu testā - GPT-4o [ieguva](https://openai.com/index/learning-to-reason-with-llms/) 56%, kamēr o1 un o3 sasniedza 78% un 88% attiecīgi, tālu pārsniedzot cilvēku ekspertu 70% vidējo rezultātu.

[^9]: OpenAI O3 visticamāk tērēja ∼10 <sup>21</sup> -10 <sup>22</sup> FLOP [lai pabeigtu katru no ARC-AGI izaicinājuma jautājumiem](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai), ko kompetenti cilvēki var izdarīt (sakot) 10-100 sekundēs, dodot skaitli vairāk kā ∼10 <sup>20</sup> FLOP/s.

[^10]: Lai gan skaitļošana ir galvenais AI sistēmu spēju mērs, tā mijiedarbojas gan ar datu kvalitāti, gan algoritmisko uzlabojumiem. Labāki dati vai algoritmi var samazināt skaitļošanas prasības, kamēr vairāk skaitļošanas dažkārt var kompensēt vājākus datus vai algoritmus.