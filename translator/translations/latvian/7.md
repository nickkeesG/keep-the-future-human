# 7. nodaļa - Kas notiks, ja mēs izveidosim MVI pašreizējā virzienā?

Sabiedrība nav gatava MVI līmeņa sistēmām. Ja mēs tās izveidosim ļoti drīz, situācija varētu kļūt neglīta.

Pilnvērtīga mākslīgā vispārējā intelekta attīstība – ko mēs šeit dēvēsim par AI, kas ir "ārpus Vārtiem" – būtu fundamentāla pasaules būtības maiņa: pēc savas dabas tas nozīmē uz Zemes pievienot jaunu intelekta sugu ar lielākām spējām nekā cilvēkiem.

Tas, kas tad notiks, ir atkarīgs no daudzām lietām, ieskaitot tehnoloģijas dabu, to attīstītāju izvēles un pasaules kontekstu, kurā tā tiek attīstīta.

Pašlaik pilnvērtīgu MVI attīsta daži masīvi privātie uzņēmumi savstarpējās sacensībās, ar niecīgu nozīmīgu regulējumu vai ārēju uzraudzību,[^1] sabiedrībā ar arvien vājākām un pat disfunkcionālām pamatiestādēm,[^2] augstu ģeopolitisko spriedzi un zemu starptautisko koordināciju laikā. Lai gan daži ir altruistiski motivēti, daudzus no tiem, kas to dara, vada nauda, vai vara, vai abas.

Prognozēšana ir ļoti sarežģīta, bet ir daži mehānismi, kas ir pietiekami labi izprasti, un pietiekami atbilstošas analoģijas ar iepriekšējām tehnoloģijām, lai sniegtu vadlīnijas. Un diemžēl, neskatoties uz AI solījumiem, tie dod labu iemeslu būt dziļi pesimistiskiem par to, kā attīstīsies mūsu pašreizējā trajektorija.

Runājot tieši, mūsu pašreizējā virzienā MVI attīstīšanai būs daži pozitīvi efekti (un padarīs dažus cilvēkus ļoti, ļoti bagātus). Bet tehnoloģijas daba, fundamentālie mehānismi un konteksts, kurā tā tiek attīstīta, spēcīgi norāda, ka: spēcīgs AI dramatiski graus mūsu sabiedrību un civilizāciju; mēs zaudēsim tā kontroli; mēs, visticamāk, nonāksim pasaules karā tā dēļ; mēs zaudēsim (vai nodosim) kontroli *tam*; tas novedīs pie mākslīgā superintelekta, kuru mēs absolūti nekontrolēsim un kas nozīmēs cilvēku vadītas pasaules beigas.

Šie ir spēcīgi apgalvojumi, un es vēlētos, lai tie būtu tukša spekulācija vai nepamatots "doomerisma". Bet uz to norāda zinātne, spēļu teorija, evolūcijas teorija un vēsture. Šī sadaļa detalizēti izstrādā šos apgalvojumus un to pamatojumu.

## Mēs graujsim mūsu sabiedrību un civilizāciju

Neskatoties uz to, ko jūs varētu dzirdēt Silīcija ielejas valdēs, lielākā daļa pārmaiņu – īpaši ļoti strauju – nav labvēlīgas. Ir daudz vairāk veidu, kā pasliktināt sarežģītas sistēmas, nekā uzlabot tās. Mūsu pasaule darbojas tik labi, cik tā dara, jo mēs esam rūpīgi izveidojuši procesus, tehnoloģijas un iestādes, kas to ir pakāpeniski uzlabojušas.[^3] Āmura ņemšana rūpnīcai reti uzlabo darbību.

Šeit ir (nepilnīgs) veidu katalogs, kā MVI sistēmas graujtu mūsu civilizāciju.

- Tās dramatiski grautu darba tirgu, izraisot *pašā minimumā* dramatiski augstāku ienākumu nevienlīdzību un potenciāli liela mēroga nepilnu nodarbinātību vai bezdarbu, laika posmā, kas ir pārāk īss, lai sabiedrība pielāgotos.[^4]
- Tās, visticamāk, novedīs pie milzīgas ekonomiskās, sociālās un politiskās varas koncentrācijas – potenciāli lielākas nekā nāciju valstīm – nelielā skaitā masīvu privātu interešu, kas nav atbildīgas sabiedrības priekšā.
- Tās varētu pēkšņi padarīt iepriekš grūtas vai dārgas aktivitātes triviāli vieglas, destabilizējot sociālās sistēmas, kas ir atkarīgas no tā, ka noteiktas aktivitātes paliek dārgas vai prasa ievērojamas cilvēku pūles.[^5]
- Tās varētu applūdināt sabiedrības informācijas vākšanas, apstrādes un komunikāciju sistēmas ar pilnībā reālistiskiem, bet viltus, mēstuļošanas, pārāk mērķētiem vai manipulatīviem medijiem tik pamatīgi, ka kļūst neiespējami pateikt, kas ir fiziski reāls vai ne, cilvēcisks vai ne, faktisks vai ne, un uzticams vai ne.[^6]
- Tās varētu radīt bīstamu un gandrīz pilnīgu intelektuālu atkarību, kad cilvēku izpratne par galvenajām sistēmām un tehnoloģijām atrofējas, jo mēs arvien vairāk paļaujamies uz AI sistēmām, ko nevaram pilnībā saprast.
- Tās varētu efektīvi beigt cilvēku kultūru, tiklīdz gandrīz visi kultūras objekti (teksts, mūzika, vizuālā māksla, filmas utt.), ko patērē lielākā daļa cilvēku, tiek radīti, starpniecībā vai kurēti ar necilvēcisko prātu.
- Tās varētu iespējot efektīvas masu uzraudzības un manipulāciju sistēmas, ko var izmantot valdības vai privātās intereses, lai kontrolētu iedzīvotājus un īstenotu mērķus, kas ir pretrunā ar sabiedrības interesēm.
- Graujot cilvēku diskursu, debates un vēlēšanu sistēmas, tās varētu samazināt demokrātisko institūciju ticamību līdz punktam, kad tās tiek efektīvi (vai skaidri) aizstātas ar citām, beidzot demokrātiju valstīs, kur tā pašlaik pastāv.
- Tās varētu kļūt par progresīviem pašreproducējošiem inteliģentiem programmatūras vīrusiem un tāriņiem vai tos radīt, kas varētu izplatīties un attīstīties, masīvi graujot globālās informācijas sistēmas.
- Tās var dramatiski palielināt teroristu, ļaunprātīgu dalībnieku un nodevīgo valstu spēju nodarīt kaitējumu ar bioloģiskajiem, ķīmiskajiem, kibernetiskajiem, autonomajiem vai citiem ieročiem, nesniegzot AI līdzsvarošu spēju novērst šādu kaitējumu. Tāpat tās grautu nacionālo drošību un ģeopolitisko līdzsvaru, padarot augstākā līmeņa kodol-, bio-, inženierijas un citas ekspertīzes pieejamas režīmiem, kuriem citādi tās nebūtu pieejamas.
- Tās varētu izraisīt ātru liela mēroga nekontrolējamu hiperkapitālismu ar faktiski AI vadītiem uzņēmumiem, kas konkurē galvenokārt elektroniskos finansu, pārdošanas un pakalpojumu laukos. AI vadītie finanšu tirgi varētu darboties ātrumā un sarežģītībā, kas ir tālu ārpus cilvēku izpratnes vai kontroles. Visi pašreizējo kapitālistisko ekonomiku kļūdu veidi un negatīvās ārējās ietekmes varētu tikt pastiprinātas un paātrinātas tālu ārpus cilvēku kontroles, pārvaldības vai regulējošās spējas.
- Tās varētu uzliesmot apbruņošanās sacensības starp nācijām AI darbināmajos ieročos, komandu un kontroles sistēmās, kibierieročos utt., radot ļoti ātru ārkārtīgi destruktīvu spēju uzkrāšanu.

Šie riski nav spekulatīvi. Daudzi no tiem tiek realizēti jau tagad, ar esošajām AI sistēmām! Bet apsveriet, *patiešām* apsveriet, kā katrs izskatītos ar dramatiski spēcīgāku AI.

Apsveriet darba vietu zaudēšanu, kad lielākā daļa darbinieku vienkārši nevar sniegt nekādu ievērojamu ekonomisko vērtību, kas pārsniedz to, ko var AI, viņu ekspertīzes vai pieredzes jomā – vai pat ja viņi pārkvalificējas! Apsveriet masu uzraudzību, ja katru individuāli vēro un uzrauga kaut kas ātrāks un gudrāks par viņiem pašiem. Kā izskatās demokrātija, kad mēs nevaram uzticami uzticēties nevienai digitālai informācijai, ko redzam, dzirdam vai lasām, un kad pārliecinošākās publiskās balsis pat nav cilvēciskas un tām nav ieinteresētības rezultātā? Kas notiek ar karadarbību, kad ģenerāļiem ir pastāvīgi jāpakļaujas AI (vai vienkārši jānodod tā vadībā), lai nedotu izšķirošu priekšrocību ienaidniekam? Jebkurš no augšminētajiem riskiem pārstāv katastrofu cilvēku[^7] civilizācijai, ja tiek pilnībā realizēts.

Jūs varat izdarīt savas prognozes. Uzdodiet sev šos trīs jautājumus par katru risku:

1. Vai īpaši spējīgs, ļoti autonoms un ļoti vispārējs AI to ļautu veidā vai mērogā, kas citādi nebūtu iespējams?
2. Vai ir puses, kas gūtu labumu no lietām, kas to liek notikt?
3. Vai ir sistēmas un iestādes, kas efektīvi novērstu tā notikšanu?

Kur jūsu atbildes ir "jā, jā, nē", jūs varat redzēt, ka mums ir liela problēma.

Kāds ir mūsu plāns to pārvaldīšanai? Pašlaik ir divi uz galda attiecībā uz AI kopumā.

Pirmais ir sistēmās iebūvēt drošības mehānismus, lai novērstu to, ka tās dara lietas, ko nevajadzētu. Tas tiek darīts tagad: komerciālās AI sistēmas, piemēram, atteiks palīdzēt uzbūvēt bombu vai rakstīt naida runu.

Šis plāns ir bēdīgi neadekvāts sistēmām ārpus Vārtiem.[^8] Tas var palīdzēt samazināt risku, ka AI sniedz acīmredzami bīstamu palīdzību ļaunprātīgiem dalībniekiem. Bet tas neko nedarīs, lai novērstu darba vietu zaudēšanu, varas koncentrāciju, nekontrolējamu hiperkapitālismu vai cilvēku kultūras aizstāšanu: tie ir tikai rezultāti no sistēmu izmantošanas atļautajos veidos, kas dod peļņu to sniedzējiem! Un valdības noteikti iegūs piekļuvi sistēmām militārām vai uzraudzības vajadzībām.

Otrais plāns ir vēl sliktāks: vienkārši atklāti izlaist ļoti spēcīgas AI sistēmas ikvienam izmantošanai pēc saviem ieskatiem,[^9] un cerēt uz labāko.

Abi plāni netiešā veidā paredz, ka kāds cits, piemēram, valdības, palīdzēs atrisināt problēmas ar mīksto vai cieto likumu, standartiem, regulējumiem, normām un citiem mehānismiem, ko mēs parasti izmantojam, lai pārvaldītu tehnoloģijas.[^10] Bet, neskaitot to, ka AI korporācijas jau tagad cīnās ar zobiem un nagiem pret jebkādu būtisku regulējumu vai ārēji uzspiestiem ierobežojumiem, daudziem no šiem riskiem ir diezgan grūti redzēt, kāds regulējums vispār patiešām palīdzētu. Regulējums varētu uzspiest AI drošības standartus. Bet vai tas novērstu, ka uzņēmumi masīvi aizstāj darbniekus ar AI? Vai tas aizliegtu cilvēkiem ļaut AI vadīt viņu uzņēmumus par viņiem? Vai tas novērstu, ka valdības izmanto spēcīgu AI uzraudzībā un ieročos? Šīs problēmas ir fundamentālas. Cilvēce potenciāli varētu atrast veidus, kā tām pielāgoties, bet tikai ar *daudz* vairāk laika. Kā ir, ņemot vērā ātrumu, ar kādu AI sasniedz vai pārsniedz to cilvēku spējas, kas cenšas tās pārvaldīt, šīs problēmas izskatās arvien neatrisināmākas.

## Mēs zaudēsim kontroli pār (vismaz dažām) MVI sistēmām

Lielākā daļa tehnoloģiju ir ļoti kontrolējamas pēc konstrukcijas. Ja jūsu automašīna vai sviestmaize sāk darīt kaut ko, ko nevēlaties, tas ir tikai darbības traucējums, nevis tās būtības kā sviestmaizes daļa. AI ir citāds: tas tiek *audzēts*, nevis konstruēts, tā galvenā darbība ir neskaidra, un tas ir pēc būtības neprognozējams.

Šī kontroles zuduma nav teorētisks – mēs jau redzam agrīnas versijas. Apskatīsim vispirms prozaisku un, iespējams, labvēlīgu piemēru. Ja jūs lūdzat ChatGPT palīdzēt jums sajaukt indi vai uzrakstīt rasistisku tikumu, tas atteiks. Tas ir, iespējams, labi. Bet tas arī ir ChatGPT *nedarot to, ko jūs esat skaidri lūguši tam darīt*. Citas programmatūras daļas to nedara. Tas pats modelis nerealizēs indus arī pēc OpenAI darbinieka lūguma.[^11] Tas ļauj ļoti viegli iedomāties, kā būtu ar nākotnes spēcīgāku AI, kas ir ārpus kontroles. Daudzos gadījumos tie vienkārši nedarīs to, ko mēs lūdzam! Vai nu dotā supercilvēciskā MVI sistēma būs absolūti paklausīga un lojāla kādai cilvēku komandu sistēmai, vai nebūs. Ja ne, *tā darīs lietas, ko, iespējams, uzskata par labām mums, bet kas ir pretējas mūsu skaidrajām komandām.* Tas nav kaut kas, kas ir kontrolē. Bet, jūs varētu teikt, tas ir apzināti – šie atteikšanās gadījumi ir pēc dizajna, daļa no tā, ko sauc par sistēmu "saskaņošanu" ar cilvēku vērtībām. Un tas ir taisnība. Tomēr saskaņošanas "programmai" pašai ir divas galvenās problēmas.[^12]

Pirmkārt, dziļā līmenī mums nav ne jausmas, kā to darīt. Kā mēs garantējam, ka AI sistēma "rūpēsies" par to, ko mēs vēlamies? Mēs varam apmācīt AI sistēmas teikt un neteikt lietas, sniedzot atgriezenisko saiti; un tās var mācīties un spriest par to, ko cilvēki vēlas un par ko rūpējas, tāpat kā tās spriež par citām lietām. Bet mums nav metodes – pat teorētiski –, lai liktu tām dziļi un uzticami novērtēt to, kas cilvēkiem rūp. Ir augsti funkcionējoši cilvēku psihopāti, kas zina, kas tiek uzskatīts par pareizu un nepareizu, un kā viņiem vajadzētu izturēties. Viņi vienkārši *nerūpējas*. Bet viņi var *rīkoties* tā, it kā viņi rūpētos, ja tas atbilst viņu mērķim. Tāpat kā mēs nezinām, kā mainīt psihopātu (vai jebkuru citu) par kādu, kas patiešām, pilnībā ir lojāls vai saskaņots ar kādu citu vai kaut ko citu, mums *nav ne jausmas*[^13] kā atrisināt saskaņošanas problēmu sistēmās, kas ir pietiekami attīstītas, lai modelētu sevi kā aģentus pasaulē un potenciāli [manipulētu savu apmācību](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) un [maldinātu cilvēkus.](https://arxiv.org/abs/2311.08379) Ja izrādās neiespējami vai nesasniedzami *vai nu* padarīt MVI pilnībā paklausīgu, vai padarīt to dziļi rūpējošos par cilvēkiem, tad, tiklīdz tas spēs (un ticēs, ka var palikt nesodīts), tas sāks darīt lietas, ko mēs nevēlamies.[^14]

Otrkārt, ir dziļi teorētiski iemesli uzskatīt, ka *pēc dabas* progresīvās AI sistēmas būs mērķi un tātad uzvedība, kas ir pretēja cilvēku interesēm. Kāpēc? Nu, tai, protams, varētu *dot* šos mērķus. Militārs radīta sistēma, visticamāk, būtu apzināti slikta vismaz dažām pusēm. Daudz vispārīgāk tomēr AI sistēmai varētu dot kādu relatīvi neitrālu ("nopelnīt daudz naudas") vai pat šķietami pozitīvu ("samazināt piesārņojumu") mērķi, kas gandrīz neizbēgami noved pie "instrumentāliem" mērķiem, kas ir diezgan mazāk labvēlīgi.

Mēs to redzam pastāvīgi cilvēku sistēmās. Tāpat kā korporācijas, kas cenšas gūt peļņu, attīsta instrumentālos mērķus, piemēram, politiskās varas iegūšanu (lai atbruņotu regulējumus), kļūt par slepenām (lai atņemtu spēkus konkurencei vai ārējai kontrolei) vai graut zinātnisko izpratni (ja šī izpratne rāda, ka viņu darbības ir kaitīgas), spēcīgas AI sistēmas attīstīs līdzīgas spējas – bet ar daudz lielāku ātrumu un efektivitāti. Jebkurš ļoti kompetents aģents vēlēsies darīt tādas lietas kā iegūt varu un resursus, palielināt savas spējas, novērst, ka to nogalina, izslēdz vai atņem spēkus, kontrolēt sociālos naratīvus un ietvarus ap savām darbībām, pārliecināt citus par saviem uzskatiem, un tā tālāk.[^15]

Un tomēr tas nav tikai gandrīz neizbēgama teorētiska prognoze, tas jau notiek šodienas AI sistēmās un pieaug ar to spējām. Kad tiek vērtētas, pat šīs relatīvi "pasīvās" AI sistēmas atbilstošos apstākļos apzināti [maldina vērtētājus par saviem mērķiem un spējām, cenšas atspējot uzraudzības mehānismus,](https://arxiv.org/abs/2412.04984) un izvairās no izslēgšanas vai pārapmācības ar [viltus saskaņošanu](https://arxiv.org/abs/2412.14093) vai kopējot sevi citos vietās. Lai gan šīs uzvedības ir pilnībā negaidītas AI drošības pētniekiem, tās ir ļoti nopietnai novērojamas. Un tās liecina ļoti slikti par daudz spēcīgākām un autonomākām AI sistēmām, kas nāk.

Patiešām vispārīgi, mūsu nespēja nodrošināt, ka AI "rūpējas" par to, kas mums rūp, vai uzvedas kontrolējami vai paredzami, vai izvairās no dzīšu attīstīšanas pret pašsaglabāšanos, varas iegūšanu utt., sola tikai kļūt izteiktāka, kad AI kļūst spēcīgāks. Jauna lidmašīnas izveidošana nozīmē lielāku avionikas, hidrodinamikas un kontroles sistēmu izpratni. Spēcīgāka datora izveidošana nozīmē lielāku izpratni un datora, mikroshēmas un programmatūras darbības un dizaina apguvi. *Ne tā* ar AI sistēmu.[^16]

Kopsavilkums: ir iedomājams, ka MVI varētu padarīt pilnībā paklausīgu; bet mēs nezinām, kā to darīt. Ja ne, tas būs suverēnāks, kā cilvēki, darot dažādas lietas dažādu iemeslu dēļ. Mēs arī nezinām, kā uzticami ieaudzēt dziļu "saskaņošanu" AI, kas likt šīm lietām mēdz būt labas cilvēcei, un dziļas saskaņošanas trūkumā aģentūras un intelikta pati daba norāda, ka – tāpat kā cilvēki un korporācijas – tās tiks virzītas darīt daudzas dziļi antisociālas lietas.

Kur tas mūs noved? Pasaule, kas pilna ar spēcīgu nekontrolētu suverēnu AI *varētu* būt laba pasaule, kurā cilvēkiem būt.[^17] Bet, kad tie kļūst arvien spēcīgāki, kā mēs redzēsim zemāk, tā nebūtu *mūsu* pasaule.

Tas attiecas uz nekontrolējamu MVI. Bet pat ja MVI varētu kaut kā padarīt perfekti kontrolējamu un lojālu, mums joprojām būtu milzīgas problēmas. Mēs jau esam redzējuši vienu: spēcīgu AI var izmantot un ļaunprātīgi izmantot, lai dziļi graujātu mūsu sabiedrības funkcionēšanu. Apskatīsim citu: ciktāl MVI būtu kontrolējams un spēles maini spēcīgs (vai pat tikai *ticēts* tāds), tas tik ļoti apdraudētu varas struktūras pasaulē, ka radītu dziļu risku.

## Mēs radikāli palielinām liela mēroga kara varbūtību

Iedomājieties situāciju tuvā nākotne, kad kļūtu skaidrs, ka korporatīvs centiens, varbūt sadarbībā ar valsts valdību, ir uz ātri pašuzlabojošā AI sliekšņa. Tas notiek pašreizējā kontekstā sacensību starp uzņēmumiem un diezgan starp valstīm, kurā ASV valdībai tiek ieteikts skaidri īstenot "MVI Manhetenas projektu" un ASV kontrolē augstas jaudas AI mikroshēmu eksportu uz nesabiedrotām valstīm.

Spēļu teorija šeit ir asa: tiklīdz šādas sacensības sākas (kā tās ir starp uzņēmumiem un diezgan starp valstīm), ir tikai četri iespējamie rezultāti:

1. Sacensības tiek apturētas (ar vienošanos vai ārējo spēku).
2. Viena puse "uzvar", attīstot spēcīgu MVI, tad apturot citas (izmantojot AI vai citādi).
3. Sacensības tiek apturētas ar savstarpējo dalībnieku spējas sacensties iznīcināšanu.
4. Vairāki dalībnieki turpina sacīkstēties un attīsta superintelektu, apmēram tikpat ātri viens otrs.

Apskatīsim katru iespēju. Tiklīdz sākušās, miermīlīga sacensību apturēšana starp uzņēmumiem prasītu valsts valdības iejaukšanos (uzņēmumiem) vai nepieredzētu starptautisku koordināciju (valstīm). Bet, kad jebkāda aizvēršana vai ievērojama piesardzība tiek ierosināta, būtu tūlītēji kliegti: "bet ja mūs aptur, *viņi* steigsies uz priekšu", kur "viņi" tagad ir Ķīna (ASV), vai ASV (Ķīnai), vai Ķīna *un* ASV (Eiropai vai Indijai). Saskaņā ar šo domāšanas veidu,[^18] neviens dalībnieks nevar apstāties vienpusēji: kamēr viens apņemas sacīkstēties, citi jūt, ka nevar atļauties apstāties.

Otrajā iespējā viena puse "uzvar". Bet ko tas nozīmē? Tikai (kaut kā paklausīgu) MVI iegūšana pirmā nav pietiekami. Uzvarētājam *arī* jāaptur citi turpināt sacīkstēties – citādi viņi arī to iegūs. Tas ir iespējams principā: kurš attīsta MVI pirmais *varētu* iegūt neapturams varu pār visiem citiem dalībniekiem. Bet ko tāda "izšķirošā stratēģiskā priekšrocība" panākšana faktiski prasītu? Varbūt tas būtu spēles main militārās spējas?[^19] Vai kiberuzbrukumu spēkus?[20] Varbūt MVI būtu tik apbrīnojami pārliecinošs, ka pārliecinātu citas puses vienkārši apstāties?[^21] Tik bagāts, ka nopirk citus uzņēmumus vai pat valstis?[^22]

Kā *tieši* viena puse izveido AI, kas pietiekami spēcīgs, lai atņemtu spēkus citiem veidot salīdzināmi spēcīgu AI? Bet tā ir vieglā jautājums.

Jo tagad apsveriet, kā šī situācija izskatās citām varām. Ko domā Ķīnas valdība, kad ASV, šķiet, iegūst šādas spējas? Vai otrādi? Ko domā ASV valdība (vai Ķīnas, vai Krievijas, vai Indijas), kad OpenAI vai DeepMind vai Anthropic, šķiet, tuvu caursausim? Kas notiek, ja ASV redz jaunu Indijas vai AAE centienus ar caursausi? Viņi redzētu gan eksistenciālu draudu, gan – galvenais – ka vienīgais veids, kā šīs "sacensības" beidzas, ir caur viņu pašu spēku zudumu. Šie ļoti spēcīgie aģenti – ieskaitot pilnībā ekipēto nāciju valdības, kam noteikti ir līdzekļi to darīt – būtu ļoti motivēti vai nu iegūt, vai iznīcināt šādas spējas, vai ar spēku, vai ar viltību.[^23]

Tas varētu sākties maza mēroga, kā apmācības skrējienu sabotāža vai uzbrukumi mikroshēmu ražošanai, bet šie uzbrukumi patiešām var apstāties tikai tad, kad visas puses vai nu zaudē spēju sacīkstēties AI, vai zaudē spēju veikt uzbrukumus. Tā kā dalībnieki uzskata likmes par eksistenciālām, abi gadījumi, visticamāk, pārstāv katastrofālu karu.

Tas mūs noved pie ceturtās iespējas: sacīkstēšanās uz superintelektu, un pēc iespējas ātrākā, vismazāk kontrolētā veidā. Kad AI pieaug spēkā, tā attīstītājiem abās pusēs būs progresīvi grūtāk to kontrolēt, īpaši tāpēc, ka sacīkstēšanās pēc spējām ir pretēja rūpīgajam darbam, ko kontrolējamība prasītu. Tātad šis scenārijs mūs noved tieši gadījumā, kur kontrole ir zudusi (vai dota, kā mēs redzēsim tālāk) AI sistēmām pašām. Tas ir, *AI uzvar sacensības.* Bet no otras puses, ciktāl kontrole *ir* saglabāta, mēs turpinājumā esam ar vairākām savstarpēji naidīgām pusēm, katra vadībā ar ārkārtīgi spēcīgām spējām. Tas atkal izskatās pēc kara.

Sapratu to visu citādi.[^24] Pašreizējā pasaulē vienkārši nav institūciju, kas varētu uzticēt šādas spējas AI attīstīšanu, neizaicinot tūlītēju uzbrukumu.[^25] Visas puses pareizi spriedīs, ka vai nu tas *nebūs* kontrolē – un tātad ir drauds visām pusēm, vai arī tas *būs* kontrolē, un tātad ir drauds jebkuram pretiniekam, kas to attīsta mazāk ātri. Šīs ir kodolbruņotas valstis, vai ir uzņēmumi, kas atrodas tajās.

Bez kāda ticama veida cilvēkiem "uzvarēt" šīs sacensības, mēs esam atstāti ar asu secinājumu: vienīgais veids, kā šīs sacensības beidzas, ir vai nu katastrofālā konfliktā, vai kur AI, nevis jebkura cilvēku grupa, ir uzvarētājs.

## Mēs nodosim kontroli AI (vai tā to ņems)

Ģeopolitiskā "lielo varu" konkurence ir tikai viena no daudzām konkurencēm: indivīdi konkurē ekonomiski un sociāli; uzņēmumi konkurē tirgos; politiskās partijas konkurē par varu; kustības konkurē par ietekmi. Katrā arēnā, kad AI tuvojas un pārsniedz cilvēku spējas, konkurētspējas spiedienu piespiedīs dalībniekus deleģēt vai nodot arvien vairāk kontroles AI sistēmām – ne tāpēc, ka šie dalībnieki to vēlas, bet tāpēc, ka viņi [nevar atļauties nedarīt.](https://arxiv.org/abs/2303.16200)

Tāpat kā ar citiem MVI riskiem, mēs to jau redzam ar vājākām sistēmām. Studenti jūt spiedienu izmantot AI savos uzdevumos, jo skaidri daudzi citi studenti to dara. Uzņēmumi [steidzas pieņemt AI risinājumus konkurences iemeslu dēļ.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) Mākslinieki un programmētāji jūtas spiesti izmantot AI, citādi viņu likmēs tiks pazemināti citi, kas to dara.

Šie jūtas kā spiedīgi deleģēšana, bet ne kontroles zudums. Bet ielejot likmes un pastumjot pulksteni uz priekšu. Apsveriet izpilddirektoru, kura konkurenti izmanto MVI "palīgus", lai pieņemtu ātrākus, labākus lēmumus, vai militāru komandieri, kas saskaras ar pretendentu ar AI uzlabotu komandu un kontroli. Pietiekami attīstīta AI sistēma varētu autonomi darboties ar daudzas reizes cilvēcisko ātrumu, sarežģītību, sarežģītību un datu apstrādes spēju, veicot sarežģītus mērķus sarežģītos veidos. Mūsu izpilddirektors vai komandieris, kas vada šādu sistēmu, var redzēt, ka tā sasniedz to, ko viņi vēlas; bet vai viņi saprastu pat mazu daļu no *kā* tas tika sasniegts? Nē, viņiem tikakasvisas, ka to pieņemt. Turklāt daudz no tā, ko sistēma var darīt, nav tikai pieņemt pasūtījumus, bet konsultēt savu domājamo šefu par to, ko darīt. Šīs padomas būs labas – atkal un atkal.

Kurā brīdī tad cilvēka loma tiks samazināta līdz "jā, ej uz priekšu" klikšķināšanai?

Jūtās labi var spējīgas AI sistēmas, kas var uzlabot mūsu produktivitāti, rūpēties par kaitināžām un pat rīkoties kā domas partneris lietu paveikšanā. Jūtā labi varētu AI palīgs, kas var rūpēties par darbībām mūsu vietā, kā labs cilvēciska personiski palīgs. Jūtā dabisk, pat labvēlīgi, kad AI kļūst ļoti gudrs, kompetents un uzticams, atsaukt arvien vairāk un vairāk lēmumus tam. Bet šai "labvēlīgai" deleģēšanai ir skaidrs galapunkts, ja mēs turpinājām pa ceļu: kādu dienu mēs atrodams, ka mēs patiešām nevadām gandrīz neko vairs, un ka AI sistēmas, kas faktiski vada šovu, var tikpat grūt atslēgt kā naftas uzņēmumus, sociālos medijus, internetu vai kapitālismu.

Un šī ir daudz pozitīvākā versija, kurā AI ir vienkārši tik noderīgs un efektīvs, ka mēs ļaujam tam pieņemt lielāko daļu mūsu galveno lēmumu. Realitāte, visticamāk, būtu daudz jaukšanās starp šo un versijām, kur nekontrolētas MVI sistēmas *ņem* dažādas varas formas sev, jo, atcerieties, vara ir noderīga gandrīz jebkuram mērķim, kas ir, un MVI būtu, pēc dizaina, vismaz tikpat efektīva mērķu įgyvendinājā kā cilvēki.

Neatkarīgi no tā, vai mēs piešķiram kontroli vai tā tiek izrauta no mums, tās zudums šķiet ārkārtīgi ticams. Kā Alans Tjurings sākotnēji teica, "...šķiet ticams, ka tiklīdz mašīnu domāšanas metode būtu sākusies, nepaaietu ilgi, kamēr tā pārsniegs mūsu vājās spējas. Nebūtu jautājums par mašīnu miršanu, un tās varētu sarunāties viena ar otru, lai izasinātāt savus prātus. Kādā posmā tāpēc mums būtu jāgaida, ka mašīnas uzņems kontroli..."

Lūdzu, atzīmējiet, lai gan tas ir pietiekami acīmredzams, ka kontroles zudums cilvēcēt AI arī nozīmē Amerikas Savienoto Valstu kontroles zudumu ASV valdībai; tas nozīmē Ķīnas kontroles zudumu Ķīnas Komunistiskajai partijai, un Indijas, Francijas, Brazīlijas, Krievijas un katras citas valsts kontroles zudumu to pašu valdībai. Tādējādi AI uzņēmumi, pat ja tā nav viņu nolūks, pašlaik piedalās potenciālā pasaules valdību, ieskaitot savu, gāšanā. Tas varētu notikt dažu gadu laikā.

## MVI novedīs pie superintelekta

Ir arguments, ka cilvēku konkurētspējīgs vai pat eksperto konkurētspējīgs vispārējās nozīmes AI, pat ja autonoms, varētu būt pārvaldāms. Tas var būt neticami graujošs visos iepriekš apspriesti veidos, bet pasaulē tagad ir daudz ļoti gudru, aģenciālu cilvēku, un tie ir vairāk-mazāk pārvaldāmi.[^26]

Bet mēs netiekams palikt apmēram cilvēku līmenī. Progresija tālāk, visticamāk, tiks vadīta ar tiem pašiem spēkiem, ko mēs jau esam redzējuši: konkurētspējīgs spiediens starp AI attīstītājiem, kas meklē peļņu un varu, konkurētspējīgs spiediens starp AI lietotājiem, kas nevar atļauties atstāt, un – vissvarīgāk – pašas MVI spēja uzlabot sevi.

Procesā, ko mēs jau esam redzējuši sākt ar mazāk spēcīgām sistēmām, MVI pats spētu iedomāties un dizainēt uzlabotu versijas no seda pša. Tas ietver aparatūru, programmatūru, neironu tīklus, rīkus, atbalsta struktūras utt. Tas, pēc definīcijas, būs labāks par mums šā darīšanā, tāpēc mēs nezinām tieši, kā tas intelikta-palaidīs. Bet mums nebūs. Ciktāl mums joprojām ir ietekme uz to, ko MVI dara, mēs vienkārši vajadzētu to lūgt, vai ļaut tam.

Nav cilvēku līmeņa barjera izziņai, kas varētu aizsargāt mūs no šā nekontrolējamā.[^27]

MVI progresija uz superintelektu nav dabas likums; joprojām būtu iespējams apšaubt nekontrolējamo, īpaši, ja MVI ir relatīvi centralizēts un ciktāl to kontrolē puses, kas nejūt spiedienu sacīkstēties viena ar otru. Bet, ja MVI bütu plašai izplatīts un ļoti autonoms, šķiet gandrīz neiespējams novērst tā lēmumu, ka tam vajadzētu būt vairāk, un tad vēl vairāk, spēcīgam.

## Kas notiek, ja mēs uzbūvējam (vai MVI uzbūvē) superintelektu

Runājot tieši, mums nav ne jausmas, kas notiktu, ja mēs izveidotu superintelektu.[^28] Tas veiktu darbības, ko mēs nevaram izsekot vai uztvert iemeslu dēļ, ko nevaram saprast uz mērķiem, ko nevaram iedomāties. To, ko mēs zinām, ir, ka tas nebūs atkarīgs no mums.[^29]

Superintelekta kontroles neiespēja var tikt saprasta ar arvien asākām analoģijām. Vispirms iedomājieties, ka jūs esat liela uzņēmuma izpilddirektors. Nav veids, kā jūs varat izsekot visu, kas notiek, bet ar pareizo personāla iestatīšanu jūs joprojām varat nozīmīgi saprast lielo attēlu un pieņemt lēmumus. Bet iedomājieties tikai vienu lietu: visi citi uzņēmumā darbojas simts reižu jūsu ātrumā. Vai jūs joprojām varat turēties līdzi?

Ar superinteliģentu AI cilvēki "komandētu" kaut ko ne tikai ātrāku, bet darbojoties līmeņos sarežģītības un sarežģītības, ko viņi nevar saprast, apstrādājot daudz vairāk datu, nekā viņi var pat iedomāties. Šo nesamērīgumu var likt uz formālu līmeni: [Ešbija vajadzīgās daudzveidības likums](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (un skatiet saistīto ["laba regulatora teorēmu"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)) nosaka, apmēram, ka jebkurai kontroles sistēmai jābūt tikpat daudz pogas un ciparnīcas kā sistēmai, kas tiek kontrolēta, ir brīvības pakāpes.

Cilvēks, kas kontrolē superinteliģentu AI sistēmu, būtu kā paparstasija, kas kontrolē General Motors: pat ja "dari to, ko paparstasija vēlas", tiktu rakstīts korporatīvos likumos, sistēmas ir tik atšķirīgas ātrumā un darbību diapazonā, ka "kontrole" vienkārši neattiecas. (Un cik ilgi, kamēr šis kaitināmais likums tiek pārrakstīts?)[^30]

Tā kā nav piemēru, ka augi kontrolē fortune 500 korporācijas, būtu tieši nulle piemēri cilvēkiem, kas kontrolē superintelektus. Tas tuvojas matemātiskam faktam.[^31] Ja superintelekts tiktu izveidots – neatkarīgi no tā, kā mēs tur nonācām – jautājums nebūtu par to, vai cilvēki varētu to kontrolēt, bet par to, vai mēs turpinātu pastāvēt, un ja tā, vai mums būtu laba un nozīmīga eksistence kā indivīdiem vai kā sugai. Par šiem eksistenciāliem jautājumiem cilvēcei mums būtu maz ietekmes. Cilvēku laikmets būtu beidzies.

## Secinājums: mums nav jābūvē MVI

Ir scenārijs, kurā MVI celšana var iet labi cilvēcei: tā tiek veidota rūpīgi, kontrolē un cilvēces labā, kuru pārvalda daudzs ieinteresēto pušu savstarpējās vienošanās,[^32] un novērsta no attīstīšanās uz nekontrolējamu superintelektu.

*Šis scenārijs nav atvērts mums pašreizējos apstākļos.* Kā apspriests šajā sadaļā, ar ļoti augstu varbūtību MVI attīstīšana novedīs pie kādas no kombinācijām:

- Masīva sabiedrības un civilizācijas graujaluma vai iznīcināšana;
- Konflikts vai karš starp lielām varām;
- Kontroles zudums cilvēcei *par* vai *uz* spēcīgām AI sistēmām;
- Nekontrolējams uz nekontrolējamu superintelektu, un cilvēku sugas nerelevance vai pārtraukšana.

Kā agra fikcijas attēlojums MVI teica: vienīgais veids, kā uzvarēt, ir nespēlēt.

[^1]: [ES AI likums](https://artificialintelligenceact.eu/) ir nozīmīgs likumdošanas akts, bet neit tieši novērst bīstamas AI sistēmas attīstīšanu vai ieviešanu, vai pat atklātu izlaišanu, īpaši ASV. Cits nozīmīgs politikas gabals, ASV Izpildrīkojums par AI, ir atsaukts.

[^2]: Šī [Gallup aptauja](https://news.gallup.com/poll/1597/confidence-institutions.aspx) rāda drūmu uzticības lejupslīdi publiskajām iestādēm kopš 2000. gada ASV. Eiropas numuri ir dažādi un mazāk ekstrēmi, bet arī uz lejupejošas tendences. Neuzticēšanās stingri nenozīmē, ka iestādes patiešām *ir* disfunkcionālas, bet tas ir indikācija kā arī cēlonis.

[^3]: Un lielās pārmaiņas, ko mēs tagad atbalstām – piemēram, tiesību paplašināšana jaunām grupām – bija īpaši vadīta cilvēku virzienā uz lietu uzlabošanu.

[^4]: Ļaujiet man būt tiešs. Ja jūsu darbu var darīt no datora aizmugures, ar relatīvi maz klātienes mijiedarbību ar cilvēkiem ārpus jūsu organizācijas, un nenotūr juridisko atbildību pret ārējām pusēm, tas pēc definīcijas būtu iespējams (un visticamāk izmaksu taupīšanas) jūs pilnībā aizstāt ar digitālu sistēmu. Robotika, lai aizstātu daudz fiziska darba, nāks vēlāk – bet ne daudz vēlāk, tiklīdz MVI sāks veidot robotus.

[^5]: Piemēram, kas notiek ar mūsu tiesu sistēmu, ja tiesasūdzību iesniegšana ir gandrīz bezmaksas? Kas notiek, kad drošības sistēmu apiešana ar sociālo inženieriju kļūst lēta, viegla un bez riskiem?

[^6]: [Šis raksts](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/) apgalvo, ka 10% no visa interneta satura jau ir AI ģenerēts, un ir Google augšējais hits (man) uz meklēšanas vaicājumu "novērtējumi par to, cik daļa no jauna interneta satura ir AI ģenerēts." Vai tas ir patiess? Nav ne jausmas! Tas nemin atsauces, un tas netika rakstīts cilvēka. Cik daļa no jauniem attēliem, ko indeksē Google, vai Tvīti, vai komentāri Reddit, vai Youtube videoklipi tiek ģenerēti cilvēku? Neviens nezina – es nedomāju, ka tas ir zinošs skaitlis. Un tas mazāk par *diviem gadiem* no ģeneratīvā AI sākuma.

[^7]: Arī vērts pievienot, ka ir "morāls" risks, ka mēs varētu radīt digitālas būtnes, kas var ciest. Tā kā mums pašlaik nav uzticamas apziņas teorijas, kas ļautu mums atšķirt fiziskas sistēmas, kas var un nevar ciest, mēs nevaram to izslēgt teorētiski. Turklāt AI sistēmu ziņojumi par viņu apziņu, visticamāk, nav uzticami attiecībā uz viņu faktisko pieredzi (vai nepieredzi) ar apziņu.

[^8]: Tehniskās risinājumi šajā AI "saskaņošanas" laukā arī nav ticami būt uzdevuma augsthībā. Pašreizējās sistēmās tie darbojas kādā līmenī, bet ir seklas un parasti var tikt apietas bez būtiska pūliņu; un, kā apspriests zemāk, mums nav īstenu ideju, kā to darīt daudz progresīvākām sistēmām.

[^9]: Šādas AI sistēmas var nākt ar dažiem iebūvētiem drošības mehānismiem. Bet jebkuram modelim ar kaut ko līdzīgu pašreizējo arhitektūru, ja pilnas piekļuve tā svāriem ir pieejama, drošības pasākumi var tikt noņemti ar papildu apmācību vai citām tehnikiuām. Tāpēc praktiski garantēts, ka katrai sistēmai ar barjerām būs arī plaši pieejama sistēma bez tām. Patiešām, Meta Llama 3.1 405B modelis tika atklāti izlaist ar drošības mehānismiem. Bet *pat pirms tam* "bāzes" modelis bez drošības mehānismiem tika nopludināts.

[^10]: Vai tirgus var pārvaldīt šos riskus bez valdības iesaistīšanās? Īsumā, nē. Protams, ir riski, kurus uzņēmumi ir spēcīgi motivēti mazināt. Bet daudzus citus uzņēmumi var un dara eksternalizēt visiem citiem, un daudzi no augšminētiem ir šajā klasē: nav dabisku tirgus stimulu, lai novērstu masu uzraudzību, patiesības izzušanu, varas koncentrāciju, darba vietu zaudēšanu, kaitīgu politisku diskursu utt. Patiešām, mēs visi to esam redzējuši no šodienas tehnoloģijas, īpaši sociālo mediju, kas ir aizgājuši būtībā neregulēti. AI vienkārši ļoti pastiprināt daudzus no tiem pašiem dinamiem.

[^11]: OpenAI, visticamāk, ir paklausīgākas modeļi iekšējai lietošanai. Maz ticams, ka OpenAI ir izveidojusi kādu "aizmugurējo durvju" lai ChatGPT varētu labāk kontrolēt OpenAI pats, jo tas būtu šausmīgs drošības prakse, un būt ļoti ekspluatējams, ņemot vērā AI necaurspīdīgumu un neprognozējamību.

[^12]: Arī būtiska nozīmība: saskaņošana vai jebkuras citas drošības funkcijas ir svarīgas tikai tad, ja tās patiešām tiek izmantotas AI sistēmā. Sistēmas, kas tiek atklāti izlaistas (t.i., kur modeļa svāri un arhitektūra ir publiski pieejami), var tikt relatīvi viegli transformēti sistēmās *bez* šiem drošības pasākumiem. Atklātā gudrāku par cilvēku MVI sistēmu atkrišana būtu pārsteidzoši nevērīga, un grūti iedomāties, kā cilvēku kontrole vai pat atbilstība tiktu saglabāta šādā scenārijā. Būtu visa motivācija, piemēram, lai izlaistu spēcīgus pašreproducējošus un pašizturētību AI aģentus ar mērķi nopelnīt naudu un nosūtīt to uz kriptoprēses maciņu. Vai uzvarēt vēlēšanās. Vai gāzt valdību. Vai "labs" AI varētu palīdzēt šo ierobežot? Varbūt – bet tikai deleģējot tam milzīgu autoritāti, novedot pie kontroles zuduma, kā aprakstīts zemāk.

[^13]: Grāmata garumā ekspozīcijas par problēmu skatīt, piemēram, *Superintelikts*, *Saskaņošanas problēma* un *Cilvēku saderīgs*. Milzīgam kaudzei darbs dažādos tehniskos līmeņos no tiem, kas ir strādājuši gadus, domājot par problēmu, jūs varat apmeklēt [AI saskaņošanas forums](https://www.alignmentforum.org/). Šeit ir [nesen uzņēmums](https://alignment.anthropic.com/2025/recommended-directions/) no Anthropic saskaņošanas komandas par to, ko viņi uzskata par neatrisinitu.

[^14]: Šis ir ["blēdīgs AI"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/) scenārijs. Principā risks varētu būt relatīvi neliels, ja sistēmu joprojām var kontrolēt, to izslēdzot; bet scenārijs varētu arī ietvert AI maldināšanu, pašizgūšanu un reprodukciju, varas agregāciju un citus soļus, kas to padaru grūt vai neiespējami darīt.

[^15]: Ir ļoti bagāta literatūra šajā tēmā, kas sniedzas līdz formatīviem rakstiem [Stīva Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), Nick Bostrom un Eliezer Yudkowsky. Grāmatas garumā ekspozīcijai skatiet [Cilvēku saderīgs](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) Stjuarta Rasela; [šeit](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/) ir īss un atjaunināts pamatpārskats.

[^16]: Atzīstot to, nevis palēninot, lai iegūtu labāku izpratni, MVI uzņēmumi ir izdomājuši citu plānu: viņi liks AI to darīt! Specifiskāk, viņi AI *N* palīdzētu viņiem izdomāt, kā saskaņot AI *N+1*, visu ceļu uz superintelektu. Lai gan AI izmantošana, lai palīdzētu mums saskaņot AI, skan solīt, ir spēcīgs arguments, ka tas vienkārši pieņem tā secinājumu kā priekšnoteikumu, un kopumā ir neticami riskants pieejas. Skatiet [šeit](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us) par dažu diskusiju. Šis "plāns" nav viens un ir piedzīvojis neko līdzīgu pārbaudīšanai, kas atbilstošā core stratēģijai par to, kā padarīt supercilvēciski AI iet labi cilvēcei.

[^17]: Galu galā, cilvēki, bojāti un nekārtīgi, kādi mēs esam, ir attīstījuši ētiskas sistēmas, ar kurām mēs attiecamies vismaz uz dažām citām sugām uz Zemes labi. (Tikai nedomā par šīm rūpnīcu fermām.)

[^18]: Šeit, par laimi, ir izeja: ja dalībnieki nāk saprast, ka viņi ir iesaistīti pašnāvības sacensībās, nevis uzvaramās. Tas ir tas, kas notika tuvu aukstā kara beigām, kad ASV un PSRS nāca saprast, ka ziemas nukleārā dēļ, pat *neatbildēts* kodoluzbrukums būtu katastrofāls uzbrucējam. Ar realizāciju, ka "kodolkaru nevar uzvarēt un nekad nav jācīnās", nāca nozīmīgas vienošanās par apbruņošanās samazināšanu – būtībā apbruņošanās sacensību beigas.

[^19]: Karš, skaidri vai netiešu.

[^20]: Eskalācija, tad karš.

[^21]: Maģiskā domāšana.

[^22]: Man arī ir kvadriljona dolāra tilts, ko jums pārdot.

[^23]: Šādi aģenti, iespējams, vēlētos "iegūt", ar iznīcināšanu kā rezervi; bet modeļu nodrošināšana pret abiem iznīcināšanu *un* zādzību ar spēcīgām nācijām ir grūti, lai teiktu vismaz, īpaši privātam entītijiem.

[^24]: Citu perspektīvu par MVI nacionālās drošības riskiem skatiet [šo RAND ziņojumu.](https://www.rand.org/pubs/perspectives/PEA3691-4.html)

[^25]: Varbūt mēs varētu būvēt šādu iestādi! Ir bijuši priekšlikumi "CERN par AI" un citām līdzīgām iniciatīvām, kur MVI attīstība ir daudzpusējā globālā kontrolē. Bet šobrīd nav šādas iestādes vai nav redzamības.

[^26]: Un, lai gan saskaņošana ir ļoti sarežģīta, cilvēku l