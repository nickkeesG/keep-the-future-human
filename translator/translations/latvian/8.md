# 8. nodaļa - Kā neveidot MVI

MVI nav neizbēgams – šodien mēs atrodamies ceļu krustojumā. Šī nodaļa sniedz priekšlikumu tam, kā mēs varētu novērst tā izveidi.

Ja ceļš, pa kuru mēs šobrīd ejam, noved pie mūsu civilizācijas iespējamās beigas, kā mēs varam mainīt ceļu?

Pieņemsim, ka vēlēšanās pārtraukt MVI un superintelekta attīstību būtu plaši izplatīta un spēcīga,[^1] jo kļūst par vispārēju izpratni, ka MVI būtu drīzāk varas absorbējošs, nevis varas piešķirošs, un būtu dziļa briesma sabiedrībai un cilvēcei. Kā mēs aizvērtu Vārtus?

Pašlaik mēs zinām tikai vienu veidu, kā *veidot* spēcīgu un vispārēju AI, kas ir ar patiešām masīviem dziļo neironu tīklu skaitļojumiem. Tā kā šīs ir neticami grūtas un dārgas lietas, ir zināmā mērā tā, ka to *nedarīšana* ir viegla.[^2] Bet mēs jau esam redzējuši spēkus, kas virza uz MVI, un spēļu teorētisko dinamiku, kas jebkurai pusei ļoti apgrūtina vienpusēju apturēšanu. Tāpēc būtu nepieciešama āršķības (t.i., valdību) iejaukšanās kombinācija, lai apturētu korporācijas, un vienošanās starp valdībām, lai apturētu sevi.[^3] Kā tas varētu izskatīties?

Vispirms ir lietderīgi atšķirt AI attīstības virzienes, kas ir *jānovērš* vai *jāaizliedz*, un tās, kas ir *jāpārvalda*. Pirmais galvenokārt būtu nekontrolējams virziens uz superintelektu.[^4] Aizliegtajai attīstībai definīcijām jābūt pēc iespējas skaidrākām, un gan verificēšanai, gan ieviešanai jābūt praktiskām. Kas ir *jāpārvalda*, būtu vispārējas, spēcīgas AI sistēmas – kas mums jau ir, un kurām būs daudz pelēko zonu, nianšu un sarežģītības. Šīm ir izšķiroša nozīme spēcīgām efektīvām institūcijām.

Mēs varam arī lietderīgi norobežot jautājumus, kas jārisina starptautiskā līmenī (tostarp starp ģeopolitiskajiem konkurentiem vai pretinieks) [^5] no tiem, ko var pārvaldīt atsevišķas jurisdikcijas, valstis vai valstu kopas. Aizliegtā attīstība lielākoties iekļaujas "starptautiskajā" kategorijā, jo vietējo tehnoloģijas attīstības aizliegumu parasti var apiet, mainot atrašanās vietu.[^6]

Visbeidzot, mēs varam apsvērt instrumentus rīku kastē. To ir daudz, tostarp tehniskie rīki, mīkstie tiesību akti (standarti, normas u.c.), stingie tiesību akti (noteikumi un prasības), atbildība, tirgus stimuli u.c. Pievērsīsim īpašu uzmanību vienam, kas ir raksturīgs AI.

## Skaitļošanas drošība un pārvaldība

Būtisks instruments augstas jaudas AI pārvaldībā būs aparatūra, kas tai nepieciešama. Programmatūra izplatās viegli, tai ir gandrīz nulles robežizmaksas ražošanā, tā šķērso robežas bez grūtībām un to var acumirklī modificēt; nekas no šiem apgalvojumiem nav patiess attiecībā uz aparatūru. Tomēr, kā mēs esam apsprieduši, milzīgs šīs "skaitļošanas jaudas" daudzums ir nepieciešams gan AI sistēmu apmācības laikā, gan secinājumu izdarīšanas laikā, lai sasniegtu visaptverošākās sistēmas. Skaitļošanas jaudu var viegli kvantificēt, uzskaitīt un auditēt ar salīdzinoši mazu neskaidrību, tiklīdz ir izstrādāti labi noteikumi šā procesa veikšanai. Vissvarīgākais, lieli skaitļošanas apjomi ir, līdzīgi bagātinātajam urānam, ļoti trūcīgs, dārgs un grūti ražojams resurss. Lai gan datoru mikroshēmas ir visur, AI nepieciešamā aparatūra ir dārga un ārkārtīgi grūti ražojama.[^7]

Tas, kas AI specializētās mikroshēmas padara daudz *labāk* pārvaldāmas kā trūcīgu resursu nekā urāns, ir tas, ka tās var ietvert aparatūras drošības mehānismus. Lielākajā daļā mūsdienu mobilo tālruņu un dažos klēpjdatoros ir specializētas mikroshēmās iebūvētas aparatūras funkcijas, kas ļauj tiem nodrošināt, ka tie instalē tikai apstiprinātu operētājsistēmas programmatūru un atjauninājumus, ka tie saglabā un aizsargā jutīgus biometriskos datus ierīcē, un ka tos var padarīt bezderīgus ikvienam, izņemot to īpašnieku, ja tie tiek pazaudēti vai nozagti. Pēdējo gadu laikā šādi aparatūras drošības pasākumi ir kļuvuši plaši atzīti un ieviesti, un parasti ir pierādījuši, ka ir diezgan droši.

Šo funkciju galvenā novitāte ir tāda, ka tās saista aparatūru un programmatūru, izmantojot kriptogrāfiju.[^8] Tas ir, tikai konkrētas datora aparatūras daļas esamība nenozīmē, ka lietotājs var ar to darīt visu, ko vēlas, izmantojot dažādu programmatūru. Un šī saistīšana arī nodrošina spēcīgu drošību, jo daudziem uzbrukumiem būtu nepieciešams *aparatūras*, nevis tikai *programmatūras* drošības pārkāpums.

Vairāki nesenie ziņojumi (piemēram, no [GovAI un līdzstrādniekiem](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips) un [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) ir norādījuši, ka līdzīgas aparatūras funkcijas, kas iebūvētas vismodernākajā AI attiecīgajā skaitļošanas aparatūrā, varētu spēlēt ārkārtīgi noderīgu lomu AI drošībā un pārvaldībā. Tie iespējo vairākas funkcijas, kas pieejamas "pārvaldniekam",[^9] kuras, iespējams, nevarētu uzminēt, ka ir pieejamas vai pat iespējamas. Kā daži svarīgi piemēri:

- *Ģeogrāfiskā atrašanās vieta*: Sistēmas var uzstādīt tā, lai mikroshēmām būtu zināma atrašanās vieta, un tās var darboties citādi (vai tikt pilnībā izslēgtas) atkarībā no atrašanās vietas.[^10]
- *Atļauto savienojumu saraksts*: katru mikroshēmu var konfigurēt ar aparatūras iespaidotu atļauto mikroshēmu sarakstu, ar kurām tā var izveidot tīklu, un tā nevar savienoties ar mikroshēmām, kas nav šajā sarakstā.[^11] Tas var ierobežot komunikatīvo mikroshēmu kopumu lielumu.[^12]
- *Mērīta secinājumu izdarīšana vai apmācība (un automātiska izslēgšana)*: Pārvaldnieks var licencēt tikai noteiktu apmācības vai secinājumu daudzumu (laikā, vai FLOP, vai iespējams žetonos), ko lietotājs var veikt, pēc kā nepieciešama jauna atļauja. Ja pieaugumi ir mazi, tad ir nepieciešama salīdzinoši nepārtraukta modeļa pārlicencēšana. Modeli tad var "izslēgt" vienkārši, nedodot šo licences signālu.[^13]
- *Ātruma ierobežojums*: Modelim tiek liegts darboties ar lielāku secinājumu ātrumu nekā kāda robeža, ko nosaka pārvaldnieks vai citādi. To varētu īstenot ar ierobežotu atļauto savienojumu kopu vai ar izpildu līdzekļiem.
- *Apliecināta apmācība*: Apmācības procedūra var sniegt kriptogrāfiski drošu pierādījumu, ka modeļa ģenerēšanā tika izmantota konkrēta kodu, datu kopa un skaitļošanas jaudas lietošanas apjoms.

## Kā neveidot superintelektu: globāli apmācības un secinājumu skaitļošanas ierobežojumi

Ņemot vērā šos apsvērumus – īpaši attiecībā uz skaitļošanu –, mēs varam apspriest, kā aizvērt Vārtus uz mākslīgo superintelektu; pēc tam mēs vērsīsimies pie pilna MVI novēršanas un AI modeļu pārvaldīšanas, kad tie tuvojas un pārsniedz cilvēka spējas dažādos aspektos.

Pirmā sastāvdaļa ir, protams, izpratne, ka superintelekts nebūtu kontrolējams, un ka tā sekas ir fundamentāli neparedzamas. Vismaz Ķīnai un ASV neatkarīgi jāizlemj šim vai citiem mērķiem neveidot superintelektu.[^14] Pēc tam starp tām un citām nepieciešama starptautiska vienošanās ar spēcīgu verificēšanas un ieviešanas mehānismu, lai nodrošinātu visām pusēm, ka to konkurenti nenovēršas un neizlemj mest kauliņus.

Lai būtu pārbaudāmi un īstenojami, ierobežojumiem jābūt stingriem ierobežojumiem un pēc iespējas viennozīmīgiem. Tas šķiet gandrīz neiespējama problēma: ierobežot sarežģītas programmatūras ar neparedzamām īpašībām spējas visā pasaulē. Par laimi situācija ir daudz labāka nekā šī, jo tieši tas, kas ir padarījis iespējamu progresīvu AI – milzīgs skaitļošanas daudzums – ir daudz, daudz vieglāk kontrolējams. Lai gan tas joprojām varētu atļaut dažas spēcīgas un bīstamas sistēmas, *nekontrolējams superintelekts* visticamāk var tikt novērsts ar stingru neironu tīklā iekļauto skaitļojumu apjoma ierobežojumu, kopā ar ātruma ierobežojumu secinājumu daudzumam, ko AI sistēma (no savienotiem neironu tīkliem un citas programmatūras) var veikt. Šīs konkrēta versija ir piedāvāta zemāk.

Var šķist, ka stingru globālu AI skaitļošanas ierobežojumu uzlikšana prasītu milzīgas starptautiskās koordinācijas un iejaucīgas, privātumu iznīcinošas uzraudzības līmenis. Par laimi, tas nebūtu nepieciešams. Ārkārtīgi [sašaurināta un ierobežota piegādes ķēde](https://arxiv.org/abs/2402.08797) nodrošina, ka, tiklīdz ierobežojums ir likumīgi noteikts (vai nu ar likumu, vai izpildu rīkojumu), šā ierobežojuma ievērošanas verificēšanai būtu nepieciešama tikai dažu lielu uzņēmumu iesaistīšanās un sadarbība.[^15]

Šādam plānam ir vairākas ļoti vēlamas iezīmes. Tas ir minimāli iebrūkošs tādā ziņā, ka tikai dažiem lieliem uzņēmumiem tiek uzliktas prasības, un tikai diezgan nozīmīgi skaitļošanas kopumi tiktu pārvaldīti. Attiecīgajās mikroshēmās jau ir aparatūras iespējas, kas nepieciešamas pirmajai versijai.[^16] Gan īstenošana, gan ieviešana paļaujas uz standarta juridiskajiem ierobežojumiem. Bet tos atbalsta aparatūras lietošanas noteikumi un aparatūras kontroles, ievērojami vienkāršojot ieviešanu un novēršot uzņēmumu, privāto grupu vai pat valstu krāpšanu. Ir bagātīgs precedents tam, ka aparatūras uzņēmumi uzliek attālinātas ierobežojumus savai aparatūras lietošanai un bloķē/atbloķē konkrētas iespējas ārēji,[^17] tostarp pat augstas jaudas CPU datu centros.[^18] Pat salīdzinoši nelielajai aparatūras un organizāciju daļai, kuras tā ietekmē, uzraudzību varētu ierobežot līdz telemetrijai, bez tiešas piekļuves datiem vai modeļiem; un šā programmatūra varētu būt atvērta pārbaudei, lai pierādītu, ka netiek reģistrēti papildu dati. Shēma ir starptautiska un sadarbīga, un diezgan elastīga un paplašināma. Tā kā ierobežojums galvenokārt attiecas uz aparatūru, nevis programmatūru, tas ir salīdzinoši agnostisks attiecībā uz AI programmatūras attīstību un izvietošanu, un ir saderīgs ar dažādām paradigmām, tostarp vairāk "decentralizētu" vai "publisku" AI, kas vērstu uz AI izraisītās varas koncentrācijas apkarošanu.

Skaitļošanā balstītai Vārtu aizvēršanai ir arī trūkumi. Pirmkārt, tā ir tālu no pilnīga risinājuma AI pārvaldības problēmai kopumā. Otrkārt, kā datoru aparatūra kļūst ātrāka, sistēma "noķertu" arvien vairāk aparatūras arvien mazākos kopumos (vai pat atsevišķos GPU).[^19] Ir arī iespējams, ka algoritmisku uzlabojumu dēļ būtu nepieciešams pat zemāks skaitļošanas ierobežojums,[^20] vai ka skaitļošanas daudzums kļūst lielākoties neatbilstošs, un Vārtu aizvēršanai tā vietā būtu nepieciešams detalizētāks uz risku vai spējām balstīts AI pārvaldības režīms. Treškārt, neatkarīgi no garantijām un neliela skarto organizāciju skaita, šāda sistēma neapšaubāmi radīs pretestību attiecībā uz privātumu un uzraudzību, citu problēmu vidū.[^21]

Protams, skaitļošanas ierobežojošas pārvaldības shēmas izstrādāšana un īstenošana īsā laika periodā būs diezgan izaicinoša. Bet tas noteikti ir izdarāms.

## A-G-I: trīskāršā krustošanās kā riska un politikas pamats

Pievērsīsimies tagad MVI. Stingrās līnijas un definīcijas šeit ir grūtākas, jo mums noteikti ir intelekts, kas ir mākslīgs un vispārējs, un pēc nevienas esošās definīcijas visi nepiekritīs, vai un kad tas pastāv. Turklāt, skaitļošanas vai secinājumu ierobežojums ir diezgan neelastīgs rīks (skaitļošana ir spēju pilnvara, kas tad ir riska pilnvara), kas – ja vien tas nav diezgan zems – nav ticams, ka novērsīs MVI, kas ir pietiekami spēcīgs, lai izraisītu sociālos vai civilizācijas traucējumus vai akūtus riskus.

Es esmu apgalvojis, ka visas akūtākās riskuus rodas no trīskāršās krustošanās starp ļoti augstām spējām, augstu autonomiju un lielu vispārīgumu. Šīs ir sistēmas, kas – ja tās vispār tiek izstrādātas – jāpārvalda ar milzīgu piesardzību. Radot stingrus standartus (ar atbildības un regulējuma palīdzību) sistēmām, kas apvieno visas trīs īpašības, mēs varam virzīt AI attīstību uz drošākām alternatīvām.

Kā ar citām nozarēm un produktiem, kas potenciāli varētu kaitēt patērētājiem vai sabiedrībai, AI sistēmām nepieciešams rūpīgs regulējums no efektīvu un pilnvarotu valdības aģentūru puses. Šim regulējumam jāatzīst MVI riskantums, un jānovērš nepieņemami riskanti augstas jaudas AI sistēmu izstrādāšana.[^22]

Tomēr plaša mēroga regulējums, īpaši ar īstiem zobiem, kas noteikti tiks pretotos nozares,[^23] prasa laiku[^24], kā arī politisko pārliecību, ka tas ir nepieciešams.[^25] Ņemot vērā progresa tempu, tas var prasīt vairāk laika, nekā mums ir pieejams.

Daudz ātrākā laika skalā un, kamēr tiek izstrādāti regulatīvie pasākumi, mēs varam dot uzņēmumiem nepieciešamos stimulus (a) atturēties no ļoti augstas riska aktivitātēm un (b) izstrādāt visaptverošas sistēmas riska novērtēšanai un mazināšanai, noskaidrojot un palielinot atbildības līmeņus bīstamākajām sistēmām. Ideja būtu uzlikt visaugstākos atbildības līmeņus – stingrus un dažos gadījumos personiskus kriminālos – sistēmām trīskāršajā autonomijas-vispārīguma-intelekta krustošanās, bet nodrošināt "drošas ostas" uz vairāk tipisko vainas balstīto atbildību sistēmām, kurās trūkst vienas no šīm īpašībām vai tā ir garantēti pārvaldāma. Tas ir, piemēram, "vāja" sistēma, kas ir vispārēja un autonoma (kā spējīgs un uzticams, bet ierobežots personiskais asistents) būtu pakļauta zemākiem atbildības līmeņiem. Tāpat šaura un autonoma sistēma, piemēram, pašbraucošs auto, joprojām būtu pakļauta nozīmīgajam regulējumam, kāds tai jau ir, bet ne pastiprinātai atbildībai. Līdzīgi augstas spējas un vispārējai sistēmai, kas ir "pasīva" un lielākoties nespējīga uz neatkarīgu rīcību. Sistēmām, kurām trūkst *divu* no trim īpašībām, ir vēl vairāk pārvaldāmas, un drošo ostu būtu vēl vieglāk pieprasīt. Šī pieeja atspoguļo to, kā mēs rīkojamies ar citām potenciāli bīstamām tehnoloģijām:[^26] augstāka atbildība par bīstamākām konfigurācijām rada dabiskus stimulus drošākām alternatīvām.

Šādu augstu atbildības līmeņu noklusējuma rezultāts, kas darbojas, lai MVI risku *internalizētu* uzņēmumos, nevis to uzslēgtu sabiedrībai, ir ticams (un cerams!) uzņēmumiem vienkārši neizstrādāt pilnu MVI, kamēr un ja vien tie nevar to patiešām padarīt uzticamu, drošu un kontrolējamu, ņemot vērā, ka *pašu vadība* ir riskam pakļautā puse. (Gadījumā, ja tas nav pietiekams, likumdošanā, kas noskaidro atbildību, arī būtu skaidri jāatļauj aizlieguma atvieglojums, t.i., tiesnesim pavēlēt apturēt aktivitātes, kas ir skaidri bīstamības zonā un argumentēti rada sabiedriskas riska.) Kad stājas spēkā regulējums, regulējuma ievērošana var kļūt par drošo ostu, un drošās ostas no zemas AI sistēmu autonomijas, šaurības vai vājuma var pārveidoties salīdzinoši vieglākos regulatīvos režīmos.

## Galvenās Vārtu aizvēršanas noteikumu daļas

Ņemot vērā iepriekš minēto diskusiju, šajā sadaļā ir sniegti priekšlikumi galvenajām daļām, kas īstenotu un uzturētu aizliegumu attiecībā uz pilnu MVI un superintelektu, kā arī cilvēka konkurētspējīgu vai ekspertu konkurētspējīgu vispārēju AI pārvaldīšanu tuvu pilna MVI slieksnim.[^27] Tai ir četras galvenās daļas: 1) skaitļošanas uzskaite un uzraudzība, 2) skaitļošanas ierobežojumi AI apmācībā un darbībā, 3) atbildības ietvars un 4) pakāpenaini drošības un drošuma standarti, kas ietver stingras regulatīvās prasības. Tie ir sīki aprakstīti turpmāk, ar sīkākām detaļām vai īstenošanas piemēriem, kas sniegti trīs pievienotajās tabulās. Svarīgi atzīmēt, ka tie ir tālu no visa, kas būs nepieciešams progresīvu AI sistēmu pārvaldīšanai; lai gan tiem būs papildu drošības un drošuma ieguvumi, tie ir vērsti uz Vārtu aizvēršanu intelekta nekontrolējamai izaugsmei un AI attīstības novirzīšanu labākā virzienā.

### 1\. Skaitļošanas uzskaite un caurspīdīgums

- Standartu organizācijai (piemēram, NIST ASV, kam seko ISO/IEEE starptautiski) jākodificē detalizēts tehnisks standarts kopējai skaitļošanai, kas izmantota AI modeļu apmācībā un darbībā, FLOP vienībās, un ātrumam FLOP/s, ar kādu tie darbojas. Detaļas tam, kā tas varētu izskatīties, ir sniegtas A pielikumā.[^28]
- Prasība – vai nu ar jauniem tiesību aktiem, vai saskaņā ar esošajām pilnvarām[^29] – jurisdikcijām, kurās notiek liela mēroga AI apmācība, jāuzliek pienākums aprēķināt un ziņot regulatīvajai struktūrai vai citai aģentūrai kopējo FLOP, kas izmantots apmācībā un visu modeļu darbībā virs 10<sup>25</sup> FLOP vai 10<sup>18</sup> FLOP/s sliekšņa.[^30]
- Šīs prasības jāievieš pakāpeniski, sākotnēji prasot labi dokumentētus labticīgus aprēķinus uz ceturkšņa pamata, ar vēlākām fāzēm, kas prasa progresīvi augstākus standartus, līdz kriptogrāfiski apliecinātam kopējam FLOP un FLOP/s, kas pievienots katram modeļa *izvadījumam*.
- Šiem ziņojumiem jābūt papildināti ar labi dokumentētiem robežizmaksu enerģijas un finansiālo izmaksu aprēķiniem, kas izmantoti katra AI izvadījuma ģenerēšanā.

Pamatojums: Šie labi aprēķinātie un caurspīdīgi ziņotie skaitļi būtu pamats apmācības un darbības ierobežojumiem, kā arī droša osta no augstākiem atbildības pasākumiem (skatīt C un D pielikumus).

### 2\. Apmācības un darbības skaitļošanas ierobežojumi

- Jurisdikcijām, kas uztur AI sistēmas, jāuzliek stingrs ierobežojums kopējai skaitļošanai, kas iet jebkura AI modeļa izvadījumā, sākot ar 10<sup>27</sup> FLOP[^31] un regulējams pēc nepieciešamības.
- Jurisdikcijām, kas uztur AI sistēmas, jāuzliek stingrs ierobežojums AI modeļu izvadījumu skaitļošanas ātrumam, sākot ar 10<sup>20</sup> FLOP/s un regulējams pēc nepieciešamības.

Pamatojums: Kopējā skaitļošana, lai gan ļoti nepilnīga, ir AI spēju (un riska) pilnvara, kas ir konkrēti mērāma un pārbaudāma, tāpēc nodrošina stingru atbalstu spēju ierobežošanai. Konkrēts īstenošanas priekšlikums ir sniegts B pielikumā.

### 3\. Pastiprinātā atbildība bīstamām sistēmām

- Progresīvas AI sistēmas, kas ir ļoti vispārēja, spējīga un autonoma, radīšanai un darbībai[^32] likumdošanas ceļā jānoskaidro, ka tā ir pakļauta stingrai, kopīgai un vairāku pušu, nevis vienas puses vainas balstītai atbildībai.[^33]
- Jābūt pieejamam juridiskam procesam apstiprinošu drošības lietu iesniegšanai, kas piešķirtu drošo ostu no stingras atbildības sistēmām, kas ir mazas (skaitļošanas izteiksmē), vājas, šauras, pasīvas vai kurām ir pietiekamas drošības, drošuma un kontrolējamības garantijas.
- Jāizklāsta skaidrs ceļš un nosacījumu kopa aizlieguma atvieglojumam, lai apturētu AI apmācības un secinājumu darbības, kas rada sabiedrisku bīstamību.

Pamatojums: AI sistēmas nevar būt atbildīgas, tāpēc mums jāpadara atbildīgas cilvēku personas un organizācijas par kaitējumu, ko tās rada (atbildība).[^34] Nekontrolējams MVI ir sabiedrības un civilizācijas apdraudējums, un drošības lietas trūkuma gadījumā to jāuzskata par nenormāli bīstamu. Uzliekot atbildības nastu izstrādātājiem pierādīt, ka spēcīgi modeļi ir pietiekami droši, lai tos neuzskatītu par "nenormāli bīstamiem", stimulē drošu attīstību, kā arī caurspīdīgumu un ierakstu glabāšanu, lai pieprasītu šīs drošās ostas. Pēc tam regulējums var novērst kaitējumu tur, kur atbildības atturēšana ir nepietiekama. Visbeidzot, AI izstrādātāji jau ir atbildīgi par kaitējumiem, ko tie rada, tāpēc atbildības juridiska noskaidrošana vissvarīgākajām sistēmām var tikt darīta nekavējoties, neizstrādājot ļoti detalizētus standartus; tie tad var attīstīties laika gaitā. Detaļas sniegtas C pielikumā.

### 4\. AI drošības regulējums

Regulatīva sistēma, kas risina liela mēroga AI akūtos riskus, minimāli prasīs:

- Atbilstošu regulatīvo struktūru identificēšanu vai radīšanu, iespējams, jaunu aģentūru;
- Visaptverošu riska novērtēšanas ietvaru;[^35]
- Ietvaru apstiprinošu drošības lietu iesniegšanai, daļēji balstītu uz riska novērtēšanas ietvaru, ko veic izstrādātāji, un *neatkarīgu* grupu un aģentūru auditam;
- Pakāpenotu licencēšanas sistēmu ar līmeņiem, kas seko spēju līmeņiem.[^36] Licences tiktu piešķirtas, balstoties uz drošības lietām un auditiem, sistēmu attīstībai un izvietošanai. Prasības svārstītos no paziņošanas zemajā galā līdz kvantitāti drošības, drošuma un kontrolējamības garantijām pirms attīstības augšgalā. Šīs novērstu sistēmu izlaišanu, kamēr tās nav pierādīti drošas, un aizliedzēt pašos pamatos nedrošu sistēmu attīstību. D pielikums sniedz priekšlikumu tam, ko šādi drošības un drošuma standarti varētu ietvert.
- Vienošanās šādus pasākumus nest uz starptautisko līmeni, tostarp starptautiskas struktūras normu un standartu saskaņošanai, un potenciāli starptautiskas aģentūras drošības lietu pārskatīšanai.

Pamatojums: Galu galā, atbildība nav pareizais mehānisms, lai novērstu liela mēroga risku sabiedrībai no jaunas tehnoloģijas. Visaptverošs regulējums ar pilnvarotām regulatīvajām struktūrām būs nepieciešams AI tāpat kā katrai citai lielajai nozarei, kas rada risku sabiedrībai.[^37]

Regulējums, lai novērstu citus izplatītus, bet mazāk akūtus riskus, visticamāk atšķirsies savā formā no jurisdikcijas līdz jurisdikcijai. Svarīgais ir izvairīties no to AI sistēmu attīstīšanas, kas ir tik riskants, ka šie riski ir nepārvaldāmi.

## Kas tad?

Nākamajā desmitgadē, kad AI kļūst izplatītāks un pamattehnoloģija attīstās, visticamāk notiks divas svarīgas lietas. Pirmkārt, esošo spēcīgo AI sistēmu regulēšana kļūs sarežģītāka, tomēr vēl nepieciešamāka. Ir ticams, ka vismaz daži pasākumi, kas risina liela mēroga drošības riskus, prasīs vienošanos starptautiskā līmenī, ar atsevišķām jurisdikcijām, kas īsteno noteikumus, pamatojoties uz starptautiskajām vienošanām.

Otrkārt, apmācības un darbības skaitļošanas ierobežojumus kļūs grūtāk uzturēt, jo aparatūra kļūs lētāka un izmaksu ziņā efektīvāka; tie var kļūt arī mazāk nozīmīgi (vai būs jābūt pat stingrākiem) ar algoritmu un arhitektūru uzlabojumiem.

Tas, ka AI kontrolēšana kļūs grūtāka, nenozīmē, ka mums jāpadodas! Šajā esejā izklāstītā plāna īstenošana dotu mums gan vērtīgu laiku, gan izšķirošu kontroli pār procesu, kas mūs ievietotu tālu, tālu labākā pozīcijā, lai izvairītos no AI eksistenciālā riska mūsu sabiedrībai, civilizācijai un sugai.

Vēl ilgtermiņā būs jāveic izvēles par to, ko mēs atļaujam. Mēs varam izvēlēties joprojām radīt kādu patiešām kontrolējama MVI formu, ciktāl tas izrādās iespējams. Vai mēs varam izlemt, ka pasaules vadīšanu ir labāk atstāt mašīnām, ja varam sevi pārliecināt, ka tās darīs to labāk un izturēsies pret mums labi. Bet šiem jābūt lēmumiem, kas pieņemti ar dziļu zinātnisku AI izpratni rokās, un pēc nozīmīgas globālas iekļaujošas diskusijas, nevis sacīkstē starp tehnoloģiju moguliem, kurā lielākā daļa cilvēces ir pilnīgi neiesaistīta un neapzinās.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) A-G-I un superintelekta pārvaldības kopsavilkums ar atbildības un regulējuma palīdzību. Atbildība ir augstākā un regulējums stiprākais Autonomijas, Vispārīguma un Intelekta trīskāršajā krustošanās. Drošās ostas no stingras atbildības un spēcīga regulējuma var iegūt ar apstiprinošām drošības lietām, kas pierāda, ka sistēma ir vāja un/vai šaura un/vai pasīva. Ierobežojumi kopējai Apmācības Skaitļošanai un Secinājumu Skaitļošanas ātrumam, kas pārbaudīti un īstenoti likumīgi un izmantojot aparatūras un kriptogrāfiskos drošības pasākumus, atbalsta drošību, izvairšuoties no pilna MVI un efektīvi aizliedzot superintelektu.


[^1]: Visticamāk, šīs izpratnes izplatīšanās prasīs vai nu intensīvas pūles no izglītības un aizstāvības grupām, kas izvirza šo argumentu, vai diezgan nozīmīgu AI izraisītu katastrofu. Mēs varam cerēt, ka tā būs pirmā.

[^2]: Paradoksāli, mēs esam pieraduši, ka Daba ierobežo mūsu tehnoloģijas, padarot tās ļoti grūti attīstāmas, īpaši zinātniski. Bet tas vairs neattiecas uz AI: galvenās zinātniskās problēmas izrādās vieglākas nekā paredzēts. Mēs nevaram paļauties uz to, ka Daba mūs glābs no sevis – tas būs jādara mums pašiem.

[^3]: Kur tieši mēs apstājamies jaunu sistēmu attīstīšanā? Šeit mums jāpieņem piesardzības princips. Tiklīdz sistēma ir izvietota, un īpaši, tiklīdz šāda sistēmas spēju līmenis izplatās, to ir ārkārtīgi grūti atgriezt. Un ja sistēma ir *izstrādāta* (īpaši ar lielām izmaksām un pūlēm), būs milzīgs spiediens to lietot vai izvietot, un kārdinājums tam tikt noplūdinātu vai nozagtam. Sistēmu attīstīšana un *tad* lēmums par to, vai tās ir dziļi nedrošas, ir bīstams ceļš.

[^4]: Būtu arī gudri aizliegt AI attīstību, kas ir pašos pamatos bīstama, piemēram, sevis reproducējošas un evolucionējošas sistēmas, tās, kas paredzētas ieslodzījuma bēgšanai, tās, kas var autonomi sevis uzlabot, apzināti maldinošs un ļaunprātīgs AI u.c.

[^5]: Ņemiet vērā, ka tas ne nepieciešami nenozīmē *īstenotu* starptautiskā līmenī ar kādu globālu struktūru: tā vietā suverēnas valstis varētu īstenot saskaņotas likumus, kā daudzos līgumos.

[^6]: Kā mēs redzēsim zemāk, AI skaitļošanas raksturs atļautu kaut ko hibrīda; bet starptautiska sadarbība joprojām būs nepieciešama.

[^7]: Piemēram, AI attiecīgo mikroshēmu ēšanu nepieciešamos aparātus ražo tikai viena firma, ASML (neraugoties uz daudziem citiem mēģinājumiem to darīt), lielāko daļu attiecīgo mikroshēmu ražo viena firma, TSMC (neraugoties uz citiem, kas mēģina konkurēt), un aparatūras projektēšanu un būvniecību no šīm mikroshēmām veic tikai daži, tostarp NVIDIA, AMD un Google.

[^8]: Vissvarīgākais, katra mikroshēma tur unikālu un nepieejamu kriptogrāfisko privāto atslēgu, ko tā var izmantot lietu "parakstīšanai".

[^9]: Pēc noklusējuma tas būtu uzņēmums, kas pārdod mikroshēmas, bet citi modeļi ir iespējami un potenciāli noderīgi.

[^10]: Pārvaldnieks var noskaidrot mikroshēmas atrašanās vietu, nosakot parakstītu ziņojumu apmaiņas laiku ar to: gaismas galīgais ātrums prasa mikroshēmai atrasties noteikta rādiusa *r* ietvaros no "stacijas", ja tā var atgriezt parakstītu ziņojumu laikā, kas mazāks par *r* / *c*, kur *c* ir gaismas ātrums. Izmantojot vairākas stacijas un zināmu tīkla īpašību izpratni, mikroshēmas atrašanās vietu var noteikt. Šās metodes skaistums ir tāds, ka lielāko daļu tās drošības nodrošina fizikas likumi. Citas metodes varētu izmantot GPS, inerciālā izsekošanu un līdzīgas tehnoloģijas.

[^11]: Alternatīvi, mikroshēmu pāriem varētu atļaut sazināties vienu ar otru tikai ar pārvaldnieka skaidru atļauju.

[^12]: Tas ir izšķiroši, jo vismaz pašlaik ir nepieciešams ļoti augstas caurlaidības savienojums starp mikroshēmām, lai uz tām apmācītu lielajus AI modeļus.

[^13]: To varētu arī uzstādīt, lai prasītu parakstītus ziņojumus no *N* no *M* dažādiem pārvaldniekiem, ļaujot vairākām pusēm dalīties pārvaldībā.

[^14]: Tas nav ne tuvu bezprecendenta – piemēram, armijas nav attīstījušas klonētu vai ģenētiski pārveidotu superkaravīru armijas, lai gan tas, iespējams, ir tehnoloģiski iespējams. Bet tās ir *izvēlējušās* to nedarīt, nevis tās ir novērsušas citas. Ieraksts nav labs tam, ka lielās pasaules varas tiek novērstas no tehnoloģijas attīstīšanas, ko tās stipri vēlas attīstīt.

[^15]: Ar dažiem ievērojamiem izņēmumiem (īpaši NVIDIA) AI specializētā aparatūra ir salīdzinoši neliela šo uzņēmumu vispārējā biznesa un ieņēmumu modeļa daļa. Turklāt plaisa starp aparatūru, kas izmantota progresīvā AI, un "patērētāju klases" aparatūru ir nozīmīga, tāpēc lielākā daļa datoru aparatūras patērētāju būtu lielākoties neietekmēti.

[^16]: Sīkākai analīzei skatiet nesenos ziņojumus no [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) un [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). Tie koncentrējas uz tehnisku iespējamību, īpaši ASV eksporta kontroles kontekstā, cenšoties ierobežot citu valstu augstas jaudas skaitļošanas spējas; bet tam ir acīmredzama pārklāšanās ar šeit iedomāto globālo ierobežojumu.

[^17]: Apple ierīces, piemēram, tiek attālināti un droši bloķētas, kad tiek ziņots par to pazaudēšanu vai nozagšanu, un tās var tikt attālināti atkal aktivizētas. Tas paļaujas uz tām pašām aparatūras drošības funkcijām, kas te apspriešanas.

[^18]: Skatīt, piemēram, IBM [capacity on demand](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) piedāvājumu, Intel [Intel on demand](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html) un Apple [private cloud compute](https://security.apple.com/blog/private-cloud-compute/).

[^19]: [Šajā pētījumā](https://epochai.org/trends#hardware-trends-section) rādīts, ka vēsturiski viena un tā pati veiktspēja ir sasniegta, izmantojot apmēram 30% mazāk dolāru gadā. Ja šī tendence turpinās, var būt nozīmīga AI un "patērētāju" mikroshēmu lietošanas pārklāšanās, un vispār nepieciešamās aparatūras daudzums augstas jaudas AI sistēmām varētu kļūt nepatīkami mazs.

[^20]: Pēc [tā paša pētījuma](https://epochai.org/trends#hardware-trends-section), dotajai veiktspējai attēlu atpazīšanā ir nepieciešams 2,5 reizes mazāk skaitļojumu katru gadu. Ja tas arī tiktu attiecināts uz visaptverošākajām AI sistēmām, skaitļošanas ierobežojums ļoti ilgi nebūtu noderīgs.

[^21]: Īpaši valstu līmenī tas izskatās daudz kā skaitļošanas nacionalizācija, jo valdībai būtu daudz kontroles pār to, kā tiek izmantota skaitļošanas jauda. Tomēr tiem, kas satraucas par valdības iesaistīšanos, tas šķiet daudz drošāks un labāk nekā visas spēcīgākās AI programmatūras *pašas* nacionalizēšana ar kādu apvienošanos starp lieliem AI uzņēmumiem un nacionālajām valdībām, kā daži sāk aizstāvēt.

[^22]: Svarīgs regulatīvs solis Eiropā tika veikts ar 2024. gada [ES AI akta](https://artificialintelligenceact.eu/) pieņemšanu. Tas klasificē AI pēc riska: aizliedzot nepieņemamas sistēmas, regulējot augstas riska sistēmas un uzliekot caurspīdīguma likumus vai nekādus pasākumus zema riska sistēmām. Tas būtiski samazinās dažus AI riskus un palielinās AI caurspīdīgumu pat ASV firmām, bet tam ir divi galvenie trūkumi. Pirmkārt, ierobežots pārklājums: lai gan tas attiecas uz jebkuru uzņēmumu, kas nodrošina AI ES, ieviešana pār ASV bāzētām firmām ir vāja, un militārais AI ir atbrīvots. Otrkārt, lai gan tas aptver GPAI, tas neatpazīst MVI vai superintelektu kā nepieņemamus riskus vai nekovērš to attīstību – tikai to ES izvietošanu. Tā rezultātā tas maz dara, lai ierobežotu MVI vai superintelekta riskus.

[^23]: Uzņēmumi bieži pārstāv, ka tie ir par saprātīgu regulējumu. Bet kā nu nekā tie gandrīz vienmēr šķiet pretojusies jebkuram *konkrētam* regulējumam; liecinieks cīņa par diezgan mazāk skārošo SB1047, ko [lielākā daļa AI uzņēmumu publiski vai privāti pretojās](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/).

[^24]: Bija apmēram 3 1/2 gadi no laika, kad tika ierosināts ES AI akts, līdz tas stājās spēkā.

[^25]: Dažreiz tiek izteikts, ka ir "pārāk agri" sākt regulēt AI. Ņemot vērā pēdējo piezīmi, tas šķiet maz ticams. Vēl viens izteiktais bažas ir, ka regulējums "kaitētu inovācijai". Bet labs regulējums tikai maina virzienu, nevis daudzumu inovācijai.

[^26]: Interesants precedents ir bīstamu materiālu transportēšanā, kas varētu izbēgt un radīt kaitējumu. Šeit [regulējums](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) un [judikatūra](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) ir izveidojuši stingru atbildību ļoti bīstamiem materiāliem, piemēram, sprāgstvielām, benzīnam, indiem, infekcioziem līdzekļiem un radioaktīvajiem atkritumiem. Citi piemēri ietver [brīdinājumus farmācijas produktos](https://www.medicalnewstoday.com/articles/boxed-warnings), [medicīnas ierīču klases](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification) u.c.

[^27]: Cits visaptverošs priekšlikums ar līdzīgiem mērķiem, kas izvirzīts ["A Narrow Path"](https://www.narrowpath.co/), aizstāv centralizētāku, aizlieguma balstītu pieeju, kas visu robežas AI attīstību virza caur vienu starptautisku struktūru, ko pārrauga spēcīgas starptautiskas institūcijas, ar skaidriem kategorisku aizliegumu, nevis pakāpenaitu ierobežojumu. Es arī atbalstītu šo plānu; tomēr tas prasīs vēl vairāk politisku gribu un koordināciju nekā šeit ierosināts.

[^28]: Dažas vadlīnijas šādam standartam tika [publicētas](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/) Frontier Model Forum. Attiecībā pret šeit ierosināto priekšlikumu, tie maldās mazākas precizitātes pusē un mazāk skaitļošanas, kas iekļauts aprēķinā.

[^29]: 2023. gada ASV AI izpildu rīkojums (tagad atsaukts) prasīja līdzīgu, bet mazāk smalkgraudainu ziņošanu. To vajadzētu pastiprināt ar aizvietojošu rīkojumu.

[^30]: Ļoti aptuveni, tagadējām ierasts H100 mikroshēmām tas atbilst apmēram 1000 kopu, kas veic secinājumus; tas ir apmēram 100 (apmēram 5 miljoni USD vērtībā) visno visjauno augšklases NVIDIA B200 mikroshēmu, kas veic secinājumus. Abos gadījumos apmācības skaitlis atbilst tam kopu skaitļošanai vairākus mēnešus.

[^31]: Šis daudzums ir lielāks nekā jebkura pašlaik apmācīta AI sistēma; lielāks vai mazāks skaitlis varētu būt pamatots, kad mēs labāk izprotam, kā AI spējas mērogojuma ar skaitļošanu.

[^32]: Tas attiecas uz tiem, kas izveido un sniedz/uztur modeļus, nevis galalietotājiem.

[^33]: Aptuveni, "stingra" atbildība nozīmē, ka izstrādātāji tiek turēti atbildīgi par produkta nodarītiem kaitējumiem *pēc noklusējuma* un ir standarts, kas izmantots "nenormāli bīstamiem" produktiem un (diezgan amizanti, bet piemēroti) savvaļas dzīvniekiem. "Kopīga un vairāku pušu" atbildība nozīmē, ka atbildība tiek piešķirta visām par produktu atbildīgajām pusēm, un šīm pusēm ir jāizšķir savstarpēji, kas nes kādu atbildību. Tas ir svarīgi sistēmām kā AI ar garu un sarežģītu vērtību ķēdi.

[^34]: Standarta vainas balstīta vienas puses atbildība nav pietiekama: vaina būs grūti gan izsekojama, gan piešķirami, jo AI sistēmas ir sarežģītas, to darbība nav saprotama, un daudz pušu var būt iesaistītas bīstamas sistēmas vai izvadījuma radīšanā. Turklāt tiesvedībām būs nepieciešami gadi adjudikācijai un visticamāk radīsies tikai sodi, kas ir nekonsekventi šiem uzņēmumiem, tāpēc ir svarīga arī personāla atbildība izpildvadībai.

[^35]: Nevajadzētu būt atbrīvojumam no drošības kritērijiem atvērta svara modeļiem. Turklāt, novērtējot risku, jāpieņem, ka drošības barjeras, kas var tikt noņemtas, tiks noņemtas no plaši pieejamajiem modeļiem, un ka pat aizvērtie modeļi izplatīsies, ja vien nav ļoti augstu garantiju, ka tie paliks droši.

[^36]: Šeit ierosināta shēma ar regulatīvu uzraudzību, ko aktivizē vispārējā spējā; tomēr ir saprātīgi dažiem īpaši riskantiem lietošanas gadījumiem aktivizēt vairāk uzraudzības – piemēram, ekspertu viroloģijas AI sistēmai, pat ja šaura un pasīva, iespējams, jāiet augstākā līmenī. Bijušajam ASV izpildu rīkojumam bija kaut kas no šīs struktūras bioloģiskajām spējām.

[^37]: Divi skaidri piemēri ir aviācija un medikamenti, ko regulē FAA un FDA, kā arī līdzīgas aģentūras citās valstīs. Šīs aģentūras ir nepilnīgas, bet ir bijušas absolūti vitāli šo nozaru funkcionēšanai un veiksme.