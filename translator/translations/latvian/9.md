# 9. nodaļa - Nākotnes veidošana — ko mums vajadzētu darīt tā vietā

AI var darīt neticami daudz laba pasaulē. Lai iegūtu visas priekšrocības bez riskiem, mums jānodrošina, ka AI paliek cilvēka rīks.

Ja mēs veiksmīgi izvēlēsimies neaizstāt cilvēci ar mašīnām — vismaz uz kādu laiku! — ko mēs varam darīt tā vietā? Vai mēs atsakāmies no AI milzīgajām iespējām kā tehnoloģijai? Zināmā mērā atbilde ir vienkārša *nē:* aizveriet Vārtus nekontrolējamam MVI un superintelektam, bet *jā* veidojiet daudzas citas AI formas, kā arī pārvaldības struktūras un institūcijas, kas mums būs nepieciešamas to vadīšanai.

Bet joprojām ir daudz ko teikt; šīs realizācijas īstenošana būtu cilvēces centrālā nodarbošanās. Šajā sadaļā aplūkoti vairāki galvenie temati:

- Kā mēs varam raksturot "instrumentālo" AI un formas, kādas tas var pieņemt.
- Ka mēs varam iegūt (gandrīz) visu, ko cilvēce vēlas, bez MVI, ar instrumentālo AI.
- Ka instrumentālās AI sistēmas ir (iespējams, principā) pārvaldāmas.
- Ka atgriešanās no MVI nenozīmē kompromisu attiecībā uz nacionālo drošību — gluži pretēji.
- Ka varas koncentrācija ir reālas bažas. Vai mēs varam to mazināt, negraujot drošību un aizsardzību?
- Ka mums būs vajadzīgas — un nepieciešamas — jaunas pārvaldības un sociālās struktūras, un AI faktiskajā var palīdzēt.

## AI Vārtu iekšpusē: instrumentālais AI

Trīskāršās krustošanās diagramma dod labu veidu, kā norobežot to, ko mēs varam saukt par "instrumentālo AI": AI, kas ir kontrolējams rīks cilvēka lietošanai, nevis nekontrolējams konkurents vai aizstājējs. Vismazāk problemātiskās AI sistēmas ir tās, kas ir autonomas, bet nav vispārējas vai īpaši spējīgas (piemēram, izsoles piedāvājumu bots), vai vispārējas, bet nav autonomas vai spējīgas (piemēram, mazs valodas modelis), vai spējīgas, bet šauras un ļoti kontrolējamas (piemēram, AlphaGo).[^1] Tās ar divām krustojošām funkcijām ir ar plašāku pielietojumu, bet augstāku risku un prasīs lielus pūliņus to pārvaldīšanai. (Tikai tāpēc, ka AI sistēma vairāk ir rīks, tas nenozīmē, ka tā ir pašsaprotami droša, tikai ka tā nav pašsaprotami *nedroša* — apsveriet motorzāģi salīdzinājumā ar mājas tīģeri.) Vārtiem jāpaliek aizvertiem (pilnībai) MVI un superintelektam trīskāršajā krustojumā, un milzīga uzmanība jāvelta AI sistēmām, kas tuvojas šim slieksnim.

Bet tas atstāj daudz jaudīga AI! Mēs varam iegūt milzīgu labumu no gudiem un vispārējiem pasīviem "orākuliem" un šaurām sistēmām, vispārējām sistēmām cilvēka, bet ne pārcilvēciska līmeņa, un tā tālāk. Daudzas tehnoloģiju kompānijas un izstrādātāji aktīvi veido šāda veida rīkus un tiem vajadzētu turpināt; tāpat kā lielākā daļa cilvēku, viņi netieši *pieņem*, ka Vārti uz MVI un superintelektu būs aizvērti.[^2]

Tāpat AI sistēmas var efektīvi kombinēt kompozītās sistēmās, kas uztur cilvēka uzraudzību, vienlaikus uzlabojot spējas. Tā vietā, lai paļautos uz neizprotamiem melnajiem kastēm, mēs varam veidot sistēmas, kur vairāki komponenti — ieskaitot gan AI, gan tradicionālo programmatūru — darbojas kopā veidā, ko cilvēki var uzraudzīt un saprast.[^3] Lai gan daži komponenti varētu būt melnas kastes, neviens nebūtu tuvu MVI — tikai kompozītā sistēma kopumā būtu gan ļoti vispārēja, gan ļoti spējīga, un strikti kontrolējamā veidā.[^4]

### Nozīmīga un garantēta cilvēka kontrole

Ko nozīmē "strikti kontrolējama"? Galvenā "instrumentālā" ietvara ideja ir atļaut sistēmas — pat ja diezgan vispārējas un jaudīgas — kas garantēti ir nozīmīgas cilvēka kontroles pakļautībā. Ko tas nozīmē? Tas ietver divus aspektus. Pirmais ir projektēšanas apsvērums: cilvēkiem vajadzētu būt dziļi un centrāli iesaistītiem tajā, ko sistēma dara, *nedeleģējot* galvenos svarīgos lēmumus AI. Tas ir lielākās daļas pašreizējo AI sistēmu raksturs. Otrkārt, ciktāl AI sistēmas ir autonomas, tām jābūt garantijām, kas ierobežo to darbības jomu. Garantijai vajadzētu būt *skaitlim*, kas raksturo kaut kā notikšanas varbūtību, un iemeslam šim skaitlim ticēt. Tas ir tas, ko mēs pieprasām citos drošības kritiskajos laukos, kur skaitļi kā "vidējais laiks starp atkazēm" un paredzamais negadījumu skaits tiek aprēķināts, pamatots un publicēts drošības lietās.[^5] Ideālais neveiksmju skaits, protams, ir nulle. Un labā ziņa ir tā, ka mēs varētu nokļūt diezgan tuvu, lai gan izmantojot diezgan atšķirīgas AI arhitektūras, izmantojot *formāli verificētu* programmu īpašību idejas (ieskaitot AI). Ideju, ko plaši pētījuši Omohundro, Tegmarks, Bendžo, Dalrimple un citi (skatīt [šeit](https://arxiv.org/abs/2309.01933) un [šeit](https://arxiv.org/abs/2405.06624)) ir izveidot programmu ar noteiktām īpašībām (piemēram: ka cilvēks to var izslēgt) un formāli *pierādīt*, ka šīs īpašības pastāv. To var izdarīt tagad diezgan īsām programmām un vienkāršām īpašībām, bet AI darbinātas pierādīšanas programmatūras (tuvojošais) spēks varētu to atļaut daudz sarežģītākām programmām (piemēram, iesaiņotājiem) un pat pašam AI. Tas ir ļoti ambiciozs projekts, bet, palielinoties spiedienam uz Vārtiem, mums būs vajadzīgi daži spēcīgi materiāli to nostiprināšanai. Matemātiskais pierādījums var būt viens no nedaudzajiem, kas ir pietiekami stiprs.

### Kur dodas AI industrija

Ar AI progresu, kas pārvirzīts, instrumentālais AI joprojām būtu milzīga industrija. Aparatūras ziņā, pat ar skaitļošanas jaudas ierobežojumiem, lai novērstu superintelektu, apmācība un secinājumu izdarīšana mazākos modeļos joprojām prasīs milzīgas specializētu komponentu daudzumus. Programmatūras pusē, AI modeļa un skaitļošanas lieluma eksplozijas neitralizēšanai vienkārši vajadzētu novest pie tā, ka kompānijas pārvirza resursus, lai padarītu mazākās sistēmas labākas, daudzveidīgākas un specializētākas, nevis vienkārši tās lielākas.[^6] Būtu pietiekami daudz vietas — vairāk iespējams — visiem tiem peļņas gūšanas Silīcija ielejas jaunuzņēmumiem.[^7]

## Instrumentālais AI var dot (gandrīz) visu, ko cilvēce vēlas, bez MVI

Intelektu, vai tas būtu bioloģisks vai mašīnu, var plaši uzskatīt par spēju plānot un īstenot darbības, kas rada nākotnes, kas vairāk atbilst mērķu kopumam. Kā tāds, intelekts ir milzīgs labums, kad tiek lietots gudri izvēlētu mērķu īstenošanai. Mākslīgais intelekts piesaista milzīgas laika un pūliņu investīcijas lielā mērā tā solīto labumu dēļ. Tāpēc mums jāvaicā: cik lielā mērā mēs joprojām gūtu AI priekšrocības, ja mēs ierobežotu tā nekontrolēto attīstību līdz superintelektam? Atbilde: mēs varētu zaudēt pārsteidzoši maz.

Vispirms apsveriet, ka pašreizējās AI sistēmas jau ir ļoti jaudīgas, un mēs īstenībā esam tikai saskrāpējuši virsmu tam, ko ar tām var izdarīt.[^8] Tās ir diezgan spējīgas "vadīt šovu" "sapratoana" jautājuma vai uzdevuma, kas tām uzdots, un kas būtu nepieciešams, lai atbildētu uz šo jautājumu vai veiktu šo uzdevumu.

Tālāk, daudz no uztraukuma par mūsdienu AI sistēmām ir to vispārības dēļ; bet dažas no spējīgākajām AI sistēmām — piemēram, tās, kas ģenerē vai atpazīst runu vai attēlus, veic zinātnisko prognozēšanu un modelēšanu, spēlē spēles utt. — ir daudz šaurākas un labi "Vārtu iekšpusē" skaitļošanas ziņā.[^9] Šīs sistēmas ir pārcilvēciskas konkrētajos uzdevumos, ko tās veic. Tām var būt robežgadījumu [^10] (vai [izmantojamu](https://arxiv.org/abs/2211.00241)) vājības to šauruma dēļ; tomēr *pilnīgi* šauras vai *pilnībā* vispārējas nav vienīgās pieejamās opcijas: ir daudzas arhitektūras starp tām.[^11]

Šie AI rīki var ievērojami paātrināt citu pozitīvo tehnoloģiju attīstību, bez MVI. Lai labāk veiktu kodolfiziku, mums nav vajadzīga AI, kas ir kodolfizikis — mums tādi ir! Ja mēs vēlamies paātrināt medicīnu, dodiet biologiem, medicīnas pētniekiem un ķīmiķiem spēcīgus rīkus. Viņi tos vēlas un izmantos ar milzīgu labumu. Mums nav vajadzīga serveru ferma, kas pilna ar miljonu digitālo ģēniju; mums ir miljoni cilvēku, kuru ģenialitāti AI var palīdzēt atklāt. Jā, būs vajadzīgs ilgāks laiks, lai iegūtu nemirstību un izārstēšanu no visām slimībām. Tas ir reāls zaudējums. Bet pat vissolītākās veselības inovācijas būtu maz noderīgas, ja AI darbināta nestabilitāte noved pie globāla konflikta vai sabiedrības sabrukuma. Mēs to parādām sev, ka dotu AI iedvesmotiem cilvēkiem iespēju vispirms mēģināt atrisināt problēmu.

Un pieņemsim, ka faktiski ir kāds milzīgs MVI pluss, ko nevar iegūt cilvēcei, izmantojot Vārtu iekšējus rīkus. Vai mēs to zaudējam, *nekad* neveidojot MVI un superintelektu? Svērojot riskus un atlīdzību šeit, ir milzīgs asimetrisks labums gaidīšanā pret steigšanos: mēs varam gaidīt, līdz to var izdarīt garantēti drošā un labvēlīgā veidā, un gandrīz visi joprojām varēs gūt labumu; ja mēs steidzamies, tas varētu būt — OpenAI izpilddirektora Sama Altmana vārdiem — [gaismu izslēgšana *mums visiem*.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

Bet ja ne-MVI rīki ir potenciāli tik spēcīgi, vai mēs varam tos pārvaldīt? Atbilde ir skaidra... varbūt.

## Instrumentālās AI sistēmas ir (iespējams, principā) pārvaldāmas

Bet tas nebūs viegli. Pašreizējās vismodernākās AI sistēmas var ievērojami dot spēku cilvēkiem un institūcijām viņu mērķu sasniegšanā. Tas ir, vispārējā ziņā, laba lieta! Tomēr ir dabiskas šādu sistēmu rīcībā esamības dinamikas — pēkšņi un bez daudz laika sabiedrībai pielāgoties — kas piedāvā nopietnus riskus, kas jāpārvalda. Ir vērts apspriest dažas lielākās šādu risku klases un kā tās varētu mazināt, pieņemot Vārtu aizvēršanu.

Viena risku klase ir tāda, ka augstas jaudas instrumentālais AI ļauj piekļuvi zināšanām vai spējām, kas iepriekš bija saistītas ar personu vai organizāciju, padarot augstas spējas plus augstas uzticības kombināciju pieejamu ļoti plašam dalībnieku spektram. Šodien, ar pietiekami daudz naudas ļaunprātīgas nolūka persona varētu nolīgt ķīmiķu komandu, lai projektētu un ražotu jaunus ķīmiskos ieročus — bet nav tik ļoti viegli iegūt šo naudu vai atrast/salikt komandu un pārliecināt viņus darīt kaut ko diezgan skaidri nelikumīgu, neētisku un bīstamu. Lai novērstu AI sistēmu šādas lomas spēlēšanu, pašreizējo metožu uzlabojumi varētu labi pietikt,[^12] kamēr vien visas šīs sistēmas un piekļuve tām tiek atbildīgi pārvaldītas. No otras puses, ja jaudīgas sistēmas tiek izlaistas vispārējai lietošanai un modificēšanai, visi iebūvētie drošības pasākumi, iespējams, ir noņemami. Tātad, lai izvairītos no riskiem šajā klasē, būs nepieciešami stingri ierobežojumi attiecībā uz to, kas var tikt publiskts — līdzīgi ierobežojumiem par kodol-, sprāgstvielu un citu bīstamu tehnoloģiju detaļām.[^13]

Otrā risku klase rodas no mašīnu mērogošanas, kas darbojas kā vai iztēlo cilvēkus. Kaitējuma līmenī atsevišķiem cilvēkiem šie riski ietver daudz efektīvākas krāpšanas, spam un makšķerēšanas, un bezpiekrišanas dziļviltojumu izplatīšanos.[^14] Kolektīvā līmenī tie ietver pamata sociālo procesu traucēšanu, piemēram, publisku diskusiju un debašu, mūsu sabiedrības informācijas un zināšanu savākšanas, apstrādes un izplatīšanas sistēmu, un mūsu politisko izvēļu sistēmu. Šo risku mazināšana, iespējams, ietvers (a) likumus, kas ierobežo cilvēku atdarināšanu ar AI sistēmām, un padara atbildīgus AI izstrādātājus, kas izveido sistēmas, kas ģenerē šādus atdarinājumus, (b) ūdenszīmju un izcelsmes sistēmas, kas identificē un klasificē (atbildīgi) ģenerētu AI saturu, un (c) jaunas sociāli-tehniskas epistemiskas sistēmas, kas var izveidot uzticamu ķēdi no datiem (piemēram, kamerām un ierakstiem) caur faktiem, izpratni un labiem pasaules modeļiem.[^15] Tas viss ir iespējams, un AI var palīdzēt ar dažām tā daļām.

Trešais vispārējais risks ir tāds, ka ciktāl daži uzdevumi tiek automatizēti, cilvēki, kas pašlaik dara šos uzdevumus, var būt ar mazāku finansiālu vērtību kā darbaspēks. Vēsturiski uzdevumu automatizēšana ir padarījusi lietas, ko iespējo šie uzdevumi, lētākas un bagātīgākas, vienlaikus sagrupējot cilvēkus, kas iepriekš darīja šos uzdevumus, tajos, kas joprojām ir iesaistīti automatizētajā versijā (parasti augstākā prasmē/algā), un tajos, kuru darbaspēks ir mazāk vērts vai maz vērts. Kopumā ir grūti prognozēt, kuros sektoros būs vajadzīgs vairāk pret mazāk cilvēku darbaspēka lielākā, bet efektīvākā sektorā. Paralēli automatizācijas dinamika mēdz palielināt nevienlīdzību un vispārējo produktivitāti, samazināt noteiktu preču un pakalpojumu izmaksas (efektivitātes pieauguma dēļ) un palielināt citu izmaksas (izmaksu slimības dēļ [cost disease](https://en.wikipedia.org/wiki/Baumol_effect)). Tiem, kas atrodas nevienlīdzības pieauguma nevēlamā pusē, ir dziļi neskaidrs, vai noteiktu preču un pakalpojumu izmaksu samazinājums pārspēj citu palielināšanos un noved pie kopējās lielākas labklājības. Tātad kā tas būs ar AI? Tā kā ir relatīvi viegli aizstāt cilvēku intelektuālo darbu ar vispārējo AI, mēs varam sagaidīt ātru šīs versiju ar cilvēkiem konkurētspējīgu vispārējo AI.[^16] Ja mēs aizvertu Vārtus MVI, daudz mazāk darbavietu tiks vairumā aizstāts ar AI aģentiem; bet milzīga darbaspēka pārvietošana joprojām ir iespējama vairāku gadu periodā.[^17] Lai izvairītos no plaši izplatītas ekonomiskās ciešanas, iespējams, būs nepieciešams ieviest gan kādu universālu pamata aktīvu vai ienākumu formu, gan arī izveidot kultūras maiņu uz cilvēkcentriska darba novērtēšanu un atlīdzināšanu, kas ir grūtāk automatizējama (nevis redzot, kā darba cenas krītas pieaugošā darbaspēka dēļ, kas izstumts no citām ekonomikas daļām.) Citas konstrukcijas, piemēram, ["datu cieņas"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society) (kurā cilvēku apmācības datu ražotājiem automātiski tiek piešķirti honorāri par vērtību, ko rada šie dati AI) var palīdzēt. AI automatizācijai ir arī otrais potenciālais negatīvais efekts, kas ir *nepiedienīga* automatizācija. Kopā ar pielietojumiem, kur AI vienkārši dara sliktāku darbu, tas ietvertu arī tos, kur AI sistēmas, iespējams, pārkāpj morālus, ētiskus vai juridiskus priekšrakstus — piemēram, dzīvības un nāves lēmumos un tiesas lietās. Tie jāapstrādā, piemērojot un paplašinot mūsu pašreizējos juridiskos ietvarus.

Visbeidzot, nozīmīgs Vārtu iekšējā AI drauds ir tā izmantošana personalizētā pārliecināšanā, uzmanības pievēršanā un manipulācijās. Mēs esam redzējuši sociālajos medijos un citas tiešsaistes platformās dziļi iesakņojušās uzmanības ekonomikas (kur tiešsaistes pakalpojumi nikni cīnās par lietotāju uzmanību) un ["uzraudzības kapitālisma"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism) sistēmu augšanu (kurā lietotāju informācija un profilēšana tiek pievienota uzmanības preču veidošanai.) Ir gandrīz noteikti, ka vairāk AI tiks likts abu pakalpojumā. AI jau ir plaši izmantots atkarību radošos barošanas algoritmos, bet tas attīstīsies par atkarību radošu AI ģenerētu saturu, pielāgotu, lai to kompulsīvi patērētu viena persona. Un šīs personas ievads, atbildes un dati tiks baroti uzmanības/reklāmas mašīnā, lai turpinātu ļauno ciklu. Tāpat, kā tehnoloģiju kompāniju nodrošinātie AI palīgi kļūst par saskarni vairāk tiešsaistes dzīvei, viņi, iespējams, aizstās meklētājprogrammas un barotnes kā mehānismus, ar kuriem notiek pārliecināšana un klientu naudas gūšana. Mūsu sabiedrības nespēja līdz šim kontrolēt šīs dinamikas neliecina par labu. Daļa no šīs dinamikas var tikt mazināta ar noteikumiem par privātumu, datu tiesībām un manipulācijām. Vairāk nonākot pie problēmas saknes, var būt nepieciešami atšķirīgi skatījumi, piemēram, uzticīgu AI asistentu (apspriests tālāk.)

Šīs diskusijas galvenā doma ir cerība: Vārtu iekšējās rīku bāzētās sistēmas — vismaz kamēr tās paliek salīdzināmas spēkā un spējā ar šodienas vismodernākajām sistēmām — ir iespējams pārvaldīt, ja ir griba un koordinācija to darīt. Pieklājīgas cilvēku institūcijas, AI rīku iedvesmotas,[^18] var to izdarīt. Mēs varētu arī neizdarīt to. Bet grūti redzēt, kā atļauja spēcīgākām sistēmām palīdzētu — izņemot to, ka nodotu tās vadīšanā un cerētu uz labāko.

## Nacionālā drošība

Sacīkstes par AI pārākumu — ko virza nacionālā drošība vai citi motīvi — virza mūs uz nekontrolētām spēcīgām AI sistēmām, kas mēdz absorbēt, nevis piešķirt spēku. MVI sacīkstes starp ASV un Ķīnu ir sacīkstes par to noteikšanu, kura tauta superintelektu iegūst pirmā.

Tātad ko vajadzētu darīt tiem, kas atbild par nacionālo drošību? Valdībām ir spēcīga pieredze kontrolējamu un drošu sistēmu veidošanā, un tām vajadzētu divkāršot pūliņus to darīšanā AI jomā, atbalstot tāda veida infrastruktūras projektus, kas vislabāk izdodas, kad darīti mērogā un ar valdības apstiprināšanu.

Tā vietā, lai bezatbildīgi "Manhatanas projekts" uz MVI,[^19] ASV valdība varētu sākt Apollo projektu kontrolējamām, drošām, uzticamām sistēmām. Tas varētu ietvert, piemēram:

- Lielu programmu (a) izstrādāt uz čipa aparatūras drošības mehānismus un (b) infrastruktūru, lai pārvaldītu jaudīga AI skaitļošanas pusi. Tie varētu balstīties uz ASV [CHIPS likumu](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) un [eksporta kontroles režīmu](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion).
- Liela mēroga iniciatīvu izstrādāt formālās verifikācijas tehnikas, lai noteiktas AI sistēmu funkcijas (piemēram, izslēgšanas poga) varētu tikt *pierādītas* kā esošas vai neesoša. Tas var izmantot pašu AI pierādījumu īpašību izstrādei.
- Nacionāla mēroga pūliņus izveidot programmatūru, kas ir verificējami droša, ko darbina AI rīki, kas var pārkodēt esošo programmatūru verificējami drošos ietvaros.
- Nacionālu investīciju projektu zinātniskajā progresā, izmantojot AI,[^20] darbojot kā partnerību starp DOE, NSF un NIH.

Vispārējā ziņā ir milzīga uzbrukuma virsma mūsu sabiedrībā, kas padara mūs ievainojamus pret riskiem no AI un tā ļaunprātīgas izmantošanas. Aizsardzība no dažiem no šiem riskiem prasīs valdības izmēra investīcijas un standartizēšanu. Tie nodrošinātu daudz vairāk drošības, nekā lejot benzīnu uz sacīkšu uz MVI uguns. Un, ja AI tiks iebūvēts ieročos un komandu-kontroles sistēmās, ir būtiski, lai AI būtu uzticams un drošs, kas pašreizējais AI vienkārši nav.

## Varas koncentrācija un tās mazināšana

Šis esejs ir koncentrējies uz cilvēka AI kontroles ideju un tās iespējamo neveiksmi. Bet cits derīgs objektīvs, caur kuru skatīties uz AI situāciju, ir caur *varas koncentrāciju.* Ļoti jaudīga AI izstrāde draud koncentrēt varu vai nu ļoti nedaudzos un ļoti lielos korporatīvos rokās, kas to ir izstrādājušas un kontrolēs, vai valdībās, kas izmanto AI kā jaunu līdzekli savu varas un kontroles uzturēšanai, vai pašās AI sistēmās. Vai kādā no šiem gadījumu bezbēdīgā maisījumā augstāk minētajos. Jebkurā no šiem gadījumiem lielākā daļa cilvēces zaudē varu, kontroli un rīcībspēju. Kā mēs varētu cīnīties pret to?

Pašs pirmais un visāistākais solis, protams, ir Vārtu aizvēršana gudākiem par cilvēkiem MVI un superintelektam. Tie skaidri var tieši aizstāt cilvēkus un cilvēku grupas. Ja tie ir korporatīvā vai valdības kontrolē, tie koncentrēs varu šajās korporācijās vai valdībās; ja tie ir "brīvi", tie koncentrēs varu sevī. Tātad pieņemsim, ka Vārti ir aizvērti. Tad ko?

Viens ierosinātais varas koncentrācijas risinājums ir "atvērtā koda" AI, kur modeļa svari ir brīvi vai plaši pieejami. Bet, kā minēts iepriekš, kad modelis ir atvērts, lielākā daļa drošības pasākumu vai drošības barjeru var tikt (un parasti tiek) noņemtas. Tātad ir asa spriedze starp no vienas puses decentralizāciju un no otras puses drošību, aizsardzību un cilvēka AI sistēmu kontroli. Ir arī iemesli būt skeptiskiem, ka atvērtie modeļi paši par sevi nozīmīgi cīnīsies pret varas koncentrāciju AI jomā vairāk nekā tie ir darījuši operētājsistēmās (joprojām dominē Microsoft, Apple un Google, neraugoties uz atvērtajām alternatīvām).[^21]

Tomēr var būt veidi, kā kvadrēt šo apli — centralizēt un mazināt riskus, vienlaikus decentralizējot spējas un ekonomisko atlīdzību. Tas prasa pārdomāt gan to, kā AI tiek izstrādāts, gan to, kā tā labumi tiek sadalīti.

Jauni publiska AI izstrādes un īpašumtiesību modeļi palīdzētu. Tas varētu pieņemt vairākas formas: valdības izstrādāts AI (pakļauts demokrātiskai uzraudzībai),[^22] bezpeļņas AI izstrādes organizācijas (piemēram, Mozilla pārlūkprogrammām), vai struktūras, kas ļauj ļoti plašu īpašumtiesību un pārvaldību. Galvenais ir tāds, ka šīs institūcijas būtu skaidri uzticētas kalpot sabiedrības interesēm, vienlaikus darbojoties stingru drošības ierobežojumu ietvaros.[^23] Labi veidoti regulatīvie un standartu/sertifikācijas režīmi arī būs būtiski, lai AI produkti, ko piedāvā rosīgā tirgus, paliktu patiesi noderīgi, nevis izmantojami pret to lietotājiem.

Ekonomiskās varas koncentrācijas ziņā mēs varam izmantot izcelsmes izsekošanu un "datu cieņu", lai nodrošinātu, ka ekonomiskie labumi plūst plašāk. Jo īpaši lielākā daļa AI spēka tagad (un nākotnē, ja mēs uzturēsim Vārtus aizvertus) rod cēloni no cilvēku ģenerētiem datiem, vai tiešiem apmācības datiem, vai cilvēka atgriezeniskās saites. Ja AI kompānijām tiktu prasīts godīgi kompensēt datu sniedzējus,[^24] tas varētu vismaz palīdzēt sadalīt ekonomisko atlīdzību plašāk. Ārpus tā, cits modelis varētu būt sabiedriska īpašumtiesība uz būtiskām lielo AI kompāniju daļām. Piemēram, valdības, kas spēj apliksnes AI kompānijas ar nodokļiem, varētu investēt ieņēmumu daļu suverēnā bagātības fondā, kas tur akcijas kompānijās un maksā dividendes iedzīvotājiem.[^25]

Šajos mehānismos ir būtiski izmantot paša AI spēku, lai palīdzētu sadalīt varu labāk, nevis vienkārši cīnīties pret AI darbināto varas koncentrāciju, izmantojot ne-AI līdzekļus. Viens spēcīgs pieejams būtu caur labi projektētiem AI asistentiem, kas darbojas ar īstu fiduciāro pienākumu pret saviem lietotājiem — liekot lietotāju intereses pirmajā vietā, īpaši virs korporatīvo sniedzēju interesēm.[^26] Šiem asistentiem jābūt patiesi uzticamiem, tehniski kompetentiem, tomēr atbilstoši ierobežotiem, balstoties uz lietošanas gadījumu un riska līmeni, un plaši pieejamiem visiem caur sabiedriskajiem, bezpeļņas vai sertificētajiem peļņas kanāliem. Tāpat kā mēs nekad neakceptētu cilvēku asistentu, kas slepeni strādā pret mūsu interesēm citai pusei, mums nevajadzētu akceptēt AI asistentus, kas uzrauga, manipulē vai izvilka vērtību no saviem lietotājiem korporatīva labuma dēļ.

Tāda pārveide fundamentāli mainītu pašreizējo dinamiku, kur indivīdi paliek vieni sarunāties ar milzīgajiem (AI darbinātajiem) korporatīvajiem un birokrātiskajiem mehānismiem, kas prioritizē vērtības izvilkšanu pār cilvēka labklājību. Lai gan ir daudzi iespējamie pieejas AI darbinātas varas plašākai pārsadalei, neviens neradīsies pēc noklusējuma: tiem jābūt apzināti izveidotiem un pārvaldītiem ar mehānismiem kā fiduciārās prasības, sabiedriskās nodrošināšana un pakāpeniski piekļuves, balstoties uz risku.

Pieejas varas koncentrācijas mazināšanai var saskarties ar būtiskiem pretvējiem no esošajiem spēkiem.[^27] Bet ir ceļi uz AI izstrādi, kas neprasa izvēlēties starp drošību un koncentrētu varu. Veidojot pareizas institūcijas tagad, mēs varētu nodrošināt, ka AI labumi tiek plaši dalīti, kamēr tā riski tiek rūpīgi pārvaldīti.

## Jaunas pārvaldības un sociālās struktūras

Mūsu pašreizējās pārvaldības struktūras cīnās: tās ir lēnas reaģēt, bieži ietekmētas īpašo interešu un [arvien vairāk sabiedrības neuzticētas.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) Tomēr tas nav iemesls tās pamest — gluži pretēji. Dažas institūcijas var būt vajadzīgs aizstāt, bet plašāk mums vajag jaunus mehānismus, kas var uzlabot un papildināt mūsu esošās struktūras, palīdzot tām funkcionēt labāk mūsu ātri attīstošajā pasaulē.

Daudz mūsu institucionālās vājības rodas ne no formālām valdības struktūrām, bet no degradētām sociālām institūcijām: mūsu sistēmām koplietotās izpratnes veidošanai, darbību koordinēšanai un nozīmīgu diskursu vadīšanai. Līdz šim AI ir paātrināja šo degradāciju, applūdinot mūsu informācijas kanālus ar ģenerētu saturu, virzienu mūs uz vispolari izzējošāko un dalošāko saturu, un padarot grūtāku atšķirt patiesību no fikcijas.

Bet AI faktiski varētu palīdzēt atjaunot un stiprināt šīs sociālās institūcijas. Apsverot trīs būtiskas jomas:

Pirmkārt, AI varētu palīdzēt atjaunot uzticību mūsu epistemiskajām sistēmām — mūsu veidiem, kā zināt, kas ir patiess. Mēs varētu izstrādāt AI darbinātas sistēmas, kas izseko un verificē informācijas izcelsmi, no neapstrādātiem datiem caur analīzi līdz secinājumiem. Šīs sistēmas varētu kombinēt kriptogrāfisko verifikāciju ar sarežģītu analīzi, lai palīdzētu cilvēkiem saprast ne tikai to, vai kaut kas ir patiess, bet arī to, kā mēs zinām, ka tas ir patiess.[^28] Uzticīgi AI asistenti varētu tikt uzticēti sekot detaļām, lai nodrošinātu, ka tās noder.

Otrkārt, AI varētu iespējot jaunas liela mēroga koordinācijas formas. Daudzi no mūsu akūtākajiem problēmiem — no klimata maiņas līdz antibiotisku rezistencei — ir fundamentāli koordinācijas problēmas. Mēs [esam iestrēguši situācijās, kas ir sliktākas, nekā tās varētu būt gandrīz visiem](https://equilibriabook.com/), jo neviens indivīds vai grupa nevar atļauties izdarīt pirmo gājienu. AI sistēmas varētu palīdzēt, modelējot sarežģītas stimulu struktūras, identificējot dzīvotspējīgus ceļus uz labākiem rezultātiem un atvieglojot uzticības veidošanas un saistību mehānismus, kas nepieciešami, lai tur nokļūtu.

Varbūt visintriģējošāk, AI varētu iespējot pilnīgi jaunas sociālā diskursa formas. Iedomājieties spēju "runāt ar pilsētu" [^29] — ne tikai skatīt statistiku, bet vest nozīmīgu dialogu ar AI sistēmu, kas apstrādā un apkopo miljonu iemītnieku uzskatus, pieredzes, vajadzības un aspirācijas. Vai apsveriet, kā AI varētu atvieglot īstu dialogu starp grupām, kas pašlaik runā garām viena otrai, palīdzot katrai pusei labāk saprast otras faktiskās bažas un vērtības, nevis to kariktūras viena otru.[^30] Vai AI varētu piedāvāt prasmīgu, ticami neitrālu starpniecību strīdos starp cilvēkiem vai pat lielām cilvēku grupām (kas visi varētu mijiedarboties ar to tieši un individuāli!) Pašreizējais AI ir pilnīgi spējīgs veikt šo darbu, bet rīki, lai to darītu, neradīsies paši no sevis, vai caur tirgus stimuliem.

Šīs iespējas varētu skanēt utopiski, īpaši ņemot vērā AI pašreizējo lomu diskursa un uzticības degradēšanā. Bet tas ir tieši tāpēc, kāpēc mums jāattīsta aktīvi šīs pozitīvās lietojuma. Aizverot Vārtus nekontrolējamam MVI un prioritizējot AI, kas uzlabo cilvēka rīcībspēju, mēs varam virzīt tehnoloģisko progresu uz nākotni, kur AI kalpo kā spēks iedvesmošanai, izturībai un kolektīvai virzībai.


[^1]: Tāpat, palikt projām no trīskāršās krustošanās, diemžēl nav tik vienkārši, kā varētu vēlēties. Spēju ļoti cietu grūšana jebkurā no trim aspektiem mēdz to palielināt citos. Jo īpaši var būt grūti izveidot ļoti vispārēju un spējīgu intelektu, ko nevar viegli padarīt autonomu. Viena pieeja ir apmācīt modeļus ["miopiskus"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) sistēmas ar sabojātu plānošanas spēju. Cita būtu koncentrēties uz inženierijas tīrām ["orākula"](https://arxiv.org/abs/1711.05541) sistēmām, kas izvairītos no atbildēšanas uz darbības orientētiem jautājumiem.

[^2]: Daudzas kompānijas nespēj saprast, ka arī viņas galu galā tiktu aizstātas ar MVI, pat ja tas aizņemtu ilgāku laiku — ja viņas to darītu, viņas varētu mazliet mazāk grūst uz tiem Vārtiem!

[^3]: AI sistēmas varētu sazināties efektīvākos, bet mazāk saprotamos veidos, bet cilvēka izpratnes uzturēšanai vajadzētu būt prioritātei.

[^4]: Šo modulārā, interpretējamā AI ideju detalizēti ir izstrādājuši vairāki pētnieki; skatiet, piemēram, ["Visaptverošo AI pakalpojumu"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) modeli no Drexler, Dalrimple un citu ["Atvērto aģentūru arhitektūru"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai). Lai gan šādas sistēmas varētu prasīt vairāk inženierpūliņu nekā monoliski neironu tīkli, kas apmācīti ar masīvu skaitļošanu, tas ir tieši tas, kur skaitļošanas ierobežojumi palīdz — padarot drošāko, caurspīdīgāko ceļu arī praktiskāko.

[^5]: Par drošības lietām vispārējā ziņā skatiet [šo rokasgrāmatu](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). Attiecībā uz AI konkrēti skatiet [Wasil et al.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [Clymer et al.](https://arxiv.org/abs/2403.10462), [Buhl et al.](https://arxiv.org/abs/2410.21572), un [Balesni et al.](https://arxiv.org/abs/2411.03336)

[^6]: Mēs faktiski jau redzam šo tendenci, ko virza tikai augstās secinājumu izdarīšanas izmaksas: mazāki un specializētāki modeļi, kas "destilēti" no lielākiem un spējīgi darboties uz mazāk dārgām aparatūrām.

[^7]: Es saprotu, kāpēc tie, kas aizraujas ar AI tehnoloģiju ekosistēmu, var iebilst pret to, ko viņi uzskata par apgrūtinošu regulējumu savai industrijā. Bet man ir tieši mulsinoši, kāpēc, piemēram, riska kapitāla investors gribētu atļaut nekontrolētu attīstību līdz MVI un superintelektam. Šīs sistēmas (un kompānijas, kamēr tās paliek kompānijas kontrolē) *apēdīs visus jaunuzņēmumus kā uzkodu*. Iespējams, pat *agrāk* nekā ēst citas nozares. Ikvienam, kas investēts plaukstošā AI ekosistēmā, vajadzētu prioritāri nodrošināt, ka MVI attīstība nenoved pie monopolizācijas ar dažiem dominējošiem spēlētājiem.

[^8]: Kā ekonomists un bijušais Deepmind pētnieks Maikls Vebbs [teica](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/), "Es domāju, ja mēs šodien apturētu visu lielāku valodu modeļu attīstību, tātad GPT-4 un Claude un jebko, un tie ir pēdējās lietas, ko mēs apmācām šajā lielumā — tātad mēs atļaujam daudz vairāk iterāciju uz šā lieluma lietām un visu veidu precizēšanu, bet neko lielāku par to, nav lielāku sasniegumu — tikai tas, kas mums ir šodien, es domāju, ir pietiekami, lai darbinātu 20 vai 30 gadus neticama ekonomiska augšana."

[^9]: Piemēram, DeepMind alphafold sistēma izmantoja tikai 100,000to daļu no GPT-4 FLOP skaita.

[^10]: Pašbraucošo automašīnu grūtības šeit ir svarīgi atzīmēt: lai gan nomināli šaurs uzdevums un sasniedzams ar pamatīgu uzticamību ar salīdzinoši mazām AI sistēmām, plašas reālās pasaules zināšanas un izpratne ir nepieciešama, lai iegūtu uzticamību līmenī, kas nepieciešams tik drošības kritiskā uzdevumā.

[^11]: Piemēram, dotam skaitļošanas budžetam, mēs, iespējams, redzētu GPAI modeļus, kas iepriekš apmācīti (piemēram) puspusē no tā budžeta, un otra puse izmantota, lai apmācītu augstu spēju šaurākā uzdevumu spektrā. Tas dotu pārcilvēciskas šauras spējas, ko atbalsta tuvu cilvēcīgs vispārējais intelekts.

[^12]: Pašreizējā dominējošā saskaņošanas tehnika ir "stiprināšanas mācīšanās ar cilvēka atgriezenisko saiti" [(RLHF)](https://arxiv.org/abs/1706.03741) un izmanto cilvēka atgriezenisko saiti, lai izveidotu atlīdzības/soda signālu AI modeļa stiprināšanas mācīšanās. Šī un līdzīgas tehnikas kā [konstitutcionālais AI](https://arxiv.org/abs/2212.08073) darbojas pārsteidzoši labi (lai gan tām trūkst noturības un tās var apiet ar mērenu pūliņu.) Turklāt pašreizējie valodu modeļi parasti ir pietiekami kompetenti vesela saprāta spriedumu veidošanā, ka viņi neizdarīs muļķīgas morālas kļūdas. Tas ir kaut kas kā saldā vieta: pietiekami gudri, lai saprastu, ko cilvēki vēlas (ciktāl to var definēt), bet ne pietiekami gudri, lai plānotu sarežģītus krāpšanas vai radītu milzīgu kaitējumu, kad viņi to saprot nepareizi.

[^13]: Ilgtermiņā jebkurš AI spēju līmenis, kas tiek izstrādāts, iespējams, izplatīsies, jo galu galā tas ir programmatūra, un noderīga. Mums būs jābūt robustiem mehānismiem, lai aizstāvētos pret riskiem, ko šādas sistēmas rada. Bet *mums tas nav tagad*, tāpēc mums jābūt ļoti mērītiem tajā, cik daudz jaudīgu AI modeļu drīkst izplatīties.

[^14]: Lielākā daļa no tiem ir bezpiekrišanas pornogrāfiski dziļviltojumi, ieskaitot nepilngadīgo.

[^15]: Daudzi no šādu risinājumu ingredientiem pastāv "bots-vai-ne" likumu formā (ES AI likumā starp citiem vietām), [nozares izcelsmes izsekošanas tehnoloģijās](https://c2pa.org/), [inovatīvos ziņu agregātoros](https://www.improvethenews.org/), prognožu [agregātoros](https://metaculus.com/) un tirgos utt.

[^16]: Automatizācijas vilnis var nesekot iepriekšējiem modeļiem tādā ziņā, ka salīdzinoši *augstas* prasmes uzdevumi, piemēram, kvalitātes rakstīšana, likuma interpretēšana vai medicīnisko padomu došana, var būt tikpat daudz vai pat vairāk ievainojami pret automatizāciju nekā zemākas prasmes uzdevumi.

[^17]: Par rūpīgu MVI ietekmes uz algām modelēšanu skatiet ziņojumu [šeit](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek), un asiņainas detaļas [šeit](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0), no Anton Korinek un līdzstrādniekiem. Viņi atklāj, ka, jo vairāk darbavietu daļas tiek automatizētas, produktivitāte un algas palielinās — līdz punktam. Kad *pārāk* daudz tiek automatizēts, produktivitāte turpina palielināties, bet algas krīt, jo cilvēki tiek aizstāti vairumā ar efektīvu AI. Tāpēc Vārtu aizvēršana ir tik noderīga: mēs iegūstam produktivitāti bez pazudušo cilvēku algām.

[^18]: Ir daudzi veidi, kā AI var tikt izmantots kā un palīdzēt veidot "aizsargājošas" tehnoloģijas, lai padarītu aizsardzību un pārvaldību noturīgāku. Skatīt [šo](https://vitalik.eth.limo/general/2025/01/05/dacc2.html) ietekmīgo ierakstu, kas apraksta šo "D/acc" programmu.

[^19]: Diezgan ironiski, ASV Manhetanas projekts, iespējams, darītu maz, lai paātrinātu laikus uz MVI — cilvēku un fiskālo investīciju uzstādījums AI progresā jau ir nospiests uz 11. Primārie rezultāti būtu iedvesmot līdzīgu projektu Ķīnā (kas izcili pārvalda nacionāla līmeņa infrastruktūras projektus), padarīt starptautiskās vienošanās, kas ierobežo AI risku, daudz grūtākas, un satraukt citus ASV ģeopolitiskos pretiniekus, piemēram, Krieviju.

[^20]: ["Nacionālā AI pētniecības resursa"](https://nairrpilot.org/) programma ir labs pašreizējais solis šajā virzienā un vajadzētu tikt paplašināta.

[^21]: Skatīt [šo analīzi](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807) par dažādajām "atvērtā" nozīmēm un sekām tehnoloģiju produktos un kā dažas ir novedušas uz vairāk, nevis mazāk, dominances iestipringājumu.

[^22]: ASV plāni [Nacionālajam AI pētniecības resursam](https://nairratdoe.ornl.gov/) un nesenie [Eiropas AI fonda](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/) laišana ir interesanti soļi šajā virzienā.

[^23]: Izaicinājums šeit nav tehnisks, bet institucionāls — mums steidzami vajag reālās pasaules piemērus un eksperimentus tajā, kā varētu izskatīties sabiedrības interešu AI attīstība.

[^24]: Tas ir pretrunā ar pašreizējiem lielo tehnoloģiju uzņēmumu biznesa modeļiem un prasītu gan juridiskas darbības, gan jaunas normas.

[^25]: Tikai dažas valdības varēs to darīt. Radikālāka ideja ir [universāls šāda veida fonds, kas ir visu cilvēku kopīgajā īpašumā.](https://futureoflife.org/project/the-windfall-trust/)

[^26]: Par šīs lietas ilgu izklāstu skatīt [šo rakstu](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338) par AI uzticību. Diemžēl AI asistentu noklusējuma trajektorija, iespējams, būs tā, kur viņi arvien vairāk ir neuzticīgi.

[^27]: Diezgan ironiski, daudzi esošie spēki arī ir AI atbalstītas vara atņemšanas riskā; bet viņiem var būt grūti to uztvert, līdz un ja vien process nekļūst diezgan tālu.

[^28]: Daži interesanti pūliņi šajā virzienā ir pārstāvēti ar [c2pa koalīciju](https://c2pa.org/) par kriptogrāfisko verifikāciju; [Verity](https://www.improvethenews.org/) un [Ground news](https://ground.news/) par labāku ziņu epistemiku; un [Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com) un prognožu tirgos par diskursa balstīšanu uz pārbaudāmām prognozēm.

[^29]: Skatīt [šo](https://talktothecity.org/) aizraujošo pilotprojektu.

[^30]: Skatīt [Kialo](https://www.kialo-edu.com/), un [Kolektīvā intelekta projekta](https://www.cip.org/) pūliņus par dažiem piemēriem.