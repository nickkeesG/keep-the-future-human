# Bab 2 - Perkara penting tentang rangkaian neural AI

Bagaimanakah sistem AI moden berfungsi, dan apakah yang mungkin akan datang dalam generasi AI yang seterusnya?

Untuk memahami bagaimana akibat daripada membangunkan AI yang lebih berkuasa akan berlaku, adalah penting untuk menghayati beberapa asas. Bahagian ini dan dua bahagian seterusnya akan membangunkan pemahaman tersebut, merangkumi secara bergilir-gilir tentang apa itu AI moden, bagaimana ia memanfaatkan pengkomputeran besar-besaran, dan dalam erti kata apa ia berkembang pesat dari segi keumuman dan keupayaan.[^1]

Terdapat banyak cara untuk mentakrifkan kecerdasan buatan, tetapi untuk tujuan kita, sifat utama AI ialah walaupun program komputer standard adalah senarai arahan tentang cara melaksanakan tugas, sistem AI pula adalah sistem yang belajar daripada data atau pengalaman untuk melaksanakan tugas *tanpa diberitahu secara eksplisit bagaimana untuk melakukannya.*

Hampir semua AI moden yang ketara adalah berdasarkan rangkaian neural. Ini adalah struktur matematik/pengkomputeran, yang diwakili oleh set nombor yang sangat besar (berbilion atau bertrilion) ("pemberat"), yang melaksanakan tugas latihan dengan baik. Pemberat ini dibentuk (atau mungkin "ditumbuhkan" atau "ditemui") dengan mengubahsuainya secara berulang supaya rangkaian neural meningkatkan skor berangka (dikenali sebagai "kerugian") yang ditakrifkan untuk berprestasi baik dalam satu atau lebih tugas.[^2] Proses ini dikenali sebagai *latihan* rangkaian neural.[^3]

Terdapat banyak teknik untuk melakukan latihan ini, tetapi butiran tersebut kurang relevan berbanding cara penskoran ditakrifkan, dan bagaimana ia menghasilkan tugas yang berbeza yang rangkaian neural lakukan dengan baik. Perbezaan utama secara sejarah telah dibuat antara AI "sempit" dan "am".

AI sempit sengaja dilatih untuk melakukan tugas tertentu atau set kecil tugas (seperti mengenali imej atau bermain catur); ia memerlukan latihan semula untuk tugas baru, dan mempunyai skop keupayaan yang sempit. Kita mempunyai AI sempit yang mengatasi manusia, bermakna untuk hampir mana-mana tugas diskret yang ditakrifkan dengan baik yang boleh dilakukan oleh seseorang, kita mungkin boleh membina skor dan kemudian berjaya melatih sistem AI sempit untuk melakukannya lebih baik daripada manusia.

Sistem AI tujuan am (GPAI) boleh melaksanakan pelbagai tugas, termasuk banyak yang tidak dilatih secara eksplisit untuk dilakukan; ia juga boleh mempelajari tugas baru sebagai sebahagian daripada operasinya. "Model multimodal" besar semasa[^4] seperti ChatGPT mencontohkan ini: dilatih pada korpus teks dan imej yang sangat besar, ia boleh terlibat dalam penaakulan yang kompleks, menulis kod, menganalisis imej, dan membantu dengan pelbagai tugas intelektual. Walaupun masih agak berbeza daripada kecerdasan manusia dalam cara yang akan kita lihat secara mendalam di bawah, keumumannya telah menyebabkan revolusi dalam AI.[^5]

## Ketidakramalan: ciri utama sistem AI

Perbezaan utama antara sistem AI dan perisian konvensional ialah dalam ketidakramalan. Output perisian standard boleh tidak dapat diramal – malah kadangkala itulah sebabnya kita menulis perisian, untuk memberikan kita hasil yang tidak dapat kita ramalkan. Tetapi perisian konvensional jarang melakukan apa-apa yang tidak diprogramkan untuk dilakukan – skop dan tingkah lakunya secara amnya seperti yang direka. Program catur peringkat teratas mungkin membuat gerakan yang tiada manusia boleh ramalkan (atau sebaliknya mereka boleh mengalahkan program catur tersebut!) tetapi ia secara amnya tidak akan melakukan apa-apa selain bermain catur.

Seperti perisian konvensional, AI sempit mempunyai skop dan tingkah laku yang boleh diramal tetapi boleh mempunyai hasil yang tidak dapat diramal. Ini sebenarnya hanya cara lain untuk mentakrifkan AI sempit: sebagai AI yang serupa dengan perisian konvensional dalam ketidakramalan dan julat operasinya.

AI tujuan am adalah berbeza: skopnya (domain yang ia gunakan), tingkah laku (jenis perkara yang dilakukannya), dan hasil (output sebenarnya) semuanya boleh tidak dapat diramal.[^6] GPT-4 dilatih hanya untuk menjana teks dengan tepat, tetapi membangunkan banyak keupayaan yang pelatihnya tidak ramalkan atau tidak sengajakan. Ketidakramalan ini berpunca daripada kerumitan latihan: kerana data latihan mengandungi output daripada banyak tugas berbeza, AI mestilah belajar untuk melaksanakan tugas ini dengan berkesan untuk meramal dengan baik.

Ketidakramalan sistem AI am ini agak asas. Walaupun pada dasarnya adalah mungkin untuk membina sistem AI dengan teliti yang mempunyai had yang dijamin pada tingkah laku mereka (seperti yang disebut kemudian dalam esei), cara sistem AI dicipta sekarang ia tidak dapat diramal dalam amalan dan bahkan pada dasarnya.

## AI pasif, agen, sistem autonomi, dan penjajaran

Ketidakramalan ini menjadi amat penting apabila kita mempertimbangkan bagaimana sistem AI sebenarnya digunakan dan digunakan untuk mencapai pelbagai matlamat.

Banyak sistem AI adalah agak pasif dalam erti kata ia terutamanya menyediakan maklumat, dan pengguna mengambil tindakan. Yang lain, biasanya dipanggil *agen*, mengambil tindakan sendiri, dengan pelbagai tahap penglibatan daripada pengguna. Yang mengambil tindakan dengan input atau pengawasan luar yang agak kurang boleh dipanggil lebih *autonomi*. Ini membentuk spektrum dari segi kebebasan tindakan, daripada alat pasif kepada agen autonomi.[^7]

Mengenai matlamat sistem AI, ini mungkin terikat secara langsung dengan objektif latihan mereka (contohnya matlamat "menang" untuk sistem yang bermain Go juga secara eksplisit adalah apa yang ia dilatih untuk lakukan). Atau ia mungkin tidak: objektif latihan ChatGPT sebahagiannya adalah untuk meramal teks, sebahagiannya untuk menjadi pembantu yang berguna. Tetapi apabila melakukan tugas tertentu, matlamatnya dibekalkan kepadanya oleh pengguna. Matlamat juga mungkin dicipta oleh sistem AI itu sendiri, hanya secara tidak langsung berkaitan dengan objektif latihannya.[^8]

Matlamat berkait rapat dengan persoalan "penjajaran," iaitu persoalan sama ada sistem AI akan *melakukan apa yang kita mahu mereka lakukan*. Persoalan mudah ini menyembunyikan tahap kehalusan yang besar.[^9] Buat masa ini, ambil perhatian bahawa "kita" dalam ayat ini mungkin merujuk kepada banyak orang dan kumpulan yang berbeza, membawa kepada jenis penjajaran yang berbeza. Contohnya, AI mungkin sangat *patuh* (atau ["setia"](https://arxiv.org/abs/2003.11157)) kepada penggunanya – di sini "kita" adalah "setiap daripada kita." Atau ia mungkin lebih *berdaulat*, didorong terutamanya oleh matlamat dan kekangan sendiri, tetapi masih bertindak secara meluas untuk kepentingan bersama kesejahteraan manusia – "kita" kemudiannya adalah "manusia" atau "masyarakat." Di tengah-tengah adalah spektrum di mana AI akan sebahagian besarnya patuh, tetapi mungkin enggan mengambil tindakan yang membahayakan orang lain atau masyarakat, melanggar undang-undang, dan sebagainya.

Kedua-dua paksi ini – tahap autonomi dan jenis penjajaran – tidak sepenuhnya bebas. Contohnya, sistem pasif berdaulat, walaupun tidak agak bercanggah sendiri, adalah konsep dalam ketegangan, begitu juga dengan agen autonomi yang patuh.[^10] Terdapat pengertian yang jelas bahawa autonomi dan kedaulatan cenderung untuk berjalan seiring. Dalam nada yang sama, ketidakramalan cenderung lebih tinggi dalam sistem AI "pasif" dan "patuh", manakala yang berdaulat atau autonomi akan cenderung lebih tidak dapat diramal. Semua ini akan menjadi penting untuk memahami kesan KBA dan superintelligence yang berpotensi.

Mencipta AI yang benar-benar sejajar, dalam apa jua bentuk, memerlukan penyelesaian tiga cabaran berbeza:

1. Memahami apa yang "kita" mahu – yang rumit sama ada "kita" bermaksud orang atau organisasi tertentu (kesetiaan) atau manusia secara meluas (kedaulatan);
2. Membina sistem yang kerap bertindak selaras dengan kehendak tersebut – pada asasnya mencipta tingkah laku positif yang konsisten;
3. Paling asasnya, membuat sistem yang benar-benar "mengambil berat" tentang kehendak tersebut daripada hanya bertindak seolah-olah mereka berbuat demikian.

Perbezaan antara tingkah laku yang boleh dipercayai dan kepedulian tulen adalah penting. Sama seperti pekerja manusia mungkin mengikut arahan dengan sempurna sementara kekurangan komitmen sebenar terhadap misi organisasi, sistem AI mungkin bertindak sejajar tanpa benar-benar menghargai keutamaan manusia. Kita boleh melatih sistem AI untuk berkata dan melakukan perkara melalui maklum balas, dan mereka boleh belajar untuk berfikir tentang apa yang manusia mahu. Tetapi membuat mereka *benar-benar* menghargai keutamaan manusia adalah cabaran yang jauh lebih mendalam.[^11]

Kesukaran yang mendalam dalam menyelesaikan cabaran penjajaran ini, dan implikasinya untuk risiko AI, akan diterokai lebih lanjut di bawah. Buat masa ini, fahami bahawa penjajaran bukan hanya ciri teknikal yang kita tampalkan kepada sistem AI, tetapi aspek asas seni bina mereka yang membentuk hubungan mereka dengan manusia.


[^1]: Untuk pengenalan yang lembut tetapi teknikal kepada pembelajaran mesin dan AI, terutamanya model bahasa, lihat [laman web ini.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) Untuk primer moden lain tentang risiko kepupusan AI, lihat [tulisan ini.](https://www.thecompendium.ai/) Untuk analisis saintifik yang komprehensif dan berwibawa tentang keadaan keselamatan AI, lihat [Laporan Keselamatan AI Antarabangsa](https://arxiv.org/abs/2501.17805) baru-baru ini.

[^2]: Latihan biasanya berlaku dengan mencari maksimum tempatan skor dalam ruang berdimensi tinggi yang diberikan oleh pemberat model. Dengan memeriksa bagaimana skor berubah apabila pemberat diubahsuai, algoritma latihan mengenal pasti pengubahsuaian mana yang paling meningkatkan skor, dan menggerakkan pemberat ke arah tersebut.

[^3]: Contohnya, dalam masalah pengecaman imej, rangkaian neural akan mengeluarkan kebarangkalian untuk label untuk imej. Skor akan berkaitan dengan kebarangkalian yang AI berikan kepada jawapan yang betul. Prosedur latihan kemudian akan menyesuaikan pemberat supaya lain kali, AI akan mengeluarkan kebarangkalian yang lebih tinggi untuk label yang betul untuk imej tersebut. Ini kemudian diulang sebilangan besar kali. Prosedur asas yang sama digunakan dalam melatih pada dasarnya semua rangkaian neural moden, walaupun dengan mekanisme penskoran yang lebih kompleks.

[^4]: Kebanyakan model multimodal menggunakan seni bina "transformer" untuk memproses dan menjana pelbagai jenis data (teks, imej, bunyi). Ini semua boleh dihuraikan kepada, dan kemudian diperlakukan atas dasar yang sama, sebagai jenis "token" yang berbeza. Model multimodal dilatih pertama untuk meramal token dengan tepat dalam set data besar, kemudian diperhalusi melalui pembelajaran pengukuhan untuk meningkatkan keupayaan dan membentuk tingkah laku.

[^5]: Bahawa model bahasa dilatih untuk melakukan satu perkara – meramal perkataan – telah menyebabkan sesetengah orang memanggil mereka AI sempit. Tetapi ini mengelirukan: kerana meramal teks dengan baik memerlukan begitu banyak keupayaan berbeza, tugas latihan ini membawa kepada sistem yang mengejutkan umum. Juga ambil perhatian bahawa sistem ini dilatih secara meluas oleh pembelajaran pengukuhan, dengan berkesan mewakili ribuan orang yang memberikan model isyarat ganjaran apabila ia melakukan kerja yang baik pada mana-mana daripada banyak perkara yang dilakukannya. Ia kemudian mewarisi keumuman yang ketara daripada orang yang memberikan maklum balas ini.

[^6]: Terdapat pelbagai cara AI tidak dapat diramal. Salah satunya ialah dalam kes am seseorang tidak boleh meramal apa yang algoritma akan lakukan tanpa benar-benar menjalankannya; terdapat [teorem](https://arxiv.org/abs/1310.3225) untuk kesan ini. Ini boleh benar hanya kerana output algoritma boleh menjadi kompleks. Tetapi ia amat jelas dan relevan dalam kes (seperti dalam catur atau Go) di mana ramalan akan membayangkan keupayaan (mengalahkan AI) yang tidak dimiliki oleh peramal yang akan menjadi. Kedua, sistem AI tertentu tidak akan sentiasa menghasilkan output yang sama walaupun diberikan input yang sama – outputnya mengandungi rawak; ini juga diganding dengan ketidakramalan algoritma. Ketiga, keupayaan yang tidak dijangka dan muncul boleh timbul daripada latihan, bermakna walaupun *jenis* perkara yang sistem AI boleh dan akan lakukan tidak dapat diramal; Jenis terakhir ini amat penting untuk pertimbangan keselamatan.

[^7]: Lihat [di sini](https://arxiv.org/abs/2502.02649) untuk kajian mendalam tentang apa yang dimaksudkan dengan "agen autonomi" (bersama-sama dengan hujah etika terhadap membina mereka).

[^8]: Anda mungkin kadangkala mendengar "AI tidak boleh mempunyai matlamat sendiri." Ini adalah karut mutlak. Adalah mudah untuk menjana contoh di mana AI mempunyai atau membangunkan matlamat yang tidak pernah diberikan kepadanya dan hanya diketahui oleh dirinya sendiri. Anda tidak melihat ini banyak dalam model multimodal popular semasa kerana ia dilatih daripada mereka; ia boleh sama mudahnya dilatih ke dalam mereka.

[^9]: Terdapat kesusasteraan yang besar. Mengenai masalah am lihat [*The Alignment Problem*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821) Christian, dan [*Human-Compatible*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) Russell. Pada sisi yang lebih teknikal lihat contohnya [kertas ini](https://arxiv.org/abs/2209.00626).

[^10]: Kita akan kemudian melihat bahawa walaupun sistem sedemikian melawan trend, itu sebenarnya menjadikan mereka sangat menarik dan berguna.

[^11]: Ini bukan bermakna kita memerlukan emosi atau kesedaran. Sebaliknya, adalah amat sukar dari luar sistem untuk mengetahui apa matlamat dalaman, keutamaan, dan nilainya. "Tulen" di sini bermakna kita mempunyai alasan yang cukup kuat untuk bergantung kepadanya bahawa dalam kes sistem kritikal kita boleh mempertaruhkan nyawa kita kepadanya.