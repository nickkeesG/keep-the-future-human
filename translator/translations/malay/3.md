# Bab 3 - Aspek utama bagaimana sistem AI am moden dibina

Kebanyakan sistem AI termaju di dunia dibina menggunakan kaedah yang mengejutkan serupa. Berikut adalah asas-asasnya.

Untuk benar-benar memahami manusia, anda perlu tahu sesuatu tentang biologi, evolusi, pembesaran anak, dan lain-lain; untuk memahami AI anda juga perlu tahu tentang bagaimana ia dibina. Sepanjang lima tahun yang lalu, sistem AI telah berkembang pesat dari segi keupayaan dan kerumitan. Faktor utama yang membolehkan ini ialah ketersediaan jumlah pengkomputeran yang sangat besar (atau secara bahasa sehari-hari "compute" apabila digunakan untuk AI).

Angka-angka ini mengagumkan. Kira-kira 10 <sup>25</sup> -10 <sup>26</sup> "operasi titik terapung" (FLOP) [^1] digunakan dalam latihan model seperti siri GPT, Claude, Gemini, dan sebagainya.[^2] (Sebagai perbandingan, jika setiap manusia di Bumi bekerja tanpa henti melakukan satu pengiraan setiap lima saat, ia akan mengambil masa kira-kira satu bilion tahun untuk mencapai ini.) Jumlah pengkomputeran yang besar ini membolehkan latihan model dengan sehingga trilion pemberat model pada terabait data – sebahagian besar daripada semua teks berkualiti yang pernah ditulis bersama-sama dengan perpustakaan besar bunyi, imej dan video. Melengkapi latihan ini dengan latihan tambahan yang menguatkan keutamaan manusia dan prestasi tugasan yang baik, model yang dilatih dengan cara ini mempamerkan prestasi yang setanding manusia merentasi pelbagai tugasan intelektual asas, termasuk penaakulan dan penyelesaian masalah.

Kami juga tahu (secara sangat, sangat kasar) berapa banyak kelajuan pengkomputeran, dalam operasi sesaat, yang mencukupi untuk kelajuan *inferens* [^3] sistem sedemikian sepadan dengan *kelajuan* pemprosesan teks manusia. Ia adalah kira-kira 10 <sup>15</sup> -10 <sup>16</sup> FLOP sesaat.[^4]

Walaupun berkuasa, model-model ini pada dasarnya terhad dalam cara-cara utama, agak serupa dengan bagaimana seorang manusia akan terhad jika dipaksa untuk hanya menghasilkan teks pada kadar perkataan tetap seminit, tanpa berhenti untuk berfikir atau menggunakan sebarang alat tambahan. Sistem AI yang lebih terkini menangani had ini melalui proses dan seni bina yang lebih kompleks menggabungkan beberapa elemen utama:

- Satu atau lebih rangkaian neural, dengan satu model menyediakan kapasiti kognitif teras, dan sehingga beberapa yang lain melaksanakan tugasan lain yang lebih sempit;
- *Alatan* yang disediakan kepada dan boleh digunakan oleh model – contohnya keupayaan untuk mencari web, mencipta atau mengedit dokumen, melaksanakan program, dan sebagainya.
- *Perancah* yang menghubungkan input dan output rangkaian neural. Perancah yang sangat mudah mungkin hanya membenarkan dua "contoh" model AI untuk bercakap antara satu sama lain, atau satu untuk memeriksa kerja yang lain.[^5]
- *Rantai pemikiran* dan teknik prompting berkaitan melakukan sesuatu yang serupa, menyebabkan model untuk contohnya menghasilkan banyak pendekatan kepada masalah, kemudian memproses pendekatan tersebut untuk jawapan agregat.
- *Latihan semula* model untuk menggunakan alatan, perancah, dan rantai pemikiran dengan lebih baik.

Kerana sambungan ini boleh menjadi sangat berkuasa (dan termasuk sistem AI itu sendiri), sistem komposit ini boleh menjadi agak canggih dan meningkatkan keupayaan AI secara dramatik.[^6] Dan baru-baru ini, teknik dalam perancah dan terutamanya prompting rantai pemikiran (dan melipat semula keputusan ke dalam latihan semula model untuk menggunakan ini dengan lebih baik) telah dibangunkan dan digunakan dalam [o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/), dan [DeepSeek R1](https://api-docs.deepseek.com/news/news250120) untuk melakukan banyak lintasan inferens sebagai tindak balas kepada pertanyaan yang diberikan.[^7] Ini membolehkan model untuk "berfikir tentang" responsnya dan meningkatkan keupayaan model ini secara dramatik untuk melakukan penaakulan berkaliber tinggi dalam tugasan sains, matematik, dan pengaturcaraan.[^8]

Untuk seni bina AI yang diberikan, peningkatan dalam pengkomputeran latihan [boleh diterjemahkan dengan boleh dipercayai](https://arxiv.org/abs/2405.10938) kepada penambahbaikan dalam set metrik yang ditakrifkan dengan jelas. Untuk keupayaan am yang kurang jelas ditakrifkan (seperti yang dibincangkan di bawah), terjemahan adalah kurang jelas dan ramalan, tetapi hampir pasti bahawa model yang lebih besar dengan lebih banyak pengkomputeran latihan akan mempunyai keupayaan baru dan lebih baik, walaupun sukar untuk meramalkan apa yang akan berlaku.

Begitu juga, sistem komposit dan terutamanya kemajuan dalam "rantai pemikiran" (dan latihan model yang berfungsi dengan baik dengannya) telah membuka kunci penskalaan dalam pengkomputeran *inferens*: untuk model teras yang dilatih yang diberikan, sekurang-kurangnya beberapa keupayaan sistem AI meningkat apabila lebih banyak pengkomputeran digunakan yang membolehkan mereka "berfikir dengan lebih keras dan lebih lama" tentang masalah yang kompleks. Ini datang dengan kos kelajuan pengkomputeran yang curam, memerlukan beratus-ratus atau beribu-ribu lagi FLOP/s untuk sepadan dengan prestasi manusia.[^9]

Walaupun hanya sebahagian daripada apa yang membawa kepada kemajuan AI yang pesat,[^10] peranan pengkomputeran dan kemungkinan sistem komposit akan terbukti penting untuk kedua-dua mencegah KBA yang tidak dapat dikawal dan membangunkan alternatif yang lebih selamat.


[^1]: 10 <sup>27</sup> bermaksud 1 diikuti oleh 25 sifar, atau sepuluh trilion trilion. FLOP hanyalah penambahan atau pendaraban aritmetik nombor dengan beberapa ketepatan. Perhatikan bahawa prestasi perkakasan AI boleh berbeza-beza dengan faktor sepuluh bergantung kepada ketepatan aritmetik dan seni bina komputer. Mengira operasi pintu logik (ANDS, ORS, AND NOTS) akan menjadi fundamental tetapi ini tidak tersedia atau diukur secara umum; untuk tujuan sekarang adalah berguna untuk menstandard operasi 16-bit (FP16), walaupun faktor penukaran yang sesuai harus diwujudkan.

[^2]: Koleksi anggaran dan data keras tersedia dari [Epoch AI](https://epochai.org/data/large-scale-ai-models) dan menunjukkan kira-kira 2×10 <sup>25</sup> 16-bit FLOP untuk GPT-4; ini kira-kira sepadan dengan [nombor yang bocor](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) untuk GPT-4. Anggaran untuk model pertengahan 2024 lain semuanya dalam faktor beberapa GPT-4.

[^3]: Inferens hanyalah proses menghasilkan output dari rangkaian neural. Latihan boleh dianggap sebagai penggantian banyak inferens dan pelarasan pemberat model.

[^4]: Untuk pengeluaran teks, GPT-4 asal memerlukan 560 TFLOP setiap token yang dihasilkan. Kira-kira 7 token/s diperlukan untuk mengikuti pemikiran manusia, jadi ini memberikan ≈3×10 <sup>15</sup> FLOP/s. Tetapi kecekapan telah menurunkan ini; [risalah NVIDIA ini](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/) contohnya menunjukkan serendah 3×10 <sup>14</sup> FLOP/s untuk model Llama 405B yang berprestasi setanding.

[^5]: Sebagai contoh yang sedikit lebih kompleks, sistem AI mungkin pertama menghasilkan beberapa penyelesaian yang mungkin untuk masalah matematik, kemudian menggunakan contoh lain untuk memeriksa setiap penyelesaian, dan akhirnya menggunakan yang ketiga untuk mensintesis keputusan ke dalam penjelasan yang jelas. Ini membolehkan penyelesaian masalah yang lebih menyeluruh dan boleh dipercayai daripada satu lintasan.

[^6]: Lihat contohnya butiran tentang ["Operator" OpenAI](https://openai.com/index/introducing-operator/), [keupayaan alat Claude](https://docs.anthropic.com/en/docs/build-with-claude/computer-use), dan [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). [Deep Research](https://openai.com/index/introducing-deep-research/) OpenAI mungkin mempunyai seni bina yang agak canggih tetapi butiran tidak tersedia.

[^7]: Deepseek R1 bergantung pada latihan berulang dan prompting model supaya model latihan akhir menghasilkan penaakulan rantai pemikiran yang meluas. Butiran seni bina tidak tersedia untuk o1 atau o3, namun Deepseek telah mendedahkan bahawa tiada "sos istimewa" tertentu diperlukan untuk membuka kunci penskalaan keupayaan dengan inferens. Tetapi walaupun menerima banyak akhbar sebagai mengubah "status quo" dalam AI, ia tidak mempengaruhi tuntutan teras esei ini.

[^8]: Model ini mengatasi model standard dengan ketara pada penanda aras penaakulan. Contohnya, dalam Penanda Aras GPQA Diamond—ujian ketat soalan sains peringkat PhD—GPT-4o [mendapat skor](https://openai.com/index/learning-to-reason-with-llms/) 56%, sementara o1 dan o3 mencapai 78% dan 88%, masing-masing, jauh melebihi skor purata 70% pakar manusia.

[^9]: O3 OpenAI mungkin membelanjakan ∼10 <sup>21</sup> -10 <sup>22</sup> FLOP [untuk menyelesaikan setiap soalan cabaran ARC-AGI](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai), yang manusia yang cekap boleh lakukan dalam (katakan) 10-100 saat, memberikan angka lebih seperti ∼10 <sup>20</sup> FLOP/s.

[^10]: Walaupun pengkomputeran adalah ukuran utama keupayaan sistem AI, ia berinteraksi dengan kedua-dua kualiti data dan penambahbaikan algoritma. Data atau algoritma yang lebih baik boleh mengurangkan keperluan pengkomputeran, sementara lebih banyak pengkomputeran kadang-kadang boleh mengimbangi data atau algoritma yang lemah.