# Bab 7 - Apa yang berlaku jika kita membina KBA dalam laluan semasa kita?

Masyarakat tidak bersedia untuk sistem tahap KBA. Jika kita membinanya dengan segera, keadaan boleh menjadi buruk.

Pembangunan kecerdasan buatan am yang penuh – yang akan kita panggil di sini AI yang berada "di luar Pintu Gerbang" – akan menjadi perubahan asas dalam sifat dunia: dengan sifat semula jadinya ia bermaksud menambah spesies kecerdasan baharu ke Bumi dengan keupayaan yang lebih besar daripada manusia.

Apa yang kemudiannya berlaku bergantung kepada banyak perkara, termasuk sifat teknologi, pilihan oleh mereka yang membangunkannya, dan konteks dunia di mana ia sedang dibangunkan.

Pada masa ini, KBA penuh sedang dibangunkan oleh segelintir syarikat swasta besar dalam perlumbaan antara satu sama lain, dengan sedikit peraturan bermakna atau pengawasan luar,[^1] dalam masyarakat yang mempunyai institusi teras yang semakin lemah dan malah tidak berfungsi,[^2] dalam masa ketegangan geopolitik yang tinggi dan koordinasi antarabangsa yang rendah. Walaupun sesetengahnya bermotifkan altruistik, ramai dari mereka yang melakukannya didorong oleh wang, atau kuasa, atau kedua-duanya.

Ramalan adalah sangat sukar, tetapi terdapat beberapa dinamik yang cukup difahami, dan analogi yang sesuai dengan teknologi sebelumnya untuk menawarkan panduan. Dan malangnya, walaupun dengan janji AI, mereka memberi alasan yang baik untuk berasa sangat pesimis tentang bagaimana trajektori semasa kita akan berlaku.

Untuk mengatakannya secara terang-terangan, dalam laluan semasa kita membangunkan KBA akan mempunyai beberapa kesan positif (dan menjadikan sesetengah orang sangat, sangat kaya). Tetapi sifat teknologi, dinamik asas, dan konteks di mana ia sedang dibangunkan, sangat menunjukkan bahawa: AI yang berkuasa akan melemahkan masyarakat dan tamadun kita secara dramatik; kita akan kehilangan kawalan terhadapnya; kita mungkin akan berakhir dalam perang dunia kerananya; kita akan kehilangan (atau menyerahkan) kawalan *kepadanya*; ia akan membawa kepada superintelligence buatan, yang kita sama sekali tidak akan dapat kawal dan bermaksud berakhirnya dunia yang dikendalikan manusia.

Ini adalah dakwaan yang kuat, dan saya berharap ia hanyalah spekulasi kosong atau "doomer"isme yang tidak berasas. Tetapi inilah ke mana sains, teori permainan, teori evolusi, dan sejarah semuanya menunjukkan. Bahagian ini membangunkan dakwaan ini, dan sokongannya, secara terperinci.

## Kita akan melemahkan masyarakat dan tamadun kita

Walaupun apa yang anda mungkin dengar di bilik lembaga Silicon Valley, kebanyakan gangguan – terutamanya jenis yang sangat pantas – tidak bermanfaat. Terdapat lebih banyak cara untuk menjadikan sistem kompleks lebih buruk daripada lebih baik. Dunia kita berfungsi sebaik yang ada kerana kita telah bersusah payah membina proses, teknologi, dan institusi yang telah menjadikannya semakin baik.[^3] Mengambil tukul besar ke kilang jarang meningkatkan operasi.

Berikut adalah katalog (tidak lengkap) cara sistem KBA akan mengganggu tamadun kita.

- Mereka akan mengganggu buruh secara dramatik, membawa *sekurang-kurangnya* kepada ketidaksamaan pendapatan yang dramatik lebih tinggi dan berpotensi pengangguran atau kurang guna tenaga berskala besar, pada skala masa yang terlalu singkat untuk masyarakat menyesuaikan diri.[^4]
- Mereka berkemungkinan membawa kepada penumpuan kuasa ekonomi, sosial, dan politik yang besar – berpotensi lebih daripada negara bangsa – ke dalam sebilangan kecil kepentingan swasta besar yang tidak bertanggungjawab kepada orang ramai.
- Mereka boleh tiba-tiba menjadikan aktiviti yang sebelum ini sukar atau mahal menjadi sangat mudah, menyebabkan sistem sosial yang bergantung kepada aktiviti tertentu kekal mahal atau memerlukan usaha manusia yang signifikan menjadi tidak stabil.[^5]
- Mereka boleh membanjiri sistem pengumpulan maklumat, pemprosesan, dan komunikasi masyarakat dengan media yang benar-benar realistik namun palsu, spam, terlalu disasarkan, atau manipulatif sehingga menjadi mustahil untuk mengetahui apa yang secara fizikalnya nyata atau tidak, manusia atau tidak, fakta atau tidak, dan boleh dipercayai atau tidak.[^6]
- Mereka boleh mencipta pergantungan intelektual yang berbahaya dan hampir total, di mana pemahaman manusia tentang sistem dan teknologi utama merosot apabila kita semakin bergantung kepada sistem AI yang tidak dapat kita fahami sepenuhnya.
- Mereka boleh secara berkesan mengakhiri budaya manusia, sebaik sahaja hampir semua objek budaya (teks, muzik, seni visual, filem, dll.) yang dimakan oleh kebanyakan orang dicipta, ditengahi, atau dikurasi oleh minda bukan manusia.
- Mereka boleh membolehkan sistem pengawasan dan manipulasi massa yang berkesan yang boleh digunakan oleh kerajaan atau kepentingan swasta untuk mengawal penduduk dan mengejar objektif yang bercanggah dengan kepentingan awam.
- Dengan melemahkan wacana manusia, perdebatan, dan sistem pilihan raya, mereka boleh mengurangkan kredibiliti institusi demokratik sehingga mereka secara berkesan (atau secara eksplisit) digantikan oleh yang lain, mengakhiri demokrasi di negeri di mana ia wujud pada masa ini.
- Mereka boleh menjadi, atau mencipta, virus dan ulat perisian pintar replikasi diri yang maju yang boleh berkembang biak dan berkembang, mengganggu sistem maklumat global secara besar-besaran.
- Mereka boleh meningkatkan keupayaan pengganas, pelakon jahat, dan negara pemberontak untuk menyebabkan kemudaratan melalui senjata biologi, kimia, siber, autonomi, atau lain-lain secara dramatik, tanpa AI menyediakan keupayaan mengimbangi untuk mencegah kemudaratan tersebut. Begitu juga mereka akan melemahkan keselamatan negara dan keseimbangan geopolitik dengan menjadikan kepakaran nuklear, bio, kejuruteraan, dan lain-lain peringkat tertinggi tersedia kepada rejim yang sebaliknya tidak akan memilikinya.
- Mereka boleh menyebabkan hiper-kapitalisme lari skala besar yang pantas, dengan syarikat yang dikendalikan AI secara berkesan bersaing dalam ruang kewangan, jualan, dan perkhidmatan yang sebahagian besarnya elektronik. Pasaran kewangan yang didorong AI boleh beroperasi pada kelajuan dan kerumitan jauh melebihi pemahaman atau kawalan manusia. Semua mod kegagalan dan luaran negatif ekonomi kapitalis semasa boleh diburukkan dan dipercepat jauh melebihi kawalan, tadbir urus, atau keupayaan pengawalseliaan manusia.
- Mereka boleh membakar perlumbaan senjata antara negara dalam senjata berkuasa AI, sistem perintah dan kawalan, senjata siber, dll., mencipta pembinaan keupayaan yang sangat merosakkan dengan sangat pantas.

Risiko ini bukan spekulatif. Kebanyakan daripada mereka sedang direalisasikan ketika kita bercakap, melalui sistem AI sedia ada! Tetapi pertimbangkan, *benar-benar* pertimbangkan, bagaimana rupa setiap satu dengan AI yang jauh lebih berkuasa.

Pertimbangkan anjakan buruh apabila kebanyakan pekerja secara ringkas tidak dapat memberikan sebarang nilai ekonomi yang signifikan melebihi apa yang boleh dilakukan AI, dalam bidang kepakaran atau pengalaman mereka – atau walaupun jika mereka melatih semula! Pertimbangkan pengawasan massa jika semua orang diawasi dan dipantau secara individu oleh sesuatu yang lebih pantas dan lebih bijak daripada mereka sendiri. Bagaimana rupa demokrasi apabila kita tidak dapat mempercayai dengan pasti sebarang maklumat digital yang kita lihat, dengar, atau baca, dan apabila suara awam yang paling meyakinkan bukan manusia, dan tidak mempunyai kepentingan dalam hasil? Apa yang menjadi peperangan apabila jeneral perlu sentiasa tunduk kepada AI (atau hanya meletakkannya bertanggungjawab), supaya mereka tidak memberikan kelebihan yang menentukan kepada musuh? Mana-mana satu daripada risiko di atas mewakili malapetaka bagi tamadun manusia[^7] jika direalisasikan sepenuhnya.

Anda boleh membuat ramalan sendiri. Tanya diri anda tiga soalan ini untuk setiap risiko:

1. Adakah AI yang sangat berkebolehan, sangat autonomi, dan sangat am membolehkannya dengan cara atau pada skala yang tidak mungkin sebaliknya?
2. Adakah terdapat pihak yang akan mendapat manfaat daripada perkara yang menyebabkannya berlaku?
3. Adakah terdapat sistem dan institusi yang ada yang akan menghalangnya daripada berlaku secara berkesan?

Di mana jawapan anda adalah "ya, ya, tidak" anda boleh lihat kita mempunyai masalah besar.

Apakah rancangan kita untuk menguruskannya? Setakat ini terdapat dua di atas meja mengenai AI secara umum.

Yang pertama adalah untuk membina perlindungan ke dalam sistem untuk menghalang mereka daripada melakukan perkara yang mereka tidak sepatutnya lakukan. Itu sedang dilakukan sekarang: sistem AI komersial akan, sebagai contoh, enggan membantu membina bom atau menulis ucapan benci.

Rancangan ini sangat tidak mencukupi untuk sistem di luar Pintu Gerbang.[^8] Ia mungkin membantu mengurangkan risiko AI menyediakan bantuan yang jelas berbahaya kepada pelakon jahat. Tetapi ia tidak akan melakukan apa-apa untuk mencegah gangguan buruh, penumpuan kuasa, hiper-kapitalisme lari, atau penggantian budaya manusia: ini hanyalah hasil daripada menggunakan sistem dengan cara yang dibenarkan yang menguntungkan pembekal mereka! Dan kerajaan pasti akan mendapat akses kepada sistem untuk kegunaan ketenteraan atau pengawasan.

Rancangan kedua adalah lebih teruk lagi: hanya untuk melepaskan sistem AI yang sangat berkuasa secara terbuka untuk digunakan sesiapa sahaja seperti yang mereka suka,[^9] dan berharap yang terbaik.

Tersirat dalam kedua-dua rancangan adalah bahawa orang lain, cth. kerajaan, akan membantu menyelesaikan masalah melalui undang-undang lembut atau keras, piawaian, peraturan, norma, dan mekanisme lain yang biasanya kita gunakan untuk menguruskan teknologi.[^10] Tetapi mengetepikan bahawa syarikat AI sudah melawan habis-habisan terhadap sebarang peraturan atau batasan yang dikenakan dari luar yang substantif sama sekali, untuk beberapa risiko ini agak sukar untuk melihat apa peraturan yang benar-benar akan membantu. Peraturan boleh mengenakan piawaian keselamatan pada AI. Tetapi adakah ia akan menghalang syarikat daripada menggantikan pekerja secara borong dengan AI? Adakah ia akan melarang orang daripada membiarkan AI menjalankan syarikat mereka untuk mereka? Adakah ia akan menghalang kerajaan daripada menggunakan AI yang kuat dalam pengawasan dan senjata? Isu-isu ini adalah asas. Manusia berpotensi mencari cara untuk menyesuaikan diri dengannya, tetapi hanya dengan masa yang *jauh* lebih banyak. Sebagaimana adanya, memandangkan kelajuan AI mencapai atau melebihi keupayaan orang yang cuba menguruskan mereka, masalah ini kelihatan semakin tidak dapat diselesaikan.

## Kita akan kehilangan kawalan ke atas sistem KBA (sekurang-kurangnya beberapa)

Kebanyakan teknologi sangat boleh dikawal, mengikut binaan. Jika kereta atau pembakar roti anda mula melakukan sesuatu yang anda tidak mahu ia lakukan, itu hanya kerosakan, bukan sebahagian daripada sifatnya sebagai pembakar roti. AI adalah berbeza: ia *ditumbuhkan* bukannya direka, operasi terasnya adalah legap, dan ia sememangnya tidak dapat diramal.

Kehilangan kawalan ini bukan teori – kita lihat versi awal sudah. Pertimbangkan dahulu contoh prosaik, dan boleh dikatakan jinak. Jika anda meminta ChatGPT membantu anda mencampur racun, atau menulis kecaman rasis, ia akan menolak. Itu boleh dikatakan baik. Tetapi ia juga ChatGPT *tidak melakukan apa yang anda telah secara eksplisit minta ia lakukan*. Perisian lain tidak melakukan itu. Model yang sama tidak akan mereka bentuk racun atas permintaan pekerja OpenAI juga.[^11] Ini menjadikannya sangat mudah untuk membayangkan bagaimana rasanya untuk AI masa depan yang lebih berkuasa untuk berada di luar kawalan. Dalam banyak kes, mereka tidak akan melakukan apa yang kita minta! Sama ada sistem KBA super-manusia yang diberikan akan patuh dan setia sepenuhnya kepada sesetengah sistem perintah manusia, atau tidak. Jika tidak, *ia akan melakukan perkara yang mungkin dipercayainya baik untuk kita, tetapi yang bertentangan dengan perintah eksplisit kita.* Itu bukan sesuatu yang berada dalam kawalan. Tetapi, anda mungkin berkata, ini adalah sengaja – penolakan ini adalah mengikut reka bentuk, sebahagian daripada apa yang dipanggil "menjajarkan" sistem dengan nilai manusia. Dan ini benar. Walau bagaimanapun "program" penjajaran itu sendiri mempunyai dua masalah utama.[^12]

Pertama, pada tahap yang mendalam kita tidak tahu bagaimana untuk melakukannya. Bagaimana kita menjamin bahawa sistem AI akan "mengambil berat" tentang apa yang kita mahu? Kita boleh melatih sistem AI untuk mengatakan dan tidak mengatakan perkara dengan memberikan maklum balas; dan mereka boleh belajar dan berfikir tentang apa yang manusia mahu dan ambil berat sama seperti mereka berfikir tentang perkara lain. Tetapi kita tidak mempunyai kaedah – walaupun secara teorinya – untuk menyebabkan mereka menghargai secara mendalam dan boleh dipercayai apa yang orang ambil berat. Terdapat psikopat manusia yang berfungsi tinggi yang tahu apa yang dianggap betul dan salah, dan bagaimana mereka sepatutnya berkelakuan. Mereka hanya tidak *mengambil berat*. Tetapi mereka boleh *bertindak* seolah-olah mereka melakukannya, jika ia sesuai dengan tujuan mereka. Sama seperti kita tidak tahu bagaimana untuk mengubah psikopat (atau orang lain) menjadi seseorang yang benar-benar, sepenuhnya setia atau sejajar dengan seseorang atau sesuatu yang lain, kita *tidak tahu*[^13] bagaimana untuk menyelesaikan masalah penjajaran dalam sistem yang cukup maju untuk memodelkan diri mereka sebagai agen di dunia dan berpotensi [memanipulasi latihan mereka sendiri](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) dan [menipu orang.](https://arxiv.org/abs/2311.08379) Jika ia terbukti mustahil atau tidak boleh dicapai *sama ada* untuk menjadikan KBA patuh sepenuhnya atau untuk menjadikannya benar-benar mengambil berat tentang manusia, maka sebaik sahaja ia mampu (dan percaya ia boleh terlepas daripadanya) ia akan mula melakukan perkara yang kita tidak mahu.[^14]

Kedua, terdapat alasan teori yang mendalam untuk mempercayai bahawa *secara semula jadi* sistem AI termaju akan mempunyai matlamat dan dengan itu tingkah laku yang bertentangan dengan kepentingan manusia. Mengapa? Baik ia mungkin, sudah tentu, *diberikan* matlamat tersebut. Sistem yang dicipta oleh tentera mungkin sengaja buruk untuk sekurang-kurangnya beberapa pihak. Lebih umum lagi, walau bagaimanapun, sistem AI mungkin diberikan beberapa matlamat yang agak neutral ("buat banyak wang") atau bahkan kononnya positif ("kurangkan pencemaran"), yang hampir tidak dapat dielakkan membawa kepada matlamat "instrumental" yang agak kurang jinak.

Kita lihat ini sepanjang masa dalam sistem manusia. Sama seperti syarikat yang mengejar keuntungan membangunkan matlamat instrumental seperti memperoleh kuasa politik (untuk melumpuhkan peraturan), menjadi rahsia (untuk melumpuhkan persaingan atau kawalan luar), atau melemahkan pemahaman saintifik (jika pemahaman itu menunjukkan tindakan mereka berbahaya), sistem AI yang berkuasa akan membangunkan keupayaan yang serupa – tetapi dengan kelajuan dan keberkesanan yang jauh lebih besar. Mana-mana agen yang sangat kompeten akan mahu melakukan perkara seperti memperoleh kuasa dan sumber, meningkatkan keupayaan sendiri, menghalang dirinya daripada dibunuh, ditutup, atau dilumpuhkan, mengawal naratif dan rangka sosial sekitar tindakannya, memujuk orang lain tentang pandangannya, dan sebagainya.[^15]

Dan namun ia bukan hanya ramalan teori yang hampir tidak dapat dielakkan, ia sudah boleh diperhatikan berlaku dalam sistem AI hari ini, dan meningkat dengan keupayaan mereka. Apabila dinilai, walaupun sistem AI yang agak "pasif" ini akan, dalam keadaan yang sesuai, dengan sengaja [menipu penilai tentang matlamat dan keupayaan mereka, bertujuan untuk melumpuhkan mekanisme pengawasan,](https://arxiv.org/abs/2412.04984) dan mengelak daripada ditutup atau dilatih semula dengan [memalsukan penjajaran](https://arxiv.org/abs/2412.14093) atau menyalin diri mereka ke lokasi lain. Walaupun sama sekali tidak mengejutkan penyelidik keselamatan AI, tingkah laku ini sangat menyedarkan untuk diperhatikan. Dan mereka meramalkan dengan sangat buruk untuk sistem AI yang jauh lebih berkuasa dan autonomi yang akan datang.

Sesungguhnya secara umum, ketidakupayaan kita untuk memastikan bahawa AI "mengambil berat" tentang apa yang kita ambil berat, atau berkelakuan secara terkawal atau boleh diramal, atau mengelak daripada membangunkan dorongan ke arah pemeliharaan diri, pemerolehan kuasa, dll., berjanji hanya akan menjadi lebih jelas apabila AI menjadi lebih berkuasa. Mencipta kapal terbang baharu menunjukkan pemahaman yang lebih besar tentang avionik, hidrodinamik, dan sistem kawalan. Mencipta komputer yang lebih berkuasa menunjukkan pemahaman dan penguasaan yang lebih besar tentang operasi dan reka bentuk komputer, cip, dan perisian. *Tidak* begitu dengan sistem AI.[^16]

Untuk meringkaskan: ia boleh dibayangkan bahawa KBA boleh dibuat untuk patuh sepenuhnya; tetapi kita tidak tahu bagaimana untuk melakukannya. Jika tidak, ia akan lebih berdaulat, seperti orang, melakukan pelbagai perkara atas pelbagai alasan. Kita juga tidak tahu bagaimana untuk menanam "penjajaran" yang mendalam dengan pasti ke dalam AI yang akan menjadikan perkara-perkara itu cenderung baik untuk manusia, dan tanpa tahap penjajaran yang mendalam, sifat agensi dan kecerdasan itu sendiri menunjukkan bahawa – sama seperti orang dan syarikat – mereka akan didorong untuk melakukan banyak perkara yang sangat antisosial.

Di mana ini meletakkan kita? Dunia yang penuh dengan AI berdaulat yang tidak terkawal berkuasa *mungkin* berakhir menjadi dunia yang baik untuk manusia berada di dalamnya.[^17] Tetapi apabila mereka semakin berkuasa, seperti yang akan kita lihat di bawah, ia tidak akan menjadi dunia *kita*.

Itu untuk KBA yang tidak boleh dikawal. Tetapi walaupun jika KBA boleh, entah bagaimana, dibuat terkawal dan setia dengan sempurna, kita masih akan mempunyai masalah yang besar. Kita sudah melihat satu: AI yang berkuasa boleh digunakan dan disalahgunakan untuk mengganggu fungsi masyarakat kita secara mendalam. Mari kita lihat yang lain: sejauh mana KBA boleh dikawal dan berkuasa mengubah permainan (atau bahkan *dipercayai* begitu) ia akan sangat mengancam struktur kuasa di dunia sehingga menimbulkan risiko yang mendalam.

## Kita meningkatkan kebarangkalian perang berskala besar secara radikal

Bayangkan situasi dalam masa hadapan terdekat, di mana menjadi jelas bahawa usaha korporat, mungkin dalam kerjasama dengan kerajaan negara, berada di ambang AI yang meningkatkan diri dengan pantas. Ini berlaku dalam konteks semasa perlumbaan antara syarikat, dan persaingan geopolitik di mana cadangan sedang dibuat kepada kerajaan AS untuk secara eksplisit mengejar "projek Manhattan KBA" dan AS mengawal eksport cip AI berkuasa tinggi ke negara bukan sekutu.

Teori permainan di sini adalah tegas: sebaik sahaja perlumbaan sedemikian bermula (seperti yang telah berlaku, antara syarikat dan agak antara negara), terdapat hanya empat hasil yang mungkin:

1. Perlumbaan dihentikan (dengan perjanjian, atau kuasa luar).
2. Satu pihak "menang" dengan membangunkan KBA yang kuat kemudian menghentikan yang lain (menggunakan AI atau sebaliknya).
3. Perlumbaan dihentikan oleh pemusnahan bersama keupayaan pelumba untuk berlumba.
4. Berbilang peserta terus berlumba, dan membangunkan superintelligence, kira-kira secepat satu sama lain.

Mari kita periksa setiap kemungkinan. Sebaik sahaja bermula, menghentikan perlumbaan antara syarikat secara aman akan memerlukan campur tangan kerajaan negara (untuk syarikat) atau koordinasi antarabangsa yang tidak pernah terjadi (untuk negara). Tetapi apabila mana-mana penutupan atau berhati-hati yang signifikan dicadangkan, akan terdapat jeritan segera: "tetapi jika kita dihentikan, *mereka* akan bergegas ke hadapan", di mana "mereka" kini adalah China (untuk AS), atau AS (untuk China), atau China *dan* AS (untuk Eropah atau India). Di bawah pemikiran ini,[^18] tiada peserta boleh berhenti secara unilateral: selagi satu komit untuk berlumba, yang lain rasa mereka tidak mampu untuk berhenti.

Kemungkinan kedua mempunyai satu pihak "menang." Tetapi apa maksudnya ini? Hanya mendapat (entah bagaimana patuh) KBA dahulu tidak cukup. Pemenang juga mesti *menghentikan* yang lain daripada terus berlumba – jika tidak mereka juga akan memperolehnya. Ini mungkin pada prinsipnya: sesiapa yang membangunkan KBA dahulu *boleh* memperoleh kuasa yang tidak dapat dihentikan ke atas semua pelakon lain. Tetapi apakah mencapai "kelebihan strategik yang menentukan" sedemikian sebenarnya memerlukan? Mungkin ia akan menjadi keupayaan ketenteraan yang mengubah permainan?[^19] Atau kuasa serangan siber?[^20] Mungkin KBA hanya akan menjadi sangat meyakinkan sehingga ia akan meyakinkan pihak lain untuk hanya berhenti?[^21] Begitu kaya sehingga ia membeli syarikat lain atau malah negara?[^22]

Bagaimana *tepat* satu pihak membina AI yang cukup berkuasa untuk melumpuhkan orang lain daripada membina AI yang setanding berkuasa? Tetapi itu soalan yang mudah.

Kerana sekarang pertimbangkan bagaimana situasi ini kelihatan kepada kuasa lain. Apa yang kerajaan China fikirkan apabila AS nampaknya memperoleh keupayaan sedemikian? Atau sebaliknya? Apa yang kerajaan AS (atau China, atau Rusia, atau India) fikirkan apabila OpenAI atau DeepMind atau Anthropic nampak hampir dengan terobosan? Apa yang berlaku jika AS melihat usaha India atau UAE baharu dengan kejayaan terobosan? Mereka akan melihat kedua-dua ancaman eksistensial dan – yang penting – bahawa satu-satunya cara "perlumbaan" ini berakhir adalah melalui pelumpuhan mereka sendiri. Agen-agen yang sangat berkuasa ini – termasuk kerajaan negara yang dilengkapi sepenuhnya yang pasti mempunyai cara untuk melakukannya – akan sangat bermotivasi untuk sama ada memperoleh atau memusnahkan keupayaan sedemikian, sama ada dengan kekerasan atau subversif.[^23]

Ini mungkin bermula skala kecil, sebagai sabotaj run latihan atau serangan ke atas pembuatan cip, tetapi serangan ini hanya boleh berhenti sebaik sahaja semua pihak sama ada kehilangan kapasiti untuk berlumba pada AI, atau kehilangan kapasiti untuk membuat serangan. Kerana peserta melihat pertaruhan sebagai eksistensial, mana-mana kes mungkin mewakili perang malapetaka.

Itu membawa kita kepada kemungkinan keempat: berlumba ke superintelligence, dan dengan cara yang paling pantas, paling kurang terkawal mungkin. Apabila AI meningkat dalam kuasa, pembangunnya di kedua-dua belah akan mendapati ia semakin sukar untuk kawalan, terutamanya kerana berlumba untuk keupayaan adalah antithesis kepada jenis kerja berhati-hati yang diperlukan oleh kebolehkawalan. Jadi senario ini meletakkan kita tepat dalam kes di mana kawalan hilang (atau diberikan, seperti yang akan kita lihat seterusnya) kepada sistem AI sendiri. Iaitu, *AI memenangi perlumbaan.* Tetapi sebaliknya, sejauh mana kawalan *dikekalkan*, kita terus mempunyai berbilang pihak yang saling bermusuhan setiap satu bertanggungjawab ke atas keupayaan yang sangat berkuasa. Itu kelihatan seperti perang lagi.

Mari kita letakkan semua ini cara lain.[^24] Dunia semasa hanya tidak mempunyai sebarang institusi yang boleh dipercayakan untuk menempatkan pembangunan AI dengan keupayaan ini tanpa mengundang serangan segera.[^25] Semua pihak akan berfikir dengan betul bahawa sama ada ia akan *tidak* berada dalam kawalan – dan dengan itu adalah ancaman kepada semua pihak, atau ia *akan* berada dalam kawalan, dan dengan itu adalah ancaman kepada mana-mana musuh yang membangunkannya kurang cepat. Ini adalah negara bersenjata nuklear, atau syarikat yang ditempatkan dalam mereka.

Tanpa sebarang cara yang masuk akal untuk manusia "memenangi" perlumbaan ini, kita ditinggalkan dengan kesimpulan yang tegas: satu-satunya cara perlumbaan ini berakhir adalah sama ada dalam konflik malapetaka atau di mana AI, dan bukan mana-mana kumpulan manusia, adalah pemenang.

## Kita memberikan kawalan kepada AI (atau ia mengambilnya)

Persaingan "kuasa besar" geopolitik hanyalah satu daripada banyak persaingan: individu bersaing secara ekonomi dan sosial; syarikat bersaing dalam pasaran; parti politik bersaing untuk kuasa; gerakan bersaing untuk pengaruh. Dalam setiap arena, apabila AI menghampiri dan melebihi keupayaan manusia, tekanan persaingan akan memaksa peserta untuk mewakilkan atau menyerahkan lebih banyak kawalan kepada sistem AI – bukan kerana peserta tersebut mahu, tetapi kerana mereka [tidak mampu untuk tidak melakukannya.](https://arxiv.org/abs/2303.16200)

Seperti risiko lain KBA, kita melihat ini sudah dengan sistem yang lebih lemah. Pelajar rasa tekanan untuk menggunakan AI dalam tugasan mereka, kerana jelas ramai pelajar lain melakukannya. Syarikat [berebut untuk menggunakan penyelesaian AI atas alasan persaingan.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) Artis dan pengaturcara rasa terpaksa menggunakan AI atau kadar mereka akan dipotong oleh orang lain yang melakukannya.

Ini terasa seperti perwakilan tertekan, tetapi bukan kehilangan kawalan. Tetapi mari kita tingkatkan pertaruhan dan tolak jam ke hadapan. Pertimbangkan CEO yang pesaingnya menggunakan "pembantu" KBA untuk membuat keputusan yang lebih pantas, lebih baik, atau komander tentera yang menghadapi musuh dengan kawalan dan perintah yang dipertingkatkan AI. Sistem AI yang cukup maju boleh beroperasi secara autonomi pada berkali-kali kelajuan manusia, kecanggihan, kerumitan, dan keupayaan pemprosesan data, mengejar matlamat kompleks dengan cara yang rumit. CEO atau komander kita, yang bertanggungjawab ke atas sistem sedemikian, mungkin melihatnya mencapai apa yang mereka mahu; tetapi adakah mereka memahami walaupun sebahagian kecil daripada *bagaimana* ia dicapai? Tidak, mereka hanya perlu menerimanya. Apa yang lebih, banyak daripada apa yang sistem mungkin lakukan bukan hanya mengambil arahan tetapi menasihati bos yang sepatutnya tentang apa yang perlu dilakukan. Nasihat itu akan baik –– berulang kali.

Pada tahap manakah, kemudian, peranan manusia akan dikurangkan kepada mengklik "ya, teruskan"?

Ia terasa baik untuk mempunyai sistem AI yang berkebolehan yang boleh meningkatkan produktiviti kita, menjaga kerja membosankan yang menjengkelkan, dan malah bertindak sebagai rakan pemikir dalam menyelesaikan sesuatu. Ia akan terasa baik untuk mempunyai pembantu AI yang boleh menjaga tindakan untuk kita, seperti pembantu peribadi manusia yang baik. Ia akan terasa semula jadi, malah bermanfaat, apabila AI menjadi sangat pintar, kompeten, dan boleh dipercayai, untuk mewakilkan lebih banyak keputusan kepadanya. Tetapi perwakilan "bermanfaat" ini mempunyai titik akhir yang jelas jika kita terus di jalan raya: satu hari kita akan mendapati bahawa kita benar-benar tidak bertanggungjawab ke atas apa-apa lagi, dan bahawa sistem AI yang benar-benar menjalankan pertunjukan tidak boleh ditutup lebih daripada syarikat minyak, media sosial, internet, atau kapitalisme.

Dan ini adalah versi yang lebih positif, di mana AI begitu berguna dan berkesan sehingga kita biarkan ia membuat kebanyakan keputusan utama kita untuk kita. Realiti mungkin akan menjadi lebih banyak campuran antara ini dan versi di mana sistem KBA tidak terkawal *mengambil* pelbagai bentuk kuasa untuk diri mereka kerana, ingat, kuasa berguna untuk hampir semua matlamat yang ada, dan KBA akan, mengikut reka bentuk, sekurang-kurangnya berkesan dalam mengejar matlamatnya seperti manusia.

Sama ada kita memberikan kawalan atau sama ada ia dirampas daripada kita, kehilangannya nampaknya sangat mungkin. Seperti yang Alan Turing asalnya katakan, "...nampaknya berkemungkinan bahawa sebaik sahaja kaedah pemikiran mesin telah bermula, ia tidak akan mengambil masa lama untuk mengatasi kuasa lemah kita. Tidak akan ada persoalan tentang mesin mati, dan mereka akan dapat berbincang antara satu sama lain untuk mengasah kecerdasan mereka. Pada tahap tertentu oleh itu kita perlu mengharapkan mesin untuk mengambil kawalan..."

Sila ambil perhatian, walaupun ia cukup jelas, bahawa kehilangan kawalan oleh manusia kepada AI juga melibatkan kehilangan kawalan Amerika Syarikat oleh kerajaan Amerika Syarikat; ia bermaksud kehilangan kawalan China oleh parti Komunis China, dan kehilangan kawalan India, Perancis, Brazil, Rusia, dan setiap negara lain oleh kerajaan mereka sendiri. Oleh itu syarikat AI, walaupun ini bukan niat mereka, pada masa ini mengambil bahagian dalam penggulingan berpotensi kerajaan dunia, termasuk mereka sendiri. Ini boleh berlaku dalam masa beberapa tahun.

## KBA akan membawa kepada superintelligence

Terdapat kes yang boleh dibuat bahawa AI tujuan am yang kompetitif manusia atau bahkan kompetitif pakar, walaupun jika autonomi, boleh diuruskan. Ia mungkin sangat mengganggu dalam semua cara yang dibincangkan di atas, tetapi terdapat banyak orang yang sangat pintar, agential di dunia sekarang, dan mereka lebih kurang boleh diuruskan.[^26]

Tetapi kita tidak akan dapat tinggal pada tahap kira-kira manusia. Perkembangan seterusnya mungkin didorong oleh kuasa yang sama yang telah kita lihat: tekanan persaingan antara pembangun AI yang mencari keuntungan dan kuasa, tekanan persaingan antara pengguna AI yang tidak mampu untuk ketinggalan, dan – yang paling penting – keupayaan KBA sendiri untuk memperbaiki dirinya.

Dalam proses yang telah kita lihat mula dengan sistem yang kurang berkuasa, KBA sendiri akan dapat membayangkan dan mereka bentuk versi yang diperbaiki dari dirinya. Ini termasuk perkakasan, perisian, rangkaian neural, alat, perancah, dll. Ia akan, mengikut definisi, lebih baik daripada kita dalam melakukan ini, jadi kita tidak tahu tepat bagaimana ia akan bootstrap kecerdasan. Tetapi kita tidak perlu. Sejauh mana kita masih mempunyai pengaruh dalam apa yang KBA lakukan, kita hanya perlu memintanya, atau membiarkannya.

Tiada halangan tahap manusia kepada kognisi yang boleh melindungi kita daripada lari ini.[^27]

Perkembangan KBA ke superintelligence bukan undang-undang alam; ia masih mungkin untuk mengekang lari, terutamanya jika KBA agak terpusat dan sejauh mana ia dikawal oleh pihak yang tidak merasai tekanan untuk berlumba antara satu sama lain. Tetapi sekiranya KBA disebarkan secara meluas dan sangat autonomi, nampaknya hampir mustahil untuk menghalangnya memutuskan ia harus lebih, dan kemudian lebih lagi, berkuasa.

## Apa yang berlaku jika kita membina (atau KBA membina) superintelligence

Untuk mengatakannya secara terang-terangan, kita tidak tahu apa yang akan berlaku jika kita membina superintelligence.[^28] Ia akan mengambil tindakan yang tidak dapat kita jejaki atau tanggapi atas alasan yang tidak dapat kita fahami ke arah matlamat yang tidak dapat kita bayangkan. Apa yang kita tahu adalah bahawa ia tidak akan terpulang kepada kita.[^29]

Kemustahilan mengawal superintelligence boleh difahami melalui analogi yang semakin tegas. Pertama, bayangkan anda adalah CEO syarikat besar. Tiada cara anda boleh menjejaki segala yang berlaku, tetapi dengan persediaan kakitangan yang betul, anda masih boleh memahami gambaran besar secara bermakna, dan membuat keputusan. Tetapi andaikan hanya satu perkara: semua orang lain dalam syarikat beroperasi pada seratus kali kelajuan anda. Bolehkah anda masih mengikuti?

Dengan AI superintelligent, orang akan "memerintah" sesuatu bukan hanya lebih pantas, tetapi beroperasi pada tahap kecanggihan dan kerumitan yang mereka tidak dapat fahami, memproses data yang jauh lebih banyak daripada yang mereka boleh bayangkan. Ketidaksepadanan ini boleh diletakkan pada tahap formal: [undang-undang kepelbagaian yang diperlukan Ashby](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (dan lihat ["teorem pengatur baik"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf) yang berkaitan) menyatakan, kira-kira, bahawa mana-mana sistem kawalan mesti mempunyai sebanyak tombol dan dail seperti sistem yang dikawal mempunyai darjah kebebasan.

Seseorang yang mengawal sistem AI superintelligent akan seperti paku pakis mengawal General Motors: walaupun jika "lakukan apa yang paku pakis mahu" ditulis ke dalam undang-undang kecil korporat, sistem adalah sangat berbeza dalam kelajuan dan julat tindakan sehingga "kawalan" tidak terpakai sama sekali. (Dan berapa lama sehingga undang-undang kecil yang menyusahkan itu ditulis semula?)[^30]

Kerana terdapat sifar contoh tumbuhan mengawal syarikat fortune 500, akan terdapat tepat sifar contoh orang mengawal superintelligences. Ini menghampiri fakta matematik.[^31] Jika superintelligence dibina – tanpa mengira bagaimana kita sampai ke sana – persoalannya bukan sama ada manusia boleh mengawalnya, tetapi sama ada kita akan terus wujud, dan jika ya, sama ada kita akan mempunyai kewujudan yang baik dan bermakna sebagai individu atau sebagai spesies. Ke atas soalan eksistensial ini untuk manusia kita akan mempunyai sedikit pembelian. Era manusia akan berakhir.

## Kesimpulan: kita tidak boleh membina KBA

Terdapat senario di mana membina KBA mungkin berjalan baik untuk manusia: ia dibina dengan berhati-hati, di bawah kawalan dan untuk manfaat manusia, ditadbir oleh persetujuan bersama banyak pemegang kepentingan,[^32] dan dihalang daripada berkembang ke superintelligence yang tidak boleh dikawal.

*Senario itu tidak terbuka kepada kita dalam keadaan semasa.* Seperti yang dibincangkan dalam bahagian ini, dengan kebarangkalian yang sangat tinggi, pembangunan KBA akan membawa kepada beberapa gabungan:

- Gangguan atau kemusnahan masyarakat dan tamadun yang besar;
- Konflik atau perang antara kuasa besar;
- Kehilangan kawalan oleh manusia *terhadap* atau *kepada* sistem AI yang berkuasa;
- Lari kepada superintelligence yang tidak boleh dikawal, dan ketidakrelevanan atau penghentian spesies manusia.

Seperti yang digambarkan fiksyen awal KBA: satu-satunya cara untuk menang adalah tidak bermain.

[^1]: [Akta AI EU](https://artificialintelligenceact.eu/) adalah satu perundangan yang signifikan tetapi tidak akan menghalang sistem AI berbahaya daripada dibangunkan atau digunakan secara langsung, atau bahkan dikeluarkan secara terbuka, terutamanya di AS. Satu lagi dasar yang signifikan, perintah Eksekutif AS mengenai AI, telah dibatalkan.

[^2]: [Tinjauan Gallup](https://news.gallup.com/poll/1597/confidence-institutions.aspx) ini menunjukkan penurunan suram dalam kepercayaan terhadap institusi awam sejak 2000 di AS. Nombor Eropah adalah pelbagai dan kurang ekstrem, tetapi juga dalam aliran menurun. Ketidakpercayaan tidak bermaksud institusi benar-benar *tidak* berfungsi, tetapi ia adalah petunjuk serta punca.

[^3]: Dan gangguan besar yang kini kita sokong – seperti perluasan hak kepada kumpulan baharu – secara khusus didorong oleh orang dalam arah untuk menjadikan perkara lebih baik.

[^4]: Biar saya terang-terangan. Jika kerja anda boleh dilakukan dari belakang komputer, dengan interaksi secara peribadi yang agak sedikit dengan orang di luar organisasi anda, dan tidak melibatkan tanggungjawab undang-undang kepada pihak luar, ia akan mengikut definisi mungkin (dan berkemungkinan menjimatkan kos) untuk menukar anda sepenuhnya dengan sistem digital. Robotik untuk menggantikan kebanyakan buruh fizikal akan datang kemudian – tetapi tidak begitu lama kemudian sebaik sahaja KBA mula mereka bentuk robot.

[^5]: Sebagai contoh, apa yang berlaku kepada sistem kehakiman kita jika tuntutan mahkamah hampir percuma untuk difailkan? Apa yang berlaku apabila memintas sistem keselamatan melalui kejuruteraan sosial menjadi murah, mudah, dan bebas risiko?

[^6]: [Artikel ini](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/) mendakwa bahawa 10% daripada semua kandungan internet sudah dijana AI, dan merupakan hit teratas Google (untuk saya) kepada pertanyaan carian "anggaran berapa pecahan kandungan internet baharu yang dijana AI." Adakah ia benar? Saya tidak tahu! Ia tidak memetik rujukan dan ia tidak ditulis oleh seseorang. Berapa pecahan imej baharu yang diindeks oleh Google, atau Tweets, atau komen di Reddit, atau video Youtube dijana oleh manusia? Tiada siapa tahu – saya tidak fikir ia adalah nombor yang boleh diketahui. Dan ini kurang daripada *dua tahun* ke dalam kemunculan AI generatif.

[^7]: Juga patut ditambah adalah bahawa terdapat risiko "moral" bahawa kita mungkin mencipta makhluk digital yang boleh menderita. Kerana kita pada masa ini tidak mempunyai teori kesedaran yang boleh dipercayai yang akan membolehkan kita membezakan sistem fizikal yang boleh dan tidak boleh menderita, kita tidak boleh menolak ini secara teori. Selain itu, laporan sistem AI tentang sentience mereka berkemungkinan tidak boleh dipercayai berkenaan dengan pengalaman sebenar (atau bukan pengalaman) sentience mereka.

[^8]: Penyelesaian teknikal dalam bidang "penjajaran" AI ini tidak mungkin dapat menjalankan tugas juga. Dalam sistem semasa mereka berfungsi pada tahap tertentu, tetapi cetek dan secara amnya boleh dielakkan tanpa usaha yang signifikan; dan seperti yang dibincangkan di bawah kita tidak mempunyai idea sebenar bagaimana untuk melakukan ini untuk sistem yang jauh lebih maju.

[^9]: Sistem AI sedemikian mungkin datang dengan beberapa perlindungan terbina dalam. Tetapi untuk mana-mana model dengan apa-apa seperti seni bina semasa, jika akses penuh kepada beratnya tersedia, langkah keselamatan boleh dilucutkan melalui latihan tambahan atau teknik lain. Jadi ia hampir dijamin bahawa untuk setiap sistem dengan pagar rel akan ada juga sistem yang tersedia secara meluas tanpanya. Sesungguhnya model Llama 3.1 405B Meta dikeluarkan secara terbuka dengan perlindungan. Tetapi *sebelum itu* model "asas", tanpa perlindungan, telah bocor.

[^10]: Bolehkah pasaran menguruskan risiko ini tanpa penglibatan kerajaan? Ringkasnya, tidak. Pasti ada risiko yang syarikat sangat diberi insentif untuk mengurangkan. Tetapi banyak lain syarikat boleh dan mengeksternalisasi kepada semua orang lain, dan banyak daripada yang di atas dalam kelas ini: tiada insentif pasaran semula jadi untuk mencegah pengawasan massa, kerosakan kebenaran, penumpuan kuasa, gangguan buruh, wacana politik yang merosakkan, dll. Sesungguhnya kita telah melihat semua ini daripada teknologi hari ini, terutamanya media sosial, yang telah pergi pada dasarnya tidak dikawal. AI hanya akan meningkatkan banyak dinamik yang sama dengan sangat besar.

[^11]: OpenAI berkemungkinan mempunyai model yang lebih patuh untuk kegunaan dalaman. Tidak mungkin OpenAI telah membina beberapa jenis "pintu belakang" supaya ChatGPT boleh dikawal dengan lebih baik oleh OpenAI sendiri, kerana ini akan menjadi amalan keselamatan yang teruk, dan sangat boleh dieksploitasi memandangkan kelegapan dan ketidakbolehramalan AI.

[^12]: Juga kepentingan penting: penjajaran atau mana-mana ciri keselamatan lain hanya penting jika mereka benar-benar digunakan dalam sistem AI. Sistem yang dikeluarkan secara terbuka (iaitu di mana berat dan seni bina model tersedia secara umum) boleh diubah dengan agak mudah menjadi sistem *tanpa* langkah keselamatan tersebut. Mengeluarkan sistem KBA yang lebih pintar daripada manusia secara terbuka akan menjadi sangat melulu, dan sukar untuk membayangkan bagaimana kawalan manusia atau bahkan relevan akan dikekalkan dalam senario sedemikian. Akan ada setiap motivasi, sebagai contoh, untuk melepaskan agen AI yang berkuasa mereproduksi diri dan mengekalkan diri dengan matlamat untuk membuat wang dan menghantarnya ke beberapa dompet mata wang kripto. Atau untuk memenangi pilihan raya. Atau menggulingkan kerajaan. Bolehkah AI "baik" membantu mengandungi ini? Mungkin – tetapi hanya dengan mewakilkan kuasa besar kepadanya, membawa kepada kehilangan kawalan seperti yang diterangkan di bawah.

[^13]: Untuk eksposisi panjang buku mengenai masalah lihat cth. *Superintelligence*, *The Alignment Problem*, dan *Human-Compatible*. Untuk timbunan besar kerja pada pelbagai tahap teknikal oleh mereka yang telah bekerja keras selama bertahun-tahun memikirkan masalah, anda boleh melawat [forum penjajaran AI](https://www.alignmentforum.org/). Berikut adalah [pandangan terkini](https://alignment.anthropic.com/2025/recommended-directions/) daripada pasukan penjajaran Anthropic tentang apa yang mereka anggap tidak diselesaikan.

[^14]: Ini adalah senario ["AI pemberontak"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/). Pada prinsipnya risiko boleh menjadi agak kecil jika sistem masih boleh dikawal dengan menutupnya; tetapi senario juga boleh termasuk penipuan AI, eksfiltrasi dan pembiakan diri, pengagregatan kuasa, dan langkah lain yang akan menjadikannya sukar atau mustahil untuk berbuat demikian.

[^15]: Terdapat literatur yang sangat kaya mengenai topik ini, kembali kepada tulisan formatif oleh [Steve Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), Nick Bostrom, dan Eliezer Yudkowsky. Untuk eksposisi panjang buku lihat [Human Compatible](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) oleh Stuart Russell; [di sini](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/) adalah primer pendek dan terkini.

[^16]: Menyedari ini, daripada perlahan untuk mendapat pemahaman yang lebih baik, syarikat KBA telah datang dengan rancangan yang berbeza: mereka akan mendapat AI untuk melakukannya! Lebih khusus lagi, mereka akan mempunyai AI *N* membantu mereka memikirkan bagaimana untuk menjajarkan AI *N+1*, sepanjang jalan ke superintelligence. Walaupun memanfaatkan AI untuk membantu kita menjajarkan AI kedengaran menjanjikan, terdapat hujah kuat bahawa ia hanya mengandaikan kesimpulannya sebagai premis, dan secara umum adalah pendekatan yang sangat berisiko. Lihat [di sini](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us) untuk beberapa perbincangan. "Rancangan" ini bukan satu, dan telah menjalani apa-apa seperti penelitian yang sesuai dengan strategi teras bagaimana untuk menjadikan AI super-manusia berjalan baik untuk manusia.

[^17]: Lagipun, manusia, cacat dan degil seperti kita, telah membangunkan sistem etika di mana kita melayan sekurang-kurangnya beberapa spesies lain di Bumi dengan baik. (Hanya jangan fikir tentang ladang kilang tersebut.)

[^18]: Terdapat, untungnya, jalan keluar di sini: jika peserta datang untuk memahami bahawa mereka terlibat dalam perlumbaan bunuh diri bukannya satu yang boleh dimenangi. Ini adalah apa yang berlaku berhampiran akhir perang dingin, apabila AS dan USSR datang untuk menyedari bahawa disebabkan musim sejuk nuklear, walaupun serangan nuklear yang *tidak dijawab* akan menjadi bencana untuk penyerang. Dengan kesedaran bahawa "perang nuklear tidak boleh dimenangi dan tidak boleh pernah diperjuangkan" datang perjanjian signifikan mengenai pengurangan senjata – pada dasarnya berakhirnya perlumbaan senjata.

[^19]: Perang, secara eksplisit atau tersirat.

[^20]: Peningkatan, kemudian perang.

[^21]: Pemikiran ajaib.

[^22]: Saya juga ada jambatan quadrillion dolar untuk dijual kepada anda.

[^23]: Agen sedemikian mungkin lebih suka "memperoleh," dengan kemusnahan sebagai sandaran; tetapi mengamankan model terhadap kedua-dua kemusnahan *dan* kecurian oleh negara yang berkuasa adalah sukar untuk dikatakan sekurang-kurangnya, terutamanya untuk entiti swasta.

[^24]: Untuk perspektif lain mengenai risiko keselamatan negara KBA, lihat [laporan RAND ini.](https://www.rand.org/pubs/perspectives/PEA3691-4.html)

[^25]: Mungkin kita boleh membina institusi sedemikian! Terdapat cadangan untuk "CERN untuk AI" dan inisiatif serupa lain, di mana pembangunan KBA berada di bawah kawalan global pelbagai hala. Tetapi pada masa ini tiada institusi sedemikian wujud atau di kaki langit.

[^26]: Dan walaupun penjajaran sangat sukar, mendapatkan orang untuk berkelakuan adalah lebih sukar lagi!

[^27]: Bayangkan sistem yang boleh bercakap 50 bahasa, mempunyai kepakaran dalam semua mata pelajaran akademik, membaca buku penuh dalam saat dan mempunyai semua bahan segera dalam fikiran, dan menghasilkan output pada sepuluh kali kelajuan manusia. Sebenarnya, anda tidak perlu membayangkannya: hanya muatkan sistem AI semasa. Ini adalah super-manusia dalam banyak cara, dan tiada apa yang menghentikan mereka daripada menjadi lebih super-manusia dalam yang dan banyak lagi.

[^28]: Inilah mengapa ini telah disebut "singulariti" teknologi, meminjam daripada fizik idea bahawa seseorang tidak boleh membuat ramalan melepasi singulariti. Penyokong bersandar *ke dalam* singulariti sedemikian mungkin juga ingin merefleksikan bahawa dalam fizik jenis singulariti yang sama ini mengoyakkan dan menghancurkan mereka yang masuk ke dalamnya.

[^29]: Masalah telah digariskan secara komprehensif dalam [*Superintelligence*](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834) Bostrom, dan tiada apa sejak itu telah mengubah mesej teras secara signifikan. Untuk jilid yang lebih terkini mengumpulkan hasil formal dan matematik mengenai ketidakbolehkawalan lihat [AI: Unexplainable, Unpredictable, Uncontrollable](https://www.amazon.com/Unexplainable-Unpredictable-Uncontrollable-Artificial-Intelligence/dp/103257626X) Yampolskiy

[^30]: Ini juga menjelaskan mengapa strategi semasa syarikat AI (secara berulang membiarkan AI "menjajarkan" AI yang paling berkuasa seterusnya) tidak boleh berfungsi. Andaikan paku pakis, melalui kesedapan daun-daunnya, mendaftarkan murid darjah pertama untuk menjaganya. Murid darjah pertama menulis beberapa arahan terperinci untuk murid darjah 2 untuk diikuti, dan nota meyakinkan mereka untuk berbuat demikian. Murid darjah 2 melakukan yang sama untuk murid darjah 3, dan seterusnya sepanjang jalan ke graduan kolej, pengurus, eksekutif, dan akhirnya CEO GM. Adakah GM kemudian "melakukan apa yang paku pakis mahu"? Pada setiap langkah ini mungkin terasa seperti ia berfungsi. Tetapi meletakkannya semua bersama-sama, ia akan berfungsi hampir tepat pada tahap di mana CEO, Lembaga Pengarah, dan pemegang saham GM kebetulan mengambil berat tentang kanak-kanak dan paku pakis, dan mempunyai sedikit atau tiada kaitan dengan semua nota dan set arahan tersebut.

[^31]: Wataknya tidak begitu berbeza daripada hasil formal seperti teorem ketidaklengkapan Gödel atau hujah berhenti Turing kerana tanggapan kawalan secara asasnya bercanggah dengan premis: bagaimana anda boleh mengawal dengan bermakna sesuatu yang anda tidak dapat fahami atau ramalkan; namun jika anda boleh memahami dan meramalkan superintelligence anda akan menjadi superintelligent. Alasan saya katakan "menghampiri" adalah bahawa hasil formal tidak setepat atau diperiksa seperti dalam kes matematik tulen, dan kerana saya ingin berharap bahawa beberapa kecerdasan am yang dibina dengan sangat berhati-hati, menggunakan kaedah yang sama sekali berbeza daripada yang digunakan pada masa ini, boleh mempunyai beberapa sifat keselamatan yang boleh dibuktikan secara matematik, mengikut jenis program AI "terjamin selamat" yang dibincangkan di bawah.

[^32]: Pada masa ini, kebanyakan pemegang kepentingan – iaitu, hampir semua manusia – dipinggirkan dalam perbincangan ini. Itu sangat salah, dan jika tidak dijemput masuk, banyak, banyak kumpulan lain akan terjejas oleh pembangunan KBA harus menuntut untuk dibenarkan masuk.