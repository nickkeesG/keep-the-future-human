# Bab 8 - Cara untuk tidak membina KBA

KBA bukanlah perkara yang tidak dapat dielakkan – hari ini kita berdiri di persimpangan jalan. Bab ini mengemukakan cadangan tentang bagaimana kita boleh menghalangnya daripada dibina.

Jika jalan yang sedang kita lalui membawa kepada kemungkinan berakhirnya tamadun kita, bagaimanakah kita menukar laluan?

Andaikan keinginan untuk menghentikan pembangunan KBA dan superintelligence meluas dan berkuasa,[^1] kerana telah menjadi pemahaman umum bahawa KBA akan menyerap kuasa dan bukannya memberikan kuasa, serta merupakan bahaya besar kepada masyarakat dan manusia. Bagaimanakah kita menutup Pintu Gerbang?

Pada masa ini kita hanya mengetahui satu cara untuk *mencipta* AI yang berkuasa dan am, iaitu melalui pengkomputeran yang benar-benar besar-besaran bagi rangkaian neural dalam. Kerana perkara ini amat sukar dan mahal untuk dilakukan, ada maksud bahawa *tidak* melakukannya adalah mudah.[^2] Tetapi kita telah melihat kuasa-kuasa yang mendorong ke arah KBA, dan dinamik teori permainan yang menjadikannya amat sukar bagi mana-mana pihak untuk berhenti secara unilateral. Jadi ia memerlukan gabungan campur tangan dari luar (iaitu kerajaan) untuk menghentikan syarikat, dan perjanjian antara kerajaan untuk menghentikan diri mereka sendiri.[^3] Bagaimanakah rupa perkara ini?

Adalah berguna untuk membezakan terlebih dahulu antara pembangunan AI yang mesti *dicegah* atau *dilarang*, dan yang mesti *diuruskan.* Yang pertama terutamanya ialah larian ke arah superintelligence.[^4] Untuk pembangunan yang dilarang, definisi hendaklah setajam mungkin, dan kedua-dua pengesahan dan penguatkuasaan hendaklah praktikal. Apa yang mesti *diuruskan* ialah sistem AI am dan berkuasa – yang sudah kita miliki, dan yang akan mempunyai banyak kawasan kelabu, nuansa, dan kerumitan. Bagi ini, institusi yang kuat dan berkesan adalah penting.

Kita juga boleh menggariskan dengan berguna isu-isu yang mesti ditangani di peringkat antarabangsa (termasuk antara saingan atau musuh geopolitik) [^5] daripada yang boleh diuruskan oleh bidang kuasa individu, negara, atau kumpulan negara. Pembangunan yang dilarang sebahagian besarnya termasuk dalam kategori "antarabangsa", kerana larangan tempatan terhadap pembangunan sesuatu teknologi biasanya boleh dielakkan dengan menukar lokasi.[^6]

Akhirnya, kita boleh mempertimbangkan alatan dalam kotak alat. Terdapat banyak, termasuk alatan teknikal, undang-undang lembut (piawaian, norma, dll., undang-undang keras (peraturan dan keperluan), liabiliti, insentif pasaran, dan sebagainya. Mari kita beri perhatian khusus kepada satu yang khusus untuk AI.

## Keselamatan dan tadbir urus pengkomputeran

Alatan teras dalam mentadbir AI berkuasa tinggi ialah perkakasan yang diperlukan. Perisian mudah tersebar, mempunyai kos pengeluaran marginal yang hampir sifar, melintasi sempadan dengan mudah, dan boleh diubah suai serta-merta; tiada perkara ini benar untuk perkakasan. Namun seperti yang telah kita bincangkan, jumlah besar "pengkomputeran" ini diperlukan semasa latihan sistem AI dan semasa inferens untuk mencapai sistem yang paling berkebolehan. Pengkomputeran boleh dikira, diambil kira, dan diaudit dengan mudah, dengan kekaburan yang agak sedikit setelah peraturan yang baik untuk melakukannya dibangunkan. Yang paling penting, jumlah pengkomputeran yang besar adalah, seperti uranium yang diperkaya, sumber yang sangat terhad, mahal dan sukar dihasilkan. Walaupun cip komputer terdapat di mana-mana, perkakasan yang diperlukan untuk AI adalah mahal dan amat sukar dikilangkan.[^7]

Apa yang menjadikan cip khusus AI jauh *lebih* mudah diurus sebagai sumber terhad daripada uranium ialah ia boleh memasukkan mekanisme keselamatan berasaskan perkakasan. Kebanyakan telefon bimbit moden, dan sesetengah komputer riba, mempunyai ciri perkakasan khusus atas cip yang membolehkan mereka memastikan bahawa mereka memasang hanya perisian dan kemas kini sistem pengendalian yang diluluskan, bahawa mereka mengekalkan dan melindungi data biometrik sensitif pada peranti, dan bahawa mereka boleh dijadikan tidak berguna kepada sesiapa selain pemiliknya jika hilang atau dicuri. Sepanjang beberapa tahun kebelakangan ini langkah keselamatan perkakasan sedemikian telah menjadi mantap dan diterima pakai secara meluas, dan secara amnya terbukti agak selamat.

Kebaharuan utama ciri-ciri ini ialah ia mengikat perkakasan dan perisian bersama menggunakan kriptografi.[^8] Iaitu, hanya mempunyai sekeping perkakasan komputer tertentu tidak bermakna pengguna boleh melakukan apa sahaja yang mereka mahu dengannya dengan menggunakan perisian yang berbeza. Dan ikatan ini juga memberikan keselamatan yang berkuasa kerana banyak serangan memerlukan pelanggaran keselamatan *perkakasan* dan bukannya hanya keselamatan *perisian*.

Beberapa laporan terkini (contohnya daripada [GovAI dan rakan kerjasama](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips), dan [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) telah menunjukkan bahawa ciri perkakasan serupa yang terbina dalam perkakasan pengkomputeran terdepan yang berkaitan AI boleh memainkan peranan yang amat berguna dalam keselamatan dan tadbir urus AI. Ia membolehkan beberapa fungsi yang tersedia kepada "pentadbir" [^9] yang mungkin tidak diduga atau bahkan mungkin. Sebagai beberapa contoh utama:

- *Geolokasi*: Sistem boleh disediakan supaya cip mempunyai lokasi yang diketahui, dan boleh bertindak secara berbeza (atau dimatikan sama sekali) berdasarkan lokasi.[^10]
- *Sambungan senarai yang dibenarkan*: setiap cip boleh dikonfigurasikan dengan senarai cip tertentu yang dikuatkuasakan perkakasan yang boleh disambungkan dengannya, dan tidak dapat disambung dengan mana-mana cip yang tidak dalam senarai ini.[^11] Ini boleh mengehadkan saiz kelompok cip komunikatif.[^12]
- *Inferens atau latihan bermeter (dan suis auto-mati)*: Pentadbir boleh melesenkan hanya sejumlah latihan atau inferens tertentu (dalam masa, atau FLOP, atau mungkin token) untuk dilakukan oleh pengguna, selepas itu kebenaran baru diperlukan. Jika kenaikannya kecil, maka pelesenan semula model yang agak berterusan diperlukan. Model kemudiannya boleh "dimatikan" hanya dengan menahan isyarat lesen ini.[^13]
- *Had laju*: Model dihalang daripada berjalan pada kelajuan inferens yang lebih tinggi daripada had tertentu yang ditentukan oleh pentadbir atau sebaliknya. Ini boleh dilaksanakan melalui set sambungan senarai yang dibenarkan yang terhad, atau dengan cara yang lebih canggih.
- *Latihan yang dibuktikan*: Prosedur latihan boleh menghasilkan bukti selamat secara kriptografi bahawa set kod, data, dan jumlah penggunaan pengkomputeran tertentu telah digunakan dalam penjanaan model.

## Cara untuk tidak membina superintelligence: had global pada latihan dan pengkomputeran inferens

Dengan pertimbangan ini – terutama berkenaan pengkomputeran – di tempat, kita boleh membincangkan cara menutup Pintu Gerbang kepada superintelligence buatan; kemudian kita akan beralih kepada mencegah KBA penuh, dan menguruskan model AI apabila ia menghampiri dan melebihi keupayaan manusia dalam aspek yang berbeza.

Ramuan pertama adalah, sudah tentu, pemahaman bahawa superintelligence tidak akan dapat dikawal, dan bahawa akibatnya pada asasnya tidak dapat diramalkan. Sekurang-kurangnya China dan AS mesti memutuskan secara bebas, untuk tujuan ini atau yang lain, untuk tidak membina superintelligence.[^14] Kemudian perjanjian antarabangsa antara mereka dan yang lain, dengan mekanisme pengesahan dan penguatkuasaan yang kuat, diperlukan untuk memberi jaminan kepada semua pihak bahawa saingan mereka tidak membelot dan memutuskan untuk mengambil risiko.

Untuk boleh disahkan dan dikuatkuasakan, had hendaklah had keras, dan setidak-tidaknya mungkin. Ini kelihatan seperti masalah yang hampir mustahil: mengehadkan keupayaan perisian yang kompleks dengan sifat yang tidak dapat diramalkan, di seluruh dunia. Nasib baik keadaannya jauh lebih baik daripada ini, kerana perkara yang telah menjadikan AI canggih mungkin – jumlah pengkomputeran yang besar – adalah jauh, jauh lebih mudah dikawal. Walaupun ia mungkin masih membenarkan beberapa sistem yang berkuasa dan berbahaya, *larian superintelligence* mungkin boleh dicegah dengan had keras pada jumlah pengkomputeran yang masuk ke dalam rangkaian neural, bersama-sama dengan had kadar pada jumlah inferens yang boleh dilakukan oleh sistem AI (rangkaian neural yang disambungkan dan perisian lain). Versi khusus ini dicadangkan di bawah.

Mungkin kelihatan bahawa meletakkan had global keras pada pengkomputeran AI memerlukan tahap koordinasi antarabangsa yang besar dan pengawasan yang mengganggu dan menghancurkan privasi. Nasib baik, ia tidak akan. [Rantaian bekalan yang sangat ketat dan tersumbat](https://arxiv.org/abs/2402.08797) memberikan bahawa setelah had ditetapkan secara sah (sama ada oleh undang-undang atau perintah eksekutif), pengesahan pematuhan terhadap had itu hanya memerlukan penglibatan dan kerjasama beberapa syarikat besar sahaja.[^15]

Pelan seperti ini mempunyai beberapa ciri yang sangat diingini. Ia adalah invasif secara minimum dalam erti kata bahawa hanya beberapa syarikat utama mempunyai keperluan yang diletakkan ke atas mereka, dan hanya kelompok pengkomputeran yang agak besar akan ditadbir. Cip berkaitan sudah mengandungi keupayaan perkakasan yang diperlukan untuk versi pertama.[^16] Kedua-dua pelaksanaan dan penguatkuasaan bergantung pada sekatan undang-undang standard. Tetapi ini disokong oleh terma penggunaan perkakasan dan oleh kawalan perkakasan, memudahkan penguatkuasaan dan menghalang penipuan oleh syarikat, kumpulan swasta, atau bahkan negara. Terdapat duluan yang cukup untuk syarikat perkakasan meletakkan sekatan jauh pada penggunaan perkakasan mereka, dan mengunci/membuka keupayaan tertentu secara luaran,[^17] termasuk bahkan dalam CPU berkuasa tinggi di pusat data.[^18] Malah untuk pecahan yang agak kecil daripada perkakasan dan organisasi yang terjejas, pengawasan boleh terhad kepada telemetri, tanpa akses terus kepada data atau model itu sendiri; dan perisian untuk ini boleh terbuka untuk pemeriksaan untuk menunjukkan bahawa tiada data tambahan sedang direkodkan. Skema ini adalah antarabangsa dan koperatif, dan agak fleksibel dan boleh dilanjutkan. Kerana had terutamanya adalah pada perkakasan dan bukannya perisian, ia agak agnostik terhadap bagaimana pembangunan dan penggunaan perisian AI berlaku, dan serasi dengan pelbagai paradigma termasuk AI yang lebih "terdesentralisasi" atau "awam" yang bertujuan memerangi penumpuan kuasa yang didorong AI.

Penutupan Pintu Gerbang berasaskan pengkomputeran mempunyai kelemahan juga. Pertama, ia jauh daripada penyelesaian penuh kepada masalah tadbir urus AI secara umum. Kedua, apabila perkakasan komputer menjadi lebih pantas, sistem akan "menangkap" lebih banyak perkakasan dalam kelompok yang semakin kecil (atau bahkan GPU individu).[^19] Ia juga mungkin bahawa disebabkan penambahbaikan algoritma, had pengkomputeran yang lebih rendah akan diperlukan pada masanya,[^20] atau bahawa jumlah pengkomputeran menjadi sebahagian besarnya tidak relevan dan menutup Pintu Gerbang akan memerlukan rejim tadbir urus berasaskan risiko atau keupayaan yang lebih terperinci untuk AI. Ketiga, tidak kira jaminan dan bilangan kecil entiti yang terjejas, sistem sedemikian pasti akan menimbulkan penolakan berkenaan privasi dan pengawasan, antara kebimbangan lain.[^21]

Sudah tentu, membangunkan dan melaksanakan skim tadbir urus pengehadan pengkomputeran dalam tempoh masa yang singkat akan agak mencabar. Tetapi ia benar-benar boleh dilakukan.

## A-G-I: Persilangan tiga sebagai asas risiko, dan dasar

Sekarang mari kita beralih kepada KBA. Garis keras dan definisi di sini lebih sukar, kerana kita pasti mempunyai kecerdasan yang buatan dan am, dan dengan tiada definisi sedia ada semua orang akan bersetuju jika atau bila ia wujud. Selain itu, had pengkomputeran atau inferens adalah alatan yang agak tumpul (pengkomputeran sebagai proksi untuk keupayaan, yang kemudiannya proksi untuk risiko) yang – melainkan ia agak rendah – tidak mungkin menghalang KBA yang cukup berkuasa untuk menyebabkan gangguan sosial atau tamadun atau risiko akut.

Saya telah berhujah bahawa risiko paling akut muncul daripada persilangan tiga keupayaan yang sangat tinggi, autonomi tinggi, dan keamanan yang hebat. Ini adalah sistem yang – jika ia dibangunkan sama sekali – mesti diuruskan dengan berhati-hati. Dengan mewujudkan piawaian yang ketat (melalui liabiliti dan peraturan) untuk sistem yang menggabungkan ketiga-tiga sifat, kita boleh menyalurkan pembangunan AI ke arah alternatif yang lebih selamat.

Seperti industri dan produk lain yang berpotensi membahayakan pengguna atau orang ramai, sistem AI memerlukan peraturan yang teliti oleh agensi kerajaan yang berkesan dan diberi kuasa. Peraturan ini harus mengiktiraf risiko yang wujud dalam KBA, dan menghalang sistem AI berkuasa tinggi yang berisiko tidak boleh diterima daripada dibangunkan.[^22]

Walau bagaimanapun, peraturan berskala besar, terutama dengan gigi sebenar yang pasti akan ditentang oleh industri,[^23] mengambil masa [^24] serta keyakinan politik bahawa ia perlu.[^25] Memandangkan kadar kemajuan, ini mungkin mengambil lebih banyak masa daripada yang kita ada.

Dalam skala masa yang lebih cepat dan semasa langkah kawal selia sedang dibangunkan, kita boleh memberikan syarikat insentif yang diperlukan untuk (a) berhenti daripada aktiviti berisiko sangat tinggi dan (b) membangunkan sistem komprehensif untuk menilai dan mengurangkan risiko, dengan menjelaskan dan meningkatkan tahap liabiliti untuk sistem paling berbahaya. Ideanya adalah untuk mengenakan tahap liabiliti tertinggi – ketat dan dalam beberapa kes jenayah peribadi – untuk sistem dalam persilangan tiga autonomi-keamanan-kecerdasan tinggi, tetapi untuk menyediakan "pelabuhan selamat" kepada liabiliti berasaskan kesalahan yang lebih biasa untuk sistem di mana salah satu daripada sifat tersebut kurang atau dijamin boleh diurus. Iaitu, sebagai contoh, sistem "lemah" yang am dan autonomi (seperti pembantu peribadi yang berkebolehan dan boleh dipercayai tetapi terhad) akan tertakluk kepada tahap liabiliti yang lebih rendah. Begitu juga sistem sempit dan autonomi seperti kereta pandu sendiri masih akan tertakluk kepada peraturan penting yang sudah ada, tetapi bukan liabiliti yang dipertingkat. Begitu juga untuk sistem yang sangat berkebolehan dan am yang "pasif" dan sebahagian besarnya tidak berupaya tindakan bebas. Sistem yang kekurangan *dua* daripada tiga sifat adalah lebih mudah diurus dan pelabuhan selamat akan lebih mudah dituntut. Pendekatan ini mencerminkan bagaimana kita mengendalikan teknologi berpotensi berbahaya lain:[^26] liabiliti yang lebih tinggi untuk konfigurasi yang lebih berbahaya mewujudkan insentif semula jadi untuk alternatif yang lebih selamat.

Hasil lalai tahap liabiliti tinggi sedemikian, yang bertindak untuk *menginternalisasi* risiko KBA kepada syarikat dan bukannya memindahkannya kepada orang ramai, berkemungkinan (dan diharapkan!) untuk syarikat tidak membangunkan KBA penuh sehingga dan melainkan mereka benar-benar boleh menjadikannya boleh dipercayai, selamat, dan terkawal memandangkan *kepimpinan mereka sendiri* adalah pihak yang berisiko. (Sekiranya ini tidak mencukupi, undang-undang yang menjelaskan liabiliti juga harus secara eksplisit membenarkan bantuan injunktif, iaitu hakim memerintahkan penghentian, untuk aktiviti yang jelas dalam zon bahaya dan boleh dikatakan menimbulkan risiko awam.) Apabila peraturan ditetapkan, mematuhi peraturan boleh menjadi pelabuhan selamat, dan pelabuhan selamat daripada autonomi rendah, kesempitan, atau kelemahan sistem AI boleh ditukar kepada rejim kawal selia yang agak ringan.

## Peruntukan utama penutupan Pintu Gerbang

Dengan perbincangan di atas dalam fikiran, bahagian ini memberikan cadangan untuk peruntukan utama yang akan melaksanakan dan mengekalkan larangan ke atas KBA penuh dan superintelligence, dan pengurusan AI kompetitif manusia atau kompetitif pakar tujuan umum berhampiran ambang KBA penuh.[^27] Ia mempunyai empat bahagian utama: 1) perakaunan dan pengawasan pengkomputeran, 2) had pengkomputeran dalam latihan dan operasi AI, 3) rangka kerja liabiliti, dan 4) piawaian keselamatan dan keselamatan berperingkat yang ditakrifkan yang memasukkan keperluan kawal selia keras. Ini diterangkan secara ringkas seterusnya, dengan butiran lanjut atau contoh pelaksanaan diberikan dalam tiga jadual yang mengiringi. Yang penting, perhatikan bahawa ini jauh daripada semua yang perlu untuk mentadbir sistem AI canggih; walaupun ia akan mempunyai faedah keselamatan tambahan, ia bertujuan untuk menutup Pintu Gerbang kepada larian kecerdasan, dan mengarahkan semula pembangunan AI ke arah yang lebih baik.

### 1\. Perakaunan pengkomputeran, dan ketelusan

- Organisasi piawaian (contohnya NIST di AS diikuti oleh ISO/IEEE di peringkat antarabangsa) harus mengkodifikasikan piawaian teknikal terperinci untuk jumlah pengkomputeran yang digunakan dalam melatih dan mengendalikan model AI, dalam FLOP, dan kelajuan dalam FLOP/s di mana ia beroperasi. Butiran untuk bagaimana ini boleh kelihatan diberikan dalam Lampiran A.[^28]
- Keperluan – sama ada oleh undang-undang baru atau di bawah kuasa sedia ada [^29] – harus dikenakan oleh bidang kuasa di mana latihan AI berskala besar berlaku untuk mengira dan melaporkan kepada badan kawal selia atau agensi lain jumlah FLOP yang digunakan dalam melatih dan mengendalikan semua model di atas ambang 10 <sup>25</sup> FLOP atau 10 <sup>18</sup> FLOP/s.[^30]
- Keperluan ini harus diperkenalkan secara berperingkat, pada mulanya memerlukan anggaran niat baik yang didokumenkan dengan baik setiap suku tahun, dengan fasa kemudian memerlukan piawaian yang semakin tinggi, sehingga ke jumlah FLOP dan FLOP/s yang dibuktikan secara kriptografi yang dilampirkan pada setiap *output* model.
- Laporan ini harus dilengkapi dengan anggaran yang didokumenkan dengan baik tentang kos tenaga dan kewangan marginal yang digunakan dalam menghasilkan setiap output AI.

Rasional: Nombor-nombor yang dikira dengan baik dan dilaporkan secara telus ini akan memberikan asas untuk had latihan dan operasi, serta pelabuhan selamat daripada langkah liabiliti yang lebih tinggi (lihat Lampiran C dan D).

### 2\. Had pengkomputeran latihan dan operasi

- Bidang kuasa yang menghos sistem AI harus mengenakan had keras pada jumlah pengkomputeran yang masuk ke dalam mana-mana output model AI, bermula pada 10 <sup>27</sup> FLOP [^31] dan boleh diselaraskan mengikut kesesuaian.
- Bidang kuasa yang menghos sistem AI harus mengenakan had keras pada kadar pengkomputeran output model AI, bermula pada 10 <sup>20</sup> FLOP/s dan boleh diselaraskan mengikut kesesuaian.

Rasional: Jumlah pengkomputeran, walaupun sangat tidak sempurna, adalah proksi untuk keupayaan AI (dan risiko) yang boleh diukur dan disahkan secara konkrit, jadi memberikan hentian keras untuk mengehadkan keupayaan. Cadangan pelaksanaan konkrit diberikan dalam Lampiran B.

### 3\. Liabiliti dipertingkatkan untuk sistem berbahaya

- Penciptaan dan operasi [^32] sistem AI canggih yang sangat am, berkebolehan, dan autonomi, harus dijelaskan secara sah melalui undang-undang untuk tertakluk kepada liabiliti ketat, bersama-dan-beberapa, dan bukannya berasaskan kesalahan pihak tunggal.[^33]
- Proses undang-undang harus tersedia untuk membuat kes keselamatan afirmatif, yang akan memberikan pelabuhan selamat daripada liabiliti ketat untuk sistem yang kecil (dari segi pengkomputeran), lemah, sempit, pasif, atau yang mempunyai jaminan keselamatan, keselamatan, dan kawalan yang mencukupi.
- Laluan eksplisit dan set syarat untuk bantuan injunktif untuk menghentikan aktiviti latihan dan inferens AI yang membentuk bahaya awam harus digariskan.

Rasional: Sistem AI tidak boleh dipertanggungjawabkan, jadi kita mesti mempertanggungjawabkan individu dan organisasi manusia untuk kemudaratan yang mereka sebabkan (liabiliti).[^34] KBA tidak terkawal adalah ancaman kepada masyarakat dan tamadun dan jika tiada kes keselamatan harus dianggap berbahaya secara tidak normal. Meletakkan beban tanggungjawab kepada pembangun untuk menunjukkan bahawa model berkuasa cukup selamat untuk tidak dianggap "berbahaya secara tidak normal" memberi insentif kepada pembangunan selamat, bersama-sama dengan ketelusan dan penyimpanan rekod untuk menuntut pelabuhan selamat tersebut. Peraturan kemudiannya boleh mencegah kemudaratan di mana pencegahan daripada liabiliti tidak mencukupi. Akhirnya, pembangun AI sudah bertanggungjawab untuk kerosakan yang mereka sebabkan, jadi menjelaskan secara sah liabiliti untuk sistem yang paling berisiko boleh dilakukan serta-merta, tanpa piawaian yang sangat terperinci dibangunkan; ini kemudiannya boleh berkembang dari masa ke masa. Butiran diberikan dalam Lampiran C.

### 4\. Peraturan keselamatan untuk AI

Sistem kawal selia yang menangani risiko akut berskala besar AI akan memerlukan sekurang-kurangnya:

- Pengenalpastian atau penciptaan set badan kawal selia yang sesuai, mungkin agensi baru;
- Rangka kerja penilaian risiko yang komprehensif;[^35]
- Rangka kerja untuk kes keselamatan afirmatif, sebahagiannya berdasarkan rangka kerja penilaian risiko, untuk dibuat oleh pembangun, dan untuk audit oleh kumpulan dan agensi *bebas*;
- Sistem pelesenan berperingkat, dengan peringkat menjejaki tahap keupayaan.[^36] Lesen akan diberikan berdasarkan kes keselamatan dan audit, untuk pembangunan dan penggunaan sistem. Keperluan akan berkisar daripada notifikasi di hujung rendah, kepada jaminan keselamatan, keselamatan, dan kawalan kuantitatif sebelum pembangunan, di hujung atas. Ini akan menghalang pelepasan sistem sehingga ia dibuktikan selamat, dan melarang pembangunan sistem yang secara intrinsik tidak selamat. Lampiran D memberikan cadangan untuk apa yang boleh diperlukan oleh piawaian keselamatan tersebut.
- Perjanjian untuk membawa langkah sedemikian ke peringkat antarabangsa, termasuk badan antarabangsa untuk menyelaraskan norma dan piawaian, dan berpotensi agensi antarabangsa untuk mengkaji kes keselamatan.

Rasional: Akhirnya, liabiliti bukan mekanisme yang tepat untuk mencegah risiko berskala besar kepada orang ramai daripada teknologi baru. Peraturan yang komprehensif, dengan badan kawal selia yang diberi kuasa, akan diperlukan untuk AI sama seperti setiap industri utama lain yang menimbulkan risiko kepada orang ramai.[^37]

Peraturan ke arah mencegah risiko berleluasa lain tetapi kurang akut mungkin berbeza dalam bentuknya dari bidang kuasa ke bidang kuasa. Perkara penting adalah untuk mengelakkan pembangunan sistem AI yang begitu berisiko sehingga risiko ini tidak dapat diurus.

## Apa kemudiannya?

Dalam dekad akan datang, apabila AI menjadi lebih berleluasa dan teknologi teras maju, dua perkara utama berkemungkinan berlaku. Pertama, peraturan sistem AI berkuasa sedia ada akan menjadi lebih sukar, namun lebih perlu. Berkemungkinan bahawa sekurang-kurangnya beberapa langkah yang menangani risiko keselamatan berskala besar akan memerlukan persetujuan di peringkat antarabangsa, dengan bidang kuasa individu menguatkuasakan peraturan berdasarkan perjanjian antarabangsa.

Kedua, had pengkomputeran latihan dan operasi akan menjadi lebih sukar dikekalkan apabila perkakasan menjadi lebih murah dan kos efisien; ia juga mungkin menjadi kurang relevan (atau perlu lebih ketat) dengan kemajuan dalam algoritma dan seni bina.

Bahawa mengawal AI akan menjadi lebih sukar tidak bermakna kita harus menyerah! Melaksanakan pelan yang digariskan dalam esei ini akan memberikan kita masa yang berharga dan kawalan penting ke atas proses yang akan meletakkan kita dalam kedudukan yang jauh, jauh lebih baik untuk mengelakkan risiko eksistensial AI kepada masyarakat, tamadun, dan spesies kita.

Dalam jangka panjang lagi, akan ada pilihan untuk dibuat tentang apa yang kita benarkan. Kita mungkin memilih untuk masih mencipta beberapa bentuk KBA yang benar-benar terkawal, sejauh mana ini terbukti mungkin. Atau kita mungkin memutuskan bahawa menjalankan dunia lebih baik diserahkan kepada mesin, jika kita boleh meyakinkan diri sendiri bahawa mereka akan melakukan kerja yang lebih baik, dan melayan kita dengan baik. Tetapi ini seharusnya menjadi keputusan yang dibuat dengan pemahaman saintifik yang mendalam tentang AI, dan selepas perbincangan global inklusif yang bermakna, bukan dalam perlumbaan antara mogul teknologi dengan sebahagian besar manusia tidak terlibat sepenuhnya dan tidak sedar.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) Ringkasan tadbir urus A-G-I dan superintelligence melalui liabiliti dan peraturan. Liabiliti adalah tertinggi, dan peraturan terkuat, di persilangan tiga Autonomi, Keamanan, dan Kecerdasan. Pelabuhan selamat daripada liabiliti ketat dan peraturan kuat boleh diperoleh melalui kes keselamatan afirmatif yang menunjukkan bahawa sistem adalah lemah dan/atau sempit dan/atau pasif. Had pada jumlah Pengkomputeran Latihan dan kadar Pengkomputeran Inferens, yang disahkan dan dikuatkuasakan secara sah dan menggunakan langkah keselamatan perkakasan dan kriptografi, menyokong keselamatan dengan mengelakkan KBA penuh dan secara berkesan melarang superintelligence.


[^1]: Kemungkinan besar, penyebaran kesedaran ini akan mengambil sama ada usaha intensif oleh kumpulan pendidikan dan advokasi yang membuat kes ini, atau bencana yang disebabkan AI yang agak ketara. Kita boleh berharap ia akan menjadi yang pertama.

[^2]: Secara paradoks, kita biasa dengan Alam mengehadkan teknologi kita dengan menjadikannya sangat sukar untuk dibangunkan, terutama dari segi saintifik. Tetapi itu bukan lagi kes untuk AI: masalah saintifik utama ternyata lebih mudah daripada yang dijangka. Kita tidak boleh bergantung pada Alam menyelamatkan kita daripada diri kita sendiri di sini – kita perlu melakukannya sendiri.

[^3]: Di manakah, tepatnya, kita berhenti dalam membangunkan sistem baru? Di sini, kita harus mengamalkan prinsip berjaga-jaga. Setelah sistem digunakan, dan terutama setelah tahap keupayaan sistem tersebut tersebar, adalah amat sukar untuk berundur. Dan jika sistem *dibangunkan* (terutama dengan kos dan usaha yang besar), akan ada tekanan besar untuk menggunakan atau menggunakannya, dan godaan untuk ia bocor atau dicuri. Membangunkan sistem dan *kemudian* memutuskan sama ada ia sangat tidak selamat adalah jalan yang berbahaya.

[^4]: Ia juga bijak untuk melarang pembangunan AI yang secara intrinsik berbahaya, seperti sistem yang mereplikasi diri dan berkembang, yang direka untuk melarikan diri dari kandang, yang boleh memperbaiki diri secara autonomi, AI yang sengaja menipu dan berniat jahat, dll.

[^5]: Perhatikan ini tidak semestinya bermakna *dikuatkuasakan* di peringkat antarabangsa oleh semacam badan global: sebaliknya negara berdaulat boleh menguatkuasakan peraturan yang dipersetujui, seperti dalam banyak perjanjian.

[^6]: Seperti yang akan kita lihat di bawah, sifat pengkomputeran AI akan membenarkan sesuatu hibrid; tetapi kerjasama antarabangsa masih diperlukan.

[^7]: Sebagai contoh, mesin yang diperlukan untuk mengukir cip berkaitan AI dibuat oleh hanya satu firma, ASML (walaupun banyak percubaan lain untuk berbuat demikian), sebahagian besar cip berkaitan dikilang oleh satu firma, TSMC (walaupun yang lain cuba bersaing), dan reka bentuk dan pembinaan perkakasan daripada cip tersebut dilakukan oleh hanya beberapa termasuk NVIDIA, AMD, dan Google.

[^8]: Yang paling penting, setiap cip memegang kunci peribadi kriptografi unik dan tidak dapat diakses yang boleh digunakannya untuk "menandatangani" perkara.

[^9]: Secara lalai ini akan menjadi syarikat yang menjual cip, tetapi model lain mungkin dan berpotensi berguna.

[^10]: Pentadbir boleh memastikan lokasi cip dengan mengukur masa pertukaran mesej bertandatangan dengannya: kelajuan cahaya yang terhingga memerlukan cip berada dalam radius tertentu *r* daripada "stesen" jika ia boleh mengembalikan mesej bertandatangan dalam masa kurang daripada *r* / *c*, di mana *c* adalah kelajuan cahaya. Menggunakan beberapa stesen, dan beberapa pemahaman tentang ciri rangkaian, lokasi cip boleh ditentukan. Keindahan kaedah ini ialah sebahagian besar keselamatannya disediakan oleh undang-undang fizik. Kaedah lain boleh menggunakan GPS, penjejakan inersia, dan teknologi serupa.

[^11]: Sebagai alternatif, pasangan cip boleh dibenarkan berkomunikasi antara satu sama lain hanya melalui kebenaran eksplisit pentadbir.

[^12]: Ini penting kerana sekurang-kurangnya pada masa ini, sambungan lebar jalur yang sangat tinggi antara cip diperlukan untuk melatih model AI besar padanya.

[^13]: Ini juga boleh disediakan untuk memerlukan mesej bertandatangan daripada *N* daripada *M* pentadbir yang berbeza, membenarkan beberapa pihak berkongsi tadbir urus.

[^14]: Ini jauh daripada tidak pernah terjadi – sebagai contoh tentera tidak membangunkan tentera supersoldier yang diklon atau diubah suai genetik, walaupun ini mungkin secara teknologi. Tetapi mereka telah *memilih* untuk tidak melakukan ini, dan bukannya dihalang oleh orang lain. Rekod prestasi tidak bagus untuk kuasa dunia utama dihalang daripada membangunkan teknologi yang mereka sangat ingin bangunkan.

[^15]: Dengan beberapa pengecualian yang ketara (khususnya NVIDIA) perkakasan khusus AI adalah bahagian yang agak kecil daripada model perniagaan dan hasil keseluruhan syarikat ini. Selain itu, jurang antara perkakasan yang digunakan dalam AI canggih dan perkakasan "gred pengguna" adalah ketara, jadi kebanyakan pengguna perkakasan komputer akan sebahagian besarnya tidak terjejas.

[^16]: Untuk analisis yang lebih terperinci, lihat laporan terkini daripada [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) dan [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). Ini memberi tumpuan kepada kebolehlaksanaan teknikal, terutama dalam konteks kawalan eksport AS yang berusaha mengekang kapasiti negara lain dalam pengkomputeran mewah; tetapi ini mempunyai pertindihan yang jelas dengan kekangan global yang dibayangkan di sini.

[^17]: Peranti Apple, sebagai contoh, dikunci dari jauh dan selamat apabila dilaporkan hilang atau dicuri, dan boleh diaktifkan semula dari jauh. Ini bergantung pada ciri keselamatan perkakasan yang sama yang dibincangkan di sini.

[^18]: Lihat contohnya penawaran [kapasiti atas permintaan](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) IBM, [Intel atas permintaan](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html) Intel., dan [pengkomputeran awan peribadi](https://security.apple.com/blog/private-cloud-compute/) Apple.

[^19]: [Kajian ini](https://epochai.org/trends#hardware-trends-section) menunjukkan bahawa dari segi sejarah prestasi yang sama telah dicapai menggunakan kira-kira 30% kurang dolar setahun. Jika trend ini berterusan, mungkin terdapat pertindihan ketara antara penggunaan cip AI dan "pengguna", dan secara amnya jumlah perkakasan yang diperlukan untuk sistem AI berkuasa tinggi boleh menjadi tidak selesa kecil.

[^20]: Mengikut [kajian yang sama](https://epochai.org/trends#hardware-trends-section), prestasi yang diberikan pada pengecaman imej telah memerlukan 2.5x kurang pengkomputeran setiap tahun. Jika ini juga berlaku untuk sistem AI yang paling berkebolehan juga, had pengkomputeran tidak akan berguna untuk tempoh yang lama.

[^21]: Khususnya, di peringkat negara ini kelihatan seperti nasionalisasi pengkomputeran, kerana kerajaan akan mempunyai banyak kawalan ke atas bagaimana kuasa pengkomputeran digunakan. Walau bagaimanapun, bagi mereka yang bimbang tentang penglibatan kerajaan, ini kelihatan jauh lebih selamat dan lebih disukai daripada perisian AI yang paling berkuasa *sendiri* dinasionalisasi melalui beberapa penggabungan antara syarikat AI utama dan kerajaan negara, seperti yang mula diperjuangkan oleh sesetengah pihak.

[^22]: Langkah kawal selia utama di Eropah telah diambil dengan kelulusan 2024 [Akta AI EU.](https://artificialintelligenceact.eu/) Ia mengklasifikasikan AI mengikut risiko: melarang sistem yang tidak boleh diterima, mengawal selia yang berisiko tinggi, dan mengenakan peraturan ketelusan, atau tiada langkah sama sekali, ke atas sistem berisiko rendah. Ia akan mengurangkan dengan ketara beberapa risiko AI, dan meningkatkan ketelusan AI walaupun untuk firma AS, tetapi mempunyai dua kecacatan utama. Pertama, jangkauan terhad: walaupun ia terpakai kepada mana-mana syarikat yang menyediakan AI di EU, penguatkuasaan ke atas firma berasaskan AS adalah lemah, dan AI ketenteraan dikecualikan. Kedua, walaupun ia meliputi GPAI, ia gagal mengiktiraf KBA atau superintelligence sebagai risiko yang tidak boleh diterima atau menghalang pembangunan mereka—hanya penggunaan EU mereka. Akibatnya, ia tidak banyak membantu untuk mengekang risiko KBA atau superintelligence.

[^23]: Syarikat sering mewakili bahawa mereka menyokong peraturan yang munasabah. Tetapi entah bagaimana mereka hampir selalu nampaknya menentang mana-mana peraturan *tertentu*; saksikan perjuangan mengenai SB1047 yang agak rendah sentuhan, yang [kebanyakan syarikat AI menentang secara terbuka atau peribadi.](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/)

[^24]: Ia adalah kira-kira 3 1/2 tahun dari masa akta AI EU dicadangkan sehingga ia berkuat kuasa.

[^25]: Kadangkala dinyatakan bahawa ia "terlalu awal" untuk mula mengawal selia AI. Memandangkan nota terakhir, itu nampaknya tidak mungkin. Satu lagi kebimbangan yang dinyatakan ialah peraturan akan "membahayakan inovasi." Tetapi peraturan yang baik hanya mengubah arah, bukan jumlah, inovasi.

[^26]: Duluan yang menarik adalah dalam pengangkutan bahan berbahaya, yang mungkin terlepas dan menyebabkan kerosakan. Di sini, [peraturan](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) dan [undang-undang kes](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) telah menetapkan liabiliti ketat untuk bahan yang sangat berbahaya seperti bahan letupan, petrol, racun, agen berjangkit, dan sisa radioaktif. Contoh lain termasuk [amaran pada farmaseutikal](https://www.medicalnewstoday.com/articles/boxed-warnings), [kelas peranti perubatan,](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification) dll.

[^27]: Satu lagi cadangan komprehensif dengan matlamat serupa yang dikemukakan dalam ["A Narrow Path"](https://www.narrowpath.co/) menyokong pendekatan berasaskan larangan yang lebih terpusat yang menyalurkan semua pembangunan AI terdepan melalui entiti antarabangsa tunggal, diawasi oleh institusi antarabangsa yang kuat, dengan larangan kategorikal yang jelas dan bukannya sekatan bergraduat. Saya juga akan menyokong pelan tersebut; walau bagaimanapun ia akan mengambil lebih banyak kemahuan politik dan koordinasi daripada yang dicadangkan di sini.

[^28]: Beberapa garis panduan untuk piawaian sedemikian telah [diterbitkan](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/) oleh Forum Model Frontier. Berbanding cadangan di sini, yang tersalah di sisi kurang ketepatan dan kurang pengkomputeran termasuk dalam kiraan.

[^29]: Perintah eksekutif AI AS 2023 (kini dibatalkan) memerlukan pelaporan serupa tetapi kurang berbutir halus. Ini harus diperkukuh oleh perintah pengganti.

[^30]: Secara kasarnya, untuk cip H100 biasa sekarang ini sepadan dengan kelompok kira-kira 1000 melakukan inferens; ia adalah kira-kira 100 (kira-kira USD $5M bernilai) cip NVIDIA B200 terbaharu terbaik melakukan inferens. Dalam kedua-dua kes nombor latihan sepadan dengan kelompok tersebut mengira untuk beberapa bulan.

[^31]: Jumlah ini lebih besar daripada mana-mana sistem AI yang dilatih pada masa ini; nombor yang lebih besar atau lebih kecil mungkin wajar apabila kita lebih memahami bagaimana keupayaan AI berskala dengan pengkomputeran.

[^32]: Ini terpakai kepada mereka yang mencipta dan menyediakan/menghos model, bukan pengguna akhir.

[^33]: Secara kasarnya, liabiliti "ketat" bermakna pembangun dipertanggungjawabkan untuk kemudaratan yang dilakukan oleh produk *secara lalai* dan adalah piawaian yang digunakan untuk produk "berbahaya secara tidak normal", dan (agak lucu tetapi sesuai) haiwan liar. Liabiliti "bersama dan beberapa" bermakna liabiliti diberikan kepada semua pihak yang bertanggungjawab untuk produk, dan pihak tersebut perlu menyelesaikan antara mereka sendiri siapa yang memikul tanggungjawab apa. Ini penting untuk sistem seperti AI dengan rantaian nilai yang panjang dan kompleks.

[^34]: Liabiliti berasaskan kesalahan pihak tunggal standard tidak mencukupi: kesalahan akan sukar dijejaki dan diberikan kerana sistem AI adalah kompleks, operasinya tidak difahami, dan banyak pihak mungkin terlibat dalam penciptaan sistem atau output berbahaya. Selain itu, tuntutan mahkamah akan mengambil masa bertahun-tahun untuk diputuskan dan mungkin hanya menghasilkan denda yang tidak penting kepada syarikat ini, jadi liabiliti peribadi untuk eksekutif adalah penting juga.

[^35]: Tidak seharusnya ada pengecualian daripada kriteria keselamatan untuk model berat terbuka. Selain itu, dalam menilai risiko harus diandaikan bahawa pagar yang boleh dikeluarkan akan dikeluarkan daripada model yang tersedia secara meluas, dan bahawa model tertutup akan tersebar melainkan terdapat jaminan yang sangat tinggi ia akan kekal selamat.

[^36]: Skim yang dicadangkan di sini mempunyai penelitian kawal selia yang dicetuskan pada keupayaan umum; walau bagaimanapun masuk akal untuk beberapa kes penggunaan yang berisiko terutama untuk mencetuskan penelitian lebih – sebagai contoh sistem AI virologi pakar, walaupun sempit dan pasif, mungkin sepatutnya masuk ke peringkat yang lebih tinggi. Bekas perintah eksekutif AS mempunyai beberapa struktur ini untuk keupayaan biologi.

[^37]: Dua contoh yang jelas ialah penerbangan dan ubat-ubatan, dikawal oleh FAA dan FDA, dan agensi serupa di negara lain. Agensi ini tidak sempurna, tetapi telah amat penting untuk fungsi dan kejayaan industri tersebut.