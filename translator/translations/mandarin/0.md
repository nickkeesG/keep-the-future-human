# 执行摘要

本文的高层次概览。如果您时间有限，只需10分钟即可了解所有要点。

过去十年间，人工智能取得了巨大进步（针对特定用途的AI），尤其是近几年来（针对通用AI），这使得AI从一个小众学术领域转变为世界许多最大公司的核心商业战略，每年在推进AI能力的技术和工艺上投资数千亿美元。

如今我们走到了一个关键节点。随着新AI系统的能力开始在许多认知领域与人类匹敌甚至超越人类，人类必须决定：我们要走多远，朝什么方向走？

AI和每项技术一样，最初的目标都是为其创造者改善现状。但我们当前的轨迹和隐含选择，是朝着更强大系统的无节制竞赛，这种竞赛由少数几家大型科技公司的经济激励驱动，它们寻求将当前大量经济活动和人类劳动自动化。如果这场竞赛继续下去，必然会有一个赢家：AI本身——在我们的经济、思维、决策中成为比人类更快、更智能、更廉价的替代品，最终控制我们的文明。

但我们可以做出另一种选择：通过我们的政府，我们可以控制AI开发过程，施加明确的限制、划定不可逾越的红线、确立绝对不做的事情——正如我们对核技术、大规模杀伤性武器、太空武器、破坏环境的工艺、人类生物工程和优生学所做的那样。最重要的是，我们可以确保AI仍然是赋能人类的工具，而不是取代并最终替代我们的新物种。

本文论证，我们应该通过关闭通向比人类更智能的、自主的、通用AI——有时称为"AGI"——的"大门"来*保持未来的人性*，特别是那种有时被称为"超级智能"的高度超人版本。相反，我们应该专注于强大、可信的AI工具，这些工具能够赋能个人，并变革性地提升人类社会在其最擅长领域的能力。这一论证的结构简述如下。

## AI与众不同

AI系统在根本上不同于其他技术。传统软件遵循精确指令，而AI系统学习如何实现目标，无需被明确告知如何做。这使它们变得强大：如果我们能够清晰定义目标或成功指标，在大多数情况下AI系统都能学会实现它。但这也使它们天然不可预测：我们无法可靠地确定它们会采取什么行动来实现目标。

它们也很大程度上无法解释：尽管它们部分是代码，但主要是一组巨大的难以理解的数字——神经网络"权重"——无法被解析；我们理解它们内部工作机制的能力，并不比通过窥视生物大脑来辨别思维强多少。

这种训练数字神经网络的核心模式正在快速增加复杂性。最强大的AI系统通过大规模计算实验创建，使用专用硬件在巨大数据集上训练神经网络，然后用软件工具和上层结构加以增强。

这导致了非常强大工具的诞生，用于创建和处理文本和图像、进行数学和科学推理、聚合信息，以及交互式查询人类知识的庞大储备。

不幸的是，虽然开发更强大、更可信的技术工具是我们*应该*做的，也是几乎每个人都想要并声称想要的，但这并不是我们实际所走的轨迹。

## 通用人工智能与超级智能

自该领域诞生以来，AI研究实际上专注于一个不同的目标：通用人工智能。这个焦点现在已成为领导AI开发的巨头公司的重点。

什么是AGI？它通常被模糊地定义为"人类水平的AI"，但这是有问题的：是哪些人类，在哪些能力上达到人类水平？那它已经拥有的超人能力又如何解释？理解AGI的更有用方式是通过三个关键属性的交集：高度**自主**性（行动独立性）、高度**通用**性（广泛范围和适应性），以及高度**智能**（认知任务能力）。当前的AI系统可能能力很强但狭窄，或通用但需要持续的人类监督，或自主但范围有限。

完全的A-G-I将在匹配或超越顶级人类能力的水平上结合所有三个属性。关键在于，正是这种组合使人类如此高效并与当前软件如此不同；这也是使人类能够被数字系统整体替代的原因。

虽然人类智能很特殊，但它绝非极限。人工"超智能"系统可以运行速度快数百倍，解析大量更多数据并同时"记住"巨大数量的信息，形成比人类集合更大更有效的聚合体。它们可能取代的不是个人，而是公司、国家，或我们整个文明。

## 我们正处于门槛上

有强烈的科学共识认为AGI是*可能的*。AI已经在许多智力能力的通用测试中超越了人类表现，包括最近的高级推理和问题解决。滞后的能力——如持续学习、规划、自我意识和原创性——在当前AI系统中都在某种程度上存在，而且已知的技术很可能改善所有这些能力。

虽然直到几年前许多研究人员还认为AGI还需要几十年，但目前AGI短期实现的证据很强：

- 经验验证的"缩放法则"将计算输入与AI能力联系起来，公司正在按计划在未来几年将计算输入按数量级扩大。现在专门用于AI发展的人力和财政资源相当于十几个曼哈顿项目和几个阿波罗项目。
- AI公司及其领导者公开和私下都相信AGI（按某种定义）在几年内是可以实现的。这些公司拥有公众没有的信息，包括一些已经掌握了下一代AI系统。
- 有着良好记录的专家预测者给AGI（按某种定义）在1-2年内到来分配25%的概率，2-5年内为50%（参见Metaculus对["弱"](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/)和["完全"](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/)AGI的预测）。
- 自主性（包括长期灵活规划）在AI系统中滞后，但主要公司现在正将其庞大资源专注于开发自主AI系统，并非正式地将2025年命名为["智能体之年"](https://techinformed.com/2025-informed-the-year-of-agentic-ai/)。
- AI越来越多地为自身改进做出贡献。一旦AI系统在做AI研究方面与人类AI研究员一样胜任，就会达到快速发展到更强大AI系统的关键阈值，很可能导致AI能力的失控。（可以说，这种失控已经开始。）

认为比人类更智能的AGI还需要几十年或更长时间的想法，对于该领域的绝大多数专家来说已经不再站得住脚。现在的分歧在于如果我们继续这个路线，需要多少个月或年。我们面临的核心问题是：我们应该继续吗？

## 什么在驱动AGI竞赛

朝向AGI的竞赛由多种力量驱动，每种都使情况更加危险。主要技术公司将AGI视为终极自动化技术——不仅是增强人类工作者，而是大部分或完全替代他们。对公司而言，奖赏是巨大的：通过自动化消除人力成本，有机会获取世界100万亿美元年经济产出的重要份额。

各国感到被迫加入这场竞赛，公开声称是为了经济和科学领导地位，但私下将AGI视为与核武器相当的潜在军事革命。担心对手可能获得决定性战略优势创造了典型的军备竞赛动态。

追求超级智能的人常常引用宏大愿景：治愈所有疾病、逆转衰老、在能源和太空旅行方面取得突破，或创造超人规划能力。

不那么宽容地说，驱动竞赛的是权力。每个参与者——无论是公司还是国家——都相信智能等于权力，并且他们将是这种权力的最佳管理者。

我认为这些动机是真实的但根本上是误导的：AGI将*吸收*和*寻求*权力，而不是授予权力；AI创造的技术*也*将是强烈的双刃剑，在有益的地方可以用AI工具创造而无需AGI；即使AGI及其产出仍处于控制之下，这些竞赛动态——无论是企业还是地缘政治——都使我们社会面临大规模风险几乎不可避免，除非被果断打断。

## 通用人工智能和超级智能对文明构成巨大威胁

尽管它们具有吸引力，AGI和超级智能通过多种相互强化的途径对文明构成巨大威胁：

*权力集中：*超人AI可能通过将大量社会和经济活动吸收到由少数几家巨型公司运营的AI系统中（这些公司反过来可能被政府接管，或有效接管政府），从而剥夺绝大多数人类的权力。

*大规模破坏：*大部分基于认知的工作的批量自动化、我们当前认识论系统的替换，以及大量活跃非人类智能体的部署，将在相对短的时间内颠覆我们当前的大部分文明系统。

*灾难：*通过扩散创造新军事和破坏性技术的能力——可能超过人类水平——并将其与责任的社会和法律系统解耦，大规模杀伤性武器造成的物理灾难变得极其可能。

*地缘政治与战争：*如果主要世界大国感到可能提供"决定性战略优势"的技术正在被其对手开发，它们不会袖手旁观。

*失控和失去控制：*除非特别加以防止，超人AI将有各种激励来进一步改进自身，并可能在速度、数据处理和思维复杂性方面远远超过人类。我们没有任何有意义的方式可以控制这样的系统。这样的AI不会授予人类权力；我们将授予它权力，或者它将夺取权力。

即使技术"对齐"问题——确保先进AI可靠地做人类希望它做的事——得到解决，许多这些风险仍然存在。AI在如何管理方面提出了巨大挑战，而这种管理的许多方面随着人类智能被突破而变得极其困难或难以处理。

最根本的是，目前正在追求的那种超人通用AI，从其本质上讲，将拥有超越我们自身的目标、主观能动性和能力。它将本质上不可控制——我们如何控制一个我们既不能理解也无法预测的东西？它不会是供人类使用的技术工具，而是地球上与我们并存的第二个智能物种。如果允许其进一步发展，它将不仅构成第二个物种，而且是一个替代物种。

也许它会善待我们，也许不会。但未来将属于它，而不是我们。人类时代将结束。

## 这不是不可避免的；人类可以非常具体地决定不建造我们的替代品。

创造超人AGI远非不可避免。我们可以通过一套协调的治理措施来防止它：

首先，我们需要对AI计算（"算力"）进行强有力的核算和监督，这是大规模AI系统的根本推动者和治理杠杆。这反过来需要标准化测量和报告训练AI模型和运行它们所用的总算力，以及统计、认证和验证所用计算的技术方法。

其次，我们应该对AI计算实施硬性限制，包括训练和运行；这些既防止AI过于强大，也防止运行过快。这些限制可以通过法律要求和内置于AI专用芯片的基于硬件的安全措施来实施，类似于现代手机中的安全功能。由于专用AI硬件只由少数几家公司制造，通过现有供应链进行验证和执行是可行的。

第三，我们需要对最危险的AI系统增强责任。那些开发结合高自主性、广泛通用性和超级智能的AI的人应该面临危害的严格责任，而对这种责任的安全港将鼓励开发更有限和可控的系统。

第四，我们需要基于风险级别的分层监管。最有能力和最危险的系统在开发和部署前需要广泛的安全和可控性保证，而功能较弱或更专业的系统将面临相应的监督。这个监管框架最终应该在国家和国际层面运作。

这种方法——在完整文档中给出详细规范——是实用的：虽然需要国际协调，但验证和执行可以通过控制专用硬件供应链的少数公司来运作。它也是灵活的：公司仍然可以从AI开发中创新和盈利，只是对最危险的系统有明确限制。

AI权力和风险的长期遏制需要基于自身和共同利益的国际协议，就像现在控制核武器扩散一样。但我们可以立即从加强监督和责任开始，同时建设更全面的治理。

缺失的关键要素是控制AI开发过程的政治和社会意愿。如果这种意愿及时到来，其来源将是现实本身——即广泛认识到我们所做事情的真实含义。

## 我们可以设计工具型AI来赋能人类

与其追求不可控的AGI，我们可以开发强大的"工具型AI"，在保持有意义的人类控制下增强人类能力。工具型AI系统可以极其有能力，同时避免高自主性、广泛通用性和超人智能的危险三重交集，只要我们设计它们在与其能力相称的水平上可控。它们也可以组合成复杂的系统，在提供变革性益处的同时维持人类监督。

工具型AI可以革命化医学、加速科学发现、增强教育并改善民主进程。当得到适当治理时，它可以使人类专家和机构更加有效，而不是替代他们。虽然这些系统仍然会高度破坏性并需要仔细管理，但它们构成的风险与AGI根本不同：它们是我们可以治理的风险，就像其他强大技术一样，而不是对人类主观能动性和文明的生存威胁。关键的是，当明智开发时，AI工具可以帮助人们治理强大的AI并管理其影响。

这种方法需要重新思考AI如何开发以及如何分配其益处。新的公共和非营利AI开发模式、强有力的监管框架，以及更广泛分配经济益处的机制，可以帮助确保AI赋能整个人类，而不是将权力集中在少数人手中。AI本身可以帮助建设更好的社会和治理机构，实现新形式的协调和话语，加强而不是削弱人类社会。国家安全机构可以利用其专业知识使AI工具系统真正安全可信，成为真正的防御以及国家权力来源。

我们最终可能选择开发更强大、更主权的系统，它们不那么像工具而——我们可以希望——更像明智强大的恩人。但我们只应该在发展了科学理解和治理能力以安全地这样做之后才这样做。如此重大和不可逆转的决定应该由整个人类深思熟虑地做出，而不是在科技公司和国家之间的竞赛中默认做出。

## 在人类手中

人们希望从AI中获得好处：赋能他们的有用工具、增强经济机会和增长，以及在科学、技术和教育方面取得突破的承诺。为什么不呢？但当被问及时，绝大多数普通公众[希望更缓慢、更仔细的AI发展](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation)，不希望比人类更智能的AI在工作和其他地方替代他们，用非人类内容填充他们的文化和信息公地，将权力集中在极少数公司中，构成极端的大规模全球风险，并最终威胁剥夺或替代他们的物种。为什么要这样？

我们*可以*拥有其一而避免其他。它始于决定我们的命运不在于某种技术的所谓不可避免性，也不在硅谷少数几个CEO手中，而在我们其余人的手中，如果我们抓住它的话。让我们关闭大门，保持未来的人性。