# 附录

补充信息，包括——算力核算的技术细节、"关闭大门"的实施示例、严格的通用人工智能责任制度详情，以及分层式的通用人工智能安全与安保标准。

## 附录A：算力核算技术细节

要实现有意义的基于算力的控制，需要一套详细的方法来计算训练和推理中使用的总算力，既要有"基准事实"，也要有良好的近似方法。以下是如何在技术层面统计"基准事实"的示例。

**定义：**

*算力因果图：* 对于AI模型的给定输出O，存在一组数字计算，改变这些计算的结果可能会改变O。（这应该保守假设，即应该有明确的理由相信某项计算独立于既在时间上较早发生又具有物理潜在因果效应路径的前序计算。）这包括AI模型在推理过程中进行的计算，以及涉及输入、数据准备和模型训练的计算。由于其中任何一个本身都可能是AI模型的输出，因此这是递归计算的，在人类对输入提供了重大改变的地方截止。

*训练算力：* 神经网络算力因果图所涉及的总算力（以FLOP或其他单位计），包括数据准备、训练、微调以及任何其他计算。

*输出算力：* 给定AI输出的算力因果图中的总算力，包括所有神经网络（及其训练算力）和该输出涉及的其他计算。

*推理算力速率：* 在一系列输出中，输出算力之间的变化率（以FLOP/s或其他单位计），即用于产生下一个输出的算力除以输出之间的时间间隔。

**示例和近似方法：**

- 对于在人类创建数据上训练的单个神经网络，训练算力就是通常报告的总训练算力。
- 对于以稳定速率进行推理的此类神经网络，推理算力速率大约等于执行推理的计算集群的总计算速度（以FLOP/s计）。
- 对于模型微调，完整模型的训练算力由未微调模型的训练算力加上微调期间的计算以及准备微调所用数据的计算组成。
- 对于蒸馏模型，完整模型的训练算力包括蒸馏模型和用于提供合成数据或其他训练输入的更大模型的训练。
- 如果训练了多个模型，但基于人类判断丢弃了许多"试验"，这些不计入保留模型的训练或输出算力。

## 附录B："关闭大门"的实施示例

**实施示例：** 以下是在训练限制为10<sup>27</sup> FLOP、推理限制为10<sup>20</sup> FLOP/s（运行AI）的情况下，"关闭大门"如何运作的一个示例：

**1. 暂停：** 出于国家安全考虑，美国行政部门要求所有总部设在美国、在美国开展业务或使用美国制造芯片的公司，停止任何可能超过10<sup>27</sup> FLOP训练算力限制的新AI训练运行。美国应与其他拥有AI开发的国家开始讨论，强烈鼓励它们采取类似步骤，并表明如果它们选择不遵守，美国的暂停可能会解除。

**2. 美国监督和许可：** 通过行政命令或现有监管机构的行动，美国要求在（比如说）一年内：

- 美国境内运营公司进行的所有估计超过10<sup>25</sup> FLOP的AI训练运行必须在美国监管机构维护的数据库中注册。（注：2023年美国AI行政命令的稍弱版本已包含这一点，要求超过10<sup>26</sup> FLOP的模型进行注册，现已被撤销。）
- 所有在美国运营或与美国政府开展业务的AI相关硬件制造商必须遵守其专用硬件及驱动软件的一系列要求。（许多这些要求可以通过现有硬件的软件和固件更新来构建，但长期和稳健的解决方案需要对后续硬件世代进行更改。）其中包括要求如果硬件是能够执行10<sup>18</sup> FLOP/s计算的高速互连集群的一部分，则需要更高级别的验证，包括由接收遥测数据并请求执行额外计算的远程"管理器"定期许可。
- 保管人向维护美国数据库的机构报告其硬件执行的总计算量。
- 逐步引入更严格的要求，以实现更安全、更灵活的监督和许可。

**3. 国际监督：**

- 美国、中国和任何其他拥有先进芯片制造能力的国家协商国际协议。
- 该协议创建一个新的国际机构，类似于国际原子能机构，负责监督AI训练和执行。
- 签署国必须要求其国内AI硬件制造商遵守至少与美国实施的要求同样严格的一系列要求。
- 保管人现在需要向其母国的机构以及国际机构内的新办公室报告AI计算数据。
- 强烈鼓励其他国家加入现有国际协议：签署国的出口管制限制非签署国获得高端硬件，而签署国可以获得管理其AI系统的技术支持。

**4. 国际验证和执行：**

- 硬件验证系统更新，既向原始保管人报告计算使用情况，也直接向国际机构办公室报告。
- 该机构通过与国际协议签署国的讨论，就计算限制达成一致，然后在签署国中具有法律效力。
- 与此同时，可能制定一套国际标准，要求超过计算阈值（但低于限制）的AI训练和运行必须遵守这些标准。
- 该机构可以在必要时（比如为了补偿更好的算法等）降低计算限制。或者，如果被认为是安全和可取的（比如达到可证明的安全保证水平），提高计算限制。

## 附录C：严格的通用人工智能责任制度详情

**严格的通用人工智能责任制度详情**

- 创建和运营具有高度通用性、能力和自主性的先进AI系统被视为"异常危险"活动。
- 因此，对于训练和运营此类系统的默认责任级别是对模型或其输出/行为造成的任何损害承担严格的连带责任（或其非美国等价物）。
- 在严重过失或故意不当行为的情况下，将对高管和董事会成员施加个人责任。这应该包括对最恶劣案例的刑事处罚。
- 有许多安全港，在这些情况下责任恢复到人员和公司通常适用的默认（在美国为过错责任）责任。
	- 在某个算力阈值以下训练和运营的模型（该阈值至少比上述限制低10倍）。
	- "弱"AI（大致低于其预期任务的人类专家水平）和/或
	- "狭窄"AI（具有固定且相当有限的任务和操作范围，专门为此设计和训练）和/或
	- "被动"AI（即使在适度修改下，在没有直接人类参与和控制的情况下采取行动或执行复杂多步骤任务的能力非常有限）。
	- 保证安全、可靠和可控的AI（可证明安全，或风险分析表明预期危害水平可忽略不计）。
- 可以基于AI开发者准备并经机构或机构认证的审计员批准的[安全案例](https://arxiv.org/abs/2410.21572)来申请安全港。要基于算力申请安全港，开发者只需提供总训练算力和最大推理速率的可信估计。
- 立法将明确概述在什么情况下对开发具有高公共危害风险的AI系统进行禁令救济是适当的。
- 公司联盟与非政府组织和政府机构合作，应制定标准和规范，定义这些术语、监管机构应如何授予安全港、AI开发者应如何制定安全案例，以及在未主动申请安全港的情况下法院应如何解释责任。

## 附录D：分层式的通用人工智能安全与安保标准

**分层式的通用人工智能安全与安保标准**

| 风险层级 | 触发条件 | 训练要求 | 部署要求 |
| --- | --- | --- | --- |
| RT-0 | AI在自主性、通用性和智能方面都较弱 | 无 | 无 |
| RT-1 | AI在自主性、通用性和智能中的一项较强 | 无 | 基于风险和用途，可能需要模型使用地国家当局批准的安全案例 |
| RT-2 | AI在自主性、通用性和智能中的两项较强 | 在对开发者有管辖权的国家当局处注册 | 将重大危害风险控制在授权水平以下的安全案例，加上模型使用地国家当局批准的独立安全审计（包括黑盒和白盒红队测试） |
| RT-3 | 在自主性、通用性和智能方面都较强的通用人工智能 | 对开发者有管辖权的国家当局预先批准安全和安保计划 | 保证将重大危害风险控制在授权水平以下的安全案例以及必要规范，包括网络安全、可控性、不可移除的终止开关、与人类价值观的对齐以及对恶意使用的稳健性。 |
| RT-4 | 任何也超过10<sup>27</sup> FLOP训练或10<sup>20</sup> FLOP/s推理的模型 | 在国际同意解除算力限制前禁止 | 在国际同意解除算力限制前禁止 |

基于算力阈值以及高自主性、通用性和智能组合的风险分类和安全/安保标准，层级如下：

- *强自主性*适用于系统能够执行或可以轻易被改造为执行多步骤任务和/或采取与现实世界相关的复杂行动，而无需重大人类监督或干预的情况。例子：自动驾驶汽车和机器人；金融交易机器人。反例：GPT-4；图像分类器
- *强通用性*表示应用范围广泛，执行模型未经刻意和专门训练的任务，以及显著的学习新任务能力。例子：GPT-4；mu-zero。反例：AlphaFold；自动驾驶汽车；图像生成器
- *强智能*对应于在模型表现最佳的任务上达到人类专家水平的性能（对于通用模型，则是在广泛任务范围内）。例子：AlphaFold；mu-zero；o3。反例：GPT-4；Siri