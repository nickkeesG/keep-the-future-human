# 第二章 - AI神经网络必备知识

现代AI系统如何运作？下一代AI可能会带来什么？

要理解开发更强大AI会产生怎样的后果，掌握一些基础知识至关重要。本章及接下来两章将阐述这些基础内容，依次介绍现代AI的本质、它如何利用大规模算力，以及它在通用性和能力方面的快速发展。[^1]

定义人工智能的方式有很多种，但就我们的讨论而言，AI的关键特性在于：标准计算机程序是执行任务的指令清单，而AI系统则是*从数据或经验中学习执行任务，无需明确告知其具体做法*。

几乎所有重要的现代AI都基于神经网络。这些是数学/计算结构，由大量（数十亿或数万亿个）数字（"权重"）表示，能够很好地完成训练任务。这些权重通过迭代调整来"制作"（或者说"培养"或"发现"），使神经网络在一个或多个任务上改善数值分数（也称为"损失"）。[^2]这个过程被称为神经网络的*训练*。[^3]

训练有很多技术方法，但这些细节远不如分数定义方式重要，也不如这些定义如何导致神经网络在不同任务上表现出色重要。历史上，"狭义"AI和"通用"AI之间一直存在关键区别。

狭义AI专门训练来完成特定任务或少量任务集合（如图像识别或下棋）；它需要为新任务重新训练，能力范围有限。我们已经拥有超人类的狭义AI，这意味着对于几乎任何人类能完成的离散明确任务，我们都可以构建评分机制，然后成功训练狭义AI系统，使其表现超越人类。

通用人工智能（GPAI）系统能够执行广泛的任务，包括许多未经明确训练的任务；它们还能在运行过程中学习新任务。当前的大型"多模态模型"[^4]如ChatGPT就体现了这一点：基于大量文本和图像语料训练，它们能够进行复杂推理、编写代码、分析图像，并协助完成各种智力任务。虽然在某些方面仍与人类智能存在显著差异（我们将在下文深入探讨），但它们的通用性已经引发了AI革命。[^5]

## 不可预测性：AI系统的关键特征

AI系统与传统软件的关键区别在于可预测性。标准软件的输出可能无法预测——实际上有时我们编写软件正是为了获得无法预测的结果。但传统软件很少做出程序设计之外的行为——其范围和行为通常符合设计预期。顶级国际象棋程序可能会走出人类无法预测的棋步（否则人类就能击败这个程序！），但它通常不会做下棋以外的事情。

与传统软件类似，狭义AI具有可预测的范围和行为，但可能产生不可预测的结果。这实际上是狭义AI的另一种定义方式：在可预测性和运作范围方面类似于传统软件的AI。

通用人工智能则不同：其范围（适用领域）、行为（所做事情的类型）和结果（实际输出）都可能无法预测。[^6]GPT-4仅仅为了准确生成文本而训练，却发展出许多训练者未曾预测或意图的能力。这种不可预测性源于训练的复杂性：由于训练数据包含来自许多不同任务的输出，AI必须有效学习执行这些任务才能做出良好预测。

通用AI系统的这种不可预测性是相当根本的。虽然原则上可以精心构建具有行为保证限制的AI系统（如本文后面所述），但以目前的AI系统创建方式，它们在实践中甚至在原则上都是不可预测的。

## 被动AI、智能体、自主系统和对齐

当我们考虑AI系统如何实际部署和使用来实现各种目标时，这种不可预测性变得尤为重要。

许多AI系统相对被动，主要提供信息，由用户采取行动。其他系统通常被称为*智能体*，它们自己采取行动，用户参与程度不同。那些在相对较少外部输入或监督下采取行动的系统可称为更加*自主*的系统。这在行动独立性方面形成一个谱系，从被动工具到自主智能体。[^7]

至于AI系统的目标，这些目标可能直接与其训练目标相关（例如，围棋程序的"获胜"目标也正是其训练目的）。或者可能不相关：ChatGPT的训练目标部分是预测文本，部分是成为有用的助手。但在执行特定任务时，其目标由用户提供。目标也可能由AI系统自己创建，与其训练目标只有非常间接的关系。[^8]

目标与"对齐"问题密切相关，即AI系统是否会*按照我们的期望行事*。这个简单问题隐藏着巨大的复杂性。[^9]现在需要注意的是，这句话中的"我们"可能指代许多不同的人和群体，导致不同类型的对齐。例如，AI可能对用户高度*服从*（或["忠诚"](https://arxiv.org/abs/2003.11157)）——这里的"我们"是"我们每个人"。或者它可能更加*独立*，主要由自己的目标和约束驱动，但仍然大体上为人类福祉的共同利益而行动——这时"我们"指的是"人类"或"社会"。介于两者之间的是一个谱系，AI在大体上服从的同时，可能拒绝采取伤害他人或社会、违反法律等行为。

这两个维度——自主程度和对齐类型——并非完全独立。例如，独立的被动系统虽然不完全自相矛盾，但概念上存在张力，服从的自主智能体也是如此。[^10]从某种明确意义上说，自主性和独立性往往相伴而生。类似地，"被动"和"服从"的AI系统往往具有更高的可预测性，而独立或自主的系统往往更加不可预测。所有这些对于理解潜在的通用人工智能(AGI)和超级智能的影响都至关重要。

创造真正对齐的AI，无论何种类型，都需要解决三个不同的挑战：

1. 理解"我们"想要什么——无论"我们"指的是特定个人或组织（忠诚），还是广泛的人类（独立），这都很复杂；
2. 构建定期按照这些期望行动的系统——本质上是创造一致的积极行为；
3. 最根本的是，让系统真正"关心"这些期望，而不仅仅是表现得好像关心。

可靠行为与真正关心之间的区别至关重要。正如人类员工可能完美地遵循命令，却对组织使命缺乏真正承诺，AI系统可能表现出对齐行为，却并不真正重视人类偏好。我们可以通过反馈训练AI系统说话和行动，它们可以学会推理人类想要什么。但让它们*真正*重视人类偏好是一个更深层的挑战。[^11]

解决这些对齐挑战的深刻困难及其对AI风险的影响，将在下文进一步探讨。现在需要理解的是，对齐不仅仅是我们为AI系统附加的技术特征，而是其架构的基本方面，塑造着它们与人类的关系。


[^1]: 关于机器学习和AI（特别是语言模型）的温和但技术性介绍，请参见[此网站。](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) 关于AI存在风险的另一个现代入门读物，请参见[此文。](https://www.thecompendium.ai/) 关于AI安全现状的全面权威科学分析，请参见最近的[国际AI安全报告。](https://arxiv.org/abs/2501.17805)

[^2]: 训练通常通过在模型权重给定的高维空间中寻找分数的局部最大值来进行。通过检查分数如何随权重调整而变化，训练算法识别哪些调整最能改善分数，并将权重向那个方向移动。

[^3]: 例如，在图像识别问题中，神经网络会输出图像标签的概率。分数会与AI对正确答案给出的概率相关。然后训练程序会调整权重，使下次AI对该图像的正确标签输出更高概率。这个过程会重复大量次数。基本上所有现代神经网络都使用相同的基本程序进行训练，尽管评分机制更复杂。

[^4]: 大多数多模态模型使用"变换器"架构来处理和生成多种类型的数据（文本、图像、声音）。这些都可以分解为不同类型的"标记"，然后以相同方式处理。多模态模型首先训练以准确预测大规模数据集中的标记，然后通过强化学习进行优化以增强能力和塑造行为。

[^5]: 语言模型训练只做一件事——预测词汇——这导致一些人称其为狭义AI。但这是误导性的：因为良好的文本预测需要许多不同的能力，这种训练任务产生了一个出人意料的通用系统。还要注意，这些系统经过强化学习的广泛训练，有效地代表了数千人在模型在其所做的许多事情中表现良好时给予奖励信号。它继承了给出反馈的人们的显著通用性。

[^6]: AI的不可预测性有多种方式。一种是在一般情况下，人们无法预测算法将做什么，除非实际运行它；有[定理](https://arxiv.org/abs/1310.3225)证明这一点。这可能仅仅因为算法的输出可能很复杂。但在某些情况下（如国际象棋或围棋），预测意味着预测者必须具备其不具备的能力（击败AI），这一点特别明确和相关。第二，给定AI系统即使在相同输入下也不会总是产生相同输出——其输出包含随机性；这也与算法不可预测性相结合。第三，意外和突现能力可能从训练中产生，这意味着甚至AI系统能够和将要做的事情的*类型*都是不可预测的；最后一种类型对安全考虑特别重要。

[^7]: 关于"自主智能体"含义的深入回顾（以及反对构建它们的伦理论据），请参见[此处](https://arxiv.org/abs/2502.02649)。

[^8]: 你有时可能听到"AI不能有自己的目标"。这完全是胡说。很容易产生AI拥有或发展出从未给予它的、只有它自己知道的目标的例子。你在当前流行的多模态模型中不常看到这种情况，因为它被训练掉了；它同样可以被训练进去。

[^9]: 有大量文献。关于一般问题，参见Christian的[《对齐问题》](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821)和Russell的[《人机兼容》](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616)。在更技术的方面，参见例如[这篇论文](https://arxiv.org/abs/2209.00626)。

[^10]: 我们后面会看到，虽然这样的系统违背趋势，但这实际上使它们非常有趣和有用。

[^11]: 这并不是说我们需要情感或感知能力。相反，从系统外部了解其内在目标、偏好和价值观是极其困难的。这里的"真正"意味着我们有足够强的理由依赖它，在关键系统的情况下我们可以把生命押在上面。