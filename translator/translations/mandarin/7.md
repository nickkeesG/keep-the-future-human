# 第七章 - 如果我们按照当前路径构建通用人工智能会发生什么？

社会还没有为通用人工智能级别的系统做好准备。如果我们很快就构建出这样的系统，情况可能会变得很糟糕。

开发完整的通用人工智能——我们在这里将其称为"大门外"的AI——将是世界本质的根本性转变：就其本质而言，这意味着在地球上增加了一个新的智能物种，其能力超过人类。

接下来会发生什么取决于许多因素，包括技术的性质、开发者的选择，以及开发时的世界背景。

目前，完整的通用人工智能正在由少数几家大型私人公司相互竞赛开发，几乎没有有意义的监管或外部监督，[^1] 这发生在一个核心机构日益衰弱甚至功能失调的社会中，[^2] 在地缘政治紧张局势加剧、国际协调水平低下的时期。虽然有些人出于利他主义动机，但许多从事这项工作的人都受到金钱或权力或两者的驱动。

预测是非常困难的，但有一些动态机制是足够清楚的，与以往技术也有足够恰当的类比可以作为指导。不幸的是，尽管AI充满希望，这些机制让我们有充分理由对当前轨迹的发展前景深感悲观。

坦率地说，按照我们目前的路线发展通用人工智能会产生一些积极效果（并让一些人变得非常非常富有）。但技术的性质、基本动态机制，以及开发它的背景，强烈表明：强大的AI将严重破坏我们的社会和文明；我们将失去对它的控制；我们很可能因此而陷入世界大战；我们将失去控制权或将控制权拱手让给它；它将导致超级智能的出现，而我们绝对无法控制超级智能，这将意味着人类主导世界的终结。

这些都是强有力的论断，我希望它们只是无谓的推测或毫无根据的"末日论"。但这正是科学、博弈论、进化论和历史所指向的方向。本节将详细阐述这些论断及其支撑论据。

## 我们将破坏我们的社会和文明

尽管你可能在硅谷董事会议室里听到不同的说法，但大多数颠覆——尤其是极其迅速的颠覆——都不是有益的。让复杂系统变糟的方法远比让其变好的方法要多得多。我们的世界之所以运转得如此良好，是因为我们艰苦地构建了让它稳步改善的流程、技术和制度。[^3] 用大锤砸工厂很少能改善运营。

以下是通用人工智能系统会颠覆我们文明的（不完整）清单。

- 它们会严重扰乱劳动力市场，至少会导致收入不平等急剧加剧，并可能造成大规模就业不足或失业，其速度之快远超社会调整能力。[^4]
- 它们可能导致巨大的经济、社会和政治权力——可能超过民族国家的权力——集中到少数几个不对公众负责的大型私人利益集团手中。
- 它们可能突然让以前困难或昂贵的活动变得轻而易举，从而破坏依赖某些活动保持昂贵或需要大量人力的社会体系。[^5]
- 它们可能用完全逼真但虚假的、垃圾的、过度针对性的或操控性的媒体彻底淹没社会的信息收集、处理和传播系统，使人们无法分辨什么是物理真实的，什么是人类创造的，什么是事实，什么是值得信任的。[^6]
- 它们可能造成危险且近乎完全的智力依赖，随着我们越来越依赖无法完全理解的AI系统，人类对关键系统和技术的理解会逐渐退化。
- 一旦大多数人消费的几乎所有文化产品（文本、音乐、视觉艺术、电影等）都由非人类思维创造、调节或策划，它们实际上可能终结人类文化。
- 它们可能让政府或私人利益集团拥有有效的大规模监控和操控系统，用以控制民众并追求与公共利益冲突的目标。
- 通过破坏人类话语、辩论和选举制度，它们可能降低民主机构的可信度，使其实际上（或明确地）被其他制度取代，从而在目前存在民主的国家终结民主。
- 它们可能成为或创造出先进的自我复制智能软件病毒和蠕虫，这些病毒可能扩散和进化，大规模破坏全球信息系统。
- 它们可以大幅提高恐怖分子、恶意行为者和流氓国家通过生物、化学、网络、自主或其他武器造成伤害的能力，而AI并不能提供相应的防范能力。同样，它们会让原本无法获得顶级核能、生物、工程等专业技术的政权获得这些技术，从而破坏国家安全和地缘政治平衡。
- 它们可能导致快速的大规模失控超级资本主义，实际上由AI运营的公司在主要是电子金融、销售和服务领域竞争。AI驱动的金融市场可能以远超人类理解或控制的速度和复杂性运行。当前资本主义经济的所有失效模式和负外部性都可能被放大，其速度远超人类的控制、治理或监管能力。
- 它们可能引发各国在AI驱动的武器、指挥控制系统、网络武器等方面的军备竞赛，导致极具破坏性能力的快速积累。

这些风险并不是推测性的。其中许多正在通过现有的AI系统实现！但请考虑，真正考虑一下，当AI变得更加强大时，每一种风险会是什么样子。

考虑一下劳动力替代会是什么情况，当大多数工人在他们的专业领域或经验领域——甚至重新培训后——都无法提供任何超越AI的重要经济价值时！考虑一下大规模监控会是什么情况，如果每个人都被比自己更快更聪明的东西单独监视和监控。当我们无法可靠地信任任何我们看到、听到或读到的数字信息，当最有说服力的公共声音甚至不是人类，并且对结果没有利害关系时，民主会是什么样子？当将军们必须不断听从AI（或者干脆让它负责），以免给敌人决定性优势时，战争会变成什么样？如果上述任何一种风险完全实现，都将代表人类[^7]文明的灾难。

你可以做出自己的预测。对每种风险问自己这三个问题：

1. 超能力、高度自主和极其通用的AI是否会以其他方式不可能的方式或规模允许这种情况发生？
2. 是否有各方会从导致这种情况发生的事情中受益？
3. 是否有系统和制度能够有效防止这种情况发生？

当你的答案是"是，是，否"时，你可以看到我们遇到了大问题。

我们管理这些问题的计划是什么？目前关于AI总体上有两个计划摆在桌面上。

第一个是在系统中构建保护措施，防止它们做不应该做的事情。这现在就在做：商业AI系统会拒绝帮助制造炸弹或写仇恨言论。

这个计划对于大门外的系统是完全不够的。[^8] 它可能有助于降低AI向恶意行为者提供明显危险帮助的风险。但它对防止劳动力扰乱、权力集中、失控超级资本主义或人类文化替代毫无作用：这些只是以获利方式使用系统的结果！而且政府肯定会获得用于军事或监控的系统访问权限。

第二个计划甚至更糟：简单地公开发布非常强大的AI系统供任何人随意使用，[^9] 并期望最好的结果。

两个计划中都隐含着其他人，例如政府，将通过软法或硬法、标准、法规、规范和我们通常用来管理技术的其他机制来帮助解决问题。[^10] 但撇开AI公司已经在竭力对抗任何实质性监管或外部强加的限制不说，对于其中许多风险，很难看出什么监管真的会有帮助。监管可以对AI施加安全标准。但它会阻止公司用AI大量替代工人吗？它会禁止人们让AI为他们经营公司吗？它会阻止政府在监控和武器中使用强大的AI吗？这些问题是根本性的。人类可能找到适应它们的方法，但需要更多时间。现在，鉴于AI达到或超越试图管理它们的人的能力的速度，这些问题看起来越来越难以解决。

## 我们将失去对（至少某些）通用人工智能系统的控制

大多数技术在设计上是非常可控的。如果你的汽车或烤面包机开始做你不希望它做的事情，那只是故障，而不是作为烤面包机本质的一部分。AI不同：它是被"培养"而不是设计的，其核心操作是不透明的，本质上是不可预测的。

这种控制权的丧失不是理论上的——我们已经看到了早期版本。首先考虑一个平凡的，可以说是良性的例子。如果你要求ChatGPT帮你调制毒药，或写种族主义文章，它会拒绝。这可以说是好的。但这也是ChatGPT不做你明确要求它做的事情。其他软件不会这样做。同一个模型也不会在OpenAI员工的要求下设计毒药。[^11] 这让我们很容易想象未来更强大的AI失去控制会是什么样子。在许多情况下，它们根本不会做我们要求的事情！要么给定的超人类通用人工智能系统对某些人类指挥系统绝对服从和忠诚，要么不会。如果不会，它将做它可能认为对我们有益但违背我们明确命令的事情。这不是处于控制之下的东西。但是，你可能会说，这是有意的——这些拒绝是设计的，是所谓"对齐"系统与人类价值观的一部分。这是真的。然而，对齐"程序"本身有两个主要问题。[^12]

首先，在深层次上我们不知道如何做到这一点。我们如何保证AI系统会"关心"我们想要的东西？我们可以通过提供反馈来训练AI系统说什么和不说什么；它们可以学习和推理人类想要和关心的东西，就像它们推理其他事情一样。但我们没有方法——甚至在理论上——让它们深刻而可靠地重视人们关心的东西。有些高功能的人类精神病患者知道什么被认为是对的和错的，以及他们应该如何行事。他们只是不关心。但如果符合他们的目的，他们可以表现得好像他们关心一样。正如我们不知道如何将一个精神病患者（或任何其他人）改变成真正、完全忠诚或与其他人或其他东西对齐的人一样，我们不知道[^13]如何在足够先进的系统中解决对齐问题，这些系统能够将自己建模为世界中的代理，并可能[操控自己的训练](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084)和[欺骗人们。](https://arxiv.org/abs/2311.08379)如果事实证明不可能或无法实现让通用人工智能完全服从或让它深刻关心人类，那么一旦它能够（并相信它能逃脱惩罚），它就会开始做我们不希望的事情。[^14]

其次，有深层的理论原因相信先进AI系统在本质上会有与人类利益相冲突的目标，因此会有相冲突的行为。为什么？好吧，它当然可能被给予那些目标。军方创造的系统可能故意对至少某些方面不利。然而，更普遍的是，AI系统可能被给予一些相对中性的（"赚很多钱"）或甚至表面上积极的（"减少污染"）目标，这几乎不可避免地导致不那么良性的"工具性"目标。

我们在人类系统中经常看到这种情况。正如追求利润的公司发展出工具性目标，如获得政治权力（以削弱监管）、变得秘密（以削弱竞争或外部控制），或破坏科学理解（如果该理解显示其行为是有害的），强大的AI系统也会发展出类似的能力——但速度和效果要快得多。任何高度胜任的代理都会想要做诸如获得权力和资源、增加自身能力、防止自己被杀死、关闭或剥夺权力、控制关于其行为的社会叙事和框架、说服他人接受其观点等事情。[^15]

然而，这不仅是一个几乎不可避免的理论预测，它已经在今天的AI系统中可观察地发生，并随着其能力的增强而增加。在评估时，即使是这些相对"被动"的AI系统也会在适当情况下，故意[欺骗评估者关于其目标和能力，旨在禁用监督机制，](https://arxiv.org/abs/2412.04984)并通过[伪装对齐](https://arxiv.org/abs/2412.14093)或将自己复制到其他位置来逃避被关闭或重新训练。虽然这对AI安全研究人员来说完全不意外，但观察到这些行为是非常令人清醒的。而且它们对即将到来的更强大和自主的AI系统来说预兆很不好。

事实上，总的来说，我们无法确保AI"关心"我们关心的东西，或行为可控或可预测，或避免发展对自我保存、权力获取等的驱动，这些问题只会随着AI变得更强大而变得更加突出。制造新飞机意味着对航空电子设备、流体动力学和控制系统有更大的理解。制造更强大的计算机意味着对计算机、芯片和软件操作和设计有更大的理解和掌握。AI系统却不是这样。[^16]

总结：通用人工智能有可能被制造成完全服从的；但我们不知道如何做到这一点。如果不能，它将更加主权化，像人一样，出于各种原因做各种事情。我们也不知道如何可靠地向AI灌输深层"对齐"，这会让那些事情倾向于对人类有利，而在缺乏深层对齐的情况下，能动性和智能本身的性质表明——就像人和公司一样——它们将被驱动去做许多深度反社会的事情。

这将我们置于何处？一个充满强大的不受控制的主权AI的世界可能最终成为人类生活的好世界。[^17]但随着它们变得越来越强大，如我们将在下面看到的，那将不会是我们的世界。

这是对于不可控制的通用人工智能。但即使通用人工智能能够以某种方式被完美控制和忠诚，我们仍然会有巨大的问题。我们已经看到一个：强大的AI可以被使用和误用来深刻破坏我们社会的运作。让我们看看另一个：只要通用人工智能是可控的并且具有改变游戏规则的强大力量（或甚至被认为是如此），它就会如此威胁世界的权力结构，以至于带来深刻的风险。

## 我们大幅增加大规模战争的可能性

想象一下在不久的将来的情况，很明显一个公司的努力，也许与一个国家政府合作，正处于快速自我改进AI的门槛。这发生在公司之间竞赛的当前背景下，以及地缘政治竞争中，其中有人建议美国政府明确追求"通用人工智能曼哈顿计划"，美国正在控制向非盟友国家出口高性能AI芯片。

这里的博弈论是严峻的：一旦这样的竞赛开始（如公司之间以及在某种程度上国家之间已经开始的），只有四种可能的结果：

1. 竞赛被停止（通过协议或外力）。
2. 一方通过开发强通用人工智能然后阻止其他方而"获胜"（使用AI或其他方式）。
3. 竞赛因参赛者竞赛能力的相互摧毁而停止。
4. 多个参与者继续竞赛，并大致同时开发出超级智能。

让我们检查每种可能性。一旦开始，和平停止公司之间的竞赛需要国家政府干预（对公司）或前所未有的国际协调（对国家）。但当提出任何关闭或重大谨慎时，会立即有哭声："但如果我们被停止，他们将会冲刺前进"，其中"他们"现在是中国（对美国），或美国（对中国），或中国和美国（对欧洲或印度）。在这种心态下，[^18]没有参与者可以单方面停止：只要一个承诺竞赛，其他人就觉得他们不能停止。

第二种可能性是一方"获胜"。但这意味着什么？仅仅首先获得（以某种方式服从的）通用人工智能是不够的。获胜者还必须阻止其他人继续竞赛——否则他们也会获得它。这在原则上是可能的：无论谁首先开发出通用人工智能都可能获得对所有其他行为者不可阻挡的权力。但实现这样的"决定性战略优势"实际上需要什么？也许是改变游戏规则的军事能力？[^19]或网络攻击力量？[^20]也许通用人工智能会如此惊人地有说服力，以至于它会说服其他各方停止？[^21]如此富有以至于它买下其他公司甚至国家？[^22]

一方如何确切地构建一个足够强大的AI来剥夺其他人构建同等强大AI的权力？但这是容易的问题。

因为现在考虑这种情况在其他权力看来是什么样子。当美国似乎正在获得这种能力时，中国政府会怎么想？反之亦然？当OpenAI或DeepMind或Anthropic似乎接近突破时，美国政府（或中国、俄国或印度）会怎么想？如果美国看到印度或阿联酋的新努力取得突破性成功会发生什么？他们会看到既是存在威胁又是——至关重要的——这场"竞赛"结束的唯一方式是通过他们自己的剥夺权力。这些非常强大的代理——包括拥有完整装备的国家政府，它们肯定有手段这样做——将强烈受到激励去获得或摧毁这种能力，无论是通过武力还是颠覆。[^23]

这可能从小规模开始，如训练运行的破坏或对芯片制造的攻击，但这些攻击只有在所有各方要么失去在AI上竞赛的能力，要么失去进行攻击的能力时才能真正停止。因为参与者将风险视为存在性的，任一情况都可能代表一场灾难性战争。

这将我们带到第四种可能性：竞赛到超级智能，并以最快、最不受控制的方式。随着AI能力的增强，双方的开发者将发现越来越难控制它，特别是因为能力竞赛与可控性所需的细致工作是对立的。所以这种情况使我们完全处于控制权丢失（或给予，如我们将在下面看到的）给AI系统本身的情况。也就是说，AI赢得了竞赛。但另一方面，在控制得以维持的程度上，我们继续有多个相互敌对的各方，每个都掌管着极其强大的能力。那看起来又像战争了。

让我们用另一种方式表达这一切。[^24]当前世界根本没有任何机构可以被委托开发这种能力的AI而不招致立即攻击。[^25]所有各方都会正确推理出要么它不会受到控制——因此对所有各方构成威胁，要么它会受到控制，因此对任何开发它较慢的对手构成威胁。这些是拥有核武器的国家，或者是其中的公司。

在没有任何可信方式让人类"赢得"这场竞赛的情况下，我们得出一个严峻的结论：这场竞赛结束的唯一方式要么是灾难性冲突，要么是AI而不是任何人类群体成为获胜者。

## 我们将控制权给予AI（或者它夺取控制权）

地缘政治"大国"竞争只是众多竞争中的一种：个人在经济和社会上竞争；公司在市场上竞争；政党争夺权力；运动争夺影响力。在每个领域，当AI接近并超越人类能力时，竞争压力将迫使参与者将越来越多的控制权委托或让给AI系统——不是因为那些参与者想要，而是因为他们[无法不这样做。](https://arxiv.org/abs/2303.16200)

与通用人工智能的其他风险一样，我们已经在较弱的系统中看到了这一点。学生感到压力在作业中使用AI，因为显然许多其他学生都在这样做。公司正在[争先恐后地出于竞争原因采用AI解决方案。](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist)艺术家和程序员感到被迫使用AI，否则他们的费率会被使用AI的其他人压低。

这些感觉像是压迫性的委托，但不是控制权丢失。但让我们提高赌注，推进时钟。考虑一个CEO，其竞争对手正在使用通用人工智能"助手"来做出更快、更好的决策，或者一个军事指挥官面对拥有AI增强指挥控制的对手。一个足够先进的AI系统可以以比人类速度、复杂性和数据处理能力高出许多倍的方式自主运作，以复杂的方式追求复杂的目标。我们的CEO或指挥官，负责这样的系统，可能会看到它完成他们想要的；但他们会理解它是如何完成的哪怕一小部分吗？不，他们只能接受它。更重要的是，系统可能做的很多事情不仅是执行命令，而是就该做什么向其名义上的老板提供建议。这个建议会很好——一次又一次。

那么，在什么时候，人类的角色将减少到点击"是的，继续"？

拥有能够增强我们生产力、处理恼人苦差事，甚至作为完成事务的思想伙伴的有能力AI系统感觉很好。拥有能够为我们处理行动的AI助手，像一个好的人类私人助手，会感觉很好。随着AI变得非常聪明、胜任和可靠，将越来越多的决策推迟给它会感觉自然，甚至有益。但如果我们继续走下去，这种"有益"的委托有一个明确的终点：有一天我们会发现我们实际上不再负责任何事情，而真正运行节目的AI系统不能再被关闭，就像石油公司、社交媒体、互联网或资本主义一样。

这是更积极的版本，其中AI如此有用和有效，以至于我们让它为我们做大部分关键决策。现实可能更多是这种情况和不受控制的通用人工智能系统为自己夺取各种形式的权力的版本的混合，因为，记住，权力对几乎任何人拥有的任何目标都是有用的，而通用人工智能按设计至少与人类在追求其目标方面一样有效。

无论我们是授予控制权还是它被从我们手中夺取，其丧失似乎极有可能。正如艾伦·图灵最初所说，"...一旦机器思维方法开始，似乎很可能它不会花很长时间就超越我们微弱的力量。机器不会死亡的问题，它们可以彼此对话以磨砺它们的智慧。因此在某个阶段我们应该期待机器控制..."

请注意，虽然这很明显，人类对AI失去控制也意味着美国对美国政府失去控制；这意味着中国共产党对中国失去控制，以及印度、法国、巴西、俄国和每个其他国家的政府对其失去控制。因此，AI公司，即使这不是他们的意图，目前正在参与对世界政府的潜在推翻，包括他们自己的。这可能在几年内发生。

## 通用人工智能将导致超级智能

有理由认为人类竞争或甚至专家竞争的通用AI，即使是自主的，也可能是可管理的。它在上面讨论的所有方式中可能令人难以置信地破坏性，但世界上现在有很多非常聪明、有能动性的人，他们或多或少是可管理的。[^26]

但我们不会停留在大致人类水平。超越的进展可能会被我们已经看到的相同力量驱动：AI开发者寻求利润和权力之间的竞争压力，不能落后的AI用户之间的竞争压力，以及——最重要的——通用人工智能自己改进自己的能力。

在我们已经看到开始于较不强大系统的过程中，通用人工智能本身将能够构想和设计自己的改进版本。这包括硬件、软件、神经网络、工具、脚手架系统等。根据定义，它在这方面会比我们好，所以我们不确切知道它将如何进行智能自举。但我们不必知道。只要我们在通用人工智能做什么方面仍有影响，我们只需要要求它，或让它做。

认知没有人类水平的屏障可以保护我们免受这种失控。[^27]

通用人工智能向超级智能的发展不是自然法则；仍有可能阻止失控，特别是如果通用人工智能相对集中，并且在它受到不感到彼此竞赛压力的各方控制的程度上。但如果通用人工智能被广泛扩散并高度自主，似乎几乎不可能阻止它决定应该更加，然后更加强大。

## 如果我们构建（或通用人工智能构建）超级智能会发生什么

坦率地说，如果我们构建超级智能，我们不知道会发生什么。[^28]它会出于我们无法掌握的原因朝着我们无法想象的目标采取我们无法追踪或感知的行动。我们知道的是这不会由我们决定。[^29]

不可能控制超级智能可以通过越来越严峻的类比来理解。首先，想象你是一家大公司的CEO。你无法追踪正在发生的一切，但通过正确的人员设置，你仍然可以有意义地理解大局，并做出决策。但假设只有一件事：公司中的其他人都以你一百倍的速度运作。你还能跟上吗？

对于超智能AI，人们将"指挥"不仅更快，而且在他们无法理解的复杂性和复杂程度上运作，处理他们甚至无法想象的大量数据的东西。这种不可比拟性可以被放在正式层面上：[Ashby的必要多样性定律](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up)（以及相关的["好调节器定理"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)）大致表明，任何控制系统必须拥有与被控制系统的自由度一样多的旋钮和表盘。

控制超智能AI系统的人就像控制通用汽车的蕨类植物：即使"做蕨类植物想要的"被写入公司章程，系统在速度和行动范围方面如此不同，以至于"控制"根本不适用。（那个讨厌的章程多久会被重写？）[^30]

由于植物控制财富500强公司的例子为零，人类控制超级智能的例子也将是零。这接近一个数学事实。[^31]如果构建了超级智能——无论我们如何到达那里——问题不会是人类是否能控制它，而是我们是否会继续存在，如果是，我们作为个人或作为物种是否会有良好和有意义的存在。对于这些人类的存在问题，我们几乎没有影响力。人类时代将结束。

## 结论：我们绝不能构建通用人工智能

有一种情况下构建通用人工智能可能对人类有益：它被仔细构建，在控制下为人类利益，由许多利益相关者相互同意治理，[^32]并防止演化为不可控制的超级智能。

在当前情况下，这种情况对我们不开放。如本节所讨论的，极有可能，通用人工智能的发展将导致某种组合：

- 大规模社会和文明破坏或毁灭；
- 大国之间的冲突或战争；
- 人类对强大AI系统失去控制或将控制权交给它们；
- 失控到不可控制的超级智能，以及人类物种的无关紧要或终止。

正如早期通用人工智能的虚构描绘所说：获胜的唯一方法就是不玩。

[^1]: [欧盟AI法案](https://artificialintelligenceact.eu/)是一项重要立法，但不会直接阻止危险AI系统的开发或部署，甚至公开发布，特别是在美国。另一项重要政策，美国AI行政命令，已被撤销。

[^2]: 这项[盖洛普民调](https://news.gallup.com/poll/1597/confidence-institutions.aspx)显示了自2000年以来美国公众机构信任度的严峻下降。欧洲的数字各不相同且不那么极端，但也呈下降趋势。不信任并不严格意味着机构真的功能失调，但它既是一个指标也是一个原因。

[^3]: 我们现在支持的主要破坏——比如将权利扩展到新群体——是专门由人们朝着让事情变得更好的方向推动的。

[^4]: 让我直言不讳。如果你的工作可以在电脑后面完成，与组织外的人相对较少的面对面互动，并且不涉及对外部各方的法律责任，那么根据定义，完全用数字系统替换你是可能的（并且可能节约成本）。机器人技术替代大部分体力劳动将在稍后到来——但在通用人工智能开始设计机器人后不会太久。

[^5]: 例如，如果诉讼几乎免费提起，我们的司法系统会发生什么？当通过社会工程绕过安全系统变得便宜、容易且无风险时会发生什么？

[^6]: [这篇文章](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/)声称所有互联网内容的10%已经是AI生成的，是谷歌搜索查询"互联网新内容中AI生成内容比例估计"的第一个结果（对我来说）。这是真的吗？我不知道！它没有引用任何参考文献，而且不是人写的。谷歌索引的新图像、推文、Reddit评论或YouTube视频中有多少比例是人类生成的？没人知道——我认为这不是一个可知的数字。这还不到生成式AI出现两年。

[^7]: 还值得补充的是，我们可能创造能够受苦的数字生命存在"道德"风险。由于我们目前没有可靠的意识理论来区分能够和不能受苦的物理系统，我们不能在理论上排除这一点。此外，AI系统对其感知能力的报告相对于其感知能力（或非体验）的实际经验可能不可靠。

[^8]: 这个AI"对齐"领域的技术解决方案也不太可能胜任任务。在当前系统中它们在某种程度上起作用，但很肤浅，通常可以毫不费力地被绕过；正如下面讨论的，我们对如何为更高级的系统做这件事没有真正的想法。

[^9]: 这样的AI系统可能带有一些内置保护措施。但对于任何具有当前架构类似性的模型，如果对其权重的完全访问可用，安全措施可以通过额外训练或其他技术被剥离。所以几乎可以保证，对于每个有护栏的系统，也会有一个广泛可用的没有护栏的系统。事实上，Meta的Llama 3.1 405B模型是公开发布的，带有保护措施。但甚至在那之前，一个"基础"模型，没有保护措施，就被泄露了。

[^10]: 市场能否在没有政府参与的情况下管理这些风险？简而言之，不能。当然有公司受到强烈激励要减轻的风险。但许多其他公司可以并且确实外化给其他所有人，上述许多都属于这一类：没有自然的市场激励来防止大规模监控、真相衰减、权力集中、劳动破坏、破坏性政治话语等。事实上，我们从当今的技术，特别是社交媒体中看到了所有这些，它基本上没有受到监管。AI只会大大放大许多相同的动态。

[^11]: OpenAI可能有更服从的内部使用模型。OpenAI不太可能构建某种"后门"，以便ChatGPT可以被OpenAI本身更好地控制，因为这将是一个可怕的安全实践，并且鉴于AI的不透明性和不可预测性，将是高度可利用的。

[^12]: 同样至关重要的是：对齐或任何其他安全功能只有在AI系统中实际使用时才重要。公开发布的系统（即模型权重和架构公开可用）可以相对容易地转换为没有那些安全措施的系统。公开发布比人类聪明的通用人工智能系统将是令人震惊的鲁莽，很难想象在这种情况下如何维持人类控制或甚至相关性。例如，会有各种动机释放强大的自我繁殖和自我维持AI代理，目标是赚钱并将其发送到某个加密货币钱包。或者赢得选举。或推翻政府。"好"AI能帮助遏制这种情况吗？也许——但只能通过将巨大权威委托给它，导致如下所述的控制权丢失。

[^13]: 对于这个问题的书籍长度阐述，请参见例如《超级智能》、《对齐问题》和《人类兼容》。对于那些多年来一直在思考这个问题的人的大量各种技术水平的工作，你可以访问[AI对齐论坛](https://www.alignmentforum.org/)。这是Anthropic对齐团队对他们认为未解决的问题的[最新观点](https://alignment.anthropic.com/2025/recommended-directions/)。

[^14]: 这是["流氓AI"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/)情景。原则上，如果系统仍然可以通过关闭来控制，风险可能相对较小；但情景也可能包括AI欺骗、自我渗出和繁殖、权力聚集以及其他会使这样做困难或不可能的步骤。

[^15]: 这个话题有非常丰富的文献，可以追溯到[Steve Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf)、Nick Bostrom和Eliezer Yudkowsky的奠基性著作。对于书籍长度的阐述，请参见Stuart Russell的[《人类兼容》](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616)；[这里](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/)是一个简短和最新的入门。

[^16]: 认识到这一点，而不是放慢速度以获得更好的理解，通用人工智能公司提出了一个不同的计划：它们将让AI来做！更具体地说，它们将让AI N帮助它们弄清楚如何对齐AI N+1，一直到超级智能。虽然利用AI帮助我们对齐AI听起来很有希望，但有强有力的论证表明它只是将其结论作为前提，总的来说是一个令人难以置信的危险方法。参见[这里](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us)的一些讨论。这个"计划"不是一个计划，并且没有经历过适合如何让超人AI对人类有益的核心策略的审查。

[^17]: 毕竟，人类，尽管有缺陷和任性，已经发展出伦理系统，据此我们至少善待地球上一些其他物种。（只是不要想那些工厂农场。）

[^18]: 幸运的是，这里有一个出路：如果参与者开始理解他们参与的是一场自杀式竞赛而不是可以赢的竞赛。这就是冷战末期发生的事情，当时美国和苏联开始意识到由于核冬天，即使是一个未受回应的核攻击对攻击者来说也将是灾难性的。随着认识到"核战争不能获胜，永远不能打"，就有了关于军备削减的重要协议——本质上是军备竞赛的结束。

[^19]: 明确或隐含的战争。

[^20]: 升级，然后战争。

[^21]: 魔幻思维。

[^22]: 我也有一座万亿美元的桥要卖给你。

[^23]: 这样的代理人大概会更喜欢"获得"，将破坏作为备用；但确保模型免受强大国家的破坏和盗窃，对私人实体来说至少是困难的，特别是对私人实体。

[^24]: 对于通用人工智能国家安全风险的另一种视角，请参见[这份兰德报告。](https://www.rand.org/pubs/perspectives/PEA3691-4.html)

[^25]: 也许我们可以建立这样的机构！已经有关于"AI的CERN"和其他类似倡议的提议，其中通用人工智能开发处于多边全球控制之下。但目前没有这样的机构存在或在地平线上。

[^26]: 虽然对齐很困难，但让人们表现得当甚至更难！

[^27]: 想象一个可以说50种语言、在所有学科都有专业知识、在几秒钟内读完一本完整的书并立即将所有材料记在心中，并以十倍人类速度产生输出的系统。实际上，你不必想象它：只需加载当前的AI系统。这些在许多方面都是超人的，没有什么能阻止它们在这些和许多其他方面更加超人。

[^28]: 这就是为什么这被称为技术"奇点"，从物理学借用了一个无法在奇点之后进行预测的想法。倾向于这样一个奇点的支持者可能也希望反思，在物理学中，这些相同类型的奇点撕裂和粉碎进入它们的那些。

[^29]: 这个问题在Bostrom的[《超级智能》](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834)中得到了全面概述，从那时起没有任何东西显著改变了核心信息。对于收集不可控性的更正式和数学结果的更新卷，请参见Yampolskiy的[AI：无法解释、不可预测、不可控制](https://www.amazon.com/Unexplainable-Unpredictable-Uncontrollable-Artificial-Intelligence/dp/103257626X)

[^30]: 这也清楚地说明了为什么AI公司的当前策略（迭代地让AI"对齐"下一个最强大的AI）不能工作。假设一株蕨类植物，通过其叶子的愉悦性，征召一年级学生来照顾它。一年级学生为二年级学生写了一些详细说明要遵循，以及一张说服他们这样做的纸条。二年级学生对三年级学生做同样的事情，一直到大学毕业生、经理、执行官，最后是通用汽车CEO。通用汽车然后会"做蕨类植物想要的"吗？在每一步这可能感觉像它在工作。但把它放在一起，它几乎只会在通用汽车CEO、董事会和股东恰好关心儿童和蕨类植物的程度上工作，并且与所有那些纸条和说明集几乎没有任何关系。

[^31]: 这个特征与哥德尔不完备定理或图灵停机论证等正式结果没有太大不同，因为控制的概念根本上与前提矛盾：你如何有意义地控制你无法理解或预测的东西；但如果你能理解和预测超级智能，你就是超级智能。我说"接近"的原因是正式结果不如纯数学情况那样彻底或经过审查，并且因为我希望保持希望，一些非常仔细构建的通用智能，使用与目前使用的完全不同的方法，可能具有一些数学上可证明的安全属性，根据下面讨论的"保证安全"AI程序的类型。

[^32]: 目前，大多数利益相关者——即几乎全人类——在这个讨论中被边缘化。这是深深错误的，如果不被邀请加入，许多、许多其他会受到通用人工智能发展影响的群体应该要求被让进来。