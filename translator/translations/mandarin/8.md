# 第八章 - 如何不构建通用人工智能

通用人工智能并非不可避免——如今我们正站在一个分岔路口。本章提出了一个关于如何防止其被构建的提案。

如果我们目前所走的道路很可能导致人类文明的终结，我们该如何改变方向？

假设阻止通用人工智能和超级智能发展的愿望变得广泛而强烈，[^1] 因为人们普遍认识到通用人工智能将会吸收而非授予权力，对社会和人类构成深刻威胁。那么我们该如何关闭大门？

目前我们只知道一种*制造*强大通用AI的方法，那就是通过深度神经网络的大规模算力。由于这些是极其困难且昂贵的工作，从某种意义上说*不*去做它们反而是容易的。[^2] 但我们已经看到了推动通用人工智能发展的力量，以及使任何一方都很难单方面停止的博弈论动态。因此，这需要外部干预（即政府）来阻止企业，以及政府间的协议来约束自己。[^3] 这会是什么样子呢？

首先区分必须*防止*或*禁止*的AI发展和必须*管理*的AI发展是有用的。前者主要是向超级智能的失控发展。[^4] 对于被禁止的发展，定义应该尽可能清晰，验证和执行都应该是实际可行的。必须*管理*的将是通用的、强大的AI系统——我们已经拥有这些系统，它们将有许多灰色地带、细微差别和复杂性。对于这些，强有力的有效机构至关重要。

我们也可以有效地划分必须在国际层面（包括地缘政治竞争对手或敌手之间）解决的问题 [^5] 和单个司法管辖区、国家或国家集团可以管理的问题。被禁止的发展主要属于"国际"类别，因为对某项技术发展的本地禁令通常可以通过改变地点来规避。[^6]

最后，我们可以考虑工具箱中的工具。有很多，包括技术工具、软法（标准、规范等）、硬法（法规和要求）、责任制、市场激励等等。让我们特别关注一个AI特有的工具。

## 算力安全与治理

治理高性能AI的核心工具将是其所需的硬件。软件传播容易，边际生产成本接近零，可以轻易跨越国界，并可以瞬间修改；这些特点硬件都不具备。然而，正如我们所讨论的，在AI系统的训练和推理过程中，要实现最强能力的系统都需要大量的"算力"。算力可以很容易地量化、核算和审计，一旦制定出良好的规则，歧义性相对较小。最关键的是，大量计算就像浓缩铀一样，是一种非常稀缺、昂贵且难以生产的资源。尽管计算机芯片无处不在，但AI所需的硬件昂贵且制造极其困难。[^7]

使AI专用芯片作为稀缺资源远*比*铀更易于管理的是，它们可以包含基于硬件的安全机制。大多数现代手机和一些笔记本电脑都有专门的片上硬件功能，使它们能够确保只安装经过批准的操作系统软件和更新，在设备上保留和保护敏感的生物识别数据，并且在丢失或被盗时可以使除其所有者之外的任何人都无法使用。在过去几年中，这种硬件安全措施已经变得非常成熟并得到广泛采用，通常被证明相当安全。

这些功能的关键新颖之处在于它们使用密码学将硬件和软件绑定在一起。[^8] 也就是说，仅仅拥有特定的计算机硬件并不意味着用户可以通过应用不同的软件来随意使用它。这种绑定还提供了强大的安全性，因为许多攻击需要突破*硬件*而不仅仅是*软件*安全。

最近的几份报告（例如来自[GovAI及其合作者](https://www.governance.ai/post/computing-power-and-the-governance-of-ai)、[CNAS](https://www.cnas.org/publications/reports/secure-governable-chips)和[RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)）指出，嵌入在尖端AI相关计算硬件中的类似硬件功能可以在AI安全和治理中发挥极其有用的作用。它们为"管理者"[^9] 提供了一些人们可能没有想到的可用或甚至可能的功能。举一些关键例子：

- *地理定位*：系统可以设置成让芯片具有已知位置，并可以根据位置采取不同行动（或完全关闭）。[^10]
- *允许列表连接*：每个芯片都可以配置硬件强制执行的允许列表，包含它可以与之联网的特定其他芯片，并且无法与不在此列表上的任何芯片连接。[^11] 这可以限制芯片通信集群的规模。[^12]
- *计量推理或训练（和自动关闭开关）*：管理者可以只许可用户执行一定数量的训练或推理（以时间、FLOP或可能的令牌为单位），之后需要获得新的许可。如果增量很小，那么就需要对模型进行相对连续的重新许可。然后可以通过拒绝提供许可信号来简单地"关闭"模型。[^13]
- *速度限制*：阻止模型以高于管理者或其他方式确定的某个限制的推理速度运行。这可以通过有限的允许列表连接集或更复杂的方式来实现。
- *认证训练*：训练过程可以产生密码学安全的证明，证明在生成模型时使用了特定的代码集、数据和算力使用量。

## 如何不构建超级智能：训练和推理算力的全球限制

有了这些考虑——特别是关于计算的考虑——我们可以讨论如何关闭通向人工超级智能的大门；然后我们将转向防止完全的通用人工智能，以及管理在不同方面接近和超越人类能力的AI模型。

第一个要素当然是理解超级智能将无法控制，其后果根本无法预测。至少中国和美国必须独立决定，出于这个或其他目的，不构建超级智能。[^14] 然后需要它们与其他国家之间的国际协议，具有强有力的验证和执行机制，以向各方保证其竞争对手不会背叛并决定孤注一掷。

为了可验证和可执行，这些限制应该是硬性限制，并尽可能明确。这似乎是一个几乎不可能解决的问题：在全球范围内限制具有不可预测属性的复杂软件的能力。幸运的是，情况比这要好得多，因为使先进AI成为可能的正是那个东西——大量算力——要控制得多得多。虽然它可能仍然允许一些强大而危险的系统，但*失控的超级智能*很可能可以通过对神经网络算力使用量的硬性上限，以及对AI系统（连接的神经网络和其他软件）可以执行的推理量的速率限制来防止。下面提出了一个具体版本。

可能看起来对AI算力设置硬性全球限制需要大量的国际协调和侵犯隐私的监控。幸运的是，并非如此。极其[紧密和瓶颈化的供应链](https://arxiv.org/abs/2402.08797)提供了一旦通过法律（无论是法律还是行政命令）设定限制，验证对该限制的合规性只需要少数大公司的参与和合作。[^15]

这样的计划有许多非常理想的特征。它的侵入性最小，因为只有少数大公司受到要求约束，只有相当大的计算集群才会受到管理。相关芯片已经包含第一个版本所需的硬件能力。[^16] 实施和执行都依赖于标准的法律限制。但这些都得到硬件使用条款和硬件控制的支持，大大简化了执行并防止公司、私人团体甚至国家的作弊。硬件公司对其硬件使用施加远程限制，以及从外部锁定/解锁特定能力，有充分的先例，[^17] 甚至包括数据中心的高性能CPU。[^18] 即使对于受影响的相当小部分硬件和组织，监督也可以仅限于遥测，无需直接访问数据或模型本身；此软件可以开放供检查，以证明没有记录额外数据。该方案是国际性和合作性的，并且相当灵活和可扩展。由于限制主要针对硬件而不是软件，它对AI软件开发和部署的方式相对不可知，并与各种范式兼容，包括旨在对抗AI驱动的权力集中的更"去中心化"或"公共"AI。

基于算力的大门关闭也有缺点。首先，它远非AI治理问题的完整解决方案。其次，随着计算机硬件变得更快，该系统将在越来越小的集群（甚至单个GPU）中"捕获"越来越多的硬件。[^19] 也有可能由于算法改进，甚至更低的算力限制在时间上是必要的，[^20] 或者算力数量变得基本无关紧要，关闭大门将需要更详细的基于风险或基于能力的AI治理制度。第三，无论保证如何以及受影响实体数量多么少，这样的系统必然会在隐私和监控等问题上产生阻力。[^21]

当然，在短时间内开发和实施算力限制治理方案将是相当具有挑战性的。但这绝对是可行的。

## A-G-I：作为风险和政策基础的三重交集

现在让我们转向通用人工智能。这里的硬性界限和定义更加困难，因为我们当然拥有人工的、通用的智能，而且按照任何现有定义，并不是每个人都会同意它是否或何时存在。此外，算力或推理限制是一个相当钝的工具（算力是能力的代理，然后能力又是风险的代理），除非它相当低，否则不太可能防止足够强大的通用人工智能造成社会或文明破坏或急性风险。

我已经论证过，最急性的风险来自极高能力、高度自主性和巨大通用性的三重交集。这些是——如果它们被开发出来的话——必须极其小心管理的系统。通过为结合所有三种属性的系统创建严格标准（通过责任制和监管），我们可以引导AI发展走向更安全的替代方案。

与其他可能对消费者或公众造成伤害的行业和产品一样，AI系统需要有效且有权力的政府机构进行仔细监管。这种监管应该认识到通用人工智能的内在风险，并防止开发不可接受风险的高性能AI系统。[^22]

然而，大规模监管，特别是具有必然会遭到行业反对的真正威慑力的监管，[^23] 需要时间 [^24] 以及认为其必要的政治信念。[^25] 考虑到进展的速度，这可能需要比我们可用时间更多的时间。

在更快的时间尺度上，在监管措施正在制定的同时，我们可以通过澄清和提高最危险系统的责任水平，给公司必要的激励来（a）停止非常高风险的活动和（b）开发评估和减轻风险的综合系统。这个想法是对处于高度自主-通用-智能三重交集的系统施加最高水平的责任——严格的，在某些情况下是个人刑事责任——但为缺少其中一个属性或保证其中一个属性可管理的系统提供更典型的基于过错责任的"安全港"。也就是说，例如，一个通用且自主但"弱"的系统（如一个有能力且值得信赖但有限的个人助理）将受到较低水平的责任。同样，像自动驾驶汽车这样狭窄且自主的系统仍将受到它已经受到的重大监管，但不会受到强化责任。类似地，对于高能力且通用但"被动"且基本无法独立行动的系统也是如此。缺乏三个属性中的*两个*的系统更易管理，安全港会更容易获得。这种方法反映了我们如何处理其他潜在危险技术：[^26] 对更危险配置的更高责任为更安全的替代方案创造了自然激励。

如此高水平责任的默认结果，它将AGI风险*内化*到公司而不是转嫁给公众，很可能（也希望！）是公司在能够真正使其值得信赖、安全且可控之前，根本不开发完全的通用人工智能，因为*他们自己的领导层*是面临风险的一方。（如果这还不够，澄清责任的立法还应该明确允许禁令救济，即法官命令停止明显处于危险区域并可能构成公共风险的活动。）随着监管到位，遵守监管可以成为安全港，来自AI系统的低自主性、狭窄性或弱性的安全港可以转换为相对较轻的监管制度。

## 大门关闭的关键条款

基于上述讨论，本节提供关键条款的提案，这些条款将实施和维持对完全通用人工智能和超级智能的禁止，以及管理接近完全通用人工智能阈值的人类竞争或专家竞争的通用AI。[^27] 它有四个关键部分：1）算力核算和监督，2）AI训练和运营中的算力上限，3）责任框架，以及4）包括硬性监管要求的分层安全标准。这些将在下面简洁描述，在三个附表中给出进一步的细节或实施示例。重要的是，注意这些远非治理先进AI系统所必需的全部；虽然它们将有额外的安全效益，但它们旨在关闭通往智能失控的大门，并将AI发展重新导向更好的方向。

### 1\. 算力核算和透明度

- 标准组织（例如美国的NIST，随后是国际上的ISO/IEEE）应该制定详细的技术标准，用于AI模型训练和运行中使用的总算力（以FLOP为单位），以及它们运行的速度（以FLOP/s为单位）。附录A给出了这可能的样子的详细信息。[^28]
- 进行大规模AI训练的司法管辖区应通过新立法或在现有权限下 [^29] 施加要求，即计算并向监管机构或其他机构报告所有超过10<sup>25</sup> FLOP或10<sup>18</sup> FLOP/s阈值的模型在训练和运行中使用的总FLOP。[^30]
- 这些要求应该分阶段实施，最初要求季度有据可查的善意估算，后续阶段要求逐步提高标准，直到每个模型*输出*附带密码学认证的总FLOP和FLOP/s。
- 这些报告应该得到有据可查的生成每个AI输出所使用的边际能源和财务成本估算的补充。

理由：这些精确计算和透明报告的数字将为训练和运营上限以及更高责任措施的安全港提供基础（见附录C和D）。

### 2\. 训练和运营算力上限

- 托管AI系统的司法管辖区应对任何AI模型输出的总算力施加硬性限制，从10<sup>27</sup> FLOP [^31] 开始，并酌情调整。
- 托管AI系统的司法管辖区应对AI模型输出的算力速率施加硬性限制，从10<sup>20</sup> FLOP/s开始，并酌情调整。

理由：总计算虽然非常不完美，但它是AI能力（和风险）的代理，是具体可测量和可验证的，因此为限制能力提供了硬性底线。附录B给出了具体的实施提案。

### 3\. 危险系统的强化责任

- 应通过立法在法律上澄清，创造和运营 [^32] 高度通用、有能力且自主的先进AI系统，应受到严格的、连带责任，而不是单方过错责任。[^33]
- 应该提供一个法律程序来制定肯定性安全案例，这将为计算量小、弱、狭窄、被动或具有足够安全、保障和可控性保证的系统提供严格责任的安全港。
- 应概述阻止构成公共危险的AI训练和推理活动的禁令救济的明确途径和条件集。

理由：AI系统不能承担责任，因此我们必须让人类个体和组织为其造成的伤害承担责任（责任制）。[^34] 不可控的通用人工智能对社会和文明构成威胁，在没有安全案例的情况下，应被视为异常危险。将强大模型足够安全以至于不被视为"异常危险"的举证责任放在开发者身上，激励安全开发，以及透明度和记录保存以获得这些安全港。然后监管可以在责任威慑不足的情况下防止伤害。最后，AI开发者已经对其造成的损害承担责任，因此在法律上澄清最具风险系统的责任可以立即完成，无需制定高度详细的标准；这些可以随时间发展。详细信息见附录C。

### 4\. AI的安全监管

解决AI大规模急性风险的监管系统至少需要：

- 确定或创建适当的监管机构集，可能是一个新机构；
- 综合风险评估框架；[^35]
- 部分基于风险评估框架的肯定性安全案例框架，由开发者制定，并由*独立*团体和机构进行审计；
- 分层许可系统，层级跟踪能力水平。[^36] 许可证将基于安全案例和审计授予，用于系统的开发和部署。要求从低端的通知到高端的开发前定量安全、保障和可控性保证不等。这些将防止系统在被证明安全之前发布，并禁止开发本质上不安全的系统。附录D提供了此类安全标准可能包含的内容的提案。
- 将此类措施提升到国际层面的协议，包括协调规范和标准的国际机构，以及可能审查安全案例的国际机构。

理由：最终，责任制不是防止新技术对公众构成大规模风险的正确机制。就像其他对公众构成风险的主要行业一样，AI需要具有授权监管机构的综合监管。[^37]

防止其他普遍但不太急性风险的监管可能因司法管辖区而异。关键是避免开发风险如此之大以致这些风险无法管理的AI系统。

## 然后呢？

在接下来的十年中，随着AI变得更加普及，核心技术不断进步，可能会发生两件关键事情。首先，现有强大AI系统的监管将变得更加困难，但更加必要。至少一些解决大规模安全风险的措施可能需要在国际层面达成协议，由各个司法管辖区基于国际协议执行规则。

其次，随着硬件变得更便宜、更具成本效益，训练和运营算力上限将变得更难维持；随着算法和架构的进步，它们也可能变得不那么相关（或需要更加严格）。

控制AI将变得更加困难并不意味着我们应该放弃！实施本文概述的计划将为我们提供宝贵的时间和对过程的关键控制，使我们处于更好得多的位置来避免AI对我们的社会、文明和物种构成的存在风险。

在更长期的未来，我们将需要就允许什么做出选择。我们可能仍然选择创建某种形式的真正可控的通用人工智能，在这被证明可能的程度上。或者我们可能决定让机器来管理世界更好，如果我们能说服自己它们会做得更好，并且善待我们。但这些应该是在掌握对AI的深度科学理解后做出的决定，并且在有意义的全球包容性讨论之后，而不是在科技巨头之间的竞赛中，大多数人类完全未参与且毫不知情的情况下。

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) 通过责任制和监管的A-G-I和超级智能治理总结。在自主性、通用性和智能性的三重交集处，责任最高，监管最强。可以通过肯定性安全案例获得严格责任和强监管的安全港，证明系统是弱的和/或狭窄的和/或被动的。通过法律以及硬件和密码学安全措施验证和执行的训练算力和推理算力速率总上限，通过避免完全通用人工智能和有效禁止超级智能来支撑安全。


[^1]: 最有可能的是，这种认识的传播需要教育和倡导团体的强烈努力来论证这一点，或者是相当严重的AI引起的灾难。我们希望是前者。

[^2]: 矛盾的是，我们习惯了自然通过使技术很难开发来限制我们的技术，特别是在科学上。但AI不再是这种情况：关键的科学问题正在证明比预期的更容易。我们不能指望自然在这里拯救我们——我们必须自己做到。

[^3]: 我们在开发新系统方面到底在哪里停止？在这里，我们应该采用预防原则。一旦系统被部署，特别是一旦该水平的系统能力扩散，就极难回滚。如果一个系统被*开发*（特别是以巨大成本和努力），就会有巨大的使用或部署压力，以及它被泄漏或窃取的诱惑。开发系统*然后*决定它们是否深度不安全是一条危险的道路。

[^4]: 禁止本质上危险的AI开发也是明智的，如自我复制和进化系统、设计用于逃脱围栏的系统、可以自主自我改进的系统、故意欺骗性和恶意的AI等。

[^5]: 注意这不一定意味着由某种全球机构在国际层面*执行*：相反，主权国家可以执行商定的规则，如在许多条约中。

[^6]: 正如我们下面将看到的，AI计算的性质将允许某种混合体；但仍然需要国际合作。

[^7]: 例如，蚀刻AI相关芯片所需的机器仅由一家公司ASML制造（尽管许多其他尝试这样做），绝大多数相关芯片由一家公司台积电制造（尽管其他公司试图竞争），从这些芯片设计和构建硬件由包括英伟达、AMD和谷歌在内的少数几家公司完成。

[^8]: 最重要的是，每个芯片都持有一个唯一且不可访问的密码学私钥，它可以用来"签署"东西。

[^9]: 默认情况下，这将是销售芯片的公司，但其他模型是可能的，并且可能有用。

[^10]: 管理者可以通过与芯片交换签名消息的时间来确定芯片的位置：如果芯片能在小于*r* / *c*的时间内返回签名消息，其中*c*是光速，那么有限的光速要求芯片在"站点"的给定半径*r*内。使用多个站点和对网络特征的一些理解，可以确定芯片的位置。这种方法的美妙之处在于其大部分安全性由物理定律提供。其他方法可以使用GPS、惯性跟踪和类似技术。

[^11]: 或者，芯片对可以仅在管理者明确许可下相互通信。

[^12]: 这是至关重要的，因为至少目前，芯片之间的非常高带宽连接是在它们上训练大型AI模型所需的。

[^13]: 这也可以设置为需要来自*M*个不同管理者中的*N*个的签名消息，允许多方共享治理。

[^14]: 这远非史无前例——例如军队没有开发克隆或基因工程超级士兵军队，尽管这在技术上可能是可能的。但他们*选择*不这样做，而不是被他人阻止。主要世界大国被阻止开发他们强烈希望开发的技术的记录并不好。

[^15]: 有几个值得注意的例外（特别是英伟达），AI专用硬件是这些公司整体业务和收入模式的相对较小部分。此外，先进AI中使用的硬件与"消费级"硬件之间的差距很大，因此大多数计算机硬件消费者基本不会受到影响。

[^16]: 有关更详细的分析，请参见[RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html)和[CNAS](https://www.cnas.org/publications/reports/secure-governable-chips)的最新报告。这些专注于技术可行性，特别是在美国出口管制寻求约束其他国家在高端计算方面的能力的背景下；但这与这里设想的全球约束有明显重叠。

[^17]: 例如，苹果设备在报告丢失或被盗时被远程安全锁定，可以远程重新激活。这依赖于这里讨论的相同硬件安全功能。

[^18]: 参见例如IBM的[按需容量](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand)产品、英特尔的[英特尔按需](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html)，以及苹果的[私有云计算](https://security.apple.com/blog/private-cloud-compute/)。

[^19]: [这项研究](https://epochai.org/trends#hardware-trends-section)显示，历史上相同的性能每年使用约30%更少的美元实现。如果这种趋势继续，AI和"消费者"芯片使用之间可能会有显著重叠，一般来说，高性能AI系统所需的硬件数量可能会变得令人不安地小。

[^20]: 根据[同一研究](https://epochai.org/trends#hardware-trends-section)，图像识别的给定性能每年需要2.5倍更少的计算。如果这也适用于最有能力的AI系统，计算限制将不会有用很长时间。

[^21]: 特别是，在国家层面，这很像计算的国有化，因为政府将对计算能力如何使用有很大控制权。然而，对于那些担心政府参与的人来说，这似乎比最强大的AI软件*本身*通过主要AI公司和国家政府之间的某种合并而被国有化要安全得多，也更可取，正如一些人开始倡导的那样。

[^22]: 欧洲在2024年通过[欧盟AI法案](https://artificialintelligenceact.eu/)采取了重大监管步骤。它按风险对AI进行分类：禁止不可接受的系统，监管高风险系统，并对低风险系统施加透明度规则或完全不采取措施。它将显著减少一些AI风险，并促进AI透明度，甚至对美国公司也是如此，但有两个关键缺陷。首先，覆盖范围有限：虽然它适用于在欧盟提供AI的任何公司，但对美国公司的执行力度较弱，军事AI也被豁免。其次，虽然它涵盖GPAI，但它未能将通用人工智能或超级智能识别为不可接受的风险或防止其开发——只是防止其在欧盟部署。因此，它对遏制通用人工智能或超级智能的风险几乎没有作用。

[^23]: 公司经常表示他们赞成合理的监管。但不知何故，他们似乎几乎总是反对任何*特定的*监管；见对相当轻度的SB1047的斗争，[大多数AI公司公开或私下反对](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/)。

[^24]: 从欧盟AI法案提出到生效大约用了3年半时间。

[^25]: 有时表达的是现在"太早"开始监管AI。考虑到上一条注释，这似乎不太可能。另一个表达的担忧是监管会"损害创新"。但良好的监管只是改变创新的方向，而不是数量。

[^26]: 一个有趣的先例是危险材料的运输，可能会逃逸并造成损害。在这里，[法规](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442)和[判例法](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf)已经为炸药、汽油、毒物、传染性病原体和放射性废物等非常危险的材料建立了严格责任。其他例子包括[药物警告](https://www.medicalnewstoday.com/articles/boxed-warnings)、[医疗设备类别](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification)等。

[^27]: 在["狭窄路径"](https://www.narrowpath.co/)中提出的另一个具有类似目标的综合提案主张采用更集中的、基于禁令的方法，将所有前沿AI开发通过单一国际实体引导，由强有力的国际机构监督，具有明确的分类禁令而不是分级限制。我也支持那个计划；然而它需要比这里提出的计划更多的政治意愿和协调。

[^28]: [前沿模型论坛](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/)发布了此类标准的一些指导原则。相对于这里的提案，那些倾向于更少精确和在统计中包含更少计算。

[^29]: 2023年美国AI行政令（现已撤销）要求类似但不太精细的报告。这应该通过替代令加强。

[^30]: 非常粗略地说，对于现在常见的H100芯片，这对应于大约1000个进行推理的集群；对于最新的顶级英伟达B200芯片进行推理，大约是100个（大约500万美元）。在两种情况下，训练数字对应于该集群计算几个月。

[^31]: 这个数量比任何目前训练的AI系统都大；随着我们更好地理解AI能力如何随算力扩展，可能需要更大或更小的数字。

[^32]: 这适用于创建和提供/托管模型的人，不适用于最终用户。

[^33]: 粗略地说，"严格"责任意味着开发者*默认*对产品造成的伤害负责，这是用于"异常危险"产品的标准，以及（有些滑稽但适当的）野生动物。"连带"责任意味着责任分配给对产品负责的所有当事方，这些当事方必须在他们之间确定谁承担什么责任。这对于像AI这样具有长而复杂价值链的系统很重要。

[^34]: 标准的基于过错的单方责任是不够的：过错将既难以追踪又难以分配，因为AI系统复杂，其运行不被理解，许多方可能参与危险系统或输出的创建。此外，诉讼需要数年才能裁决，可能仅导致对这些公司来说微不足道的罚款，因此对高管的个人责任也很重要。

[^35]: 对开放权重模型不应有安全标准的豁免。此外，在评估风险时，应假设可以去除的防护栏将从广泛可用的模型中去除，即使封闭模型也会扩散，除非有很高的保证它们会保持安全。

[^36]: 这里提出的方案以通用能力触发监管审查；然而，一些特别危险的用例触发更多审查是有意义的——例如，专家病毒学AI系统，即使狭窄且被动，也应该进入更高层级。前美国行政令对生物能力有一些这种结构。

[^37]: 两个明显的例子是航空和药物，由FAA和FDA以及其他国家的类似机构监管。这些机构并不完美，但对这些行业的功能和成功绝对至关重要。