# 第九章 - 工程化未来——我们应该如何行动

AI能够为世界带来巨大的好处。要获得所有好处而避免风险，我们必须确保AI始终是人类的工具。

如果我们成功选择不让机器取代人类——至少暂时如此！——那么我们还能做什么？我们是否要放弃AI作为一项技术的巨大前景？在某种程度上，答案很简单：*否*。我们要关闭通向不可控AGI和超级智能的大门，但*确实*要构建许多其他形式的AI，以及管理这些AI所需的治理结构和制度。

但仍有很多内容值得探讨；实现这一目标将是人类的核心任务。本节探讨几个关键主题：

- 我们如何描述"工具型"AI及其可能的形式。
- 我们可以在没有AGI的情况下，通过工具型AI获得人类想要的（几乎）一切。
- 工具型AI系统（在原则上可能）是可管理的。
- 摒弃AGI并不意味着在国家安全上妥协——恰恰相反。
- 权力集中是一个真实的担忧。我们能否在不破坏安全和保障的前提下缓解这个问题？
- 我们将需要——也必须拥有——新的治理和社会结构，而AI实际上可以提供帮助。

## 大门内的AI：工具型AI

三重交集图提供了一个很好的方式来界定我们可以称之为"工具型AI"的概念：作为人类可控制工具而非不可控制的竞争对手或替代品的AI。问题最少的AI系统是那些具有自主性但不具备通用性或超强能力的系统（如拍卖竞价机器人），或具备通用性但不自主或能力有限的系统（如小型语言模型），或能力强但狭窄且高度可控的系统（如AlphaGo）。[^1]具有两个交集特征的系统应用更广泛但风险更高，需要重大努力来管理。（仅仅因为一个AI系统更像工具，并不意味着它本质上安全，只是意味着它不是本质上*不安全*——想想电锯与宠物老虎的区别。）大门必须对（完全的）AGI和位于三重交集的超级智能保持关闭，对接近该阈值的AI系统必须极其谨慎。

但这仍然留下了许多强大的AI！我们可以从智能而通用的被动"预言机"和狭窄系统、具有人类水平但非超人水平的通用系统等获得巨大效用。许多科技公司和开发者正在积极构建这类工具，应该继续下去；像大多数人一样，他们隐含地*假设*通向AGI和超级智能的大门将被关闭。[^2]

此外，AI系统可以有效地结合成复合系统，在增强能力的同时保持人类监督。我们可以构建多个组件（包括AI和传统软件）以人类能够监控和理解的方式协同工作的系统，而不是依赖不可解释的黑盒。[^3]虽然某些组件可能是黑盒，但没有一个会接近AGI——只有作为整体的复合系统才会既高度通用又高度能干，并且以严格可控的方式。[^4]

### 有意义且有保障的人类控制

"严格可控"意味着什么？"工具型"框架的一个关键理念是允许系统——即使相当通用和强大——保证处于有意义的人类控制之下。这意味着什么？它包含两个方面。首先是设计考虑：人类应该深度且核心地参与系统正在做的事情，*不*将关键重要决策委托给AI。这是当前大多数AI系统的特征。其次，就AI系统自主的程度而言，它们必须有保证限制其行动范围。保证应该是描述某事发生概率的*数字*，以及相信该数字的理由。这是我们在其他安全关键领域所要求的，在这些领域，"故障间平均时间"和预期事故数量等数字被计算、支持并在安全案例中公布。[^5]理想的故障数字当然是零。好消息是，我们可能相当接近这个目标，尽管使用相当不同的AI架构，运用*形式化验证*程序（包括AI）属性的思想。这个想法由Omohundro、Tegmark、Bengio、Dalrymple等人详细探索（见[这里](https://arxiv.org/abs/2309.01933)和[这里](https://arxiv.org/abs/2405.06624)），即构造具有某些属性（例如：人类可以关闭它）的程序，并形式化*证明*这些属性成立。这现在可以对相当短的程序和简单属性做到，但AI驱动的证明软件的（即将到来的）力量可能允许它用于更复杂的程序（例如包装器）甚至AI本身。这是一个非常雄心勃勃的计划，但随着对大门压力的增长，我们将需要一些强大的材料来加固它们。数学证明可能是少数足够强大的材料之一。

### AI产业的去向

随着AI进步的重新定向，工具型AI仍将是一个巨大的产业。在硬件方面，即使有算力限制来防止超级智能，在较小模型中的训练和推理仍将需要大量专用组件。在软件方面，化解AI模型和计算规模爆炸应该只会导致公司将资源重新定向到让较小系统变得更好、更多样化和更专业化，而不是简单地让它们变得更大。[^6]对于所有那些赚钱的硅谷初创公司来说，将有充足的空间——可能更多。[^7]

## 工具型AI可以产生人类想要的（几乎）一切，无需AGI

智能，无论是生物的还是机器的，都可以广泛地被认为是规划和执行活动的能力，以实现更符合一组目标的未来。因此，当用于追求明智选择的目标时，智能具有巨大的好处。人工智能吸引大量时间和精力投资，主要是因为它承诺的好处。所以我们应该问：如果我们遏制其向超级智能的失控发展，我们在多大程度上仍能获得AI的好处？答案是：我们失去的可能出乎意料地少。

首先考虑到，当前的AI系统已经非常强大，而我们只是刚刚开始发掘它们能做什么。[^8]它们在"运行整个过程"方面相当有能力，即"理解"向它们提出的问题或任务，以及回答这个问题或完成那个任务需要什么。

其次，对现代AI系统的许多兴奋都源于它们的通用性；但一些最有能力的AI系统——如生成或识别语音或图像、进行科学预测和建模、玩游戏等——要狭窄得多，在算力方面完全"在大门内"。[^9]这些系统在它们所做的特定任务上是超人的。由于其狭窄性，它们可能有边缘情况[^10]（或[可利用的](https://arxiv.org/abs/2211.00241)）弱点；然而，*完全*狭窄或*完全*通用并非唯一可用的选项：中间有许多架构。[^11]

这些AI工具可以在没有AGI的情况下极大地加速其他积极技术的进步。要在核物理方面做得更好，我们不需要AI成为核物理学家——我们有这样的人！如果我们想加速医学发展，就给生物学家、医学研究人员和化学家提供强大的工具。他们想要这些工具，并将使用它们获得巨大收益。我们不需要一个充满百万数字天才的服务器农场；我们有数百万人类，AI可以帮助发挥他们的天赋。是的，获得永生和治愈所有疾病需要更长时间。这是一个真实的代价。但即使是最有前景的健康创新，如果AI驱动的不稳定导致全球冲突或社会崩溃，也将毫无用处。我们有义务让AI赋能的人类首先尝试解决这个问题。

假设AGI确实有一些无法通过人类使用门内工具获得的巨大好处。我们通过*永远不*构建AGI和超级智能就失去了这些吗？在权衡风险和回报时，等待与急于求成之间存在巨大的不对称收益：我们可以等到能够以有保障的安全和有益方式做到这一点，几乎每个人仍能获得回报；如果我们急于求成，用OpenAI首席执行官萨姆·阿尔特曼的话说，可能会——[对我们*所有人*来说都是灯灭了。](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

但如果非AGI工具潜在地如此强大，我们能管理它们吗？答案很明确...也许可以。

## 工具型AI系统（在原则上可能）是可管理的

但这不会容易。当前最先进的AI系统可以极大地增强人们和机构实现目标的能力。总的来说，这是好事！然而，拥有这样的系统——突然且没有太多时间让社会适应——存在自然动态，带来需要管理的严重风险。值得讨论几个主要风险类别，以及在假设大门关闭的情况下如何减少这些风险。

一类风险是高功率工具型AI允许获得此前与某个人或组织绑定的知识或能力，使得高能力加高忠诚度的组合对非常广泛的行为者可用。今天，一个恶意的人有足够的钱可以雇用一组化学家来设计和生产新的化学武器——但拥有那笔钱或找到/组建团队并说服他们做明显非法、不道德和危险的事情并不那么容易。为了防止AI系统发挥这种作用，对当前方法的改进可能就足够了，[^12]只要所有这些系统和对它们的访问都得到负责任的管理。另一方面，如果强大的系统被发布供一般使用和修改，任何内置的安全措施都可能被移除。因此，为了避免这类风险，需要对可以公开发布的内容实行强有力的限制——类似于对核技术、爆炸物和其他危险技术细节的限制。[^13]

第二类风险源于扩大行为像人或冒充人的机器。在对个人伤害层面，这些风险包括更有效的诈骗、垃圾邮件和网络钓鱼，以及非同意深度伪造的激增。[^14]在集体层面，它们包括对核心社会过程的破坏，如公共讨论和辩论、我们社会的信息和知识收集、处理和传播系统，以及我们的政治选择系统。缓解这种风险可能涉及（a）限制AI系统冒充人类的法律，并让创建产生此类冒充的系统的AI开发者承担责任，（b）识别和分类（负责任地）生成的AI内容的水印和来源追踪系统，以及（c）新的社会技术认识论系统，可以从数据（例如相机和录音）到事实、理解和良好的世界模型创建可信的链条。[^15]所有这些都是可能的，AI可以帮助完成其中一些部分。

第三个一般风险是，在某些任务被自动化的程度上，目前从事这些任务的人类的劳动财务价值可能降低。历史上，自动化任务使那些任务支持的事情变得更便宜、更丰富，同时将之前从事这些任务的人分类为仍然参与自动化版本的人（通常技能/薪酬更高），和那些劳动价值较低或很少的人。总体而言，很难预测在由此产生的更大但更高效的部门中，哪些部门需要更多或更少的人类劳动。同时，自动化动态往往增加不平等和总体生产力，降低某些商品和服务的成本（通过效率提升），并增加其他商品和服务的成本（通过[成本病](https://en.wikipedia.org/wiki/Baumol_effect)）。对于处于不平等增长不利一方的人来说，完全不清楚这些某些商品和服务的成本降低是否超过其他成本的增加，并导致整体福祉的提高。那么AI会如何发展？由于人类智力劳动相对容易被通用AI取代，我们可以预期在具有人类竞争力的通用AI方面出现这种情况的快速版本。[^16]如果我们关闭AGI的大门，许多工作不会被AI代理整体取代；但在几年内仍然可能发生巨大的劳动力转移。[^17]为了避免广泛的经济痛苦，可能需要实施某种形式的全民基本资产或收入，以及推动文化转变，重视和奖励更难自动化的以人为中心的劳动（而不是因为被推出经济其他部分的可用劳动力增加而看到劳动价格下降）。其他构造，如["数据尊严"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society)（其中训练数据的人类生产者自动获得该数据在AI中创造价值的版税），可能有所帮助。AI的自动化还有第二个潜在不利影响，即*不当*自动化。除了AI简单地做得更差的应用外，这还包括AI系统可能违反道德、伦理或法律戒律的应用——例如在生死决策和司法事务中。必须通过应用和扩展我们当前的法律框架来处理这些。

最后，门内AI的一个重大威胁是它在个性化说服、注意力捕获和操纵中的使用。我们已经在社交媒体和其他在线平台上看到了根深蒂固的注意力经济（在线服务为用户注意力激烈竞争）和["监控资本主义"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism)系统（其中用户信息和画像被加入到注意力商品化中）的增长。几乎可以肯定，更多AI将被用于为两者服务。AI已经大量用于令人上瘾的信息流算法，但这将演变为令人上瘾的AI生成内容，定制为让单个人强迫性消费。而那个人的输入、反应和数据将被输入注意力/广告机器，以延续恶性循环。同样，随着科技公司提供的AI助手成为更多在线生活的界面，它们可能会取代搜索引擎和信息流，成为说服和客户货币化发生的机制。我们社会迄今未能控制这些动态的情况并不乐观。其中一些动态可能通过有关隐私、数据权利和操纵的法规得到缓解。更深入地解决问题的根源可能需要不同的视角，如忠诚AI助手的视角（下文讨论）。

这次讨论的结果是希望：门内基于工具的系统——至少在它们保持与当今最先进系统相当的功率和能力的情况下——如果有意愿和协调去做，可能是可管理的。由AI工具增强的体面人类制度[^18]可以做到。我们也可能失败。但很难看出允许更强大的系统如何有帮助——除非让它们负责并希望最好的结果。

## 国家安全

AI霸权竞赛——由国家安全或其他动机驱动——驱使我们走向不受控制的强大AI系统，这些系统倾向于吸收而非赋予权力。美中之间的AGI竞赛是一场决定哪个国家首先获得超级智能的竞赛。

那么负责国家安全的人应该怎么做？政府在构建可控和安全的系统方面有丰富经验，他们应该在AI中加倍这样做，支持在规模化和政府认可下最成功的基础设施项目类型。

美国政府可以启动一个可控、安全、可信系统的阿波罗计划，而不是朝着AGI的鲁莽"曼哈顿计划"。[^19]这可能包括例如：

- 一个重大计划，用于（a）开发片上硬件安全机制和（b）基础设施，以管理强大AI的算力方面。这些可以建立在美国[芯片法案](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local)和[出口管制制度](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion)的基础上。
- 一个大规模倡议，开发形式化验证技术，以便AI系统的特定功能（如关闭开关）可以被*证明*存在或不存在。这可以利用AI本身来开发属性证明。
- 一个全国规模的努力，创建可验证安全的软件，由能够将现有软件重新编码为可验证安全框架的AI工具提供动力。
- 一个使用AI进行科学进步的国家投资项目，[^20]作为能源部、国家科学基金会和国立卫生研究院之间的合作伙伴关系运行。

总的来说，我们社会存在一个巨大的攻击面，使我们容易受到AI及其误用的风险。防范其中一些风险将需要政府规模的投资和标准化。这些将提供比向AGI竞赛火上浇油更多的安全性。如果AI要被构建到武器和指挥控制系统中，AI必须是可信和安全的，而当前的AI根本不是。

## 权力集中及其缓解

本文重点关注人类对AI的控制及其潜在失败。但审视AI情况的另一个有效视角是*权力集中*。开发非常强大的AI威胁将权力集中到极少数极大的企业手中，这些企业已经开发并将控制它，或者集中到使用AI作为维持自己权力和控制的新手段的政府中，或者集中到AI系统本身中。或者上述的某种不神圣的混合。在任何这些情况下，大多数人类都会失去权力、控制和能动性。我们如何对抗这种情况？

第一步也是最重要的一步，当然是对比人类更聪明的AGI和超级智能关闭大门。这些明确可以直接取代人类和人类群体。如果它们在企业或政府控制下，它们将把权力集中在这些企业或政府中；如果它们是"自由的"，它们将把权力集中到自己身上。所以让我们假设大门是关闭的。然后呢？

一个解决权力集中的提议解决方案是"开源"AI，其中模型权重是免费或广泛可用的。但如前所述，一旦模型是开放的，大多数安全措施或护栏都可以（并且通常是）被剥离。因此，在去中心化和安全、保障以及AI系统的人类控制之间存在尖锐的紧张关系。也有理由怀疑开放模型本身是否会有意义地对抗AI中的权力集中，比它们在操作系统中的作用更大（尽管有开放替代品，仍由微软、苹果和谷歌主导）。[^21]

然而，可能有方法来解决这个问题——既集中化和缓解风险，又去中心化能力和经济回报。这需要重新思考AI的开发方式以及如何分配其好处。

公共AI开发和所有权的新模式会有所帮助。这可能采取几种形式：政府开发的AI（受民主监督约束），[^22]非营利AI开发组织（如浏览器的Mozilla），或支持非常广泛的所有权和治理的结构。关键是这些机构将被明确授权为公共利益服务，同时在强有力的安全约束下运作。[^23]精心制作的监管和标准/认证制度也将至关重要，以便充满活力的市场提供的AI产品保持真正有用，而不是对用户进行剥削。

在经济权力集中方面，我们可以使用来源追踪和"数据尊严"来确保经济利益更广泛地流动。特别是，现在大多数AI权力（以及如果我们保持大门关闭的未来）都源于人类生成的数据，无论是直接训练数据还是人类反馈。如果AI公司被要求公平补偿数据提供者，[^24]这至少可以帮助更广泛地分配经济回报。除此之外，另一个模式可能是大型AI公司重要部分的公共所有权。例如，能够对AI公司征税的政府可以将收入的一部分投资于持有公司股票的主权财富基金，并向民众支付股息。[^25]

这些机制的关键是使用AI本身的力量来帮助更好地分配权力，而不是简单地使用非AI手段来对抗AI驱动的权力集中。一个强有力的方法是通过精心设计的AI助手，它们对用户运行真正的信托责任——将用户的利益放在首位，特别是高于企业提供者的利益。[^26]这些助手必须真正可信、技术上胜任但根据用例和风险水平适当限制，并通过公共、非营利或认证的营利渠道广泛提供给所有人。正如我们永远不会接受一个暗中为另一方对抗我们利益工作的人类助手一样，我们不应该接受为了企业利益而监视、操纵或从用户身上提取价值的AI助手。

这样的转变将从根本上改变当前的动态，即个人被迫独自与巨大的（AI驱动的）企业和官僚机器谈判，这些机器优先考虑价值提取而不是人类福祉。虽然有许多可能的方法来更广泛地重新分配AI驱动的权力，但没有一个会默认出现：它们必须通过信托要求、公共提供和基于风险的分层访问等机制进行有意设计和治理。

缓解权力集中的方法可能面临来自既得利益的重大阻力。[^27]但有通向AI开发的路径，不需要在安全和集中权力之间做选择。通过现在建立正确的机构，我们可以确保AI的好处被广泛分享，同时其风险得到仔细管理。

## 新的治理和社会结构

我们当前的治理结构正在挣扎：它们反应缓慢，经常被特殊利益俘获，并且[越来越不被公众信任。](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx)然而，这不是放弃它们的理由——恰恰相反。一些机构可能需要更换，但更广泛地说，我们需要能够增强和补充我们现有结构的新机制，帮助它们在我们快速发展的世界中更好地运作。

我们制度弱点的大部分源于不是正式政府结构，而是退化的社会机构：我们发展共同理解、协调行动和进行有意义话语的系统。到目前为止，AI已经加速了这种退化，用生成的内容淹没我们的信息渠道，指向最具极化和分裂性的内容，并使区分真假变得更加困难。

但AI实际上可以帮助重建和加强这些社会机构。考虑三个关键领域：

首先，AI可以帮助恢复对我们认识论系统——我们知道什么是真实的方式——的信任。我们可以开发AI驱动的系统，追踪和验证信息的来源，从原始数据到分析再到结论。这些系统可以将密码学验证与复杂分析相结合，帮助人们理解不仅某事是否为真，而且我们如何知道它是真的。[^28]忠诚的AI助手可能被委托跟进细节，以确保它们经得起检验。

其次，AI可以实现新形式的大规模协调。我们最紧迫的许多问题——从气候变化到抗生素耐药性——根本上是协调问题。我们[困在对几乎每个人来说都比可能更糟糕的情况中](https://equilibriabook.com/)，因为没有个人或团体能够承担先行动的风险。AI系统可以通过建模复杂的激励结构、识别通向更好结果的可行路径，以及促进到达那里所需的信任建设和承诺机制来提供帮助。

也许最有趣的是，AI可以实现全新形式的社会话语。想象能够"与城市对话"[^29]——不只是查看统计数据，而是与处理和综合数百万居民观点、经历、需求和愿望的AI系统进行有意义的对话。或者考虑AI如何能够促进目前彼此错过要点的群体之间的真正对话，帮助各方更好地理解对方的实际关切和价值观，而不是他们对彼此的讽刺画。[^30]或者AI可以为人们甚至大群体之间的争议提供熟练、可信中立的调解（他们都可以直接和个别地与它互动！）当前的AI完全有能力做这项工作，但做这些事情的工具不会自己出现，或者通过市场激励出现。

这些可能性可能听起来很乌托邦，特别是考虑到AI目前在退化话语和信任方面的作用。但这正是为什么我们必须积极开发这些积极应用。通过关闭通向不可控AGI的大门，优先考虑增强人类能动性的AI，我们可以引导技术进步走向AI作为赋权、复原力和集体进步力量的未来。

[^1]: 话虽如此，远离三重交集不幸地不如人们希望的那样容易。在三个方面中的任何一个方面非常努力地推动能力往往会在其他方面增加它。特别是，创建一个极其通用和有能力的智能可能很难不让它容易变成自主的。一种方法是训练["短视"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia)系统，削弱规划能力。另一种是专注于工程纯["预言机"](https://arxiv.org/abs/1711.05541)系统，它们会回避回答面向行动的问题。

[^2]: 许多公司没有意识到他们最终也会被AGI取代，即使需要更长时间——如果他们意识到了，他们可能不会那么用力推那些大门！

[^3]: AI系统可能以更高效但不太易懂的方式交流，但维持人类理解应该优先考虑。

[^4]: 模块化、可解释AI的这个想法已经被几位研究人员详细开发；见例如Drexler的["综合AI服务"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf)模型，Dalrymple等人的["开放能动架构"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai)。虽然这样的系统可能需要比用大量计算训练的整体神经网络更多的工程努力，但这正是计算限制有帮助的地方——使更安全、更透明的路径也成为更实用的路径。

[^5]: 关于安全案例的一般情况，见[此手册](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16)。特别是关于AI，见[Wasil等人](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274)，[Clymer等人](https://arxiv.org/abs/2403.10462)，[Buhl等人](https://arxiv.org/abs/2410.21572)，和[Balesni等人](https://arxiv.org/abs/2411.03336)

[^6]: 我们实际上已经看到这种趋势，仅仅是由推理的高成本驱动：从更大模型中"蒸馏"出的更小、更专业的模型，能够在成本较低的硬件上运行。

[^7]: 我理解为什么那些对AI技术生态系统感到兴奋的人可能反对他们认为对其行业繁重的监管。但坦率地说，让我困惑的是，比如说，风险投资家为什么会想要允许向AGI和超级智能的失控发展。这些系统（和公司，当它们仍然在公司控制下时）将*把所有初创公司当作小食吃掉*。可能甚至比吃掉其他行业*更快*。任何投资于繁荣AI生态系统的人都应该优先确保AGI开发不会导致少数主导参与者的垄断。

[^8]: 正如经济学家和前Deepmind研究员迈克尔·韦伯[所说的](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/)，"我认为如果我们今天停止所有更大语言模型的开发，所以GPT-4和Claude以及其他的，它们是我们训练那种规模的最后一批——所以我们允许对那种规模的东西进行更多迭代和各种微调，但没有比那更大的，没有更大的进展——仅仅我们今天所拥有的我认为就足以推动20或30年的令人难以置信的经济增长。"

[^9]: 例如，DeepMind的alphafold系统仅使用了GPT-4的FLOP数的十万分之一。

[^10]: 自动驾驶汽车的困难在这里很重要：虽然名义上是一个狭窄任务，并且使用相对较小的AI系统以相当的可靠性实现，但广泛的现实世界知识和理解对于获得这样一个安全关键任务所需的可靠性水平是必要的。

[^11]: 例如，在给定的计算预算下，我们可能会看到GPAI模型在（比如）该预算的一半进行预训练，另一半用于在更狭窄的任务范围内训练非常高的能力。这将提供由接近人类的通用智能支撑的超人狭窄能力。

[^12]: 当前主导的对齐技术是"人类反馈强化学习"[(RLHF)](https://arxiv.org/abs/1706.03741)，使用人类反馈为AI模型的强化学习创建奖励/惩罚信号。这种技术和相关技术如[宪法AI](https://arxiv.org/abs/2212.08073)工作得出人意料地好（尽管它们缺乏稳健性，可能被适度努力规避）。此外，当前语言模型在常识推理方面通常足够胜任，不会犯愚蠢的道德错误。这在某种程度上是一个甜蜜点：足够聪明，能够理解人们想要什么（在可以定义的程度上），但不够聪明，无法策划复杂的欺骗或在出错时造成巨大伤害。

[^13]: 从长远来看，任何被开发的AI能力水平都可能扩散，因为它最终是软件，而且有用。我们需要有稳健的机制来防范此类系统构成的风险。但我们*现在没有*，所以我们必须非常谨慎地允许多少强大的AI模型扩散。

[^14]: 其中绝大多数是非同意的色情深度伪造，包括未成年人。

[^15]: 此类解决方案的许多成分存在，形式为"机器人还是不是"法律（在EU AI法案等中），[行业来源追踪技术](https://c2pa.org/)，[创新新闻聚合器](https://www.improvethenews.org/)，预测[聚合器](https://metaculus.com/)和市场等。

[^16]: 自动化浪潮可能不会遵循以前的模式，因为相对*高*技能任务，如优质写作、解释法律或提供医疗建议，可能与较低技能任务一样，甚至更容易受到自动化的影响。

[^17]: 有关AGI对工资影响的仔细建模，见[这里](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek)的报告，详细内容[这里](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0)，来自Anton Korinek和合作者。他们发现，随着工作的更多部分被自动化，生产率和工资都上升——到一定程度。一旦*太多*被自动化，生产率继续提高，但工资暴跌，因为人们被高效的AI整体取代。这就是为什么关闭大门如此有用：我们得到生产率而没有消失的人类工资。

[^18]: 有许多方法可以将AI用作并帮助构建"防御性"技术，以使保护和管理更加稳健。见描述这个"D/acc"议程的[这篇](https://vitalik.eth.limo/general/2025/01/05/dacc2.html)有影响力的帖子。

[^19]: 有些讽刺的是，美国曼哈顿计划可能对加速AGI时间表没什么作用——人力和财政对AI进步投资的表盘已经调到11了。主要结果将是激发中国的类似项目（中国在国家级基础设施项目方面表现出色），使限制AI风险的国际协议变得更加困难，并使美国的其他地缘政治对手如俄罗斯感到担忧。

[^20]: ["国家AI研究资源"](https://nairrpilot.org/)计划是这个方向上很好的当前步骤，应该扩展。

[^21]: 见技术产品中"开放"的各种含义和含义以及一些如何导致更多而非更少主导地位巩固的[这项分析](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807)。

[^22]: 美国的[国家AI研究资源](https://nairratdoe.ornl.gov/)计划和最近推出的[欧洲AI基金会](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/)是这个方向上有趣的步骤。

[^23]: 这里的挑战不是技术上的而是制度上的——我们迫切需要公共利益AI开发可能看起来像什么的现实世界例子和实验。

[^24]: 这违背了当前大型科技商业模式，需要法律行动和新规范。

[^25]: 只有一些政府能够这样做。一个更激进的想法是[这种类型的普遍基金，在所有人类的联合所有权下。](https://futureoflife.org/project/the-windfall-trust/)

[^26]: 有关这种情况的详细阐述，见关于AI忠诚度的[这篇论文](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338)。不幸的是，AI助手的默认轨迹可能是一个越来越不忠诚的轨迹。

[^27]: 有些讽刺的是，许多既得利益也面临AI支持的去权力化的风险；但对他们来说，除非和直到这个过程相当深入，否则可能很难察觉到这一点。

[^28]: 这个方向上一些有趣的努力由[c2pa联盟](https://c2pa.org/)在密码学验证方面代表；[Verity](https://www.improvethenews.org/)和[Ground news](https://ground.news/)在更好的新闻认识论方面；以及[Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com)和预测市场在将话语建立在可证伪的预测基础上。

[^29]: 见这个[迷人的试点项目](https://talktothecity.org/)。

[^30]: 见[Kialo](https://www.kialo-edu.com/)，以及[集体智慧项目](https://www.cip.org/)的努力，了解一些例子。