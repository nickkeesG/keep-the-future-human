# Streszczenie wykonawcze

Przegląd najważniejszych punktów eseju. Jeśli masz mało czasu, poznaj wszystkie główne argumenty w zaledwie 10 minut.

Dramatyczne postępy w dziedzinie sztucznej inteligencji w ostatniej dekadzie (w przypadku AI o wąskim zastosowaniu) i w ostatnich latach (w przypadku AI ogólnego przeznaczenia) przekształciły AI z niszowej dziedziny akademickiej w podstawową strategię biznesową wielu największych światowych korporacji, z setkami miliardów dolarów rocznych inwestycji w techniki i technologie rozwijające możliwości AI.

Stoimy teraz w krytycznym momencie. Gdy możliwości nowych systemów AI zaczynają dorównywać i przewyższać ludzkie w wielu domenach poznawczych, ludzkość musi zdecydować: jak daleko idziemy i w którym kierunku?

AI, jak każda technologia, zaczynało od celu poprawy sytuacji swojego twórcy. Ale nasza obecna trajektoria i domyślny wybór to niepohamowany wyścig ku coraz potężniejszym systemom, napędzany bodźcami ekonomicznymi kilku gigantycznych firm technologicznych dążących do automatyzacji dużych obszarów obecnej działalności gospodarczej i ludzkiej pracy. Jeśli ten wyścig będzie trwał znacznie dłużej, zwycięzca jest nieunikniony: samo AI – szybsza, mądrzejsza, tańsza alternatywa dla ludzi w naszej gospodarce, naszym myśleniu, naszych decyzjach, a ostatecznie w kontroli nad naszą cywilizacją.

Ale możemy dokonać innego wyboru: poprzez nasze rządy możemy przejąć kontrolę nad procesem rozwoju AI, aby narzucić jasne ograniczenia, linie, których nie przekroczymy, i rzeczy, których po prostu nie będziemy robić – tak jak zrobiliśmy w przypadku technologii jądrowych, broni masowego rażenia, broni kosmicznej, procesów niszczących środowisko, bioinżynierii ludzi i eugeniki. Co najważniejsze, możemy zapewnić, że AI pozostanie narzędziem wzmacniającym ludzi, a nie nowym gatunkiem, który nas zastąpi i ostatecznie wyprze.

Ten esej argumentuje, że powinniśmy *zachować przyszłość dla człowieka* poprzez zamknięcie "Bram" dla mądrzejszej od człowieka, autonomicznej AI ogólnego przeznaczenia – czasami nazywanej "AGI" – a szczególnie dla wersji znacznie przewyższającej człowieka, czasami nazywanej "superinteligencją". Zamiast tego powinniśmy skupić się na potężnych, godnych zaufania narzędziach AI, które mogą wzmocnić jednostki i przełomowo poprawić zdolności ludzkich społeczeństw do robienia tego, co robią najlepiej. Struktura tego argumentu wygląda następująco w skrócie.

## AI jest inne

Systemy AI różnią się fundamentalnie od innych technologii. Podczas gdy tradycyjne oprogramowanie wykonuje precyzyjne instrukcje, systemy AI uczą się, jak osiągać cele bez wyraźnego instruowania, jak to robić. To czyni je potężnymi: jeśli możemy czysto zdefiniować cel lub metrykę sukcesu, w większości przypadków system AI może nauczyć się go osiągać. Ale to także czyni je z natury nieprzewidywalnymi: nie możemy w sposób niezawodny określić, jakie działania podejmą, aby osiągnąć swoje cele.

Są także w dużej mierze niewyjaśnialne: choć częściowo są kodem, to głównie stanowią ogromny zbiór nieczytelnych liczb – "wag" sieci neuronowych – których nie można przeanalizować; nie jesteśmy dużo lepsi w rozumieniu ich wewnętrznego działania niż w rozpoznawaniu myśli przez zaglądanie do biologicznego mózgu.

Ten podstawowy sposób trenowania cyfrowych sieci neuronowych szybko wzrasta w złożoności. Najmocniejsze systemy AI są tworzone poprzez masywne eksperymenty obliczeniowe, wykorzystujące specjalistyczny sprzęt do trenowania sieci neuronowych na ogromnych zbiorach danych, które są następnie wzbogacane o narzędzia programowe i nadbudowę.

Doprowadziło to do stworzenia bardzo potężnych narzędzi do tworzenia i przetwarzania tekstu i obrazów, wykonywania rozumowania matematycznego i naukowego, agregowania informacji oraz interaktywnego przeszukiwania ogromnego zasobu ludzkiej wiedzy.

Niestety, choć rozwój potężniejszych, bardziej godnych zaufania narzędzi technologicznych to właśnie to, co *powinniśmy* robić i czego niemal wszyscy chcą i mówią, że chcą, to nie jest to trajektoria, na której faktycznie się znajdujemy.

## AGI i superinteligencja

Od zarania tej dziedziny badania nad AI skupiały się zamiast tego na innym celu: Sztucznej Inteligencji Ogólnej. To skupienie stało się teraz celem tytanicznych firm przewodzących rozwojowi AI.

Czym jest AGI? Często jest niejasno definiowane jako "AI na poziomie ludzkim", ale to problematyczne: którzy ludzie i w jakich zdolnościach jest na poziomie ludzkim? A co z nadludzkimi zdolnościami, które już posiada? Bardziej użytecznym sposobem rozumienia AGI jest przecięcie trzech kluczowych właściwości: wysokiej **A**utonomii (niezależności działania), wysokiej **O**gólności (szerokiego zakresu i adaptowalności) oraz wysokiej **I**nteligencji (kompetencji w zadaniach poznawczych). Obecne systemy AI mogą być wysoce zdolne, ale wąskie, lub ogólne, ale wymagające stałego ludzkiego nadzoru, lub autonomiczne, ale ograniczone w zakresie.

Pełne A-O-I łączyłoby wszystkie trzy właściwości na poziomach dorównujących lub przewyższających najlepsze ludzkie możliwości. Co kluczowe, to właśnie ta kombinacja czyni ludzi tak skutecznymi i tak odmiennymi od obecnego oprogramowania; to także umożliwiłoby hurtowe zastąpienie ludzi przez systemy cyfrowe.

Choć ludzka inteligencja jest wyjątkowa, wcale nie stanowi ograniczenia. Sztuczne systemy "superinteligentne" mogłyby działać setki razy szybciej, przetwarzać znacznie więcej danych i utrzymywać ogromne ilości "na uwadze" jednocześnie, oraz tworzyć agregaty znacznie większe i skuteczniejsze niż zbiory ludzi. Mogłyby wyprze nie jednostki, ale firmy, narody czy naszą cywilizację jako całość.

## Jesteśmy u progu

Istnieje silny naukowy konsensus, że AGI jest *możliwe*. AI już przewyższa ludzkie wyniki w wielu ogólnych testach zdolności intelektualnych, w tym ostatnio w rozumowaniu i rozwiązywaniu problemów wysokiego poziomu. Opóźnione zdolności – takie jak ciągłe uczenie się, planowanie, samoświadomość i oryginalność – wszystkie istnieją na pewnym poziomie w obecnych systemach AI, a znane techniki mogące je wszystkie poprawić już istnieją.

Podczas gdy jeszcze kilka lat temu wielu badaczy postrzegało AGI jako odległe o dekady, obecnie dowody na krótkie terminy osiągnięcia AGI są silne:

- Empirycznie zweryfikowane "prawa skalowania" łączą wkład obliczeniowy ze zdolnościami AI, a korporacje są na trasie do zwiększenia wkładu obliczeniowego o rzędy wielkości w nadchodzących latach. Zasoby ludzkie i finansowe poświęcone rozwojowi AI równają się teraz tym z tuzina Projektów Manhattan i kilku Projektów Apollo.
- Korporacje AI i ich liderzy publicznie i prywatnie wierzą, że AGI (według jakiejś definicji) jest osiągalne w ciągu kilku lat. Te firmy mają informacje, których społeczeństwo nie ma, włącznie z posiadaniem następnej generacji systemów AI.
- Eksperci prognozujący ze sprawdzonym doświadczeniem przypisują 25% prawdopodobieństwo przybyciu AGI (według jakiejś definicji) w ciągu 1-2 lat i 50% dla 2-5 lat (patrz prognozy Metaculus dla ['słabego'](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) i ['pełnego'](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/) AGI).
- Autonomia (włącznie z dalekozasięgowym elastycznym planowaniem) pozostaje w tyle w systemach AI, ale główne firmy skupiają teraz swoje ogromne zasoby na rozwijaniu autonomicznych systemów AI i nieformalnie nazwały 2025 ["rokiem agenta."](https://techinformed.com/2025-informed-the-year-of-agentic-ai/)
- AI coraz bardziej przyczynia się do własnego ulepszania. Gdy systemy AI będą tak kompetentne jak ludzie-badacze AI w prowadzeniu badań nad AI, zostanie osiągnięty krytyczny próg szybkiego postępu ku znacznie potężniejszym systemom AI i prawdopodobnie doprowadzi to do lawinowego wzrostu możliwości AI. (Można argumentować, że ta lawina już się zaczęła.)

Idea, że mądrzejsze od człowieka AGI jest odległe o dekady lub więcej, po prostu nie jest już do utrzymania dla ogromnej większości ekspertów w tej dziedzinie. Nieporozumienia dotyczą teraz tego, ile miesięcy lub lat to zajmie, jeśli pozostaniemy na tym kursie. Podstawowe pytanie, przed którym stoimy, to: czy powinniśmy?

## Co napędza wyścig ku AGI

Wyścig ku AGI jest napędzany przez wiele sił, z których każda czyni sytuację bardziej niebezpieczną. Główne firmy technologiczne postrzegają AGI jako ostateczną technologię automatyzacji – nie tylko wspomagającą pracowników, ale w dużej mierze lub całkowicie ich zastępującą. Dla firm nagroda jest ogromna: możliwość przechwycenia znacznej części światowej rocznej produkcji gospodarczej wynoszącej 100 bilionów dolarów poprzez automatyzację kosztów ludzkiej pracy.

Narody czują się zmuszone do przyłączenia się do tego wyścigu, publicznie powołując się na przywództwo gospodarcze i naukowe, ale prywatnie postrzegając AGI jako potencjalną rewolucję w sprawach wojskowych porównywalną z bronią jądrową. Strach, że rywale mogą uzyskać decydującą przewagę strategiczną, tworzy klasyczną dynamikę wyścigu zbrojeń.

Ci dążący do superinteligencji często przywołują wielkie wizje: wyleczenie wszystkich chorób, odwrócenie starzenia, osiągnięcie przełomów w energii i podróżach kosmicznych czy stworzenie nadludzkich zdolności planistycznych.

Mniej przychylnie, tym co napędza wyścig, jest władza. Każdy uczestnik – czy to firma, czy kraj – wierzy, że inteligencja równa się władzy i że będzie najlepszym strażnikiem tej władzy.

Argumentuję, że te motywacje są rzeczywiste, ale fundamentalnie błędne: AGI *wchłonie* i *będzie szukać* władzy zamiast jej udzielać; technologie stworzone przez AI będą *także* silnie obosieczne, a tam gdzie korzystne, można je stworzyć za pomocą narzędzi AI i bez AGI; i nawet o ile AGI i jego produkty pozostaną pod kontrolą, ta dynamika wyścigowa – zarówno korporacyjna, jak i geopolityczna – sprawia, że wielkoskalowe zagrożenia dla naszego społeczeństwa są niemal nieuniknione, chyba że zostaną zdecydowanie przerwane.

## AGI i superinteligencja stanowią dramatyczne zagrożenie dla cywilizacji

Pomimo swojej atrakcyjności, AGI i superinteligencja stwarzają dramatyczne zagrożenia dla cywilizacji poprzez wiele wzajemnie wzmacniających się ścieżek:

*Koncentracja władzy:* nadludzka AI mogłaby pozbawić władzy ogromną większość ludzkości poprzez wchłonięcie ogromnych obszarów działalności społecznej i gospodarczej w systemy AI prowadzone przez garstką gigantycznych firm (które z kolei mogą zostać przejęte przez rządy lub faktycznie je przejąć).

*Masowe zakłócenia:* masowa automatyzacja większości prac opartych na poznaniu, zastąpienie naszych obecnych systemów epistemicznych i wdrożenie ogromnej liczby aktywnych nieludzkich agentów wywróciłoby większość naszych obecnych systemów cywilizacyjnych w stosunkowo krótkim czasie.

*Katastrofy:* poprzez rozprzestrzenienie zdolności – potencjalnie ponad ludzki poziom – do tworzenia nowych technologii wojskowych i destrukcyjnych i odłączenie jej od systemów społecznych i prawnych ugruntowujących odpowiedzialność, katastrofy fizyczne z broni masowego rażenia stają się dramatycznie bardziej prawdopodobne.

*Geopolityka i wojna:* główne światowe mocarstwa nie będą bezczynnie siedziały, jeśli poczują, że technologia mogąca zapewnić "decydującą przewagę strategiczną" jest rozwijana przez ich przeciwników.

*Wymknięcie się spod kontroli i utrata kontroli:* Chyba że zostanie to specjalnie zapobieżone, nadludzka AI będzie miała wszelkie bodźce do dalszego ulepszania siebie i mogłaby znacznie przewyższyć ludzi w szybkości, przetwarzaniu danych i wyrafinowaniu myślenia. Nie ma znaczącego sposobu, w jaki moglibyśmy kontrolować taki system. Takie AI nie udzieli władzy ludziom; my udzielimy władzy jemu, albo ono ją przejmie.

Wiele z tych zagrożeń pozostaje nawet jeśli techniczny problem "wyrównania" – zapewnienie, że zaawansowane AI niezawodnie robi to, co ludzie chcą, żeby robiło – zostanie rozwiązany. AI przedstawia ogromne wyzwanie w kwestii tego, jak będzie zarządzane, a bardzo wiele aspektów tego zarządzania staje się niesamowicie trudne lub nierozwiązywalne, gdy ludzka inteligencja zostanie przekroczona.

Najbardziej fundamentalnie, rodzaj nadludzkiej AI ogólnego przeznaczenia obecnie rozwijany miałby z natury cele, sprawczość i zdolności przewyższające nasze własne. Byłby z natury niekontrolowalny – jak możemy kontrolować coś, czego nie możemy ani zrozumieć, ani przewidzieć? Nie byłby technologicznym narzędziem do ludzkiego użytku, ale drugim gatunkiem inteligencji na Ziemi obok naszego. Gdyby pozwolono mu rozwijać się dalej, stanowiłby nie tylko drugi gatunek, ale gatunek zastępczy.

Być może traktowałby nas dobrze, być może nie. Ale przyszłość należałaby do niego, nie do nas. Era ludzka dobiegłaby końca.

## To nie jest nieuniknione; ludzkość może bardzo konkretnie zdecydować, aby nie budować swojego następcy.

Stworzenie nadludzkiej AGI jest dalekie od nieuniknionego. Możemy temu zapobiec poprzez skoordynowany zestaw środków zarządzania:

Po pierwsze, potrzebujemy solidnej księgowości i nadzoru nad mocą obliczeniową AI ("compute"), która jest fundamentalnym czynnikiem umożliwiającym i dźwignią do zarządzania wielkoskalowymi systemami AI. To z kolei wymaga standaryzowanego pomiaru i raportowania całkowitej mocy obliczeniowej używanej w trenowaniu modeli AI i ich uruchamianiu, oraz technicznych metod liczenia, certyfikowania i weryfikowania używanej mocy obliczeniowej.

Po drugie, powinniśmy wprowadzić twarde limity na moc obliczeniową AI, zarówno dla treningu, jak i operacji; zapobiegają one AI byciu zarówno zbyt potężną, jak i działaniu zbyt szybko. Te limity mogą być wprowadzone zarówno poprzez wymagania prawne, jak i środki bezpieczeństwa oparte na sprzęcie wbudowane w chipy specjalizowane dla AI, analogiczne do funkcji bezpieczeństwa w nowoczesnych telefonach. Ponieważ specjalistyczny sprzęt AI jest wytwarzany przez tylko garstką firm, weryfikacja i egzekwowanie są wykonalne poprzez istniejący łańcuch dostaw.

Po trzecie, potrzebujemy zwiększonej odpowiedzialności za najniebezpieczniejsze systemy AI. Ci rozwijający AI łączące wysoką autonomię, szeroką ogólność i nadrzędną inteligencję powinni ponosić ścisłą odpowiedzialność za szkody, podczas gdy bezpieczne przystanie od tej odpowiedzialności zachęcałoby do rozwoju bardziej ograniczonych i kontrolowalnych systemów.

Po czwarte, potrzebujemy regulacji wielopoziomowej opartej na poziomach ryzyka. Najbardziej zdolne i niebezpieczne systemy wymagałyby rozległych gwarancji bezpieczeństwa i kontrolowalności przed rozwojem i wdrożeniem, podczas gdy mniej potężne lub bardziej wyspecjalizowane systemy podlegałyby proporcjonalnemu nadzorowi. Te ramy regulacyjne powinny ostatecznie działać zarówno na poziomie krajowym, jak i międzynarodowym.

To podejście – ze szczegółową specyfikacją podaną w pełnym dokumencie – jest praktyczne: choć potrzebna będzie międzynarodowa koordynacja, weryfikacja i egzekwowanie mogą działać poprzez małą liczbę firm kontrolujących łańcuch dostaw specjalistycznego sprzętu. Jest także elastyczne: firmy wciąż mogą innowować i czerpać zyski z rozwoju AI, tylko z jasnymi limitami na najniebezpieczniejsze systemy.

Długoterminowe ograniczenie władzy i ryzyka AI wymagałoby międzynarodowych porozumień opartych zarówno na własnym, jak i wspólnym interesie, tak jak kontrola proliferacji broni jądrowej robi to teraz. Ale możemy zacząć natychmiast od zwiększonego nadzoru i odpowiedzialności, budując ku bardziej kompleksowemu zarządzaniu.

Kluczowym brakującym składnikiem jest polityczna i społeczna wola przejęcia kontroli nad procesem rozwoju AI. Źródłem tej woli, jeśli przyjdzie na czas, będzie sama rzeczywistość – to znaczy, z powszechnego uświadomienia sobie prawdziwych implikacji tego, co robimy.

## Możemy zaprojektować AI Narzędziowe, aby wzmocnić ludzkość

Zamiast dążyć do niekontrolowalnej AGI, możemy rozwijać potężne "AI Narzędziowe", które zwiększa ludzkie zdolności pozostając pod znaczącą ludzką kontrolą. Systemy AI Narzędziowego mogą być niezwykle zdolne, unikając jednocześnie niebezpiecznego potrójnego przecięcia wysokiej autonomii, szerokiej ogólności i nadludzkiej inteligencji, o ile zaprojektujemy je tak, aby były kontrolowalne na poziomie współmiernym z ich zdolnościami. Mogą też być łączone w wyrafinowane systemy zachowujące ludzki nadzór przy dostarczaniu transformacyjnych korzyści.

AI Narzędziowe może zrewolucjonizować medycynę, przyspieszyć odkrycia naukowe, ulepszyć edukację i poprawić procesy demokratyczne. Gdy właściwie zarządzane, może uczynić ludzkich ekspertów i instytucje bardziej skutecznymi zamiast ich zastępować. Choć takie systemy będą wciąż wysoce destrukcyjne i wymagają ostrożnego zarządzania, zagrożenia, które stwarzają, różnią się fundamentalnie od AGI: to zagrożenia, którymi możemy zarządzać, jak te z innych potężnych technologii, nie egzystencjalne zagrożenia dla ludzkiej sprawczości i cywilizacji. I kluczowo, gdy mądrze rozwijane, narzędzia AI mogą pomagać ludziom zarządzać potężną AI i radzić sobie z jej skutkami.

To podejście wymaga przemyślenia zarówno sposobu rozwoju AI, jak i dystrybucji jego korzyści. Nowe modele publicznego i non-profit rozwoju AI, solidne ramy regulacyjne i mechanizmy szerszej dystrybucji korzyści ekonomicznych mogą pomóc zapewnić, że AI wzmacnia ludzkość jako całość zamiast koncentrować władzę w kilku rękach. Samo AI może pomóc budować lepsze instytucje społeczne i zarządzające, umożliwiając nowe formy koordynacji i dyskursu wzmacniające zamiast podważające ludzkie społeczeństwo. Establishmenty bezpieczeństwa narodowego mogą wykorzystać swoją ekspertyzę, aby uczynić systemy narzędzi AI rzeczywiście bezpiecznymi i godnymi zaufania oraz prawdziwym źródłem obrony, a także narodowej potęgi.

Możemy ostatecznie wybrać rozwój jeszcze potężniejszych i bardziej suwerennych systemów, które są mniej jak narzędzia, a – możemy mieć nadzieję – bardziej jak mądrzy i potężni dobroczyńcy. Ale powinniśmy to robić dopiero po tym, jak rozwinęliśmy naukowe zrozumienie i zdolność zarządzania, aby zrobić to bezpiecznie. Taka monumentalna i nieodwracalna decyzja powinna być podjęta świadomie przez ludzkość jako całość, nie domyślnie w wyścigu między firmami technologicznymi a narodami.

## W ludzkich rękach

Ludzie chcą dobra, które płynie z AI: użytecznych narzędzi, które ich wzmacniają, doładowują możliwości ekonomiczne i wzrost oraz obiecują przełomy w nauce, technologii i edukacji. Dlaczego nie mieliby? Ale gdy zapytani, przytłaczające większości społeczeństwa [chcą wolniejszego i bardziej ostrożnego rozwoju AI](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation) i nie chcą mądrzejszej od człowieka AI, która zastąpi ich w pracy i gdzie indziej, wypełni ich kulturę i przestrzeń informacyjną treścią nie-ludzką, skoncentruje władzę w maleńkim zestawie korporacji, stworzy skrajne wielkoskalowe globalne zagrożenia i ostatecznie zagrozi pozbawieniu władzy lub zastąpieniu ich gatunku. Dlaczego mieliby?

*Możemy* mieć jedno bez drugiego. Zaczyna się od decyzji, że nasze przeznaczenie nie leży w rzekomej nieuchronności jakiejś technologii ani w rękach kilku CEO w Dolinie Krzemowej, ale w reszcie naszych rąk, jeśli je w nie weźmiemy. Zamknijmy Bramy i zachowajmy przyszłość dla człowieka.