# Rozdział 1 - Wprowadzenie

To, jak odpowiemy na perspektywę powstania AI przewyższającej człowieka, jest najważniejszą kwestią naszych czasów. Ten esej przedstawia drogę naprzód.

Możemy znajdować się u kresu ery człowieka.

W ciągu ostatnich dziesięciu lat rozpoczął się proces, który jest wyjątkowy w historii naszego gatunku. Jego konsekwencje będą w znacznej mierze determinować przyszłość ludzkości. Począwszy od około 2015 roku, badaczom udało się opracować *wąską* sztuczną inteligencję (AI) – systemy, które potrafią wygrywać w grach takich jak Go, rozpoznawać obrazy i mowę, i tak dalej, lepiej niż jakikolwiek człowiek.[^1]

To niesamowity sukces, który przynosi niezwykle użyteczne systemy i produkty wzmacniające możliwości ludzkości. Ale wąska sztuczna inteligencja nigdy nie była prawdziwym celem tej dziedziny. Celem było raczej stworzenie AI *ogólnego* przeznaczenia, szczególnie systemów często nazywanych „sztuczną inteligencją ogólną" (AGI) lub „superinteligencją", które są jednocześnie równie dobre lub lepsze od ludzi w niemal *wszystkich* zadaniach, podobnie jak AI jest obecnie nadludzka w Go, szachach, pokerze, wyścigach dronów itd. To deklarowany cel wielu głównych firm zajmujących się AI.[^2]

*Te wysiłki również odnoszą sukces.* Systemy AI ogólnego przeznaczenia takie jak ChatGPT, Gemini, Llama, Grok, Claude i Deepseek, oparte na masywnych obliczeniach i górach danych, osiągnęły równość z przeciętnymi ludźmi w szerokiej gamie zadań, a nawet dorównują ludzkim ekspertom w niektórych dziedzinach. Teraz inżynierowie AI w niektórych z największych firm technologicznych ścigają się, by pchnąć te gigantyczne eksperymenty w zakresie inteligencji maszynowej na kolejne poziomy, na których dorównują, a następnie przewyższają pełen zakres ludzkich możliwości, ekspertyzy i autonomii.

*To jest nieuchronne.* W ciągu ostatnich dziesięciu lat szacunki ekspertów dotyczące tego, jak długo to potrwa – jeśli będziemy kontynuować obecny kurs – spadły z dekad (lub stuleci) do pojedynczych lat.

Ma to również epochalne znaczenie i wiąże się z transcendentnym ryzykiem. Zwolennicy AGI postrzegają ją jako pozytywną transformację, która rozwiąże problemy naukowe, wyleczy choroby, opracuje nowe technologie i zautomatyzuje harówkę. I AI z pewnością mogłaby pomóc w osiągnięciu wszystkich tych rzeczy – rzeczywiście już to robi. Ale przez dziesięciolecia wielu wnikliwych myślicieli, od Alana Turinga przez Stephena Hawkinga po współczesnych Geoffreya Hintona i Yoshuę Bengio[^3], wydawało ostre ostrzeżenie: zbudowanie naprawdę inteligentniejszej od człowieka, ogólnej, autonomicznej AI co najmniej całkowicie i nieodwracalnie wywróci społeczeństwo do góry nogami, a maksymalnie doprowadzi do wyginięcia człowieka.[^4]

Superinteligentna AI szybko zbliża się na naszej obecnej ścieżce, ale jest daleka od nieuchronności. Ten esej to rozbudowany argument na rzecz tego, dlaczego i jak powinniśmy *zamknąć Bramy* przed tą zbliżającą się nieludzką przyszłością i co powinniśmy zrobić zamiast tego.

[^1]: Ten [wykres](https://time.com/6300942/ai-progress-charts/) pokazuje zestaw zadań; można by dodać do tego wykresu wiele podobnych krzywych. Ten szybki postęp w wąskiej AI zaskoczył nawet ekspertów w tej dziedzinie, z benchmarkami przekraczanymi na lata przed przewidywaniami.

[^2]: Deepmind, OpenAI, Anthropic i X.ai zostały założone ze specyficznym celem opracowania AGI. Na przykład statut OpenAI wyraźnie określa jego cel jako opracowanie „sztucznej inteligencji ogólnej, która przynosi korzyści całej ludzkości", podczas gdy misją DeepMind jest „rozwiązanie inteligencji, a następnie użycie jej do rozwiązania wszystkiego innego". Meta, Microsoft i inne firmy realizują teraz zasadniczo podobne ścieżki. Meta stwierdziła, że [planuje opracować AGI i udostępnić ją otwarcie.](https://www.forbes.com/sites/johnkoetsier/2024/01/18/zuckerberg-on-ai-meta-building-agi-for-everyone-and-open-sourcing-it/)

[^3]: Hinton i Bengio to dwaj z najczęściej cytowanych badaczy AI, obaj zdobyli Nobla dziedziny AI, nagrodę Turinga, a Hinton dodatkowo zdobył Nagrodę Nobla (w fizyce).

[^4]: Budowanie czegoś o takim ryzyku, pod wpływem zachęt komercyjnych i przy prawie zerowym nadzorze rządowym, jest całkowicie bezprecedensowe. Nie ma nawet kontrowersji co do ryzyka wśród tych, którzy to budują! Liderzy Deepmind, OpenAI i Anthropic, wśród wielu innych ekspertów, wszyscy dosłownie podpisali [oświadczenie](https://www.safe.ai/work/statement-on-ai-risk), że zaawansowana AI stanowi *ryzyko wyginięcia dla ludzkości.* Dzwony alarmowe nie mogłyby dzwonić głośniej, i można jedynie dojść do wniosku, że ci, którzy je ignorują, po prostu nie traktują AGI i superinteligencji poważnie. Jednym z celów tego eseju jest pomóc im zrozumieć, dlaczego powinni.