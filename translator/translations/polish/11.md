# Aneksy

Informacje uzupełniające, w tym - szczegóły techniczne dotyczące rozliczania mocy obliczeniowej, przykład implementacji 'zamknięcia bram', szczegóły surowego reżimu odpowiedzialności za AGI oraz wielopoziomowe podejście do standardów bezpieczeństwa i ochrony AGI.

## Aneks A: Szczegóły techniczne rozliczania mocy obliczeniowej

Szczegółowa metoda zarówno dla "prawdy podstawowej" jak i dobrych przybliżeń całkowitej mocy obliczeniowej używanej w treningu i inferencji jest wymagana dla znaczących kontroli opartych na mocy obliczeniowej. Oto przykład jak "prawda podstawowa" mogłaby być zliczana na poziomie technicznym.

**Definicje:**

*Graf przyczynowy obliczeń:* Dla danego wyjścia O modelu AI, istnieje zestaw obliczeń cyfrowych, dla których zmiana wyniku tego obliczenia mogłaby potencjalnie zmienić O. (To powinno być konserwatywnie założone, tj. powinien istnieć jasny powód by wierzyć, że obliczenie jest niezależne od prekursora, który zarówno występuje wcześniej w czasie i ma fizyczną potencjalną ścieżkę przyczynowego oddziaływania.) To obejmuje obliczenia wykonane przez model AI podczas inferencji, jak również obliczenia, które weszły w skład danych wejściowych, przygotowania danych i treningu modelu. Ponieważ każde z nich może samo być wyjściem z modelu AI, jest to obliczane rekursywnie, odcinane tam gdzie człowiek dostarczył znaczącą zmianę do danych wejściowych.

*Moc obliczeniowa treningu:* Całkowita moc obliczeniowa, w FLOP lub innych jednostkach, zawarta w grafie przyczynowym obliczeń sieci neuronowej (włączając przygotowanie danych, trening i dostrajanie, oraz wszelkie inne obliczenia.)

*Moc obliczeniowa wyjścia:* Całkowita moc obliczeniowa w grafie przyczynowym obliczeń danego wyjścia AI, włączając wszystkie sieci neuronowe (i włączając ich Moc obliczeniową treningu) i inne obliczenia wchodzące w skład tego wyjścia.

*Tempo mocy obliczeniowej inferencji:* W serii wyjść, tempo zmian (w FLOP/s lub innych jednostkach) Mocy obliczeniowej wyjścia między wyjściami, tj. moc obliczeniowa użyta do wyprodukowania następnego wyjścia, podzielona przez zmierzony odstęp czasowy między wyjściami.

**Przykłady i przybliżenia:**

- Dla pojedynczej sieci neuronowej wytrenowanej na danych stworzonych przez ludzi, Moc obliczeniowa treningu to po prostu całkowita moc obliczeniowa treningu jak jest zwyczajowo raportowana.
- Dla takiej sieci neuronowej wykonującej inferencję w stałym tempie, Tempo mocy obliczeniowej inferencji to w przybliżeniu całkowita prędkość klastra obliczeniowego wykonującego inferencję w FLOP/s.
- Dla dostrajania modelu, Moc obliczeniowa treningu kompletnego modelu jest dana przez Moc obliczeniową treningu modelu nie-dostrajanego plus obliczenia wykonane podczas dostrajania i do przygotowania wszelkich danych używanych w dostrajaniu.
- Dla modelu destylowanego, Moc obliczeniowa treningu kompletnego modelu obejmuje trening zarówno modelu destylowanego jak i większego modelu użytego do dostarczenia syntetycznych danych lub innego wejścia treningowego.
- Jeśli kilka modeli jest trenowanych, ale wiele "prób" jest odrzucanych na podstawie ludzkiej oceny, te nie liczą się do Mocy obliczeniowej treningu lub wyjścia zachowanego modelu.

## Aneks B: Przykład implementacji zamknięcia bram

**Przykład implementacji:** Oto jeden przykład jak zamknięcie bram mogłoby działać, przy założeniu limitu 10<sup>27</sup> FLOP dla treningu i 10<sup>20</sup> FLOP/s dla inferencji (uruchamiania AI):

**1. Pauza:** Z powodów bezpieczeństwa narodowego, władza wykonawcza USA prosi wszystkie firmy z siedzibą w USA, prowadzące działalność w USA, lub używające chipów wyprodukowanych w USA, aby zaprzestały wszelkich nowych przebiegów treningu AI, które mogłyby przekroczyć limit 10<sup>27</sup> FLOP Mocy obliczeniowej treningu. USA powinny rozpocząć dyskusje z innymi krajami goszczącymi rozwój AI, silnie zachęcając je do podjęcia podobnych kroków i wskazując, że pauza USA może zostać zniesiona, gdyby zdecydowały się nie zastosować.

**2. Nadzór i licencjonowanie USA:** Poprzez zarządzenie wykonawcze lub działanie istniejącej agencji regulacyjnej, USA wymagają, aby w ciągu (powiedzmy) jednego roku:

- Wszystkie przebiegi treningu AI szacowane powyżej 10<sup>25</sup> FLOP wykonane przez firmy działające w USA były zarejestrowane w bazie danych prowadzonej przez amerykańską agencję regulacyjną. (Uwaga: Nieco słabsza wersja tego była już zawarta w anulowanym zarządzeniu wykonawczym USA z 2023 roku dotyczącym AI, wymagającym rejestracji dla modeli powyżej 10<sup>26</sup> FLOP.)
- Wszyscy producenci sprzętu związanego z AI działający w USA lub prowadzący interesy z rządem USA przestrzegali zestawu wymagań dotyczących ich specjalistycznego sprzętu i oprogramowania go sterującego. (Wiele z tych wymagań mogłoby zostać wbudowanych w aktualizacje oprogramowania i firmware dla istniejącego sprzętu, ale długoterminowe i solidne rozwiązania wymagałyby zmian w późniejszych generacjach sprzętu.) Wśród nich jest wymaganie, że jeśli sprzęt jest częścią szybko połączonego klastra zdolnego do wykonywania 10<sup>18</sup> FLOP/s obliczeń, wymagany jest wyższy poziom weryfikacji, który obejmuje regularne pozwolenie przez zdalny "gubernator", który otrzymuje zarówno telemetrię jak i prośby o wykonanie dodatkowych obliczeń.
- Zarządca raportuje całkowite obliczenia wykonane na jego sprzęcie do agencji prowadzącej amerykańską bazę danych.
- Silniejsze wymagania są wprowadzane fazowo, aby umożliwić zarówno bezpieczniejszy jak i bardziej elastyczny nadzór i pozwolenia.

**3. Nadzór międzynarodowy:**

- USA, Chiny i wszelkie inne kraje goszczące zaawansowane zdolności produkcji chipów negocjują międzynarodowe porozumienie.
- To porozumienie tworzy nową międzynarodową agencję, analogiczną do Międzynarodowej Agencji Energii Atomowej, odpowiedzialną za nadzór treningu i wykonywania AI.
- Kraje sygnatariusze muszą wymagać od swoich krajowych producentów sprzętu AI przestrzegania zestawu wymagań co najmniej tak silnych jak te nałożone w USA.
- Zarządcy są teraz zobowiązani do raportowania liczb obliczeń AI zarówno do agencji w swoich krajach macierzystych jak i do nowego biura w międzynarodowej agencji.
- Dodatkowe kraje są silnie zachęcane do przyłączenia się do istniejącego międzynarodowego porozumienia: kontrole eksportu przez kraje sygnatariusze ograniczają dostęp do zaawansowanego sprzętu przez nie-sygnatariuszy, podczas gdy sygnatariusze mogą otrzymać wsparcie techniczne w zarządzaniu swoimi systemami AI.

**4. Międzynarodowa weryfikacja i egzekwowanie:**

- System weryfikacji sprzętu jest aktualizowany tak, aby raportował użycie obliczeń zarówno do oryginalnego zarządcy jak i bezpośrednio do biura międzynarodowej agencji.
- Agencja, poprzez dyskusję z sygnatariuszami międzynarodowego porozumienia, uzgadnia ograniczenia obliczeniowe, które następnie nabierają mocy prawnej w krajach sygnatariuszy.
- Równolegle, zestaw międzynarodowych standardów może zostać opracowany tak, aby trening i uruchamianie AI powyżej progu obliczeń (ale poniżej limitu) były zobowiązane do przestrzegania tych standardów.
- Agencja może, jeśli konieczne dla kompensacji lepszych algorytmów itp., obniżyć limit obliczeń. Lub, jeśli zostanie uznane za bezpieczne i wskazane (na poziomie dowodliwych gwarancji bezpieczeństwa), podnieść limit obliczeń.

## Aneks C: Szczegóły surowego reżimu odpowiedzialności za AGI

**Szczegóły surowego reżimu odpowiedzialności za AGI**

- Tworzenie i działanie zaawansowanego systemu AI, który jest wysoce ogólny, zdolny i autonomiczny, jest uważane za działalność "nienormalnie niebezpieczną".
- W związku z tym, domyślna odpowiedzialność za trening i działanie takich systemów to surowa, solidarna odpowiedzialność (lub jej nie-amerykański odpowiednik) za wszelkie szkody wyrządzone przez model lub jego wyjścia/działania.
- Odpowiedzialność osobista będzie nałożona na kadry kierownicze i członków zarządu w przypadkach rażącego zaniedbania lub umyślnego wykroczenia. To powinno obejmować kary kryminalne dla najbardziej rażących przypadków.
- Istnieją liczne bezpieczne przystanie, pod którymi odpowiedzialność wraca do domyślnej (opartej na winie, w USA) odpowiedzialności, której ludzie i firmy byliby normalnie poddani.
	- Modele trenowane i działające poniżej pewnego progu obliczeniowego (który byłby co najmniej 10x niższy niż limity opisane powyżej.)
	- AI, które jest "słabe" (z grubsza, poniżej poziomu ludzkiego eksperta w zadaniach, do których jest przeznaczone) i/lub
	- AI, które jest "wąskie" (mające ustalony i dość ograniczony zakres zadań i operacji, do których jest specjalnie zaprojektowane i wytrenowane) i/lub
	- AI, które jest "pasywne" (bardzo ograniczone w swojej zdolności – nawet pod skromną modyfikacją – do podejmowania działań lub wykonywania złożonych wieloetapowych zadań bez bezpośredniego ludzkiego zaangażowania i kontroli.)
	- AI, które jest gwarantowane jako bezpieczne, zabezpieczone i kontrolowalne (dowodliwie bezpieczne, lub analiza ryzyka wskazuje na znikomy poziom oczekiwanej szkody.)
- Bezpieczne przystanie mogą być zgłaszane na podstawie [przypadku bezpieczeństwa](https://arxiv.org/abs/2410.21572) przygotowanego przez dewelopera AI i zatwierdzonego przez agencję lub audytora akredytowanego przez agencję. Aby zgłosić bezpieczne przystanie oparte na obliczeniach, deweloper musi tylko dostarczyć wiarygodne oszacowania całkowitej Mocy obliczeniowej treningu i maksymalnego Tempa inferencji
- Prawodawstwo wyraźnie określiłoby sytuacje, w których nakaz sądowy zaprzestania rozwoju systemów AI z wysokim ryzykiem szkody publicznej byłby odpowiedni.
- Konsorcja firm, współpracujące z organizacjami pozarządowymi i agencjami rządowymi, powinny opracować standardy i normy definiujące te terminy, jak regulatorzy powinni przyznawać bezpieczne przystanie, jak deweloperzy AI powinni rozwijać przypadki bezpieczeństwa, i jak sądy powinny interpretować odpowiedzialność tam gdzie bezpieczne przystanie nie są proaktywnie zgłaszane.

## Aneks D: Wielopoziomowe podejście do standardów bezpieczeństwa i ochrony AGI

**Wielopoziomowe podejście do standardów bezpieczeństwa i ochrony AGI**

| Poziom ryzyka | Wyzwalacz(e) | Wymagania dla treningu | Wymaganie dla wdrożenia |
| --- | --- | --- | --- |
| PR-0 | AI słabe w autonomii, ogólności i inteligencji | brak | brak |
| PR-1 | AI silne w jednym z: autonomii, ogólności i inteligencji | brak | W oparciu o ryzyko i użycie, potencjalnie przypadki bezpieczeństwa zatwierdzone przez władze narodowe wszędzie tam, gdzie model może być używany |
| PR-2 | AI silne w dwóch z: autonomii, ogólności i inteligencji | Rejestracja u władz narodowych mających jurysdykcję nad deweloperem | Przypadek bezpieczeństwa ograniczający ryzyko większej szkody poniżej autoryzowanych poziomów plus niezależne audyty bezpieczeństwa (włączając redteaming czarnej i białej skrzynki) zatwierdzone przez władze narodowe wszędzie tam, gdzie model może być używany |
| PR-3 | AGI silne w autonomii, ogólności i inteligencji | Wstępne zatwierdzenie planu bezpieczeństwa i ochrony przez władze narodowe mające jurysdykcję nad deweloperem | Przypadek bezpieczeństwa gwarantujący ograniczone ryzyko większej szkody poniżej autoryzowanych poziomów oraz wymagane specyfikacje, włączając cyberbezpieczeństwo, kontrolowalność, nieusuwalny wyłącznik awaryjny, wyrównanie z ludzkimi wartościami i odporność na złośliwe użycie. |
| PR-4 | Jakikolwiek model, który również przekracza 10<sup>27</sup> FLOP Treningu lub 10<sup>20</sup> FLOP/s Inferencji | Zakazane do czasu międzynarodowo uzgodnionego zniesienia limitu obliczeniowego | Zakazane do czasu międzynarodowo uzgodnionego zniesienia limitu obliczeniowego |

Klasyfikacje ryzyka i standardy bezpieczeństwa/ochrony, z poziomami opartymi na progach obliczeniowych jak również kombinacjach wysokiej autonomii, ogólności i inteligencji:

- *Silna autonomia* ma zastosowanie jeśli system jest zdolny do wykonywania, lub może być łatwo przystosowany do wykonywania, wieloetapowych zadań i/lub podejmowania złożonych działań, które są istotne w świecie rzeczywistym, bez znaczącego ludzkiego nadzoru lub interwencji. Przykłady: autonomiczne pojazdy i roboty; boty do handlu finansowego. Nie-przykłady: GPT-4; klasyfikatory obrazów
- *Silna ogólność* wskazuje szeroki zakres zastosowań, wykonywanie zadań, do których model nie był celowo i specjalnie trenowany, i znaczącą zdolność uczenia się nowych zadań. Przykłady: GPT-4; mu-zero. Nie-przykłady: AlphaFold; autonomiczne pojazdy; generatory obrazów
- *Silna inteligencja* odpowiada dorównywaniu ludzkiemu poziomowi eksperta w zadaniach, w których model działa najlepiej (a dla modelu ogólnego, w szerokim zakresie zadań.) Przykłady: AlphaFold; mu-zero; o3. Nie-przykłady: GPT-4; Siri