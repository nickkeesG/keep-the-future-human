# Rozdział 2 - Podstawowe informacje o sieciach neuronowych AI

Jak działają nowoczesne systemy AI i co możemy spodziewać się po kolejnym pokoleniu AI?

Aby zrozumieć, jak przebiegną konsekwencje rozwoju potężniejszej AI, kluczowe jest opanowanie pewnych podstaw. Ten i następne dwa rozdziały je omawiają, przedstawiając kolejno czym jest nowoczesna AI, jak wykorzystuje masowe obliczenia oraz w jakim sensie szybko zyskuje na ogólności i możliwościach.[^1]

Istnieje wiele sposobów definiowania sztucznej inteligencji, ale dla naszych celów kluczową właściwością AI jest to, że podczas gdy standardowy program komputerowy to lista instrukcji dotyczących wykonania zadania, system AI to taki, który uczy się z danych lub doświadczenia wykonywać zadania *nie będąc wprost instruowanym, jak to robić.*

Niemal cała istotna nowoczesna AI opiera się na sieciach neuronowych. To struktury matematyczne/obliczeniowe, reprezentowane przez bardzo duży (miliardy lub biliony) zbiór liczb („wagi"), które dobrze wykonują zadanie treningowe. Te wagi są tworzone (a może „hodowane" lub „znajdowane") poprzez iteracyjne dostrajanie tak, aby sieć neuronowa poprawiała wynik liczbowy (zwany także „stratą") zdefiniowany w kierunku dobrego wykonywania jednego lub więcej zadań.[^2] Ten proces nazywa się *treningiem* sieci neuronowej.[^3]

Istnieje wiele technik przeprowadzania tego treningu, ale te szczegóły są znacznie mniej istotne niż sposoby definiowania oceniania i to, jak skutkują różnymi zadaniami, które sieć neuronowa wykonuje dobrze. Kluczowe rozróżnienie historycznie przeprowadzano między AI „wąską" a „ogólną".

AI wąska jest celowo trenowana do wykonywania konkretnego zadania lub niewielkiego zestawu zadań (takich jak rozpoznawanie obrazów czy gra w szachy); wymaga ponownego treningu dla nowych zadań i ma wąski zakres możliwości. Mamy nadludzką AI wąską, co oznacza, że dla niemal każdego dyskretnego, dobrze zdefiniowanego zadania, które może wykonać człowiek, prawdopodobnie potrafimy skonstruować ocenę, a następnie skutecznie wytrenować system AI wąskiej, aby wykonywał to lepiej niż człowiek.

Systemy AI ogólnego przeznaczenia (GPAI) potrafią wykonywać szeroki zakres zadań, w tym wiele takich, do których nie były wprost trenowane; mogą także uczyć się nowych zadań w ramach swojego działania. Obecne duże „modele multimodalne" [^4] jak ChatGPT to przykłady tego: wytrenowane na bardzo dużym korpusie tekstów i obrazów, potrafią prowadzić złożone rozumowania, pisać kod, analizować obrazy i pomagać w ogromnej gamie zadań intelektualnych. Choć wciąż znacznie różnią się od ludzkiej inteligencji w sposób, który szczegółowo zobaczymy poniżej, ich ogólność spowodowała rewolucję w AI.[^5]

## Nieprzewidywalność: kluczowa cecha systemów AI

Główną różnicą między systemami AI a konwencjonalnym oprogramowaniem jest przewidywalność. Rezultat standardowego oprogramowania może być nieprzewidywalny – rzeczywiście, czasami właśnie dlatego piszemy oprogramowanie, aby dało nam wyniki, których nie mogliśmy przewidzieć. Ale konwencjonalne oprogramowanie rzadko robi coś, do czego nie zostało zaprogramowane – jego zakres i zachowanie są generalnie zgodne z projektem. Najwyższej klasy program szachowy może wykonywać ruchy, których żaden człowiek nie mógłby przewidzieć (w przeciwnym razie mogliby pokonać ten program szachowy!), ale generalnie nie będzie robił niczego poza graniem w szachy.

Podobnie jak konwencjonalne oprogramowanie, AI wąska ma przewidywalny zakres i zachowanie, ale może mieć nieprzewidywalne rezultaty. To w rzeczywistości tylko inny sposób definiowania AI wąskiej: jako AI podobnej do konwencjonalnego oprogramowania pod względem przewidywalności i zakresu działania.

AI ogólnego przeznaczenia jest inna: jej zakres (dziedziny, w których się stosuje), zachowanie (rodzaje rzeczy, które robi) i rezultaty (rzeczywiste wyniki) mogą być nieprzewidywalne.[^6] GPT-4 został wytrenowany tylko do dokładnego generowania tekstu, ale rozwinął wiele możliwości, których jego trenerzy nie przewidzieli ani nie zamierzali. Ta nieprzewidywalność wynika ze złożoności treningu: ponieważ dane treningowe zawierają wyniki z wielu różnych zadań, AI musi skutecznie nauczyć się wykonywać te zadania, aby dobrze przewidywać.

Ta nieprzewidywalność ogólnych systemów AI jest dość fundamentalna. Choć w zasadzie możliwe jest staranne skonstruowanie systemów AI, które mają zagwarantowane ograniczenia swojego zachowania (jak wspomniemy później w eseju), sposób, w jaki systemy AI są obecnie tworzone, sprawia, że są nieprzewidywalne w praktyce, a nawet w zasadzie.

## AI pasywna, agenci, systemy autonomiczne i wyrównanie

Ta nieprzewidywalność staje się szczególnie ważna, gdy rozważamy, jak systemy AI są faktycznie wdrażane i używane do osiągania różnych celów.

Wiele systemów AI jest stosunkowo pasywnych w tym sensie, że głównie dostarczają informacji, a użytkownik podejmuje działania. Inne, powszechnie nazywane *agentami*, same podejmują działania, z różnym poziomem zaangażowania użytkownika. Te, które podejmują działania przy stosunkowo mniejszej zewnętrznej kontroli lub nadzorze, można określić jako bardziej *autonomiczne*. Tworzy to spektrum pod względem niezależności działania, od pasywnych narzędzi po autonomicznych agentów.[^7]

Jeśli chodzi o cele systemów AI, mogą być bezpośrednio powiązane z ich celem treningowym (np. cel „wygrywania" dla systemu grającego w Go to także wprost to, do czego został wytrenowany). Albo mogą nie być: cel treningowy ChatGPT to po części przewidywanie tekstu, po części bycie pomocnym asystentem. Ale wykonując dane zadanie, jego cel jest mu dostarczany przez użytkownika. Cele mogą także być tworzone przez sam system AI, tylko bardzo pośrednio związane z jego celem treningowym.[^8]

Cele są ściśle powiązane z kwestią „wyrównania", czyli pytaniem o to, czy systemy AI będą *robić to, czego chcemy, żeby robiły*. To proste pytanie ukrywa ogromny poziom subtelności.[^9] Na razie zauważmy, że „my" w tym zdaniu może odnosić się do wielu różnych ludzi i grup, prowadząc do różnych typów wyrównania. Na przykład, AI może być wysoce *posłuszna* (lub [„lojalna"](https://arxiv.org/abs/2003.11157)) swojemu użytkownikowi – tu „my" to „każdy z nas". Albo może być bardziej *suwerenna*, kierując się głównie własnymi celami i ograniczeniami, ale wciąż działając szeroko w wspólnym interesie ludzkiego dobrobytu – „my" to wtedy „ludzkość" lub „społeczeństwo". Pomiędzy znajduje się spektrum, gdzie AI byłaby w dużej mierze posłuszna, ale mogłaby odmawiać podejmowania działań szkodzących innym lub społeczeństwu, naruszających prawo itp.

Te dwie osie – poziom autonomii i typ wyrównania – nie są całkowicie niezależne. Na przykład, suwerenny system pasywny, choć nie całkiem sprzeczny sam ze sobą, to koncepcja w napięciu, podobnie jak posłuszny agent autonomiczny.[^10] Jest wyraźny sens, w jakim autonomia i suwerenność idą w parze. Podobnie przewidywalność bywa wyższa w systemach AI „pasywnych" i „posłusznych", podczas gdy suwerenne lub autonomiczne będą bardziej nieprzewidywalne. To wszystko będzie kluczowe dla zrozumienia konsekwencji potencjalnej AGI i superinteligencji.

Stworzenie naprawdę wyrównanej AI, jakiegokolwiek rodzaju, wymaga rozwiązania trzech różnych wyzwań:

1. Zrozumienie czego „my" chcemy – co jest złożone, czy „my" oznacza konkretną osobę lub organizację (lojalność), czy ludzkość szeroko (suwerenność);
2. Budowanie systemów, które regularnie działają zgodnie z tymi pragnieniami – zasadniczo tworzenie spójnego pozytywnego zachowania;
3. Najbardziej fundamentalnie, tworzenie systemów, które naprawdę „dbają" o te pragnienia, a nie tylko działają tak, jakby to robiły.

Rozróżnienie między niezawodnym zachowaniem a prawdziwą troską jest kluczowe. Tak jak ludzki pracownik może doskonale wykonywać polecenia, nie mając żadnego rzeczywistego zaangażowania w misję organizacji, system AI może działać w sposób wyrównany, nie ceniąc naprawdę ludzkich preferencji. Możemy trenować systemy AI, żeby mówiły i robiły rzeczy poprzez informacje zwrotne, i mogą nauczyć się rozumować o tym, czego chcą ludzie. Ale sprawienie, żeby *naprawdę* ceniły ludzkie preferencje, to znacznie głębsze wyzwanie.[^11]

Ogromne trudności w rozwiązaniu tych wyzwań wyrównania i ich implikacje dla ryzyka AI zostaną zbadane dalej poniżej. Na razie zrozumcie, że wyrównanie to nie tylko techniczna funkcja, którą doklejamy do systemów AI, ale fundamentalny aspekt ich architektury, który kształtuje ich relację z ludzkością.


[^1]: Dla łagodnego, ale technicznego wprowadzenia do uczenia maszynowego i AI, szczególnie modeli językowych, zobacz [tę stronę.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) Dla innego nowoczesnego wprowadzenia do zagrożeń AI związanych z wyginięciem, zobacz [ten artykuł.](https://www.thecompendium.ai/) Dla kompleksowej i autorytatywnej naukowej analizy stanu bezpieczeństwa AI, zobacz niedawny [Międzynarodowy Raport Bezpieczeństwa AI.](https://arxiv.org/abs/2501.17805)

[^2]: Trening zazwyczaj przebiega poprzez szukanie lokalnego maksimum wyniku w wielowymiarowej przestrzeni danej przez wagi modelu. Sprawdzając, jak wynik zmienia się przy dostrajaniu wag, algorytm treningowy identyfikuje, które dostrojenia najlepiej poprawiają wynik i przesuwa wagi w tym kierunku.

[^3]: Na przykład, w problemie rozpoznawania obrazów, sieć neuronowa wyprowadzałaby prawdopodobieństwa dla etykiet obrazu. Wynik byłby związany z prawdopodobieństwem, które AI przypisuje poprawnej odpowiedzi. Procedura treningowa dostrajałaby następnie wagi tak, żeby następnym razem AI wyprowadzała wyższe prawdopodobieństwo dla poprawnej etykiety tego obrazu. To jest następnie powtarzane ogromną liczbę razy. Ta sama podstawowa procedura jest używana w treningu zasadniczo wszystkich nowoczesnych sieci neuronowych, choć z bardziej złożonymi mechanizmami oceniania.

[^4]: Większość modeli multimodalnych używa architektury „transformer" do przetwarzania i generowania różnych typów danych (tekst, obrazy, dźwięk). Wszystkie te mogą być rozłożone na, a następnie traktowane na równi, jako różne typy „tokenów". Modele multimodalne są trenowane najpierw do dokładnego przewidywania tokenów w masowych zbiorach danych, potem udoskonalane poprzez uczenie ze wzmocnieniem w celu wzbogacenia możliwości i kształtowania zachowań.

[^5]: To, że modele językowe są trenowane do robienia jednej rzeczy – przewidywania słów – sprawiło, że niektórzy nazywają je AI wąską. To jednak mylące: ponieważ dobre przewidywanie tekstu wymaga bardzo wielu różnych możliwości, to zadanie treningowe prowadzi do zaskakująco ogólnego systemu. Zauważcie także, że te systemy są intensywnie trenowane przez uczenie ze wzmocnieniem, skutecznie reprezentując tysiące ludzi dających modelowi sygnał nagrody, gdy dobrze wykonuje którekolwiek z wielu rzeczy, które robi. Dziedziczy więc znaczną ogólność od ludzi dających tę informację zwrotną.

[^6]: Jest wiele sposobów, w jakich AI jest nieprzewidywalna. Jeden to że w ogólnym przypadku nie można przewidzieć, co zrobi algorytm, nie uruchamiając go faktycznie; są [twierdzenia](https://arxiv.org/abs/1310.3225) na ten temat. To może być prawdą po prostu dlatego, że wynik algorytmów może być złożony. Ale jest szczególnie jasne i istotne w przypadku (takim jak szachy czy Go), gdzie przewidywanie implikowałoby możliwość (pokonanie AI), której potencjalny przewidywacz nie ma. Po drugie, dany system AI nie zawsze wyprodukuje ten sam wynik nawet przy tym samym wejściu – jego wyniki zawierają losowość; to także łączy się z nieprzewidywalnością algorytmiczną. Po trzecie, nieoczekiwane i wyłaniające się możliwości mogą powstać z treningu, co oznacza, że nawet *typy* rzeczy, które system AI może i będzie robić, są nieprzewidywalne; Ten ostatni typ jest szczególnie ważny dla rozważań bezpieczeństwa.

[^7]: Zobacz [tutaj](https://arxiv.org/abs/2502.02649) dla pogłębionego przeglądu tego, co oznacza „agent autonomiczny" (wraz z etycznymi argumentami przeciwko ich budowaniu).

[^8]: Czasami możecie słyszeć „AI nie może mieć własnych celów". To kompletny nonsens. Łatwo jest wygenerować przykłady, gdzie AI ma lub rozwija cele, które nigdy jej nie podano i są znane tylko jej samej. Nie widzicie tego często w obecnych popularnych modelach multimodalnych, bo jest to z nich wytrenowane; równie łatwo można by to w nie wytrenować.

[^9]: Jest obszerna literatura. O ogólnym problemie zobacz *The Alignment Problem* Christiana i *Human-Compatible* Russella. Na bardziej technicznej stronie zobacz np. [ten artykuł](https://arxiv.org/abs/2209.00626).

[^10]: Zobaczymy później, że choć takie systemy przełamują trend, właśnie to czyni je bardzo interesującymi i użytecznymi.

[^11]: Nie oznacza to, że wymagamy emocji czy świadomości. Raczej jest to ogromnie trudne z zewnątrz systemu wiedzieć, jakie są jego wewnętrzne cele, preferencje i wartości. „Prawdziwe" tutaj oznaczałoby, że mamy wystarczająco silne powody, by na tym polegać, że w przypadku krytycznych systemów możemy na to postawić nasze życie.