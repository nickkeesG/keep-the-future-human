# Rozdział 3 - Kluczowe aspekty tworzenia nowoczesnych systemów sztucznej inteligencji ogólnej

Większość najbardziej zaawansowanych systemów AI na świecie powstaje przy użyciu zaskakująco podobnych metod. Oto podstawy.

Aby naprawdę zrozumieć człowieka, trzeba wiedzieć coś o biologii, ewolucji, wychowaniu dzieci i więcej; aby zrozumieć AI, również trzeba wiedzieć, jak jest tworzona. W ciągu ostatnich pięciu lat systemy AI ewoluowały ogromnie zarówno pod względem możliwości, jak i złożoności. Kluczowym czynnikiem umożliwiającym ten rozwój była dostępność bardzo dużych ilości mocy obliczeniowej (potocznie nazywanej "compute" w kontekście AI).

Liczby są oszałamiające. Około 10<sup>25</sup>-10<sup>26</sup> "operacji zmiennoprzecinkowych" (FLOP)[^1] jest używanych w treningu modeli takich jak seria GPT, Claude, Gemini itp.[^2] (Dla porównania, gdyby każdy człowiek na Ziemi pracował bez przerwy, wykonując jedno obliczenie co pięć sekund, zajęłoby to około miliarda lat). Ta ogromna ilość mocy obliczeniowej umożliwia trening modeli z bilionami parametrów na terabajtach danych – dużej części wszystkich wysokiej jakości tekstów, jakie kiedykolwiek napisano, wraz z ogromnymi bibliotekami dźwięków, obrazów i filmów. Uzupełniając ten trening dodatkowymi, rozległymi treningami wzmacniającymi preferencje ludzkie i dobrą wydajność zadaniową, modele trenowane w ten sposób wykazują wydajność konkurencyjną z ludzką w znacznym zakresie podstawowych zadań intelektualnych, w tym rozumowania i rozwiązywania problemów.

Wiemy także (bardzo, bardzo w przybliżeniu), jaka szybkość obliczeniowa, w operacjach na sekundę, wystarcza, aby szybkość *inferencji*[^3] takiego systemu dorównała *szybkości* ludzkiego przetwarzania tekstu. To około 10<sup>15</sup>-10<sup>16</sup> FLOP na sekundę.[^4]

Choć potężne, te modele są ze swej natury ograniczone w kluczowych aspektach, podobnie jak byłby ograniczony pojedynczy człowiek zmuszony do zwykłego wypuszczania tekstu przy stałym tempie słów na minutę, bez zatrzymywania się do myślenia czy używania dodatkowych narzędzi. Nowsze systemy AI radzą sobie z tymi ograniczeniami poprzez bardziej złożony proces i architekturę łączącą kilka kluczowych elementów:

- Jedna lub więcej sieci neuronowych, z jednym modelem zapewniającym podstawową zdolność poznawczą i kilkoma innymi wykonującymi inne, bardziej wąskie zadania;
- *Narzędzia* udostępniane modelowi i przez niego używane – na przykład możliwość przeszukiwania internetu, tworzenia lub edytowania dokumentów, wykonywania programów itp.
- *Szkielet wspomagający*, który łączy wejścia i wyjścia sieci neuronowych. Bardzo prosty szkielet może po prostu pozwolić dwóm "instancjom" modelu AI na konwersację ze sobą, lub jednej na sprawdzanie pracy drugiej.[^5]
- *Łańcuch rozumowania* i powiązane techniki promptowania robią coś podobnego, powodując, że model na przykład generuje wiele podejść do problemu, a następnie przetwarza te podejścia dla zagregowanej odpowiedzi.
- *Ponowny trening* modeli, aby lepiej wykorzystywały narzędzia, szkielety wspomagające i łańcuchy rozumowania.

Ponieważ te rozszerzenia mogą być bardzo potężne (i obejmować same systemy AI), te złożone systemy mogą być dość wyrafinowane i dramatycznie wzmacniać możliwości AI.[^6] A niedawno techniki szkieletów wspomagających, a zwłaszcza promptowanie łańcuchem rozumowania (i włączanie wyników z powrotem do ponownego treningu modeli, aby lepiej je wykorzystywały) zostały opracowane i zastosowane w [o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/) i [DeepSeek R1](https://api-docs.deepseek.com/news/news250120) do wykonywania wielu przebiegów inferencji w odpowiedzi na dane zapytanie.[^7] To w efekcie pozwala modelowi "zastanowić się nad" swoją odpowiedzią i dramatycznie wzmacnia zdolność tych modeli do wysokiej jakości rozumowania w zadaniach naukowych, matematycznych i programistycznych.[^8]

Dla danej architektury AI zwiększenia mocy obliczeniowej treningu [można niezawodnie przełożyć](https://arxiv.org/abs/2405.10938) na ulepszenia w zestawie jasno zdefiniowanych metryk. Dla mniej precyzyjnie zdefiniowanych zdolności ogólnych (takich jak te omawiane poniżej) przełożenie jest mniej jasne i przewidywalne, ale jest niemal pewne, że większe modele z większą mocą obliczeniową treningu będą miały nowe i lepsze możliwości, nawet jeśli trudno przewidzieć, jakie to będą.

Podobnie, systemy złożone, a zwłaszcza postępy w "łańcuchu rozumowania" (i treningu modeli, które dobrze z nim współpracują) odblokowały skalowanie mocy obliczeniowej *inferencji*: dla danego wytrenowanego modelu podstawowego przynajmniej niektóre możliwości systemu AI zwiększają się wraz z aplikowaniem większej mocy obliczeniowej, która pozwala im "myśleć ciężej i dłużej" nad złożonymi problemami. Wiąże się to ze stromym kosztem szybkości obliczeniowej, wymagając setek lub tysięcy więcej FLOP/s, aby dorównać ludzkiej wydajności.[^9]

Choć to tylko część tego, co prowadzi do szybkiego postępu AI,[^10] rola mocy obliczeniowej i możliwość systemów złożonych okaże się kluczowa zarówno dla zapobiegania niekontrolowanej AGI, jak i rozwijania bezpieczniejszych alternatyw.

[^1]: 10<sup>27</sup> oznacza 1 z 25 zerami, czyli dziesięć bilionów bilionów. FLOP to po prostu arytmetyczne dodawanie lub mnożenie liczb z określoną precyzją. Należy zauważyć, że wydajność sprzętu AI może się różnić dziesięciokrotnie w zależności od precyzji arytmetyki i architektury komputera. Liczenie operacji bramek logicznych (AND, OR, NOT) byłoby fundamentalne, ale nie są one powszechnie dostępne ani benchmarkowane; dla obecnych celów użyteczne jest standaryzowanie na operacjach 16-bitowych (FP16), chociaż należy ustalić odpowiednie współczynniki konwersji.

[^2]: Zbiór oszacowań i twardych danych jest dostępny od [Epoch AI](https://epochai.org/data/large-scale-ai-models) i wskazuje około 2×10<sup>25</sup> 16-bitowych FLOP dla GPT-4; to z grubsza pasuje do [liczb, które wyciekły](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) dla GPT-4. Oszacowania dla innych modeli z połowy 2024 roku mieszczą się w granicach kilku razy więcej niż GPT-4.

[^3]: Inferencja to po prostu proces generowania wyjścia z sieci neuronowej. Trening można uznać za sukcesję wielu inferencji i dostrajania wag modelu.

[^4]: Do produkcji tekstu pierwotny GPT-4 wymagał 560 TFLOP na wygenerowany token. Około 7 tokenów/s jest potrzebne, aby nadążyć za ludzką myślą, więc to daje ≈3×10<sup>15</sup> FLOP/s. Ale usprawnienia to zmniejszyły; [ta broszura NVIDIA](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/) na przykład wskazuje zaledwie 3×10<sup>14</sup> FLOP/s dla porównywalnie działającego modelu Llama 405B.

[^5]: Jako nieco bardziej złożony przykład, system AI może najpierw wygenerować kilka możliwych rozwiązań problemu matematycznego, następnie użyć innej instancji do sprawdzenia każdego rozwiązania, a na koniec użyć trzeciej do syntezy wyników w jasne wyjaśnienie. To pozwala na dokładniejsze i bardziej niezawodne rozwiązywanie problemów niż pojedyncze przejście.

[^6]: Zobacz na przykład szczegóły na temat ["Operatora" OpenAI](https://openai.com/index/introducing-operator/), [możliwości narzędzi Claude'a](https://docs.anthropic.com/en/docs/build-with-claude/computer-use) i [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). [Deep Research](https://openai.com/index/introducing-deep-research/) OpenAI prawdopodobnie ma dość wyrafinowaną architekturę, ale szczegóły nie są dostępne.

[^7]: Deepseek R1 opiera się na iteracyjnym treningu i promptowaniu modelu tak, aby ostateczny wytrenowany model tworzył rozbudowane rozumowanie łańcuchowe. Szczegóły architektoniczne nie są dostępne dla o1 lub o3, jednak Deepseek ujawnił, że nie ma potrzeby szczególnego "sekretnego składnika" do odblokowania skalowania możliwości z inferencją. Ale pomimo otrzymania ogromnej uwagi prasy jako podważającego "status quo" w AI, nie wpływa to na główne twierdzenia tego eseju.

[^8]: Te modele znacznie przewyższają standardowe modele w benchmarkach rozumowania. Na przykład w GPQA Diamond Benchmark – rygorystycznym teście pytań naukowych na poziomie doktoratu – GPT-4o [uzyskał](https://openai.com/index/learning-to-reason-with-llms/) 56%, podczas gdy o1 i o3 osiągnęły odpowiednio 78% i 88%, znacznie przewyższając 70% średni wynik ekspertów ludzkich.

[^9]: O3 OpenAI prawdopodobnie wydatkował ∼10<sup>21</sup>-10<sup>22</sup> FLOP [na ukończenie każdego z pytań wyzwania ARC-AGI](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai), które kompetentni ludzie mogą zrobić w (powiedzmy) 10-100 sekund, dając liczbę bardziej jak ∼10<sup>20</sup> FLOP/s.

[^10]: Choć moc obliczeniowa jest kluczową miarą możliwości systemu AI, współdziała ona zarówno z jakością danych, jak i ulepszeniami algorytmicznymi. Lepsze dane lub algorytmy mogą zmniejszyć wymagania obliczeniowe, podczas gdy większa moc obliczeniowa może czasami kompensować słabsze dane lub algorytmy.