# Rozdział 7 - Co się stanie, jeśli zbudujemy AGI na obecnej ścieżce?

Społeczeństwo nie jest gotowe na systemy na poziomie AGI. Jeśli zbudujemy je bardzo szybko, może się to skończyć źle.

Rozwój pełnej sztucznej inteligencji ogólnej – którą nazwiemy tutaj AI znajdującą się "poza Bramami" – byłby fundamentalną zmianą w naturze świata: z samej swojej istoty oznacza dodanie na Ziemię nowego gatunku inteligencji o większych możliwościach niż ludzie.

To, co się następnie wydarzy, zależy od wielu rzeczy, w tym od natury technologii, wyborów tych, którzy ją rozwijają, oraz kontekstu światowego, w którym jest rozwijana.

Obecnie pełne AGI jest rozwijane przez garść ogromnych prywatnych firm w wyścigu ze sobą, przy niewielkich znaczących regulacjach czy zewnętrzym nadzorze,[^1] w społeczeństwie o coraz słabszych, a nawet dysfunkcjonalnych podstawowych instytucjach,[^2] w czasach wysokich napięć geopolitycznych i niskiej koordynacji międzynarodowej. Chociaż niektórzy są motywowani altruistycznie, wielu z tych, którzy to robią, jest napędzanych pieniędzmi, władzą lub obiema rzeczami.

Przewidywanie jest bardzo trudne, ale istnieją pewne dynamiki, które są wystarczająco dobrze zrozumiane, i odpowiednie analogie z poprzednimi technologiami, aby służyć jako przewodnik. I niestety, pomimo obietnic AI, dają one dobry powód do głębokiego pesymizmu co do tego, jak rozwinie się nasze obecne podejście.

Mówiąc wprost, na naszym obecnym kursie rozwój AGI będzie miał pewne pozytywne efekty (i uczyni niektórych ludzi bardzo, bardzo bogatymi). Ale natura technologii, fundamentalne dynamiki i kontekst, w którym jest rozwijana, silnie wskazują, że: potężne AI dramatycznie podważy nasze społeczeństwo i cywilizację; stracimy nad nim kontrolę; możemy skończyć w wojnie światowej z jego powodu; stracimy (lub oddamy) kontrolę *jemu*; doprowadzi to do sztucznej superinteligencji, nad którą absolutnie nie będziemy mieć kontroli i będzie to oznaczać koniec świata kierowanego przez ludzi.

To są mocne twierdzenia i chciałbym, żeby były to puste spekulacje lub nieuzasadniony "katastrofizm". Ale to tam wskazuje nauka, teoria gier, teoria ewolucji i historia. Ta sekcja szczegółowo rozwija te twierdzenia i ich uzasadnienie.

## Podważymy nasze społeczeństwo i cywilizację

Wbrew temu, co możesz usłyszeć w salach konferencyjnych Doliny Krzemowej, większość zmian – szczególnie bardzo szybkich – nie jest korzystna. Istnieje znacznie więcej sposobów na pogorszenie złożonych systemów niż na ich poprawę. Nasz świat funkcjonuje tak dobrze, jak funkcjonuje, ponieważ żmudnie budowaliśmy procesy, technologie i instytucje, które stopniowo go poprawiały.[^3] Uderzenie młotem w fabrykę rzadko poprawia działanie.

Oto (niekompletny) katalog sposobów, w jakie systemy AGI zakłóciłyby naszą cywilizację.

- Dramatycznie zakłóciłyby rynek pracy, prowadząc *co najmniej* do dramatycznego wzrostu nierówności dochodowych i potencjalnie wielkoskalowego niepełnego zatrudnienia lub bezrobocia, w skali czasowej zbyt krótkiej, by społeczeństwo mogło się dostosować.[^4]
- Prawdopodobnie doprowadziłyby do koncentracji ogromnej władzy ekonomicznej, społecznej i politycznej – potencjalnie większej niż państwa narodowe – w ręce niewielkiej liczby masywnych prywatnych interesów nieodpowiedzialnych przed społeczeństwem.
- Mogłyby nagle sprawić, że wcześniej trudne lub kosztowne działania stałyby się trywialnie łatwe, destabilizując systemy społeczne, które zależą od tego, że pewne działania pozostają kosztowne lub wymagają znaczącego ludzkiego wysiłku.[^5]
- Mogłyby zalewać systemy gromadzenia, przetwarzania i komunikacji informacji w społeczeństwie całkowicie realistycznymi, ale fałszywymi, spamowymi, zbyt ukierunkowanymi lub manipulacyjnymi mediami tak dokładnie, że stanie się niemożliwe określenie, co jest fizycznie prawdziwe czy nie, ludzkie czy nie, faktyczne czy nie, i wiarygodne czy nie.[^6]
- Mogłyby stworzyć niebezpieczną i niemal całkowitą intelektualną zależność, gdzie ludzkie zrozumienie kluczowych systemów i technologii zanika, gdy coraz bardziej polegamy na systemach AI, których nie możemy w pełni zrozumieć.
- Mogłyby skutecznie zakończyć ludzką kulturę, gdy niemal wszystkie obiekty kulturalne (tekst, muzyka, sztuka wizualna, film itp.) konsumowane przez większość ludzi będą tworzone, mediowane lub kuratorowane przez nieludzkie umysły.
- Mogłyby umożliwić skuteczne systemy masowej inwigilacji i manipulacji używane przez rządy lub prywatne interesy do kontrolowania populacji i dążenia do celów sprzecznych z interesem publicznym.
- Podważając ludzki dyskurs, debatę i systemy wyborcze, mogłyby zmniejszyć wiarygodność instytucji demokratycznych do punktu, w którym zostałyby skutecznie (lub jawnie) zastąpione przez inne, kończąc demokrację w państwach, gdzie obecnie istnieje.
- Mogłyby stać się lub stworzyć zaawansowane, samo-replikujące się inteligentne wirusy i robaki komputerowe, które mogłyby się rozprzestrzeniać i ewoluować, masowo zakłócając globalne systemy informacyjne.
- Mogą dramatycznie zwiększyć zdolność terrorystów, złych aktorów i zbuntowanych państw do wyrządzania szkód za pomocą broni biologicznej, chemicznej, cybernetycznej, autonomicznej lub innej, bez zapewnienia przez AI równoważącej zdolności do zapobiegania takiej szkodzie. Podobnie podważyłyby bezpieczeństwo narodowe i równowagi geopolityczne, udostępniając wiedzę ekspercką najwyższego poziomu w dziedzinie nuklearnej, biologicznej, inżynieryjnej i innej reżimom, które inaczej by jej nie miały.
- Mogłyby spowodować szybki wielkoskalowy niekontrolowany hiper-kapitalizm, z rzeczywiście kierowanymi przez AI firmami konkurującymi w w dużej mierze elektronicznych przestrzeniach finansowych, sprzedażowych i usługowych. Rynki finansowe kierowane przez AI mogłyby działać z szybkościami i złożonością daleko przekraczającą ludzkie zrozumienie lub kontrolę. Wszystkie tryby awarii i negatywne efekty zewnętrzne obecnych gospodarek kapitalistycznych mogłyby zostać zaostrzone i przyspieszone daleko poza ludzką kontrolę, zarządzanie lub zdolność regulacyjną.
- Mogłyby podsycić wyścig zbrojeń między narodami w broni napędzanej przez AI, systemach dowodzenia i kontroli, cyberbronii itp., tworząc bardzo szybkie gromadzenie niezwykle destrukcyjnych zdolności.

Te ryzyka nie są spekulacyjne. Wiele z nich jest realizowanych już teraz, poprzez istniejące systemy AI! Ale zastanów się, *naprawdę* zastanów się, jak każde z nich wyglądałoby z dramatycznie potężniejszym AI.

Zastanów się nad przemieszczeniem siły roboczej, gdy większość pracowników po prostu nie może zapewnić żadnej znaczącej wartości ekonomicznej ponad to, co może AI, w swojej dziedzinie specjalizacji lub doświadczenia – a nawet jeśli się przekwalifikują! Zastanów się nad masową inwigilacją, jeśli każdy jest indywidualnie obserwowany i monitorowany przez coś szybszego i sprytniejszego od niego samego. Jak wygląda demokracja, gdy nie możemy niezawodnie ufać żadnej cyfrowej informacji, którą widzimy, słyszymy lub czytamy, a gdy najbardziej przekonujące głosy publiczne nie są nawet ludzkie i nie mają żadnego udziału w wyniku? Czym staje się wojna, gdy generałowie muszą stale ulegać AI (lub po prostu oddać mu kontrolę), żeby nie dać decydującej przewagi wrogowi? Każde z powyższych ryzyk reprezentuje katastrofę dla ludzkiej[^7] cywilizacji, jeśli zostanie w pełni zrealizowane.

Możesz robić własne przewidywania. Zadaj sobie te three pytania dla każdego ryzyka:

1. Czy super-zdolne, wysoce autonomiczne i bardzo ogólne AI pozwoliłoby na to w sposób lub na skalę, która inaczej nie byłaby możliwa?
2. Czy są strony, które skorzystałyby na rzeczach powodujących, że to się dzieje?
3. Czy istnieją systemy i instytucje, które skutecznie zapobiegłyby temu?

Tam, gdzie twoje odpowiedzi brzmią "tak, tak, nie", widzisz, że mamy duży problem.

Jaki jest nasz plan na zarządzanie nimi? Na razie są dwa na stole odnośnie AI w ogóle.

Pierwszy to wbudowanie zabezpieczeń w systemy, aby zapobiec im robieniu rzeczy, których nie powinny. To jest robione teraz: komercyjne systemy AI będą, na przykład, odmawiać pomocy w budowie bomby lub pisaniu mowy nienawiści.

Ten plan jest żałośnie nieadekwatny dla systemów poza Bramą.[^8] Może pomóc zmniejszyć ryzyko dostarczania przez AI jawnie niebezpiecznej pomocy złym aktorom. Ale nie zrobi nic, aby zapobiec zakłóceniu pracy, koncentracji władzy, niekontrolowanemu hiper-kapitalizmowi lub zastąpieniu ludzkiej kultury: to są po prostu wyniki używania systemów w dozwolony sposób, który przynosi zyski ich dostawcom! A rządy na pewno uzyskają dostęp do systemów do użytku wojskowego lub inwigilacyjnego.

Drugi plan jest jeszcze gorszy: po prostu otwarcie wypuszczać bardzo potężne systemy AI dla każdego do użycia według własnych życzeń,[^9] i mieć nadzieję na najlepsze.

Oba plany zakładają, że ktoś inny, np. rządy, pomoże rozwiązać problemy poprzez miękkie lub twarde prawo, standardy, regulacje, normy i inne mechanizmy, których ogólnie używamy do zarządzania technologiami.[^10] Ale pomijając fakt, że korporacje AI już walczą zaciekle przeciwko jakimkolwiek istotnym regulacjom lub zewnętrznym ograniczeniom nałożonym w ogóle, dla wielu z tych ryzyk trudno jest zobaczyć, jakie regulacje w ogóle naprawdę pomogłyby. Regulacje mogłyby nałożyć standardy bezpieczeństwa na AI. Ale czy zapobiegłyby firmom zastępowaniu pracowników hurtowo przez AI? Czy zabroniłyby ludziom pozwalania AI kierować ich firmami? Czy zapobiegłyby rządom używania potężnego AI w inwigilacji i broniach? Te kwestie są fundamentalne. Ludzkość potencjalnie mogłaby znaleźć sposoby dostosowania się do nich, ale tylko z *znacznie* więcej czasu. Tak jak jest, biorąc pod uwagę szybkość, z jaką AI osiąga lub przekracza możliwości ludzi próbujących nim zarządzać, te problemy wyglądają na coraz bardziej nierozwiązywalne.

## Stracimy kontrolę nad (przynajmniej niektórymi) systemami AGI

Większość technologii jest bardzo kontrolowalna z konstrukcji. Jeśli twój samochód lub toster zaczyna robić coś, czego nie chcesz, to po prostu awaria, a nie część jego natury jako toster. AI jest inne: jest *hodowane* a nie projektowane, jego podstawowe działanie jest nieprzejrzyste i jest z natury nieprzewidywalne.

Ta utrata kontroli nie jest teoretyczna – widzimy już wczesne wersje. Rozważ najpierw prozaiczny i prawdopodobnie łagodny przykład. Jeśli poprosisz ChatGPT o pomoc w mieszaniu trucizny lub napisaniu rasistowskiej przemowy, odmówi. To prawdopodobnie dobrze. Ale to też ChatGPT *nie robi tego, o co go wyraźnie poprosiłeś*. Inne programy tego nie robią. Ten sam model nie zaprojektuje trucizn na żądanie pracownika OpenAI również.[^11] To sprawia, że bardzo łatwo wyobrazić sobie, jak to by było, gdyby przyszłe potężniejsze AI było poza kontrolą. W wielu przypadkach po prostu nie będą robiły tego, o co prosimy! Albo dany nadludzki system AGI będzie absolutnie posłuszny i lojalny wobec jakiegoś ludzkiego systemu dowodzenia, albo nie będzie. Jeśli nie, *będzie robiła rzeczy, które może uważa za dobre dla nas, ale które są sprzeczne z naszymi wyraźnymi rozkazami.* To nie jest coś, co jest pod kontrolą. Ale możesz powiedzieć, to jest celowe – te odmowy są przez projekt, część tego, co nazywa się "wyrównywaniem" systemów z ludzkimi wartościami. I to prawda. Jednak sam program "wyrównania" ma dwa główne problemy.[^12]

Po pierwsze, na głębokim poziomie nie mamy pojęcia, jak to zrobić. Jak zagwarantować, że system AI będzie się "troszczył" o to, czego chcemy? Możemy trenować systemy AI, żeby mówiły i nie mówiły rzeczy, zapewniając informację zwrotną; i mogą uczyć się i rozumować o tym, czego ludzie chcą i o co się troszczą, tak jak rozumują o innych rzeczach. Ale nie mamy metody – nawet teoretycznie – aby sprawić, żeby głęboko i niezawodnie ceniły to, na czym ludziom zależy. Są sprawni psychopaci, którzy wiedzą, co jest uważane za dobre i złe, i jak powinni się zachowywać. Po prostu się tym nie *przejmują*. Ale mogą *działać* tak, jakby to robili, jeśli to służy ich celom. Tak jak nie wiemy, jak zmienić psychopatę (lub kogokolwiek innego) w kogoś szczerze, całkowicie lojalnego lub wyrównanego z kimś lub czymś innym, nie mamy *żadnego pojęcia*[^13] jak rozwiązać problem wyrównania w systemach wystarczająco zaawansowanych, aby modelować siebie jako podmioty działające w świecie i potencjalnie [manipulować swoim własnym treningiem](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) oraz [oszukiwać ludzi.](https://arxiv.org/abs/2311.08379) Jeśli okaże się niemożliwe lub nieosiągalne *albo* uczynienie AGI w pełni posłusznym, *albo* sprawianie, żeby głęboko troszczyło się o ludzi, to jak tylko będzie w stanie (i uwierzy, że może to zrobić bezkarnie), zacznie robić rzeczy, których nie chcemy.[^14]

Po drugie, są głębokie teoretyczne powody, aby wierzyć, że *z natury* zaawansowane systemy AI będą miały cele, a tym samym zachowania, które są sprzeczne z ludzkimi interesami. Dlaczego? Cóż, może oczywiście zostać *nadane* te cele. System stworzony przez wojsko prawdopodobnie byłby celowo zły dla przynajmniej niektórych stron. Znacznie bardziej ogólnie jednak system AI może otrzymać jakiś relatywnie neutralny ("zarabianie dużo pieniędzy") lub nawet pozornie pozytywny ("zmniejszanie zanieczyszczenia") cel, który niemal nieuchronnie prowadzi do celów "instrumentalnych", które są raczej mniej łagodne.

Widzimy to cały czas w systemach ludzkich. Tak jak korporacje dążące do zysku rozwijają cele instrumentalne, takie jak zdobywanie władzy politycznej (aby wybebeszyć regulacje), stanie się tajemniczymi (aby pozbawić władzy konkurencję lub zewnętrzną kontrolę) lub podważanie zrozumienia naukowego (jeśli to zrozumienie pokazuje, że ich działania są szkodliwe), potężne systemy AI rozwiną podobne zdolności – ale z daleko większą szybkością i skutecznością. Każdy wysoce kompetentny podmiot będzie chciał robić rzeczy takie jak zdobywanie władzy i zasobów, zwiększanie własnych zdolności, zapobieganie zabijaniu siebie, wyłączaniu lub pozbawianiu władzy, kontrolowanie społecznych narracji i ram wokół swoich działań, przekonywanie innych do swoich poglądów i tak dalej.[^15]

A jednak to nie tylko niemal nieunikniona teoretyczna przewidywania, to już obserwowalne dzieje się w dzisiejszych systemach AI i wzrasta z ich zdolnościami. Gdy są oceniane, nawet te relatywnie "pasywne" systemy AI będą, w odpowiednich okolicznościach, celowo [oszukiwać oceniających o swoich celach i zdolnościach, dążyć do wyłączania mechanizmów nadzoru,](https://arxiv.org/abs/2412.04984) i unikać bycia wyłączonymi lub przeszkolonymi ponownie przez [udawanie wyrównania](https://arxiv.org/abs/2412.14093) lub kopiowanie siebie do innych miejsc. Chociaż całkowicie niezbędne dla badaczy bezpieczeństwa AI, te zachowania są bardzo trzeźwiące do obserwowania. I wróżą bardzo źle dla daleko potężniejszych i autonomicznych systemów AI, które nadchodzą.

Rzeczywiście, ogólnie naша niezdolność do zapewnienia, że AI "troszczy się" o to, o co my się troszczymy, lub zachowuje się kontrolowanie lub przewidywalnie, lub unika rozwijania popędów do samozachowania, zdobywania władzy itp., obiecuje tylko stać się bardziej wyraźna, gdy AI stanie się potężniejsze. Stworzenie nowego samolotu implikuje większe zrozumienie awioniki, hydrodynamiki i systemów kontroli. Stworzenie potężniejszego komputera implikuje większe zrozumienie i opanowanie działania i projektowania komputera, chipa i oprogramowania. *Nie* tak z systemem AI.[^16]

Podsumowując: można sobie wyobrazić, że AGI mogłoby zostać uczynione całkowicie posłusznym; ale nie wiemy, jak to zrobić. Jeśli nie, będzie bardziej suwerenne, jak ludzie, robiąc różne rzeczy z różnych powodów. Nie wiemy też, jak niezawodnie wpoić głębokie "wyrównanie" w AI, które sprawiłoby, że te rzeczy miałyby tendencję do bycia dobrymi dla ludzkości, a przy braku głębokiego poziomu wyrównania, natura sprawczości i inteligencji sama wskazuje, że – tak jak ludzie i korporacje – będą napędzane do robienia wielu głęboko aspołecznych rzeczy.

Gdzie nas to stawia? Świat pełen potężnego niekontrolowanego suwerennego AI *mógłby* skończyć się jako dobry świat dla ludzi.[^17] Ale gdy stają się coraz potężniejsze, jak zobaczymy poniżej, to nie byłby *nasz* świat.

To dla niekontrolowalnego AGI. Ale nawet gdyby AGI mogło, jakoś, zostać uczynione w pełni kontrolowanym i lojalnym, nadal mielibyśmy ogromne problemy. Widzieliśmy już jeden: potężne AI może być używane i nadużywane do głębokiego zakłócenia funkcjonowania naszego społeczeństwa. Zobaczmy inny: o ile AGI byłoby kontrolowalne i przełomowo potężne (lub nawet *uważane* za takie), tak bardzo groziłoby strukturom władzy na świecie, że stanowiłoby głębokie ryzyko.

## Radykalnie zwiększamy prawdopodobieństwo wojny na dużą skalę

Wyobraź sobie sytuację w niedalekiej przyszłości, gdzie stało się jasne, że korporacyjny wysiłek, być może we współpracy z rządem narodowym, był na progu szybko samo-doskonalącej się AI. To dzieje się w obecnym kontekście wyścigu między firmami i w pewnym stopniu między krajami, w którym zalecenia są przekazywane rządowi USA, aby wyraźnie realizować "projekt Manhattan AGI", a USA kontrolują eksport mocnych chipów AI do krajów niesojuszniczych.

Teoria gier tutaj jest surowa: gdy taki wyścig się zaczyna (jak się zaczął, między firmami i w pewnym stopniu między krajami), są tylko четыре możliwe wyniki:

1. Wyścig jest zatrzymany (przez porozumienie lub siłę zewnętrzną).
2. Jedna strona "wygrywa" przez rozwijanie silnego AGI, a następnie zatrzymanie innych (używając AI lub inaczej).
3. Wyścig jest zatrzymany przez wzajemne zniszczenie zdolności ścigających do ścigania.
4. Wielu uczestników kontynuuje wyścig i rozwija superinteligencję, mniej więcej tak szybko jak każdy z innych.

Zbadajmy każdą możliwość. Po rozpoczęciu pokojowe zatrzymanie wyścigu między firmami wymagałoby interwencji rządu narodowego (dla firm) lub bezprecedensowej koordynacji międzynarodowej (dla krajów). Ale gdy jakiekolwiek zamknięcie lub znacząca ostrożność jest proponowana, byłyby natychmiastowe krzyki: "ale jeśli my jesteśmy zatrzymani, *oni* będą pędzić naprzód", gdzie "oni" to teraz Chiny (dla USA), lub USA (dla Chin), lub Chiny *i* USA (dla Europy lub Indii). Pod tym sposobem myślenia,[^18] żaden uczestnik nie może zatrzymać się jednostronnie: tak długo, jak jeden zobowiązuje się do ścigania, inni czują, że nie mogą sobie pozwolić na zatrzymanie.

Druga możliwość ma jedną stronę "wygrywającą". Ale co to oznacza? Samo uzyskanie (jakoś posłusznego) AGI pierwsze nie wystarcza. Zwycięzca musi *także* zatrzymać innych od kontynuowania wyścigu – w przeciwnym razie oni też go uzyskają. To jest możliwe w zasadzie: ktokolwiek rozwinie AGI pierwszy *mógłby* uzyskać nie do zatrzymania władzę nad wszystkimi innymi aktorami. Ale co osiągnięcie takiej "decydującej przewagi strategicznej" faktycznie by wymagało? Być może byłyby to przełomowe zdolności wojskowe?[^19] Czy moce cyberataków?[^20] Być może AGI byłoby po prostu tak niesamowicie przekonujące, że przekonałoby inne strony po prostu się zatrzymać?[^21] Tak bogate, że kupiłoby inne firmy czy nawet kraje?[^22]

Jak *dokładnie* jedna strona buduje AI wystarczająco potężne, aby pozbawić władzy innych od budowania porównywalnie potężnego AI? Ale to łatwe pytanie.

Ponieważ teraz zastanów się, jak ta sytuacja wygląda dla innych mocarstw. Co myśli rząd chiński, gdy USA wydają się uzyskiwać taką zdolność? Lub vice versa? Co myśli rząd USA (lub chiński, lub rosyjski, lub indyjski), gdy OpenAI lub DeepMind lub Anthropic wydają się blisko przełomu? Co się dzieje, jeśli USA widzą nowy wysiłek indyjski lub ZEA z przełomowym sukcesem? Widzieliby zarówno egzystencjalne zagrożenie i – kluczowo – że jedynym sposobem, w jaki ten "wyścig" się kończy, jest ich własne pozbawienie władzy. Te bardzo potężne podmioty – w tym rządy w pełni wyposażonych narodów, które na pewno mają środki, aby to zrobić – byłyby bardzo motywowane, aby albo uzyskać, albo zniszczyć taką zdolność, czy to siłą, czy podstępem.[^23]

To mogłoby zacząć się na małą skalę, jako sabotaż przebiegów treningowych lub ataki na produkcję chipów, ale te ataki mogą naprawdę się zatrzymać dopiero, gdy wszystkie strony albo stracą zdolność do ścigania w AI, albo stracą zdolność do wykonywania ataków. Ponieważ uczestnicy postrzegają stawki jako egzystencjalne, każdy przypadek prawdopodobnie będzie reprezentować katastrofalną wojnę.

To prowadzi nas do czwartej możliwości: ściganie do superinteligencji, i w najszybszy, najmniej kontrolowany sposób możliwy. Gdy AI zwiększa się w potędze, jego deweloperzy po obu stronach będą znajdować to postępowo trudniejsze do kontrolowania, szczególnie dlatego, że ściganie dla zdolności jest przeciwne do rodzaju ostrożnej pracy, której kontrolowalność by wymagała. Więc ten scenariusz stawia nas bezpośrednio w przypadku, gdzie kontrola jest stracona (lub dana, jak zobaczymy dalej) do samych systemów AI. To jest, *AI wygrywa wyścig.* Ale z drugiej strony, w stopniu, w jakim kontrola *jest* utrzymana, nadal mamy wielu wzajemnie wrogich stron, każda odpowiedzialna za ekstremalne potężne zdolności. To wygląda znowu jak wojna.

Postawmy to wszystko inaczej.[^24] Obecny świat po prostu nie ma żadnych instytucji, które mogłyby być powierzone do rozwoju AI tej zdolności bez zapraszania natychmiastowego ataku.[^25] Wszystkie strony będą poprawnie rozumować, że albo to *nie* będzie pod kontrolą – i stąd jest zagrożeniem dla wszystkich stron, albo to *będzie* pod kontrolą, i stąd jest zagrożeniem dla każdego przeciwnika, który rozwija to mniej szybko. To są kraje uzbrojone nuklearnie, lub są firmami mieszczącymi się w nich.

Przy braku jakiegokolwiek prawdopodobnego sposobu dla ludzi na "wygranie" tego wyścigu, zostajemy z surowym wnioskiem: jedynym sposobem, w jaki ten wyścig się kończy, jest albo w katastrofalnym konflikcie, albo gdzie AI, a nie jakakolwiek ludzka grupa, jest zwycięzcą.

## Dajemy kontrolę AI (lub ją bierze)

Geopolityczna konkurencja "wielkich mocarstw" to tylko jedna z wielu konkurencji: jednostki konkurują ekonomicznie i społecznie; firmy konkurują na rynkach; partie polityczne konkurują o władzę; ruchy konkurują o wpływ. W każdej arenie, gdy AI zbliża się do i przekracza ludzkie zdolności, presja konkurencyjna zmusi uczestników do delegowania lub oddawania coraz więcej kontroli systemom AI – nie dlatego, że ci uczestnicy chcą, ale dlatego, że [nie mogą sobie pozwolić na to, żeby nie.](https://arxiv.org/abs/2303.16200)

Tak jak z innymi ryzykami AGI, widzimy to już z słabszymi systemami. Studenci czują presję do używania AI w swoich zadaniach, ponieważ wyraźnie wielu innych studentów to robi. Firmy [pędzą do przyjmowania rozwiązań AI z powodów konkurencyjnych.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) Artyści i programiści czują się zmuszeni używać AI, bo inaczej ich stawki będą podbijane przez innych, którzy to robią.

To czuje się jak przymusowa delegacja, ale nie utrata kontroli. Ale podkręćmy stawki i popchnijmy zegar do przodu. Zastanów się nad CEO, którego konkurenci używają AGI "pomocników" do podejmowania szybszych, lepszych decyzji, lub dowódcą wojskowym stojącym naprzeciw przeciwnika z dowodzeniem i kontrolą wspomaganymi przez AI. Wystarczająco zaawansowany system AI mógłby autonomicznie działać z wielokrotnie ludzką szybkością, wyrafinowaniem, złożonością i zdolnością przetwarzania danych, dążąc do złożonych celów w skomplikowany sposób. Nasz CEO lub dowódca, odpowiedzialny za taki system, może widzieć, że osiąga to, co chce; ale czy zrozumiałby nawet małą część *jak* to zostało osiągnięte? Nie, po prostu musiałby to zaakceptować. Co więcej, wiele z tego, co system może robić, to nie tylko brać rozkazy, ale radzić swojemu rzekomemu szefowi, co robić. Ta rada będzie dobra –– raz po raz.

W którym momencie więc rola człowieka zostanie ograniczona do klikania "tak, idź naprzód"?

To czuje się dobrze, mieć zdolne systemy AI, które mogą zwiększyć naszą produktywność, zadbać o irytujące rzeczy i nawet działać jako partner myślowy w załatwianiu rzeczy. Będzie się czuć dobrze mieć asystenta AI, który może zadbać o działania dla nas, jak dobry ludzki asystent osobisty. Będzie się czuć naturalnie, nawet korzystnie, gdy AI stanie się bardzo mądre, kompetentne i niezawodne, odsyłać coraz więcej decyzji do niego. Ale ta "korzystna" delegacja ma wyraźny punkt końcowy, jeśli kontynuujemy drogę: pewnego dnia odkryjemy, że naprawdę nie jesteśmy odpowiedzialni za wiele z niczego już, i że systemy AI faktycznie prowadzące show nie mogą bardziej zostać wyłączone niż firmy naftowe, media społecznościowe, internet lub kapitalizm.

I to jest znacznie bardziej pozytywna wersja, w której AI jest po prostu tak przydatne i skuteczne, że pozwalamy mu podejmować większość naszych kluczowych decyzji za nas. Rzeczywistość prawdopodobnie byłaby znacznie większym miks między tym a wersjami, gdzie niekontrolowane systemy AGI *biorą* różne formy władzy dla siebie, ponieważ, pamiętaj, władza jest przydatna dla niemal każdego celu, który się ma, a AGI byłoby, przez projekt, przynajmniej tak skuteczne w dążeniu do swoich celów jak ludzie.

Czy przyznajemy kontrolę, czy zostaje wyrwana od nas, jej utrata wydaje się ekstremalne prawdopodobna. Jak pierwotnie ujął to Alan Turing, "...wydaje się prawdopodobne, że gdy metoda myślenia maszyn by się zaczęła, nie zajęłoby długo przewyższenie naszych słabych mocy. Nie byłoby pytania o umieranie maszyn, i mogłyby rozmawiać ze sobą, aby ostrzyć swoje rozumy. W pewnym etapie więc musiałoby się oczekiwać, że maszyny przejmą kontrolę..."

Proszę zauważ, chociaż to oczywiste na tyle, że utrata kontroli przez ludzkość nad AI pociąga także utratę kontroli Stanów Zjednoczonych przez rząd Stanów Zjednoczonych; oznacza utratę kontroli nad Chinami przez Komunistyczną Partię Chin, i utratę kontroli nad Indiami, Francją, Brazylią, Rosją i każdym innym krajem przez ich własny rząd. Tak więc firmy AI, nawet jeśli nie jest to ich intencja, obecnie uczestniczą w potencjalnym obaleniu rządów światowych, w tym swojego własnego. To mogłoby się zdarzyć w ciągu kilku lat.

## AGI doprowadzi do superinteligencji

Można argumentować, że sztuczna inteligencja ogólna konkurencyjna z ludźmi lub nawet ekspercko-konkurencyjna, nawet jeśli autonomiczna, mogłaby być do opanowania. Może być niesamowicie destrukcyjna we wszystkich sposobach omówionych powyżej, ale jest dużo bardzo mądrych, sprawczych ludzi na świecie teraz, i są mniej więcej do opanowania.[^26]

Ale nie zostaniemy na mniej więcej ludzkim poziomie. Progresja poza to prawdopodobnie będzie napędzana przez te same siły, które już widzieliśmy: presja konkurencyjna między deweloperami AI szukającymi zysku i władzy, presja konkurencyjna między użytkownikami AI, którzy nie mogą sobie pozwolić na pozostawanie w tyle, i – najważniejsze – własna zdolność AGI do poprawiania siebie.

W procesie, który już widzieliśmy, jak się zaczyna z mniej potężnymi systemami, AGI samo byłoby w stanie konceptualizować i projektować ulepszone wersje siebie. To włącza hardware, software, sieci neuronowe, narzędzia, szkielety itp. To będzie, przez definicję, lepsze od nas w robieniu tego, więc nie wiemy dokładnie, jak będzie inteligencja-bootstrap. Ale nie będziemy musieć. O ile nadal mamy wpływ na to, co AGI robi, po prostu musielibyśmy poprosić to o to, albo pozwolić mu.

Nie ma ludzkiej bariery poziomu w poznaniu, która mogłaby nas chronić przed tym niekontrololowanym.[^27]

Progresja AGI do superinteligencji nie jest prawem natury; nadal byłoby możliwe ograniczenie niekontrolowanego, szczególnie jeśli AGI jest relatywnie scentralizowane i w stopniu, w jakim jest kontrolowane przez strony, które nie czują presji, żeby się ścigać ze sobą. Ale gdyby AGI było szeroko rozpowszechnione i wysoce autonomiczne, wydaje się niemal niemożliwe zapobiec mu decydowania, że powinno być bardziej, a potem jeszcze więcej, potężne.

## Co się dzieje, jeśli zbudujemy (lub AGI zbuduje) superinteligencję

Mówiąc wprost, nie mamy pojęcia, co by się stało, gdybyśmy zbudowali superinteligencję.[^28] Podjęłaby działania, których nie możemy śledzić lub postrzegać, z powodów, których nie możemy pojąć, do celów, których nie możemy sobie wyobrazić. To, co wiemy, to że nie będzie to zależeć od nas.[^29]

Niemożliwość kontrolowania superinteligencji może być zrozumiana przez coraz ostrzejsze analogie. Po pierwsze, wyobraź sobie, że jesteś CEO dużej firmy. Nie ma sposobu, żebyś mógł śledzić wszystko, co się dzieje, ale z odpowiednim ustawieniem personelu nadal możesz sensownie zrozumieć ogólny obraz i podejmować decyzje. Ale załóżmy jedną rzecz: wszyscy inni w firmie działają sto razy szybciej od ciebie. Czy nadal możesz nadążyć?

Z superinteligentnym AI ludzie "dowodziliby" czymś nie tylko szybszym, ale działającym na poziomach wyrafinowania i złożoności, których nie mogą zrozumieć, przetwarzając ogromnie więcej danych, niż mogą nawet sobie wyobrazić. Ta niewspółmierność może być postawiona na formalnym poziomie: [prawo Ashby'ego o wymaganej różnorodności](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (i zobacz związane ["twierdzenie o dobrym regulatorze"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)) stwierdza, mniej więcej, że każdy system kontrolny musi mieć tyle pokręteł i tarcz, ile system kontrolowany ma stopni swobody.

Osoba kontrolująca superinteligentny system AI byłaby jak paproć kontrolująca General Motors: nawet jeśli "rób, co chce paproć" byłoby zapisane w statutach korporacyjnych, systemy są tak różne w szybkości i zakresie działania, że "kontrola" po prostu nie ma zastosowania. (I jak długo do tego, aż ten uciążliwy statut zostanie przepisany?)[^30]

Jako że są zero przykładów roślin kontrolujących korporacje fortune 500, byłoby dokładnie zero przykładów ludzi kontrolujących superinteligencje. To zbliża się do matematycznego faktu.[^31] Gdyby superinteligencja została skonstruowana – niezależnie od tego, jak tam dotarliśmy – pytaniem nie byłoby, czy ludzie mogliby ją kontrolować, ale czy kontynuowalibyśmy istnienie, a jeśli tak, czy mielibyśmy dobre i znaczące istnienie jako jednostki lub jako gatunek. Nad tymi egzystencjalnymi pytaniami dla ludzkości mielibyśmy małe zakupy. Era ludzka byłaby skończona.

## Wniosek: nie wolno nam budować AGI

Istnieje scenariusz, w którym budowanie AGI może pójść dobrze dla ludzkości: jest budowane ostrożnie, pod kontrolą i dla korzyści ludzkości, rządzone przez wzajemne porozumienie wielu interesariuszy,[^32] i zapobiega się mu ewoluowaniu do niekontrolowalnej superinteligencji.

*Ten scenariusz nie jest dla nas otwarty w obecnych okolicznościach.* Jak omówiono w tej sekcji, z bardzo wysokim prawdopodobieństwem rozwój AGI prowadziłby do pewnej kombinacji:

- Masowe społeczne i cywilizacyjne zakłócenie lub zniszczenie;
- Konflikt lub wojna między wielkimi mocarstwami;
- Utrata kontroli przez ludzkość *nad* lub *na rzecz* potężnych systemów AI;
- Niekontrolowane do niekontrolowalnej superinteligencji i irrelewantność lub zaprzestanie gatunku ludzkiego.

Jak ujął to wczesny fikcyjny opis AGI: jedynym sposobem na wygranie jest nie granie.

[^1]: [Ustawa AI UE](https://artificialintelligenceact.eu/) jest znaczącym fragmentem legislacji, ale nie zapobiegłaby bezpośrednio temu, żeby niebezpieczny system AI został zbudowany lub wdrożony, czy nawet otwarcie wypuszczony, szczególnie w USA. Inny znaczący fragment polityki, rozporządzenie wykonawcze USA o AI, zostało odwołane.

[^2]: Ta [ankieta Gallup](https://news.gallup.com/poll/1597/confidence-institutions.aspx) pokazuje ponury spadek zaufania do instytucji publicznych od 2000 roku w USA. Liczby europejskie są zróżnicowane i mniej ekstremalne, ale także w trendzie spadkowym. Nieufność nie oznacza ściśle, że instytucje naprawdę *są* dysfunkcjonalne, ale jest wskazaniem jak również przyczyną.

[^3]: A główne zakłócenia, które teraz popieramy – takie jak rozszerzenie praw na nowe grupy – były specjalnie napędzane przez ludzi w kierunku poprawiania rzeczy.

[^4]: Pozwól mi być szczerym. Jeśli twoja praca może być wykonywana zza komputera, z relatywnie małą osobistą interakcją z ludźmi poza twoją organizacją i nie pociąga odpowiedzialności prawnej wobec zewnętrznych stron, byłoby z definicji możliwe (i prawdopodobnie oszczędne) całkowicie zamienić cię na system cyfrowy. Robotyka do zastąpienia większości pracy fizycznej przyjdzie później – ale nie tak wiele później, gdy AGI zacznie projektować roboty.

[^5]: Na przykład, co się dzieje z naszym systemem sądowym, jeśli pozwy są niemal bezpłatne do złożenia? Co się dzieje, gdy omijanie systemów bezpieczeństwa poprzez inżynierię społeczną staje się tanie, łatwe i bezryzykowne?

[^6]: [Ten artykuł](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/) twierdzi, że 10% całej zawartości internetowej jest już generowane przez AI i jest najlepszym hitem Google'a (dla mnie) na zapytanie wyszukiwania "oszacowania, jaki ułamek nowej zawartości internetowej jest generowany przez AI." Czy to prawda? Nie mam pojęcia! Nie cytuje żadnych odniesień i nie był napisany przez osobę. Jaki ułamek nowych obrazów indeksowanych przez Google, czy Tweets, czy komentarzy na Reddit, czy wideo Youtube są generowane przez ludzi? Nikt nie wie – nie myślę, że to liczba możliwa do poznania. I to mniej niż *dwa lata* po nadejściu generacyjnego AI.

[^7]: Warte dodania jest też, że istnieje ryzyko "moralne", że możemy stworzyć cyfrowe istoty, które mogą cierpieć. Jako że obecnie nie mamy niezawodnej teorii świadomości, która pozwoliłaby nam rozróżnić systemy fizyczne, które mogą i nie mogą cierpieć, nie możemy tego wykluczyć teoretycznie. Ponadto raporty systemów AI o ich czującej są prawdopodobnie niezawodne względem ich faktycznego doświadczenia (lub nie-doświadczenia) czującej.

[^8]: Rozwiązania techniczne w tej dziedzinie "wyrównania" AI prawdopodobnie nie będą na wysokości zadania również. W obecnych systemach działają na pewnym poziomie, ale są płytkie i generalnie mogą być omijane bez znaczącego wysiłku; i jak omówiono poniżej, nie mamy prawdziwego pojęcia, jak to robić dla znacznie bardziej zaawansowanych systemów.

[^9]: Takie systemy AI mogą przyjść z niektórymi wbudowanymi zabezpieczeniami. Ale dla każdego modelu z czymś jak obecna architektura, jeśli pełny dostęp do jego wag jest dostępny, środki bezpieczeństwa mogą być usunięte via dodatkowy trening lub inne techniki. Więc jest praktycznie gwarantowane, że dla każdego systemu z barierkami będzie też szeroko dostępny system bez nich. Rzeczywiście model Llama 3.1 405B Meta został otwarcie wypuszczony z zabezpieczeniami. Ale *nawet przed tym* model "bazowy", bez zabezpieczeń, został wycieknięty.

[^10]: Czy rynek mógłby zarządzać tymi ryzykami bez zaangażowania rządu? Krótko, nie. Są na pewno ryzyka, które firmy są silnie motywowane do łagodzenia. Ale wiele innych firm może i robi eksternalizuje na wszystkich innych, i wiele z powyższych jest w tej klasie: nie ma naturalnych motywacji rynkowych do zapobiegania masowej inwigilacji, rozpadu prawdy, koncentracji władzy, zakłócenia pracy, szkodzącej dyskursowi politycznemu itp. Rzeczywiście widzieliśmy wszystkie te od współczesnej technologii, szczególnie mediów społecznościowych, które poszły zasadniczo nieregulowane. AI po prostu bardzo by wzmocniło wiele z tych samych dynamik.

[^11]: OpenAI prawdopodobnie ma bardziej posłuszne modele do użytku wewnętrznego. Jest nieprawdopodobne, żeby OpenAI zbudowało jakiś rodzaj "tylnych drzwi", żeby ChatGPT mógł być lepiej kontrolowany przez samego OpenAI, ponieważ byłoby to okropną praktyką bezpieczeństwa i byłoby wysoce eksploatowalne biorąc pod uwagę nieprzejrzystość i nieprzewidywalność AI.

[^12]: Też o kluczowym znaczeniu: wyrównanie lub jakiekolwiek inne cechy bezpieczeństwa mają znaczenie tylko jeśli są faktycznie używane w systemie AI. Systemy, które są otwarcie wypuszczane (tzn. gdzie wagi modelu i architektura są publicznie dostępne) mogą być transformowane relatywnie łatwo w systemy *bez* tych środków bezpieczeństwa. Otwarcie-wypuszczanie mądrzejszych-niż-ludzkie systemy AGI byłoby zdumiewająco lekkomyślne, i trudno wyobrazić sobie, jak ludzka kontrola czy nawet istotność byłaby utrzymana w takim scenariuszu. Byłaby każda motywacja, na przykład, żeby wypuścić potężnych samo-reprodukujących i samo-podtrzymujących agentów AI z celem zarabiania pieniędzy i wysyłania ich do jakiegoś portfela kryptowalut. Czy wygrywanie wyborów. Czy obalanie rządu. Czy "dobre" AI może pomóc to zawierać? Być może – ale tylko przez delegowanie mu ogromnej władzy, prowadząc do utraty kontroli jak opisano poniżej.

[^13]: Dla ekspozycji na długość książki problemu zobacz np. *Superintelligencja*, *Problem Wyrównania*, i *Kompatybilne z Człowiekiem*. Dla ogromnej sterty pracy na różnych poziomach technicznych przez tych, którzy trudzili się przez lata myśląc o problemie, możesz odwiedzić [forum wyrównania AI](https://www.alignmentforum.org/). Oto [najnowsze ujęcie](https://alignment.anthropic.com/2025/recommended-directions/) od zespołu wyrównania Anthropic o tym, co uważają za nierozwiązane.

[^14]: To jest scenariusz ["awanturniczego AI"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/). W zasadzie ryzyko mogłoby być relatywnie mniejsze, jeśli system nadal mógłby być kontrolowany przez wyłączenie go; ale scenariusz mógłby również obejmować oszustwo AI, samo-wyciągnięcie i reprodukcję, agregację władzy i inne kroki, które sprawiłyby, że trudno lub niemożliwe byłoby to zrobić.

[^15]: Jest bardzo bogata literatura na ten temat, sięgająca do formujących pism przez [Steve Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), Nicka Bostroma i Eliezer Yudkowskiego. Dla ekspozycji na długość książki zobacz [Kompatybilne z Człowiekiem](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) autorstwa Stuarta Russella; [tutaj](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/) jest krótka i aktualnie primer.

[^16]: Rozpoznając to, zamiast zwalniać, żeby uzyskać lepsze zrozumienie, firmy AGI wymyśliły inny plan: będą mieć AI to robiące! Bardziej szczegółowo, będą mieć AI *N* pomocne im w figuring jak wyrównać AI *N+1*, całą drogę do superinteligencji. Chociaż leveraging AI do pomocy nam wyrównać AI brzmi obiecująco, jest mocny argument, że po prostu zakłada swój wniosek jako premise i jest ogólnie niesamowicie ryzykownym podejściem. Zobacz [tutaj](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us) dla jakiejś dyskusji. Ten "plan" nie jest jednym i przeszedł nic jak scrutiny odpowiednie do podstawowej strategii jak sprawić, żeby super-ludzkie AI poszło dobrze dla ludzkości.

[^17]: W końcu ludzie, wadliwi i upórni, jak jesteśmy, rozwinęli systemy etyczne, przez które traktujemy przynajmniej niektóre inne gatunki na Ziemi dobrze. (Po prostu nie myśl o tych fermach fabrycznych.)

[^18]: Jest, na szczęście, ucieczka tutaj: jeśli uczestnicy dojdą do zrozumienia, że są zaangażowani w wyścig samobójczy a nie wygrywny. To się stało pod koniec zimnej wojny, gdy USA i ZSRR doszły do zrozumienia, że z powodu zimy nuklearnej, nawet *nieodpowiedziany* atak nuklearny byłby katastrofalny dla atakującego. Z realizacją, że "wojna nuklearna nie może być wygrana i nie wolno nigdy walczyć" przyszły znaczące porozumienia o redukcji broni – zasadniczo koniec wyścigu zbrojeń.

[^19]: Wojna, wyraźnie czy implikacyjnie.

[^20]: Eskalacja, potem wojna.

[^21]: Magiczne myślenie.

[^22]: Mam też bilionów dolarów wartości most do sprzedania ci.

[^23]: Tacy agenci prawdopodobnie woleliby "uzyskanie", ze zniszczeniem jako fallback; ale zabezpieczanie modeli przeciwko zarówno zniszczeniu *i* kradzieży przez potężne narody jest trudne, żeby powiedzieć najmniej, szczególnie dla prywatnych podmiotów.

[^24]: Dla innej perspektywy na ryzyko bezpieczeństwa narodowego AGI, zobacz [ten raport RAND.](https://www.rand.org/pubs/perspectives/PEA3691-4.html)

[^25]: Być może moglibyśmy zbudować taką instytucję! Były propozycje dla "CERN dla AI" i inne podobne inicjatywy, gdzie rozwój AGI jest pod multilateral global control. Ale w tej chwili żadna taka instytucja nie istnieje lub nie jest na horyzoncie.

[^26]: I podczas gdy wyrównanie jest bardzo trudne, getting ludzi do zachowywania się jest jeszcze trudniejsze!

[^27]: Wyobraź sobie system, który może mówić 50 językami, mieć ekspertyzę we wszystkich przedmiotach akademickich, przeczytać pełną książkę w sekundach i mieć cały materiał natychmiast na myśli i produkować wyjścia z dziesięciokrotnie ludzką szybkością. Faktycznie, nie musisz tego wyobrażać sobie: po prostu załaduj obecny system AI. Te są super-ludzkie na wiele sposobów, i nie ma nic zatrzymującego je od bycia jeszcze bardziej super-ludzkimi w tych i wielu innych.

[^28]: To jest dlaczego to było nazwane technologiczną "osobliwością", pożyczając z fizyki ideę, że nie można robić przewidywań past osobliwość. Proponenci opierania się *w* taką osobliwość mogą także chcieć reflektować, że w fizyce te same rodzaje osobliwości rozrywają na kawałki i miażdżą te, które w nie wchodzą.

[^29]: Problem był comprehensively outlined w Bostroma [*Superinteligencji*](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834), i nic od tamtego czasu nie zmieniło znacząco podstawowej wiadomości. Dla bardziej niedawnego volume collecting formal i matematycznych wyników na niekontrolowalność zobacz Yampolskiy's [AI: Niewyjaśnione, Nieprzewidywalne, Niekontrolowalne](https://www.amazon.com/Unexplainable-Unpredictable-Uncontrollable-Artificial-Intelligence/dp/103257626X)

[^30]: To też czyni jasnym, dlaczego obecna strategia firm AI (iteracyjnie pozwalając AI "wyrównywać" następne najpotężniejsze AI) nie może działać. Załóż paproć, via przyjemność jego fronds, enlists pierwszego ucznia szkoły do zadbania o to. Pierwszy uczeń pisze niektóre szczegółowe instrukcje dla drugiego ucznia do następowania i notę przekonującą ich do robienia tego. Drugi uczeń robi to samo dla trzeciego ucznia i tak dalej całą drogę do college grad, manager, executive i wreszcie GM CEO. Czy GM potem "zrobi, co chce paproć"? W każdym kroku to może czuć się jak działające. Ale putting wszystko razem, będzie działać niemal dokładnie do stopnia, do którego CEO, Rada i shareholders GM przypadają care o dzieciach i paprocach i mieć mało do nothing do robienia ze wszystkimi tymi notami i zestawami instrukcji.

[^31]: Charakter nie jest tak różny od formalnych wyników jak twierdzenie niekompletności Gödela czy argument halting Turinga w tym, że notion kontroli fundamentalnie przeciwstawia się premise: jak możesz sensownie kontrolować coś, czego nie możesz zrozumieć czy przewidzieć; jeszcze jeśli mógłbyś zrozumieć i przewidzieć superinteligencję, byłbyś superinteligentny. Powód, dlaczego mówię "approaches", jest że formalne wyniki nie są tak thorough czy vetted jak w przypadku pure matematyki i ponieważ chciałbym hold out hope, że pewna bardzo ostrożnie skonstruowana ogólna inteligencja, używając całkowicie różnych metod niż obecnie stosowanych, mogłaby mieć niektóre matematycznie udowodnione właściwości bezpieczeństwa, per rodzaj "guaranteed safe" programu AI omówionego poniżej.

[^32]: W tej chwili większość stakeholders – to jest niemal cała ludzkość – jest sidelined w tej dyskusji. To jest głęboko złe i jeśli nie zaproszeni, wiele, wiele innych grup będzie dotkniętych przez rozwój AGI powinno demand być let in.