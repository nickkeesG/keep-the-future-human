# Rozdział 8 - Jak nie budować AGI

AGI nie jest nieuniknione – dziś stoimy na rozdrożu. Ten rozdział przedstawia propozycję, jak moglibyśmy zapobiec jego powstaniu.

Jeśli droga, którą obecnie podążamy, prowadzi do prawdopodobnego końca naszej cywilizacji, jak zmienić kierunek?

Załóżmy, że pragnienie zatrzymania rozwoju AGI i superinteligencji stałoby się powszechne i silne,[^1] ponieważ rozpowszechniłoby się zrozumienie, że AGI pochłaniałoby władzę zamiast jej udzielać, i stanowiłoby ogromne zagrożenie dla społeczeństwa i ludzkości. Jak zamknęlibyśmy Bramy?

Obecnie znamy tylko jeden sposób *tworzenia* potężnej i ogólnej sztucznej inteligencji – poprzez naprawdę masowe obliczenia głębokich sieci neuronowych. Ponieważ są to niesamowicie trudne i kosztowne przedsięwzięcia, w pewnym sensie ich *niepodejmowanie* jest łatwe.[^2] Ale widzieliśmy już siły napędzające rozwój w kierunku AGI oraz dynamikę teorii gier, która bardzo utrudnia jednostronne zatrzymanie się którejkolwiek ze stron. Potrzebna byłaby więc kombinacja interwencji z zewnątrz (tj. rządów) w celu powstrzymania korporacji oraz porozumień między rządami w celu powstrzymania siebie nawzajem.[^3] Jak mogłoby to wyglądać?

Przydatne jest najpierw rozróżnienie między rozwojem AI, który musi być *zapobiegany* lub *zakazany*, a tym, który musi być *zarządzany*. Pierwszy dotyczyłby głównie niekontrolowanego rozwoju w kierunku superinteligencji.[^4] Dla zakazanego rozwoju definicje powinny być możliwie precyzyjne, a weryfikacja i egzekwowanie – praktyczne. To, co musi być *zarządzane*, dotyczyłoby ogólnych, potężnych systemów AI – które już mamy i które będą mieć wiele szarych obszarów, niuansów i złożoności. W tym przypadku kluczowe są silne, skuteczne instytucje.

Możemy także użytecznie rozgraniczyć kwestie, które muszą być adresowane na poziomie międzynarodowym (włącznie z rywalami lub przeciwnikami geopolitycznymi)[^5] od tych, którymi mogą zarządzać poszczególne jurysdykcje, kraje lub grupy krajów. Zakazany rozwój w dużej mierze należy do kategorii "międzynarodowej", ponieważ lokalny zakaz rozwoju technologii można generalnie obejść, zmieniając lokalizację.[^6]

Wreszcie, możemy rozważyć narzędzia w zestawie narzędzi. Jest ich wiele, włączając narzędzia techniczne, prawo miękkie (standardy, normy itp.), prawo twarde (regulacje i wymogi), odpowiedzialność prawną, zachęty rynkowe i tak dalej. Skupmy szczególną uwagę na jednym, które jest specyficzne dla AI.

## Bezpieczeństwo i zarządzanie mocą obliczeniową

Kluczowym narzędziem w zarządzaniu wysokowydajną AI będzie sprzęt, którego wymaga. Oprogramowanie rozprzestrzenia się łatwo, ma niemal zerowy krańcowy koszt produkcji, przekracza granice trywialne i może być natychmiast modyfikowane; żadne z tych stwierdzeń nie jest prawdziwe dla sprzętu. Jednak jak omawialiśmy, ogromne ilości tej "mocy obliczeniowej" są niezbędne zarówno podczas treningu systemów AI, jak i podczas inferencji, aby osiągnąć najbardziej zdolne systemy. Moc obliczeniową można łatwo kwantyfikować, rozliczać i audytować, przy względnie niewielkiej dwuznaczności, gdy zostaną opracowane dobre reguły tego dokonywania. Co najważniejsze, duże ilości obliczeń są, podobnie jak wzbogacony uran, bardzo rzadkim, drogim i trudnym do wytworzenia zasobem. Chociaż chipy komputerowe są wszechobecne, sprzęt wymagany do AI jest drogi i niezwykle trudny w produkcji.[^7]

To, co czyni chipy wyspecjalizowane pod AI o wiele *bardziej* zarządzalnym rzadkim zasobem niż uran, to fakt, że mogą one zawierać sprzętowe mechanizmy bezpieczeństwa. Większość nowoczesnych telefonów komórkowych i niektóre laptopy mają wyspecjalizowane funkcje sprzętowe na chipie, które pozwalają im zapewnić, że instalują tylko zatwierdzone oprogramowanie systemowe i aktualizacje, że przechowują i chronią wrażliwe dane biometryczne na urządzeniu oraz że można je uczynić bezużytecznymi dla kogokolwiek oprócz ich właściciela w przypadku zgubienia lub kradzieży. W ciągu ostatnich kilku lat takie sprzętowe środki bezpieczeństwa stały się dobrze ugruntowane i szeroko przyjęte, i generalnie okazały się całkiem bezpieczne.

Kluczową nowością tych funkcji jest to, że łączą sprzęt i oprogramowanie za pomocą kryptografii.[^8] To znaczy, samo posiadanie konkretnego elementu sprzętu komputerowego nie oznacza, że użytkownik może robić z nim wszystko, co chce, stosując różne oprogramowanie. A to powiązanie zapewnia także potężne bezpieczeństwo, ponieważ wiele ataków wymagałoby naruszenia bezpieczeństwa *sprzętu*, a nie tylko *oprogramowania*.

Kilka niedawnych raportów (np. od [GovAI i współpracowników](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips) i [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) wskazało, że podobne funkcje sprzętowe wbudowane w najnowocześniejszy sprzęt obliczeniowy związany z AI mogłyby odgrywać niezwykle użyteczną rolę w bezpieczeństwie i zarządzaniu AI. Umożliwiają one szereg funkcji dostępnych "zarządcy",[^9] których istnienia można by nie podejrzewać lub nawet nie uważać za możliwe. Jako kluczowe przykłady:

- *Geolokalizacja*: Systemy można skonfigurować tak, aby chipy miały znaną lokalizację i mogły działać inaczej (lub być całkowicie wyłączone) w zależności od lokalizacji.[^10]
- *Połączenia z listy dozwolonych*: każdy chip może być skonfigurowany ze sprzętowo egzekwowaną listą dozwolonych konkretnych innych chipów, z którymi może się łączyć sieciowo, i być niezdolny do łączenia się z chipami spoza tej listy.[^11] To może ograniczyć wielkość komunikujących się klastrów chipów.[^12]
- *Odmierzona inferencja lub trening (i automatyczne wyłączanie)*: Zarządca może licencjonować tylko określoną ilość treningu lub inferencji (w czasie, lub FLOP, lub ewentualnie tokenach) do wykonania przez użytkownika, po czym wymagane jest nowe pozwolenie. Jeśli przyrosty są małe, wymagane jest stosunkowo ciągłe ponowne licencjonowanie modelu. Model można wtedy "wyłączyć" po prostu wstrzymując ten sygnał licencji.[^13]
- *Ograniczenie prędkości*: Model ma zapobiec działaniu z większą prędkością inferencji niż jakiś limit określony przez zarządcę lub w inny sposób. Można to zaimplementować przez ograniczony zestaw połączeń z listy dozwolonych lub przez bardziej wyrafinowane środki.
- *Poświadczony trening*: Procedura treningu może dostarczyć kryptograficznie bezpieczny dowód, że konkretny zestaw kodów, danych i ilość użycia mocy obliczeniowej zostały zastosowane w generowaniu modelu.

## Jak nie budować superinteligencji: globalne limity na moc obliczeniową treningu i inferencji

Mając te rozważania – szczególnie dotyczące obliczeń – na miejscu, możemy omówić, jak zamknąć Bramy przed sztuczną superinteligencją; następnie zwrócimy się do zapobiegania pełnemu AGI i zarządzaniu modelami AI, gdy zbliżają się i przekraczają ludzkie zdolności w różnych aspektach.

Pierwszym składnikiem jest, oczywiście, zrozumienie, że superinteligencja nie byłaby kontrolowalna i że jej konsekwencje są fundamentalnie nieprzewidywalne. Przynajmniej Chiny i USA muszą niezależnie zdecydować, z tego lub innych powodów, nie budować superinteligencji.[^14] Następnie potrzebne jest międzynarodowe porozumienie między nimi a innymi, z silnym mechanizmem weryfikacji i egzekwowania, aby zapewnić wszystkie strony, że ich rywale nie odstępują i nie decydują się rzucić kostką.

Aby być weryfikowalne i wykonalne, limity powinny być twardymi limitami i możliwie jednoznaczne. Wydaje się to praktycznie niemożliwym problemem: ograniczenie możliwości złożonego oprogramowania z nieprzewidywalnymi właściwościami na całym świecie. Na szczęście sytuacja jest znacznie lepsza od tej, ponieważ to właśnie, co uczyniło zaawansowaną AI możliwą – ogromna ilość mocy obliczeniowej – jest znacznie, znacznie łatwiejsze do kontrolowania. Chociaż mogłoby to wciąż pozwolić na niektóre potężne i niebezpieczne systemy, *niekontrolowany rozwój superinteligencji* prawdopodobnie można zapobiec przez twardy limit na ilość obliczeń, które wchodzą do sieci neuronowej, wraz z ograniczeniem szybkości ilości inferencji, którą system AI (połączonych sieci neuronowych i innego oprogramowania) może wykonywać. Konkretna wersja tego jest proponowana poniżej.

Może się wydawać, że umieszczenie twardych globalnych limitów na obliczenia AI wymagałoby ogromnych poziomów koordynacji międzynarodowej i inwazyjnej, naruszającej prywatność inwigilacji. Na szczęście tak nie byłoby. Niezwykle [ciasny i wąski łańcuch dostaw](https://arxiv.org/abs/2402.08797) sprawia, że gdy limit zostanie ustalony prawnie (czy to przez prawo, czy zarządzenie wykonawcze), weryfikacja zgodności z tym limitem wymagałaby zaangażowania i współpracy tylko garstki dużych firm.[^15]

Plan jak ten ma szereg wysoce pożądanych cech. Jest minimalnie inwazyjny w tym sensie, że tylko kilku głównym firmom nakłada się wymogi, i tylko dość znaczące klastry obliczeń byłyby zarządzane. Odpowiednie chipy już zawierają możliwości sprzętowe potrzebne do pierwszej wersji.[^16] Zarówno implementacja, jak i egzekwowanie opierają się na standardowych ograniczeniach prawnych. Ale są one wspierane przez warunki użytkowania sprzętu i przez kontrole sprzętowe, znacznie upraszczając egzekwowanie i zapobiegając oszustwom przez firmy, grupy prywatne, a nawet kraje. Istnieje obfity precedens dla firm sprzętowych nakładających zdalne ograniczenia na użytkowanie ich sprzętu i blokowania/odblokowywania konkretnych możliwości zewnętrznie,[^17] włączając nawet w wysokowydajne CPU w centrach danych.[^18] Nawet dla dość małego ułamka sprzętu i organizacji dotkniętych tym, nadzór mógłby być ograniczony do telemetrii, bez bezpośredniego dostępu do danych czy samych modeli; a oprogramowanie do tego mogłoby być otwarte do inspekcji, aby wykazać, że nie są rejestrowane żadne dodatkowe dane. Schemat jest międzynarodowy i kooperatywny, oraz dość elastyczny i rozszerzalny. Ponieważ limit dotyczy głównie sprzętu, a nie oprogramowania, jest względnie agnostyczny co do tego, jak rozwój i wdrażanie oprogramowania AI przebiega, i jest kompatybilny z różnorodnością paradygmatów, włączając bardziej "zdecentralizowaną" lub "publiczną" AI mającą na celu walkę z koncentracją władzy napędzaną przez AI.

Zamknięcie Bram oparte na obliczeniach ma także wady. Po pierwsze, jest daleko od pełnego rozwiązania problemu zarządzania AI w ogóle. Po drugie, gdy sprzęt komputerowy staje się szybszy, system "złapałby" coraz więcej sprzętu w coraz mniejszych klastrach (a nawet pojedynczych GPU).[^19] Możliwe jest także, że z powodu ulepszeń algorytmicznych potrzebny byłby jeszcze niższy limit obliczeniowy,[^20] lub że ilość obliczeń staje się w dużej mierze nieistotna, a zamknięcie Bramy wymagałoby zamiast tego bardziej szczegółowego reżimu zarządzania opartego na ryzyku lub możliwościach dla AI. Po trzecie, bez względu na gwarancje i małą liczbę dotkniętych podmiotów, taki system z pewnością wywoła sprzeciw dotyczący prywatności i inwigilacji, między innymi.[^21]

Oczywiście rozwój i implementacja schematu zarządzania ograniczającego obliczenia w krótkim okresie czasu będzie dość wyzwaniem. Ale absolutnie jest to wykonalne.

## A-G-I: Potrójny przecięcie jako podstawa ryzyka i polityki

Zwróćmy się teraz do AGI. Twarde linie i definicje są tutaj trudniejsze, ponieważ z pewnością mamy inteligencję, która jest sztuczna i ogólna, i według żadnej istniejącej definicji nie wszyscy zgodzą się, czy i kiedy istnieje. Ponadto limit obliczeniowy lub inferencyjny jest nieco tępym narzędziem (moc obliczeniowa jest zastępczym wskaźnikiem dla zdolności, która następnie jest zastępczym wskaźnikiem dla ryzyka), które – chyba że jest dość niskie – prawdopodobnie nie zapobiegnie AGI wystarczająco potężnemu, aby spowodować społeczne lub cywilizacyjne zakłócenia lub ostre ryzyko.

Argumentowałem, że najbardziej ostre ryzyko wyłania się z potrójnego przecięcia bardzo wysokich możliwości, wysokiej autonomii i wielkiej ogólności. To są systemy, które – jeśli w ogóle zostaną opracowane – muszą być zarządzane z ogromną ostrożnością. Tworząc rygorystyczne standardy (poprzez odpowiedzialność prawną i regulacje) dla systemów łączących wszystkie te trzy właściwości, możemy skierować rozwój AI w kierunku bezpieczniejszych alternatyw.

Jak w przypadku innych branż i produktów, które mogą potencjalnie zaszkodzić konsumentom lub społeczeństwu, systemy AI wymagają starannej regulacji przez skuteczne i uprawomocnione agencje rządowe. Ta regulacja powinna uznawać nieodłączne ryzyko AGI i zapobiegać rozwojowi nieakceptowalnie ryzykownych systemów AI o wysokiej mocy.[^22]

Jednak regulacja na dużą skalę, szczególnie z prawdziwymi zębami, które na pewno spotkają się ze sprzeciwem przemysłu,[^23] wymaga czasu[^24] oraz politycznego przekonania, że jest niezbędna.[^25] Biorąc pod uwagę tempo postępu, może to zająć więcej czasu, niż mamy do dyspozycji.

W znacznie szybszej skali czasowej i podczas opracowywania środków regulacyjnych, możemy dać firmom niezbędne zachęty do (a) powstrzymania się od bardzo wysokoryzykownych działań i (b) opracowania kompleksowych systemów oceny i łagodzenia ryzyka, poprzez wyjaśnienie i zwiększenie poziomów odpowiedzialności prawnej dla najbardziej niebezpiecznych systemów. Idea polegałaby na nałożeniu najwyższych poziomów odpowiedzialności – bezwzględnej, a w niektórych przypadkach osobistej karnej – na systemy w potrójnym przecięciu wysokiej autonomii-ogólności-inteligencji, ale zapewnieniu "bezpiecznych przystani" do bardziej typowej odpowiedzialności opartej na winie dla systemów, w których jedna z tych właściwości jest nieobecna lub zagwarantowana jako zarządzalna. To znaczy, na przykład, "słaby" system, który jest ogólny i autonomiczny (jak zdolny i godny zaufania, ale ograniczony asystent osobisty) podlegałby niższym poziomom odpowiedzialności. Podobnie wąski i autonomiczny system jak samojezdny samochód wciąż podlegałby znacznej regulacji, której już podlega, ale nie wzmocnionej odpowiedzialności. Podobnie dla wysoce zdolnego i ogólnego systemu, który jest "pasywny" i w dużej mierze niezdolny do niezależnego działania. Systemy pozbawione *dwóch* z trzech właściwości są jeszcze bardziej zarządzalne, a bezpieczne przystanie byłyby jeszcze łatwiejsze do uzyskania. To podejście odzwierciedla to, jak radzimi sobie z innymi potencjalnie niebezpiecznymi technologiami:[^26] wyższa odpowiedzialność dla bardziej niebezpiecznych konfiguracji tworzy naturalne zachęty dla bezpieczniejszych alternatyw.

Domyślnym wynikiem takich wysokich poziomów odpowiedzialności, które działają w celu *internalizacji* ryzyka AGI do firm zamiast przeniesienia go na społeczeństwo, jest prawdopodobnie (i miejmy nadzieję!) po prostu nierozwijanie pełnego AGI przez firmy, dopóki i chyba że mogą naprawdę uczynić go godnym zaufania, bezpiecznym i kontrolowalnym, biorąc pod uwagę, że to *ich własne kierownictwo* jest stroną narażoną na ryzyko. (W przypadku, gdy to nie wystarczy, ustawodawstwo wyjaśniające odpowiedzialność powinno także wyraźnie pozwalać na środki zaradcze w postaci zakazu, tj. sędzia nakazujący zatrzymanie działań, które wyraźnie znajdują się w strefie niebezpieczeństwa i prawdopodobnie stanowią ryzyko publiczne.) Gdy regulacje wchodzą w życie, przestrzeganie regulacji może stać się bezpieczną przystanią, a bezpieczne przystanie z niskiej autonomii, wąskości lub słabości systemów AI mogą przekształcić się w stosunkowo lżejsze reżimy regulacyjne.

## Kluczowe postanowienia zamknięcia Bram

Mając powyższą dyskusję na uwadze, ta sekcja przedstawia propozycje kluczowych postanowień, które implementowałyby i utrzymywałyby zakaz pełnego AGI i superinteligencji oraz zarządzanie konkurencyjną z ludźmi lub przewyższającą ekspertów sztuczną inteligencją ogólnego przeznaczenia blisko progu pełnego AGI.[^27] Ma cztery kluczowe elementy: 1) rozliczanie i nadzór mocy obliczeniowej, 2) limity mocy obliczeniowej w treningu i działaniu AI, 3) ramy odpowiedzialności prawnej oraz 4) wielopoziomowe standardy bezpieczeństwa i ochrony definiujące twarde wymogi regulacyjne. Są one zwięźle opisane dalej, z dodatkowymi szczegółami lub przykładami implementacji podanymi w trzech towarzyszących tabelach. Co ważne, zauważ, że są to daleko nie wszystko, co będzie niezbędne do zarządzania zaawansowanymi systemami AI; choć będą miały dodatkowe korzyści bezpieczeństwa i ochrony, są one nastawione na zamknięcie Bramy przed niekontrolowanym rozwojem inteligencji i przekierowanie rozwoju AI w lepszym kierunku.

### 1\. Rozliczanie mocy obliczeniowej i przejrzystość

- Organizacja standardów (np. NIST w USA, po której następują ISO/IEEE międzynarodowo) powinna skodyfikować szczegółowy standard techniczny dla całkowitej mocy obliczeniowej używanej w treningu i działaniu modeli AI, w FLOP, oraz prędkości w FLOP/s, z jaką działają. Szczegóły tego, jak to mogłoby wyglądać, podane są w Załączniku A.[^28]
- Wymóg – czy to przez nowe ustawodawstwo, czy na podstawie istniejących uprawnień[^29] – powinien być nałożony przez jurysdykcje, w których ma miejsce trening AI na dużą skalę, aby obliczać i raportować do organu regulacyjnego lub innej agencji całkowite FLOP używane w treningu i działaniu wszystkich modeli powyżej progu 10<sup>25</sup> FLOP lub 10<sup>18</sup> FLOP/s.[^30]
- Te wymogi powinny być wprowadzane etapowo, początkowo wymagając dobrze udokumentowanych szacunków w dobrej wierze na podstawie kwartalnej, z późniejszymi fazami wymagającymi progresywnie wyższych standardów, aż do kryptograficznie poświadczonych całkowitych FLOP i FLOP/s dołączonych do każdego *wyniku* modelu.
- Te raporty powinny być uzupełnione przez dobrze udokumentowane szacunki krańcowych kosztów energetycznych i finansowych używanych w generowaniu każdego wyniku AI.

Uzasadnienie: Te dobrze obliczone i transparentnie raportowane liczby zapewniłyby podstawę dla limitów treningu i działania, a także bezpieczną przystań od wyższych środków odpowiedzialności (zobacz Załączniki C i D).

### 2\. Limity mocy obliczeniowej treningu i działania

- Jurysdykcje hostujące systemy AI powinny nałożyć twardy limit na całkowitą moc obliczeniową wchodzącą do każdego wyniku modelu AI, zaczynając od 10<sup>27</sup> FLOP[^31] i regulowalną według potrzeb.
- Jurysdykcje hostujące systemy AI powinny nałożyć twardy limit na szybkość mocy obliczeniowej wyników modelu AI, zaczynając od 10<sup>20</sup> FLOP/s i regulowalną według potrzeb.

Uzasadnienie: Całkowite obliczenia, choć bardzo niedoskonałe, są zastępczym wskaźnikiem zdolności AI (i ryzyka), który jest konkretnie mierzalny i weryfikowalny, więc zapewnia twardą barierę dla ograniczenia możliwości. Konkretna propozycja implementacji jest podana w Załączniku B.

### 3\. Wzmocniona odpowiedzialność za niebezpieczne systemy

- Tworzenie i działanie[^32] zaawansowanego systemu AI, który jest wysoce ogólny, zdolny i autonomiczny, powinno być prawnie wyjaśnione poprzez ustawodawstwo jako podlegające bezwzględnej, solidarnej, a nie jednostronnej odpowiedzialności opartej na winie.[^33]
- Proces prawny powinien być dostępny do tworzenia pozytywnych argumentów bezpieczeństwa, które udzieliłyby bezpiecznej przystani od bezwzględnej odpowiedzialności dla systemów, które są małe (pod względem mocy obliczeniowej), słabe, wąskie, pasywne lub które mają wystarczające gwarancje bezpieczeństwa, ochrony i kontrolowalności.
- Wyraźna ścieżka i zestaw warunków dla środków zaradczych w postaci zakazu zatrzymania działań treningu i inferencji AI, które stanowią publiczne niebezpieczeństwo, powinny być nakreślone.

Uzasadnienie: Systemy AI nie mogą być pociągnięte do odpowiedzialności, więc musimy pociągnąć do odpowiedzialności ludzkie jednostki i organizacje za szkody, które powodują (odpowiedzialność).[^34] Niekontrolowane AGI stanowi zagrożenie dla społeczeństwa i cywilizacji i przy braku argumentu bezpieczeństwa powinno być uważane za nienormalnie niebezpieczne. Nałożenie ciężaru odpowiedzialności na deweloperów, aby pokazać, że potężne modele są wystarczająco bezpieczne, aby nie być uważanymi za "nienormalnie niebezpieczne", zachęca do bezpiecznego rozwoju, wraz z przejrzystością i prowadzeniem dokumentacji w celu uzyskania tych bezpiecznych przystani. Regulacja może wtedy zapobiec szkodzie tam, gdzie odstraszanie od odpowiedzialności jest niewystarczające. Wreszcie, deweloperzy AI są już odpowiedzialni za szkody, które powodują, więc prawne wyjaśnienie odpowiedzialności dla najbardziej ryzykownych systemów można zrobić natychmiast, bez opracowywania bardzo szczegółowych standardów; te mogą się następnie rozwijać z czasem. Szczegóły podane są w Załączniku C.

### 4\. Regulacja bezpieczeństwa dla AI

System regulacyjny, który adresuje ostre ryzyko AI na dużą skalę, będzie wymagał minimum:

- Identyfikacji lub stworzenia odpowiedniego zestawu organów regulacyjnych, prawdopodobnie nowej agencji;
- Kompleksowych ram oceny ryzyka;[^35]
- Ram dla pozytywnych argumentów bezpieczeństwa, opartych częściowo na ramach oceny ryzyka, które mają być tworzone przez deweloperów i audytowane przez *niezależne* grupy i agencje;
- Wielopoziomowego systemu licencyjnego, z poziomami śledzącymi poziomy zdolności.[^36] Licencje byłyby udzielane na podstawie argumentów bezpieczeństwa i audytów, dla rozwoju i wdrażania systemów. Wymogi wahałyby się od powiadomienia na dolnym końcu do kwantytatywnych gwarancji bezpieczeństwa, ochrony i kontrolowalności przed rozwojem na górnym końcu. Zapobiegałyby one wydaniu systemów, dopóki nie zostanie wykazane, że są bezpieczne, i zakazywałyby rozwoju wewnętrznie niebezpiecznych systemów. Załącznik D przedstawia propozycję tego, co takie standardy bezpieczeństwa i ochrony mogłyby zawierać.
- Porozumień w celu przeniesienia takich środków na poziom międzynarodowy, włączając międzynarodowe organy harmonizujące normy i standardy, a potencjalnie międzynarodowe agencje przeglądające argumenty bezpieczeństwa.

Uzasadnienie: Ostatecznie odpowiedzialność prawna nie jest właściwym mechanizmem zapobiegania ryzyku na dużą skalę dla społeczeństwa ze strony nowej technologii. Kompleksowa regulacja, z uprawomocnionymi organami regulacyjnymi, będzie potrzebna dla AI tak jak dla każdej innej głównej branży stanowiącej ryzyko dla społeczeństwa.[^37]

Regulacja w kierunku zapobiegania innym wszechobecnym, ale mniej ostrym ryzykom prawdopodobnie będzie różnić się formą od jurysdykcji do jurysdykcji. Kluczową rzeczą jest unikanie rozwoju systemów AI, które są tak ryzykowne, że te ryzyko są niezarządzalne.

## Co wtedy?

W ciągu następnej dekady, gdy AI stanie się bardziej wszechobecna, a podstawowa technologia się rozwinie, prawdopodobnie wydarzą się dwie kluczowe rzeczy. Po pierwsze, regulacja istniejących potężnych systemów AI stanie się trudniejsza, ale jeszcze bardziej niezbędna. Prawdopodobne jest, że przynajmniej niektóre środki adresujące ryzyko bezpieczeństwa na dużą skalę będą wymagały porozumienia na poziomie międzynarodowym, z poszczególnymi jurysdykcjami egzekwującymi zasady oparte na międzynarodowych porozumieniach.

Po drugie, limity mocy obliczeniowej treningu i działania staną się trudniejsze do utrzymania, gdy sprzęt stanie się tańszy i bardziej efektywny kosztowo; mogą także stać się mniej istotne (lub wymagać jeszcze większego zaostrzenia) wraz z postępami w algorytmach i architekturach.

To, że kontrolowanie AI stanie się trudniejsze, nie znaczy, że powinniśmy się poddać! Implementacja planu nakreślonego w tym eseju dałaby nam zarówno cenny czas, jak i kluczową kontrolę nad procesem, które postawiłyby nas w daleko, daleko lepszej pozycji do uniknięcia egzystencjalnego ryzyka AI dla naszego społeczeństwa, cywilizacji i gatunku.

W jeszcze dłuższej perspektywie będą wybory do podjęcia co do tego, na co pozwolimy. Możemy zdecydować się wciąż na stworzenie jakiejś formy naprawdę kontrolowalnego AGI, w stopniu, w jakim okaże się to możliwe. Lub możemy zdecydować, że prowadzenie świata lepiej pozostawić maszynom, jeśli możemy przekonać siebie, że zrobią to lepiej i będą nas dobrze traktować. Ale powinny to być decyzje podjęte z głębokim naukowym zrozumieniem AI w ręku i po znaczącej globalnej, włączającej dyskusji, a nie w wyścigu między potentatami technologicznymi z większością ludzkości całkowicie niezaangażowaną i nieświadomą.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) Podsumowanie zarządzania A-G-I i superinteligencji poprzez odpowiedzialność prawną i regulacje. Odpowiedzialność jest najwyższa, a regulacja najsilniejsza, w potrójnym przecięciu Autonomii, Ogólności i Inteligencji. Bezpieczne przystanie od bezwzględnej odpowiedzialności i silnej regulacji można uzyskać poprzez pozytywne argumenty bezpieczeństwa wykazujące, że system jest słaby i/lub wąski i/lub pasywny. Limity na całkowitą Moc Obliczeniową Treningu i szybkość Mocy Obliczeniowej Inferencji, weryfikowane i egzekwowane prawnie oraz przy użyciu sprzętowych i kryptograficznych środków bezpieczeństwa, stanowią ostatnią linię obrony bezpieczeństwa, unikając pełnego AGI i skutecznie zakazując superinteligencji.


[^1]: Najprawdopodobniej rozprzestrzenianie się tej realizacji będzie wymagało intensywnych wysiłków grup edukacyjnych i adwokackich prezentujących ten argument lub dość znaczącej katastrofy spowodowanej przez AI. Możemy mieć nadzieję, że będzie to to pierwsze.

[^2]: Paradoksalnie, przyzwyczailiśmy się, że Natura ogranicza naszą technologię, czyniąc ją bardzo trudną do rozwoju, szczególnie naukowo. Ale tak już nie jest w przypadku AI: kluczowe problemy naukowe okazują się łatwiejsze niż oczekiwano. Nie możemy liczyć na to, że Natura uratuje nas przed sobą – będziemy musieli to zrobić sami.

[^3]: Gdzie dokładnie zatrzymujemy się w rozwoju nowych systemów? Tutaj powinniśmy przyjąć zasadę ostrożności. Gdy system zostanie wdrożony, a szczególnie gdy ten poziom zdolności systemu rozprzestrzeni się, jest niezwykle trudno to cofnąć. A jeśli system zostanie *opracowany* (szczególnie przy wielkich kosztach i wysiłku), będzie ogromna presja, aby go użyć lub wdrożyć, i pokusa, aby został przeciekł lub skradziony. Opracowywanie systemów, a *następnie* decydowanie, czy są głęboko niebezpieczne, to niebezpieczna droga.

[^4]: Mądre byłoby także zakazanie rozwoju AI, który jest wewnętrznie niebezpieczny, jak systemy samoreplikujące się i ewoluujące, te zaprojektowane do ucieczki z ograniczeń, te mogące autonomicznie się samodoskonalić, celowo oszukańcze i złośliwe AI itp.

[^5]: Zauważ, że nie oznacza to koniecznie *egzekwowanych* na poziomie międzynarodowym przez jakiś rodzaj globalnego organu: zamiast tego suwerenne narody mogłyby egzekwować uzgodnione zasady, jak w wielu traktatach.

[^6]: Jak zobaczymy poniżej, natura obliczeń AI pozwoliłaby na coś w rodzaju hybrydy; ale współpraca międzynarodowa będzie wciąż potrzebna.

[^7]: Na przykład maszyny wymagane do trawieniu chipów związanych z AI są wytwarzane tylko przez jedną firmę, ASML (pomimo wielu innych prób), zdecydowana większość odpowiednich chipów jest produkowana przez jedną firmę, TSMC (pomimo prób konkurencji przez innych), a projektowanie i konstrukcja sprzętu z tych chipów jest wykonywana przez zaledwie kilku, włączając NVIDIA, AMD i Google.

[^8]: Co najważniejsze, każdy chip posiada unikalny i niedostępny kryptograficzny klucz prywatny, którego może używać do "podpisywania" rzeczy.

[^9]: Domyślnie byłaby to firma sprzedająca chipy, ale inne modele są możliwe i potencjalnie użyteczne.

[^10]: Zarządca może ustalić lokalizację chipa, mierząc czas wymiany podpisanych wiadomości z nim: skończona prędkość światła wymaga, aby chip znajdował się w promieniu *r* od "stacji", jeśli może zwrócić podpisaną wiadomość w czasie mniejszym niż *r* / *c*, gdzie *c* to prędkość światła. Używając wielu stacji i pewnego zrozumienia charakterystyk sieci, lokalizacja chipa może być określona. Piękno tej metody polega na tym, że większość jej bezpieczeństwa jest zapewniana przez prawa fizyki. Inne metody mogłyby używać GPS, śledzenia inercyjnego i podobnych technologii.

[^11]: Alternatywnie pary chipów mogłyby być dozwolone do komunikacji ze sobą tylko za wyraźnym pozwoleniem zarządcy.

[^12]: Jest to kluczowe, ponieważ przynajmniej obecnie bardzo wysokie pasmo połączenia między chipami jest potrzebne do trenowania dużych modeli AI na nich.

[^13]: Może to być także skonfigurowane, aby wymagać podpisanych wiadomości od *N* z *M* różnych zarządców, pozwalając wielu stronom na wspólne zarządzanie.

[^14]: To daleko nie ma precedensu – na przykład wojska nie opracowały armii sklonowanych lub genetycznie zmodyfikowanych supersołdatów, choć prawdopodobnie jest to technologicznie możliwe. Ale *zdecydowały się* tego nie robić, zamiast być powstrzymywane przez innych. Historia nie jest świetna, jeśli chodzi o powstrzymywanie głównych mocarstw światowych przed rozwój technologii, którą silnie chcą rozwijać.

[^15]: Z kilkoma zauważalnymi wyjątkami (w szczególności NVIDIA) sprzęt wyspecjalizowany pod AI stanowi względnie małą część ogólnego modelu biznesowego i przychodów tych firm. Ponadto luka między sprzętem używanym w zaawansowanej AI a sprzętem "konsumenckim" jest znacząca, więc większość konsumentów sprzętu komputerowego byłaby w dużej mierze nietknięta.

[^16]: Dla bardziej szczegółowej analizy, zobacz niedawne raporty od [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) i [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). Skupiają się one na wykonalności technicznej, szczególnie w kontekście amerykańskich kontroli eksportowych dążących do ograniczenia zdolności innych krajów w obliczeniach wysokiej klasy; ale ma to oczywiste pokrywanie się z globalnym ograniczeniem wyobrażonym tutaj.

[^17]: Na przykład urządzenia Apple są zdalnie i bezpiecznie blokowane, gdy są zgłaszane jako zgubione lub skradzione, i mogą być ponownie aktywowane zdalnie. Opiera się to na tych samych funkcjach bezpieczeństwa sprzętowego omówionych tutaj.

[^18]: Zobacz np. oferowaną przez IBM [pojemność na żądanie](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand), Intel [Intel na żądanie](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html) i Apple [prywatne obliczenia w chmurze](https://security.apple.com/blog/private-cloud-compute/).

[^19]: [To badanie](https://epochai.org/trends#hardware-trends-section) pokazuje, że historycznie ta sama wydajność była osiągana przy użyciu około 30% mniejszej liczby dolarów rocznie. Jeśli ten trend będzie kontynuowany, może być znaczące pokrywanie się między AI a "konsumenckimi" chipami, i ogólnie ilość potrzebnego sprzętu dla systemów AI wysokiej mocy mogłaby stać się niekomfortowo mała.

[^20]: Według [tego samego badania](https://epochai.org/trends#hardware-trends-section), dana wydajność w rozpoznawaniu obrazów wymagała 2,5 razy mniej obliczeń rocznie. Gdyby miało to także dotyczyć najbardziej zdolnych systemów AI, limit obliczeniowy nie byłby użyteczny przez bardzo długo.

[^21]: W szczególności na poziomie kraju wygląda to bardzo podobnie do nacjonalizacji obliczeń, w tym sensie, że rząd miałby dużą kontrolę nad tym, jak wykorzystywana jest moc obliczeniowa. Jednak dla tych, którzy martwią się o zaangażowanie rządu, wydaje się to daleko bezpieczniejsze i preferowane od najbardziej potężnego oprogramowania AI *samego w sobie* będącego znacjonalizowanym poprzez jakąś fuzję między głównymi firmami AI a rządami narodowymi, jak niektórzy zaczynają się opowiadać.

[^22]: Główny krok regulacyjny w Europie został podjęty z uchwaleniem [Aktu AI UE](https://artificialintelligenceact.eu/) w 2024 roku. Klasyfikuje AI według ryzyka: zakazując nieakceptowalnych systemów, regulując wysokoryzykowne i nakładając reguły przejrzystości, lub żadne środki, na systemy niskiego ryzyka. Znacznie zmniejszy niektóre ryzyko AI i zwiększy przejrzystość AI nawet dla firm amerykańskich, ale ma dwie kluczowe wady. Po pierwsze, ograniczony zasięg: choć dotyczy każdej firmy dostarczającej AI w UE, egzekwowanie wobec firm amerykańskich jest słabe, a wojskowa AI jest zwolniona. Po drugie, choć obejmuje GPAI, nie uznaje AGI lub superinteligencji za nieakceptowalne ryzyko lub nie zapobiega ich rozwojowi – tylko ich wdrażaniu w UE. W rezultacie robi niewiele, aby ograniczyć ryzyko AGI lub superinteligencji.

[^23]: Firmy często reprezentują, że są za rozsądną regulacją. Ale jakoś niemal zawsze wydają się sprzeciwiać każdej *konkretnej* regulacji; świadczy o tym walka o dość łagodną SB1047, której [większość firm AI publiczne lub prywatne sprzeciwiała się](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/).

[^24]: Było około 3½ roku od czasu zaproponowania aktu AI UE do jego wejścia w życie.

[^25]: Czasami wyrażane jest, że "za wcześnie" zaczynać regulować AI. Biorąc pod uwagę ostatnią notę, wydaje się to mało prawdopodobne. Inną wyrażaną troską jest, że regulacja "zaszkodziłaby innowacji". Ale dobra regulacja po prostu zmienia kierunek, a nie ilość innowacji.

[^26]: Ciekawym precedensem jest transport materiałów niebezpiecznych, które mogą uciec i spowodować szkody. Tutaj [regulacja](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) i [orzecznictwo](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) ustaliły bezwzględną odpowiedzialność dla bardzo niebezpiecznych materiałów jak materiały wybuchowe, benzyna, trucizny, środki zakaźne i odpady radioaktywne. Inne przykłady obejmują [ostrzeżenia na farmaceutykach](https://www.medicalnewstoday.com/articles/boxed-warnings), [klasy urządzeń medycznych](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification) itp.

[^27]: Inna kompleksowa propozycja o podobnych celach przedstawiona w ["Wąska Ścieżka"](https://www.narrowpath.co/) opowiada się za bardziej scentralizowanym, opartym na zakazie podejściem, które kieruje cały rozwój pionierskiej AI przez jedną międzynarodową jednostkę, nadzorowaną przez silne instytucje międzynarodowe, z jasnymi zakazami kategorycznymi zamiast stopniowanych ograniczeń. Poparłbym także ten plan; jednak będzie wymagał jeszcze więcej woli politycznej i koordynacji niż ten proponowany tutaj.

[^28]: Niektóre wytyczne dla takiego standardu zostały [opublikowane](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/) przez Forum Modeli Pionierskich. W stosunku do propozycji tutaj, te błądzą w stronę mniejszej precyzji i mniejszej mocy obliczeniowej wliczonej do sumy.

[^29]: Zarządzenie wykonawcze AI z 2023 roku w USA (teraz uchylone) wymagało podobnego, ale mniej szczegółowego raportowania. Powinno być to wzmocnione przez zastępujące zarządzenie.

[^30]: Bardzo w przybliżeniu, dla obecnie powszechnych chipów H100 odpowiada to klastrom około 1000 wykonujących inferencję; to około 100 (około 5 mln USD wartości) najnowszych najwyższej klasy chipów NVIDIA B200 wykonujących inferencję. W obu przypadkach liczba treningu odpowiada temu klastrowi obliczającemu przez kilka miesięcy.

[^31]: Ta ilość jest większa niż jakikolwiek obecnie trenowany system AI; większa lub mniejsza liczba mogłaby być uzasadniona, gdy lepiej zrozumiemy, jak zdolność AI skaluje się z mocą obliczeniową.

[^32]: Dotyczy to tych tworzących i dostarczających/hostujących modele, a nie użytkowników końcowych.

[^33]: W przybliżeniu, odpowiedzialność "bezwzględna" oznacza, że deweloperzy są pociągani do odpowiedzialności za szkody wyrządzone przez produkt *domyślnie* i jest standardem używanym dla produktów "nienormalnie niebezpiecznych", i (nieco zabawnie, ale odpowiednio) dzikich zwierząt. Odpowiedzialność "solidarna" oznacza, że odpowiedzialność jest przypisana wszystkim stronom odpowiedzialnym za produkt, a te strony muszą wyjaśnić między sobą, kto ponosi jaką odpowiedzialność. Jest to ważne dla systemów jak AI z długim i złożonym łańcuchem wartości.

[^34]: Standardowa odpowiedzialność oparta na winie jednostronnej nie wystarczy: winę będzie trudno prześledzić i przypisać, ponieważ systemy AI są złożone, ich działanie nie jest rozumiane, a wiele stron może być zaangażowanych w tworzenie niebezpiecznego systemu lub wyniku. Ponadto procesy sądowe będą trwać lata, a prawdopodobnie skończą się tylko grzywnami, które są nieistotne dla tych firm, więc ważna jest także osobista odpowiedzialność dla kadry kierowniczej.

[^35]: Nie powinno być zwolnienia z kryteriów bezpieczeństwa dla modeli o otwartych wagach. Ponadto w ocenie ryzyka powinno się założyć, że zabezpieczenia, które można usunąć, zostaną usunięte z szeroko dostępnych modeli, i że nawet zamknięte modele będą się rozprzestrzeniać, chyba że jest bardzo wysokie zapewnienie, że pozostaną bezpieczne.

[^36]: Schemat proponowany tutaj ma kontrolę regulacyjną uruchamianą na ogólnej zdolności; jednak sensowne jest, aby niektóre szczególnie ryzykowne przypadki użycia uruchamiały większą kontrolę – na przykład ekspertowy system AI wirusologii, nawet jeśli wąski i pasywny, prawdopodobnie powinien iść w wyższy poziom. Poprzednie zarządzenie wykonawcze USA miało część tej struktury dla zdolności biologicznych.

[^37]: Dwa jasne przykłady to lotnictwo i leki, regulowane przez FAA i FDA oraz podobne agencje w innych krajach. Te agencje są niedoskonałe, ale były absolutnie vitalne dla funkcjonowania i sukcesu tych branż.