# Rozdział 9 - Kształtowanie przyszłości — co powinniśmy robić zamiast tego

AI może przynieść niewiarygodne dobro światu. Aby uzyskać wszystkie korzyści bez ryzyka, musimy zapewnić, że AI pozostanie narzędziem człowieka.

Jeśli z powodzeniem wybierzemy, by nie zastępować ludzkości maszynami – przynajmniej przez jakiś czas! – co możemy zrobić zamiast tego? Czy rezygnujemy z ogromnego potencjału AI jako technologii? Na pewnym poziomie odpowiedź brzmi po prostu *nie:* zamknijmy Bramy przed niekontrolowalną AGI i superinteligencją, ale *budujmy* wiele innych form AI, a także struktury zarządzania i instytucje, których potrzebujemy, aby nimi kierować.

Ale wciąż jest wiele do powiedzenia; realizacja tego byłaby centralnym zajęciem ludzkości. Ta sekcja eksploruje kilka kluczowych tematów:

- Jak możemy scharakteryzować AI „Narzędziowe" i formy, jakie może przyjmować.
- Że możemy uzyskać (prawie) wszystko, czego chce ludzkość, bez AGI, przy pomocy AI Narzędziowego.
- Że systemy AI Narzędziowego są (prawdopodobnie, w zasadzie) możliwe do zarządzania.
- Że odwrócenie się od AGI nie oznacza kompromisu w kwestii bezpieczeństwa narodowego – wręcz przeciwnie.
- Że koncentracja władzy to realna troska. Czy możemy ją złagodzić bez podważania bezpieczeństwa i ochrony?
- Że będziemy chcieli – i potrzebowali – nowych struktur zarządzania i społecznych, a AI może faktycznie pomóc.

## AI wewnątrz Bram: AI Narzędziowe

Diagram potrójnego przecięcia daje dobry sposób na wyznaczenie tego, co możemy nazwać „AI Narzędziowym": AI, które jest kontrolowalnym narzędziem do użytku przez człowieka, a nie niekontrolowalnym rywalem czy zamiennikiem. Najmniej problematyczne systemy AI to te, które są autonomiczne, ale nie ogólne ani super zdolne (jak bot do licytacji aukcyjnych), lub ogólne, ale nie autonomiczne czy zdolne (jak mały model językowy), lub zdolne, ale wąskie i bardzo kontrolowalne (jak AlphaGo).[^1] Te z dwoma przecinającymi się cechami mają szersze zastosowanie, ale wyższe ryzyko i będą wymagały znacznych wysiłków w zarządzaniu. (To, że system AI jest bardziej narzędziem, nie oznacza, że jest z natury bezpieczny, tylko że nie jest z natury *niebezpieczny* – rozważ piłę łańcuchową kontra tygrysa domowego.) Brama musi pozostać zamknięta przed (pełną) AGI i superinteligencją w potrójnym przecięciu, a ogromną ostrożność trzeba zachować z systemami AI zbliżającymi się do tego progu.

Ale to pozostawia mnóstwo potężnego AI! Możemy uzyskać ogromną użyteczność z inteligentnych i ogólnych pasywnych „wyroczni" i systemów wąskich, systemów ogólnych na poziomie ludzkim, ale nie nadludzkim, i tak dalej. Wiele firm technologicznych i deweloperów aktywnie buduje tego rodzaju narzędzia i powinno kontynuować; jak większość ludzi, zakładają w sposób dorozumiany, że Bramy do AGI i superinteligencji zostaną zamknięte.[^2]

Ponadto systemy AI można skutecznie łączyć w systemy złożone, które zachowują ludzki nadzór przy jednoczesnym wzmacnianiu zdolności. Zamiast polegać na niezbadanych czarnych skrzynkach, możemy budować systemy, w których wiele komponentów – w tym zarówno AI, jak i tradycyjne oprogramowanie – współpracuje w sposób, który ludzie mogą monitorować i rozumieć.[^3] Podczas gdy niektóre komponenty mogą być czarnymi skrzynkami, żaden nie byłby blisko AGI – tylko system złożony jako całość byłby zarówno bardzo ogólny, jak i bardzo zdolny, i to w sposób ściśle kontrolowalny.[^4]

### Znacząca i gwarantowana kontrola człowieka

Co oznacza „ściśle kontrolowalny"? Kluczową ideą ram „Narzędzia" jest umożliwienie systemów – nawet jeśli całkiem ogólnych i potężnych – które są gwarantowane jako pozostające pod znaczącą kontrolą człowieka. Co to oznacza? Obejmuje to dwa aspekty. Pierwszy to kwestia projektowa: ludzie powinni być głęboko i centralnie zaangażowani w to, co system robi, *nie* delegując kluczowych ważnych decyzji na AI. To charakteryzuje większość obecnych systemów AI. Po drugie, w stopniu, w jakim systemy AI są autonomiczne, muszą mieć gwarancje ograniczające ich zakres działania. Gwarancja powinna być *liczbą* charakteryzującą prawdopodobieństwo wystąpienia czegoś i powodem, by wierzyć w tę liczbę. To właśnie żądamy w innych dziedzinach krytycznych pod względem bezpieczeństwa, gdzie liczby jak „średni czas między awariami" i oczekiwane liczby wypadków są obliczane, uzasadniane i publikowane w przypadkach bezpieczeństwa.[^5] Idealna liczba awarii to oczywiście zero. A dobra wiadomość jest taka, że możemy zbliżyć się całkiem blisko, choć używając całkiem różnych architektur AI, wykorzystując idee *formalnie zweryfikowanych* właściwości programów (w tym AI). Idea, eksplorowana obszernie przez Omohundro, Tegmarka, Bengio, Dalrymple'a i innych (zobacz [tutaj](https://arxiv.org/abs/2309.01933) i [tutaj](https://arxiv.org/abs/2405.06624)), polega na skonstruowaniu programu z określonymi właściwościami (na przykład: że człowiek może go wyłączyć) i formalnym *udowodnieniu*, że te właściwości zachodzą. Można to robić teraz dla całkiem krótkich programów i prostych właściwości, ale (nadchodząca) moc oprogramowania do dowodów wspieranego przez AI mogłaby pozwolić na to dla znacznie bardziej złożonych programów (np. opakowań) a nawet samego AI. To bardzo ambitny program, ale gdy presja na Bramy rośnie, będziemy potrzebowali jakichś potężnych materiałów je wzmacniających. Dowód matematyczny może być jednym z niewielu wystarczająco silnych.

### Gdzie przemysł AI

Przy przekierowaniu postępu AI, AI Narzędziowe wciąż byłoby ogromnym przemysłem. Pod względem sprzętu, nawet z ograniczeniami mocy obliczeniowej zapobiegającymi superinteligencji, trening i inferencja w mniejszych modelach nadal będą wymagały ogromnych ilości wyspecjalizowanych komponentów. Po stronie oprogramowania, rozładowanie eksplozji w rozmiarze modeli AI i obliczeniach powinno po prostu prowadzić firmy do przekierowania zasobów na uczynienie mniejszych systemów lepszymi, bardziej zróżnicowanymi i bardziej wyspecjalizowanymi, zamiast po prostu czynienia ich większymi.[^6] Byłoby mnóstwo miejsca – prawdopodobnie więcej – dla wszystkich tych zarabiających pieniądze startupów z Doliny Krzemowej.[^7]

## AI Narzędziowe może przynieść (prawie) wszystko, czego chce ludzkość, bez AGI

Inteligencja, czy biologiczna, czy maszynowa, może być szeroko rozważana jako zdolność planowania i wykonywania działań przynoszących przyszłości bardziej zgodne z zestawem celów. Jako taka inteligencja jest enormalnie korzystna, gdy używana w dążeniu do mądrze wybranych celów. Sztuczna inteligencja przyciąga ogromne inwestycje czasu i wysiłku głównie z powodu obiecanych korzyści. Więc powinniśmy zapytać: w jakim stopniu nadal zbieralibyśmy korzyści z AI, gdybyśmy powstrzymali jej ucieczkę do superinteligencji? Odpowiedź: możemy stracić zaskakująco mało.

Rozważmy najpierw, że obecne systemy AI są już bardzo potężne i naprawdę tylko zarysowaliśmy powierzchnię tego, co można z nimi zrobić.[^8] Są dość zdolne do „prowadzenia przedstawienia" w kategoriach „zrozumienia" pytania lub zadania im przedstawionego i tego, co byłoby potrzebne, aby odpowiedzieć na to pytanie lub wykonać to zadanie.

Następnie, wiele z ekscytacji wokół nowoczesnych systemów AI wynika z ich ogólności; ale niektóre z najbardziej zdolnych systemów AI – takie jak te, które generują lub rozpoznają mowę czy obrazy, robią przewidywania i modelowanie naukowe, grają w gry itp. – są znacznie węższe i dobrze „wewnątrz Bram" pod względem obliczeń.[^9] Te systemy są nadludzkie w konkretnych zadaniach, które wykonują. Mogą mieć słabości przypadków brzegowych [^10] (lub [wykorzystywalne](https://arxiv.org/abs/2211.00241)) ze względu na swoją wąskość; jednak *całkowicie* wąskie czy *w pełni* ogólne nie są jedynymi dostępnymi opcjami: jest wiele architektur pomiędzy.[^11]

Te narzędzia AI mogą znacznie przyspieszyć rozwój innych pozytywnych technologii, bez AGI. Aby lepiej robić fizykę jądrową, nie potrzebujemy, by AI było fizykiem jądrowym – mamy takich! Jeśli chcemy przyspieszyć medycynę, dajmy biologom, badaczom medycznym i chemikom potężne narzędzia. Chcą ich i użyją ich z ogromną korzyścią. Nie potrzebujemy farmy serwerowej pełnej miliona cyfrowych geniuszy; mamy miliony ludzi, których geniusz AI może pomóc wydobyć. Tak, zajmie więcej czasu, by uzyskać nieśmiertelność i lekarstwo na wszystkie choroby. To prawdziwy koszt. Ale nawet najbardziej obiecujące innowacje zdrowotne byłyby małoużyteczne, gdyby napędzana przez AI niestabilność prowadziła do globalnego konfliktu czy załamania społecznego. Jesteśmy to winni sobie samym, by dać wspieranym przez AI ludziom szansę na ten problem najpierw.

A załóżmy, że jest faktycznie jakaś ogromna korzyść z AGI, której nie można uzyskać przez ludzkość używającą narzędzi wewnątrz-Bramowych. Czy tracimy te korzyści przez *nigdy* nie budowanie AGI i superinteligencji? Ważąc ryzyko i nagrody tutaj, jest ogromna asymetryczna korzyść w czekaniu kontra śpieszeniu się: możemy czekać, aż można to zrobić w gwarantowany bezpieczny i korzystny sposób, i prawie wszyscy nadal będą mogli zbierać nagrody; jeśli się śpieszymy, może to być – słowami CEO OpenAI Sama Altmana – [zgaszenie świateł dla *wszystkich* z nas.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

Ale jeśli narzędzia nie-AGI są potencjalnie tak potężne, czy możemy nimi zarządzać? Odpowiedź brzmi wyraźnie... może.

## Systemy AI Narzędziowego są (prawdopodobnie, w zasadzie) możliwe do zarządzania

Ale nie będzie łatwo. Obecne najnowocześniejsze systemy AI mogą znacznie wzmocnić ludzi i instytucje w osiąganiu ich celów. To generalnie dobra rzecz! Jednak są naturalne dynamiki posiadania takich systemów do dyspozycji – nagle i bez dużo czasu dla społeczeństwa na adaptację – które oferują poważne ryzyko wymagające zarządzania. Warto omówić kilka głównych klas takiego ryzyka i jak można je zmniejszyć, zakładając zamknięcie Bramy.

Jedna klasa ryzyka to wysokiej mocy AI Narzędziowe umożliwiające dostęp do wiedzy czy zdolności, która wcześniej była związana z osobą lub organizacją, czyniąc kombinację wysokiej zdolności plus wysokiej lojalności dostępną bardzo szerokiej gamie aktorów. Dziś, z wystarczającą ilością pieniędzy osoba o złych intencjach mogłaby wynająć zespół chemików do projektowania i produkcji nowej broni chemicznej – ale nie jest tak bardzo łatwo mieć te pieniądze czy znaleźć/zebrać zespół i przekonać ich do robienia czegoś wyraźnie nielegalnego, nieetycznego i niebezpiecznego. Aby zapobiec systemom AI graniu takiej roli, ulepszenia obecnych metod mogą w pełni wystarczyć,[^12] o ile wszystkie te systemy i dostęp do nich są odpowiedzialnie zarządzane. Z drugiej strony, jeśli potężne systemy są wypuszczane do ogólnego użytku i modyfikacji, wszelkie wbudowane środki bezpieczeństwa są prawdopodobnie usuwalne. Więc aby uniknąć ryzyka w tej klasie, silne ograniczenia co do tego, co może być publicznie wypuszczane – analogiczne do ograniczeń na szczegóły technologii jądrowych, wybuchowych i innych niebezpiecznych – będą wymagane.[^13]

Druga klasa ryzyka wynika ze skalowania maszyn, które działają jak lub udają ludzi. Na poziomie szkody dla indywidualnych osób te ryzyka obejmują znacznie skuteczniejsze oszustwa, spam i phishing oraz proliferację deepfake'ów bez zgody.[^14] Na poziomie zbiorowym obejmują zakłócenie podstawowych procesów społecznych, jak publiczna dyskusja i debata, nasze społeczne systemy zbierania, przetwarzania i rozpowszechniania informacji i wiedzy oraz nasze systemy wyboru politycznego. Łagodzenie tego ryzyka prawdopodobnie będzie obejmowało (a) prawa ograniczające udawanie ludzi przez systemy AI i obciążające odpowiedzialnością deweloperów AI tworzących systemy generujące takie udawanie, (b) systemy znakowania wodnego i pochodzenia identyfikujące i klasyfikujące (odpowiedzialnie) generowaną przez AI zawartość, oraz (c) nowe socjo-techniczne systemy epistemiczne mogące stworzyć zaufany łańcuch od danych (np. kamery i nagrania) przez fakty, zrozumienie i dobre modele świata.[^15] Wszystko to jest możliwe i AI może pomóc z niektórymi jego częściami.

Trzecie ogólne ryzyko to to, że w stopniu, w jakim niektóre zadania są automatyzowane, ludzie obecnie wykonujący te zadania mogą mieć mniejszą wartość finansową jako siła robocza. Historycznie automatyzacja zadań sprawiała, że rzeczy umożliwione przez te zadania stawały się tańsze i bardziej obfite, jednocześnie sortując ludzi wcześniej wykonujących te zadania na tych nadal zaangażowanych w zautomatyzowaną wersję (generalnie przy wyższych kwalifikacjach/płacy) i tych, których praca jest warta mniej lub niewiele. W sumie trudno przewidzieć, w których sektorach więcej kontra mniej ludzkiej pracy będzie wymagane w powstałym większym, ale bardziej wydajnym sektorze. Równolegle dynamika automatyzacji prowadzi do zwiększenia nierówności i ogólnej produktywności, zmniejszenia kosztu pewnych dóbr i usług (przez wzrosty wydajności) oraz zwiększenia kosztu innych (przez [chorobę kosztów](https://en.wikipedia.org/wiki/Baumol_effect)). Dla tych po niekorzystnej stronie wzrostu nierówności głęboko niejasne jest, czy spadek kosztów w tych pewnych dobrach i usługach przewyższa wzrost w innych i prowadzi do ogólnie większego dobrostanu. Więc jak to pójdzie dla AI? Z powodu względnej łatwości, z jaką ludzka praca intelektualna może być zastąpiona przez ogólne AI, możemy oczekiwać szybkiej wersji tego z konkurencyjnym dla człowieka AI ogólnego przeznaczenia.[^16] Jeśli zamkniemy Bramę do AGI, znacznie mniej miejsc pracy będzie hurtowo zastąpionych przez agentów AI; ale ogromne przemieszczenie siły roboczej nadal jest prawdopodobne w okresie lat.[^17] Aby uniknąć rozpowszechnionego cierpienia ekonomicznego, prawdopodobnie konieczne będzie wdrożenie zarówno jakiejś formy uniwersalnych aktywów podstawowych czy dochodu, jak również zaprojektowanie kulturowej zmiany w kierunku wartościowania i nagradzania pracy ludzkiej, która jest trudniejsza do automatyzacji (zamiast patrzenia, jak ceny pracy spadają przez wzrost dostępnej siły roboczej wypchnięcej z innych części gospodarki). Inne konstrukty, takie jak „[godność danych](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society)" (w której ludzie produkujący dane treningowe są auto-nagradzani tantiemami za wartość stworzoną przez te dane w AI), mogą pomóc. Automatyzacja przez AI ma też drugi potencjalny negatywny efekt, którym jest *niewłaściwa* automatyzacja. Wraz z aplikacjami, gdzie AI po prostu wykonuje gorszą pracę, obejmowałoby to te, gdzie systemy AI prawdopodobnie naruszą zasady moralne, etyczne czy prawne – na przykład w decyzjach o życiu i śmierci oraz w sprawach sądowych. Te muszą być traktowane przez stosowanie i rozszerzanie naszych obecnych ram prawnych.

Wreszcie, znaczące zagrożenie AI wewnątrz-bramowego to jego użycie w spersonalizowanej perswazji, przechwytywaniu uwagi i manipulacji. Widzieliśmy w mediach społecznościowych i innych platformach internetowych wzrost głęboko zakorzenionej ekonomii uwagi (gdzie usługi internetowe zaciekle walczą o uwagę użytkowników) i systemów [„kapitalizmu inwigilacji"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism) (w których informacje o użytkownikach i profilowanie są dodane do utowarowienia uwagi). Niemal pewne jest, że więcej AI zostanie postawione w służbie obu. AI jest już intensywnie używane w algorytmach uzależniających od kanałów, ale to ewoluuje w uzależniającą zawartość generowaną przez AI, dostosowaną, by być kompulsywnie konsumowaną przez pojedynczą osobę. A wejścia, odpowiedzi i dane tej osoby będą karmione do maszyny uwaga/reklama, by kontynuować błędne koło. Ponadto, gdy pomocnicy AI dostarczani przez firmy technologiczne staną się interfejsem dla więcej życia online, prawdopodobnie zastąpią wyszukiwarki i kanały jako mechanizm, przez który perswazja i monetyzacja klientów występuje. Niepowodzenie naszego społeczeństwa w kontrolowaniu tej dynamiki dotąd nie wróży dobrze. Część tej dynamiki może być zmniejszona przez regulacje dotyczące prywatności, praw do danych i manipulacji. Dotarcie bardziej do korzenia problemu może wymagać różnych perspektyw, takich jak lojalni asystenci AI (omówieni poniżej).

Wniosek z tej dyskusji brzmi: nadzieja: systemy narzędziowe wewnątrz-Bram – przynajmniej dopóki pozostają porównywalne w mocy i zdolności z dzisiejszymi najnowocześniejszymi systemami – są prawdopodobnie możliwe do zarządzania, jeśli jest wola i koordynacja, by to robić. Przyzwoite ludzkie instytucje, wspierane przez narzędzia AI,[^18] mogą to zrobić. Moglibyśmy też w tym nie powieść. Ale trudno zobaczyć, jak pozwolenie na potężniejsze systemy pomogłoby – poza postawieniem ich za ster i marzeniem o najlepszym.

## Bezpieczeństwo narodowe

Wyścigi o supremację AI – napędzane motywacjami bezpieczeństwa narodowego czy innymi – pchają nas ku niekontrolowanym potężnym systemom AI, które będą skłonne wchłaniać, a nie obdarzać władza. Wyścig AGI między USA a Chinami to wyścig o ustalenie, który naród dostanie superinteligencję pierwszy.

Co więc powinni robić ci odpowiedzialni za bezpieczeństwo narodowe? Rządy mają silne doświadczenie w budowaniu kontrolowalnych i bezpiecznych systemów i powinny podwoić wysiłki w robieniu tego w AI, wspierając rodzaj projektów infrastrukturalnych, które odnoszą najlepszy sukces, gdy robione na dużą skalę i z rządowym poparciem.

Zamiast lekkomyślnego „projektu Manhattan" w kierunku AGI,[^19] rząd USA mógłby uruchomić projekt Apollo dla kontrolowalnych, bezpiecznych, godnych zaufania systemów. Mogłoby to obejmować na przykład:

- Główny program (a) opracowania mechanizmów bezpieczeństwa sprzętowego na chipie i (b) infrastruktury do zarządzania stroną mocy obliczeniowej potężnego AI. Mogłyby one opierać się na amerykańskim [akcie CHIPS](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) i [reżimie kontroli eksportu](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion).
- Inicjatywę na dużą skalę do opracowania technik formalnej weryfikacji, tak by konkretne cechy systemów AI (jak wyłącznik) mogły być *udowodnione* jako obecne lub nieobecne. Może to wykorzystać samo AI do opracowania dowodów właściwości.
- Wysiłek na skalę narodową do stworzenia oprogramowania, które jest weryfikowalnie bezpieczne, napędzane przez narzędzia AI mogące przekodowywać istniejące oprogramowanie w weryfikowalnie bezpieczne ramy.
- Narodowy projekt inwestycyjny w rozwój naukowy używający AI,[^20] działający jako partnerstwo między DOE, NSF i NIH.

Ogólnie jest ogromna powierzchnia ataku na nasze społeczeństwo, która czyni nas podatnymi na ryzyko z AI i jego nadużycia. Ochrona przed niektórymi z tych ryzyk będzie wymagała inwestycji i standaryzacji na poziomie rządowym. Te dałyby znacznie więcej bezpieczeństwa niż dolewanie benzyny do ognia wyścigów w kierunku AGI. A jeśli AI ma być wbudowane w uzbrojenie i systemy dowodzenia i kontroli, kluczowe jest, by AI było godne zaufania i bezpieczne, czym obecne AI po prostu nie jest.

## Koncentracja władzy i jej łagodzenie

Ten esej skupił się na idei ludzkiej kontroli AI i jej potencjalnej porażce. Ale inną ważną soczewką, przez którą można postrzegać sytuację AI, jest *koncentracja władzy.* Rozwój bardzo potężnego AI grozi skoncentrowaniem władzy albo w bardzo nielicznych i bardzo dużych korporacyjnych rękach, które go opracowały i będą kontrolować, albo w rządach używających AI jako nowego środka do utrzymania własnej władzy i kontroli, albo w samych systemach AI. Lub jakiejś niegodziwej mieszance powyższego. W każdym z tych przypadków większość ludzkości traci władzę, kontrolę i sprawczość. Jak moglibyśmy z tym walczyć?

Pierwszy i najważniejszy krok to oczywiście zamknięcie Bramy przed mądrzejszą niż człowiek AGI i superinteligencją. Te mogą wprost zastąpić ludzi i grupy ludzi. Jeśli są pod kontrolą korporacyjną lub rządową, skoncentrują władzę w tych korporacjach czy rządach; jeśli są „wolne", skoncentrują władzę w sobie. Więc załóżmy, że Bramy są zamknięte. Co wtedy?

Jednym proponowanym rozwiązaniem koncentracji władzy jest AI „open-source", gdzie wagi modelu są swobodnie lub szeroko dostępne. Ale jak wspomniano wcześniej, gdy model jest otwarty, większość środków bezpieczeństwa czy barier może być (i generalnie jest) usunięta. Więc jest ostra napięcie między z jednej strony decentralizacją, a z drugiej strony bezpieczeństwem, ochroną i ludzką kontrolą systemów AI. Są też powody do sceptycyzmu, że otwarte modele same z siebie znacząco zwalczą koncentrację władzy w AI bardziej, niż zrobiły to w systemach operacyjnych (nadal zdominowanych przez Microsoft, Apple i Google pomimo otwartych alternatyw).[^21]

Jednak mogą być sposoby na rozwiązanie tego kółka – centralizację i łagodzenie ryzyka przy jednoczesnej decentralizacji zdolności i nagrody ekonomicznej. To wymaga przemyślenia zarówno tego, jak AI jest opracowywane, jak jego korzyści są dystrybuowane.

Nowe modele publicznego rozwoju i własności AI pomogłyby. Mogłoby to przyjąć kilka form: rządowo-opracowane AI (podlegające nadzorowi demokratycznemu),[^22] organizacje rozwoju AI non-profit (jak Mozilla dla przeglądarek) czy struktury umożliwiające bardzo szeroką własność i zarządzanie. Kluczowe jest, by te instytucje były wprost upoważnione do służenia interesowi publicznemu przy działaniu pod silnymi ograniczeniami bezpieczeństwa.[^23] Dobrze sformułowane reżimy regulacyjne i standardów/certyfikacji też będą żywotne, tak by produkty AI oferowane przez żywy rynek pozostawały naprawdę użyteczne, a nie eksploatacyjne wobec użytkowników.

Pod względem koncentracji władzy ekonomicznej możemy użyć śledzenia pochodzenia i „godności danych", by zapewnić, że korzyści ekonomiczne płyną szerzej. W szczególności większość mocy AI teraz (i w przyszłości, jeśli utrzymamy Bramy zamknięte) wywodzi się z danych generowanych przez ludzi, czy to bezpośrednie dane treningowe, czy ludzkie opinie. Gdyby firmy AI były wymagane do sprawiedliwego wynagradzania dostarczycieli danych,[^24] mogłoby to przynajmniej pomóc dystrybuować nagrody ekonomiczne szerzej. Poza tym inny model mógłby być publiczna własność znacznych frakcji dużych firm AI. Na przykład rządy zdolne do opodatkowania firm AI mogłyby zainwestować frakcję wpływów w suwerenny fundusz majątkowy, który trzyma akcje w firmach i płaci dywidendy ludności.[^25]

Kluczowe w tych mechanizmach jest użycie mocy samego AI do polepszenia dystrybucji władzy, zamiast po prostu walki z koncentracją władzy napędzaną przez AI przy użyciu środków nie-AI. Jedno potężne podejście byłoby przez dobrze zaprojektowanych asystentów AI, którzy działają z prawdziwym obowiązkiem powierniczym wobec użytkowników – stawiając interesy użytkowników pierwszo, szczególnie ponad korporacyjnymi dostawcami.[^26] Ci asystenci muszą być naprawdę godni zaufania, kompetentni technicznie, ale odpowiednio ograniczeni bazując na przypadku użycia i poziomie ryzyka, i szeroko dostępni dla wszystkich przez kanały publiczne, non-profit lub certyfikowane komercyjne. Tak jak nigdy nie zaakceptowalibyśmy ludzkiego asystenta, który potajemnie pracuje przeciw naszym interesom dla innej strony, nie powinniśmy akceptować asystentów AI, którzy inwigilują, manipulują czy wyciągają wartość od użytkowników dla korporacyjnej korzyści.

Taka transformacja fundamentalnie zmieniłaby obecną dynamikę, gdzie jednostki są pozostawione do negocjowania w pojedynkę z ogromnymi (wspieranymi przez AI) korporacyjnymi i biurokratycznymi maszynami, które priorytetyzują wyciąganie wartości nad ludzkim dobrostanem. Podczas gdy jest wiele możliwych podejść do redystrybucji władzy napędzanej przez AI szerzej, żadne nie pojawi się domyślnie: muszą być celowo zaprojektowane i zarządzane mechanizmami jak wymogi powiernicze, dostarczanie publiczne i dostęp warstwowy bazujący na ryzyku.

Podejścia do łagodzenia koncentracji władzy mogą napotkać znaczny opór ze strony władz obecnych.[^27] Ale są ścieżki rozwoju AI, które nie wymagają wybierania między bezpieczeństwem a skoncentrowaną władzą. Budując odpowiednie instytucje teraz, moglibyśmy zapewnić, że korzyści AI są szeroko dzielone, a jego ryzyko ostrożnie zarządzane.

## Nowe struktury zarządzania i społeczne

Nasze obecne struktury zarządzania się z trudem radzą: są wolne w odpowiadaniu, często przejęte przez interesy szczególne i [coraz bardziej nie ufają im ludzie.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) Jednak nie jest to powód do ich porzucenia – wręcz przeciwnie. Niektóre instytucje mogą wymagać zastąpienia, ale szerzej potrzebujemy nowych mechanizmów mogących wzmacniać i uzupełniać nasze istniejące struktury, pomagając im lepiej funkcjonować w naszym szybko ewoluującym świecie.

Wiele z naszej instytucjonalnej słabości wywodzi się nie z formalnych struktur rządowych, ale ze zdegradowanych instytucji społecznych: naszych systemów rozwoju wspólnego zrozumienia, koordynacji działania i prowadzenia znaczącego dyskursu. Dotąd AI przyspieszyło tę degradację, zalewając nasze kanały informacyjne generowaną zawartością, wskazując nas na najbardziej polaryzującą i dzielącą zawartość oraz utrudniając odróżnienie prawdy od fikcji.

Ale AI mogłoby faktycznie pomóc w odbudowie i wzmacnianiu tych instytucji społecznych. Rozważmy trzy kluczowe obszary:

Po pierwsze, AI mogłoby pomóc przywrócić zaufanie do naszych systemów epistemicznych – naszych sposobów poznawania tego, co jest prawdziwe. Moglibyśmy opracować wspierane przez AI systemy śledzące i weryfikujące pochodzenie informacji, od surowych danych przez analizę do wniosków. Te systemy mogłyby łączyć weryfikację kryptograficzną z wyrafinowaną analizą, by pomóc ludziom zrozumieć nie tylko czy coś jest prawdziwe, ale jak wiemy, że jest prawdziwe.[^28] Lojalni asystenci AI mogliby być obciążeni śledzeniem szczegółów, by zapewnić, że się sprawdzają.

Po drugie, AI mogłoby umożliwić nowe formy koordynacji na dużą skalę. Wiele z naszych najpilniejszych problemów – od zmiany klimatu po oporność na antybiotyki – to fundamentalnie problemy koordynacji. Jesteśmy [uwięzieni w sytuacjach, które są gorsze, niż mogłyby być dla niemal wszystkich](https://equilibriabook.com/), bo żadna jednostka czy grupa nie może sobie pozwolić na pierwszy ruch. Systemy AI mogłyby pomóc przez modelowanie złożonych struktur zachęt, identyfikację wykonalnych ścieżek do lepszych wyników i ułatwianie budowy zaufania i mechanizmów zobowiązań potrzebnych, by tam dotrzeć.

Może najbardziej intrygująco, AI mogłoby umożliwić całkowicie nowe formy dyskursu społecznego. Wyobraź sobie możliwość „rozmowy z miastem" [^29] – nie tylko przeglądania statystyk, ale prowadzenia znaczącego dialogu z systemem AI przetwarzającym i syntetyzującym poglądy, doświadczenia, potrzeby i aspiracje milionów mieszkańców. Lub rozważmy, jak AI mogłoby ułatwić prawdziwy dialog między grupami, które obecnie gadają obok siebie, pomagając każdej stronie lepiej zrozumieć rzeczywiste troski i wartości drugiej, a nie ich karykatury siebie nawzajem.[^30] Lub AI mogłoby oferować wykwalifikowane, wiarygodnie neutralne pośredniczenie w sporach między ludźmi czy nawet dużymi grupami ludzi (którzy wszyscy mogliby z nim bezpośrednio i indywidualnie interakcjować!) Obecne AI jest całkowicie zdolne do robienia tej pracy, ale narzędzia do tego nie powstaną same z siebie czy przez zachęty rynkowe.

Te możliwości mogą brzmieć utopijnie, szczególnie biorąc pod uwagę obecną rolę AI w degradacji dyskursu i zaufania. Ale to właśnie dlatego musimy aktywnie opracowywać te pozytywne aplikacje. Zamykając Bramy przed niekontrolowalną AGI i priorytetyzując AI wzmacniające ludzką sprawczość, możemy skierować postęp technologiczny ku przyszłości, gdzie AI służy jako siła wzmocnienia, odporności i zbiorowego rozwoju.


[^1]: To powiedziawszy, trzymanie się z daleka od potrójnego przecięcia niestety nie jest tak łatwe, jak można by chcieć. Forsowanie zdolności bardzo mocno w którymkolwiek z trzech aspektów zwykle zwiększa je w pozostałych. W szczególności może być trudno stworzyć skrajnie ogólną i zdolną inteligencję, której nie można łatwo uczynić autonomiczną. Jedno podejście to trenowanie modeli [„krótkowzrocznych"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) z ograniczoną zdolnością planowania. Inne to skupienie się na inżynierii czystych systemów [„wyrocznii"](https://arxiv.org/abs/1711.05541), które stronią od odpowiadania na pytania zorientowane na działania.

[^2]: Wiele firm nie zdaje sobie sprawy, że one też zostałyby ostatecznie wyparte przez AGI, nawet jeśli zajęłoby to dłużej – gdyby zdawały, może naciskałyby na te Bramy nieco mniej!

[^3]: Systemy AI mogą komunikować się w wydajniejszy, ale mniej zrozumiały sposób, ale utrzymanie ludzkiego zrozumienia powinno mieć priorytet.

[^4]: Ta idea modularnego, interpretowalnego AI została opracowana szczegółowo przez kilku badaczy; zobacz np. model [„Kompleksowych Usług AI"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) Drexlera, [„Otwartą Architekturę Sprawczości"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai) Dalrymple'a i innych. Podczas gdy takie systemy mogą wymagać więcej wysiłku inżynierskiego niż monolityczne sieci neuronowe trenowane z ogromną mocą obliczeniową, to właśnie tam ograniczenia obliczeniowe pomagają – czyniąc bezpieczniejszą, bardziej transparentną ścieżkę także bardziej praktyczną.

[^5]: O przypadkach bezpieczeństwa generalnie zobacz [ten podręcznik](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). Dotycząc AI w szczególności, zobacz [Wasil i in.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [Clymer i in.](https://arxiv.org/abs/2403.10462), [Buhl i in.](https://arxiv.org/abs/2410.21572) i [Balesni i in.](https://arxiv.org/abs/2411.03336)

[^6]: Faktycznie już widzimy ten trend napędzany tylko wysokim kosztem inferencji: mniejsze i bardziej wyspecjalizowane modele „destylowane" z większych i zdolne do działania na tańszym sprzęcie.

[^7]: Rozumiem, dlaczego ci podekscytowani ekosystemem technologii AI mogą przeciwstawiać się temu, co postrzegają jako uciążliwe regulacje swojej branży. Ale szczerze mówiąc, zagadkowe dla mnie jest, dlaczego, powiedzmy, inwestor venture capital chciałby pozwolić na ucieczkę do AGI i superinteligencji. Te systemy (i firmy, dopóki pozostają pod kontrolą firm) *zjedzą wszystkie startupy jako przekąskę*. Prawdopodobnie nawet *wcześniej* niż zjedzenie innych branż. Każdy zainwestowany w kwitnący ekosystem AI powinien priorytetizować zapewnienie, że rozwój AGI nie prowadzi do monopolizacji przez kilku dominujących graczy.

[^8]: Jak powiedział ekonomista i były badacz Deepmind Michael Webb [tutaj](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/), „Myślę, że gdybyśmy zatrzymali dzisiaj cały rozwój większych modeli językowych, więc GPT-4 i Claude i cokolwiek, i to są ostatnie rzeczy, które trenujemy tej wielkości – więc pozwalamy na mnóstwo iteracji na rzeczach tej wielkości i wszelkich strojeniach, ale nic większego niż to, żadne większe postępy – tylko to, co mamy dzisiaj, myślę, wystarczy do napędzania 20 lub 30 lat niewiarygodnego wzrostu ekonomicznego."

[^9]: Na przykład system alphafold DeepMind użył tylko 1/100000 liczby FLOP GPT-4.

[^10]: Trudność samojeżdżących samochodów jest ważna do odnotowania tutaj: podczas gdy nominalnie wąskie zadanie i osiągalne z uczciwą niezawodnością z względnie małymi systemami AI, rozległa wiedza i zrozumienie świata rzeczywistego są konieczne do uzyskania niezawodności na poziomie potrzebnym w tak krytycznym pod względem bezpieczeństwa zadaniu.

[^11]: Na przykład, mając budżet obliczeniowy, prawdopodobnie zobaczylibyśmy modele GPAI wstępnie trenowane na (powiedzmy) połowie tego budżetu, a drugą połowę użytą do trenowania bardzo wysokiej zdolności w węższym zakresie zadań. To dałoby nadludzką wąską zdolność podpartą przez bliską człowiekowi inteligencję ogólną.

[^12]: Obecna dominująca technika dostosowania to „uczenie się przez wzmocnienie przez opinie ludzkie" [(RLHF)](https://arxiv.org/abs/1706.03741) i używa opinii ludzkich do stworzenia sygnału nagrody/kary dla uczenia się przez wzmocnienie modelu AI. Ta i pokrewne techniki jak [konstytucyjne AI](https://arxiv.org/abs/2212.08073) działają zaskakująco dobrze (choć brakuje im solidności i można je obejść przy skromnym wysiłku). Dodatkowo obecne modele językowe są generalnie wystarczająco kompetentne w rozumowaniu zdroworozsądkowym, że nie popełnią głupich błędów moralnych. To coś w rodzaju słodkiego punktu: wystarczająco mądre, by rozumieć, czego ludzie chcą (w stopniu, w jakim można to zdefiniować), ale nie wystarczająco mądre, by planować skomplikowane oszustwa czy powodować ogromne szkody, gdy się myląt.

[^13]: W długim terminie każdy poziom zdolności AI, który zostanie opracowany, prawdopodobnie się rozprzestrzeni, ponieważ ostatecznie to oprogramowanie i użyteczne. Będziemy musieli mieć solidne mechanizmy obrony przed ryzykami stwarzanymi przez takie systemy. Ale *nie mamy tego teraz*, więc musimy być bardzo odmierzeni w tym, jak wiele potężnych modeli AI pozwala się rozprzestrzenić.

[^14]: Zdecydowana większość z nich to niekonsensualne pornograficzne deepfake'i, w tym nieletnich.

[^15]: Wiele składników takich rozwiązań istnieje, w formie przepisów „bot-czy-nie" (w akcie AI UE między innymi), [technologiach śledzenia pochodzenia przemysłu](https://c2pa.org/), [innowacyjnych agregatorach wiadomości](https://www.improvethenews.org/), [agregatach](https://metaculus.com/) przewidywań i rynkach itd.

[^16]: Fala automatyzacji może nie podążać za wcześniejszymi wzorcami, w tym, że stosunkowo *wysokie* zadania umiejętności, takie jak jakościowe pisanie, interpretacja prawa czy dawanie porad medycznych, mogą być tak samo lub nawet bardziej podatne na automatyzację niż zadania niższych umiejętności.

[^17]: Dla ostrożnego modelowania efektu AGI na płace zobacz raport [tutaj](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek) i krwawe szczegóły [tutaj](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0) od Antona Korineka i współpracowników. Odkrywają, że gdy więcej części pracy jest automatyzowanych, produktywność i płace rosną – do pewnego punktu. Gdy *zbyt* wiele jest automatyzowane, produktywność nadal rośnie, ale płace walą się, bo ludzie są zastępowani hurtowo przez wydajne AI. Dlatego zamknięcie Bram jest tak użyteczne: dostajemy produktywność bez znikniętych ludzkich płac.

[^18]: Jest wiele sposobów, w jakie AI może być używane jako i do pomocy w budowaniu technologii „obronnych", by uczynić ochrony i zarządzanie bardziej solidnymi. Zobacz [ten](https://vitalik.eth.limo/general/2025/01/05/dacc2.html) wpływowy post opisujący tę agendę „D/acc".

[^19]: Nieco ironicznie, amerykański projekt Manhattan prawdopodobnie niewiele by zrobił do przyspieszenia terminów do AGI – tarcza ludzkiej i fiskalnej inwestycji w postęp AI jest już przypięta na 11. Główne rezultaty byłyby inspirowanie podobnego projektu w Chinach (które wyróżniają się w projektach infrastrukturalnych na poziomie narodowym), utrudnienie międzynarodowych umów ograniczających ryzyko AI i zaalarmowanie innych geopolitycznych przeciwników USA, takich jak Rosja.

[^20]: Program [„Narodowego Zasobu Badań AI"](https://nairrpilot.org/) to dobry obecny krok w tym kierunku i powinien być rozszerzony.

[^21]: Zobacz [tę analizę](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807) różnych znaczeń i implikacji „otwartości" w produktach technologicznych i jak niektóre doprowadziły do więcej, a nie mniej, utrwalenia dominacji.

[^22]: Plany w USA na [Narodowy Zasób Badań AI](https://nairratdoe.ornl.gov/) i niedawne uruchomienie [Europejskiej Fundacji AI](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/) to interesujące kroki w tym kierunku.

[^23]: Wyzwanie tutaj nie jest techniczne, ale instytucjonalne – pilnie potrzebujemy rzeczywistych przykładów i eksperymentów tego, jak mogłby wyglądać rozwój AI w interesie publicznym.

[^24]: To idzie przeciw obecnym modelom biznesowym dużych technologii i wymagałoby zarówno działań prawnych, jak i nowych norm.

[^25]: Tylko niektóre rządy będą w stanie to robić. Bardziej radykalną ideą jest [uniwersalny fundusz tego typu, pod współwłasnością wszystkich ludzi.](https://futureoflife.org/project/the-windfall-trust/)

[^26]: Dla obszernego wywodu tego przypadku zobacz [ten artykuł](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338) o lojalności AI. Niestety domyślna trajektoria asystentów AI prawdopodobnie będzie tą, gdzie są coraz bardziej nielojalni.

[^27]: Nieco ironicznie, wiele władz obecnych też jest zagrożonych dezaktywacją wspieraną przez AI; ale może być trudno im to dostrzec, dopóki i chyba że proces nie zajdzie dość daleko.

[^28]: Niektóre interesujące wysiłki w tym kierunku są reprezentowane przez [koalicję c2pa](https://c2pa.org/) w kwestii weryfikacji kryptograficznej; [Verity](https://www.improvethenews.org/) i [Ground news](https://ground.news/) w kwestii lepszej epistemologii wiadomości; oraz [Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com) i rynki przewidywań w kwestii ugruntowania dyskursu w falsyfikowalnych przewidywaniach.

[^29]: Zobacz [ten](https://talktothecity.org/) fascynujący projekt pilotażowy.

[^30]: Zobacz [Kialo](https://www.kialo-edu.com/) i wysiłki [Projektu Inteligencji Zbiorowej](https://www.cip.org/) dla niektórych przykładów.