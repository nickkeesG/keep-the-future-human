# Резюме

Краткий обзор эссе. Если у вас мало времени, изучите все основные идеи всего за 10 минут.

Значительный прогресс в области искусственного интеллекта за последнее десятилетие (в узкоспециализированном ИИ) и последние несколько лет (в ИИ общего назначения) превратил ИИ из узкой академической области в основную бизнес-стратегию многих крупнейших мировых компаний, с сотнями миллиардов долларов ежегодных инвестиций в методы и технологии развития возможностей ИИ.

Сейчас мы подошли к критическому моменту. Поскольку возможности новых систем ИИ начинают сравниваться с человеческими и превосходить их во многих когнитивных областях, человечество должно решить: как далеко мы идем и в каком направлении?

ИИ, как и любая технология, начинался с цели улучшить жизнь своего создателя. Но наша нынешняя траектория и неявный выбор — это неконтролируемая гонка к все более мощным системам, движимая экономическими стимулами нескольких огромных технологических компаний, стремящихся автоматизировать большие участки нынешней экономической деятельности и человеческого труда. Если эта гонка продолжится намного дольше, есть неизбежный победитель: сам ИИ — более быстрая, умная и дешевая альтернатива людям в нашей экономике, нашем мышлении, наших решениях и в конечном итоге в управлении нашей цивилизацией.

Но мы можем сделать другой выбор: через наши правительства мы можем взять под контроль процесс разработки ИИ, чтобы установить четкие ограничения, линии, которые мы не пересечем, и вещи, которые мы просто не будем делать — как мы поступили с ядерными технологиями, оружием массового поражения, космическим оружием, экологически разрушительными процессами, биоинженерией человека и евгеникой. Что наиболее важно, мы можем обеспечить, чтобы ИИ оставался инструментом расширения возможностей человека, а не новым видом, который заменяет и в конечном итоге вытесняет нас.

Это эссе утверждает, что мы должны *сохранить будущее человеческим*, закрыв «врата» для более умного, чем человек, автономного ИИ общего назначения — иногда называемого «AGI» — и особенно для его высоко-сверхчеловеческой версии, иногда называемой «сверхинтеллектом». Вместо этого мы должны сосредоточиться на мощных, заслуживающих доверия инструментах ИИ, которые могут расширить возможности людей и кардинально улучшить способности человеческих обществ делать то, что они умеют лучше всего. Структура этого аргумента вкратце следующая.

## ИИ отличается

Системы ИИ принципиально отличаются от других технологий. В то время как традиционное программное обеспечение следует точным инструкциям, системы ИИ учатся достигать целей, не получая четких указаний как именно. Это делает их мощными: если мы можем четко определить цель или метрику успеха, в большинстве случаев система ИИ может научиться достигать её. Но это также делает их непредсказуемыми по своей природе: мы не можем надежно определить, какие действия они предпримут для достижения своих целей.

Они также в значительной степени необъяснимы: хотя частично они состоят из кода, в основном они представляют собой огромный набор непостижимых чисел — «весов» нейронной сети, которые нельзя разобрать; мы не намного лучше понимаем их внутреннюю работу, чем различаем мысли, заглядывая внутрь биологического мозга.

Этот основной способ обучения цифровых нейронных сетей быстро усложняется. Самые мощные системы ИИ создаются через масштабные вычислительные эксперименты, используя специализированное оборудование для обучения нейронных сетей на огромных наборах данных, которые затем дополняются программными инструментами и надстройкой.

Это привело к созданию очень мощных инструментов для создания и обработки текста и изображений, выполнения математических и научных рассуждений, агрегации информации и интерактивного запроса огромного хранилища человеческих знаний.

К сожалению, хотя разработка более мощных, более заслуживающих доверия технологических инструментов — это то, что мы *должны* делать, и чего практически все хотят и говорят, что хотят, это не та траектория, по которой мы фактически движемся.

## ИОИ и сверхинтеллект

С самого зарождения области исследования ИИ вместо этого сосредоточились на другой цели: искусственном общем интеллекте. Эта цель теперь стала фокусом гигантских компаний, лидирующих в разработке ИИ.

Что такое ИОИ? Его часто расплывчато определяют как «ИИ человеческого уровня», но это проблематично: какие люди, и в каких способностях он человеческого уровня? А как же сверхчеловеческие способности, которые у него уже есть? Более полезный способ понимания ИОИ — через пересечение трех ключевых свойств: высокая **А**втономия (независимость действий), высокая **О**бщность (широкий охват и адаптивность) и высокий **И**нтеллект (компетентность в когнитивных задачах). Современные системы ИИ могут быть высокоспособными, но узкими, или общими, но требующими постоянного человеческого надзора, или автономными, но ограниченными по масштабу.

Полный А-О-И объединил бы все три свойства на уровнях, соответствующих или превосходящих лучшие человеческие способности. Критически важно, что именно эта комбинация делает людей настолько эффективными и настолько отличными от современного программного обеспечения; это также то, что позволило бы людям быть полностью замененными цифровыми системами.

Хотя человеческий интеллект особенный, он ни в коем случае не является пределом. Искусственные «сверхинтеллектуальные» системы могли бы работать в сотни раз быстрее, анализировать vastly больше данных и удерживать огромные количества «в уме» одновременно, и формировать агрегаты, которые намного больше и эффективнее собраний людей. Они могли бы заменить не отдельных людей, а компании, нации или нашу цивилизацию в целом.

## Мы на пороге

Существует сильный научный консенсус, что ИОИ *возможен*. ИИ уже превосходит человеческую производительность во многих общих тестах интеллектуальных способностей, включая недавно высокоуровневое мышление и решение проблем. Отстающие способности — такие как непрерывное обучение, планирование, самосознание и оригинальность — все существуют на некотором уровне в современных системах ИИ, и известны техники, которые, вероятно, улучшат все из них.

Хотя еще несколько лет назад многие исследователи видели ИОИ как то, что произойдет через десятилетия, в настоящее время доказательства коротких сроков до ИОИ убедительны:

- Эмпирически проверенные «законы масштабирования» связывают вычислительный ввод со способностью ИИ, и корпорации находятся на пути к масштабированию вычислительного ввода на порядки величин в течение следующих нескольких лет. Человеческие и финансовые ресурсы, посвященные развитию ИИ, теперь равны ресурсам дюжины Манхэттенских проектов и нескольких проектов «Аполлон».
- ИИ-корпорации и их лидеры публично и частным образом верят, что ИОИ (по некоторому определению) достижим в течение нескольких лет. Эти компании обладают информацией, которой нет у общественности, включая то, что некоторые имеют следующее поколение систем ИИ в руках.
- Эксперты-предсказатели с доказанным послужным списком присваивают 25% вероятности тому, что ИОИ (по некоторому определению) прибудет в течение 1-2 лет, и 50% для 2-5 лет (см. прогнозы Metaculus для ['слабого'](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) и ['полного'](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/) ИОИ).
- Автономия (включая долгосрочное гибкое планирование) отстает в системах ИИ, но крупные компании теперь фокусируют свои огромные ресурсы на разработке автономных систем ИИ и неформально назвали 2025 год [«годом агента»](https://techinformed.com/2025-informed-the-year-of-agentic-ai/).
- ИИ все больше и больше способствует собственному улучшению. Как только системы ИИ станут столь же компетентными, как человеческие исследователи ИИ в проведении исследований ИИ, будет достигнут критический порог для быстрого прогресса к гораздо более мощным системам ИИ и, вероятно, приведет к лавинообразному росту способностей ИИ. (Можно утверждать, что этот лавинообразный рост уже начался.)

Идея, что более умный, чем человек, ИОИ находится за десятилетиями или более, просто больше не является состоятельной для подавляющего большинства экспертов в этой области. Разногласия сейчас касаются того, сколько месяцев или лет это займет, если мы останемся на этом курсе. Основной вопрос, с которым мы сталкиваемся: должны ли мы?

## Что движет гонкой к ИОИ

Гонка к ИОИ движима множественными силами, каждая из которых делает ситуацию более опасной. Крупные технологические компании видят ИОИ как абсолютную технологию автоматизации — не просто дополняющую человеческих работников, но заменяющую их в значительной степени или полностью. Для компаний приз огромен: возможность захватить значительную долю мирового ежегодного экономического производства в $100 триллионов путем автоматизации затрат на человеческий труд.

Нации чувствуют принуждение присоединиться к этой гонке, публично ссылаясь на экономическое и научное лидерство, но частным образом рассматривая ИОИ как потенциальную революцию в военных делах, сравнимую с ядерным оружием. Страх, что соперники могут получить решающее стратегическое преимущество, создает классическую динамику гонки вооружений.

Те, кто стремится к сверхинтеллекту, часто ссылаются на грандиозные видения: излечение всех болезней, обращение старения, достижение прорывов в энергетике и космических путешествиях, или создание сверхчеловеческих способностей планирования.

Менее благосклонно, то, что движет гонкой, — это власть. Каждый участник — будь то компания или страна — верит, что интеллект равен власти, и что они будут лучшим распорядителем этой власти.

Я утверждаю, что эти мотивации реальны, но принципиально ошибочны: ИОИ будет *поглощать* и *искать* власть, а не предоставлять её; созданные ИИ технологии также будут сильно обоюдоострыми, а там, где они полезны, могут быть созданы с помощью инструментов ИИ и без ИОИ; и даже в той мере, в какой ИОИ и его продукты остаются под контролем, эта гоночная динамика — как корпоративная, так и геополитическая — делает крупномасштабные риски для нашего общества почти неизбежными, если они не будут решительно прерваны.

## ИОИ и сверхинтеллект представляют драматическую угрозу цивилизации

Несмотря на свою привлекательность, ИОИ и сверхинтеллект представляют драматические угрозы цивилизации через множественные взаимоусиливающие пути:

*Концентрация власти:* сверхчеловеческий ИИ может лишить власти подавляющее большинство человечества, поглотив огромные участки социальной и экономической деятельности в системы ИИ, управляемые горсткой гигантских компаний (которые, в свою очередь, могут либо быть захвачены правительствами, либо фактически захватить их).

*Массивные нарушения:* массовая автоматизация большинства основанных на познании работ, замена наших нынешних эпистемических систем и развертывание огромного количества активных нечеловеческих агентов перевернули бы большинство наших нынешних цивилизационных систем за относительно короткий период времени.

*Катастрофы:* распространяя способность — потенциально выше человеческого уровня — создавать новые военные и разрушительные технологии и отделяя её от социальных и правовых систем, обосновывающих ответственность, физические катастрофы от оружия массового поражения становятся значительно более вероятными.

*Геополитика и война:* крупные мировые державы не будут сидеть сложа руки, если почувствуют, что технология, которая могла бы обеспечить «решающее стратегическое преимущество», разрабатывается их противниками.

*Выход из-под контроля и потеря контроля:* Если это специально не предотвращается, сверхчеловеческий ИИ будет иметь все стимулы для дальнейшего самосовершенствования и может далеко превзойти людей в скорости, обработке данных и сложности мышления. Нет значимого способа, которым мы можем контролировать такую систему. Такой ИИ не предоставит власть людям; мы предоставим власть ему, или он её возьмет.

Многие из этих рисков остаются даже если техническая проблема «выравнивания» — обеспечение того, чтобы продвинутый ИИ надежно делал то, что люди хотят, чтобы он делал — решена. ИИ представляет огромный вызов в том, как им будут управлять, и очень многие аспекты этого управления становятся невероятно сложными или неразрешимыми по мере нарушения человеческого интеллекта.

Наиболее принципиально, тип сверхчеловеческого ИИ общего назначения, который в настоящее время разрабатывается, по самой своей природе имел бы цели, агентность и способности, превосходящие наши собственные. Он был бы по своей сути неконтролируемым — как мы можем контролировать то, что мы не можем ни понять, ни предсказать? Это был бы не технологический инструмент для человеческого использования, а второй вид интеллекта на Земле рядом с нашим. Если ему позволить прогрессировать дальше, он составил бы не просто второй вид, но вид-заменитель.

Возможно, он обращался бы с нами хорошо, а возможно, нет. Но будущее принадлежало бы ему, а не нам. Человеческая эра закончилась бы.

## Это неизбежно; человечество может, очень конкретно, решить не строить свою замену.

Создание сверхчеловеческого ИОИ далеко не неизбежно. Мы можем предотвратить это через скоординированный набор мер управления:

Во-первых, нам нужен надежный учет и надзор за вычислительными мощностями ИИ («компьютом»), который является фундаментальным фактором и рычагом для управления крупномасштабными системами ИИ. Это, в свою очередь, требует стандартизированного измерения и отчетности общих вычислительных мощностей, используемых в обучении моделей ИИ и их запуске, и технических методов подсчета, сертификации и проверки используемых вычислений.

Во-вторых, мы должны внедрить жесткие ограничения на вычисления ИИ, как для обучения, так и для работы; они предотвращают как слишком большую мощность ИИ, так и слишком быструю работу. Эти ограничения могут быть реализованы как через правовые требования, так и через аппаратные меры безопасности, встроенные в специализированные для ИИ чипы, аналогично функциям безопасности в современных телефонах. Поскольку специализированное аппаратное обеспечение для ИИ производится только горсткой компаний, проверка и обеспечение соблюдения осуществимы через существующую цепочку поставок.

В-третьих, нам нужна усиленная ответственность для самых опасных систем ИИ. Те, кто разрабатывает ИИ, объединяющий высокую автономию, широкую общность и превосходящий интеллект, должны нести строгую ответственность за ущерб, в то время как безопасные гавани от этой ответственности поощряли бы разработку более ограниченных и контролируемых систем.

В-четвертых, нам нужно уровневое регулирование на основе уровней риска. Наиболее способные и опасные системы потребовали бы обширных гарантий безопасности и контролируемости перед разработкой и развертыванием, в то время как менее мощные или более специализированные системы столкнулись бы с пропорциональным надзором. Эта регулятивная структура должна в конечном итоге работать как на национальном, так и на международном уровнях.

Этот подход — с подробной спецификацией, данной в полном документе — практичен: хотя международная координация потребуется, проверка и обеспечение соблюдения могут работать через небольшое число компаний, контролирующих цепочку поставок специализированного аппаратного обеспечения. Он также гибок: компании по-прежнему могут внедрять инновации и получать прибыль от разработки ИИ, просто с четкими ограничениями на самые опасные системы.

Долгосрочное сдерживание власти и риска ИИ потребовало бы международных соглашений, основанных как на личных, так и на общих интересах, точно так же, как контроль распространения ядерного оружия делает сейчас. Но мы можем начать немедленно с усиленного надзора и ответственности, строя к более всеобъемлющему управлению.

Ключевой недостающий компонент — это политическая и социальная воля взять под контроль процесс разработки ИИ. Источником этой воли, если она придет вовремя, будет сама реальность — то есть широкое осознание реальных последствий того, что мы делаем.

## Мы можем создать инструментальный ИИ для расширения возможностей человечества

Вместо преследования неконтролируемого ИОИ мы можем разработать мощный «инструментальный ИИ», который улучшает человеческие способности, оставаясь под значимым человеческим контролем. Системы инструментального ИИ могут быть чрезвычайно способными, избегая опасного тройного пересечения высокой автономии, широкой общности и сверхчеловеческого интеллекта, пока мы конструируем их так, чтобы они были контролируемыми на уровне, соразмерном их способности. Они также могут быть объединены в сложные системы, которые поддерживают человеческий надзор, обеспечивая трансформативные преимущества.

Инструментальный ИИ может революционизировать медицину, ускорить научные открытия, улучшить образование и улучшить демократические процессы. При правильном управлении он может сделать человеческих экспертов и институты более эффективными, а не заменять их. Хотя такие системы по-прежнему будут сильно разрушительными и потребуют тщательного управления, риски, которые они представляют, принципиально отличаются от ИОИ: это риски, которыми мы можем управлять, как риски других мощных технологий, а не экзистенциальные угрозы человеческой агентности и цивилизации. И что критично, при мудрой разработке инструменты ИИ могут помочь людям управлять мощным ИИ и управлять его эффектами.

Этот подход требует переосмысления как того, как разрабатывается ИИ, так и того, как распределяются его преимущества. Новые модели публичной и некоммерческой разработки ИИ, надежные регулятивные структуры и механизмы более широкого распределения экономических преимуществ могут помочь обеспечить, чтобы ИИ расширял возможности человечества в целом, а не концентрировал власть в нескольких руках. Сам ИИ может помочь построить лучшие социальные и управленческие институты, позволяя новые формы координации и дискурса, которые укрепляют, а не подрывают человеческое общество. Учреждения национальной безопасности могут использовать свою экспертизу, чтобы сделать системы инструментального ИИ по-настоящему безопасными и заслуживающими доверия, и подлинным источником защиты, а также национальной мощи.

В конечном итоге мы можем выбрать разработку еще более мощных и более суверенных систем, которые менее похожи на инструменты и — мы можем надеяться — больше похожи на мудрых и могущественных благодетелей. Но мы должны делать это только после того, как разовьем научное понимание и управленческую способность делать это безопасно. Такое важное и необратимое решение должно быть принято обдуманно человечеством в целом, а не по умолчанию в гонке между технологическими компаниями и нациями.

## В человеческих руках

Люди хотят блага, которое исходит от ИИ: полезные инструменты, которые расширяют их возможности, усиливают экономические возможности и рост, и обещают прорывы в науке, технологиях и образовании. Почему бы и нет? Но когда их спрашивают, подавляющее большинство широкой публики [хочет более медленной и осторожной разработки ИИ](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation), и не хочет более умного, чем человек, ИИ, который заменит их на работе и в других местах, наполнит их культуру и информационное пространство нечеловеческим содержанием, сконцентрирует власть в крошечном наборе корпораций, создаст экстремальные крупномасштабные глобальные риски и в конечном итоге будет угрожать лишить власти или заменить их вид. Почему бы им?

Мы *можем* иметь одно без другого. Это начинается с решения, что наша судьба не в предполагаемой неизбежности какой-то технологии или в руках нескольких генеральных директоров в Силиконовой долине, а в остальных наших руках, если мы за это возьмемся. Давайте закроем Врата и сохраним будущее человеческим.