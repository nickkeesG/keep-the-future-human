# Приложения

Дополнительная информация, включающая технические детали учёта вычислительных мощностей, пример реализации «закрытия врат», подробности строгого режима ответственности за ИОИ и многоуровневый подход к стандартам безопасности ИОИ.

## Приложение А: Технические детали учёта вычислительных мощностей

Для осмысленного контроля на основе вычислительных мощностей требуется детальная методика как для «абсолютной истины», так и для хороших приближений общего объёма вычислений, используемых при обучении и выводе. Вот пример того, как «абсолютная истина» могла бы подсчитываться на техническом уровне.

**Определения:**

*Причинно-следственный граф вычислений:* Для данного выхода O модели ИИ существует набор цифровых вычислений, изменение результата которых потенциально может изменить O. (Это следует предполагать консервативно, то есть должна быть чёткая причина полагать, что вычисление не зависит от предшественника, который происходит раньше во времени и имеет физический потенциальный причинный путь воздействия.) Это включает вычисления, выполняемые моделью ИИ во время вывода, а также вычисления, которые использовались для входных данных, подготовки данных и обучения модели. Поскольку любое из этих вычислений само может быть выходом модели ИИ, это вычисляется рекурсивно, с обрывом там, где человек внёс значительное изменение во входные данные.

*Вычисления обучения:* Общий объём вычислений в FLOP или других единицах, охватываемый причинно-следственным графом вычислений нейронной сети (включая подготовку данных, обучение, тонкую настройку и любые другие вычисления).

*Вычисления вывода:* Общий объём вычислений в причинно-следственном графе вычислений данного выхода ИИ, включая все нейронные сети (и включая их Вычисления обучения) и другие вычисления, участвующие в этом выходе.

*Скорость вычислений вывода:* В серии выходов — скорость изменения (в FLOP/с или других единицах) Вычислений вывода между выходами, то есть вычисления, используемые для производства следующего выхода, делённые на временной интервал между выходами.

**Примеры и приближения:**

- Для одной нейронной сети, обученной на созданных человеком данных, Вычисления обучения — это просто общие вычисления обучения, как обычно сообщается.
- Для такой нейронной сети, выполняющей вывод с постоянной скоростью, Скорость вычислений вывода приблизительно равна общей скорости вычислительного кластера, выполняющего вывод, в FLOP/с.
- Для тонкой настройки модели Вычисления обучения полной модели определяются как Вычисления обучения не-тонко-настроенной модели плюс вычисления, выполненные во время тонкой настройки и для подготовки любых данных, используемых при тонкой настройке.
- Для дистиллированной модели Вычисления обучения полной модели включают обучение как дистиллированной модели, так и большей модели, используемой для предоставления синтетических данных или других обучающих входных данных.
- Если обучено несколько моделей, но многие «попытки» отброшены на основе человеческого суждения, они не засчитываются в Вычисления обучения или Вычисления вывода сохранённой модели.

## Приложение Б: Пример реализации закрытия врат

**Пример реализации:** Вот один пример того, как могло бы работать закрытие врат при лимите в 10<sup>27</sup> FLOP для обучения и 10<sup>20</sup> FLOP/с для вывода (работы ИИ):

**1. Пауза:** По соображениям национальной безопасности исполнительная власть США просит все компании, базирующиеся в США, ведущие бизнес в США или использующие чипы, произведённые в США, прекратить любые новые запуски обучения ИИ, которые могут превысить лимит Вычислений обучения в 10<sup>27</sup> FLOP. США должны начать обсуждения с другими странами, где развивается ИИ, настоятельно призывая их предпринять аналогичные шаги и указывая, что пауза США может быть снята, если они решат не выполнять требования.

**2. Надзор и лицензирование США:** Путём исполнительного указа или действий существующего регулирующего агентства США требуют, чтобы в течение (скажем) одного года:

- Все запуски обучения ИИ с оценкой выше 10<sup>25</sup> FLOP, выполняемые компаниями, работающими в США, были зарегистрированы в базе данных, поддерживаемой американским регулирующим агентством. (Примечание: несколько более слабая версия этого уже была включена в отменённый исполнительный указ США по ИИ 2023 года, требующий регистрации моделей выше 10<sup>26</sup> FLOP.)
- Все производители оборудования, связанного с ИИ, работающие в США или ведущие бизнес с правительством США, соблюдали набор требований к своему специализированному оборудованию и программному обеспечению, управляющему им. (Многие из этих требований могли бы быть встроены в обновления программного обеспечения и прошивки существующего оборудования, но долгосрочные и надёжные решения потребовали бы изменений в более поздних поколениях оборудования.) Среди них требование о том, что если оборудование является частью высокоскоростного взаимосвязанного кластера, способного выполнять 10<sup>18</sup> FLOP/с вычислений, требуется более высокий уровень верификации, который включает регулярное разрешение от удалённого «регулятора», получающего как телеметрию, так и запросы на выполнение дополнительных вычислений.
- Владелец сообщает общий объём вычислений, выполненных на его оборудовании, агентству, поддерживающему базу данных США.
- Поэтапно вводятся более строгие требования, позволяющие как более безопасный, так и более гибкий надзор и выдачу разрешений.

**3. Международный надзор:**

- США, Китай и любые другие страны, где размещены передовые производства чипов, ведут переговоры о международном соглашении.
- Это соглашение создаёт новое международное агентство, аналогичное Международному агентству по атомной энергии, отвечающее за надзор за обучением и работой ИИ.
- Страны-подписанты должны требовать от своих отечественных производителей оборудования для ИИ соблюдения набора требований по крайней мере таких же строгих, как те, что введены в США.
- Владельцы теперь обязаны сообщать числа вычислений ИИ как агентствам в своих странах, так и новому офису в международном агентстве.
- Дополнительные страны настоятельно поощряются присоединиться к существующему международному соглашению: экспортный контроль стран-подписантов ограничивает доступ к высокотехнологичному оборудованию для стран, не подписавших соглашение, в то время как подписанты могут получить техническую поддержку в управлении своими системами ИИ.

**4. Международная верификация и принуждение:**

- Система верификации оборудования обновляется так, что сообщает об использовании вычислений как первоначальному владельцу, так и напрямую офису международного агентства.
- Агентство через обсуждение с подписантами международного соглашения договаривается об ограничениях вычислений, которые затем приобретают юридическую силу в странах-подписантах.
- Параллельно может быть разработан набор международных стандартов, так что обучение и работа ИИ выше порога вычислений (но ниже лимита) будут обязаны соблюдать эти стандарты.
- Агентство может, если необходимо для компенсации лучших алгоритмов и т.д., понизить лимит вычислений. Или, если это считается безопасным и целесообразным (на уровне доказуемых гарантий безопасности), повысить лимит вычислений.

## Приложение В: Подробности строгого режима ответственности за ИОИ

**Подробности строгого режима ответственности за ИОИ**

- Создание и эксплуатация передовой системы ИИ, которая является высоко общей, способной и автономной, считается «ненормально опасной» деятельностью.
- Таким образом, уровень ответственности по умолчанию за обучение и эксплуатацию таких систем — строгая, солидарная ответственность (или её неамериканский эквивалент) за любой вред, причинённый моделью или её выходами/действиями.
- Личная ответственность будет налагаться на руководителей и членов совета директоров в случаях грубой небрежности или умышленного неправомерного поведения. Это должно включать уголовные наказания для самых вопиющих случаев.
- Существуют многочисленные «безопасные гавани», при которых ответственность возвращается к ответственности по умолчанию (основанной на вине, в США), которой обычно подлежали бы люди и компании.
	- Модели, обученные и эксплуатируемые ниже некоторого порога вычислений (который был бы по крайней мере в 10 раз ниже описанных выше лимитов.)
	- ИИ, который является «слабым» (грубо говоря, ниже уровня человеческого эксперта в задачах, для которых он предназначен) и/или
	- ИИ, который является «узким» (имеющий фиксированный и довольно ограниченный круг задач и операций, для которых он специально разработан и обучен) и/или
	- ИИ, который является «пассивным» (очень ограниченный в своей способности — даже при небольших модификациях — предпринимать действия или выполнять сложные многошаговые задачи без прямого человеческого участия и контроля.)
	- ИИ, который гарантированно безопасен, защищён и контролируем (доказуемо безопасен, или анализ рисков указывает на незначительный уровень ожидаемого вреда.)
- «Безопасные гавани» могут заявляться на основе [обоснования безопасности](https://arxiv.org/abs/2410.21572), подготовленного разработчиком ИИ и одобренного агентством или аудитором, аккредитованным агентством. Чтобы заявить «безопасную гавань» на основе вычислений, разработчик должен просто предоставить достоверные оценки общих Вычислений обучения и максимальной Скорости вывода
- Законодательство должно чётко очерчивать ситуации, при которых судебные запреты на разработку систем ИИ с высоким риском общественного вреда были бы уместными.
- Консорциумы компаний, работая с НПО и правительственными агентствами, должны разработать стандарты и нормы, определяющие эти термины, как регуляторы должны предоставлять «безопасные гавани», как разработчики ИИ должны развивать обоснования безопасности, и как суды должны интерпретировать ответственность там, где «безопасные гавани» не заявлены проактивно.

## Приложение Г: Многоуровневый подход к стандартам безопасности ИОИ

**Многоуровневый подход к стандартам безопасности ИОИ**

| Уровень риска | Триггер(ы) | Требования для обучения | Требования для развёртывания |
| --- | --- | --- | --- |
| УР-0 | ИИ слабый в автономности, общности и интеллекте | нет | нет |
| УР-1 | ИИ сильный в одном из: автономность, общность и интеллект | нет | На основе риска и использования, потенциально обоснования безопасности, одобренные национальными властями везде, где модель может использоваться |
| УР-2 | ИИ сильный в двух из: автономность, общность и интеллект | Регистрация у национального органа, имеющего юрисдикцию над разработчиком | Обоснование безопасности, ограничивающее риск серьёзного вреда ниже разрешённых уровней, плюс независимые аудиты безопасности (включая чёрно-ящичное и бело-ящичное красное команды), одобренные национальными властями везде, где модель может использоваться |
| УР-3 | ИОИ сильный в автономности, общности и интеллекте | Предварительное одобрение плана безопасности национальным органом, имеющим юрисдикцию над разработчиком | Обоснование безопасности, гарантирующее ограниченный риск серьёзного вреда ниже разрешённых уровней, а также требуемые спецификации, включая кибербезопасность, контролируемость, несъёмный аварийный выключатель, выравнивание с человеческими ценностями и устойчивость к злонамеренному использованию. |
| УР-4 | Любая модель, которая также превышает либо 10<sup>27</sup> FLOP Обучения, либо 10<sup>20</sup> FLOP/с Вывода | Запрещено до международно согласованного снятия лимита вычислений | Запрещено до международно согласованного снятия лимита вычислений |

Классификации рисков и стандарты безопасности, с уровнями, основанными на пороговых значениях вычислений, а также комбинациях высокой автономности, общности и интеллекта:

- *Сильная автономность* применяется, если система способна выполнять или может быть легко приспособлена для выполнения многошаговых задач и/или предпринимать сложные действия, которые актуальны в реальном мире, без значительного человеческого надзора или вмешательства. Примеры: автономные транспортные средства и роботы; боты для финансовой торговли. Неподходящие примеры: GPT-4; классификаторы изображений
- *Сильная общность* указывает на широкий спектр применения, выполнение задач, для которых модель не была намеренно и специально обучена, и значительную способность изучать новые задачи. Примеры: GPT-4; mu-zero. Неподходящие примеры: AlphaFold; автономные транспортные средства; генераторы изображений
- *Сильный интеллект* соответствует соответствию производительности человеческого эксперта в задачах, в которых модель работает лучше всего (и для общей модели — по широкому кругу задач.) Примеры: AlphaFold; mu-zero; o3. Неподходящие примеры: GPT-4; Siri