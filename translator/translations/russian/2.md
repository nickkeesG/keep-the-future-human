# Глава 2 - Что нужно знать о нейронных сетях ИИ

Как работают современные системы ИИ и что нас может ждать в следующем поколении ИИ?

Чтобы понять, как будут развиваться последствия создания более мощного ИИ, необходимо усвоить некоторые основы. Эта и следующие две главы раскрывают их, рассматривая по очереди, что представляет собой современный ИИ, как он использует масштабные вычисления, и в каком смысле он быстро растёт в универсальности и возможностях.[^1]

Существует множество способов определить искусственный интеллект, но для наших целей ключевое свойство ИИ заключается в том, что если обычная компьютерная программа представляет собой список инструкций для выполнения задачи, то система ИИ — это та, которая учится на данных или опыте выполнять задачи *без явного указания, как это делать.*

Практически весь значимый современный ИИ основан на нейронных сетях. Это математические/вычислительные структуры, представленные очень большим (миллиарды или триллионы) набором чисел («весов»), которые хорошо справляются с обучающей задачей. Эти веса создаются (или, возможно, «выращиваются» или «находятся») путём их итеративного изменения так, чтобы нейронная сеть улучшала численную оценку (также называемую «функцией потерь»), определённую для хорошего выполнения одной или нескольких задач.[^2] Этот процесс известен как *обучение* нейронной сети.[^3]

Существует множество методов такого обучения, но эти детали гораздо менее важны, чем способы определения оценки и то, как они приводят к различным задачам, которые нейронная сеть выполняет хорошо. Исторически проводилось ключевое различие между «узким» и «общим» ИИ.

Узкий ИИ целенаправленно обучается выполнять конкретную задачу или небольшой набор задач (например, распознавание изображений или игру в шахматы); для новых задач требуется переобучение, и его возможности ограничены. У нас есть сверхчеловеческий узкий ИИ, что означает: практически для любой дискретной чётко определённой задачи, которую может выполнить человек, мы, вероятно, можем создать оценку, а затем успешно обучить узкую систему ИИ выполнять её лучше человека.

Системы ИИ общего назначения (GPAI) могут выполнять широкий спектр задач, включая многие из тех, для которых их явно не обучали; они также могут изучать новые задачи в процессе своей работы. Современные крупные «мультимодальные модели»[^4] вроде ChatGPT служат примером этого: обученные на очень большом корпусе текста и изображений, они могут заниматься сложными рассуждениями, писать код, анализировать изображения и помогать с огромным множеством интеллектуальных задач. Хотя они всё ещё сильно отличаются от человеческого интеллекта способами, которые мы подробно рассмотрим ниже, их универсальность произвела революцию в ИИ.[^5]

## Непредсказуемость: ключевая особенность систем ИИ

Ключевое различие между системами ИИ и обычным программным обеспечением заключается в предсказуемости. Вывод стандартного программного обеспечения может быть непредсказуемым — более того, иногда именно поэтому мы пишем программы, чтобы получить результаты, которые не могли предсказать. Но обычное программное обеспечение редко делает что-то, на что не было запрограммировано — его область действия и поведение обычно соответствуют замыслу. Первоклассная шахматная программа может делать ходы, которые не может предсказать ни один человек (иначе они могли бы победить эту шахматную программу!), но она, как правило, не будет делать ничего, кроме игры в шахматы.

Как и обычное программное обеспечение, узкий ИИ имеет предсказуемую область действия и поведение, но может давать непредсказуемые результаты. Это просто другой способ определить узкий ИИ: как ИИ, который подобен обычному программному обеспечению в своей предсказуемости и диапазоне действий.

ИИ общего назначения отличается: его область действия (домены, в которых он применяется), поведение (виды действий, которые он совершает) и результаты (его фактические выводы) — всё это может быть непредсказуемым.[^6] GPT-4 был обучен просто точно генерировать текст, но развил множество способностей, которые его разработчики не предсказывали и не планировали. Эта непредсказуемость проистекает из сложности обучения: поскольку обучающие данные содержат результаты многих различных задач, ИИ должен фактически научиться выполнять эти задачи, чтобы хорошо предсказывать.

Эта непредсказуемость систем общего ИИ весьма фундаментальна. Хотя в принципе возможно тщательно создать системы ИИ с гарантированными ограничениями их поведения (как упоминается далее в эссе), при нынешнем способе создания системы ИИ непредсказуемы на практике и даже в принципе.

## Пассивный ИИ, агенты, автономные системы и выравнивание

Эта непредсказуемость становится особенно важной, когда мы рассматриваем, как системы ИИ фактически развёртываются и используются для достижения различных целей.

Многие системы ИИ относительно пассивны в том смысле, что они в основном предоставляют информацию, а пользователь предпринимает действия. Другие, обычно называемые *агентами*, сами предпринимают действия с различной степенью участия пользователя. Те, которые предпринимают действия при относительно меньшем внешнем вмешательстве или надзоре, можно назвать более *автономными*. Это образует спектр с точки зрения независимости действий — от пассивных инструментов до автономных агентов.[^7]

Что касается целей систем ИИ, они могут быть напрямую связаны с их обучающей задачей (например, цель «победы» для системы, играющей в го, также явно соответствует тому, чему её обучали). Или могут не быть: обучающая задача ChatGPT частично состоит в предсказании текста, частично — в том, чтобы быть полезным помощником. Но при выполнении конкретной задачи его цель задаёт пользователь. Цели также могут создаваться самой системой ИИ, лишь очень опосредованно связанные с её обучающей задачей.[^8]

Цели тесно связаны с вопросом «выравнивания», то есть с вопросом о том, будут ли системы ИИ *делать то, что мы хотим, чтобы они делали*. Этот простой вопрос скрывает огромный уровень сложности.[^9] Пока что отметим, что «мы» в этом предложении может относиться ко многим разным людям и группам, что приводит к разным типам выравнивания. Например, ИИ может быть весьма *послушным* (или [«лояльным»](https://arxiv.org/abs/2003.11157)) своему пользователю — здесь «мы» означает «каждый из нас». Или он может быть более *суверенным*, руководствуясь в первую очередь своими собственными целями и ограничениями, но всё же действуя в целом в общих интересах человеческого благополучия — «мы» тогда означает «человечество» или «общество». Посередине находится спектр, где ИИ был бы в основном послушным, но мог бы отказаться от действий, которые вредят другим или обществу, нарушают закон и т.д.

Эти две оси — уровень автономии и тип выравнивания — не полностью независимы. Например, суверенная пассивная система, хотя и не вполне противоречива сама себе, является концепцией в напряжении, как и послушный автономный агент.[^10] Есть очевидный смысл, в котором автономия и суверенность стремятся идти рука об руку. В том же духе предсказуемость, как правило, выше в «пассивных» и «послушных» системах ИИ, тогда как суверенные или автономные будут стремиться быть более непредсказуемыми. Всё это будет критически важно для понимания последствий потенциального ИОИ и сверхинтеллекта.

Создание по-настоящему выровненного ИИ любого типа требует решения трёх отдельных задач:

1. Понимание того, чего «мы» хотим — что сложно независимо от того, означает ли «мы» конкретного человека или организацию (лояльность) или человечество в широком смысле (суверенность);
2. Создание систем, которые регулярно действуют в соответствии с этими желаниями — по сути, создание последовательного позитивного поведения;
3. Самое главное — создание систем, которые искренне «заботятся» об этих желаниях, а не просто ведут себя так, как будто заботятся.

Различие между надёжным поведением и искренней заботой критически важно. Так же как человек-сотрудник может идеально следовать приказам, не имея реальной приверженности миссии организации, система ИИ может действовать выровнено, не ценя по-настоящему человеческие предпочтения. Мы можем обучить системы ИИ говорить и делать вещи через обратную связь, и они могут научиться рассуждать о том, чего хотят люди. Но заставить их *искренне* ценить человеческие предпочтения — гораздо более глубокий вызов.[^11]

Глубокие трудности в решении этих задач выравнивания и их последствия для риска ИИ будут рассмотрены далее ниже. Пока что поймите, что выравнивание — это не просто техническая особенность, которую мы прикрепляем к системам ИИ, а фундаментальный аспект их архитектуры, который формирует их отношения с человечеством.


[^1]: Для мягкого, но технического введения в машинное обучение и ИИ, особенно языковые модели, см. [этот сайт.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) Для ещё одного современного введения в риски исчезновения от ИИ см. [эту статью.](https://www.thecompendium.ai/) Для исчерпывающего и авторитетного научного анализа состояния безопасности ИИ см. недавний [Международный отчёт по безопасности ИИ.](https://arxiv.org/abs/2501.17805)

[^2]: Обучение обычно происходит путём поиска локального максимума оценки в многомерном пространстве, заданном весами модели. Проверяя, как изменяется оценка при изменении весов, алгоритм обучения определяет, какие изменения больше всего улучшают оценку, и сдвигает веса в этом направлении.

[^3]: Например, в задаче распознавания изображений нейронная сеть выводила бы вероятности меток для изображения. Оценка была бы связана с вероятностью, которую ИИ приписывает правильному ответу. Процедура обучения затем настраивала бы веса так, чтобы в следующий раз ИИ выводил более высокую вероятность для правильной метки для этого изображения. Затем это повторяется огромное количество раз. Та же базовая процедура используется при обучении практически всех современных нейронных сетей, хотя и с более сложным механизмом оценки.

[^4]: Большинство мультимодальных моделей используют архитектуру «трансформер» для обработки и генерации множественных типов данных (текст, изображения, звук). Все они могут быть разложены и затем обработаны на равных как различные типы «токенов». Мультимодальные модели сначала обучаются точно предсказывать токены в массивных наборах данных, затем улучшаются через обучение с подкреплением для усиления способностей и формирования поведения.

[^5]: То, что языковые модели обучаются делать одно — предсказывать слова, — заставило некоторых называть их узким ИИ. Но это вводит в заблуждение: поскольку хорошее предсказание текста требует столь многих различных способностей, эта обучающая задача приводит к удивительно общей системе. Также отметим, что эти системы активно обучаются с помощью обучения с подкреплением, фактически представляя тысячи людей, дающих модели сигнал вознаграждения, когда она хорошо справляется с любой из многих вещей, которые она делает. Затем она наследует значительную универсальность от людей, дающих эту обратную связь.

[^6]: Существует несколько способов, в которых ИИ непредсказуем. Один в том, что в общем случае нельзя предсказать, что будет делать алгоритм, не запустив его фактически; есть [теоремы](https://arxiv.org/abs/1310.3225) по этому поводу. Это может быть верно просто потому, что результат алгоритмов может быть сложным. Но это особенно ясно и важно в случае (например, в шахматах или го), где предсказание подразумевало бы способность (победить ИИ), которой потенциальный предсказатель не имеет. Во-вторых, данная система ИИ не всегда будет производить одинаковый результат даже при одинаковом входе — её результаты содержат случайность; это также связано с алгоритмической непредсказуемостью. В-третьих, неожиданные и эмергентные способности могут возникнуть из обучения, что означает, что даже *типы* вещей, которые система ИИ может и будет делать, непредсказуемы; этот последний тип особенно важен для соображений безопасности.

[^7]: См. [здесь](https://arxiv.org/abs/2502.02649) подробный обзор того, что понимается под «автономным агентом» (вместе с этическими аргументами против их создания).

[^8]: Вы иногда можете услышать «ИИ не может иметь собственных целей». Это абсолютная чушь. Легко генерировать примеры, где ИИ имеет или развивает цели, которые ему никогда не давались и известны только ему самому. Вы не видите этого много в современных популярных мультимодальных моделях, потому что это обучается из них; это с такой же лёгкостью может быть обучено в них.

[^9]: Есть большая литература. По общей проблеме см. книгу Кристиана [*Проблема выравнивания*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821) и Рассела [*Совместимый с человеком*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616). С более технической стороны см., например, [эту статью](https://arxiv.org/abs/2209.00626).

[^10]: Мы позже увидим, что хотя такие системы идут против тенденции, это фактически делает их очень интересными и полезными.

[^11]: Это не означает, что мы требуем эмоций или разумности. Скорее, чрезвычайно трудно извне системы знать, каковы её внутренние цели, предпочтения и ценности. «Искренний» здесь означало бы, что у нас есть достаточно сильные основания полагаться на это, что в случае критических систем мы можем поставить на это свои жизни.