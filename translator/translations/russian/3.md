# Глава 3 - Ключевые аспекты создания современных систем общего ИИ

Большинство самых передовых ИИ-систем в мире создается с использованием удивительно схожих методов. Вот основы.

Чтобы по-настоящему понять человека, нужно знать что-то о биологии, эволюции, воспитании детей и многом другом; чтобы понять ИИ, также нужно знать о том, как он создается. За последние пять лет ИИ-системы колоссально эволюционировали как в плане возможностей, так и сложности. Ключевым фактором стала доступность очень больших объемов вычислений (или в разговорной речи "вычислительных мощностей" применительно к ИИ).

Цифры поражают. Около 10<sup>25</sup>-10<sup>26</sup> "операций с плавающей точкой" (FLOP)[^1] используется при обучении таких моделей, как серия GPT, Claude, Gemini и др.[^2] (Для сравнения: если бы каждый человек на Земле работал без остановки, выполняя одно вычисление каждые пять секунд, потребовалось бы около миллиарда лет, чтобы это осуществить.) Этот огромный объем вычислений позволяет обучать модели с триллионами весов модели на терабайтах данных — значительной части всего качественного текста, который когда-либо был написан, наряду с большими библиотеками звуков, изображений и видео. Дополняя это обучение обширной дополнительной тренировкой, укрепляющей человеческие предпочтения и хорошую производительность задач, модели, обученные таким образом, демонстрируют производительность, сопоставимую с человеческой, в значительном спектре базовых интеллектуальных задач, включая рассуждение и решение проблем.

Мы также знаем (очень, очень приблизительно), какая скорость вычислений в операциях в секунду достаточна для того, чтобы скорость *вывода*[^3] такой системы соответствовала *скорости* обработки текста человеком. Это примерно 10<sup>15</sup>-10<sup>16</sup> FLOP в секунду.[^4]

Хотя эти модели мощные, по своей природе они ограничены ключевыми способами, весьма аналогично тому, как был бы ограничен отдельный человек, если бы его заставили просто выдавать текст с фиксированной скоростью слов в минуту, не останавливаясь для размышлений или использования каких-либо дополнительных инструментов. Более современные ИИ-системы устраняют эти ограничения через более сложный процесс и архитектуру, объединяющую несколько ключевых элементов:

- Одна или несколько нейронных сетей, при этом одна модель обеспечивает основные когнитивные способности, а до нескольких других выполняют более узкие задачи;
- *Инструментарий*, предоставленный модели и используемый ею — например, способность искать в интернете, создавать или редактировать документы, выполнять программы и т.д.
- *Каркас*, который соединяет входы и выходы нейронных сетей. Очень простой каркас может просто позволить двум "экземплярам" ИИ-модели беседовать друг с другом, или одному проверять работу другого.[^5]
- *Цепочки рассуждений* и связанные техники подсказок делают нечто подобное, заставляя модель, например, генерировать множество подходов к проблеме, затем обрабатывать эти подходы для получения совокупного ответа.
- *Переобучение* моделей для лучшего использования инструментов, каркаса и цепочек рассуждений.

Поскольку эти расширения могут быть очень мощными (и включают сами ИИ-системы), эти композитные системы могут быть весьма сложными и кардинально улучшать возможности ИИ.[^6] И недавно техники каркасов и особенно подсказок с цепочками рассуждений (и встраивания результатов обратно в переобучение моделей для лучшего их использования) были разработаны и применены в [o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/) и [DeepSeek R1](https://api-docs.deepseek.com/news/news250120) для выполнения множества проходов вывода в ответ на данный запрос.[^7] Это фактически позволяет модели "думать" над своим ответом и кардинально повышает способность этих моделей выполнять высококачественные рассуждения в научных, математических и программистских задачах.[^8]

Для данной архитектуры ИИ увеличения в вычислениях для обучения [могут надежно переводиться](https://arxiv.org/abs/2405.10938) в улучшения в наборе четко определенных метрик. Для менее четко определенных общих способностей (таких как обсуждаемые ниже) перевод менее ясен и предсказуем, но почти наверняка более крупные модели с большими вычислениями для обучения будут иметь новые и лучшие способности, даже если трудно предсказать, какими они будут.

Аналогично, композитные системы и особенно достижения в "цепочках рассуждений" (и обучение моделей, которые хорошо с ними работают) открыли масштабирование в вычислениях *вывода*: для данной обученной основной модели по крайней мере некоторые способности ИИ-системы увеличиваются по мере применения большего количества вычислений, что позволяет им "думать усерднее и дольше" над сложными проблемами. Это происходит за счет значительных затрат скорости вычислений, требуя в сотни или тысячи раз больше FLOP/с для достижения человеческой производительности.[^9]

Хотя вычисления — это лишь часть того, что ведет к быстрому прогрессу ИИ,[^10] роль вычислений и возможность композитных систем окажутся решающими как для предотвращения неконтролируемого ИОИ, так и для разработки более безопасных альтернатив.

[^1]: 10<sup>27</sup> означает 1 с 25 нулями после нее, или десять триллионов триллионов. FLOP — это просто арифметическое сложение или умножение чисел с некоторой точностью. Отметим, что производительность аппаратного обеспечения ИИ может различаться в десять раз в зависимости от точности арифметики и архитектуры компьютера. Подсчет операций логических вентилей (AND, OR, NOT) был бы фундаментальным, но они не являются общедоступными или эталонными; для настоящих целей полезно стандартизировать 16-битные операции (FP16), хотя должны быть установлены соответствующие коэффициенты преобразования.

[^2]: Коллекция оценок и точных данных доступна от [Epoch AI](https://epochai.org/data/large-scale-ai-models) и указывает на около 2×10<sup>25</sup> 16-битных FLOP для GPT-4; это примерно соответствует [цифрам, которые просочились](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) для GPT-4. Оценки для других моделей середины 2024 года все находятся в пределах нескольких раз от GPT-4.

[^3]: Вывод — это просто процесс генерации выхода из нейронной сети. Обучение можно рассматривать как последовательность множества выводов и корректировок весов модели.

[^4]: Для производства текста оригинальный GPT-4 требовал 560 TFLOP на сгенерированный токен. Около 7 токенов/с необходимо для поспевания за человеческой мыслью, что дает ≈3×10<sup>15</sup> FLOP/с. Но эффективность снизила это; [эта брошюра NVIDIA](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/) например указывает всего 3×10<sup>14</sup> FLOP/с для модели Llama 405B сопоставимой производительности.

[^5]: В качестве чуть более сложного примера ИИ-система может сначала сгенерировать несколько возможных решений математической задачи, затем использовать другой экземпляр для проверки каждого решения, и наконец использовать третий для синтеза результатов в ясное объяснение. Это позволяет более тщательное и надежное решение проблем, чем за один проход.

[^6]: См. например детали об ["Operator" OpenAI](https://openai.com/index/introducing-operator/), [возможностях инструментов Claude](https://docs.anthropic.com/en/docs/build-with-claude/computer-use) и [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). [Deep Research](https://openai.com/index/introducing-deep-research/) OpenAI вероятно имеет довольно сложную архитектуру, но детали недоступны.

[^7]: Deepseek R1 опирается на итеративное обучение и подсказки модели так, что финальная обученная модель создает обширные цепочки рассуждений. Архитектурные детали недоступны для o1 или o3, однако Deepseek показал, что нет особого "секретного соуса", необходимого для разблокирования масштабирования возможностей с выводом. Но несмотря на получение большого внимания прессы как переворачивающий "статус-кво" в ИИ, это не влияет на основные утверждения этого эссе.

[^8]: Эти модели значительно превосходят стандартные модели в эталонных тестах рассуждений. Например, в GPQA Diamond Benchmark — строгом тесте научных вопросов уровня PhD — GPT-4o [набрал](https://openai.com/index/learning-to-reason-with-llms/) 56%, в то время как o1 и o3 достигли 78% и 88% соответственно, значительно превысив средний балл человеческих экспертов в 70%.

[^9]: O3 OpenAI вероятно затратил ∼10<sup>21</sup>-10<sup>22</sup> FLOP [для завершения каждого из вопросов челленджа ARC-AGI](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai), которые компетентные люди могут выполнить за (скажем) 10-100 секунд, давая цифру скорее около ∼10<sup>20</sup> FLOP/с.

[^10]: Хотя вычисления являются ключевым показателем возможностей ИИ-системы, они взаимодействуют как с качеством данных, так и с алгоритмическими улучшениями. Лучшие данные или алгоритмы могут снизить вычислительные требования, в то время как больше вычислений иногда может компенсировать слабые данные или алгоритмы.