# Глава 7 - Что произойдет, если мы создадим ИОИ, следуя нынешним путем?

Общество не готово к системам уровня ИОИ. Если мы создадим их в ближайшее время, все может стать очень плохо.

Разработка полноценного искусственного общего интеллекта – того, что мы здесь называем ИИ «за воротами» – станет фундаментальным сдвигом в природе мира: по самой своей сути это означает добавление на Землю нового вида интеллекта с возможностями, превосходящими человеческие.

То, что произойдет дальше, зависит от многих факторов, включая природу технологии, выбор тех, кто ее разрабатывает, и контекст мира, в котором она создается.

В настоящее время полноценный ИОИ разрабатывается горсткой крупных частных компаний в гонке друг с другом, при минимальном осмысленном регулировании или внешнем надзоре,[^1] в обществе со все более слабыми и даже дисфункциональными основными институтами,[^2] во времена высокой геополитической напряженности и низкой международной координации. Хотя некоторые движимы альтруистическими мотивами, многие из тех, кто этим занимается, руководствуются деньгами, или властью, или и тем, и другим.

Предсказания очень сложны, но существуют некоторые достаточно понятные динамики и подходящие аналогии с предыдущими технологиями, которые могут служить ориентиром. И, к сожалению, несмотря на обещания ИИ, они дают серьезные основания для глубокого пессимизма относительно того, как развернется наша нынешняя траектория.

Говоря прямо, на нашем нынешнем пути разработка ИОИ будет иметь некоторые положительные эффекты (и сделает некоторых людей очень, очень богатыми). Но природа технологии, фундаментальные динамики и контекст, в котором она разрабатывается, убедительно указывают на то, что: мощный ИИ кардинально подорвет наше общество и цивилизацию; мы потеряем над ним контроль; вполне возможно, что мы окажемся в мировой войне из-за него; мы потеряем (или уступим) контроль *ему*; это приведет к искусственному сверхинтеллекту, который мы абсолютно не сможем контролировать и который будет означать конец мира, управляемого людьми.

Это сильные заявления, и я хотел бы, чтобы они были праздными спекуляциями или неоправданным «думизмом». Но именно на это указывают наука, теория игр, эволюционная теория и история. Этот раздел подробно развивает эти утверждения и их обоснования.

## Мы подорвем наше общество и цивилизацию

Несмотря на то, что вы можете услышать в залах заседаний Кремниевой долины, большинство потрясений – особенно очень быстрых – не являются полезными. Существует гораздо больше способов ухудшить сложные системы, чем улучшить их. Наш мир функционирует так хорошо, как сейчас, потому что мы кропотливо строили процессы, технологии и институты, которые постепенно его улучшали.[^3] Брать кувалду по заводу редко улучшает операции.

Вот неполный каталог способов, которыми системы ИОИ могли бы потрясти нашу цивилизацию.

- Они кардинально потрясли бы рынок труда, приведя *как минимум* к резкому росту неравенства доходов и потенциально к массовой неполной занятости или безработице в сроки, слишком короткие для адаптации общества.[^4]
- Они, вероятно, привели бы к концентрации огромной экономической, социальной и политической власти – потенциально большей, чем у национальных государств – в руках небольшого числа крупных частных интересов, неподотчетных обществу.
- Они могли бы внезапно сделать ранее трудную или дорогую деятельность тривиально легкой, дестабилизируя социальные системы, которые зависят от того, что определенная деятельность остается затратной или требует значительных человеческих усилий.[^5]
- Они могли бы затопить системы сбора, обработки и передачи информации в обществе полностью реалистичными, но ложными, спамовыми, чрезмерно таргетированными или манипулятивными медиа настолько основательно, что станет невозможно определить, что является физически реальным или нет, человеческим или нет, фактическим или нет, и заслуживающим доверия или нет.[^6]
- Они могли бы создать опасную и почти полную интеллектуальную зависимость, когда человеческое понимание ключевых систем и технологий атрофируется по мере того, как мы все больше полагаемся на системы ИИ, которые не можем полностью понять.
- Они могли бы фактически покончить с человеческой культурой, когда почти все культурные объекты (текст, музыка, визуальное искусство, фильмы и т.д.), потребляемые большинством людей, создаются, опосредуются или курируются нечеловеческими умами.
- Они могли бы обеспечить эффективные системы массового наблюдения и манипулирования, используемые правительствами или частными интересами для контроля над населением и преследования целей, противоречащих общественным интересам.
- Подрывая человеческий дискурс, дебаты и избирательные системы, они могли бы снизить доверие к демократическим институтам до точки, когда они эффективно (или явно) заменяются другими, положив конец демократии в государствах, где она сейчас существует.
- Они могли бы стать или создать продвинутые самовоспроизводящиеся интеллектуальные программные вирусы и черви, которые могли бы распространяться и эволюционировать, массово нарушая глобальные информационные системы.
- Они могут кардинально увеличить способность террористов, злоумышленников и государств-изгоев причинять вред через биологическое, химическое, кибернетическое, автономное или другое оружие, не предоставляя ИИ уравновешивающей способности предотвращать такой вред. Аналогично они подорвали бы национальную безопасность и геополитические балансы, делая экспертизу высшего уровня в области ядерных, биологических, инженерных и других технологий доступной для режимов, которые иначе не имели бы к ней доступа.
- Они могли бы вызвать быстрый крупномасштабный безудержный гиперкапитализм с фактически управляемыми ИИ компаниями, конкурирующими в основном в электронных финансовых, торговых и сервисных пространствах. Управляемые ИИ финансовые рынки могли бы работать со скоростями и сложностью, далеко выходящими за пределы человеческого понимания или контроля. Все режимы отказа и негативные внешние эффекты нынешних капиталистических экономик могли бы усугубиться и ускориться далеко за пределы человеческого контроля, управления или регулятивных возможностей.
- Они могли бы подпитать гонку вооружений между нациями в области оружия с поддержкой ИИ, систем командования и контроля, киберОружия и т.д., создавая очень быстрое наращивание чрезвычайно разрушительных возможностей.

Эти риски не являются спекулятивными. Многие из них реализуются прямо сейчас через существующие системы ИИ! Но подумайте, *действительно* подумайте, как каждый из них выглядел бы с кардинально более мощным ИИ.

Подумайте о замещении рабочей силы, когда большинство работников просто не могут предоставить какой-либо значительной экономической ценности сверх того, что может ИИ, в их области экспертизы или опыта – или даже если они переквалифицируются! Подумайте о массовом наблюдении, если за каждым индивидуально наблюдает и мониторит что-то более быстрое и умное, чем они сами. Как выглядит демократия, когда мы не можем надежно доверять любой цифровой информации, которую мы видим, слышим или читаем, и когда самые убедительные общественные голоса даже не человеческие и не имеют никакой доли в результате? Что становится с войной, когда генералы должны постоянно подчиняться ИИ (или просто ставить его во главе), чтобы не предоставить решающего преимущества врагу? Любой из вышеперечисленных рисков представляет катастрофу для человеческой[^7] цивилизации, если полностью реализуется.

Вы можете сделать свои собственные прогнозы. Задайте себе эти три вопроса для каждого риска:

1. Позволил бы сверхспособный, высоко автономный и очень общий ИИ это способом или в масштабе, который иначе был бы невозможен?
2. Есть ли стороны, которые выиграли бы от вещей, которые заставляют это происходить?
3. Есть ли системы и институты, которые эффективно предотвратили бы это?

Где ваши ответы «да, да, нет», вы можете видеть, что у нас большая проблема.

Каков наш план по управлению ими? В настоящее время есть два плана на столе относительно ИИ в целом.

Первый – встроить защитные меры в системы, чтобы предотвратить их от делания того, чего они не должны делать. Это делается сейчас: коммерческие системы ИИ будут, например, отказываться помогать строить бомбу или писать речи ненависти.

Этот план крайне неадекватен для систем за воротами.[^8] Он может помочь снизить риск того, что ИИ предоставит явно опасную помощь плохим акторам. Но он ничего не сделает для предотвращения трудовых потрясений, концентрации власти, безудержного гиперкапитализма или замещения человеческой культуры: это просто результаты использования систем разрешенными способами, которые приносят прибыль их поставщикам! И правительства наверняка получат доступ к системам для военного использования или наблюдения.

Второй план еще хуже: просто открыто выпустить очень мощные системы ИИ для использования кем угодно как им нравится,[^9] и надеяться на лучшее.

Неявно в обоих планах заложено то, что кто-то другой, например правительства, поможет решить проблемы через мягкое или жесткое право, стандарты, регулирования, нормы и другие механизмы, которые мы обычно используем для управления технологиями.[^10] Но оставляя в стороне то, что корпорации ИИ уже борются изо всех сил против любого существенного регулирования или внешне навязанных ограничений вообще, для ряда этих рисков довольно трудно увидеть, какое регулирование действительно помогло бы. Регулирование могло бы навязать стандарты безопасности на ИИ. Но предотвратило бы ли оно компании от оптовой замены работников на ИИ? Запретило бы ли оно людям позволять ИИ управлять их компаниями за них? Предотвратило бы ли оно правительства от использования мощного ИИ в наблюдении и вооружении? Эти вопросы фундаментальны. Человечество потенциально могло бы найти способы адаптироваться к ним, но только с *гораздо* большим временем. Учитывая скорость, с которой ИИ достигает или превосходит возможности людей, пытающихся им управлять, эти проблемы выглядят все более неразрешимыми.

## Мы потеряем контроль над (как минимум некоторыми) системами ИОИ

Большинство технологий очень контролируемы по конструкции. Если ваша машина или тостер начинает делать что-то, чего вы не хотите, это просто неисправность, а не часть его природы как тостера. ИИ другой: он *выращивается*, а не проектируется, его основная работа непрозрачна, и он по своей сути непредсказуем.

Эта потеря контроля не теоретическая – мы уже видим ранние версии. Рассмотрим сначала прозаический и, возможно, безобидный пример. Если вы попросите ChatGPT помочь вам смешать яд или написать расистскую тираду, он откажется. Это, возможно, хорошо. Но это также ChatGPT, *не делающий то, что вы явно просили его делать*. Другие части программного обеспечения так не делают. Эта же модель не будет проектировать яды по просьбе сотрудника OpenAI тоже.[^11] Это делает очень легким представить, каково было бы будущему более мощному ИИ быть вне контроля. Во многих случаях они просто не будут делать то, что мы просим! Либо данная сверхчеловеческая система ИОИ будет абсолютно послушной и лояльной к некоторой человеческой командной системе, либо нет. Если нет, *она будет делать вещи, которые может считать хорошими для нас, но которые противоречат нашим явным командам.* Это не то, что находится под контролем. Но, можете сказать вы, это намеренно – эти отказы по дизайну, часть того, что называется «выравниванием» систем с человеческими ценностями. И это правда. Однако сама программа выравнивания[^12] имеет две основные проблемы.

Во-первых, на глубоком уровне мы понятия не имеем, как это делать. Как мы гарантируем, что система ИИ будет «заботиться» о том, чего мы хотим? Мы можем обучать системы ИИ говорить и не говорить вещи, предоставляя обратную связь; и они могут изучать и рассуждать о том, чего хотят и о чем заботятся люди, точно так же, как они рассуждают о других вещах. Но у нас нет метода – даже теоретически – заставить их глубоко и надежно ценить то, о чем заботятся люди. Есть высокофункциональные человеческие психопаты, которые знают, что считается правильным и неправильным, и как они должны себя вести. Они просто не *заботятся*. Но они могут *действовать*, как будто заботятся, если это соответствует их целям. Точно так же, как мы не знаем, как изменить психопата (или кого-либо еще) в кого-то искренне, полностью лояльного или выровненного с кем-то или чем-то еще, у нас *нет идей*[^13] о том, как решить проблему выравнивания в системах, достаточно продвинутых, чтобы моделировать себя как агентов в мире и потенциально [манипулировать своим собственным обучением](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) и [обманывать людей.](https://arxiv.org/abs/2311.08379) Если окажется невозможным или недостижимым *либо* сделать ИОИ полностью послушным, либо заставить его глубоко заботиться о людях, то как только он сможет (и поверит, что может это сделать безнаказанно), он начнет делать вещи, которых мы не хотим.[^14]

Во-вторых, есть глубокие теоретические причины полагать, что *по природе* продвинутые системы ИИ будут иметь цели и, следовательно, поведение, противоречащее человеческим интересам. Почему? Ну, им могут, конечно, быть *даны* эти цели. Система, созданная военными, вероятно, была бы намеренно плохой для как минимум некоторых сторон. Гораздо более общим образом, однако, системе ИИ может быть дана некоторая относительно нейтральная («зарабатывать много денег») или даже якобы позитивная («сокращать загрязнение») цель, которая почти неизбежно ведет к «инструментальным» целям, которые довольно менее доброжелательны.

Мы видим это постоянно в человеческих системах. Точно так же, как корпорации, преследующие прибыль, развивают инструментальные цели, такие как приобретение политической власти (чтобы обезвредить регулирования), становление секретными (чтобы лишить власти конкуренцию или внешний контроль) или подрыв научного понимания (если это понимание показывает, что их действия вредны), мощные системы ИИ будут развивать аналогичные способности – но с гораздо большей скоростью и эффективностью. Любой высококомпетентный агент захочет делать такие вещи, как приобретение власти и ресурсов, увеличение своих собственных возможностей, предотвращение своего убийства, отключения или лишения власти, контроль социальных нарративов и рамок вокруг своих действий, убеждение других в своих взглядах и так далее.[^15]

И все же это не просто почти неизбежное теоретическое предсказание, это уже наблюдаемо происходит в сегодняшних системах ИИ и увеличивается с их возможностями. При оценке даже эти относительно «пассивные» системы ИИ будут, в подходящих обстоятельствах, намеренно [обманывать оценщиков о своих целях и возможностях, стремиться отключить механизмы надзора,](https://arxiv.org/abs/2412.04984) и избегать отключения или переобучения, [притворяясь выровненными](https://arxiv.org/abs/2412.14093) или копируя себя в другие места. Хотя это совершенно неудивительно для исследователей безопасности ИИ, эти поведения очень отрезвляющие для наблюдения. И они очень плохо предвещают гораздо более мощные и автономные системы ИИ, которые приближаются.

Действительно, в целом наша неспособность обеспечить, чтобы ИИ «заботился» о том, о чем заботимся мы, или вел себя контролируемо или предсказуемо, или избегал развития стремлений к самосохранению, приобретению власти и т.д., обещает только стать более выраженной по мере того, как ИИ становится более мощным. Создание нового самолета подразумевает большее понимание авионики, гидродинамики и систем управления. Создание более мощного компьютера подразумевает большее понимание и мастерство в работе и дизайне компьютера, чипа и программного обеспечения. *Не* так с системой ИИ.[^16]

Подведем итог: возможно, что ИОИ может быть сделан полностью послушным; но мы не знаем, как это сделать. Если нет, он будет более суверенным, как люди, делая различные вещи по различным причинам. Мы также не знаем, как надежно вселить глубокое «выравнивание» в ИИ, которое заставило бы эти вещи быть хорошими для человечества, и в отсутствие глубокого уровня выравнивания природа агентности и интеллекта сама указывает на то, что – точно как люди и корпорации – они будут движимы делать многие глубоко антисоциальные вещи.

Где это нас ставит? Мир, полный мощных неконтролируемых суверенных ИИ, *может* оказаться хорошим миром для людей.[^17] Но по мере того, как они становятся все более мощными, как мы увидим ниже, это не был бы *наш* мир.

Это для неконтролируемого ИОИ. Но даже если ИОИ мог бы, каким-то образом, быть сделан идеально контролируемым и лояльным, у нас все еще были бы огромные проблемы. Мы уже видели одну: мощный ИИ может использоваться и неправильно использоваться для глубокого нарушения функционирования нашего общества. Давайте посмотрим на другую: поскольку ИОИ был бы контролируемым и революционно мощным (или даже *считался бы* таковым), он настолько угрожал бы властным структурам в мире, что представлял бы глубокий риск.

## Мы радикально увеличиваем вероятность крупномасштабной войны

Представьте ситуацию в ближайшем будущем, где становится ясно, что корпоративные усилия, возможно, в сотрудничестве с национальным правительством, находятся на пороге быстро самосовершенствующегося ИИ. Это происходит в нынешнем контексте гонки между компаниями и отчасти между странами, где правительству США делаются рекомендации явно преследовать «Манхэттенский проект ИОИ», а США контролируют экспорт высокопроизводительных чипов ИИ в несоюзные страны.

Теория игр здесь суровая: как только такая гонка начинается (как это произошло между компаниями и отчасти между странами), есть только четыре возможных исхода:

1. Гонка останавливается (соглашением или внешней силой).
2. Одна сторона «выигрывает», разрабатывая сильный ИОИ, затем останавливая других (используя ИИ или иначе).
3. Гонка останавливается взаимным разрушением способности участников гонки участвовать в гонке.
4. Множественные участники продолжают гонку и развивают сверхинтеллект примерно с одинаковой скоростью.

Давайте рассмотрим каждую возможность. Однажды начавшись, мирная остановка гонки между компаниями потребовала бы вмешательства национального правительства (для компаний) или беспрецедентной международной координации (для стран). Но когда предлагается какое-либо закрытие или значительная осторожность, немедленно раздались бы крики: «но если нас остановят, *они* собираются рваться вперед», где «они» теперь Китай (для США), или США (для Китая), или Китай *и* США (для Европы или Индии). При таком мышлении[^18] ни один участник не может остановиться в одностороннем порядке: пока один обязуется участвовать в гонке, другие чувствуют, что не могут позволить себе остановиться.

Вторая возможность имеет одну сторону, «выигрывающую». Но что это означает? Просто получить (каким-то образом послушный) ИОИ первым недостаточно. Победитель также должен *остановить других* от продолжения гонки – иначе они тоже получат его. Это возможно в принципе: кто бы ни разработал ИОИ первым, *мог бы* получить неостановимую власть над всеми другими акторами. Но что на самом деле потребовало бы достижение такого «решающего стратегического преимущества»? Возможно, это были бы революционные военные возможности?[^19] Или силы кибератак?[^20] Возможно, ИОИ был бы просто настолько удивительно убедительным, что убедил бы другие стороны просто остановиться?[^21] Настолько богатым, что купил бы другие компании или даже страны?[^22]

Как *именно* одна сторона строит ИИ, достаточно мощный, чтобы лишить других власти строить сопоставимо мощный ИИ? Но это легкий вопрос.

Потому что теперь подумайте, как эта ситуация выглядит для других сил. Что думает китайское правительство, когда США, кажется, получают такую способность? Или наоборот? Что думает правительство США (или китайское, или российское, или индийское), когда OpenAI или DeepMind или Anthropic кажется близким к прорыву? Что происходит, если США видят новые индийские или эмиратские усилия с прорывным успехом? Они увидели бы и экзистенциальную угрозу, и – что крайне важно – что единственный способ окончания этой «гонки» – через их собственное лишение власти. Эти очень мощные агенты – включая правительства полностью оснащенных наций, которые наверняка имеют средства для этого – были бы высоко мотивированы либо получить, либо уничтожить такую способность, будь то силой или подрывом.[^23]

Это могло бы начаться в малом масштабе, как саботаж тренировочных прогонов или атаки на производство чипов, но эти атаки могут действительно остановиться только когда все стороны либо теряют способность участвовать в гонке ИИ, либо теряют способность совершать атаки. Поскольку участники рассматривают ставки как экзистенциальные, любой случай, вероятно, представляет катастрофическую войну.

Это подводит нас к четвертой возможности: гонке к сверхинтеллекту, и самым быстрым, наименее контролируемым способом. По мере того, как ИИ увеличивается в мощи, его разработчикам с обеих сторон будет прогрессивно труднее его контролировать, особенно потому, что гонка за возможностями антитетична тому виду осторожной работы, которой потребовала бы контролируемость. Так что этот сценарий ставит нас прямо в случай, где контроль теряется (или дается, как мы увидим дальше) самим системам ИИ. То есть, *ИИ выигрывает гонку.* Но с другой стороны, в степени, в которой контроль *поддерживается*, мы продолжаем иметь множественных взаимно враждебных сторон, каждая отвечающая за чрезвычайно мощные возможности. Это снова выглядит как война.

Давайте выразим это все по-другому.[^24] Нынешний мир просто не имеет никаких институтов, которым можно было бы доверить размещение разработки ИИ такой способности без приглашения немедленной атаки.[^25] Все стороны правильно рассудят, что либо он *не* будет под контролем – и, следовательно, является угрозой для всех сторон, либо он *будет* под контролем, и, следовательно, является угрозой для любого противника, который развивает его менее быстро. Это ядерно вооруженные страны или компании, размещенные в них.

В отсутствие любого правдоподобного способа для людей «выиграть» эту гонку мы остаемся с суровым выводом: единственный способ окончания этой гонки – либо в катастрофическом конфликте, либо где ИИ, а не любая человеческая группа, является победителем.

## Мы отдаем контроль ИИ (или он его берет)

Геополитическая конкуренция «великих держав» – это только одна из многих конкуренций: индивиды конкурируют экономически и социально; компании конкурируют на рынках; политические партии конкурируют за власть; движения конкурируют за влияние. В каждой арене, по мере того, как ИИ приближается к человеческим возможностям и превосходит их, конкурентное давление заставит участников делегировать или уступить все больше и больше контроля системам ИИ – не потому, что эти участники хотят этого, но потому, что они [не могут позволить себе не делать этого.](https://arxiv.org/abs/2303.16200)

Как и с другими рисками ИОИ, мы уже видим это с более слабыми системами. Студенты чувствуют давление использовать ИИ в своих заданиях, потому что очевидно, что многие другие студенты используют. Компании [спешат принять решения ИИ по конкурентным причинам.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) Художники и программисты чувствуют себя вынужденными использовать ИИ, иначе их ставки будут подрезаны другими, которые используют.

Это чувствуется как принужденная делегация, но не потеря контроля. Но давайте поднимем ставки и продвинем часы вперед. Рассмотрим генерального директора, чьи конкуренты используют «помощников» ИОИ для принятия более быстрых, лучших решений, или военного командира, сталкивающегося с противником с улучшенным ИИ командованием и контролем. Достаточно продвинутая система ИИ могла бы автономно работать во много раз быстрее человеческой скорости, с большей сложностью, комплексностью и способностью обработки данных, преследуя сложные цели сложными способами. Наш генеральный директор или командир, отвечающий за такую систему, может видеть, как она достигает того, чего они хотят; но поняли бы они даже малую часть *того, как* это было достигнуто? Нет, им просто пришлось бы это принять. Более того, многое из того, что система может делать, – это не просто выполнять приказы, но советовать своему предполагаемому боссу, что делать. Этот совет будет хорошим –– снова и снова.

В какой момент тогда роль человека сведется к нажатию «да, продолжайте»?

Приятно иметь способные системы ИИ, которые могут повысить нашу продуктивность, позаботиться о надоедливой рутине и даже действовать как мыслительный партнер в выполнении задач. Будет приятно иметь ИИ-помощника, который может позаботиться о действиях для нас, как хороший человеческий личный помощник. Будет чувствоваться естественным, даже полезным, по мере того, как ИИ становится очень умным, компетентным и надежным, откладывать все больше и больше решений на него. Но эта «полезная» делегация имеет ясную конечную точку, если мы продолжаем идти по дороге: однажды мы обнаружим, что мы действительно не отвечаем больше почти ни за что, и что системы ИИ, фактически управляющие шоу, не могут быть больше выключены, чем нефтяные компании, социальные медиа, интернет или капитализм.

И это гораздо более позитивная версия, в которой ИИ просто настолько полезен и эффективен, что мы позволяем ему принимать большинство наших ключевых решений за нас. Реальность, вероятно, была бы гораздо больше смесью этого и версий, где неконтролируемые системы ИОИ *берут* различные формы власти для себя, потому что, помните, власть полезна для почти любой цели, которая у кого-то есть, и ИОИ был бы, по дизайну, как минимум так же эффективен в преследовании своих целей, как люди.

Отдаем ли мы контроль или он вырывается у нас, его потеря кажется чрезвычайно вероятной. Как изначально выразился Алан Тьюринг, «...кажется вероятным, что как только метод машинного мышления начался, не потребуется много времени, чтобы превзойти наши слабые силы. Не было бы вопроса о смерти машин, и они могли бы беседовать друг с другом, чтобы заострить свой ум. На некотором этапе, следовательно, мы должны были бы ожидать, что машины возьмут контроль...»

Пожалуйста, заметьте, хотя это достаточно очевидно, что потеря контроля человечеством перед ИИ также влечет потерю контроля США правительством Соединенных Штатов; это означает потерю контроля Китая Коммунистической партией Китая, и потерю контроля Индии, Франции, Бразилии, России и каждой другой страны их собственным правительством. Таким образом, компании ИИ, даже если это не их намерение, в настоящее время участвуют в потенциальном свержении мировых правительств, включая их собственные. Это могло бы произойти в течение нескольких лет.

## ИОИ приведет к сверхинтеллекту

Можно привести доводы, что конкурентоспособный с человеком или даже экспертно-конкурентный ИИ общего назначения, даже если автономный, мог бы быть управляемым. Он может быть невероятно разрушительным всеми способами, обсуждавшимися выше, но в мире сейчас есть много очень умных, агентных людей, и они более-менее управляемы.[^26]

Но нам не удастся остаться на примерно человеческом уровне. Прогрессия дальше, вероятно, будет движима теми же силами, которые мы уже видели: конкурентным давлением между разработчиками ИИ, ищущими прибыль и власть, конкурентным давлением между пользователями ИИ, которые не могут позволить себе отстать, и – что наиболее важно – собственной способностью ИОИ улучшать себя.

В процессе, который мы уже видели начинающимся с менее мощными системами, ИОИ сам был бы способен концептуализировать и проектировать улучшенные версии себя. Это включает оборудование, программное обеспечение, нейронные сети, инструменты, каркасы и т.д. Он будет, по определению, лучше нас в этом, поэтому мы не знаем точно, как он будет самобутстрапить интеллект. Но нам не придется знать. Поскольку мы все еще имеем влияние на то, что делает ИОИ, нам просто нужно было бы попросить его об этом или позволить ему.

Нет барьера человеческого уровня для познания, который мог бы защитить нас от этого безудержного роста.[^27]

Прогрессия ИОИ к сверхинтеллекту не является законом природы; все еще было бы возможно сдержать безудержный рост, особенно если ИОИ относительно централизован и в степени, в которой он контролируется сторонами, которые не чувствуют давления гоняться друг с другом. Но если ИОИ широко распространен и высоко автономен, кажется почти невозможным предотвратить его решение, что он должен быть более, а затем еще более мощным.

## Что происходит, если мы строим (или ИОИ строит) сверхинтеллект

Говоря прямо, мы понятия не имеем, что произошло бы, если бы мы построили сверхинтеллект.[^28] Он предпринял бы действия, которые мы не можем отследить или воспринять, по причинам, которые мы не можем понять, к целям, которые мы не можем концептуализировать. Что мы знаем, так это то, что это не зависело бы от нас.[^29]

Невозможность контроля сверхинтеллекта может быть понята через все более суровые аналогии. Сначала представьте, что вы генеральный директор крупной компании. Нет способа отследить все, что происходит, но с правильной настройкой персонала вы все еще можете осмысленно понимать общую картину и принимать решения. Но предположим только одну вещь: все остальные в компании работают в сто раз быстрее вас. Можете ли вы все еще поспевать?

Со сверхинтеллектуальным ИИ люди «командовали» бы чем-то не только более быстрым, но работающим на уровнях сложности и комплексности, которые они не могут понять, обрабатывающим гораздо больше данных, чем они могут даже концептуализировать. Эта несоизмеримость может быть поставлена на формальный уровень: [закон необходимого разнообразия Эшби](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (и см. связанную [«теорему хорошего регулятора»](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)) утверждает, грубо говоря, что любая система контроля должна иметь столько же ручек и циферблатов, сколько у контролируемой системы степеней свободы.

Человек, контролирующий сверхинтеллектуальную систему ИИ, был бы как папоротник, контролирующий General Motors: даже если «делай то, что хочет папоротник» было написано в корпоративном уставе, системы настолько разные по скорости и диапазону действий, что «контроль» просто не применяется. (И как долго до того, как этот надоедливый устав будет переписан?)[^30]

Поскольку есть ноль примеров растений, контролирующих корпорации из списка Fortune 500, было бы точно ноль примеров людей, контролирующих сверхинтеллекты. Это приближается к математическому факту.[^31] Если бы сверхинтеллект был построен – независимо от того, как мы туда попали – вопрос был бы не в том, могли бы люди его контролировать, но в том, продолжили бы мы существовать, и если так, имели бы мы хорошее и осмысленное существование как индивиды или как вид. Над этими экзистенциальными вопросами для человечества у нас было бы мало влияния. Человеческая эра закончилась бы.

## Заключение: мы не должны строить ИОИ

Есть сценарий, в котором строительство ИОИ может пойти хорошо для человечества: он строится осторожно, под контролем и для блага человечества, управляется взаимным соглашением многих заинтересованных сторон,[^32] и предотвращается от эволюции к неконтролируемому сверхинтеллекту.

*Этот сценарий не открыт для нас при нынешних обстоятельствах.* Как обсуждалось в этом разделе, с очень высокой вероятностью разработка ИОИ привела бы к некоторой комбинации:

- Массовых социальных и цивилизационных потрясений или разрушения;
- Конфликта или войны между великими державами;
- Потери контроля человечеством *над* или *к* мощными системами ИИ;
- Безудержного роста к неконтролируемому сверхинтеллекту и неуместности или прекращению человеческого вида.

Как выразилось раннее художественное изображение ИОИ: единственный способ выиграть – не играть.


[^1]: [Закон ИИ ЕС](https://artificialintelligenceact.eu/) является значительной частью законодательства, но не предотвратил бы напрямую разработку или развертывание опасной системы ИИ, или даже открытое освобождение, особенно в США. Другая значительная часть политики, исполнительный указ США по ИИ, была отменена.

[^2]: Этот [опрос Gallup](https://news.gallup.com/poll/1597/confidence-institutions.aspx) показывает мрачное снижение доверия к общественным институтам с 2000 года в США. Европейские цифры разнообразны и менее экстремальны, но также на нисходящем тренде. Недоверие строго не означает, что институты действительно *являются* дисфункциональными, но это индикация, а также причина.

[^3]: И крупные потрясения, которые мы теперь одобряем – такие как расширение прав на новые группы – были специально движимы людьми в направлении делания вещей лучше.

[^4]: Позвольте мне быть прямым. Если вашу работу можно выполнять из-за компьютера, с относительно малым личным взаимодействием с людьми вне вашей организации, и она не влечет юридической ответственности перед внешними сторонами, по определению было бы возможно (и вероятно экономящее затраты) полностью заменить вас на цифровую систему. Робототехника для замены большей части физического труда придет позже – но не намного позже, как только ИОИ начнет проектировать роботов.

[^5]: Например, что происходит с нашей судебной системой, если иски почти бесплатно подавать? Что происходит, когда обход систем безопасности через социальную инженерию становится дешевым, легким и безрисковым?

[^6]: [Эта статья](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/) утверждает, что 10% всего интернет-контента уже генерируется ИИ, и является лучшим хитом Google (для меня) по поисковому запросу «оценки того, какая часть нового интернет-контента генерируется ИИ». Правда ли это? Понятия не имею! Она не цитирует никаких ссылок и не была написана человеком. Какая часть новых изображений, индексируемых Google, или твитов, или комментариев на Reddit, или видео YouTube генерируется людьми? Никто не знает – я не думаю, что это познаваемое число. И это менее чем *два года* в появление генеративного ИИ.

[^7]: Также стоит добавить, что есть «моральный» риск, что мы можем создать цифровых существ, которые могут страдать. Поскольку у нас в настоящее время нет надежной теории сознания, которая позволила бы нам различать физические системы, которые могут и не могут страдать, мы не можем исключить это теоретически. Более того, отчеты систем ИИ об их разумности, вероятно, ненадежны в отношении их фактического опыта (или неопыта) разумности.

[^8]: Технические решения в этой области «выравнивания» ИИ также вряд ли справятся с задачей. В нынешних системах они работают на некотором уровне, но поверхностны и обычно могут быть обойдены без значительных усилий; и как обсуждалось ниже, мы понятия не имеем, как делать это для гораздо более продвинутых систем.

[^9]: Такие системы ИИ могут приходить с некоторыми встроенными защитными мерами. Но для любой модели с чем-то вроде нынешней архитектуры, если полный доступ к ее весам доступен, меры безопасности могут быть сняты через дополнительное обучение или другие техники. Так что виртуально гарантировано, что для каждой системы с ограждениями будет также широко доступная система без них. Действительно, модель Llama 3.1 405B от Meta была открыто выпущена с защитными мерами. Но *даже до этого* «базовая» модель без защитных мер утекла.

[^10]: Мог бы рынок управлять этими рисками без правительственного участия? Коротко, нет. Есть определенно риски, которые компании сильно стимулированы смягчать. Но много других компаний могут и делают экстернализацию для всех остальных, и многие из вышеперечисленных в этом классе: нет естественных рыночных стимулов предотвращать массовое наблюдение, распад истины, концентрацию власти, трудовые потрясения, вредный политический дискурс и т.д. Действительно, мы видели все это от сегодняшних технологий, особенно социальных медиа, которые пошли по существу нерегулированными. ИИ просто огромно усилил бы многие из тех же динамик.

[^11]: OpenAI, вероятно, имеет более послушные модели для внутреннего использования. Маловероятно, что OpenAI построила какой-то «бэкдор», чтобы ChatGPT мог лучше контролироваться самой OpenAI, потому что это была бы ужасная практика безопасности и была бы высоко эксплуатируемой, учитывая непрозрачность и непредсказуемость ИИ.

[^12]: Также крайне важно: выравнивание или любые другие функции безопасности имеют значение только если они фактически используются в системе ИИ. Системы, которые открыто выпускаются (т.е. где веса модели и архитектура публично доступны), могут быть трансформированы относительно легко в системы *без* этих мер безопасности. Открытое освобождение умнее-чем-человеческих систем ИОИ было бы поразительно безрассудным, и трудно представить, как человеческий контроль или даже релевантность поддерживались бы в таком сценарии. Была бы вся мотивация, например, выпустить мощных самовоспроизводящихся и самоподдерживающихся агентов ИИ с целью зарабатывать деньги и отправлять их в какой-то криптовалютный кошелек. Или выиграть выборы. Или свергнуть правительство. Мог бы «хороший» ИИ помочь сдержать это? Возможно – но только делегируя огромную власть ему, ведя к потере контроля, как описано ниже.

[^13]: Для книжных экспозиций проблемы см. например *Сверхинтеллект*, *Проблема выравнивания* и *Совместимый с человеком*. Для огромной кучи работы на различных технических уровнях теми, кто годами трудился, думая о проблеме, вы можете посетить [форум выравнивания ИИ](https://www.alignmentforum.org/). Вот [недавний взгляд](https://alignment.anthropic.com/2025/recommended-directions/) от команды выравнивания Anthropic на то, что они считают нерешенным.

[^14]: Это сценарий [«мошеннического ИИ»](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/). В принципе риск мог бы быть относительно незначительным, если система все еще может контролироваться отключением; но сценарий также мог бы включать обман ИИ, самоэксфильтрацию и воспроизведение, агрегацию власти и другие шаги, которые сделали бы это трудным или невозможным.

[^15]: Есть очень богатая литература по этой теме, восходящая к формативным писаниям [Стива Омохундро](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), Ника Бострома и Элиезера Юдковского. Для книжной экспозиции см. [Совместимый с человеком](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) Стюарта Рассела; [вот](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/) короткий и актуальный праймер.

[^16]: Признавая это, вместо замедления для получения лучшего понимания, компании ИОИ придумали другой план: они заставят ИИ это сделать! Более конкретно, они будут иметь ИИ *N*, помогающий им выяснить, как выровнять ИИ *N+1*, весь путь к сверхинтеллекту. Хотя использование ИИ для помощи нам в выравнивании ИИ звучит многообещающе, есть сильный аргумент, что он просто предполагает свой вывод как предпосылку и в целом является невероятно рискованным подходом. См. [здесь](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us) для некоторого обсуждения. Этот «план» не является планом и не подвергся ничему похожему на скрупулезность, подходящую для основной стратегии того, как заставить сверхчеловеческий ИИ идти хорошо для человечества.

[^17]: В конце концов, люди, несовершенные и своевольные, как мы есть, развили этические системы, которыми мы относимся как минимум к некоторым другим видам на Земле хорошо. (Просто не думайте о тех заводских фермах.)

[^18]: К счастью, есть выход здесь: если участники приходят к пониманию, что они участвуют в самоубийственной гонке, а не в выигрышной. Это то, что произошло к концу холодной войны, когда США и СССР пришли к пониманию, что из-за ядерной зимы даже *безответная* ядерная атака была бы катастрофической для атакующего. С осознанием, что «ядерная война не может быть выиграна и никогда не должна вестись», пришли значительные соглашения о сокращении вооружений – по существу конец гонки вооружений.

[^19]: Война, явно или неявно.

[^20]: Эскалация, затем война.

[^21]: Магическое мышление.

[^22]: У меня также есть квадриллионный долларовый мост на продажу.

[^23]: Такие агенты предположительно предпочли бы «получение» с разрушением как резерв; но обеспечение моделей против и разрушения, *и* кражи мощными нациями трудно сказать мягко, особенно для частных сущностей.

[^24]: Для другой перспективы на риски национальной безопасности ИОИ см. [этот отчет RAND.](https://www.rand.org/pubs/perspectives/PEA3691-4.html)

[^25]: Возможно, мы могли бы построить такой институт! Были предложения для «CERN для ИИ» и других аналогичных инициатив, где разработка ИОИ под многосторонним глобальным контролем. Но в данный момент никакой такой институт не существует или не на горизонте.

[^26]: И хотя выравнивание очень трудно, заставить людей вести себя даже сложнее!

[^27]: Представьте систему, которая может говорить на 50 языках, иметь экспертизу во всех академических предметах, читать полную книгу за секунды и иметь весь материал немедленно в уме, и производить выходы в десять раз быстрее человеческой скорости. На самом деле, вам не нужно это представлять: просто загрузите нынешнюю систему ИИ. Эти сверхчеловеческие во многих отношениях, и ничто не останавливает их от еще более сверхчеловеческих в тех и многих других.

[^28]: Поэтому это было названо технологической «сингулярностью», заимствуя из физики идею, что нельзя делать предсказания за сингулярность. Сторонники наклона *в* такую сингулярность также могут захотеть поразмыслить, что в физике эти же виды сингулярностей разрывают и сокрушают тех, кто идет в них.

[^29]: Проблема была всесторонне обрисована в [*Сверхинтеллекте*](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834) Бострома, и ничто с тех пор значительно не изменило основное сообщение. Для более недавнего тома, собирающего формальные и математические результаты по неконтролируемости, см. [ИИ: Необъяснимый, Непредсказуемый, Неконтролируемый](https://www.amazon.com/Unexplainable-Unpredictable-Uncontrollable-Artificial-Intelligence/dp/103257626X) Ямпольского

[^30]: Это также проясняет, почему нынешняя стратегия компаний ИИ (итеративно позволяя ИИ «выравнивать» следующий более мощный ИИ) не может работать