# Глава 8 - Как не создавать ИОИ

ИОИ не неизбежен — сегодня мы стоим на развилке дорог. В этой главе представлено предложение о том, как можно предотвратить его создание.

Если дорога, по которой мы сейчас идем, ведет к вероятному концу нашей цивилизации, как нам сменить направление?

Предположим, что желание прекратить разработку ИОИ и сверхинтеллекта стало широко распространенным и влиятельным,[^1] поскольку всеобщим пониманием стало то, что ИОИ будет поглощать власть, а не предоставлять её, и представляет серьезную угрозу для общества и человечества. Как мы закроем Врата?

В настоящее время мы знаем только один способ *создания* мощного и общего ИИ — через поистине массивные вычисления глубоких нейронных сетей. Поскольку это невероятно сложные и дорогие процессы, в каком-то смысле *не* заниматься этим легко.[^2] Но мы уже видели силы, которые движут к ИОИ, и игровую динамику, которая делает очень трудным для любой стороны односторонне остановиться. Поэтому потребуется сочетание вмешательства извне (то есть правительств) для остановки корпораций и соглашений между правительствами для остановки самих себя.[^3] Как это могло бы выглядеть?

Полезно сначала различать разработки ИИ, которые должны быть *предотвращены* или *запрещены*, и те, которыми нужно *управлять*. К первым относилось бы прежде всего неконтролируемое развитие до сверхинтеллекта.[^4] Для запрещенной разработки определения должны быть максимально четкими, а как верификация, так и принуждение должны быть практичными. То, чем нужно *управлять*, — это общие, мощные системы ИИ, которые у нас уже есть и которые будут иметь много серых зон, нюансов и сложностей. Для них крайне важны сильные эффективные институты.

Мы также можем с пользой разграничить вопросы, которые должны решаться на международном уровне (включая между геополитическими соперниками или противниками)[^5], от тех, которыми могут управлять отдельные юрисдикции, страны или группы стран. Запрещенная разработка в основном попадает в категорию «международных», поскольку местный запрет на разработку технологии обычно можно обойти, изменив местоположение.[^6]

Наконец, мы можем рассмотреть инструменты в арсенале. Их много, включая технические инструменты, мягкое право (стандарты, нормы и т.д.), жесткое право (регулирования и требования), ответственность, рыночные стимулы и так далее. Уделим особое внимание одному инструменту, специфичному для ИИ.

## Безопасность и управление вычислительными мощностями

Основным инструментом управления высокопроизводительным ИИ будет оборудование, которое он требует. Программное обеспечение легко распространяется, имеет близкие к нулю предельные издержки производства, легко пересекает границы и может быть мгновенно изменено; ничего из этого не верно для оборудования. Тем не менее, как мы обсуждали, огромные объемы этих «вычислительных мощностей» необходимы как во время обучения систем ИИ, так и во время вывода для достижения наиболее способных систем. Вычислительные мощности можно легко количественно оценить, учесть и проверить, с относительно небольшой двусмысленностью, как только будут разработаны хорошие правила для этого. Самое главное, большие объемы вычислений, как и обогащенный уран, являются очень дефицитным, дорогим и трудно производимым ресурсом. Хотя компьютерные чипы повсеместны, оборудование, необходимое для ИИ, дорого и чрезвычайно трудно в производстве.[^7]

Что делает специализированные для ИИ чипы *гораздо* более управляемыми как дефицитный ресурс по сравнению с ураном, — это то, что они могут включать аппаратные механизмы безопасности. Большинство современных мобильных телефонов и некоторые ноутбуки имеют специализированные аппаратные функции на чипе, которые позволяют им гарантировать установку только одобренного программного обеспечения операционной системы и обновлений, сохранять и защищать конфиденциальные биометрические данные на устройстве, и делать их бесполезными для всех, кроме их владельца, в случае потери или кражи. За последние несколько лет такие меры аппаратной безопасности стали хорошо зарекомендовавшими себя и широко принятыми, и в целом показали себя достаточно безопасными.

Ключевая новизна этих функций заключается в том, что они связывают оборудование и программное обеспечение вместе с помощью криптографии.[^8] То есть, простое обладание конкретным элементом компьютерного оборудования не означает, что пользователь может делать с ним все, что хочет, применяя различное программное обеспечение. И эта связь также обеспечивает мощную безопасность, поскольку многие атаки потребовали бы взлома *аппаратной*, а не только *программной* безопасности.

Несколько недавних докладов (например, от [GovAI и коллабораторов](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips) и [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) отмечали, что аналогичные аппаратные функции, встроенные в передовое компьютерное оборудование, связанное с ИИ, могут играть чрезвычайно полезную роль в безопасности и управлении ИИ. Они позволяют «регулятору»[^9] использовать ряд функций, доступность или даже возможность которых можно было бы не предполагать. В качестве ключевых примеров:

- *Геолокация*: Системы можно настроить так, чтобы чипы имели известное местоположение и могли действовать по-разному (или быть полностью отключенными) в зависимости от местоположения.[^10]
- *Разрешенные подключения*: каждый чип может быть настроен с аппаратно-принудительным белым списком конкретных других чипов, с которыми он может работать в сети, и не может подключаться к любым чипам, не включенным в этот список.[^11] Это может ограничить размер коммуникативных кластеров чипов.[^12]
- *Дозированный вывод или обучение (и автоотключение)*: Регулятор может лицензировать только определенный объем обучения или вывода (по времени, или FLOP, или возможно токенам) для выполнения пользователем, после чего требуется новое разрешение. Если приращения малы, то требуется относительно непрерывное перелицензирование модели. Модель затем можно «отключить» просто удерживая этот лицензионный сигнал.[^13]
- *Ограничение скорости*: Модель предотвращается от работы со скоростью вывода выше некоторого предела, который определяется регулятором или иначе. Это можно реализовать через ограниченный набор разрешенных подключений или более сложными средствами.
- *Засвидетельствованное обучение*: Процедура обучения может дать криптографически безопасное доказательство того, что конкретный набор кодов, данных и объем использования вычислительных мощностей были использованы при генерации модели.

## Как не создавать сверхинтеллект: глобальные ограничения на вычислительные мощности для обучения и вывода

С учетом этих соображений — особенно касающихся вычислений — мы можем обсудить, как закрыть Врата к искусственному сверхинтеллекту; затем мы обратимся к предотвращению полного ИОИ и управлению моделями ИИ по мере того, как они приближаются и превышают человеческие способности в различных аспектах.

Первый компонент — это, конечно, понимание того, что сверхинтеллект не будет контролируемым, и что его последствия принципиально непредсказуемы. По крайней мере Китай и США должны независимо решить, для этой или других целей, не создавать сверхинтеллект.[^14] Затем необходимо международное соглашение между ними и другими, с сильным механизмом верификации и принуждения, чтобы заверить все стороны, что их соперники не отступают и не решают рискнуть.

Чтобы быть проверяемыми и осуществимыми, ограничения должны быть жесткими ограничениями и максимально однозначными. Это кажется практически невозможной проблемой: ограничение возможностей сложного программного обеспечения с непредсказуемыми свойствами по всему миру. К счастью, ситуация гораздо лучше, потому что именно то, что сделало возможным передовой ИИ — огромный объем вычислений — гораздо, гораздо легче контролировать. Хотя это все еще может позволить некоторые мощные и опасные системы, *неконтролируемый сверхинтеллект* может быть предотвращен жестким ограничением на объем вычислений, идущих в нейронную сеть, вместе с ограничением скорости на объем вывода, который система ИИ (из связанных нейронных сетей и другого программного обеспечения) может выполнять. Конкретная версия этого предложена ниже.

Может показаться, что установление жестких глобальных ограничений на вычисления ИИ потребует огромных уровней международной координации и навязчивого, разрушающего приватность наблюдения. К счастью, это не так. Чрезвычайно [узкая и имеющая узкие места цепочка поставок](https://arxiv.org/abs/2402.08797) обеспечивает то, что как только ограничение установлено юридически (будь то законом или исполнительным указом), верификация соблюдения этого ограничения потребует только участия и сотрудничества горстки крупных компаний.[^15]

План такого рода имеет ряд крайне желательных характеристик. Он минимально инвазивен в том смысле, что требования предъявляются только к нескольким крупным компаниям, и управляются только достаточно значительные кластеры вычислений. Соответствующие чипы уже содержат аппаратные возможности, необходимые для первой версии.[^16] Как реализация, так и принуждение полагаются на стандартные правовые ограничения. Но они подкрепляются условиями использования оборудования и аппаратными элементами управления, значительно упрощая принуждение и предотвращая обман со стороны компаний, частных групп или даже стран. Существует достаточный прецедент для компаний-производителей оборудования, устанавливающих удаленные ограничения на использование их оборудования и блокирующих/разблокирующих конкретные возможности извне,[^17] включая даже высокопроизводительные процессоры в центрах обработки данных.[^18] Даже для довольно небольшой доли оборудования и организаций, затронутых этим, надзор может быть ограничен телеметрией, без прямого доступа к данным или самим моделям; и программное обеспечение для этого может быть открыто для проверки, чтобы показать, что никаких дополнительных данных не записывается. Схема является международной и кооперативной, и довольно гибкой и расширяемой. Поскольку ограничение в основном касается оборудования, а не программного обеспечения, оно относительно агностично к тому, как происходит разработка и развертывание программного обеспечения ИИ, и совместимо с разнообразием парадигм, включая более «децентрализованный» или «публичный» ИИ, направленный на борьбу с концентрацией власти, вызванной ИИ.

Закрытие Врат на основе вычислений также имеет недостатки. Во-первых, это далеко не полное решение проблемы управления ИИ в целом. Во-вторых, по мере того как компьютерное оборудование становится быстрее, система будет «захватывать» все больше и больше оборудования во все меньших и меньших кластерах (или даже отдельных GPU).[^19] Также возможно, что из-за алгоритмических улучшений со временем потребуется еще более низкий предел вычислений,[^20] или что количество вычислений станет в значительной степени неактуальным, и закрытие Врат вместо этого потребует более детального режима управления ИИ на основе риска или возможностей. В-третьих, независимо от гарантий и небольшого количества затронутых субъектов, такая система неизбежно создаст сопротивление относительно приватности и наблюдения, среди прочих проблем.[^21]

Конечно, разработка и реализация схемы управления, ограничивающей вычисления, за короткий период времени будет довольно сложной задачей. Но это абсолютно выполнимо.

## А-Г-И: Тройное пересечение как основа риска и политики

Теперь обратимся к ИОИ. Жесткие границы и определения здесь более трудны, потому что у нас определенно есть интеллект, который является искусственным и общим, и согласно любому существующему определению не все будут согласны, существует ли он и когда. Более того, ограничение вычислений или вывода является несколько грубым инструментом (вычисления являются прокси для возможностей, которые затем являются прокси для риска), который — если он не очень низкий — вряд ли предотвратит ИОИ, который достаточно мощен, чтобы вызвать социальные или цивилизационные нарушения или острые риски.

Я утверждал, что наиболее острые риски возникают от тройного пересечения очень высоких возможностей, высокой автономности и большой общности. Это системы, которые — если они вообще разрабатываются — должны управляться с огромной осторожностью. Создавая строгие стандарты (через ответственность и регулирование) для систем, сочетающих все три свойства, мы можем направить разработку ИИ к более безопасным альтернативам.

Как и в других отраслях и продуктах, которые потенциально могут навредить потребителям или общественности, системы ИИ требуют тщательного регулирования эффективными и уполномоченными государственными агентствами. Это регулирование должно признавать внутренние риски ИОИ и предотвращать разработку неприемлемо рискованных высокопроизводительных систем ИИ.[^22]

Однако крупномасштабное регулирование, особенно с реальными зубами, которому наверняка будет противодействовать индустрия,[^23] требует времени,[^24] а также политической убежденности в его необходимости.[^25] Учитывая темп прогресса, это может занять больше времени, чем у нас есть.

На гораздо более быстрой временной шкале и по мере разработки регулятивных мер мы можем дать компаниям необходимые стимулы для (a) воздержания от очень высокорискованной деятельности и (b) разработки комплексных систем для оценки и смягчения риска, путем уточнения и увеличения уровней ответственности для наиболее опасных систем. Идея состоит в том, чтобы наложить самые высокие уровни ответственности — строгой и в некоторых случаях личной криминальной — для систем в тройном пересечении высокой автономности-общности-интеллекта, но обеспечить «безопасные гавани» к более типичной ответственности на основе вины для систем, у которых отсутствует одно из этих свойств или оно гарантированно управляемо. То есть, например, «слабая» система, которая является общей и автономной (как способный и заслуживающий доверия, но ограниченный персональный помощник), будет подлежать более низким уровням ответственности. Аналогично узкая и автономная система, такая как самоуправляемый автомобиль, все еще будет подлежать значительному регулированию, которому она уже подлежит, но не усиленной ответственности. Подобным образом для высокоспособной и общей системы, которая является «пассивной» и в значительной степени неспособной к независимым действиям. Системы, лишенные *двух* из трех свойств, еще более управляемы, и безопасные гавани будет еще легче заявить. Этот подход отражает то, как мы обращаемся с другими потенциально опасными технологиями:[^26] более высокая ответственность за более опасные конфигурации создает естественные стимулы для более безопасных альтернатив.

Результат по умолчанию таких высоких уровней ответственности, которые действуют, чтобы *интернализировать* риск ИОИ для компаний, а не переложить его на общественность, вероятно (и желательно!), состоит в том, что компании просто не разрабатывают полный ИОИ до тех пор и если они не смогут действительно сделать его заслуживающим доверия, безопасным и контролируемым, учитывая, что *их собственное руководство* — это стороны, подвергающиеся риску. (В случае, если этого недостаточно, законодательство, уточняющее ответственность, должно также явно разрешать судебное предписание, то есть судье приказать остановить деятельность, которая явно находится в опасной зоне и спорно представляет общественный риск.) По мере введения регулирования соблюдение регулирования может стать безопасной гаванью, и безопасные гавани от низкой автономности, узости или слабости систем ИИ могут превратиться в относительно более легкие регулятивные режимы.

## Ключевые положения закрытия Врат

С учетом вышеприведенного обсуждения этот раздел предоставляет предложения для ключевых положений, которые реализовали бы и поддержали запрет на полный ИОИ и сверхинтеллект, и управление общецелевым ИИ человеческого или экспертного уровня компетенций вблизи порога полного ИОИ.[^27] У него есть четыре ключевых элемента: 1) учет и надзор за вычислительными мощностями, 2) ограничения вычислительных мощностей в обучении и работе ИИ, 3) система ответственности и 4) уровневые стандарты безопасности и защиты, которые включают жесткие регулятивные требования. Они кратко описаны далее, с дальнейшими деталями или примерами реализации, приведенными в трех сопроводительных таблицах. Важно отметить, что это далеко не все, что будет необходимо для управления передовыми системами ИИ; хотя они будут иметь дополнительные преимущества безопасности и защиты, они направлены на закрытие Врат к неконтролируемому интеллекту и перенаправление разработки ИИ в лучшем направлении.

### 1\. Учет вычислительных мощностей и прозрачность

- Организация стандартов (например, NIST в США, за которым следуют ISO/IEEE международно) должна кодифицировать детальный технический стандарт для общих вычислительных мощностей, используемых в обучении и работе моделей ИИ, в FLOP, и скорость в FLOP/с, на которой они работают. Детали того, как это может выглядеть, приведены в Приложении A.[^28]
- Требование — либо новым законодательством, либо в рамках существующих полномочий[^29] — должно быть наложено юрисдикциями, в которых происходит крупномасштабное обучение ИИ, вычислять и сообщать регулятивному органу или другому агентству общие FLOP, используемые в обучении и работе всех моделей выше порога 10<sup>25</sup> FLOP или 10<sup>18</sup> FLOP/с.[^30]
- Эти требования должны быть поэтапно внедрены, изначально требуя хорошо документированных добросовестных оценок на квартальной основе, с последующими фазами, требующими прогрессивно более высоких стандартов, вплоть до криптографически засвидетельствованных общих FLOP и FLOP/с, прикрепленных к каждому *выводу* модели.
- Эти отчеты должны дополняться хорошо документированными оценками предельной энергии и финансовых затрат, используемых в генерации каждого вывода ИИ.

Обоснование: Эти хорошо вычисленные и прозрачно сообщенные числа обеспечили бы основу для ограничений обучения и работы, а также безопасную гавань от мер более высокой ответственности (см. Приложения C и D).

### 2\. Ограничения вычислительных мощностей для обучения и работы

- Юрисдикции, размещающие системы ИИ, должны ввести жесткое ограничение на общие вычислительные мощности, идущие в любой вывод модели ИИ, начиная с 10<sup>27</sup> FLOP[^31] и регулируемые по мере необходимости.
- Юрисдикции, размещающие системы ИИ, должны ввести жесткое ограничение на скорость вычислений выводов моделей ИИ, начиная с 10<sup>20</sup> FLOP/с и регулируемые по мере необходимости.

Обоснование: Общие вычисления, хотя и очень несовершенные, являются прокси для возможностей ИИ (и риска), который конкретно измерим и проверяем, поэтому обеспечивает жесткий тупик для ограничения возможностей. Конкретное предложение по реализации дано в Приложении B.

### 3\. Усиленная ответственность за опасные системы

- Создание и эксплуатация[^32] передовой системы ИИ, которая является высоко общей, способной и автономной, должны быть юридически уточнены через законодательство как подлежащие строгой, совместной и солидарной, а не односторонней ответственности на основе вины.[^33]
- Должен быть доступен правовой процесс для предоставления утвердительных случаев безопасности, которые предоставили бы безопасную гавань от строгой ответственности для систем, которые малы (в плане вычислений), слабы, узки, пассивны или которые имеют достаточные гарантии безопасности, защиты и контролируемости.
- Должен быть очерчен явный путь и набор условий для судебного предписания для остановки деятельности по обучению и выводу ИИ, которая составляет общественную опасность.

Обоснование: Системы ИИ не могут нести ответственность, поэтому мы должны возложить ответственность на человеческих индивидуумов и организации за вред, который они причиняют (ответственность).[^34] Неконтролируемый ИОИ — это угроза обществу и цивилизации, и при отсутствии случая безопасности должен считаться аномально опасным. Возложение бремени ответственности на разработчиков показать, что мощные модели достаточно безопасны, чтобы не считаться «аномально опасными», стимулирует безопасную разработку, наряду с прозрачностью и ведением записей для заявления этих безопасных гаваней. Регулирование может затем предотвратить вред там, где сдерживание от ответственности недостаточно. Наконец, разработчики ИИ уже несут ответственность за ущерб, который они причиняют, поэтому юридическое уточнение ответственности для наиболее рискованных систем может быть сделано немедленно, без разработки высоко детальных стандартов; они могут затем развиваться со временем. Детали даны в Приложении C.

### 4\. Регулирование безопасности для ИИ

Регулятивная система, которая адресует крупномасштабные острые риски ИИ, потребует как минимум:

- Идентификацию или создание подходящего набора регулятивных органов, вероятно нового агентства;
- Комплексную систему оценки риска;[^35]
- Систему для утвердительных случаев безопасности, основанную частично на системе оценки риска, которые должны предоставляться разработчиками, и для аудита *независимыми* группами и агентствами;
- Уровневую систему лицензирования, с уровнями, отслеживающими уровни возможностей.[^36] Лицензии предоставлялись бы на основе случаев безопасности и аудитов для разработки и развертывания систем. Требования варьировались бы от уведомления на нижнем конце до количественных гарантий безопасности, защиты и контролируемости перед разработкой на верхнем конце. Это предотвратило бы выпуск систем до тех пор, пока они не продемонстрированы безопасными, и запретило бы разработку внутренне небезопасных систем. Приложение D предоставляет предложение того, что могли бы включать такие стандарты безопасности и защиты.
- Соглашения для приведения таких мер на международный уровень, включая международные органы для гармонизации норм и стандартов, и потенциально международные агентства для обзора случаев безопасности.

Обоснование: В конечном счете, ответственность — не правильный механизм для предотвращения крупномасштабного риска для общественности от новой технологии. Комплексное регулирование с уполномоченными регулятивными органами будет необходимо для ИИ, как и для каждой другой крупной индустрии, представляющей риск для общественности.[^37]

Регулирование для предотвращения других распространенных, но менее острых рисков, вероятно, будет варьироваться по форме от юрисдикции к юрисдикции. Решающее — избежать разработки систем ИИ, которые настолько рискованны, что эти риски неуправляемы.

## Что тогда?

За следующее десятилетие, по мере того как ИИ становится более распространенным и основная технология продвигается, вероятно произойдут две ключевые вещи. Во-первых, регулирование существующих мощных систем ИИ станет более трудным, но еще более необходимым. Вероятно, что по крайней мере некоторые меры, адресующие крупномасштабные риски безопасности, потребуют соглашения на международном уровне, с отдельными юрисдикциями, исполняющими правила на основе международных соглашений.

Во-вторых, ограничения вычислительных мощностей для обучения и работы станут труднее поддерживать, поскольку оборудование становится дешевле и более эффективным по стоимости; они также могут стать менее актуальными (или нуждаться в том, чтобы быть еще более жесткими) с продвижениями в алгоритмах и архитектурах.

То, что контроль ИИ станет труднее, не означает, что мы должны сдаться! Реализация плана, очерченного в этом эссе, дала бы нам как ценное время, так и решающий контроль над процессом, который поставил бы нас в гораздо, гораздо лучшее положение для избежания экзистенциального риска ИИ для нашего общества, цивилизации и вида.

В еще более долгосрочной перспективе будут выборы, которые нужно сделать относительно того, что мы разрешаем. Мы можем выбрать создать какую-то форму подлинно контролируемого ИОИ, в степени, в которой это окажется возможным. Или мы можем решить, что управление миром лучше оставить машинам, если мы сможем убедить себя, что они справятся с этим лучше и будут хорошо обращаться с нами. Но это должны быть решения, принятые с глубоким научным пониманием ИИ в руках и после содержательного глобального инклюзивного обсуждения, а не в гонке между техномагнатами с большинством человечества, полностью не вовлеченного и не осведомленного.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) Резюме управления А-Г-И и сверхинтеллектом через ответственность и регулирование. Ответственность наивысшая, а регулирование сильнейшее, в тройном пересечении Автономности, Общности и Интеллекта. Безопасные гавани от строгой ответственности и сильного регулирования могут быть получены через утвердительные случаи безопасности, демонстрирующие, что система слаба и/или узка и/или пассивна. Ограничения на общие Вычислительные мощности для Обучения и скорость Вычислительных мощностей для Вывода, проверяемые и исполняемые юридически и с использованием аппаратных и криптографических мер безопасности, подкрепляют безопасность, избегая полного ИОИ и эффективно запрещая сверхинтеллект.

[^1]: Скорее всего, распространение этого осознания потребует либо интенсивных усилий образовательных групп и групп адвокации, делающих это дело, либо довольно значительной катастрофы, вызванной ИИ. Мы можем надеяться, что это будет первое.

[^2]: Парадоксально, мы привыкли к тому, что Природа ограничивает нашу технологию, делая её очень трудной для развития, особенно научно. Но это уже не случай для ИИ: ключевые научные проблемы оказываются легче, чем ожидалось. Мы не можем рассчитывать на то, что Природа спасет нас от самих себя здесь — нам придется сделать это самим.

[^3]: Где именно мы останавливаемся в разработке новых систем? Здесь мы должны принять принцип предосторожности. Как только система развернута, и особенно когда этот уровень системных возможностей распространяется, чрезвычайно трудно откатить назад. И если система *разработана* (особенно с большими затратами и усилиями), будет огромное давление использовать или развернуть её, и соблазн для неё быть утечкой или украденной. Разработка систем и *затем* решение, являются ли они глубоко небезопасными, — опасная дорога.

[^4]: Также было бы мудро запретить разработку ИИ, которая внутренне опасна, такую как саморепликующиеся и эволюционирующие системы, те, которые предназначены для побега из заключения, те, которые могут автономно самоулучшаться, намеренно обманчивые и злонамеренные ИИ и т.д.

[^5]: Обратите внимание, это не обязательно означает *исполняемый* на международном уровне каким-то глобальным органом: вместо этого суверенные нации могли бы исполнять согласованные правила, как во многих договорах.

[^6]: Как мы увидим ниже, природа вычислений ИИ позволила бы что-то вроде гибрида; но международное сотрудничество все еще будет необходимо.

[^7]: Например, машины, необходимые для травления связанных с ИИ чипов, производятся только одной фирмой, ASML (несмотря на многие другие попытки сделать это), подавляющее большинство соответствующих чипов производится одной фирмой, TSMC (несмотря на попытки других конкурировать), и проектирование и конструирование оборудования из этих чипов делается только несколькими, включая NVIDIA, AMD и Google.

[^8]: Самое важное, каждый чип держит уникальный и недоступный криптографический приватный ключ, который он может использовать для «подписания» вещей.

[^9]: По умолчанию это была бы компания, продающая чипы, но другие модели возможны и потенциально полезны.

[^10]: Регулятор может установить местоположение чипа, измеряя время обмена подписанными сообщениями с ним: конечная скорость света требует, чтобы чип был в пределах данного радиуса *r* от «станции», если он может вернуть подписанное сообщение за время менее чем *r* / *c*, где *c* — скорость света. Используя множественные станции и некоторое понимание сетевых характеристик, местоположение чипа может быть определено. Красота этого метода в том, что большая часть его безопасности обеспечивается законами физики. Другие методы могли бы использовать GPS, инерциальное отслеживание и подобные технологии.

[^11]: Альтернативно, парам чипов могло бы быть разрешено общаться друг с другом только через явное разрешение регулятора.

[^12]: Это крайне важно, потому что по крайней мере в настоящее время очень высокоскоростное соединение между чипами необходимо для обучения больших моделей ИИ на них.

[^13]: Это также может быть настроено для требования подписанных сообщений от *N* из *M* различных регуляторов, позволяя множественным сторонам разделять управление.

[^14]: Это далеко не беспрецедентно — например, военные не разработали армии клонированных или генетически инженерных суперсолдат, хотя это вероятно технологически возможно. Но они *выбрали* не делать этого, а не были предотвращены другими. Послужной список не велик для крупных мировых держав, которых предотвращают от развития технологии, которую они сильно желают развить.

[^15]: За парой заметных исключений (в частности NVIDIA) специализированное для ИИ оборудование является относительно небольшой частью общего бизнеса и модели доходов этих компаний. Более того, разрыв между оборудованием, используемым в передовом ИИ, и «потребительским» оборудованием значителен, поэтому большинство потребителей компьютерного оборудования было бы в значительной степени не затронуто.

[^16]: Для более детального анализа см. недавние доклады от [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) и [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). Они фокусируются на технической осуществимости, особенно в контексте экспортных контролей США, стремящихся ограничить способности других стран в высокопроизводительных вычислениях; но это имеет очевидное пересечение с глобальным ограничением, предусмотренным здесь.

[^17]: Устройства Apple, например, удаленно и безопасно блокируются, когда сообщается об их потере или краже, и могут быть повторно активированы удаленно. Это полагается на те же функции аппаратной безопасности, обсуждаемые здесь.

[^18]: См., например, предложение [capacity on demand](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) от IBM, [Intel on demand](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html) от Intel и [private cloud compute](https://security.apple.com/blog/private-cloud-compute/) от Apple.

[^19]: [Это исследование](https://epochai.org/trends#hardware-trends-section) показывает, что исторически та же производительность достигалась, используя примерно на 30% меньше долларов в год. Если эта тенденция продолжится, может быть значительное пересечение между использованием ИИ и «потребительских» чипов, и в целом объем необходимого оборудования для высокопроизводительных систем ИИ мог бы стать неудобно малым.

[^20]: По [тому же исследованию](https://epochai.org/trends#hardware-trends-section), данная производительность в распознавании изображений требовала в 2.5 раза меньше вычислений каждый год. Если это также справедливо для наиболее способных систем ИИ, ограничение вычислений не было бы полезным очень долго.

[^21]: В частности, на уровне страны это выглядит очень похоже на национализацию вычислений, в том, что правительство имело бы много контроля над тем, как используется вычислительная мощность. Однако для тех, кто обеспокоен правительственным вмешательством, это кажется гораздо более безопасным и предпочтительным, чем наиболее мощное программное обеспечение ИИ *само* национализированное через некоторое слияние между крупными компаниями ИИ и национальными правительствами, за что некоторые начинают выступать.

[^22]: Крупный регулятивный шаг в Европе был сделан с принятием в 2024 году [Акта ЕС об ИИ](https://artificialintelligenceact.eu/). Он классифицирует ИИ по риску: запрещая неприемлемые системы, регулируя высокорискованные, и налагая правила прозрачности, или никаких мер вообще, на низкорискованные системы. Он значительно снизит некоторые риски ИИ и повысит прозрачность ИИ даже для американских фирм, но имеет два ключевых недостатка. Во-первых, ограниченный охват: хотя он применяется к любой компании, предоставляющей ИИ в ЕС, исполнение над американскими фирмами слабо, и военный ИИ освобожден. Во-вторых, хотя он покрывает GPAI, он не признает ИОИ или сверхинтеллект как неприемлемые риски или не предотвращает их разработку — только их развертывание в ЕС. В результате он делает мало для сдерживания рисков ИОИ или сверхинтеллекта.

[^23]: Компании часто заявляют, что они за разумное регулирование. Но каким-то образом они почти всегда, кажется, противятся любому *конкретному* регулированию; свидетелем бой за довольно легкое SB1047, которому [большинство компаний ИИ публично или частно противилось](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/).

[^24]: Прошло около 3 1/2 лет с того времени, когда акт ЕС об ИИ был предложен, до того, как он вступил в силу.

[^25]: Иногда выражается, что «слишком рано» начинать регулировать ИИ. Учитывая последнее замечание, это вряд ли кажется вероятным. Другая выраженная обеспокоенность — что регулирование «навредит инновациям». Но хорошее регулирование просто изменяет направление, а не объем инноваций.

[^26]: Интересный прецедент — в транспорте опасных материалов, которые могут сбежать и причинить ущерб. Здесь [регулирование](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) и [прецедентное право](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) установили строгую ответственность за очень опасные материалы, такие как взрывчатки, бензин, яды, инфекционные агенты и радиоактивные отходы. Другие примеры включают [предупреждения на фармацевтиках](https://www.medicalnewstoday.com/articles/boxed-warnings), [классы медицинских устройств](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification) и т.д.

[^27]: Другое комплексное предложение с подобными целями, изложенное в [«Узком пути»](https://www.narrowpath.co/), выступает за более централизованный, основанный на запрете подход, который направляет всю разработку передового ИИ через единую международную сущность, контролируемую сильными международными институтами, с четкими категорическими запретами, а не градуированными ограничениями. Я также поддержал бы этот план; однако он потребует еще больше политической воли и координации, чем предложенный здесь.

[^28]: Некоторые руководящие принципы для такого стандарта были [опубликованы](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/) Frontier Model Forum. Относительно предложения здесь те ошибаются в сторону меньшей точности и меньшего включения вычислений в подсчет.

[^29]: Исполнительный приказ США об ИИ 2023 года (теперь отозван) требовал подобной, но менее детальной отчетности. Это должно быть усилено заменяющим приказом.

[^30]: Очень приблизительно, для теперь обычных чипов H100 это соответствует кластерам примерно 1000, делающим вывод; это около 100 (около 5 млн долларов США стоимости) самых новейших топовых чипов NVIDIA B200, делающих вывод. В обоих случаях число обучения соответствует этому кластеру, вычисляющему несколько месяцев.

[^31]: Это количество больше любой в настоящее время обученной системы ИИ; большее или меньшее число могло бы быть оправдано, поскольку мы лучше понимаем, как возможности ИИ масштабируются с вычислениями.

[^32]: Это применяется к тем, кто создает и предоставляет/хостит модели, а не к конечным пользователям.

[^33]: Грубо говоря, «строгая» ответственность означает, что разработчики несут ответственность за вред, нанесенный продуктом *по умолчанию*, и является стандартом, используемым для «аномально опасных» продуктов, и (несколько забавно, но уместно) диких животных. «Совместная и солидарная» ответственность означает, что ответственность назначается всем сторонам, ответственным за продукт, и эти стороны должны разобраться между собой, кто несет какую ответственность. Это важно для систем, таких как ИИ, с длинной и сложной цепочкой ценности.

[^34]: Стандартная ответственность на основе вины одной стороны недостаточна: вину будет трудно отследить и назначить, потому что системы ИИ сложны, их работа не понимается, и многие стороны могут быть вовлечены в создание опасной системы или вывода. Кроме того, судебные процессы займут годы на рассмотрение и вероятно приведут только к штрафам, которые несущественны для этих компаний, поэтому личная ответственность для исполнителей также важна.

[^35]: Не должно быть освобождения от критериев безопасности для моделей с открытыми весами. Более того, при оценке риска должно предполагаться, что защитные меры, которые могут быть удалены, будут удалены из широко доступных моделей, и что даже закрытые модели будут распространяться, если нет очень высокой уверенности, что они останутся безопасными.

[^36]: Предложенная здесь схема имеет регулятивное внимание, запускаемое общими возможностями; однако имеет смысл, чтобы некоторые особенно рискованные случаи использования запускали больше внимания — например, экспертная система ИИ по вирусологии, даже если узкая и пассивная, вероятно должна идти в более высокий уровень. Бывший исполнительный приказ США имел некоторые из этой структуры для биологических возможностей.

[^37]: Два четких примера — авиация и лекарства, регулируемые FAA и FDA, и подобными агентствами в других странах. Эти агентства несовершенны, но были абсолютно жизненно важными для функционирования и успеха этих индустрий.