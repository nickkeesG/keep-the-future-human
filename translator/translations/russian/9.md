# Глава 9 — Конструирование будущего — что нам следует делать вместо этого

ИИ способен принести невероятную пользу миру. Чтобы получить все преимущества без рисков, мы должны обеспечить, чтобы ИИ оставался человеческим инструментом.

Если мы успешно выберем не заменять человечество машинами — по крайней мере, на некоторое время! — что мы можем делать вместо этого? Откажемся ли мы от огромного потенциала ИИ как технологии? На определенном уровне ответ простой: *нет* — закроем Врата для неконтролируемого ИОИ и сверхинтеллекта, но *будем* создавать множество других форм ИИ, а также структуры управления и институты, необходимые для их регулирования.

Но здесь есть о чем поговорить; реализация этого станет центральной задачей человечества. Этот раздел исследует несколько ключевых тем:

- Как мы можем охарактеризовать "инструментальный" ИИ и формы, которые он может принимать.
- Что мы можем получить (почти) все, чего хочет человечество, без ИОИ, используя инструментальный ИИ.
- Что системы инструментального ИИ (вероятно, в принципе) управляемы.
- Что отказ от ИОИ не означает компромисса в области национальной безопасности — совсем наоборот.
- Что концентрация власти является реальной проблемой. Можем ли мы смягчить ее, не подрывая безопасность?
- Что нам понадобятся — и потребуются — новые структуры управления и социальные институты, и ИИ на самом деле может помочь.

## ИИ внутри Врат: инструментальный ИИ

Диаграмма тройного пересечения дает хороший способ определить то, что мы можем назвать "инструментальным ИИ": ИИ, который является контролируемым инструментом для человеческого использования, а не неконтролируемым соперником или заменой. Наименее проблематичными являются системы ИИ, которые автономны, но не общие или сверхспособные (как бот для торговых аукционов), или общие, но не автономные или способные (как малая языковая модель), или способные, но узкие и очень контролируемые (как AlphaGo).[^1] Те, что обладают двумя пересекающимися характеристиками, имеют более широкое применение, но более высокий риск и потребуют значительных усилий для управления. (То, что система ИИ больше похожа на инструмент, не означает, что она изначально безопасна, лишь то, что она не является изначально *небезопасной* — сравните бензопилу с домашним тигром.) Врата должны оставаться закрытыми для (полного) ИОИ и сверхинтеллекта в тройном пересечении, и необходимо проявлять огромную осторожность с системами ИИ, приближающимися к этому порогу.

Но это оставляет много мощного ИИ! Мы можем извлечь огромную пользу из умных и общих пассивных "оракулов" и узких систем, общих систем на человеческом, но не сверхчеловеческом уровне, и так далее. Многие технологические компании и разработчики активно создают такие инструменты и должны продолжать; как и большинство людей, они неявно *предполагают*, что Врата к ИОИ и сверхинтеллекту будут закрыты.[^2]

Кроме того, системы ИИ можно эффективно объединять в составные системы, которые сохраняют человеческий надзор, повышая при этом возможности. Вместо того чтобы полагаться на непостижимые черные ящики, мы можем строить системы, где множество компонентов — включая как ИИ, так и традиционное программное обеспечение — работают вместе способами, которые люди могут отслеживать и понимать.[^3] Хотя некоторые компоненты могут быть черными ящиками, ни один не будет близок к ИОИ — только составная система в целом будет одновременно высокообщей и высокоспособной, и притом строго контролируемым образом.[^4]

### Значимый и гарантированный человеческий контроль

Что означает "строго контролируемый"? Ключевая идея концепции "инструмента" — позволить системам — даже если они довольно общие и мощные — которые гарантированно находятся под значимым человеческим контролем. Что это означает? Это включает два аспекта. Первый — это соображение дизайна: люди должны быть глубоко и центрально вовлечены в то, что делает система, *не* делегируя ключевые важные решения ИИ. Таков характер большинства современных систем ИИ. Второй: в той степени, в какой системы ИИ автономны, они должны иметь гарантии, ограничивающие сферу их действий. Гарантия должна быть *числом*, характеризующим вероятность того, что что-то произойдет, и причиной верить в это число. Это то, чего мы требуем в других критически важных для безопасности областях, где такие числа, как "среднее время между отказами" и ожидаемое количество аварий, вычисляются, обосновываются и публикуются в обоснованиях безопасности.[^5] Идеальное число для отказов, конечно, ноль. И хорошая новость в том, что мы можем подойти довольно близко, хотя и используя совершенно иные архитектуры ИИ, применяя идеи *формально верифицированных* свойств программ (включая ИИ). Идея, подробно исследованная Омохундро, Тегмарком, Бенджио, Далримплом и другими (см. [здесь](https://arxiv.org/abs/2309.01933) и [здесь](https://arxiv.org/abs/2405.06624)), заключается в создании программы с определенными свойствами (например: что человек может ее отключить) и формальном *доказательстве* того, что эти свойства выполняются. Это можно делать сейчас для довольно коротких программ и простых свойств, но (грядущая) мощь программного обеспечения для доказательств с поддержкой ИИ может позволить это для гораздо более сложных программ (например, оболочек) и даже самого ИИ. Это очень амбициозная программа, но по мере нарастания давления на Врата нам понадобятся мощные материалы для их укрепления. Математическое доказательство может быть одним из немногих, которое достаточно прочно.

### Куда движется индустрия ИИ

При перенаправлении прогресса ИИ инструментальный ИИ все равно останется огромной индустрией. В плане аппаратного обеспечения, даже с ограничениями вычислительных мощностей для предотвращения сверхинтеллекта, обучение и инференс в меньших моделях все равно потребует огромного количества специализированных компонентов. Что касается программного обеспечения, то обезвреживание взрывного роста размера моделей ИИ и вычислений должно просто привести к тому, что компании перенаправят ресурсы на улучшение, диверсификацию и специализацию меньших систем, а не просто на их увеличение.[^6] Там будет много места — вероятно, больше — для всех тех приносящих деньги стартапов Кремниевой долины.[^7]

## Инструментальный ИИ может дать (почти) все, чего хочет человечество, без ИОИ

Интеллект, будь то биологический или машинный, можно в широком смысле рассматривать как способность планировать и выполнять действия, создающие будущее, более соответствующее набору целей. Как таковой, интеллект приносит огромную пользу, когда используется в стремлении к мудро выбранным целям. Искусственный интеллект привлекает огромные инвестиции времени и усилий в основном из-за обещанных им выгод. Поэтому мы должны спросить: в какой степени мы все еще получим выгоды от ИИ, если сдержим его неконтролируемое развитие до сверхинтеллекта? Ответ: мы можем потерять удивительно мало.

Рассмотрим сначала то, что современные системы ИИ уже очень мощны, и мы действительно лишь коснулись поверхности того, что с ними можно делать.[^8] Они вполне способны "управлять процессом" в плане "понимания" вопроса или задачи, представленной им, и того, что потребуется для ответа на этот вопрос или выполнения этой задачи.

Далее, большая часть восторга по поводу современных систем ИИ обусловлена их общностью; но некоторые из самых способных систем ИИ — такие как те, что генерируют или распознают речь или изображения, делают научные предсказания и моделирование, играют в игры и т.д. — гораздо более узки и хорошо "внутри Врат" в плане вычислений.[^9] Эти системы сверхчеловечны в конкретных задачах, которые они выполняют. У них могут быть слабости в крайних случаях[^10] (или [уязвимости для эксплуатации](https://arxiv.org/abs/2211.00241)) из-за их узости; однако *полностью* узкие или *полностью* общие — не единственные доступные варианты: есть много архитектур между ними.[^11]

Эти инструменты ИИ могут значительно ускорить развитие других позитивных технологий без ИОИ. Чтобы улучшить ядерную физику, нам не нужен ИИ-физик-ядерщик — у нас есть такие! Если мы хотим ускорить медицину, дадим биологам, медицинским исследователям и химикам мощные инструменты. Они хотят их и будут использовать с огромной выгодой. Нам не нужна серверная ферма с миллионом цифровых гениев; у нас есть миллионы людей, чью гениальность может помочь раскрыть ИИ. Да, потребуется больше времени, чтобы получить бессмертие и лекарство от всех болезней. Это реальная цена. Но даже самые многообещающие медицинские инновации будут малополезны, если нестабильность, вызванная ИИ, приведет к глобальному конфликту или коллапсу общества. Мы обязаны дать людям, усиленным ИИ, возможность сначала попробовать решить проблему.

И предположим, что действительно есть какая-то огромная выгода от ИОИ, которую нельзя получить человечеству, используя инструменты внутри Врат. Теряем ли мы ее, *никогда* не создавая ИОИ и сверхинтеллект? Взвешивая здесь риски и награды, есть огромная асимметричная выгода в ожидании против спешки: мы можем подождать, пока это не сможет быть сделано гарантированно безопасным и полезным образом, и почти все все равно смогут пожинать плоды; если мы будем спешить, это может быть — по словам генерального директора OpenAI Сэма Альтмана — [конец света для *всех* нас.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

Но если инструменты без ИОИ потенциально настолько мощны, можем ли мы ими управлять? Ответ четкий... возможно.

## Системы инструментального ИИ (вероятно, в принципе) управляемы

Но это будет нелегко. Современные передовые системы ИИ могут значительно расширить возможности людей и институтов в достижении их целей. Это, в общем, хорошо! Однако есть естественная динамика наличия таких систем в нашем распоряжении — внезапно и без достаточного времени для адаптации общества — которая создает серьезные риски, требующие управления. Стоит обсудить несколько основных классов таких рисков и то, как их можно уменьшить, предполагая закрытие Врат.

Один класс рисков — это предоставление мощным инструментальным ИИ доступа к знаниям или возможностям, которые ранее были привязаны к человеку или организации, делая комбинацию высоких возможностей плюс высокой лояльности доступной очень широкому кругу субъектов. Сегодня, имея достаточно денег, человек со злыми намерениями мог бы нанять команду химиков для разработки и производства нового химического оружия — но не так уж легко иметь такие деньги или найти/собрать команду и убедить их сделать что-то явно незаконное, неэтичное и опасное. Чтобы предотвратить игру систем ИИ такой ролью, улучшения существующих методов вполне могут оказаться достаточными,[^12] при условии, что все эти системы и доступ к ним ответственно управляются. С другой стороны, если мощные системы выпускаются для общего использования и модификации, любые встроенные меры безопасности, вероятно, можно удалить. Поэтому, чтобы избежать рисков в этом классе, потребуются строгие ограничения на то, что может быть публично выпущено — аналогично ограничениям на детали ядерных, взрывчатых и других опасных технологий.[^13]

Второй класс рисков связан с масштабированием машин, которые действуют как люди или выдают себя за людей. На уровне вреда отдельным людям эти риски включают гораздо более эффективные мошенничества, спам и фишинг, а также распространение дипфейков без согласия.[^14] На коллективном уровне они включают нарушение основных социальных процессов, таких как публичные дискуссии и дебаты, наши общественные системы сбора, обработки и распространения информации и знаний, а также наши системы политического выбора. Смягчение этого риска, вероятно, потребует (а) законов, ограничивающих выдавание ИИ-системами себя за людей, и возложения ответственности на разработчиков ИИ, создающих системы, генерирующие такие подделки, (б) систем водяных знаков и происхождения, которые идентифицируют и классифицируют (ответственно) генерированный ИИ контент, и (в) новых социотехнических эпистемических систем, которые могут создать доверенную цепочку от данных (например, камер и записей) через факты, понимание и хорошие модели мира.[^15] Все это возможно, и ИИ может помочь с некоторыми частями этого.

Третий общий риск заключается в том, что в той степени, в которой некоторые задачи автоматизируются, люди, в настоящее время выполняющие эти задачи, могут иметь меньшую финансовую ценность как рабочая сила. Исторически автоматизация задач делала вещи, обеспечиваемые этими задачами, дешевле и более доступными, при этом разделяя людей, ранее выполнявших эти задачи, на тех, кто все еще вовлечен в автоматизированную версию (как правило, с более высокими навыками/оплатой), и тех, чья рабочая сила стоит меньше или мало. В итоге трудно предсказать, в каких секторах потребуется больше, а в каких меньше человеческого труда в результирующем большем, но более эффективном секторе. Параллельно динамика автоматизации имеет тенденцию увеличивать неравенство и общую производительность, уменьшать стоимость определенных товаров и услуг (через повышение эффективности) и увеличивать стоимость других (через [болезнь стоимости](https://en.wikipedia.org/wiki/Baumol_effect)). Для тех, кто оказался на неблагоприятной стороне роста неравенства, совершенно неясно, перевешивает ли снижение стоимости тех определенных товаров и услуг рост других и приводит к общему большему благополучию. Так как же это будет с ИИ? Из-за относительной легкости, с которой человеческий интеллектуальный труд может быть заменен общим ИИ, мы можем ожидать быструю версию этого с общецелевым ИИ, конкурентоспособным с человеком.[^16] Если мы закроем Врата к ИОИ, гораздо меньше рабочих мест будет оптом заменено ИИ-агентами; но огромное перемещение рабочей силы все равно вероятно в течение нескольких лет.[^17] Чтобы избежать широко распространенных экономических страданий, вероятно, будет необходимо внедрить как некую форму универсального базового достояния или дохода, так и спроектировать культурный сдвиг к оценке и вознаграждению человекоориентированного труда, который сложнее автоматизировать (а не видеть, как цены на труд падают из-за роста доступной рабочей силы, вытесненной из других частей экономики). Другие конструкты, такие как ["достоинство данных"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society) (при котором человеческие производители обучающих данных автоматически получают роялти за ценность, созданную этими данными в ИИ), могут помочь. Автоматизация ИИ также имеет второй потенциальный неблагоприятный эффект — *неуместную* автоматизацию. Наряду с применениями, где ИИ просто работает хуже, это включало бы те, где системы ИИ, вероятно, нарушают моральные, этические или правовые принципы — например, в решениях жизни и смерти и в судебных вопросах. С этим нужно бороться, применяя и расширяя наши текущие правовые рамки.

Наконец, значительная угроза ИИ внутри врат — это его использование в персонализированном убеждении, захвате внимания и манипулировании. Мы видели в социальных сетях и других онлайн-платформах рост глубоко укоренившейся экономики внимания (где онлайн-сервисы ожесточенно борются за внимание пользователей) и систем ["капитализма слежки"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism) (в которых пользовательская информация и профилирование добавляются к коммодификации внимания). Почти наверняка больше ИИ будет поставлено на службу обеим. ИИ уже активно используется в вызывающих привыкание алгоритмах лент, но это эволюционирует в вызывающий привыкание контент, генерированный ИИ, настроенный для компульсивного потребления одним человеком. И ввод, ответы и данные этого человека будут подаваться в машину внимания/рекламы для продолжения порочного круга. Кроме того, поскольку ИИ-помощники, предоставляемые технологическими компаниями, становятся интерфейсом для большей части онлайн-жизни, они, вероятно, заменят поисковые системы и ленты как механизм, с помощью которого происходит убеждение и монетизация клиентов. Неспособность нашего общества контролировать эту динамику до сих пор не предвещает ничего хорошего. Часть этой динамики может быть ослаблена через регулирование, касающееся приватности, прав на данные и манипулирования. Более глубокое решение проблемы может потребовать других перспектив, таких как лояльные ИИ-помощники (обсуждаемые ниже).

Суть этого обсуждения — в надежде: системы на основе инструментов внутри Врат — по крайней мере, пока они остаются сравнимыми по мощи и возможностям с сегодняшними самыми передовыми системами — вероятно, управляемы, если есть воля и координация для этого. Достойные человеческие институты, усиленные инструментами ИИ,[^18] могут это сделать. Мы также могли бы потерпеть неудачу в этом. Но трудно увидеть, как разрешение более мощных систем помогло бы — кроме как поставив их во главе и надеясь на лучшее.

## Национальная безопасность

Гонки за превосходство в ИИ — движимые национальной безопасностью или другими мотивами — ведут нас к неконтролируемым мощным системам ИИ, которые имеют тенденцию поглощать, а не наделять властью. Гонка ИОИ между США и Китаем — это гонка за то, какая нация получит сверхинтеллект первой.

Так что же должны делать вместо этого те, кто отвечает за национальную безопасность? Правительства имеют большой опыт создания контролируемых и безопасных систем, и они должны удвоить усилия в этом направлении в ИИ, поддерживая такие инфраструктурные проекты, которые лучше всего удаются при выполнении в масштабе и с правительственным одобрением.

Вместо безрассудного "Манхэттенского проекта" к ИОИ,[^19] правительство США могло бы запустить проект "Аполлон" для контролируемых, безопасных, надежных систем. Это могло бы включать, например:

- Крупную программу по (а) разработке аппаратных механизмов безопасности на чипах и (б) инфраструктуры для управления вычислительной стороной мощного ИИ. Они могли бы строиться на основе американского [закона о CHIPS](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) и [режима экспортного контроля](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion).
- Масштабную инициативу по разработке техник формальной верификации, чтобы определенные свойства систем ИИ (как выключатель) могли быть *доказаны* как присутствующие или отсутствующие. Это может использовать сам ИИ для разработки доказательств свойств.
- Национальный проект по созданию программного обеспечения, которое проверяемо безопасно, приводимого в действие инструментами ИИ, которые могут перекодировать существующее программное обеспечение в проверяемо безопасные фреймворки.
- Национальный инвестиционный проект в научное развитие с использованием ИИ,[^20] работающий как партнерство между Министерством энергетики, NSF и NIH.

В общем, есть огромная поверхность атак на наше общество, которая делает нас уязвимыми к рискам от ИИ и его неправильного использования. Защита от некоторых из этих рисков потребует правительственных инвестиций и стандартизации. Это обеспечило бы гораздо больше безопасности, чем лить бензин на огонь гонок к ИОИ. И если ИИ будет встроен в вооружения и системы командования и управления, критически важно, чтобы ИИ был надежным и безопасным, чем современный ИИ просто не является.

## Концентрация власти и ее смягчение

Это эссе сосредоточилось на идее человеческого контроля над ИИ и его потенциальной неудаче. Но другая валидная линза для рассмотрения ситуации с ИИ — через *концентрацию власти.* Разработка очень мощного ИИ угрожает сконцентрировать власть либо в очень немногих и очень крупных корпоративных руках, которые разработали и будут контролировать его, либо в правительствах, использующих ИИ как новое средство поддержания собственной власти и контроля, либо в самих системах ИИ. Или в какой-то нечестивой смеси вышеперечисленного. В любом из этих случаев большинство человечества теряет власть, контроль и деятельность. Как мы можем с этим бороться?

Самый первый и важнейший шаг, конечно, — закрытие Врат к более умному, чем человек, ИОИ и сверхинтеллекту. Они явно могут напрямую заменить людей и группы людей. Если они находятся под корпоративным или правительственным контролем, они сконцентрируют власть в этих корпорациях или правительствах; если они "свободны", они сконцентрируют власть в себе. Итак, предположим, что Врата закрыты. Тогда что?

Одно предложенное решение концентрации власти — ИИ "с открытым исходным кодом", где веса модели свободно или широко доступны. Но как упоминалось ранее, как только модель открыта, большинство мер безопасности или ограждений могут быть (и обычно бывают) устранены. Поэтому есть острое напряжение между, с одной стороны, децентрализацией, а с другой стороны, безопасностью и человеческим контролем систем ИИ. Также есть основания скептически относиться к тому, что открытые модели сами по себе значимо борются с концентрацией власти в ИИ больше, чем они это делали в операционных системах (все еще доминируемых Microsoft, Apple и Google несмотря на открытые альтернативы).[^21]

Тем не менее, могут быть способы согласовать этот круг — централизовать и смягчить риски, децентрализуя возможности и экономическое вознаграждение. Это требует переосмысления того, как разрабатывается ИИ и как распределяются его выгоды.

Новые модели публичной разработки и владения ИИ помогли бы. Это могло принимать несколько форм: правительственно-разработанный ИИ (подлежащий демократическому надзору),[^22] некоммерческие организации разработки ИИ (как Mozilla для браузеров), или структуры, обеспечивающие очень широкое владение и управление. Ключевым является то, что эти институты были бы явно призваны служить общественному интересу, работая под строгими ограничениями безопасности.[^23] Хорошо сработанные регулятивные режимы и режимы стандартов/сертификации также будут жизненно важны, чтобы ИИ-продукты, предлагаемые живым рынком, оставались по-настоящему полезными, а не эксплуатационными по отношению к их пользователям.

В плане концентрации экономической власти мы можем использовать отслеживание происхождения и "достоинство данных", чтобы обеспечить более широкое распространение экономических выгод. В частности, большинство мощи ИИ сейчас (и в будущем, если мы держим Врата закрытыми) происходит из данных, генерированных человеком, будь то прямые обучающие данные или человеческая обратная связь. Если бы ИИ-компании были обязаны справедливо компенсировать поставщикам данных,[^24] это могло бы по крайней мере помочь распределить экономические награды более широко. Кроме этого, другой моделью может быть публичное владение значительными долями крупных ИИ-компаний. Например, правительства, способные облагать налогом ИИ-компании, могли бы инвестировать долю поступлений в суверенный фонд благосостояния, который держит акции компаний и выплачивает дивиденды населению.[^25]

Ключевым в этих механизмах является использование силы самого ИИ для лучшего распределения власти, а не просто борьба с концентрацией власти, управляемой ИИ, неИИ средствами. Один мощный подход был бы через хорошо спроектированных ИИ-помощников, которые действуют с подлинной фидуциарной обязанностью перед своими пользователями — ставя интересы пользователей на первое место, особенно выше корпоративных поставщиков.[^26] Эти помощники должны быть по-настоящему заслуживающими доверия, технически компетентными, но соответственно ограниченными на основе случая использования и уровня риска, и широко доступными всем через публичные, некоммерческие или сертифицированные коммерческие каналы. Точно так же, как мы никогда не приняли бы человека-помощника, который тайно работает против наших интересов для другой стороны, мы не должны принимать ИИ-помощников, которые наблюдают, манипулируют или извлекают ценность от своих пользователей для корпоративной выгоды.

Такая трансформация кардинально изменила бы текущую динамику, где индивиды остаются вести переговоры в одиночку с огромными (усиленными ИИ) корпоративными и бюрократическими машинами, которые приоритизируют извлечение стоимости над человеческим благосостоянием. Хотя есть много возможных подходов к более широкому перераспределению власти, управляемой ИИ, ни один не появится по умолчанию: они должны быть сознательно спроектированы и управляемы механизмами, такими как фидуциарные требования, публичное обеспечение и многоуровневый доступ на основе риска.

Подходы к смягчению концентрации власти могут столкнуться со значительным противодействием от действующих властей.[^27] Но есть пути к разработке ИИ, которые не требуют выбора между безопасностью и концентрированной властью. Строя правильные институты сейчас, мы могли бы обеспечить широкое разделение выгод ИИ при тщательном управлении его рисками.

## Новые структуры управления и социальные структуры

Наши нынешние структуры управления испытывают трудности: они медленно реагируют, часто захвачены особыми интересами и [все больше вызывают недоверие у общественности.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) Тем не менее, это не причина отказываться от них — совсем наоборот. Некоторые институты могут нуждаться в замене, но шире нам нужны новые механизмы, которые могут улучшить и дополнить наши существующие структуры, помогая им лучше функционировать в нашем быстро развивающемся мире.

Большая часть нашей институциональной слабости происходит не из формальных правительственных структур, а из деградированных социальных институтов: наших систем для развития общего понимания, координации действий и проведения значимого дискурса. До сих пор ИИ ускорил эту деградацию, наводняя наши информационные каналы генерированным контентом, направляя нас к самому поляризующему и разделяющему контенту и затрудняя различение правды от вымысла.

Но ИИ действительно мог бы помочь восстановить и укрепить эти социальные институты. Рассмотрим три ключевые области:

Во-первых, ИИ мог бы помочь восстановить доверие к нашим эпистемическим системам — нашим способам знания того, что истинно. Мы могли бы разработать системы с поддержкой ИИ, которые отслеживают и проверяют происхождение информации, от сырых данных через анализ к заключениям. Эти системы могли бы сочетать криптографическую верификацию со сложным анализом, чтобы помочь людям понять не только, истинно ли что-то, но и как мы знаем, что это истинно.[^28] Лояльные ИИ-помощники могли бы быть поручены следовать деталям, чтобы убедиться, что они подтверждаются.

Во-вторых, ИИ мог бы обеспечить новые формы крупномасштабной координации. Многие из наших самых насущных проблем — от изменения климата до устойчивости к антибиотикам — в основе своей являются проблемами координации. Мы [застряли в ситуациях, которые хуже, чем могли бы быть почти для всех](https://equilibriabook.com/), потому что ни один индивид или группа не могут позволить себе сделать первый ход. Системы ИИ могли бы помочь, моделируя сложные структуры стимулов, идентифицируя жизнеспособные пути к лучшим результатам и облегчая построение доверия и механизмы обязательств, необходимые, чтобы туда добраться.

Возможно, самое интригующее, ИИ мог бы обеспечить совершенно новые формы социального дискурса. Представьте себе возможность "говорить с городом"[^29] — не просто просматривать статистику, но вести значимый диалог с системой ИИ, которая обрабатывает и синтезирует взгляды, опыт, потребности и стремления миллионов жителей. Или подумайте о том, как ИИ мог бы облегчить подлинный диалог между группами, которые в настоящее время говорят мимо друг друга, помогая каждой стороне лучше понять реальные проблемы и ценности другой, а не их карикатуры друг друга.[^30] Или ИИ мог бы предложить квалифицированное, достоверно нейтральное посредничество споров между людьми или даже большими группами людей (которые могли бы все взаимодействовать с ним напрямую и индивидуально!) Современный ИИ полностью способен выполнять эту работу, но инструменты для этого не появятся сами по себе или через рыночные стимулы.

Эти возможности могут звучать утопически, особенно учитывая нынешнюю роль ИИ в деградации дискурса и доверия. Но именно поэтому мы должны активно разрабатывать эти позитивные применения. Закрывая Врата к неконтролируемому ИОИ и приоритизируя ИИ, который усиливает человеческую деятельность, мы можем направить технологический прогресс к будущему, где ИИ служит силой расширения возможностей, устойчивости и коллективного развития.

[^1]: Тем не менее, держаться подальше от тройного пересечения, к сожалению, не так легко, как хотелось бы. Усиленное продвижение возможностей в любом из трех аспектов имеет тенденцию увеличивать их в других. В частности, может быть трудно создать чрезвычайно общий и способный интеллект, который не может быть легко превращен в автономный. Один подход — тренировать модели ["близорукими"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) системами с ограниченной способностью планирования. Другим было бы сосредоточиться на инженерии чистых систем ["оракулов"](https://arxiv.org/abs/1711.05541), которые уклонялись бы от ответов на ориентированные на действия вопросы.

[^2]: Многие компании не понимают, что они тоже в конце концов были бы замещены ИОИ, даже если это займет больше времени — если бы они поняли, они могли бы меньше давить на эти Врата!

[^3]: Системы ИИ могли бы общаться более эффективными, но менее понятными способами, но поддержание человеческого понимания должно иметь приоритет.

[^4]: Эта идея модульного, интерпретируемого ИИ была подробно разработана несколькими исследователями; см., например, модель ["Комплексных услуг ИИ"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) Дрекслера, ["Открытую архитектуру агентности"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai) Далримпла и других. Хотя такие системы могли бы потребовать больше инженерных усилий, чем монолитные нейронные сети, тренированные с массивными вычислениями, именно здесь вычислительные ограничения помогают — делая более безопасный, более прозрачный путь также более практичным.

[^5]: О случаях безопасности в общем см. [этот справочник](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). Касательно ИИ в частности, см. [Wasil et al.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [Clymer et al.](https://arxiv.org/abs/2403.10462), [Buhl et al.](https://arxiv.org/abs/2410.21572), и [Balesni et al.](https://arxiv.org/abs/2411.03336)

[^6]: Мы на самом деле уже видим эту тенденцию, движимую просто высокой стоимостью инференса: меньшие и более специализированные модели, "дистиллированные" из больших и способные работать на менее дорогом аппаратном обеспечении.

[^7]: Я понимаю, почему те, кто воодушевлен экосистемой ИИ-технологий, могут противиться тому, что они видят как обременительное регулирование их индустрии. Но для меня откровенно озадачивающе, почему, скажем, венчурный капиталист хотел бы позволить неконтролируемый переход к ИОИ и сверхинтеллекту. Эти системы (и компании, пока они остаются под контролем компаний) *съедят все стартапы как закуску*. Вероятно, даже *раньше*, чем съедят другие индустрии. Любой, кто инвестирован в процветающую ИИ-экосистему, должен приоритизировать обеспечение того, чтобы разработка ИОИ не привела к монополизации несколькими доминирующими игроками.

[^8]: Как выразился экономист и бывший исследователь Deepmind Майкл Уэбб, "Я думаю, если мы прекратим всю разработку больших языковых моделей сегодня, так что GPT-4 и Claude и что там еще — последние вещи, которые мы тренируем такого размера — так что мы позволяем много больше итераций на вещах такого размера и все виды тонкой настройки, но ничего больше, чем то, никаких больших продвижений — просто то, что у нас есть сегодня, я думаю, достаточно для питания 20 или 30 лет невероятного экономического роста."

[^9]: Например, система alphafold компании DeepMind использовала только одну стотысячную от числа FLOP GPT-4.

[^10]: Трудности самоуправляемых автомобилей важно отметить здесь: хотя номинально узкая задача и достижимая с достаточной надежностью с относительно малыми системами ИИ, обширные реальные знания и понимание необходимы, чтобы получить надежность на уровне, необходимом в такой критически важной для безопасности задаче.

[^11]: Например, при данном вычислительном бюджете мы вероятно увидели бы модели GPAI, предварительно обученные на (скажем) половине этого бюджета, и другая половина использована для тренировки очень высоких возможностей в более узком диапазоне задач. Это дало бы сверхчеловеческие узкие возможности, поддержанные близким к человеческому общим интеллектом.

[^12]: Современная доминирующая техника выравнивания — это ["обучение с подкреплением по человеческой обратной связи"](https://arxiv.org/abs/1706.03741) [(RLHF)](https://arxiv.org/abs/1706.03741) и использует человеческую обратную связь для создания сигнала награды/наказания для обучения с подкреплением модели ИИ. Это и связанные техники, такие как [конституциональный ИИ](https://arxiv.org/abs/2212.08073), работают удивительно хорошо (хотя им не хватает устойчивости, и их можно обойти с умеренными усилиями). Кроме того, современные языковые модели обычно достаточно компетентны в здравом смысле, чтобы не делать глупых моральных ошибок. Это что-то вроде сладкого пятна: достаточно умны, чтобы понимать, что люди хотят (в той степени, в которой это может быть определено), но недостаточно умны, чтобы планировать сложные обманы или причинять огромный вред, когда они ошибаются.

[^13]: В долгосрочной перспективе любой уровень возможностей ИИ, который разрабатывается, вероятно, распространится, поскольку в конечном счете это программное обеспечение, и полезное. Нам нужно будет иметь надежные механизмы защиты от рисков, которые представляют такие системы. Но у нас *этого нет сейчас*, поэтому мы должны быть очень осторожными в том, сколько мощным моделям ИИ разрешено распространяться.

[^14]: Подавляющее большинство из них — неконсенсуальные порнографические дипфейки, включая несовершеннолетних.

[^15]: Многие ингредиенты для таких решений существуют в форме законов "бот-или-не-бот" (в Акте ИИ ЕС среди других мест), [отраслевых технологий отслеживания происхождения](https://c2pa.org/), [инновационных новостных агрегаторов](https://www.improvethenews.org/), [агрегаторов](https://metaculus.com/) предсказаний и рынков и т.д.

[^16]: Волна автоматизации может не следовать предыдущим паттернам в том, что относительно *высоко*-квалифицированные задачи, такие как качественное письмо, интерпретация права или предоставление медицинских советов, могут быть столь же или даже более уязвимы для автоматизации, чем низкоквалифицированные задачи.

[^17]: Для тщательного моделирования влияния ИОИ на зарплаты см. отчет [здесь](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek), и кровавые подробности [здесь](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0), от Антона Коринека и сотрудников. Они обнаружили, что по мере автоматизации большего числа частей работ производительность и зарплаты растут — до точки. Как только *слишком* много автоматизировано, производительность продолжает расти, но зарплаты рушатся, потому что люди заменяются оптом эффективным ИИ. Вот почему закрытие Врат так полезно: мы получаем производительность без исчезнувших человеческих зарплат.

[^18]: Есть много способов использования ИИ как, и для помощи в строительстве, "защитных" технологий, чтобы сделать защиты и управление более устойчивыми. См. [этот](https://vitalik.eth.limo/general/2025/01/05/dacc2.html) влиятельный пост, описывающий эту повестку "D/acc".

[^19]: Несколько иронично, что американский Манхэттенский проект, вероятно, мало что сделал бы для ускорения временных рамок к ИОИ — циферблат человеческих и фискальных инвестиций в прогресс ИИ уже прикован к 11. Первичными результатами было бы вдохновить аналогичный проект в Китае (который преуспевает в инфраструктурных проектах национального уровня), сделать международные соглашения, ограничивающие риск ИИ, гораздо сложнее, и встревожить других геополитических противников США, таких как Россия.

[^20]: Программа ["Национального ресурса исследований ИИ"](https://nairrpilot.org/) — хороший текущий шаг в этом направлении и должна быть расширена.

[^21]: См. [этот анализ](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807) различных значений и последствий "открытости" в технологических продуктах и как некоторые привели к большему, а не меньшему, укреплению доминирования.

[^22]: Планы в США для [Национального ресурса исследований ИИ](https://nairratdoe.ornl.gov/) и недавний запуск [Европейского фонда ИИ](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/) являются интересными шагами в этом направлении.

[^23]: Вызов здесь не технический, а институциональный — нам срочно нужны реальные примеры и эксперименты в том, как могла бы выглядеть разработка ИИ в общественных интересах.

[^24]: Это идет против текущих бизнес-моделей больших технологических компаний и потребовало бы как правовых действий, так и новых норм.

[^25]: Только некоторые правительства смогут это сделать. Более радикальная идея — [универсальный фонд такого типа, под совместным владением всех людей.](https://futureoflife.org/project/the-windfall-trust/)

[^26]: Для длительного изложения этого случая см. [эту статью](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338) о лояльности ИИ. К сожалению, траектория по умолчанию ИИ-помощников, вероятно, будет такой, где они становятся все более нелояльными.

[^27]: Несколько иронично, что многие действующие власти также подвержены риску лишения власти, поддерживаемого ИИ; но для них может быть трудно воспринять это, пока и если процесс не зайдет довольно далеко.

[^28]: Некоторые интересные усилия в этом направлении представлены [коалицией c2pa](https://c2pa.org/) по криптографической верификации; [Verity](https://www.improvethenews.org/) и [Ground news](https://ground.news/) по лучшей новостной эпистемике; и [Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com) и рынками предсказаний по обоснованию дискурса в фальсифицируемых предсказаниях.

[^29]: См. [этот](https://talktothecity.org/) увлекательный пилотный проект.

[^30]: См. [Kialo](https://www.kialo-edu.com/) и усилия [Проекта коллективного интеллекта](https://www.cip.org/) для некоторых примеров.