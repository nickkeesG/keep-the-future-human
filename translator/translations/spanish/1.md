# Capítulo 1 - Introducción

Cómo responderemos a la perspectiva de una IA más inteligente que los humanos es el tema más urgente de nuestro tiempo. Este ensayo proporciona un camino a seguir.

Podríamos estar al final de la era humana.

Algo comenzó en los últimos diez años que es único en la historia de nuestra especie. Sus consecuencias determinarán, en gran medida, el futuro de la humanidad. A partir de aproximadamente 2015, los investigadores han logrado desarrollar inteligencia artificial (IA) *especializada* – sistemas que pueden ganar en juegos como el Go, reconocer imágenes y voz, y más, mejor que cualquier humano.[^1]

Este es un éxito extraordinario, y está produciendo sistemas y productos extremadamente útiles que empoderarán a la humanidad. Pero la inteligencia artificial especializada nunca ha sido el verdadero objetivo del campo. Más bien, la meta ha sido crear sistemas de IA de propósito *general*, particularmente aquellos que a menudo se llaman "inteligencia artificial general" (IAG) o "superinteligencia" que son simultáneamente tan buenos o mejores que los humanos en casi *todas* las tareas, así como la IA ahora es sobrehumana en Go, ajedrez, póker, carreras de drones, etc. Este es el objetivo declarado de muchas compañías importantes de IA.[^2]

*Estos esfuerzos también están teniendo éxito.* Los sistemas de IA de propósito general como ChatGPT, Gemini, Llama, Grok, Claude y Deepseek, basados en cómputos masivos y montañas de datos, han alcanzado la paridad con humanos típicos en una amplia variedad de tareas, e incluso igualan a expertos humanos en algunos dominios. Ahora los ingenieros de IA de algunas de las compañías tecnológicas más grandes compiten para impulsar estos experimentos gigantes en inteligencia artificial hacia los siguientes niveles, en los que igualan y luego superan todo el rango de capacidades, experiencia y autonomía humanas.

*Esto es inminente.* Durante los últimos diez años, las estimaciones de expertos sobre cuánto tiempo tomará esto – si continuamos nuestro curso actual – han caído de décadas (o siglos) a años de un solo dígito.

También es de importancia trascendental y de riesgo supremo. Los proponentes de la IAG la ven como una transformación positiva que resolverá problemas científicos, curará enfermedades, desarrollará nuevas tecnologías y automatizará el trabajo tedioso. Y la IA ciertamente podría ayudar a lograr todas estas cosas – de hecho ya lo está haciendo. Pero a lo largo de las décadas, muchos pensadores cuidadosos, desde Alan Turing hasta Stephen Hawking y los actuales Geoffrey Hinton y Yoshua Bengio [^3] han emitido una advertencia severa: construir IA verdaderamente más inteligente que los humanos, general y autónoma, como mínimo trastornará completa e irrevocablemente a la sociedad, y como máximo resultará en la extinción humana.[^4]

La IA superinteligente se aproxima rápidamente en nuestro camino actual, pero está lejos de ser inevitable. Este ensayo es un argumento extendido sobre por qué y cómo deberíamos *cerrar las Puertas* a este futuro inhumano que se aproxima, y qué deberíamos hacer en su lugar.

[^1]: Este [gráfico](https://time.com/6300942/ai-progress-charts/) muestra un conjunto de tareas; muchas curvas similares podrían agregarse a esta gráfica. Este rápido progreso en IA especializada ha sorprendido incluso a expertos en el campo, con puntos de referencia siendo superados años antes de las predicciones.

[^2]: Deepmind, OpenAI, Anthropic y X.ai fueron todas fundadas con el objetivo específico de desarrollar IAG. Por ejemplo, la carta de OpenAI establece explícitamente su objetivo como desarrollar "inteligencia artificial general que beneficie a toda la humanidad," mientras que la misión de DeepMind es "resolver la inteligencia, y luego usar eso para resolver todo lo demás." Meta, Microsoft y otros ahora persiguen caminos sustancialmente similares. Meta ha dicho que [planea desarrollar IAG y lanzarla abiertamente.](https://www.forbes.com/sites/johnkoetsier/2024/01/18/zuckerberg-on-ai-meta-building-agi-for-everyone-and-open-sourcing-it/)

[^3]: Hinton y Bengio son dos de los investigadores de IA más citados, ambos han ganado el Nobel de la IA, el Premio Turing, y Hinton además ha ganado un Premio Nobel (en física).

[^4]: Construir algo de este riesgo, bajo incentivos comerciales y supervisión gubernamental casi nula, es completamente sin precedentes. ¡Ni siquiera hay controversia sobre el riesgo entre quienes lo están construyendo! Los líderes de Deepmind, OpenAI y Anthropic, entre muchos otros expertos, han firmado literalmente una [declaración](https://www.safe.ai/work/statement-on-ai-risk) que dice que la IA avanzada presenta un *riesgo de extinción para la humanidad.* Las alarmas no podrían estar sonando más fuerte, y uno solo puede concluir que quienes las ignoran simplemente no se están tomando en serio la IAG y la superinteligencia. Un objetivo de este ensayo es ayudarlos a entender por qué deberían hacerlo.