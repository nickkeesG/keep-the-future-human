# Apéndices

Información suplementaria, incluyendo detalles técnicos sobre contabilidad de cómputo, un ejemplo de implementación de un 'cierre de puertas', detalles para un régimen estricto de responsabilidad legal de IAG, y un enfoque escalonado para estándares de seguridad y protección de IAG.

## Apéndice A: Detalles técnicos de contabilidad de cómputo

Se requiere un método detallado tanto para la "fuente de verdad" como para buenas aproximaciones del cómputo total utilizado en entrenamiento e inferencia para lograr controles basados en cómputo con significado real. He aquí un ejemplo de cómo podría contabilizarse la "fuente de verdad" a nivel técnico.

**Definiciones:**

*Grafo causal de cómputo:* Para una salida O dada de un modelo de IA, existe un conjunto de computaciones digitales para las cuales cambiar el resultado de esa computación podría potencialmente cambiar O. (Esto debe asumirse de forma conservadora, es decir, debe haber una razón clara para creer que una computación es independiente de un precursor que tanto ocurre antes en el tiempo como tiene una ruta causal física potencial de efecto.) Esto incluye la computación realizada por el modelo de IA durante la inferencia, así como las computaciones que fueron parte de la entrada, preparación de datos y entrenamiento del modelo. Debido a que cualquiera de estas puede ser en sí misma la salida de un modelo de IA, esto se calcula recursivamente, cortando donde un humano ha proporcionado un cambio significativo a la entrada.

*Cómputo de Entrenamiento:* El cómputo total, en FLOP u otras unidades, implicado por el grafo causal de cómputo de una red neuronal (incluyendo preparación de datos, entrenamiento y ajuste fino, y cualquier otra computación.)

*Cómputo de Salida:* El cómputo total en el grafo causal de cómputo de una salida de IA dada, incluyendo todas las redes neuronales (e incluyendo su Cómputo de Entrenamiento) y otras computaciones que contribuyen a esa salida.

*Tasa de Cómputo de Inferencia:* En una serie de salidas, la tasa de cambio (en FLOP/s u otras unidades) del Cómputo de Salida entre salidas, es decir, el cómputo utilizado para producir la siguiente salida, dividido por el intervalo temporal entre las salidas.

**Ejemplos y aproximaciones:**

- Para una sola red neuronal entrenada con datos creados por humanos, el Cómputo de Entrenamiento es simplemente el cómputo total de entrenamiento como se reporta habitualmente.
- Para tal red neuronal realizando inferencia a una tasa constante, la Tasa de Cómputo de Inferencia es aproximadamente la velocidad total del clúster de computación que realiza la inferencia en FLOP/s.
- Para el ajuste fino de modelos, el Cómputo de Entrenamiento del modelo completo se obtiene sumando el Cómputo de Entrenamiento del modelo no ajustado más la computación realizada durante el ajuste fino y para preparar cualquier dato utilizado en el ajuste fino.
- Para un modelo destilado, el Cómputo de Entrenamiento del modelo completo incluye el entrenamiento tanto del modelo destilado como del modelo más grande utilizado para proporcionar datos sintéticos u otra entrada de entrenamiento.
- Si se entrenan varios modelos, pero muchos "intentos" se descartan basándose en juicio humano, estos no cuentan hacia el Cómputo de Entrenamiento o de Salida del modelo retenido.

## Apéndice B: Ejemplo de implementación de un cierre de puertas

**Ejemplo de Implementación:** He aquí un ejemplo de cómo podría funcionar un cierre de puertas, dado un límite de 10<sup>27</sup> FLOP para entrenamiento y 10<sup>20</sup> FLOP/s para inferencia (ejecutar la IA):

**1\. Pausa:** Por razones de seguridad nacional, el poder ejecutivo de EE.UU. solicita a todas las empresas con sede en EE.UU., que hagan negocios en EE.UU., o que usen chips fabricados en EE.UU., cesar y desistir de cualquier nueva ejecución de entrenamiento de IA que pueda exceder el límite de 10<sup>27</sup> FLOP de Cómputo de Entrenamiento. EE.UU. debe iniciar discusiones con otros países que albergan desarrollo de IA, alentándolos enérgicamente a tomar medidas similares e indicando que la pausa estadounidense puede ser levantada si eligen no cumplir.

**2\. Supervisión y licencias de EE.UU.:** Por orden ejecutiva o acción de una agencia regulatoria existente, EE.UU. requiere que dentro de (digamos) un año:

- Todas las ejecuciones de entrenamiento de IA estimadas por encima de 10<sup>25</sup> FLOP realizadas por empresas que operan en EE.UU. sean registradas en una base de datos mantenida por una agencia regulatoria estadounidense. (Nota: Una versión ligeramente más débil de esto ya había sido incluida en la orden ejecutiva estadounidense sobre IA de 2023, ahora rescindida, requiriendo registro para modelos por encima de 10<sup>26</sup> FLOP.)
- Todos los fabricantes de hardware relevante para IA que operan en EE.UU. o hacen negocios con el gobierno estadounidense adhieran a un conjunto de requisitos sobre su hardware especializado y el software que lo maneja. (Muchos de estos requisitos podrían incorporarse en actualizaciones de software y firmware para hardware existente, pero las soluciones a largo plazo y robustas requerirían cambios en generaciones posteriores de hardware.) Entre estos está el requisito de que si el hardware es parte de un clúster interconectado de alta velocidad capaz de ejecutar 10<sup>18</sup> FLOP/s de computación, se requiere un nivel más alto de verificación, que incluye permisos regulares por un "gobernador" remoto que recibe tanto telemetría como solicitudes para realizar computación adicional.
- El custodio reporte la computación total realizada en su hardware a la agencia que mantiene la base de datos estadounidense.
- Se implementan gradualmente requisitos más estrictos para permitir una supervisión y permisos tanto más seguros como más flexibles.

**3\. Supervisión internacional:**

- EE.UU., China y cualquier otro país que albergue capacidad avanzada de fabricación de chips negocian un acuerdo internacional.
- Este acuerdo crea una nueva agencia internacional, análoga a la Agencia Internacional de Energía Atómica, encargada de supervisar el entrenamiento y ejecución de IA.
- Los países signatarios deben requerir que sus fabricantes domésticos de hardware de IA cumplan con un conjunto de requisitos al menos tan estrictos como los impuestos en EE.UU.
- Los custodios ahora deben reportar números de computación de IA tanto a las agencias en sus países de origen como a una nueva oficina dentro de la agencia internacional.
- Se alienta enérgicamente a países adicionales a unirse al acuerdo internacional existente: los controles de exportación por parte de países signatarios restringen el acceso a hardware de alta gama por no signatarios, mientras que los signatarios pueden recibir apoyo técnico en el manejo de sus sistemas de IA.

**4\. Verificación y cumplimiento internacional:**

- El sistema de verificación de hardware se actualiza para que reporte el uso de computación tanto al custodio original como también directamente a la oficina de la agencia internacional.
- La agencia, mediante discusión con los signatarios del acuerdo internacional, acuerda limitaciones de computación que luego adquieren fuerza legal en los países signatarios.
- En paralelo, puede desarrollarse un conjunto de estándares internacionales para que el entrenamiento y ejecución de IAs por encima de un umbral de computación (pero por debajo del límite) deban adherirse a esos estándares.
- La agencia puede, si es necesario para compensar mejores algoritmos etc., reducir el límite de computación. O, si se considera seguro y aconsejable (al nivel de garantías de seguridad demostrables), elevar el límite de computación.

## Apéndice C: Detalles para un régimen estricto de responsabilidad legal de IAG

**Detalles para un régimen estricto de responsabilidad legal de IAG**

- La creación y operación de un sistema de IA avanzado que es altamente general, capaz y autónomo, se considera una actividad "anormalmente peligrosa".
- Como tal, la responsabilidad legal por defecto para entrenar y operar tales sistemas es responsabilidad legal estricta, conjunta y solidaria (o su equivalente fuera de EE.UU.) por cualquier daño causado por el modelo o sus salidas/acciones.
- Se impondrá responsabilidad personal para ejecutivos y miembros de juntas directivas en casos de negligencia grave o conducta dolosa. Esto debe incluir sanciones penales para los casos más atroces.
- Existen numerosos refugios seguros bajo los cuales la responsabilidad legal revierte a la responsabilidad por defecto (basada en culpa, en EE.UU.) a la cual las personas y empresas normalmente estarían sujetas.
	- Modelos entrenados y operados por debajo de algún umbral de cómputo (que sería al menos 10x menor que los límites descritos arriba.)
	- IA que es "débil" (aproximadamente, por debajo del nivel de experto humano en las tareas para las cuales está destinada) y/o
	- IA que es "estrecha" (que tiene un alcance fijo y bastante limitado de tareas y operaciones para las cuales está específicamente diseñada y entrenada) y/o
	- IA que es "pasiva" (muy limitada en su capacidad – incluso bajo modificación modesta – para tomar acciones o realizar tareas complejas de múltiples pasos sin participación y control humano directo.)
	- Una IA que está garantizada de ser segura, protegida y controlable (demostrable mente segura, o un análisis de riesgo indica un nivel insignificante de daño esperado.)
- Los refugios seguros pueden reclamarse sobre la base de un [caso de seguridad](https://arxiv.org/abs/2410.21572) preparado por el desarrollador de IA y aprobado por una agencia o auditor acreditado por una agencia. Para reclamar un refugio seguro basado en cómputo, el desarrollador debe solo suministrar estimaciones creíbles del Cómputo de Entrenamiento total y Tasa de Inferencia máxima
- La legislación delinearía explícitamente situaciones bajo las cuales la medida cautelar del desarrollo de sistemas de IA con alto riesgo de daño público sería apropiada.
- Los consorcios de empresas, trabajando con ONGs y agencias gubernamentales, deben desarrollar estándares y normas que definan estos términos, cómo los reguladores deben otorgar refugios seguros, cómo los desarrolladores de IA deben desarrollar casos de seguridad, y cómo las cortes deben interpretar la responsabilidad legal donde los refugios seguros no se reclaman proactivamente.

## Apéndice D: Un enfoque escalonado para estándares de seguridad y protección de IAG

**Un enfoque escalonado para estándares de seguridad y protección de IAG**

| Nivel de Riesgo | Disparador(es) | Requisitos para entrenamiento | Requisito para implementación |
| --- | --- | --- | --- |
| NR-0 | IA débil en autonomía, generalidad e inteligencia | ninguno | ninguno |
| NR-1 | IA fuerte en uno de autonomía, generalidad e inteligencia | ninguno | Basado en riesgo y uso, potencialmente casos de seguridad aprobados por autoridades nacionales donde quiera que el modelo pueda ser usado |
| NR-2 | IA fuerte en dos de autonomía, generalidad e inteligencia | Registro con autoridad nacional con jurisdicción sobre el desarrollador | Caso de seguridad que acote el riesgo de daño mayor por debajo de niveles autorizados más auditorías de seguridad independientes (incluyendo redteaming de caja negra y caja blanca) aprobadas por autoridades nacionales donde quiera que el modelo pueda ser usado |
| NR-3 | IAG fuerte en autonomía, generalidad e inteligencia | Pre-aprobación de plan de seguridad y protección por autoridad nacional con jurisdicción sobre el desarrollador | Caso de seguridad que garantice riesgo acotado de daño mayor por debajo de niveles autorizados así como especificaciones requeridas, incluyendo ciberseguridad, controlabilidad, un interruptor de emergencia no removible, alineación con valores humanos, y robustez ante uso malicioso. |
| NR-4 | Cualquier modelo que también exceda 10<sup>27</sup> FLOP de Entrenamiento o 10<sup>20</sup> FLOP/s de Inferencia | Prohibido pendiente levantamiento acordado internacionalmente del límite de cómputo | Prohibido pendiente levantamiento acordado internacionalmente del límite de cómputo |

Clasificaciones de riesgo y estándares de seguridad/protección, con niveles basados en umbrales de cómputo así como combinaciones de alta autonomía, generalidad e inteligencia:

- *Autonomía fuerte* aplica si el sistema es capaz de realizar, o puede ser fácilmente modificado para realizar, tareas de múltiples pasos y/o tomar acciones complejas que son relevantes al mundo real, sin supervisión o intervención humana significativa. Ejemplos: vehículos autónomos y robots; bots de comercio financiero. No ejemplos: GPT-4; clasificadores de imágenes
- *Generalidad fuerte* indica un amplio alcance de aplicación, desempeño de tareas para las cuales el modelo no fue deliberada y específicamente entrenado, y capacidad significativa para aprender nuevas tareas. Ejemplos: GPT-4; mu-zero. No ejemplos: AlphaFold; vehículos autónomos; generadores de imágenes
- *Inteligencia fuerte* corresponde a igualar el desempeño a nivel de experto humano en las tareas en las cuales el modelo se desempeña mejor (y para un modelo general, a través de un rango amplio de tareas.) Ejemplos: AlphaFold; mu-zero; o3. No ejemplos: GPT-4; Siri