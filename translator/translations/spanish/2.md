# Capítulo 2 - Aspectos esenciales de las redes neuronales de IA

¿Cómo funcionan los sistemas de IA modernos y qué podríamos esperar de la próxima generación de IAs?

Para entender cómo se desarrollarán las consecuencias de crear IA más poderosa, es fundamental interiorizar algunos conceptos básicos. Esta sección y las dos siguientes los desarrollan, cubriendo sucesivamente qué es la IA moderna, cómo aprovecha cálculos masivos, y los sentidos en que está creciendo rápidamente en generalidad y capacidad.[^1]

Hay muchas formas de definir la inteligencia artificial, pero para nuestros propósitos la propiedad clave de la IA es que mientras un programa de computadora estándar es una lista de instrucciones sobre cómo realizar una tarea, un sistema de IA es uno que aprende de datos o experiencia para realizar tareas *sin que se le diga explícitamente cómo hacerlo.*

Casi toda la IA moderna relevante se basa en redes neuronales. Estas son estructuras matemáticas/computacionales, representadas por un conjunto muy grande (miles de millones o billones) de números ("pesos"), que realizan bien una tarea de entrenamiento. Estos pesos se elaboran (o quizás se "cultivan" o "encuentran") ajustándolos iterativamente para que la red neuronal mejore una puntuación numérica (también conocida como "pérdida") definida para desempeñarse bien en una o más tareas.[^2] Este proceso se conoce como *entrenar* la red neuronal.[^3]

Hay muchas técnicas para realizar este entrenamiento, pero esos detalles son mucho menos relevantes que las formas en que se define la puntuación, y cómo estas resultan en diferentes tareas que la red neuronal realiza bien. Históricamente se ha trazado una diferencia clave entre IA "limitada" y "general".

La IA limitada se entrena deliberadamente para hacer una tarea particular o un pequeño conjunto de tareas (como reconocer imágenes o jugar ajedrez); requiere reentrenamiento para nuevas tareas, y tiene un alcance limitado de capacidad. Tenemos IA limitada sobrehumana, lo que significa que para casi cualquier tarea discreta bien definida que una persona puede hacer, probablemente podemos construir una puntuación y luego entrenar exitosamente un sistema de IA limitada para hacerlo mejor que un humano.

Los sistemas de IA de propósito general (IAPG) pueden realizar una amplia gama de tareas, incluyendo muchas para las que no fueron explícitamente entrenados; también pueden aprender nuevas tareas como parte de su operación. Los actuales "modelos multimodales" grandes[^4] como ChatGPT ejemplifican esto: entrenados en un corpus muy grande de texto e imágenes, pueden participar en razonamiento complejo, escribir código, analizar imágenes y asistir con una vasta gama de tareas intelectuales. Aunque siguen siendo bastante diferentes de la inteligencia humana en formas que veremos en profundidad más adelante, su generalidad ha causado una revolución en la IA.[^5]

## Impredecibilidad: una característica clave de los sistemas de IA

Una diferencia clave entre los sistemas de IA y el software convencional está en la predecibilidad. La salida del software estándar puede ser impredecible – de hecho a veces por eso escribimos software, para obtener resultados que no podríamos haber predicho. Pero el software convencional rara vez hace algo para lo que no fue programado – su alcance y comportamiento son generalmente según lo diseñado. Un programa de ajedrez de primer nivel puede hacer movimientos que ningún humano podría predecir (¡o de lo contrario podrían vencer a ese programa de ajedrez!) pero generalmente no hará nada más que jugar ajedrez.

Como el software convencional, la IA limitada tiene alcance y comportamiento predecibles pero puede tener resultados impredecibles. Esto es realmente solo otra forma de definir IA limitada: como IA que es similar al software convencional en su predecibilidad y rango de operación.

La IA de propósito general es diferente: su alcance (los dominios sobre los que se aplica), comportamiento (los tipos de cosas que hace), y resultados (sus salidas reales) pueden ser todos impredecibles.[^6] GPT-4 fue entrenado solo para generar texto con precisión, pero desarrolló muchas capacidades que sus entrenadores no predijeron ni intentaron. Esta impredecibilidad surge de la complejidad del entrenamiento: porque los datos de entrenamiento contienen salidas de muchas tareas diferentes, la IA debe efectivamente aprender a realizar estas tareas para predecir bien.

Esta impredecibilidad de los sistemas de IA general es bastante fundamental. Aunque en principio es posible construir cuidadosamente sistemas de IA que tengan límites garantizados en su comportamiento (como se menciona más adelante en el ensayo), la forma en que se crean los sistemas de IA ahora los hace impredecibles en la práctica e incluso en principio.

## IA pasiva, agentes, sistemas autónomos y alineación

Esta impredecibilidad se vuelve particularmente importante cuando consideramos cómo se despliegan y usan realmente los sistemas de IA para lograr varios objetivos.

Muchos sistemas de IA son relativamente pasivos en el sentido de que principalmente proporcionan información, y el usuario toma acciones. Otros, comúnmente denominados *agentes*, toman acciones por sí mismos, con niveles variables de participación del usuario. Aquellos que toman acciones con relativamente menos entrada o supervisión externa pueden denominarse más *autónomos*. Esto forma un espectro en términos de independencia de acción, desde herramientas pasivas hasta agentes autónomos.[^7]

En cuanto a los objetivos de los sistemas de IA, estos pueden estar directamente ligados a su objetivo de entrenamiento (por ejemplo, el objetivo de "ganar" para un sistema que juega Go también es explícitamente para lo que fue entrenado). O pueden no estarlo: el objetivo de entrenamiento de ChatGPT es en parte predecir texto, en parte ser un asistente útil. Pero al hacer una tarea dada, su objetivo le es suministrado por el usuario. Los objetivos también pueden ser creados por un sistema de IA por sí mismo, solo muy indirectamente relacionados con su objetivo de entrenamiento.[^8]

Los objetivos están estrechamente ligados a la cuestión de la "alineación", es decir, la cuestión de si los sistemas de IA *harán lo que queremos que hagan*. Esta pregunta simple oculta un enorme nivel de sutileza.[^9] Por ahora, nótese que "queremos" en esta oración podría referirse a muchas personas y grupos diferentes, llevando a diferentes tipos de alineación. Por ejemplo, una IA podría ser altamente *obediente* (o ["leal"](https://arxiv.org/abs/2003.11157)) a su usuario – aquí "queremos" es "cada uno de nosotros". O podría ser más *soberana*, siendo impulsada principalmente por sus propios objetivos y limitaciones, pero aún actuando ampliamente en el interés común del bienestar humano – "queremos" es entonces "la humanidad" o "la sociedad". En el medio hay un espectro donde una IA sería ampliamente obediente, pero podría rehusarse a tomar acciones que dañen a otros o a la sociedad, violen la ley, etc.

Estos dos ejes – nivel de autonomía y tipo de alineación – no son completamente independientes. Por ejemplo, un sistema pasivo soberano, aunque no es completamente autocontradictorio, es un concepto en tensión, como lo es un agente autónomo obediente.[^10] Hay un claro sentido en que la autonomía y la soberanía tienden a ir de la mano. En una vena similar, la predecibilidad tiende a ser mayor en sistemas de IA "pasivos" y "obedientes", mientras que los soberanos o autónomos tenderán a ser más impredecibles. Todo esto será crucial para entender las ramificaciones de una IAG potencial y superinteligencia.

Crear IA verdaderamente alineada, de cualquier tipo, requiere resolver tres desafíos distintos:

1. Entender qué "queremos" – lo cual es complejo ya sea que "queremos" signifique una persona u organización específica (lealtad) o la humanidad ampliamente (soberanía);
2. Construir sistemas que actúen regularmente de acuerdo con esos deseos – esencialmente crear comportamiento positivo consistente;
3. Más fundamentalmente, hacer sistemas que genuinamente "se preocupen" por esos deseos en lugar de meramente actuar como si lo hicieran.

La distinción entre comportamiento confiable y cuidado genuino es crucial. Así como un empleado humano podría seguir órdenes perfectamente mientras carece de cualquier compromiso real con la misión de la organización, un sistema de IA podría actuar alineado sin valorar verdaderamente las preferencias humanas. Podemos entrenar sistemas de IA para decir y hacer cosas a través de retroalimentación, y pueden aprender a razonar sobre lo que los humanos quieren. Pero hacer que *genuinamente* valoren las preferencias humanas es un desafío mucho más profundo.[^11]

Las profundas dificultades para resolver estos desafíos de alineación, y sus implicaciones para el riesgo de IA, se explorarán más adelante. Por ahora, entiéndase que la alineación no es solo una característica técnica que agregamos a los sistemas de IA, sino un aspecto fundamental de su arquitectura que da forma a su relación con la humanidad.


[^1]: Para una introducción suave pero técnica al aprendizaje automático e IA, particularmente modelos de lenguaje, ver [este sitio.](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e) Para otro manual moderno sobre riesgos de extinción por IA, ver [esta pieza.](https://www.thecompendium.ai/) Para un análisis científico integral y autorizado del estado de la seguridad de IA, ver el reciente [Reporte Internacional de Seguridad de IA.](https://arxiv.org/abs/2501.17805)

[^2]: El entrenamiento típicamente ocurre buscando un máximo local de la puntuación en un espacio de alta dimensión dado por los pesos del modelo. Al verificar cómo cambia la puntuación cuando los pesos se ajustan, el algoritmo de entrenamiento identifica qué ajustes mejoran más la puntuación, y mueve los pesos en esa dirección.

[^3]: Por ejemplo, en un problema de reconocimiento de imágenes, la red neuronal produciría probabilidades para etiquetas de la imagen. Una puntuación estaría relacionada con la probabilidad que la IA asigna a la respuesta correcta. El procedimiento de entrenamiento entonces ajustaría los pesos para que la próxima vez, la IA produzca una probabilidad mayor para la etiqueta correcta para esa imagen. Esto se repite entonces un número enorme de veces. El mismo procedimiento básico se usa en entrenar esencialmente todas las redes neuronales modernas, aunque con mecanismos de puntuación más complejos.

[^4]: La mayoría de los modelos multimodales usan la arquitectura "transformer" para procesar y generar múltiples tipos de datos (texto, imágenes, sonido). Todos estos pueden descomponerse en, y luego tratarse en el mismo plano, como diferentes tipos de "tokens". Los modelos multimodales se entrenan primero para predecir tokens con precisión dentro de conjuntos de datos masivos, luego se refinan a través de aprendizaje por refuerzo para mejorar capacidades y dar forma a comportamientos.

[^5]: Que los modelos de lenguaje se entrenan para hacer una cosa – predecir palabras – ha causado que algunos los llamen IA limitada. Pero esto es engañoso: porque predecir texto bien requiere tantas capacidades diferentes, esta tarea de entrenamiento lleva a un sistema sorprendentemente general. También nótese que estos sistemas se entrenan extensivamente por aprendizaje por refuerzo, efectivamente representando miles de personas dando al modelo una señal de recompensa cuando hace un buen trabajo en cualquiera de las muchas cosas que hace. Entonces hereda generalidad significativa de las personas que dan esta retroalimentación.

[^6]: Hay múltiples formas en que la IA es impredecible. Una es que en el caso general uno no puede predecir qué hará un algoritmo sin realmente ejecutarlo; hay [teoremas](https://arxiv.org/abs/1310.3225) a este efecto. Esto puede ser cierto solo porque la salida de algoritmos puede ser compleja. Pero es particularmente claro y relevante en el caso (como en ajedrez o Go) donde la predicción implicaría una capacidad (vencer a la IA) que el predictor potencial no tiene. Segundo, un sistema de IA dado no siempre producirá la misma salida incluso dada la misma entrada – sus salidas contienen aleatoriedad; esto también se acopla con impredecibilidad algorítmica. Tercero, capacidades inesperadas y emergentes pueden surgir del entrenamiento, significando que incluso los *tipos* de cosas que un sistema de IA puede y hará son impredecibles; Este último tipo es particularmente importante para consideraciones de seguridad.

[^7]: Ver [aquí](https://arxiv.org/abs/2502.02649) para una revisión en profundidad de qué se entiende por un "agente autónomo" (junto con argumentos éticos contra construirlos).

[^8]: Puedes escuchar a veces "la IA no puede tener sus propios objetivos". Esto es una completa tontería. Es fácil generar ejemplos donde la IA tiene o desarrolla objetivos que nunca le fueron dados y son conocidos solo por ella misma. No ves esto mucho en los modelos multimodales populares actuales porque se les entrena para que no lo hagan; podría entrenárseles igual de fácil para que lo hagan.

[^9]: Hay una amplia literatura. Sobre el problema general ver *The Alignment Problem* [*El Problema de Alineación*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821) de Christian, y *Human-Compatible* [*Compatible con Humanos*](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) de Russell. En un lado más técnico ver por ejemplo [este artículo](https://arxiv.org/abs/2209.00626).

[^10]: Veremos más tarde que aunque tales sistemas van contra la tendencia, eso realmente los hace muy interesantes y útiles.

[^11]: Esto no es decir que requerimos emociones o sensibilidad. Más bien, es enormemente difícil desde fuera de un sistema saber cuáles son sus objetivos internos, preferencias y valores. "Genuino" aquí significaría que tenemos razón lo suficientemente fuerte para depender de ello que en el caso de sistemas críticos podemos apostar nuestras vidas en ello.