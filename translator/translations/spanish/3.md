# Capítulo 3 - Aspectos clave de cómo se crean los sistemas modernos de IA general

La mayoría de los sistemas de IA más avanzados del mundo se crean utilizando métodos sorprendentemente similares. Estos son los fundamentos.

Para comprender realmente a un ser humano necesitas saber algo sobre biología, evolución, crianza infantil y más; para entender la IA también necesitas saber cómo se crea. En los últimos cinco años, los sistemas de IA han evolucionado tremendamente tanto en capacidad como en complejidad. Un factor clave que lo ha hecho posible ha sido la disponibilidad de cantidades muy grandes de computación (o coloquialmente "cómputo" cuando se aplica a la IA).

Las cifras son asombrosas. Aproximadamente 10<sup>25</sup>-10<sup>26</sup> "operaciones de punto flotante" (FLOP)[^1] se utilizan en el entrenamiento de modelos como la serie GPT, Claude, Gemini, etc.[^2] (Para comparar, si cada ser humano en la Tierra trabajara sin parar haciendo un cálculo cada cinco segundos, tomaría alrededor de mil millones de años lograr esto.) Esta enorme cantidad de computación permite el entrenamiento de modelos con hasta billones de pesos del modelo sobre terabytes de datos: una gran fracción de todo el texto de calidad que jamás se haya escrito junto con grandes bibliotecas de sonidos, imágenes y video. Complementando este entrenamiento con entrenamiento adicional extenso que refuerza las preferencias humanas y el buen rendimiento en tareas, los modelos entrenados de esta manera exhiben rendimiento competitivo con los humanos a través de un rango significativo de tareas intelectuales básicas, incluyendo razonamiento y resolución de problemas.

También sabemos (de manera muy, muy aproximada) cuánta velocidad de computación, en operaciones por segundo, es suficiente para que la velocidad de *inferencia*[^3] de tal sistema iguale la *velocidad* del procesamiento de texto humano. Son aproximadamente 10<sup>15</sup>-10<sup>16</sup> FLOP por segundo.[^4]

Aunque poderosos, estos modelos están por su naturaleza limitados en formas clave, bastante análogas a cómo un humano individual estaría limitado si se viera forzado a simplemente producir texto a una tasa fija de palabras por minuto, sin detenerse a pensar o usar herramientas adicionales. Los sistemas de IA más recientes abordan estas limitaciones a través de un proceso y arquitectura más compleja que combina varios elementos clave:

- Una o más redes neuronales, con un modelo proporcionando la capacidad cognitiva central, y hasta varios otros realizando otras tareas más específicas;
- *Herramientas* proporcionadas y utilizables por el modelo: por ejemplo, la capacidad de buscar en la web, crear o editar documentos, ejecutar programas, etc.
- *Andamiaje* que conecta las entradas y salidas de las redes neuronales. Un andamio muy simple podría permitir que dos "instancias" de un modelo de IA conversen entre sí, o que una verifique el trabajo de otra.[^5]
- Las técnicas de *cadena de razonamiento* y prompts relacionados hacen algo similar, causando que un modelo genere, por ejemplo, muchos enfoques para un problema, luego procese esos enfoques para una respuesta agregada.
- *Reentrenamiento* de modelos para hacer mejor uso de herramientas, andamiaje y cadena de razonamiento.

Debido a que estas extensiones pueden ser muy poderosas (e incluir los propios sistemas de IA), estos sistemas compuestos pueden ser bastante sofisticados y mejorar dramáticamente las capacidades de IA.[^6] Y recientemente, las técnicas de andamiaje y especialmente los prompts de cadena de razonamiento (e incorporar los resultados de vuelta al reentrenamiento de modelos para usar estos mejor) se han desarrollado y empleado en [o1](https://openai.com/o1/), [o3](https://openai.com/index/openai-o3-mini/), y [DeepSeek R1](https://api-docs.deepseek.com/news/news250120) para hacer muchas pasadas de inferencia en respuesta a una consulta dada.[^7] Esto en efecto permite al modelo "pensar sobre" su respuesta y aumenta dramáticamente la capacidad de estos modelos para hacer razonamiento de alto calibre en tareas de ciencia, matemáticas y programación.[^8]

Para una arquitectura de IA dada, los aumentos en computación de entrenamiento [pueden traducirse confiablemente](https://arxiv.org/abs/2405.10938) en mejoras en un conjunto de métricas claramente definidas. Para capacidades generales definidas de manera menos precisa (como las discutidas a continuación), la traducción es menos clara y predictiva, pero es casi seguro que modelos más grandes con más computación de entrenamiento tendrán capacidades nuevas y mejores, incluso si es difícil predecir cuáles serán.

De manera similar, los sistemas compuestos y especialmente los avances en "cadena de razonamiento" (y entrenamiento de modelos que funcionan bien con ella) han desbloqueado el escalamiento en computación de *inferencia*: para un modelo central entrenado dado, al menos algunas capacidades del sistema de IA aumentan conforme se aplica más computación que les permite "pensar más duro y más tiempo" sobre problemas complejos. Esto viene con un costo alto en velocidad de computación, requiriendo cientos o miles de FLOP/s más para igualar el rendimiento humano.[^9]

Aunque es solo una parte de lo que está llevando al rápido progreso de la IA,[^10] el papel de la computación y la posibilidad de sistemas compuestos resultará crucial tanto para prevenir una IAG incontrolable como para desarrollar alternativas más seguras.

[^1]: 10<sup>27</sup> significa 1 seguido de 25 ceros, o diez billones de billones. Un FLOP es simplemente una suma o multiplicación aritmética de números con cierta precisión. Nota que el rendimiento del hardware de IA puede variar por un factor de diez más dependiendo de la precisión de la aritmética y la arquitectura de la computadora. Contar operaciones de compuertas lógicas (AND, OR, AND NOT) sería fundamental pero estas no están comúnmente disponibles o son evaluadas comparativamente; para propósitos presentes es útil estandarizar en operaciones de 16 bits (FP16), aunque deberían establecerse factores de conversión apropiados.

[^2]: Una colección de estimaciones y datos duros está disponible en [Epoch AI](https://epochai.org/data/large-scale-ai-models) e indica aproximadamente 2×10<sup>25</sup> FLOP de 16 bits para GPT-4; esto coincide aproximadamente con [números que fueron filtrados](https://mpost.io/gpt-4s-leaked-details-shed-light-on-its-massive-scale-and-impressive-architecture/) para GPT-4. Las estimaciones para otros modelos de mediados de 2024 están todas dentro de un factor de pocos de GPT-4.

[^3]: La inferencia es simplemente el proceso de generar una salida de una red neuronal. El entrenamiento puede considerarse una sucesión de muchas inferencias y ajustes de pesos del modelo.

[^4]: Para la producción de texto, el GPT-4 original requería 560 TFLOP por token generado. Alrededor de 7 tokens/s se necesitan para mantenerse al día con el pensamiento humano, así que esto da ≈3×10<sup>15</sup> FLOP/s. Pero las eficiencias han reducido esto; [este folleto de NVIDIA](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/) por ejemplo indica tan poco como 3×10<sup>14</sup> FLOP/s para un modelo Llama 405B de rendimiento comparable.

[^5]: Como un ejemplo ligeramente más complejo, un sistema de IA podría primero generar varias soluciones posibles a un problema matemático, luego usar otra instancia para verificar cada solución, y finalmente usar una tercera para sintetizar los resultados en una explicación clara. Esto permite una resolución de problemas más exhaustiva y confiable que una sola pasada.

[^6]: Ver por ejemplo detalles sobre el ["Operator" de OpenAI](https://openai.com/index/introducing-operator/), [capacidades de herramientas de Claude](https://docs.anthropic.com/en/docs/build-with-claude/computer-use), y [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). La [Deep Research](https://openai.com/index/introducing-deep-research/) de OpenAI probablemente tiene una arquitectura bastante sofisticada pero los detalles no están disponibles.

[^7]: Deepseek R1 se basa en entrenar y hacer prompts iterativamente al modelo para que el modelo entrenado final cree un razonamiento extenso de cadena de razonamiento. Los detalles arquitectónicos no están disponibles para o1 u o3, sin embargo Deepseek ha revelado que no se requiere ninguna "salsa especial" particular para desbloquear el escalamiento de capacidad con inferencia. Pero a pesar de recibir mucha atención en la prensa como trastornando el "statu quo" en IA, no impacta las afirmaciones centrales de este ensayo.

[^8]: Estos modelos superan significativamente a los modelos estándar en benchmarks de razonamiento. Por ejemplo, en el GPQA Diamond Benchmark—una prueba rigurosa de preguntas de ciencia a nivel de doctorado—GPT-4o [obtuvo](https://openai.com/index/learning-to-reason-with-llms/) 56%, mientras que o1 y o3 lograron 78% y 88%, respectivamente, superando por mucho el puntaje promedio del 70% de expertos humanos.

[^9]: El O3 de OpenAI probablemente gastó ∼10<sup>21</sup>-10<sup>22</sup> FLOP [para completar cada una de las preguntas del desafío ARC-AGI](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai), que humanos competentes pueden hacer en (digamos) 10-100 segundos, dando una cifra más parecida a ∼10<sup>20</sup> FLOP/s.

[^10]: Aunque la computación es una medida clave de la capacidad del sistema de IA, interactúa tanto con la calidad de los datos como con mejoras algorítmicas. Mejores datos o algoritmos pueden reducir los requisitos computacionales, mientras que más computación a veces puede compensar datos o algoritmos más débiles.