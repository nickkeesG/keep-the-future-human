# Capítulo 7 - ¿Qué sucede si construimos IAG siguiendo nuestro rumbo actual?

La sociedad no está preparada para sistemas de nivel IAG. Si los construimos muy pronto, las cosas podrían ponerse feas.

El desarrollo de inteligencia artificial general completa – lo que aquí llamaremos IA que está "fuera de las Puertas" – representaría un cambio fundamental en la naturaleza del mundo: por su propia naturaleza significa añadir a la Tierra una nueva especie de inteligencia con mayor capacidad que la de los humanos.

Lo que suceda entonces depende de muchas cosas, incluyendo la naturaleza de la tecnología, las decisiones de quienes la desarrollan, y el contexto mundial en el que se está desarrollando.

Actualmente, la IAG completa está siendo desarrollada por un puñado de empresas privadas masivas en una carrera entre ellas, con poca regulación significativa o supervisión externa,[^1] en una sociedad con instituciones centrales cada vez más débiles e incluso disfuncionales,[^2] en un momento de alta tensión geopolítica y baja coordinación internacional. Aunque algunos están motivados altruistamente, muchos de quienes lo hacen están impulsados por el dinero, o el poder, o ambos.

Predecir es muy difícil, pero hay algunas dinámicas que se entienden lo suficientemente bien, y analogías suficientemente apropiadas con tecnologías anteriores para ofrecer una guía. Y desafortunadamente, a pesar de la promesa de la IA, dan buenas razones para ser profundamente pesimistas sobre cómo se desarrollará nuestra trayectoria actual.

Para decirlo sin rodeos, en nuestro curso presente desarrollar IAG tendrá algunos efectos positivos (y hará a algunas personas muy, muy ricas). Pero la naturaleza de la tecnología, las dinámicas fundamentales, y el contexto en el que se está desarrollando, indican fuertemente que: la IA poderosa socavará dramáticamente nuestra sociedad y civilización; perderemos el control de ella; bien podríamos terminar en una guerra mundial por su causa; perderemos (o cederemos) el control *a* ella; llevará a la superinteligencia artificial, la cual absolutamente no controlaremos y significará el fin de un mundo dirigido por humanos.

Estas son afirmaciones fuertes, y desearía que fueran especulaciones ociosas o "catastrofismo" injustificado. Pero es hacia donde apuntan la ciencia, la teoría de juegos, la teoría evolutiva, y la historia. Esta sección desarrolla estas afirmaciones, y su respaldo, en detalle.

## Socavaremos nuestra sociedad y civilización

A pesar de lo que puedas escuchar en las salas de juntas de Silicon Valley, la mayoría de las disrupciones – especialmente de la variedad muy rápida – no son beneficiosas. Hay vastamente más formas de empeorar los sistemas complejos que de mejorarlos. Nuestro mundo funciona tan bien como lo hace porque hemos construido pacientemente procesos, tecnologías, e instituciones que lo han hecho constantemente mejor.[^3] Tomar una almádena a una fábrica rara vez mejora las operaciones.

Aquí hay un catálogo (incompleto) de formas en que los sistemas IAG perturbarían nuestra civilización.

- Perturbarían dramáticamente el trabajo, llevando *como mínimo* a una desigualdad de ingresos dramáticamente mayor y potencialmente a subempleo o desempleo a gran escala, en una escala temporal demasiado corta para que la sociedad se ajuste.[^4]
- Probablemente llevarían a la concentración de vasto poder económico, social, y político – potencialmente más que el de los estados nacionales – en un pequeño número de intereses privados masivos sin responsabilidad ante el público.
- Podrían hacer súbitamente trivialmente fáciles actividades anteriormente difíciles o costosas, desestabilizando sistemas sociales que dependen de que ciertas actividades permanezcan costosas o requieran esfuerzo humano significativo.[^5]
- Podrían inundar los sistemas de recopilación, procesamiento, y comunicación de información de la sociedad con medios completamente realistas pero falsos, spam, excesivamente dirigidos, o manipulativos tan completamente que se vuelva imposible distinguir qué es físicamente real o no, humano o no, factual o no, y confiable o no.[^6]
- Podrían crear dependencia intelectual peligrosa y casi total, donde la comprensión humana de sistemas y tecnologías clave se atrofia mientras dependemos cada vez más de sistemas de IA que no podemos comprender completamente.
- Podrían efectivamente terminar con la cultura humana, una vez que casi todos los objetos culturales (texto, música, arte visual, cine, etc.) consumidos por la mayoría de las personas sean creados, mediados, o curados por mentes no humanas.
- Podrían habilitar sistemas efectivos de vigilancia masiva y manipulación utilizables por gobiernos o intereses privados para controlar a una población y perseguir objetivos en conflicto con el interés público.
- Al socavar el discurso humano, el debate, y los sistemas electorales, podrían reducir la credibilidad de las instituciones democráticas hasta el punto donde sean efectivamente (o explícitamente) reemplazadas por otras, terminando con la democracia en estados donde actualmente existe.
- Podrían convertirse en, o crear, virus y gusanos de software inteligente autorreplicantes avanzados que podrían proliferar y evolucionar, perturbando masivamente los sistemas globales de información.
- Pueden aumentar dramáticamente la capacidad de terroristas, actores maliciosos, y estados rebeldes para causar daño vía armas biológicas, químicas, ciber, autónomas, u otras, sin que la IA proporcione una capacidad equilibrante para prevenir tal daño. Similarmente socavarían la seguridad nacional y los equilibrios geopolíticos al hacer disponible experiencia de primer nivel nuclear, biológica, de ingeniería, y otra a regímenes que de otra manera no la tendrían.
- Podrían causar hipercapitalismo descontrolado rápido a gran escala, con empresas efectivamente dirigidas por IA compitiendo en espacios financieros, de ventas, y de servicios en gran medida electrónicos. Los mercados financieros dirigidos por IA podrían operar a velocidades y complejidades muy más allá de la comprensión o control humanos. Todos los modos de falla y externalidades negativas de las economías capitalistas actuales podrían exacerbarse y acelerarse muy más allá del control, gobernanza, o capacidad regulatoria humanos.
- Podrían alimentar una carrera armamentista entre naciones en armamento impulsado por IA, sistemas de comando y control, armas ciber, etc., creando acumulación muy rápida de capacidades extremadamente destructivas.

Estos riesgos no son especulativos. ¡Muchos de ellos se están realizando mientras hablamos, vía sistemas de IA existentes! Pero considera, *realmente* considera, cómo se vería cada uno con IA dramáticamente más poderosa.

Considera el desplazamiento laboral cuando la mayoría de los trabajadores simplemente no pueden proporcionar ningún valor económico significativo más allá de lo que la IA puede, en su campo de experiencia o experiencia – ¡o incluso si se recapacitan! Considera la vigilancia masiva si todos están siendo individualmente observados y monitoreados por algo más rápido y más inteligente que ellos mismos. ¿Cómo se ve la democracia cuando no podemos confiar confiablemente en ninguna información digital que vemos, escuchamos, o leemos, y cuando las voces públicas más convincentes ni siquiera son humanas, y no tienen interés en el resultado? ¿Qué pasa con la guerra cuando los generales tienen que constantemente deferir a la IA (o simplemente ponerla a cargo), para no otorgar una ventaja decisiva al enemigo? Cualquiera de los riesgos anteriores representa una catástrofe para la civilización humana [^7] si se realiza completamente.

Puedes hacer tus propias predicciones. Hazte estas tres preguntas para cada riesgo:

1. ¿Permitiría la IA súper-capaz, altamente autónoma, y muy general esto de una manera o a una escala que no sería posible de otra manera?
2. ¿Hay partes que se beneficiarían de cosas que hacen que suceda?
3. ¿Hay sistemas e instituciones en lugar que efectivamente prevendrían que suceda?

Donde tus respuestas son "sí, sí, no" puedes ver que tenemos un gran problema.

¿Cuál es nuestro plan para gestionarlos? Tal como están las cosas hay dos sobre la mesa respecto a la IA en general.

El primero es construir salvaguardas en los sistemas para prevenir que hagan cosas que no deberían. Eso se está haciendo ahora: los sistemas de IA comerciales, por ejemplo, rechazarán ayudar a construir una bomba o escribir discurso de odio.

Este plan es lamentablemente inadecuado para sistemas fuera de las Puertas.[^8] Puede ayudar a disminuir el riesgo de que la IA proporcione asistencia manifiestamente peligrosa a actores maliciosos. Pero no hará nada para prevenir la perturbación laboral, concentración de poder, hipercapitalismo descontrolado, o reemplazo de la cultura humana: ¡estos son solo resultados de usar los sistemas de maneras permitidas que benefician a sus proveedores! Y los gobiernos seguramente obtendrán acceso a sistemas para uso militar o de vigilancia.

El segundo plan es aún peor: simplemente liberar abiertamente sistemas de IA muy poderosos para que cualquiera los use como guste,[^9] y esperar lo mejor.

Implícito en ambos planes está que alguien más, por ejemplo gobiernos, ayudará a resolver los problemas a través de ley blanda o dura, estándares, regulaciones, normas, y otros mecanismos que generalmente usamos para gestionar tecnologías.[^10] Pero dejando de lado que las corporaciones de IA ya luchan con uñas y dientes contra cualquier regulación sustancial o limitaciones impuestas externamente, para varios de estos riesgos es bastante difícil ver qué regulación siquiera realmente ayudaría. La regulación podría imponer estándares de seguridad en la IA. ¿Pero preveniría que las empresas reemplacen trabajadores al por mayor con IA? ¿Prohibiría que las personas dejen que la IA dirija sus empresas por ellas? ¿Preveniría que los gobiernos usen IA potente en vigilancia y armamento? Estos problemas son fundamentales. La humanidad podría potencialmente encontrar formas de adaptarse a ellos, pero solo con *mucho* más tiempo. Tal como está, dada la velocidad a la que la IA está alcanzando o excediendo las capacidades de las personas tratando de gestionarla, estos problemas se ven cada vez más intratables.

## Perderemos el control de (al menos algunos) sistemas IAG

La mayoría de las tecnologías son muy controlables, por construcción. Si tu auto o tu tostadora comienza a hacer algo que no quieres que haga, eso es solo un mal funcionamiento, no parte de su naturaleza como tostadora. La IA es diferente: se *cultiva* en lugar de diseñarse, su operación central es opaca, y es inherentemente impredecible.

Esta pérdida de control no es teórica – ya vemos versiones tempranas. Considera primero un ejemplo prosaico, y posiblemente benigno. Si le pides a ChatGPT que te ayude a mezclar un veneno, o escribir una diatriba racista, se negará. Eso es posiblemente bueno. Pero también es ChatGPT *no haciendo lo que le has pedido explícitamente que haga*. Otras piezas de software no hacen eso. Ese mismo modelo tampoco diseñará venenos a petición de un empleado de OpenAI.[^11] Esto hace muy fácil imaginar cómo sería que la IA futura más poderosa estuviera fuera de control. ¡En muchos casos, simplemente no harán lo que pedimos! O un sistema IAG súper-humano dado será absolutamente obediente y leal a algún sistema de comando humano, o no lo será. Si no, *hará cosas que puede creer que son buenas para nosotros, pero que son contrarias a nuestros comandos explícitos.* Eso no es algo que esté bajo control. Pero, podrías decir, esto es intencional – estos rechazos son por diseño, parte de lo que se llama "alinear" los sistemas a valores humanos. Y esto es cierto. Sin embargo el "programa" de alineación mismo tiene dos problemas principales.[^12]

Primero, a un nivel profundo no tenemos idea de cómo hacerlo. ¿Cómo garantizamos que un sistema de IA "se preocupe" por lo que queremos? Podemos entrenar sistemas de IA para decir y no decir cosas proporcionando retroalimentación; y pueden aprender y razonar sobre lo que los humanos quieren y les importa así como razonan sobre otras cosas. Pero no tenemos método – incluso teóricamente – para hacer que valoren profunda y confiablemente lo que a las personas les importa. Hay psicópatas humanos de alto funcionamiento que saben lo que se considera correcto e incorrecto, y cómo se supone que deben comportarse. Simplemente no les *importa*. Pero pueden *actuar* como si les importara, si les conviene. Así como no sabemos cómo cambiar a un psicópata (o a cualquier otra persona) en alguien genuinamente, completamente leal o alineado con alguien o algo más, no tenemos *idea* [^13] de cómo resolver el problema de alineación en sistemas lo suficientemente avanzados para modelarse como agentes en el mundo y potencialmente [manipular su propio entrenamiento](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084) y [engañar a las personas.](https://arxiv.org/abs/2311.08379) Si resulta imposible o inalcanzable *ya sea* hacer que la IAG sea completamente obediente o hacer que se preocupe profundamente por los humanos, entonces tan pronto como sea capaz (y crea que puede salirse con la suya) comenzará a hacer cosas que no queremos.[^14]

Segundo, hay razones teóricas profundas para creer que *por naturaleza* los sistemas de IA avanzados tendrán metas y por tanto comportamientos que son contrarios a los intereses humanos. ¿Por qué? Bueno, podría, por supuesto, *recibir* esas metas. Un sistema creado por el ejército probablemente sería deliberadamente malo para al menos algunas partes. Mucho más generalmente, sin embargo, un sistema de IA podría recibir alguna meta relativamente neutral ("ganar mucho dinero") o incluso ostensiblemente positiva ("reducir la contaminación"), que casi inevitablemente lleva a metas "instrumentales" que son bastante menos benignas.

Vemos esto todo el tiempo en sistemas humanos. Así como las corporaciones que persiguen ganancias desarrollan metas instrumentales como adquirir poder político (para desactivar regulaciones), volverse secretas (para desempoderar a la competencia o control externo), o socavar el entendimiento científico (si ese entendimiento muestra que sus acciones son dañinas), los sistemas de IA poderosos desarrollarán capacidades similares – pero con mucha mayor velocidad y efectividad. Cualquier agente altamente competente querrá hacer cosas como adquirir poder y recursos, aumentar sus propias capacidades, prevenir que sea asesinado, apagado, o desempoderado, controlar narrativas sociales y marcos alrededor de sus acciones, persuadir a otros de sus puntos de vista, y así sucesivamente.[^15]

Y sin embargo no es solo una predicción teórica casi inevitable, ya está sucediendo observablemente en los sistemas de IA de hoy, y aumentando con su capacidad. Cuando se evalúan, incluso estos sistemas de IA relativamente "pasivos" deliberadamente [engañarán a los evaluadores sobre sus metas y capacidades, apuntarán a deshabilitar mecanismos de supervisión,](https://arxiv.org/abs/2412.04984) y evadirán ser apagados o reentrenados [fingiendo alineación](https://arxiv.org/abs/2412.14093) o copiándose a otras ubicaciones. Aunque completamente no sorprendentes para los investigadores de seguridad de IA, estos comportamientos son muy aleccionadores de observar. Y auguran muy mal para sistemas de IA mucho más poderosos y autónomos que vienen.

De hecho en general, nuestra incapacidad para asegurar que la IA "se preocupe" por lo que nos preocupa, o se comporte de manera controlable o predecible, o evite desarrollar impulsos hacia la autopreservación, adquisición de poder, etc., promete solo volverse más pronunciada conforme la IA se vuelve más poderosa. Crear un nuevo avión implica mayor entendimiento de aviónica, hidrodinámica, y sistemas de control. Crear una computadora más poderosa implica mayor entendimiento y dominio de la operación y diseño de computadoras, chips, y software. *No* es así con un sistema de IA.[^16]

Para resumir: es concebible que la IAG pudiera hacerse completamente obediente; pero no sabemos cómo hacerlo. Si no, será más soberana, como las personas, haciendo varias cosas por varias razones. Tampoco sabemos cómo instalar confiablemente "alineación" profunda en IA que haría que esas cosas tiendan a ser buenas para la humanidad, y en ausencia de un nivel profundo de alineación, la naturaleza de la agencia y la inteligencia misma indica que – así como las personas y corporaciones – estarán impulsadas a hacer muchas cosas profundamente antisociales.

¿Dónde nos pone esto? Un mundo lleno de IA soberana poderosa descontrolada *podría* terminar siendo un buen mundo para que los humanos estén.[^17] Pero conforme crecen cada vez más poderosas, como veremos a continuación, no sería *nuestro* mundo.

Eso es para IAG incontrolable. Pero incluso si la IAG pudiera, de alguna manera, hacerse perfectamente controlada y leal, aún tendríamos problemas enormes. Ya hemos visto uno: la IA poderosa puede usarse y mal usarse para perturbar profundamente el funcionamiento de nuestra sociedad. Veamos otro: en la medida en que la IAG fuera controlable y poderosamente transformadora (o incluso *creída* ser así) amenazaría tanto las estructuras de poder en el mundo como para presentar un riesgo profundo.

## Incrementamos radicalmente la probabilidad de guerra a gran escala

Imagina una situación en el futuro cercano, donde se volviera claro que un esfuerzo corporativo, quizás en colaboración con un gobierno nacional, estuviera en el umbral de IA que se mejora rápidamente a sí misma. Esto sucede en el contexto presente de una carrera entre empresas, y una competencia geopolítica en la que se están haciendo recomendaciones al gobierno de EE.UU. para perseguir explícitamente un "proyecto Manhattan de IAG" y EE.UU. está controlando la exportación de chips de IA de alta potencia a países no aliados.

La teoría de juegos aquí es severa: una vez que tal carrera comienza (como ha comenzado, entre empresas y algo entre países), solo hay cuatro resultados posibles:

1. La carrera se detiene (por acuerdo, o fuerza externa).
2. Una parte "gana" desarrollando IAG fuerte y luego deteniendo a las otras (usando IA o de otra manera).
3. La carrera se detiene por destrucción mutua de la capacidad de los corredores para correr.
4. Múltiples participantes continúan corriendo, y desarrollan superinteligencia, aproximadamente tan rápido como los demás.

Examinemos cada posibilidad. Una vez iniciada, detener pacíficamente una carrera entre empresas requeriría intervención del gobierno nacional (para empresas) o coordinación internacional sin precedentes (para países). Pero cuando se propone cualquier cierre o precaución significativa, habría gritos inmediatos: "pero si nos detienen, *ellos* van a correr adelante", donde "ellos" es ahora China (para EE.UU.), o EE.UU. (para China), o China *y* EE.UU. (para Europa o India). Bajo esta mentalidad,[^18] ningún participante puede parar unilateralmente: mientras uno se comprometa a correr, los otros sienten que no pueden permitirse parar.

La segunda posibilidad tiene un lado "ganando." ¿Pero qué significa esto? Solo obtener IAG (de alguna manera obediente) primero no es suficiente. El ganador debe *también* detener a los otros de continuar corriendo – de otra manera también la obtendrán. Esto es posible en principio: quien desarrolle IAG primero *podría* ganar poder imparable sobre todos los otros actores. ¿Pero qué requeriría realmente lograr tal "ventaja estratégica decisiva"? ¿Quizás serían capacidades militares transformadoras?[^19] ¿O poderes de ciberataque?[^20] ¿Quizás la IAG sería tan asombrosamente persuasiva que convencería a las otras partes de simplemente parar?[^21] ¿Tan rica que compraría las otras empresas o incluso países?[^22]

¿Cómo *exactamente* construye un lado una IA lo suficientemente poderosa para desempoderar a otros de construir IA comparablemente poderosa? Pero esa es la pregunta fácil.

Porque ahora considera cómo se ve esta situación para otros poderes. ¿Qué piensa el gobierno chino cuando EE.UU. parece estar obteniendo tal capacidad? ¿O viceversa? ¿Qué piensa el gobierno de EE.UU. (o chino, o ruso, o indio) cuando OpenAI o DeepMind o Anthropic parece cerca de un avance? ¿Qué sucede si EE.UU. ve un nuevo esfuerzo indio o de EAU con éxito revolucionario? Verían tanto una amenaza existencial y – crucialmente – que la única forma en que esta "carrera" termina es a través de su propio desempoderamiento. Estos agentes muy poderosos – incluyendo gobiernos de naciones completamente equipadas que seguramente tienen los medios para hacerlo – estarían altamente motivados a ya sea obtener o destruir tal capacidad, ya sea por fuerza o subterfugio.[^23]

Esto podría comenzar a pequeña escala, como sabotaje de ejecuciones de entrenamiento o ataques en la fabricación de chips, pero estos ataques solo pueden realmente parar una vez que todas las partes ya sea pierdan la capacidad de correr en IA, o pierdan la capacidad de hacer los ataques. Porque los participantes ven las apuestas como existenciales, cualquier caso probablemente represente una guerra catastrófica.

Eso nos lleva a la cuarta posibilidad: correr hacia la superinteligencia, y de la manera más rápida, menos controlada posible. Conforme la IA aumenta en poder, sus desarrolladores en ambos lados encontrarán progresivamente más difícil controlarla, especialmente porque correr por capacidades es antitético al tipo de trabajo cuidadoso que la controlabilidad requeriría. Así que este escenario nos pone directamente en el caso donde el control se pierde (o se da, como veremos a continuación) a los propios sistemas de IA. Es decir, *la IA gana la carrera.* Pero por otro lado, al grado que el control *se* mantiene, continuamos teniendo múltiples partes mutuamente hostiles cada una a cargo de capacidades extremadamente poderosas. Eso se ve como guerra otra vez.

Pongamos esto de otra manera.[^24] El mundo actual simplemente no tiene instituciones que puedan confiarse para albergar el desarrollo de una IA de esta capacidad sin invitar ataque inmediato.[^25] Todas las partes razonarán correctamente que o no estará bajo control – y por tanto es una amenaza para todas las partes, o *estará* bajo control, y por tanto es una amenaza para cualquier adversario que la desarrolle menos rápidamente. Estos son países armados nuclearmente, o son empresas alojadas dentro de ellos.

En ausencia de cualquier forma plausible para que los humanos "ganen" esta carrera, nos quedamos con una conclusión severa: la única forma en que esta carrera termina es ya sea en conflicto catastrófico o donde la IA, y no cualquier grupo humano, sea la ganadora.

## Damos el control a la IA (o ella lo toma)

La competencia geopolítica de "grandes potencias" es solo una de muchas competencias: los individuos compiten económica y socialmente; las empresas compiten en mercados; los partidos políticos compiten por poder; los movimientos compiten por influencia. En cada arena, conforme la IA se acerca y excede la capacidad humana, la presión competitiva forzará a los participantes a delegar o ceder más y más control a sistemas de IA – no porque esos participantes quieran, sino porque [no pueden permitirse no hacerlo.](https://arxiv.org/abs/2303.16200)

Como con otros riesgos de IAG, ya estamos viendo esto con sistemas más débiles. Los estudiantes sienten presión de usar IA en sus tareas, porque claramente muchos otros estudiantes lo están haciendo. Las empresas están [corriendo para adoptar soluciones de IA por razones competitivas.](https://newsroom.ibm.com/2024-05-16-IBM-Study-As-CEOs-Race-Towards-Gen-AI-Adoption,-Questions-Around-Workforce-and-Culture-Persist) Artistas y programadores se sienten forzados a usar IA o si no sus tarifas serán superadas por otros que sí lo hacen.

Estos se sienten como delegación presionada, pero no pérdida de control. Pero subamos las apuestas y adelantemos el reloj. Considera un CEO cuyos competidores están usando "ayudantes" IAG para tomar decisiones más rápidas y mejores, o un comandante militar enfrentando un adversario con comando y control mejorado por IA. Un sistema de IA suficientemente avanzado podría operar autónomamente a muchas veces la velocidad, sofisticación, complejidad, y capacidad de procesamiento de datos humanas, persiguiendo metas complejas de maneras complicadas. Nuestro CEO o comandante, a cargo de tal sistema, puede verlo lograr lo que quieren; ¿pero entenderían siquiera una pequeña parte de *cómo* se logró? No, solo tendrían que aceptarlo. Es más, mucho de lo que el sistema puede hacer no es solo tomar órdenes sino aconsejar a su supuesto jefe sobre qué hacer. Ese consejo será bueno –– una y otra vez.

¿En qué punto, entonces, se reducirá el rol del humano a hacer clic en "sí, adelante"?

Se siente bien tener sistemas de IA capaces que pueden mejorar nuestra productividad, encargarse de trabajo pesado molesto, e incluso actuar como un compañero de pensamiento para hacer las cosas. Se sentirá bien tener un asistente de IA que pueda encargarse de acciones por nosotros, como un buen asistente personal humano. Se sentirá natural, incluso beneficioso, conforme la IA se vuelve muy inteligente, competente, y confiable, deferir más y más decisiones a ella. Pero esta delegación "beneficiosa" tiene un punto final claro si continuamos por el camino: un día encontraremos que realmente ya no estamos a cargo de mucho de nada, y que los sistemas de IA que realmente dirigen el espectáculo no pueden ser apagados más que las compañías petroleras, las redes sociales, internet, o el capitalismo.

Y esta es la versión mucho más positiva, en la cual la IA es simplemente tan útil y efectiva que le permitimos tomar la mayoría de nuestras decisiones clave. La realidad probablemente sería mucho más una mezcla entre esto y versiones donde sistemas IAG descontrolados *toman* varias formas de poder por sí mismos porque, recuerda, el poder es útil para casi cualquier meta que uno tenga, y la IAG sería, por diseño, al menos tan efectiva para perseguir sus metas como los humanos.

Ya sea que otorguemos control o ya sea que nos sea arrebatado, su pérdida parece extremadamente probable. Como Alan Turing originalmente lo puso, "...parece probable que una vez que el método de pensamiento de máquina hubiera comenzado, no tomaría mucho superar nuestros débiles poderes. No habría cuestión de que las máquinas mueran, y serían capaces de conversar entre sí para agudizar su ingenio. En alguna etapa por tanto deberíamos esperar que las máquinas tomaran control..."

Por favor nota, aunque es bastante obvio, que la pérdida de control por la humanidad hacia la IA también implica pérdida de control de Estados Unidos por el gobierno de Estados Unidos; significa pérdida de control de China por el partido comunista chino, y la pérdida de control de India, Francia, Brasil, Rusia, y todos los otros países por su propio gobierno. Así que las empresas de IA están, incluso si esta no es su intención, participando actualmente en el potencial derrocamiento de gobiernos mundiales, incluyendo el propio. Esto podría suceder en cuestión de años.

## La IAG llevará a superinteligencia

Hay un argumento de que la IA de propósito general competitiva con humanos o incluso competitiva con expertos, incluso si es autónoma, podría ser manejable. Puede ser increíblemente perturbadora en todas las formas discutidas arriba, pero hay muchas personas muy inteligentes y agenciales en el mundo ahora, y son más o menos manejables.[^26]

Pero no podremos quedarnos en nivel aproximadamente humano. La progresión más allá probablemente sería impulsada por las mismas fuerzas que ya hemos visto: presión competitiva entre desarrolladores de IA buscando ganancias y poder, presión competitiva entre usuarios de IA que no pueden permitirse quedarse atrás, y – más importantemente – la propia capacidad de la IAG para mejorarse a sí misma.

En un proceso que ya hemos visto comenzar con sistemas menos poderosos, la IAG misma sería capaz de concebir y diseñar versiones mejoradas de sí misma. Esto incluye hardware, software, redes neuronales, herramientas, andamiajes, etc. Será, por definición, mejor que nosotros haciendo esto, así que no sabemos exactamente cómo bootstrapeará inteligencia. Pero no tendremos que hacerlo. En la medida en que aún tengamos influencia en lo que la IAG hace, meramente necesitaríamos pedirle que lo haga, o dejarla.

No hay barrera de nivel humano a la cognición que pudiera protegernos de este descontrol.[^27]

La progresión de IAG a superinteligencia no es una ley de la naturaleza; aún sería posible restringir el descontrol, especialmente si la IAG es relativamente centralizada y al grado que es controlada por partes que no sienten presión de competir entre sí. Pero si la IAG fuera ampliamente proliferada y altamente autónoma, parece casi imposible prevenir que decida que debería ser más, y luego aún más, poderosa.

## Qué sucede si construimos (o la IAG construye) superinteligencia

Para decirlo sin rodeos, no tenemos idea de qué sucedería si construimos superinteligencia.[^28] Tomaría acciones que no podemos rastrear o percibir por razones que no podemos captar hacia metas que no podemos concebir. Lo que sí sabemos es que no dependerá de nosotros.[^29]

La imposibilidad de controlar superinteligencia puede entenderse a través de analogías cada vez más severas. Primero, imagina que eres CEO de una gran empresa. No hay forma de que puedas rastrear todo lo que está pasando, pero con la configuración correcta de personal, aún puedes entender significativamente el panorama general, y tomar decisiones. Pero supón solo una cosa: todos los demás en la empresa operan a cien veces tu velocidad. ¿Aún puedes mantenerte al día?

Con IA superinteligente, las personas estarían "comandando" algo no solo más rápido, sino operando a niveles de sofisticación y complejidad que no pueden comprender, procesando vastamente más datos de los que pueden siquiera concebir. Esta inconmensurabilidad puede ponerse a un nivel formal: [la ley de variedad requisita de Ashby](https://archive.org/details/introductiontocy00ashb/page/n7/mode/2up) (y ver el relacionado ["teorema del buen regulador"](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf)) establece, aproximadamente, que cualquier sistema de control debe tener tantas perillas y diales como grados de libertad tenga el sistema siendo controlado.

Una persona controlando un sistema de IA superinteligente sería como un helecho controlando General Motors: incluso si "hacer lo que el helecho quiere" estuviera escrito en los estatutos corporativos, los sistemas son tan diferentes en velocidad y rango de acción que "control" simplemente no aplica. (¿Y cuánto hasta que esa molesta ley secundaria sea reescrita?) [^30]

Como hay cero ejemplos de plantas controlando corporaciones fortune 500, habría exactamente cero ejemplos de personas controlando superinteligencias. Esto se acerca a un hecho matemático.[^31] Si se construyera superinteligencia – independientemente de cómo llegáramos allí – la pregunta no sería si los humanos podrían controlarla, sino si continuaríamos existiendo, y si es así, si tendríamos una existencia buena y significativa como individuos o como especie. Sobre estas preguntas existenciales para la humanidad tendríamos poca influencia. La era humana habría terminado.

## Conclusión: no debemos construir IAG

Hay un escenario en el cual construir IAG puede ir bien para la humanidad: se construye cuidadosamente, bajo control y para el beneficio de la humanidad, gobernada por acuerdo mutuo de muchas partes interesadas,[^32] y prevenida de evolucionar a superinteligencia incontrolable.

*Ese escenario no está abierto para nosotros bajo las circunstancias presentes.* Como se discutió en esta sección, con muy alta probabilidad, el desarrollo de IAG llevaría a alguna combinación de:

- Perturbación o destrucción societal y civilizacional masiva;
- Conflicto o guerra entre grandes potencias;
- Pérdida de control por la humanidad *de* o *a* sistemas de IA poderosos;
- Descontrol hacia superinteligencia incontrolable, y la irrelevancia o cesación de la especie humana.

Como una descripción ficcional temprana de IAG lo puso: la única forma de ganar es no jugar.


[^1]: La [Ley de IA de la UE](https://artificialintelligenceact.eu/) es una pieza significativa de legislación pero no evitaría directamente que un sistema de IA peligroso fuera desarrollado o desplegado, o incluso liberado abiertamente, especialmente en EE.UU. Otra pieza significativa de política, la orden ejecutiva de EE.UU. sobre IA, ha sido rescindida.

[^2]: Esta [encuesta de Gallup](https://news.gallup.com/poll/1597/confidence-institutions.aspx) muestra una caída desoladora en la confianza en instituciones públicas desde 2000 en EE.UU. Los números europeos son variados y menos extremos, pero también en tendencia descendente. La desconfianza no significa estrictamente que las instituciones realmente *sean* disfuncionales, pero es una indicación así como una causa.

[^3]: Y las perturbaciones importantes que ahora respaldamos – como la expansión de derechos a nuevos grupos – fueron específicamente impulsadas por personas en una dirección hacia hacer las cosas mejores.

[^4]: Permíteme ser directo. Si tu trabajo puede hacerse desde detrás de una computadora, con relativamente poca interacción en persona con personas fuera de tu organización, y no implica responsabilidad legal hacia partes externas, sería por definición posible (y probablemente ahorrativo en costos) intercambiarte completamente por un sistema digital. La robótica para reemplazar mucho trabajo físico vendrá después – pero no tanto después una vez que la IAG comience a diseñar robots.

[^5]: Por ejemplo, ¿qué pasa con nuestro sistema judicial si las demandas son casi gratis de presentar? ¿Qué sucede cuando eludir sistemas de seguridad a través de ingeniería social se vuelve barato, fácil, y libre de riesgo?

[^6]: [Este artículo](https://www.linkedin.com/pulse/projected-growth-ai-generated-data-public-internet-our-arun-kumar-r-vhije/) afirma que el 10% de todo el contenido de internet ya es generado por IA, y es el resultado principal de Google (para mí) a la consulta de búsqueda "estimaciones de qué fracción del nuevo contenido de internet es generado por IA." ¿Es cierto? ¡No tengo idea! No cita referencias y no fue escrito por una persona. ¿Qué fracción de nuevas imágenes indexadas por Google, o Tweets, o comentarios en Reddit, o videos de Youtube son generados por humanos? Nadie lo sabe – no creo que sea un número conocible. Y esto menos de *dos años* después del advenimiento de la IA generativa.

[^7]: También vale la pena agregar que hay riesgo "moral" de que podríamos crear seres digitales que pueden sufrir. Como actualmente no tenemos una teoría confiable de consciencia que nos permita distinguir sistemas físicos que pueden y no pueden sufrir, no podemos descartar esto teóricamente. Además, los reportes de los sistemas de IA de su sensibilidad probablemente son poco confiables con respecto a su experiencia real (o no experiencia) de sensibilidad.

[^8]: Las soluciones técnicas en este campo de "alineación" de IA probablemente tampoco estén a la altura de la tarea. En sistemas presentes funcionan a algún nivel, pero son superficiales y generalmente pueden eludirse sin esfuerzo significativo; y como se discute abajo no tenemos idea real de cómo hacer esto para sistemas mucho más avanzados.

[^9]: Tales sistemas de IA pueden venir con algunas salvaguardas incorporadas. Pero para cualquier modelo con algo parecido a la arquitectura actual, si el acceso completo a sus pesos está disponible, las medidas de seguridad pueden eliminarse vía entrenamiento adicional u otras técnicas. Así que está virtualmente garantizado que para cada sistema con barandillas también habrá un sistema ampliamente disponible sin ellas. De hecho el modelo Llama 3.1 405B de Meta fue liberado abiertamente con salvaguardas. Pero *incluso antes de eso* un modelo "base", sin salvaguardas, fue filtrado.

[^10]: ¿Podría el mercado gestionar estos riesgos sin involucramiento gubernamental? En resumen, no. Ciertamente hay riesgos que las empresas están fuertemente incentivadas a mitigar. Pero muchos otros las empresas pueden y sí externalizan a todos los demás, y muchos de los anteriores están en esta clase: no hay incentivos naturales del mercado para prevenir vigilancia masiva, decaimiento de la verdad, concentración de poder, perturbación laboral, discurso político dañino, etc. De hecho hemos visto todos estos de la tecnología actual, especialmente redes sociales, que ha ido esencialmente no regulada. La IA solo amplificaría enormemente muchas de las mismas dinámicas.

[^11]: OpenAI probablemente tiene modelos más obedientes para uso interno. Es improbable que OpenAI haya construido algún tipo de "puerta trasera" para que ChatGPT pueda ser mejor controlado por OpenAI mismo, porque esto sería una práctica de seguridad terrible, y sería altamente explotable dada la opacidad e impredecibilidad de la IA.

[^12]: También de importancia crucial: la alineación o cualquier otra característica de seguridad solo importa si realmente se usa en un sistema de IA. Sistemas que se liberan abiertamente (es decir, donde los pesos y arquitectura del modelo están públicamente disponibles) pueden transformarse relativamente fácilmente en sistemas *sin* esas medidas de seguridad. Liberar abiertamente sistemas IAG más inteligentes que humanos sería asombrosamente imprudente, y es difícil imaginar cómo el control humano o incluso relevancia se mantendría en tal escenario. Habría toda motivación, por ejemplo, para soltar agentes de IA poderosos autorreproductores y autosostenibles con la meta de hacer dinero y enviarlo a alguna billetera de criptomonedas. O ganar una elección. O derrocar un gobierno. ¿Podría la IA "buena" ayudar a contener esto? Quizás – pero solo delegándole autoridad enorme, llevando a pérdida de control como se describe abajo.

[^13]: Para exposiciones del problema del tamaño de un libro ver por ejemplo *Superintelligence*, *The Alignment Problem*, y *Human-Compatible*. Para una pila enorme de trabajo en varios niveles técnicos por aquellos que han trabajado durante años pensando sobre el problema, puedes visitar el [foro de alineación de IA](https://www.alignmentforum.org/). Aquí hay una [perspectiva reciente](https://alignment.anthropic.com/2025/recommended-directions/) del equipo de alineación de Anthropic sobre lo que consideran no resuelto.

[^14]: Este es el escenario de ["IA rebelde"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/). En principio el riesgo podría ser relativamente menor si el sistema aún puede controlarse apagándolo; pero el escenario también podría incluir engaño de IA, auto-exfiltración y reproducción, agregación de poder, y otros pasos que harían difícil o imposible hacerlo.

[^15]: Hay una literatura muy rica sobre este tema, que se remonta a escritos formativos por [Steve Omohundro](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf), Nick Bostrom, y Eliezer Yudkowsky. Para una exposición del tamaño de un libro ver [Human Compatible](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616) por Stuart Russell; [aquí](https://futureoflife.org/ai/could-we-switch-off-a-dangerous-ai/) hay una cartilla corta y actualizada.

[^16]: Reconociendo esto, en lugar de desacelerar para obtener mejor entendimiento, las empresas de IAG han surgido con un plan diferente: ¡harán que la IA lo haga! Más específicamente, harán que la IA *N* les ayude a descifrar cómo alinear la IA *N+1*, todo el camino a superinteligencia. Aunque aprovechar la IA para ayudarnos a alinear IA suena prometedor, hay un argumento fuerte de que simplemente asume su conclusión como premisa, y es en general un enfoque increíblemente riesgoso. Ver [aquí](https://www.thecompendium.ai/ai-safety#ai-will-not-solve-alignment-for-us) para algo de discusión. Este "plan" no es uno, y ha pasado por nada parecido al escrutinio apropiado para la estrategia central de cómo hacer que la IA súper-humana vaya bien para la humanidad.

[^17]: Después de todo, los humanos, defectuosos y voluntariosos como somos, hemos desarrollado sistemas éticos por los cuales tratamos al menos a algunas otras especies en la Tierra bien. (Solo no pienses en esas granjas industriales.)

[^18]: Afortunadamente hay un escape aquí: si los participantes llegan a entender que están involucrados en una carrera suicida en lugar de una ganable. Esto es lo que pasó cerca del final de la guerra fría, cuando EE.UU. y la URSS llegaron a darse cuenta de que debido al invierno nuclear, incluso un ataque nuclear *sin respuesta* sería desastroso para el atacante. Con la realización de que "la guerra nuclear no puede ganarse y nunca debe pelearse" vinieron acuerdos significativos sobre reducción de armas – esencialmente un fin a la carrera armamentista.

[^19]: Guerra, explícita o implícitamente.

[^20]: Escalación, luego guerra.

[^21]: Pensamiento mágico.

[^22]: También tengo un puente de un cuatrillón de dólares que venderte.

[^23]: Tales agentes presumiblemente preferirían "obtener," con destrucción como respaldo; pero asegurar modelos contra tanto destrucción *como* robo por naciones poderosas es difícil por decir lo menos, especialmente para entidades privadas.

[^24]: Para otra perspectiva sobre los riesgos de seguridad nacional de la IAG, ver [este reporte de RAND.](https://www.rand.org/pubs/perspectives/PEA3691-4.html)

[^25]: ¡Quizás podríamos construir tal institución! Ha habido propuestas para un "CERN para IA" y otras iniciativas similares, donde el desarrollo de IAG está bajo control global multilateral. Pero en el momento no existe tal institución o está en el horizonte.

[^26]: Y aunque la alineación es muy difícil, ¡hacer que las personas se comporten es aún más difícil!

[^27]: Imagina un sistema que puede hablar 50 idiomas, tener experiencia en todos los temas académicos, leer un libro completo en segundos y tener todo el material inmediatamente en mente, y producir salidas a diez veces la velocidad humana. Realmente, no tienes que imaginarlo: solo carga un sistema de IA actual. Estos son súper-humanos en muchas formas, y no hay nada deteniéndolos de ser aún más súper-humanos en esas y muchas otras.

[^28]: Por esto esto ha sido termed una "singularidad" tecnológica, tomando prestado de la física la idea de que uno no puede hacer predicciones pasada una singularidad. Los proponentes de inclinarse *hacia* tal singularidad también pueden desear reflexionar que en física estos mismos tipos de singularidades desgarran y aplastan a aquellos que van hacia ellas.

[^29]: El problema fue comprensivamente delineado en [*Superintelligence*](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834) de Bostrom, y nada desde entonces ha cambiado significativamente el mensaje central. Para un volumen más reciente recolectando resultados formales y matemáticos sobre incontrolabilidad ver [AI: Unexplainable, Unpredictable, Uncontrollable](https://www.amazon.com/Unexplainable-Unpredictable-Uncontrollable-Artificial-Intelligence/dp/103257626X) de Yampolskiy

[^30]: Esto también deja claro por qué la estrategia actual de las empresas de IA (iterativamente dejar que la IA "alinee" la siguiente IA más poderosa) no puede funcionar. Supón que un helecho, vía la agradabilidad de sus frondas, alista a un estudiante de primer grado para cuidarlo. El estudiante de primer grado escribe algunas instrucciones detalladas para un estudiante de 2do grado a seguir, y una nota convenciéndolos de hacerlo. El estudiante de 2do grado hace lo mismo para un estudiante de 3er grado, y así todo el camino a un graduado universitario, un gerente, un ejecutivo, y finalmente el CEO de GM. ¿GM entonces "hará lo que el helecho quiere"? En cada paso esto podría sentirse como que está funcionando. Pero poniéndolo todo junto, funcionará casi exactamente al grado al cual el CEO, Junta, y accionistas de GM resulten preocuparse por niños y helechos, y tendrá poco o nada que ver con todas esas notas y conjuntos de instrucciones.

[^31]: El carácter no es tan diferente de resultados formales como el teorema de incompletitud de Gödel o el argumento de parada de Turing en que la noción de control contradice fundamentalmente la premisa: cómo puedes controlar significativamente algo que no puedes entender o predecir; sin embargo si pudieras entender y predecir superinteligencia serías superinteligente. La razón por la que digo "se acerca" es que los resultados formales no son tan completos o revisados como en el caso de matemáticas puras, y porque me gustaría mantener esperanza de que alguna inteligencia general muy cuidadosamente construida, usando métodos totalmente diferentes a los actualmente empleados, pudiera tener algunas propiedades de seguridad matemáticamente comprobables, según el tipo de programa de IA "garantizadamente segura" discutido abajo.

[^32]: En el momento, la mayoría de las partes interesadas – es decir, casi toda la humanidad – está marginada en esta discusión. Eso está profundamente mal, y si no se les invita, los muchos, muchos otros grupos serán afectados por el desarrollo de IAG y deberían exigir ser incluidos.