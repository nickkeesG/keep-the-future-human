# Capítulo 8 - Cómo no construir IAG

La IAG no es inevitable: hoy nos encontramos en una encrucijada. Este capítulo presenta una propuesta sobre cómo podríamos evitar que se construya.

Si el camino que actualmente seguimos conduce al probable fin de nuestra civilización, ¿cómo cambiamos de rumbo?

Supongamos que el deseo de dejar de desarrollar IAG y superinteligencia fuera generalizado y poderoso,[^1] porque se vuelve entendimiento común que la IAG sería absorbedora de poder en lugar de otorgadora de poder, y un peligro profundo para la sociedad y la humanidad. ¿Cómo cerraríamos las Puertas?

En la actualidad conocemos una sola forma de *crear* IA poderosa y general, que es mediante computaciones verdaderamente masivas de redes neuronales profundas. Dado que estas son cosas increíblemente difíciles y costosas de hacer, hay un sentido en el cual *no* hacerlas es fácil.[^2] Pero ya hemos visto las fuerzas que impulsan hacia la IAG, y las dinámicas de teoría de juegos que hacen muy difícil que cualquier parte se detenga unilateralmente. Por lo tanto, tomaría una combinación de intervención desde el exterior (es decir, gobiernos) para detener a las corporaciones, y acuerdos entre gobiernos para detenerse a sí mismos.[^3] ¿Cómo podría verse esto?

Es útil primero distinguir entre desarrollos de IA que deben ser *prevenidos* o *prohibidos*, y aquellos que deben ser *gestionados.* Lo primero sería principalmente el escape hacia la superinteligencia.[^4] Para el desarrollo prohibido, las definiciones deben ser lo más precisas posible, y tanto la verificación como la aplicación deben ser prácticas. Lo que debe ser *gestionado* serían los sistemas de IA generales y poderosos – que ya tenemos, y que tendrán muchas áreas grises, matices y complejidad. Para estos, las instituciones fuertes y efectivas son cruciales.

También podemos delinear útilmente los asuntos que deben abordarse a nivel internacional (incluyendo entre rivales o adversarios geopolíticos) [^5] de aquellos que jurisdicciones individuales, países o grupos de países pueden manejar. El desarrollo prohibido cae en gran medida en la categoría "internacional", porque una prohibición local del desarrollo de una tecnología generalmente puede eludirse cambiando de ubicación.[^6]

Finalmente, podemos considerar las herramientas en la caja de herramientas. Hay muchas, incluyendo herramientas técnicas, derecho blando (estándares, normas, etc.), derecho duro (regulaciones y requisitos), responsabilidad legal, incentivos de mercado, y así sucesivamente. Prestemos especial atención a una que es particular de la IA.

## Seguridad y gobernanza del cómputo

Una herramienta fundamental para gobernar la IA de alta potencia será el hardware que requiere. El software prolifera fácilmente, tiene un costo de producción marginal cercano a cero, cruza fronteras trivialmente y puede modificarse instantáneamente; nada de esto es cierto para el hardware. Sin embargo, como hemos discutido, enormes cantidades de este "cómputo" son necesarias tanto durante el entrenamiento de sistemas de IA como durante la inferencia para lograr los sistemas más capaces. El cómputo puede ser fácilmente cuantificado, contabilizado y auditado, con relativamente poca ambigüedad una vez que se desarrollan buenas reglas para hacerlo. Más crucialmente, grandes cantidades de computación son, como el uranio enriquecido, un recurso muy escaso, costoso y difícil de producir. Aunque los chips de computadora son ubicuos, el hardware requerido para IA es costoso y enormemente difícil de fabricar.[^7]

Lo que hace a los chips especializados en IA *mucho más* manejables como recurso escaso que el uranio es que pueden incluir mecanismos de seguridad basados en hardware. La mayoría de los teléfonos celulares modernos, y algunas laptops, tienen características de hardware especializado en el chip que les permiten asegurar que instalen solo software de sistema operativo aprobado y actualizaciones, que retengan y protejan datos biométricos sensibles en el dispositivo, y que puedan volverse inútiles para cualquiera excepto su propietario si se pierden o son robados. Durante los últimos varios años, tales medidas de seguridad de hardware se han vuelto bien establecidas y ampliamente adoptadas, y generalmente han demostrado ser bastante seguras.

La novedad clave de estas características es que vinculan hardware y software usando criptografía.[^8] Es decir, solo tener una pieza particular de hardware de computadora no significa que un usuario pueda hacer lo que quiera con ella aplicando diferentes softwares. Y esta vinculación también proporciona seguridad poderosa porque muchos ataques requerirían una violación del hardware en lugar de solo la seguridad del *software*.

Varios informes recientes (por ejemplo, de [GovAI y colaboradores](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips), y [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf)) han señalado que características de hardware similares incorporadas en hardware de computación de vanguardia relevante para IA podrían desempeñar un papel extremadamente útil en la seguridad y gobernanza de IA. Habilitan una serie de funciones disponibles para un "gobernador" [^9] que uno podría no imaginar que estuvieran disponibles o incluso fueran posibles. Como algunos ejemplos clave:

- *Geolocalización*: Los sistemas pueden configurarse para que los chips tengan una ubicación conocida, y puedan actuar diferentemente (o apagarse por completo) basado en la ubicación.[^10]
- *Conexiones en lista de permitidos*: cada chip puede configurarse con una lista de permitidos aplicada por hardware de otros chips particulares con los cuales puede conectarse en red, y ser incapaz de conectarse con cualquier chip que no esté en esta lista.[^11] Esto puede limitar el tamaño de grupos comunicativos de chips.[^12]
- *Inferencia o entrenamiento medido (y auto-apagado)*: Un gobernador puede licenciar solo una cierta cantidad de entrenamiento o inferencia (en tiempo, o FLOP, o posiblemente tokens) para ser realizada por un usuario, después de lo cual se requiere nuevo permiso. Si los incrementos son pequeños, entonces se requiere re-licenciamiento relativamente continuo de un modelo. El modelo puede entonces "apagarse" simplemente reteniendo esta señal de licencia.[^13]
- *Límite de velocidad*: Se previene que un modelo funcione a velocidad de inferencia mayor que algún límite determinado por un gobernador u otro. Esto podría implementarse mediante un conjunto limitado de conexiones en lista de permitidos, o por medios más sofisticados.
- *Entrenamiento atestiguado*: Un procedimiento de entrenamiento puede generar prueba criptográficamente segura de que un conjunto particular de códigos, datos y cantidad de uso de cómputo fueron empleados en la generación del modelo.

## Cómo no construir superinteligencia: límites globales en cómputo de entrenamiento e inferencia

Con estas consideraciones – especialmente respecto a la computación – en su lugar, podemos discutir cómo cerrar las Puertas a la superinteligencia artificial; luego nos dirigiremos a prevenir la IAG completa, y gestionar modelos de IA mientras se aproximan y exceden la capacidad humana en diferentes aspectos.

El primer ingrediente es, por supuesto, el entendimiento de que la superinteligencia no sería controlable, y que sus consecuencias son fundamentalmente impredecibles. Al menos China y Estados Unidos deben decidir independientemente, para este u otros propósitos, no construir superinteligencia.[^14] Luego se necesita un acuerdo internacional entre ellos y otros, con un mecanismo fuerte de verificación y aplicación, para asegurar a todas las partes que sus rivales no están defeccionando y decidiendo lanzar los dados.

Para ser verificables y aplicables, los límites deben ser límites duros, y lo más inequívocos posible. Esto parece un problema virtualmente imposible: limitar las capacidades de software complejo con propiedades impredecibles, mundialmente. Afortunadamente la situación es mucho mejor que esto, porque lo mismo que ha hecho posible la IA avanzada – una enorme cantidad de cómputo – es mucho, mucho más fácil de controlar. Aunque podría aún permitir algunos sistemas poderosos y peligrosos, la *superinteligencia desbocada* puede probablemente prevenirse mediante un tope duro en la cantidad de computación que va a una red neuronal, junto con un límite de tasa en la cantidad de inferencia que un sistema de IA (de redes neuronales conectadas y otro software) puede realizar. Una versión específica de esto se propone más adelante.

Puede parecer que poner límites globales duros en la computación de IA requeriría enormes niveles de coordinación internacional y vigilancia intrusiva que destruya la privacidad. Afortunadamente, no sería así. La [cadena de suministro extremadamente estrecha y con cuello de botella](https://arxiv.org/abs/2402.08797) proporciona que una vez que se establezca un límite legalmente (ya sea por ley u orden ejecutiva), la verificación del cumplimiento de ese límite solo requeriría participación y cooperación de un puñado de grandes empresas.[^15]

Un plan como este tiene una serie de características altamente deseables. Es mínimamente invasivo en el sentido de que solo unas pocas empresas importantes tienen requisitos impuestos sobre ellas, y solo grupos bastante significativos de computación serían gobernados. Los chips relevantes ya contienen las capacidades de hardware necesarias para una primera versión.[^16] Tanto la implementación como la aplicación dependen de restricciones legales estándar. Pero estas están respaldadas por términos de uso del hardware y por controles de hardware, simplificando vastamente la aplicación y previniendo trampas por empresas, grupos privados, o incluso países. Hay amplio precedente para empresas de hardware que ponen restricciones remotas en el uso de su hardware, y bloquean/desbloquean capacidades particulares externamente,[^17] incluyendo incluso en CPU de alta potencia en centros de datos.[^18] Incluso para la fracción bastante pequeña de hardware y organizaciones afectadas, la supervisión podría limitarse a telemetría, sin acceso directo a datos o modelos mismos; y el software para esto podría estar abierto a inspección para exhibir que no se están registrando datos adicionales. El esquema es internacional y cooperativo, y bastante flexible y extensible. Porque el límite principalmente está en el hardware en lugar del software, es relativamente agnóstico sobre cómo ocurre el desarrollo e implementación del software de IA, y es compatible con variedad de paradigmas incluyendo IA más "descentralizada" o "pública" dirigida a combatir la concentración de poder impulsada por IA.

El cierre de Puertas basado en computación también tiene desventajas. Primero, está lejos de ser una solución completa al problema de la gobernanza de IA en general. Segundo, conforme el hardware de computadora se vuelve más rápido, el sistema "atraparía" más y más hardware en grupos más y más pequeños (o incluso GPU individuales).[^19] También es posible que debido a mejoras algorítmicas incluso un límite de computación más bajo sea necesario en el tiempo,[^20] o que la cantidad de computación se vuelva en gran medida irrelevante y cerrar la Puerta en su lugar necesitara un régimen de gobernanza más detallado basado en riesgo o capacidad para IA. Tercero, sin importar las garantías y el pequeño número de entidades afectadas, tal sistema está destinado a crear resistencia respecto a privacidad y vigilancia, entre otras preocupaciones.[^21]

Por supuesto, desarrollar e implementar un esquema de gobernanza que limite el cómputo en un período corto de tiempo será bastante desafiante. Pero absolutamente es factible.

## I-A-G: La triple-intersección como base del riesgo, y de la política

Dirigámonos ahora a la IAG. Las líneas duras y definiciones aquí son más difíciles, porque ciertamente tenemos inteligencia que es artificial y general, y por ninguna definición existente todos estarán de acuerdo si o cuándo existe. Además, un límite de cómputo o inferencia es una herramienta algo contundente (siendo el cómputo un proxy para la capacidad, que luego es un proxy para el riesgo) que – a menos que sea bastante bajo – es improbable que prevenga IAG que sea lo suficientemente poderosa para causar disrupción social o civilizacional o riesgos agudos.

He argumentado que los riesgos más agudos emergen de la triple-intersección de capacidad muy alta, alta autonomía y gran generalidad. Estos son los sistemas que – si se desarrollan en absoluto – deben ser gestionados con enorme cuidado. Al crear estándares estrictos (mediante responsabilidad legal y regulación) para sistemas que combinen las tres propiedades, podemos canalizar el desarrollo de IA hacia alternativas más seguras.

Como con otras industrias y productos que podrían potencialmente dañar a consumidores o al público, los sistemas de IA requieren regulación cuidadosa por agencias gubernamentales efectivas y empoderadas. Esta regulación debe reconocer los riesgos inherentes de la IAG, y prevenir que se desarrollen sistemas de IA de alta potencia inaceptablemente riesgosos.[^22]

Sin embargo, la regulación a gran escala, especialmente con dientes reales que seguramente serán opuestos por la industria,[^23] toma tiempo [^24] así como convicción política de que es necesaria.[^25] Dado el ritmo del progreso, esto puede tomar más tiempo del que tenemos disponible.

En una escala de tiempo mucho más rápida y mientras se desarrollan medidas regulatorias, podemos dar a las empresas los incentivos necesarios para (a) desistir de actividades de muy alto riesgo y (b) desarrollar sistemas comprensivos para evaluar y mitigar riesgo, aclarando y aumentando los niveles de responsabilidad legal para los sistemas más peligrosos. La idea sería imponer los niveles más altos de responsabilidad legal – estricta y en algunos casos criminal personal – para sistemas en la triple-intersección de alta autonomía-generalidad-inteligencia, pero proporcionar "puertos seguros" a responsabilidad legal más típica basada en fallas para sistemas en los cuales una de esas propiedades está faltando o se garantiza que es manejable. Es decir, por ejemplo, un sistema "débil" que es general y autónomo (como un asistente personal capaz y confiable pero limitado) estaría sujeto a niveles más bajos de responsabilidad legal. Igualmente un sistema estrecho y autónomo como un auto que se conduce solo estaría aún sujeto a la regulación significativa que ya tiene, pero no a responsabilidad legal mejorada. Similarmente para un sistema altamente capaz y general que es "pasivo" y en gran medida incapaz de acción independiente. Los sistemas que carecen de *dos* de las tres propiedades son aún más manejables y los puertos seguros serían incluso más fáciles de reclamar. Este enfoque refleja cómo manejamos otras tecnologías potencialmente peligrosas:[^26] mayor responsabilidad legal para configuraciones más peligrosas crea incentivos naturales para alternativas más seguras.

El resultado por defecto de tales altos niveles de responsabilidad legal, que actúan para *internalizar* el riesgo de IAG a las empresas en lugar de descargarlo al público, es probable (¡y esperemos!) que las empresas simplemente no desarrollen IAG completa hasta y a menos que puedan genuinamente hacerla confiable, segura y controlable dado que *su propio liderazgo* son las partes en riesgo. (En caso de que esto no sea suficiente, la legislación aclarando responsabilidad legal también debería permitir explícitamente alivio judicial, es decir, un juez ordenando una detención, para actividades que están claramente en la zona de peligro y posiblemente presentan un riesgo público.) Conforme la regulación entra en su lugar, cumplir con la regulación puede convertirse en el puerto seguro, y los puertos seguros de baja autonomía, estrechez, o debilidad de sistemas de IA pueden convertirse en regímenes regulatorios relativamente más ligeros.

## Provisiones clave de un cierre de Puertas

Con la discusión anterior en mente, esta sección proporciona propuestas para provisiones clave que implementarían y mantendrían la prohibición en IAG completa y superinteligencia, y gestión de IA general competitiva con humanos o competitiva con expertos cerca del umbral de IAG completa.[^27] Tiene cuatro piezas clave: 1) contabilidad y supervisión de cómputo, 2) topes de cómputo en entrenamiento y operación de IA, 3) un marco de responsabilidad legal, y 4) estándares de seguridad y protección por niveles definidos que incluyen requisitos regulatorios duros. Estos se describen sucintamente a continuación, con más detalles o ejemplos de implementación dados en tres tablas acompañantes. Importante, note que estos están lejos de ser todo lo que será necesario para gobernar sistemas avanzados de IA; mientras tendrán beneficios adicionales de seguridad y protección, están dirigidos a cerrar la Puerta al escape de inteligencia, y redirigir el desarrollo de IA en una mejor dirección.

### 1\. Contabilidad de cómputo, y transparencia

- Una organización de estándares (por ejemplo NIST en Estados Unidos seguido por ISO/IEEE internacionalmente) debe codificar un estándar técnico detallado para el cómputo total usado en entrenar y operar modelos de IA, en FLOP, y la velocidad en FLOP/s a la cual operan. Los detalles de cómo podría verse esto se dan en el Apéndice A.[^28]
- Un requisito – ya sea por nueva legislación o bajo autoridad existente [^29] – debe ser impuesto por jurisdicciones en las cuales toma lugar entrenamiento de IA a gran escala para computar y reportar a un cuerpo regulatorio u otra agencia el FLOP total usado en entrenar y operar todos los modelos arriba de un umbral de 10 <sup>25</sup> FLOP o 10 <sup>18</sup> FLOP/s.[^30]
- Estos requisitos deben introducirse por fases, inicialmente requiriendo estimados de buena fe bien documentados en una base trimestral, con fases posteriores requiriendo progresivamente estándares más altos, hasta FLOP total y FLOP/s criptográficamente atestiguados adjuntos a cada *salida* del modelo.
- Estos reportes deben ser complementados por estimados bien documentados de costo energético y financiero marginal usado en generar cada salida de IA.

Justificación: Estos números bien computados y transparentemente reportados proporcionarían la base para topes de entrenamiento y operación, así como un puerto seguro de medidas de responsabilidad legal más altas (ver Apéndices C y D).

### 2\. Topes de cómputo de entrenamiento y operación

- Las jurisdicciones que hospedan sistemas de IA deben imponer un límite duro en el cómputo total que va a cualquier salida de modelo de IA, comenzando en 10 <sup>27</sup> FLOP [^31] y ajustable según sea apropiado.
- Las jurisdicciones que hospedan sistemas de IA deben imponer un límite duro en la tasa de cómputo de salidas de modelo de IA, comenzando en 10 <sup>20</sup> FLOP/s y ajustable según sea apropiado.

Justificación: La computación total, aunque muy imperfecta, es un proxy para la capacidad (y riesgo) de IA que es concretamente medible y verificable, así que proporciona un respaldo duro para limitar capacidades. Una propuesta de implementación concreta se da en el Apéndice B.

### 3\. Responsabilidad legal mejorada para sistemas peligrosos

- La creación y operación [^32] de un sistema avanzado de IA que es altamente general, capaz y autónomo, debe ser aclarada legalmente mediante legislación para estar sujeta a responsabilidad legal estricta, solidaria, en lugar de basada en falla de una sola parte.[^33]
- Un proceso legal debe estar disponible para hacer casos de seguridad afirmativos, que otorgarían puerto seguro de responsabilidad legal estricta para sistemas que son pequeños (en términos de cómputo), débiles, estrechos, pasivos, o que tienen suficientes garantías de seguridad, protección y controlabilidad.
- Una vía explícita y conjunto de condiciones para alivio judicial para detener actividades de entrenamiento e inferencia de IA que constituyen un peligro público debe ser delineado.

Justificación: Los sistemas de IA no pueden ser responsabilizados, así que debemos responsabilizar a individuos humanos y organizaciones por el daño que causan (responsabilidad legal).[^34] La IAG incontrolable es una amenaza para la sociedad y civilización y en ausencia de un caso de seguridad debe considerarse anormalmente peligrosa. Poner la carga de responsabilidad en los desarrolladores para mostrar que los modelos poderosos son lo suficientemente seguros para no ser considerados "anormalmente peligrosos" incentiva el desarrollo seguro, junto con transparencia y mantenimiento de registros para reclamar esos puertos seguros. La regulación puede entonces prevenir daño donde la disuasión de la responsabilidad legal es insuficiente. Finalmente, los desarrolladores de IA ya son responsables por los daños que causan, así que aclarar legalmente la responsabilidad legal para los más riesgosos de los sistemas puede hacerse inmediatamente, sin que se desarrollen estándares altamente detallados; estos pueden entonces desarrollarse con el tiempo. Los detalles se dan en el Apéndice C.

### 4\. Regulación de seguridad para IA

Un sistema regulatorio que aborde riesgos agudos a gran escala de IA requerirá como mínimo:

- La identificación o creación de un conjunto apropiado de cuerpos regulatorios, probablemente una nueva agencia;
- Un marco comprensivo de evaluación de riesgo;[^35]
- Un marco para casos de seguridad afirmativos, basado en parte en el marco de evaluación de riesgo, para ser hechos por desarrolladores, y para auditoría por grupos y agencias *independientes*;
- Un sistema de licenciamiento por niveles, con niveles siguiendo niveles de capacidad.[^36] Las licencias serían otorgadas en base a casos de seguridad y auditorías, para desarrollo e implementación de sistemas. Los requisitos irían desde notificación en el extremo bajo, hasta garantías cuantitativas de seguridad, protección y controlabilidad antes del desarrollo, en el extremo alto. Estos prevendrían el lanzamiento de sistemas hasta que se demuestre que son seguros, y prohibirían el desarrollo de sistemas intrínsecamente inseguros. El Apéndice D proporciona una propuesta de lo que tales estándares de seguridad y protección podrían conllevar.
- Acuerdos para llevar tales medidas al nivel internacional, incluyendo cuerpos internacionales para armonizar normas y estándares, y potencialmente agencias internacionales para revisar casos de seguridad.

Justificación: En última instancia, la responsabilidad legal no es el mecanismo correcto para prevenir riesgo a gran escala al público de una nueva tecnología. La regulación comprensiva, con cuerpos regulatorios empoderados, será necesaria para IA así como para cada otra industria importante que presenta riesgo al público.[^37]

La regulación hacia prevenir otros riesgos generalizados pero menos agudos es probable que varíe en su forma de jurisdicción a jurisdicción. Lo crucial es evitar desarrollar los sistemas de IA que son tan riesgosos que estos riesgos son inmanejables.

## ¿Qué entonces?

Durante la próxima década, conforme la IA se vuelve más generalizada y la tecnología central avanza, es probable que pasen dos cosas clave. Primero, la regulación de sistemas poderosos existentes de IA se volverá más difícil, sin embargo incluso más necesaria. Es probable que al menos algunas medidas abordando riesgos de seguridad a gran escala requieran acuerdo a nivel internacional, con jurisdicciones individuales aplicando reglas basadas en acuerdos internacionales.

Segundo, los topes de cómputo de entrenamiento y operación se volverán más difíciles de mantener conforme el hardware se vuelve más barato y más eficiente en costo; también pueden volverse menos relevantes (o necesitar ser incluso más estrictos) con avances en algoritmos y arquitecturas.

¡Que controlar la IA se vuelva más difícil no significa que debamos rendirnos! Implementar el plan delineado en este ensayo nos daría tanto tiempo valioso como control crucial sobre el proceso que nos pondría en una posición mucho, mucho mejor para evitar el riesgo existencial de la IA para nuestra sociedad, civilización y especie.

En el término aún más largo, habrá elecciones que hacer sobre qué permitimos. Podemos elegir aún crear alguna forma de IAG genuinamente controlable, al grado que esto demuestre ser posible. O podemos decidir que administrar el mundo se deja mejor a las máquinas, si podemos convencernos de que harán un mejor trabajo, y nos tratarán bien. Pero estas deben ser decisiones hechas con entendimiento científico profundo de la IA en mano, y después de discusión global inclusiva significativa, no en una carrera entre magnates tecnológicos con la mayoría de la humanidad completamente no involucrada y desconocedora.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) Resumen de la gobernanza de I-A-G y superinteligencia mediante responsabilidad legal y regulación. La responsabilidad legal es más alta, y la regulación más fuerte, en la triple-intersección de Autonomía, Generalidad e Inteligencia. Los puertos seguros de responsabilidad legal estricta y regulación fuerte pueden obtenerse mediante casos de seguridad afirmativos demostrando que un sistema es débil y/o estrecho y/o pasivo. Los topes en Cómputo Total de Entrenamiento y tasa de Cómputo de Inferencia, verificados y aplicados legalmente y usando medidas de seguridad de hardware y criptográficas, respaldan la seguridad evitando IAG completa y efectivamente prohibiendo la superinteligencia.


[^1]: Muy probablemente, la expansión de esta realización tomará ya sea esfuerzo intenso por grupos de educación y promoción haciendo este caso, o un desastre bastante significativo causado por IA. Podemos esperar que sea lo primero.

[^2]: Paradójicamente, estamos acostumbrados a que la Naturaleza limite nuestra tecnología haciéndola muy difícil de desarrollar, especialmente científicamente. Pero ese ya no es el caso para IA: los problemas científicos clave están resultando ser más fáciles de lo anticipado. No podemos contar con que la Naturaleza nos salve de nosotros mismos aquí – tendremos que hacerlo nosotros.

[^3]: ¿Dónde, exactamente, nos detenemos en desarrollar nuevos sistemas? Aquí, debemos adoptar un principio precautorio. Una vez que un sistema es implementado, y especialmente una vez que ese nivel de capacidad del sistema prolifera, es excesivamente difícil retroceder. Y si un sistema es *desarrollado* (especialmente a gran costo y esfuerzo), habrá enorme presión para usarlo o implementarlo, y tentación de que sea filtrado o robado. Desarrollar sistemas y *luego* decidir si son profundamente inseguros es un camino peligroso.

[^4]: También sería sabio prohibir el desarrollo de IA que es intrínsecamente peligroso, como sistemas auto-replicantes y evolutivos, aquellos diseñados para escapar del encierro, aquellos que pueden auto-mejorarse autónomamente, IA deliberadamente engañosa y maliciosa, etc.

[^5]: Note que esto no significa necesariamente *aplicado* a nivel internacional por algún tipo de cuerpo global: en su lugar las naciones soberanas podrían aplicar reglas acordadas, como en muchos tratados.

[^6]: Como veremos más adelante, la naturaleza de la computación de IA permitiría algo de un híbrido; pero la cooperación internacional aún será necesaria.

[^7]: Por ejemplo, las máquinas requeridas para grabar chips relevantes para IA son hechas por solo una firma, ASML (a pesar de muchos otros intentos de hacerlo), la vasta mayoría de chips relevantes son manufacturados por una firma, TSMC (a pesar de otros intentando competir), y el diseño y construcción de hardware de esos chips hecho por solo unos pocos incluyendo NVIDIA, AMD, y Google.

[^8]: Más importante, cada chip sostiene una llave privada criptográfica única e inaccesible que puede usar para "firmar" cosas.

[^9]: Por defecto esto sería la empresa vendiendo los chips, pero otros modelos son posibles y potencialmente útiles.

[^10]: Un gobernador puede determinar la ubicación de un chip cronometrando el intercambio de mensajes firmados con él: la velocidad finita de la luz requiere que el chip esté dentro de un radio dado *r* de una "estación" si puede devolver un mensaje firmado en un tiempo menor que *r* / *c*, donde *c* es la velocidad de la luz. Usando múltiples estaciones, y algún entendimiento de características de red, la ubicación del chip puede determinarse. La belleza de este método es que la mayoría de su seguridad es suministrada por las leyes de la física. Otros métodos podrían usar GPS, rastreo inercial, y tecnologías similares.

[^11]: Alternativamente, pares de chips podrían permitirse comunicarse entre sí solo mediante permiso explícito de un gobernador.

[^12]: Esto es crucial porque al menos actualmente, conexión de muy alto ancho de banda entre chips es necesaria para entrenar modelos grandes de IA en ellos.

[^13]: Esto también podría configurarse para requerir mensajes firmados de *N* de *M* gobernadores diferentes, permitiendo que múltiples partes compartan gobernanza.

[^14]: Esto está lejos de no tener precedente – por ejemplo los militares no han desarrollado ejércitos de supersoldados clonados o genéticamente modificados, aunque esto es probablemente tecnológicamente posible. Pero han *elegido* no hacer esto, en lugar de ser prevenidos por otros. El historial no es grandioso para grandes potencias mundiales siendo prevenidas de desarrollar una tecnología que desean fuertemente desarrollar.

[^15]: Con un par de excepciones notables (en particular NVIDIA) el hardware especializado en IA es una parte relativamente pequeña del modelo general de negocio e ingresos de estas empresas. Además, la brecha entre hardware usado en IA avanzada y hardware "de grado consumidor" es significativa, así que la mayoría de consumidores de hardware de computadora serían en gran medida no afectados.

[^16]: Para análisis más detallado, vea los informes recientes de [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) y [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips). Estos se enfocan en factibilidad técnica, especialmente en el contexto de controles de exportación de Estados Unidos buscando restringir la capacidad de otros países en computación de alta gama; pero esto tiene superposición obvia con la restricción global prevista aquí.

[^17]: Los dispositivos Apple, por ejemplo, son bloqueados remota y seguramente cuando se reportan perdidos o robados, y pueden ser re-activados remotamente. Esto depende de las mismas características de seguridad de hardware discutidas aquí.

[^18]: Vea por ejemplo la oferta de [capacidad bajo demanda](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) de IBM, [Intel bajo demanda](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html) de Intel, y [computación en nube privada](https://security.apple.com/blog/private-cloud-compute/) de Apple.

[^19]: [Este estudio](https://epochai.org/trends#hardware-trends-section) muestra que históricamente el mismo desempeño se ha logrado usando aproximadamente 30% menos dólares por año. Si esta tendencia continúa, puede haber superposición significativa entre uso de chips de IA y "consumidor", y en general la cantidad de hardware necesario para sistemas de IA de alta potencia podría volverse incómodamente pequeña.

[^20]: Por el [mismo estudio](https://epochai.org/trends#hardware-trends-section), el desempeño dado en reconocimiento de imagen ha requerido 2.5x menos computación cada año. Si esto fuera también a sostenerse para los sistemas de IA más capaces también, un límite de computación no sería uno útil por mucho tiempo.

[^21]: En particular, a nivel de país esto parece mucho una nacionalización de la computación, en que el gobierno tendría mucho control sobre cómo se usa el poder computacional. Sin embargo, para aquellos preocupados sobre la participación gubernamental, esto parece mucho más seguro que y preferible a que el software de IA más poderoso *mismo* sea nacionalizado mediante alguna fusión entre grandes empresas de IA y gobiernos nacionales, como algunos están empezando a abogar.

[^22]: Un paso regulatorio importante en Europa fue tomado con la aprobación en 2024 de la [Ley de IA de la UE.](https://artificialintelligenceact.eu/) Clasifica IA por riesgo: prohibiendo sistemas inaceptables, regulando los de alto riesgo, e imponiendo reglas de transparencia, o ninguna medida en absoluto, sobre sistemas de bajo riesgo. Reducirá significativamente algunos riesgos de IA, y impulsará la transparencia de IA incluso para firmas estadounidenses, pero tiene dos fallas clave. Primero, alcance limitado: mientras aplica a cualquier empresa proporcionando IA en la UE, la aplicación sobre firmas basadas en Estados Unidos es débil, y la IA militar está exenta. Segundo, mientras cubre GPAI, falla en reconocer IAG o superinteligencia como riesgos inaceptables o prevenir su desarrollo—solo su implementación en la UE. Como resultado, hace poco para frenar los riesgos de IAG o superinteligencia.

[^23]: Las empresas a menudo representan que están a favor de regulación razonable. Pero de alguna manera casi siempre parecen oponerse a cualquier regulación *particular*; atestigüe la lucha sobre la bastante ligera SB1047, que [la mayoría de empresas de IA se opusieron pública o privadamente.](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/)

[^24]: Fueron aproximadamente 3 1/2 años desde el tiempo que la ley de IA de la UE fue propuesta hasta que entró en efecto.

[^25]: A veces se expresa que es "demasiado temprano" para empezar a regular IA. Dada la última nota, eso difícilmente parece probable. Otra preocupación expresada es que la regulación "dañaría la innovación." Pero la buena regulación solo cambia la dirección, no cantidad, de innovación.

[^26]: Un precedente interesante está en el transporte de materiales peligrosos, que podrían escapar y causar daño. Aquí, [regulación](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) y [jurisprudencia](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) han establecido responsabilidad legal estricta para materiales muy peligrosos como explosivos, gasolina, venenos, agentes infecciosos, y desecho radioactivo. Otros ejemplos incluyen [advertencias en farmacéuticos](https://www.medicalnewstoday.com/articles/boxed-warnings), [clases de dispositivos médicos,](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification) etc.

[^27]: Otra propuesta comprensiva con objetivos similares presentada en ["Un Sendero Estrecho"](https://www.narrowpath.co/) aboga por un enfoque más centralizado, basado en prohibición que canaliza todo el desarrollo de IA de frontera a través de una sola entidad internacional, supervisada por instituciones internacionales fuertes, con prohibiciones categóricas claras en lugar de restricciones graduadas. También respaldo ese plan; sin embargo tomará incluso más voluntad política y coordinación que el propuesto aquí.

[^28]: Algunas directrices para tal estándar fueron [publicadas](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/) por el Foro de Modelo de Frontera. Relativo a la propuesta aquí, esas yerran del lado de menos precisión y menos cómputo incluido en el conteo.

[^29]: La orden ejecutiva de IA de Estados Unidos de 2023 (ahora rescindida) requirió reportes similares pero menos finos. Esto debe fortalecerse por una orden de reemplazo.

[^30]: Muy aproximadamente, para chips H100 ahora-comunes esto corresponde a grupos de aproximadamente 1000 haciendo inferencia; son aproximadamente 100 (aproximadamente USD $5M valor) de los chips NVIDIA B200 más nuevos de primera línea haciendo inferencia. En ambos casos el número de entrenamiento corresponde a ese grupo computando por varios meses mes.

[^31]: Esta cantidad es mayor que cualquier sistema de IA actualmente entrenado; un número mayor o menor podría justificarse conforme entendemos mejor cómo la capacidad de IA escala con el cómputo.

[^32]: Esto aplica a aquellos creando y proporcionando/hospedando los modelos, no usuarios finales.

[^33]: Aproximadamente, responsabilidad legal "estricta" significa que los desarrolladores son responsabilizados por daños hechos por un producto *por defecto* y es un estándar usado para productos "anormalmente peligrosos", y (algo divertido pero apropiadamente) animales salvajes. Responsabilidad legal "solidaria" significa que la responsabilidad legal es asignada a todas las partes responsables por un producto, y esas partes tienen que arreglar entre sí mismas quién lleva qué responsabilidad. Esto es importante para sistemas como IA con una cadena de valor larga y compleja.

[^34]: La responsabilidad legal estándar basada en falla de una sola parte no es suficiente: la falla será tanto difícil de rastrear como asignar porque los sistemas de IA son complejos, su operación no es entendida, y muchas partes pueden estar involucradas en la creación de un sistema o salida peligrosa. Además, las demandas tomarán años para adjudicar y probablemente resulten meramente en multas que son inconsequentes para estas empresas, así que la responsabilidad legal personal para ejecutivos es importante también.

[^35]: No debe haber exención de criterios de seguridad para modelos de peso abierto. Además, al evaluar riesgo debe asumirse que las barreras que pueden ser removidas serán removidas de modelos ampliamente disponibles, y que incluso modelos cerrados proliferarán a menos que haya una muy alta aseguración de que permanecerán seguros.

[^36]: El esquema propuesto aquí tiene escrutinio regulatorio disparado en capacidad general; sin embargo tiene sentido para algunos casos de uso especialmente riesgosos disparar más escrutinio – por ejemplo un sistema de IA de virología experto, incluso si estrecho y pasivo, probablemente debería ir en un nivel más alto. La anterior orden ejecutiva estadounidense tenía algo de esta estructura para capacidades biológicas.

[^37]: Dos ejemplos claros son aviación y medicinas, reguladas por la FAA y FDA, y agencias similares en otros países. Estas agencias son imperfectas, pero han sido absolutamente vitales para el funcionamiento y éxito de esas industrias.