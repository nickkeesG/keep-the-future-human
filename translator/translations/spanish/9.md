# Capítulo 9 - Diseñando el futuro: qué deberíamos hacer en su lugar

La IA puede hacer un bien increíble en el mundo. Para obtener todos los beneficios sin los riesgos, debemos asegurar que la IA siga siendo una herramienta humana.

Si decidimos exitosamente no suplantarnos a nosotros mismos por máquinas—¡al menos por un tiempo!—¿qué podemos hacer en su lugar? ¿Renunciamos a la enorme promesa de la IA como tecnología? En algún nivel la respuesta es un simple *no:* cerrar las Puertas a la IAG incontrolable y la superinteligencia, pero *sí* construir muchas otras formas de IA, así como las estructuras de gobernanza e instituciones que necesitaremos para gestionarlas.

Pero aún hay mucho que decir; hacer que esto suceda sería una ocupación central de la humanidad. Esta sección explora varios temas clave:

- Cómo podemos caracterizar la IA "Herramienta" y las formas que puede adoptar.
- Que podemos obtener (casi) todo lo que la humanidad quiere sin IAG, con IA Herramienta.
- Que los sistemas de IA Herramienta son (probablemente, en principio) manejables.
- Que alejarse de la IAG no significa comprometer la seguridad nacional—todo lo contrario.
- Que la concentración de poder es una preocupación real. ¿Podemos mitigarla sin socavar la seguridad?
- Que querremos—y necesitaremos—nuevas estructuras de gobernanza y sociales, y la IA puede realmente ayudar.

## IA dentro de las Puertas: IA Herramienta

El diagrama de triple intersección ofrece una buena manera de delinear lo que podemos llamar "IA Herramienta": IA que es una herramienta controlable para uso humano, en lugar de un rival o reemplazo incontrolable. Los sistemas de IA menos problemáticos son aquellos que son autónomos pero no generales ni súper capaces (como un bot de ofertas en subastas), o generales pero no autónomos ni capaces (como un modelo de lenguaje pequeño), o capaces pero específicos y muy controlables (como AlphaGo).[^1] Aquellos con dos características que se intersectan tienen aplicaciones más amplias pero mayor riesgo y requerirán grandes esfuerzos para gestionarlos. (El hecho de que un sistema de IA sea más una herramienta no significa que sea inherentemente seguro, simplemente que no es inherentemente *inseguro*—considera una motosierra versus un tigre mascota.) La Puerta debe permanecer cerrada a la IAG (completa) y la superinteligencia en la triple intersección, y debe tenerse un enorme cuidado con los sistemas de IA que se acerquen a ese umbral.

Pero esto deja mucha IA poderosa. Podemos obtener una enorme utilidad de "oráculos" pasivos inteligentes y generales y sistemas específicos, sistemas generales a nivel humano pero no sobrehumano, etc. Muchas empresas tecnológicas y desarrolladores están construyendo activamente estos tipos de herramientas y deberían continuar; como la mayoría de la gente, implícitamente *asumen* que las Puertas a la IAG y la superinteligencia permanecerán cerradas.[^2]

Además, los sistemas de IA pueden combinarse efectivamente en sistemas compuestos que mantienen supervisión humana mientras mejoran las capacidades. En lugar de depender de cajas negras inescrutables, podemos construir sistemas donde múltiples componentes—incluyendo tanto IA como software tradicional—trabajen juntos de maneras que los humanos puedan monitorear y entender.[^3] Mientras algunos componentes podrían ser cajas negras, ninguno estaría cerca de la IAG—solo el sistema compuesto como un todo sería tanto altamente general como altamente capaz, y de manera estrictamente controlable.[^4]

### Control humano significativo y garantizado

¿Qué significa "estrictamente controlable"? Una idea clave del marco "Herramienta" es permitir sistemas—incluso si son bastante generales y poderosos—que estén garantizados de estar bajo control humano significativo. ¿Qué significa esto? Implica dos aspectos. Primero es una consideración de diseño: los humanos deberían estar profunda y centralmente involucrados en lo que el sistema está haciendo, *sin* delegar decisiones importantes clave a la IA. Este es el carácter de la mayoría de los sistemas de IA actuales. Segundo, en la medida en que los sistemas de IA son autónomos, deben tener garantías que limiten su alcance de acción. Una garantía debería ser un *número* que caracterice la probabilidad de que algo suceda, y una razón para creer en ese número. Esto es lo que exigimos en otros campos críticos para la seguridad, donde números como "tiempo medio entre fallas" y números esperados de accidentes se calculan, respaldan y publican en casos de seguridad.[^5] El número ideal para fallas es cero, por supuesto. Y la buena noticia es que podríamos acercarnos bastante, aunque usando arquitecturas de IA bastante diferentes, usando ideas de propiedades *formalmente verificadas* de programas (incluyendo IA). La idea, explorada extensamente por Omohundro, Tegmark, Bengio, Dalrymple y otros (ver [aquí](https://arxiv.org/abs/2309.01933) y [aquí](https://arxiv.org/abs/2405.06624)) es construir un programa con ciertas propiedades (por ejemplo: que un humano pueda apagarlo) y *probar* formalmente que esas propiedades se mantienen. Esto se puede hacer ahora para programas bastante cortos y propiedades simples, pero el poder (venidero) del software de pruebas impulsado por IA podría permitirlo para programas mucho más complejos (p. ej., envoltorios) e incluso la IA misma. Este es un programa muy ambicioso, pero a medida que crece la presión sobre las Puertas, vamos a necesitar algunos materiales poderosos reforzándolas. La prueba matemática puede ser uno de los pocos que sea lo suficientemente fuerte.

### Hacia dónde va la industria de la IA

Con el progreso de la IA redirigido, la IA Herramienta seguiría siendo una industria enorme. En términos de hardware, incluso con límites de cómputo para prevenir la superinteligencia, el entrenamiento y la inferencia en modelos más pequeños aún requerirán enormes cantidades de componentes especializados. En el lado del software, desactivar la explosión en el tamaño de modelos de IA y computación simplemente debería llevar a las empresas a redirigir recursos hacia hacer los sistemas más pequeños mejores, más diversos y más especializados, en lugar de simplemente hacerlos más grandes.[^6] Habría mucho espacio—probablemente más—para todas esas startups lucrativas de Silicon Valley.[^7]

## La IA Herramienta puede producir (casi) todo lo que la humanidad quiere, sin IAG

La inteligencia, ya sea biológica o de máquina, puede considerarse ampliamente como la capacidad de planificar y ejecutar actividades que generen futuros más alineados con un conjunto de objetivos. Como tal, la inteligencia es de enorme beneficio cuando se usa en la búsqueda de objetivos sabiamente elegidos. La inteligencia artificial está atrayendo enormes inversiones de tiempo y esfuerzo en gran parte debido a sus beneficios prometidos. Así que deberíamos preguntar: ¿hasta qué grado aún cosecharíamos los beneficios de la IA si contenemos su descontrol hacia la superinteligencia? La respuesta: podríamos perder sorprendentemente poco.

Considera primero que los sistemas de IA actuales ya son muy poderosos, y realmente solo hemos arañado la superficie de lo que se puede hacer con ellos.[^8] Son razonablemente capaces de "dirigir el espectáculo" en términos de "entender" una pregunta o tarea que se les presenta, y lo que se necesitaría para responder esa pregunta o hacer esa tarea.

Además, mucho de la emoción sobre los sistemas de IA modernos se debe a su generalidad; pero algunos de los sistemas de IA más capaces—como los que generan o reconocen voz o imágenes, hacen predicción y modelado científico, juegan juegos, etc.—son mucho más específicos y están bien "dentro de las Puertas" en términos de computación.[^9] Estos sistemas son súper-humanos en las tareas particulares que realizan. Pueden tener debilidades en casos límite[^10] (o [explotables](https://arxiv.org/abs/2211.00241)) debido a su especificidad; sin embargo, *totalmente* específico o *completamente* general no son las únicas opciones disponibles: hay muchas arquitecturas intermedias.[^11]

Estas herramientas de IA pueden acelerar enormemente el avance en otras tecnologías positivas, sin IAG. Para hacer mejor física nuclear, no necesitamos que la IA sea un físico nuclear—¡tenemos esos! Si queremos acelerar la medicina, démosles a los biólogos, investigadores médicos y químicos herramientas poderosas. Las quieren y las usarán para un beneficio enorme. No necesitamos una granja de servidores llena de un millón de genios digitales; tenemos millones de humanos cuyo genio la IA puede ayudar a resaltar. Sí, tomará más tiempo obtener la inmortalidad y la cura para todas las enfermedades. Este es un costo real. Pero incluso las innovaciones de salud más prometedoras serían de poca utilidad si la inestabilidad impulsada por IA lleva a conflictos globales o colapso social. Nos debemos a nosotros mismos darle una oportunidad primero a los humanos empoderados por IA al problema.

Y supón que hay, de hecho, algún beneficio enorme de la IAG que no puede obtenerse por la humanidad usando herramientas dentro de las Puertas. ¿Perdemos esos al *nunca* construir IAG y superinteligencia? Al sopesar los riesgos y recompensas aquí, hay un enorme beneficio asimétrico en esperar versus apresurarse: podemos esperar hasta que pueda hacerse de manera garantizada segura y beneficiosa, y casi todos aún podrán cosechar las recompensas; si nos apresuramos, podría ser—en palabras del CEO de OpenAI Sam Altman—[luces apagadas para *todos* nosotros.](https://www.businessinsider.com/chatgpt-openai-ceo-worst-case-ai-lights-out-for-all-2023-1?op=1)

Pero si las herramientas no-IAG son potencialmente tan poderosas, ¿podemos gestionarlas? La respuesta es un claro... tal vez.

## Los sistemas de IA Herramienta son (probablemente, en principio) manejables

Pero no será fácil. Los sistemas de IA de vanguardia actuales pueden empoderar enormemente a las personas e instituciones para lograr sus objetivos. ¡Esto es, en general, algo bueno! Sin embargo, hay dinámicas naturales de tener tales sistemas a nuestra disposición—súbitamente y sin mucho tiempo para que la sociedad se adapte—que ofrecen riesgos serios que necesitan ser gestionados. Vale la pena discutir algunas clases principales de tales riesgos, y cómo pueden ser disminuidos, asumiendo un cierre de Puerta.

Una clase de riesgos es la de IA Herramienta de alto poder que permite acceso a conocimiento o capacidad que previamente había estado vinculado a una persona u organización, haciendo disponible una combinación de alta capacidad más alta lealtad a un conjunto muy amplio de actores. Hoy, con suficiente dinero una persona de malas intenciones podría contratar un equipo de químicos para diseñar y producir nuevas armas químicas—pero no es tan fácil tener ese dinero o encontrar/ensamblar el equipo y convencerlos de hacer algo claramente ilegal, no ético y peligroso. Para prevenir que los sistemas de IA jueguen tal papel, las mejoras en los métodos actuales pueden bastar,[^12] siempre que todos esos sistemas y el acceso a ellos sean gestionados responsablemente. Por otro lado, si sistemas poderosos se liberan para uso general y modificación, cualquier medida de seguridad incorporada probablemente sea removible. Así que para evitar riesgos en esta clase, se requerirán restricciones fuertes sobre lo que puede ser liberado públicamente—análogas a restricciones en detalles de tecnologías nucleares, explosivas y otras peligrosas.[^13]

Una segunda clase de riesgos surge del escalamiento de máquinas que actúan como o se hacen pasar por personas. A nivel de daño a personas individuales, estos riesgos incluyen estafas mucho más efectivas, spam y phishing, y la proliferación de deepfakes no consensuales.[^14] A nivel colectivo, incluyen la disrupción de procesos sociales centrales como la discusión y debate público, nuestros sistemas societales de recolección, procesamiento y diseminación de información y conocimiento, y nuestros sistemas de elección política. Mitigar este riesgo probablemente involucre (a) leyes que restrinjan la suplantación de personas por sistemas de IA, y que hagan legalmente responsables a los desarrolladores de IA que crean sistemas que generan tales suplantaciones, (b) sistemas de marcas de agua y procedencia que identifiquen y clasifiquen (responsablemente) contenido de IA generado, y (c) nuevos sistemas epistémicos socio-técnicos que puedan crear una cadena confiable desde datos (p. ej., cámaras y grabaciones) hasta hechos, entendimiento y buenos modelos del mundo.[^15] Todo esto es posible, y la IA puede ayudar con algunas partes de ello.

Un tercer riesgo general es que en la medida en que algunas tareas son automatizadas, los humanos que actualmente realizan esas tareas pueden tener menos valor financiero como trabajo. Históricamente, automatizar tareas ha hecho que las cosas habilitadas por esas tareas sean más baratas y abundantes, mientras clasifica a las personas que previamente hacían esas tareas en aquellas que aún están involucradas en la versión automatizada (generalmente con mayor habilidad/pago), y aquellas cuyo trabajo vale menos o poco. En términos netos es difícil predecir en qué sectores se requerirá más versus menos trabajo humano en el sector resultante más grande pero más eficiente. En paralelo, la dinámica de automatización tiende a aumentar la desigualdad y productividad general, disminuir el costo de ciertos bienes y servicios (vía aumentos de eficiencia), y aumentar el costo de otros (vía [enfermedad de costos](https://en.wikipedia.org/wiki/Baumol_effect)). Para aquellos en el lado desfavorecido del aumento de desigualdad, es profundamente poco claro si la disminución de costo en esos ciertos bienes y servicios supera el aumento en otros, y lleva a un mayor bienestar general. Entonces, ¿cómo será esto para la IA? Debido a la relativa facilidad con que el trabajo intelectual humano puede ser reemplazado por IA general, podemos esperar una versión rápida de esto con IA general de propósito competitiva con humanos.[^16] Si cerramos la Puerta a la IAG, muchos menos empleos serán reemplazados completamente por agentes de IA; pero un desplazamiento laboral enorme aún es probable durante un período de años.[^17] Para evitar sufrimiento económico generalizado, probablemente será necesario implementar tanto alguna forma de activos básicos universales o ingreso, como también diseñar un cambio cultural hacia valorar y recompensar el trabajo centrado en humanos que sea más difícil de automatizar (en lugar de ver los precios laborales caer debido al aumento en trabajo disponible empujado fuera de otras partes de la economía.) Otras construcciones, como la de ["dignidad de datos"](https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society) (en la que los productores humanos de datos de entrenamiento reciben automáticamente regalías por el valor creado por esos datos en IA) pueden ayudar. La automatización por IA también tiene un segundo efecto adverso potencial, que es la automatización *inapropiada*. Junto con aplicaciones donde la IA simplemente hace un peor trabajo, esto incluiría aquellas donde los sistemas de IA probablemente violen preceptos morales, éticos o legales—por ejemplo en decisiones de vida y muerte, y en asuntos judiciales. Estos deben tratarse aplicando y extendiendo nuestros marcos legales actuales.

Finalmente, una amenaza significativa de la IA dentro de las puertas es su uso en persuasión personalizada, captura de atención y manipulación. Hemos visto en redes sociales y otras plataformas en línea el crecimiento de una economía de atención profundamente arraigada (donde los servicios en línea batallan ferozmente por la atención del usuario) y sistemas de ["capitalismo de vigilancia"](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism) (en los que la información y perfilado del usuario se añade a la comodificación de la atención.) Es casi seguro que más IA será puesta al servicio de ambos. La IA ya se usa mucho en algoritmos de feeds adictivos, pero esto evolucionará hacia contenido generado por IA adictivo, personalizado para ser consumido compulsivamente por una sola persona. Y la entrada, respuestas y datos de esa persona, serán alimentados a la máquina de atención/publicidad para continuar el ciclo vicioso. Además, a medida que los asistentes de IA proporcionados por empresas tecnológicas se conviertan en la interfaz para más vida en línea, probablemente reemplazarán a los motores de búsqueda y feeds como el mecanismo por el cual ocurre la persuasión y monetización de clientes. El fracaso de nuestra sociedad para controlar estas dinámicas hasta ahora no augura bien. Algo de esta dinámica puede disminuirse vía regulaciones concernientes a privacidad, derechos de datos y manipulación. Llegar más a la raíz del problema puede requerir perspectivas diferentes, como la de asistentes de IA leales (discutidos abajo.)

El resultado de esta discusión es de esperanza: los sistemas basados en herramientas dentro de las Puertas—al menos mientras permanezcan comparables en poder y capacidad a los sistemas más avanzados de hoy—son probablemente manejables si hay voluntad y coordinación para hacerlo. Las instituciones humanas decentes, empoderadas por herramientas de IA,[^18] pueden hacerlo. También podríamos fallar en hacerlo. Pero es difícil ver cómo permitir sistemas más poderosos ayudaría—excepto poniéndolos a cargo y esperando lo mejor.

## Seguridad nacional

Las carreras por supremacía de IA—impulsadas por seguridad nacional u otras motivaciones—nos llevan hacia sistemas de IA poderosos no controlados que tenderían a absorber, en lugar de otorgar, poder. Una carrera de IAG entre EE.UU. y China es una carrera para determinar qué nación obtiene superinteligencia primero.

Entonces, ¿qué deberían hacer en su lugar los encargados de la seguridad nacional? Los gobiernos tienen experiencia sólida construyendo sistemas controlables y seguros, y deberían reforzar hacerlo así en IA, apoyando el tipo de proyectos de infraestructura que tienen más éxito cuando se hacen a escala y con respaldo gubernamental.

En lugar de un "proyecto Manhattan" imprudente hacia la IAG,[^19] el gobierno estadounidense podría lanzar un proyecto Apolo para sistemas controlables, seguros y confiables. Esto podría incluir por ejemplo:

- Un programa importante para (a) desarrollar los mecanismos de seguridad de hardware en chip y (b) la infraestructura, para gestionar el lado de cómputo de la IA poderosa. Estos podrían construirse sobre la [ley CHIPS](https://www.commerce.gov/news/blog/2024/08/two-years-later-funding-chips-and-science-act-creating-quality-jobs-growing-local) de EE.UU. y el [régimen de controles de exportación](https://www.bis.gov/press-release/biden-harris-administration-announces-regulatory-framework-responsible-diffusion).
- Una iniciativa a gran escala para desarrollar técnicas de verificación formal para que características particulares de sistemas de IA (como un interruptor de apagado) puedan ser *probadas* estar presentes o ausentes. Esto puede aprovechar la IA misma para desarrollar pruebas de propiedades.
- Un esfuerzo a escala nacional para crear software que sea verificablemente seguro, impulsado por herramientas de IA que pueden recodificar software existente en marcos verificablemente seguros.
- Un proyecto de inversión nacional en avance científico usando IA,[^20] funcionando como una asociación entre el DOE, NSF y NIH.

En general, hay una superficie de ataque enorme en nuestra sociedad que nos hace vulnerables a riesgos de la IA y su mal uso. Protegerse de algunos de estos riesgos requerirá inversión y estandarización de tamaño gubernamental. Estos proporcionarían vastamente más seguridad que echar gasolina al fuego de carreras hacia la IAG. Y si la IA va a ser construida en armamento y sistemas de comando y control, es crucial que la IA sea confiable y segura, lo cual la IA actual simplemente no es.

## Concentración de poder y sus mitigaciones

Este ensayo se ha enfocado en la idea del control humano de la IA y su potencial falla. Pero otra lente válida a través de la cual ver la situación de IA es a través de la *concentración de poder.* El desarrollo de IA muy poderosa amenaza con concentrar poder ya sea en las muy pocas y muy grandes manos corporativas que la han desarrollado y la controlarán, o en gobiernos usando IA como un nuevo medio para mantener su propio poder y control, o en los sistemas de IA mismos. O alguna mezcla impía de lo anterior. En cualquiera de estos casos la mayoría de la humanidad pierde poder, control y agencia. ¿Cómo podríamos combatir esto?

El primer paso y más importante, por supuesto, es un cierre de Puerta a la IAG más inteligente que humanos y superinteligencia. Estas explícitamente pueden reemplazar directamente a humanos y grupos de humanos. Si están bajo control corporativo o gubernamental concentrarán poder en esas corporaciones o gobiernos; si son "libres" concentrarán poder en sí mismas. Así que asumamos que las Puertas están cerradas. ¿Entonces qué?

Una solución propuesta a la concentración de poder es la IA "código abierto", donde los pesos del modelo están disponibles libre o ampliamente. Pero como se mencionó antes, una vez que un modelo es abierto, la mayoría de las medidas de seguridad o protecciones pueden ser (y generalmente son) eliminadas. Así que hay una tensión aguda entre por un lado la descentralización, y por el otro la seguridad y el control humano de sistemas de IA. También hay razones para ser escépticos de que los modelos abiertos por sí mismos combatirán significativamente la concentración de poder en IA más de lo que lo han hecho en sistemas operativos (aún dominados por Microsoft, Apple y Google a pesar de alternativas abiertas).[^21]

Sin embargo, puede haber formas de cuadrar este círculo—centralizar y mitigar riesgos mientras se descentraliza capacidad y recompensa económica. Esto requiere repensar tanto cómo se desarrolla la IA como cómo se distribuyen sus beneficios.

Nuevos modelos de desarrollo y propiedad pública de IA ayudarían. Esto podría tomar varias formas: IA desarrollada por el gobierno (sujeta a supervisión democrática),[^22] organizaciones de desarrollo de IA sin fines de lucro (como Mozilla para navegadores), o estructuras que permitan propiedad y gobernanza muy amplia. La clave es que estas instituciones estarían explícitamente constituidas para servir el interés público mientras operan bajo fuertes restricciones de seguridad.[^23] Regímenes regulatorios y de estándares/certificaciones bien elaborados también serán vitales, para que los productos de IA ofrecidos por un mercado vibrante permanezcan genuinamente útiles en lugar de explotadores hacia sus usuarios.

En términos de concentración de poder económico, podemos usar rastreo de procedencia y "dignidad de datos" para asegurar que los beneficios económicos fluyan más ampliamente. En particular, la mayor parte del poder de IA ahora (y en el futuro si mantenemos las Puertas cerradas) surge de datos generados por humanos, ya sean datos de entrenamiento directos o retroalimentación humana. Si las empresas de IA fueran requeridas a compensar a los proveedores de datos justamente,[^24] esto podría al menos ayudar a distribuir las recompensas económicas más ampliamente. Más allá de esto, otro modelo podría ser la propiedad pública de fracciones significativas de grandes empresas de IA. Por ejemplo, gobiernos capaces de gravar empresas de IA podrían invertir una fracción de recibos en un fondo de riqueza soberano que posea acciones en las empresas, y pague dividendos a la población.[^25]

Crucial en estos mecanismos es usar el poder de la IA misma para ayudar a distribuir poder mejor, en lugar de simplemente luchar contra la concentración de poder impulsada por IA usando medios no-IA. Un enfoque poderoso sería a través de asistentes de IA bien diseñados que operen con deber fiduciario genuino hacia sus usuarios—poniendo los intereses de los usuarios primero, especialmente por encima de los proveedores corporativos.[^26] Estos asistentes deben ser verdaderamente confiables, técnicamente competentes pero apropiadamente limitados basado en caso de uso y nivel de riesgo, y ampliamente disponibles para todos a través de canales públicos, sin fines de lucro, o con fines de lucro certificados. Así como nunca aceptaríamos un asistente humano que secretamente trabaje contra nuestros intereses para otra parte, no deberíamos aceptar asistentes de IA que vigilen, manipulen o extraigan valor de sus usuarios para beneficio corporativo.

Tal transformación alteraría fundamentalmente la dinámica actual donde los individuos quedan para negociar solos con vastas máquinas corporativas y burocráticas (impulsadas por IA) que priorizan la extracción de valor sobre el bienestar humano. Mientras hay muchos enfoques posibles para redistribuir el poder impulsado por IA más ampliamente, ninguno emergerá por defecto: deben ser deliberadamente diseñados y gobernados con mecanismos como requerimientos fiduciarios, provisión pública y acceso escalonado basado en riesgo.

Los enfoques para mitigar la concentración de poder pueden enfrentar vientos en contra significativos de poderes incumbentes.[^27] Pero hay caminos hacia el desarrollo de IA que no requieren elegir entre seguridad y poder concentrado. Construyendo las instituciones correctas ahora, podríamos asegurar que los beneficios de la IA sean ampliamente compartidos mientras sus riesgos son cuidadosamente gestionados.

## Nuevas estructuras de gobernanza y sociales

Nuestras estructuras de gobernanza actuales están luchando: son lentas para responder, a menudo capturadas por intereses especiales, y [cada vez más desconfiadas por el público.](https://news.gallup.com/poll/508169/historically-low-faith-institutions-continues.aspx) Sin embargo, esto no es una razón para abandonarlas—todo lo contrario. Algunas instituciones pueden necesitar reemplazo, pero más ampliamente necesitamos nuevos mecanismos que puedan mejorar y suplementar nuestras estructuras existentes, ayudándolas a funcionar mejor en nuestro mundo que evoluciona rápidamente.

Mucho de nuestra debilidad institucional surge no de estructuras gubernamentales formales, sino de instituciones sociales degradadas: nuestros sistemas para desarrollar entendimiento compartido, coordinar acción y conducir discurso significativo. Hasta ahora, la IA ha acelerado esta degradación, inundando nuestros canales de información con contenido generado, señalándonos al contenido más polarizante y divisivo, y haciendo más difícil distinguir verdad de ficción.

Pero la IA podría realmente ayudar a reconstruir y fortalecer estas instituciones sociales. Considera tres áreas cruciales:

Primero, la IA podría ayudar a restaurar confianza en nuestros sistemas epistémicos—nuestras formas de saber qué es verdad. Podríamos desarrollar sistemas impulsados por IA que rastreen y verifiquen la procedencia de información, desde datos en bruto hasta análisis y conclusiones. Estos sistemas podrían combinar verificación criptográfica con análisis sofisticado para ayudar a las personas a entender no solo si algo es verdad, sino cómo sabemos que es verdad.[^28] Los asistentes de IA leales podrían encargarse de seguir los detalles para asegurar que se confirmen.

Segundo, la IA podría habilitar nuevas formas de coordinación a gran escala. Muchos de nuestros problemas más apremiantes—desde el cambio climático hasta la resistencia a antibióticos—son fundamentalmente problemas de coordinación. Estamos [atrapados en situaciones que son peores de lo que podrían ser para casi todos](https://equilibriabook.com/), porque ningún individuo o grupo puede permitirse hacer el primer movimiento. Los sistemas de IA podrían ayudar modelando estructuras de incentivos complejas, identificando caminos viables hacia mejores resultados, y facilitando la construcción de confianza y mecanismos de compromiso necesarios para llegar ahí.

Tal vez más intrigante, la IA podría habilitar formas completamente nuevas de discurso social. Imagina poder "hablar con una ciudad"[^29]—no solo ver estadísticas, sino tener un diálogo significativo con un sistema de IA que procese y sintetice las opiniones, experiencias, necesidades y aspiraciones de millones de residentes. O considera cómo la IA podría facilitar diálogo genuino entre grupos que actualmente se hablan sin escucharse, ayudando a cada lado a entender mejor las preocupaciones y valores reales del otro en lugar de sus caricaturas mutuas.[^30] O la IA podría ofrecer intermediación hábil y creíblemente neutral de disputas entre personas o incluso grandes grupos de personas (¡que podrían todos interactuar con ella directa e individualmente!) La IA actual es totalmente capaz de hacer este trabajo, pero las herramientas para hacerlo no surgirán por sí mismas, o vía incentivos de mercado.

Estas posibilidades podrían sonar utópicas, especialmente dado el papel actual de la IA en degradar el discurso y la confianza. Pero eso es precisamente por lo que debemos desarrollar activamente estas aplicaciones positivas. Cerrando las Puertas a la IAG incontrolable y priorizando IA que mejore la agencia humana, podemos dirigir el progreso tecnológico hacia un futuro donde la IA sirva como una fuerza para el empoderamiento, la resistencia y el avance colectivo.

[^1]: Dicho eso, mantenerse alejado de la triple intersección desafortunadamente no es tan fácil como uno podría desear. Presionar la capacidad muy fuerte en cualquiera de los tres aspectos tiende a aumentarla en los otros. En particular, puede ser difícil crear una inteligencia extremadamente general y capaz que no pueda ser fácilmente convertida en autónoma. Un enfoque es entrenar modelos ["miopes"](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia) con capacidad de planificación limitada. Otro sería enfocarse en diseñar sistemas ["oráculo"](https://arxiv.org/abs/1711.05541) puros que se alejarían de responder preguntas orientadas a la acción.

[^2]: ¡Muchas empresas fallan en darse cuenta de que ellas también eventualmente serían desplazadas por IAG, incluso si toma más tiempo—si lo hicieran, podrían presionar un poco menos en esas Puertas!

[^3]: Los sistemas de IA podrían comunicarse de maneras más eficientes pero menos inteligibles, pero mantener el entendimiento humano debería tomar prioridad.

[^4]: Esta idea de IA modular e interpretable ha sido desarrollada en detalle por varios investigadores; ver p. ej. el modelo ["Servicios Comprensivos de IA"](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) de Drexler, la ["Arquitectura de Agencia Abierta"](https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai) de Dalrymple y otros. Mientras tales sistemas podrían requerir más esfuerzo de ingeniería que redes neuronales monolíticas entrenadas con computación masiva, eso es precisamente donde los límites de computación ayudan—haciendo que el camino más seguro y transparente también sea el más práctico.

[^5]: Sobre casos de seguridad en general ver [este manual](https://onlinelibrary.wiley.com/doi/10.1002/9781119443070.ch16). Pertinente a IA en particular, ver [Wasil et al.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274), [Clymer et al.](https://arxiv.org/abs/2403.10462), [Buhl et al.](https://arxiv.org/abs/2410.21572), y [Balesni et al.](https://arxiv.org/abs/2411.03336)

[^6]: De hecho ya estamos viendo esta tendencia impulsada solo por el alto costo de inferencia: modelos más pequeños y más especializados "destilados" de los más grandes y capaces de ejecutarse en hardware menos costoso.

[^7]: Entiendo por qué aquellos emocionados sobre el ecosistema tecnológico de IA pueden oponerse a lo que ven como regulación onerosa en su industria. Pero es francamente desconcertante para mí por qué, digamos, un capitalista de riesgo querría permitir el descontrol hacia IAG y superinteligencia. Esos sistemas (y empresas, mientras permanezcan bajo control de empresas) se *comerán todas las startups como aperitivo*. Probablemente incluso *antes* que comerse otras industrias. Cualquiera invertido en un ecosistema de IA próspero debería priorizar asegurar que el desarrollo de IAG no lleve a monopolización por unos pocos jugadores dominantes.

[^8]: Como el economista y ex investigador de Deepmind Michael Webb [lo puso](https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/), "Creo que si paráramos todo desarrollo de modelos de lenguaje más grandes hoy, así que GPT-4 y Claude y lo que sea, y son las últimas cosas que entrenamos de ese tamaño—así que estamos permitiendo mucha más iteración en cosas de ese tamaño y todo tipo de ajustes finos, pero nada más grande que eso, no mayores avances—solo lo que tenemos hoy creo que es suficiente para impulsar 20 o 30 años de crecimiento económico increíble."

[^9]: Por ejemplo, el sistema alphafold de DeepMind usó solo una centésima de milésima del número de FLOP de GPT-4.

[^10]: La dificultad de los automóviles autónomos es importante notar aquí: aunque nominalmente una tarea específica, y lograble con confiabilidad justa con sistemas de IA relativamente pequeños, conocimiento y entendimiento extensivo del mundo real es necesario para obtener confiabilidad al nivel necesario en tal tarea crítica para la seguridad.

[^11]: Por ejemplo, dado un presupuesto de computación, probablemente veríamos modelos de IAGG pre-entrenados en (digamos) la mitad de ese presupuesto, y la otra mitad usada para entrenar capacidad muy alta en un rango más específico de tareas. Esto daría capacidad específica súper-humana respaldada por inteligencia general casi humana.

[^12]: La técnica de alineación dominante actual es "aprendizaje por refuerzo con retroalimentación humana" [(ARLH)](https://arxiv.org/abs/1706.03741) y usa retroalimentación humana para crear una señal de recompensa/castigo para aprendizaje por refuerzo del modelo de IA. Esta y técnicas relacionadas como [IA constitucional](https://arxiv.org/abs/2212.08073) están funcionando sorprendentemente bien (aunque carecen de robustez y pueden ser eludidas con esfuerzo modesto.) Además, los modelos de lenguaje actuales son generalmente competentes en razonamiento de sentido común que no cometerán errores morales tontos. Este es algo así como un punto dulce: lo suficientemente inteligente para entender lo que la gente quiere (en la medida en que puede definirse), pero no lo suficientemente inteligente para planear decepciones elaboradas o causar daño enorme cuando se equivocan.

[^13]: A largo plazo, cualquier nivel de capacidad de IA que se desarrolle probablemente se prolifere, ya que en última instancia es software, y útil. Necesitaremos tener mecanismos robustos para defendernos contra los riesgos que tales sistemas plantean. Pero *no tenemos eso ahora* así que debemos ser muy medidos en cuánto se permite que se proliferen modelos de IA poderosos.

[^14]: La gran mayoría de estos son deepfakes pornográficos no consensuales, incluyendo de menores.

[^15]: Muchos ingredientes para tales soluciones existen, en forma de leyes "bot-o-no" (en la ley de IA de la UE entre otros lugares), [tecnologías de rastreo de procedencia de la industria](https://c2pa.org/), [agregadores de noticias innovadores](https://www.improvethenews.org/), [agregadores](https://metaculus.com/) de predicción y mercados, etc.

[^16]: La ola de automatización puede no seguir patrones previos, en que tareas relativamente *altas* de habilidad como escritura de calidad, interpretación de ley, o dar consejo médico, pueden ser tanto o incluso más vulnerables a automatización que tareas de menor habilidad.

[^17]: Para modelado cuidadoso del efecto de IAG en salarios, ver el reporte [aquí](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek), y detalles sangrientos [aquí](https://www.dropbox.com/scl/fi/viob7f5yv13zy0ziezlcg/AGI_Scenarios.pdf?rlkey=8hxq9rm82kksocw1zjilcxf8v&e=1&dl=0), de Anton Korinek y colaboradores. Encuentran que a medida que más piezas de empleos son automatizadas, productividad y salarios suben—hasta un punto. Una vez que *demasiado* es automatizado, la productividad continúa aumentando, pero los salarios se desploman porque las personas son reemplazadas completamente por IA eficiente. Por esto es tan útil cerrar las Puertas: obtenemos la productividad sin los salarios humanos desaparecidos.

[^18]: Hay muchas formas en que la IA puede usarse como, y para ayudar a construir, tecnologías "defensivas" para hacer protecciones y gestión más robustas. Ver [esta](https://vitalik.eth.limo/general/2025/01/05/dacc2.html) publicación influyente describiendo esta agenda "D/acc".

[^19]: Algo irónicamente, un proyecto Manhattan estadounidense probablemente haría poco para acelerar las líneas de tiempo hacia IAG—el dial de inversión humana y fiscal en progreso de IA ya está fijado en 11. Los resultados primarios serían inspirar un proyecto similar en China (que sobresale en proyectos de infraestructura a nivel nacional), hacer acuerdos internacionales limitando el riesgo de IA mucho más difícil, y alarmar a otros adversarios geopolíticos de EE.UU. como Rusia.

[^20]: El programa ["Recurso Nacional de Investigación de IA"](https://nairrpilot.org/) es un buen paso actual en esta dirección y debería expandirse.

[^21]: Ver [este análisis](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807) de los varios significados e implicaciones de "abierto" en productos tecnológicos y cómo algunos han llevado a más, en lugar de menos, atrincheramiento de dominancia.

[^22]: Los planes en EE.UU. para un [Recurso Nacional de Investigación de IA](https://nairratdoe.ornl.gov/) y el lanzamiento reciente de una [Fundación Europea de IA](https://fortune.com/2025/02/10/france-tech-companies-and-philanthropies-back-400-million-foundation-to-support-public-interest-ai/) son pasos interesantes en esta dirección.

[^23]: El desafío aquí no es técnico sino institucional—necesitamos urgentemente ejemplos y experimentos del mundo real de cómo podría verse el desarrollo de IA de interés público.

[^24]: Esto va contra los modelos de negocio actuales de grandes tecnológicas y requeriría tanto acción legal como nuevas normas.

[^25]: Solo algunos gobiernos podrán hacerlo. Una idea más radical es [un fondo universal de este tipo, bajo propiedad conjunta de todos los humanos.](https://futureoflife.org/project/the-windfall-trust/)

[^26]: Para una exposición extensa de este caso ver [este artículo](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3930338) sobre lealtad de IA. Desafortunadamente la trayectoria por defecto de asistentes de IA probablemente sea una donde son cada vez más desleales.

[^27]: Algo irónicamente, muchos poderes incumbentes también están en riesgo de desempoderamiento respaldado por IA; pero puede ser difícil para ellos percibir esto hasta y a menos que el proceso llegue bastante lejos.

[^28]: Algunos esfuerzos interesantes en esta dirección están representados por [la coalición c2pa](https://c2pa.org/) sobre verificación criptográfica; [Verity](https://www.improvethenews.org/) y [Ground news](https://ground.news/) sobre mejores epistémicas de noticias; y [Metaculus](https://keepthefuturehuman.ai/essay/docs/metaculus.com) y mercados de predicción sobre fundamentar el discurso en predicciones falsificables.

[^29]: Ver [este](https://talktothecity.org/) fascinante proyecto piloto.

[^30]: Ver [Kialo](https://www.kialo-edu.com/), y esfuerzos del [Proyecto de Inteligencia Colectiva](https://www.cip.org/) para algunos ejemplos.