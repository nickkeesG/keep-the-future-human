# Bölüm 8 - YGZ'yi Nasıl İnşa Etmemeli

YGZ kaçınılmaz değildir – bugün bir yol ayrımında duruyoruz. Bu bölüm, inşa edilmesini nasıl engelleyebileceğimize dair bir öneri sunuyor.

Şu anda bulunduğumuz yol uygarlığımızın muhtemel sonuna götürüyorsa, nasıl yol değiştiriz?

YGZ ve süper zeka geliştirmeyi durdurma arzusunun yaygın ve güçlü olduğunu varsayalım,[^1] çünkü YGZ'nin güç veren değil güç emen bir teknoloji olacağı ve toplum ile insanlık için derin bir tehlike oluşturacağı yaygın bir anlayış haline gelmiştir. Kapıları nasıl kapatırız?

Şu anda güçlü ve genel AI yapmanın yalnızca bir yolunu biliyoruz, o da derin sinir ağlarının gerçekten devasa hesaplamaları aracılığıyla. Bunlar inanılmaz derecede zor ve pahalı işler olduğundan, bunları *yapmamak* bir anlamda kolaydır.[^2] Ancak YGZ'ye doğru iten güçleri ve herhangi bir tarafın tek taraflı olarak durmasını çok zorlaştıran oyun teorisi dinamiklerini zaten gördük. Bu nedenle şirketleri durdurmak için dışarıdan (yani hükümetlerden) müdahale ile hükümetlerin kendilerini durdurmak için aralarında anlaşmalar yapması gerekir.[^3] Bu nasıl görünebilir?

Önce *engellenmesi* veya *yasaklanması* gereken AI gelişmeleri ile *yönetilmesi* gerekenleri ayırt etmek faydalıdır. Birinci kategori öncelikle süper zekaya doğru kontrolsüz kaçış olacaktır.[^4] Yasaklanan geliştirme için tanımlar mümkün olduğunca keskin olmalı ve hem doğrulama hem de uygulama pratik olmalıdır. *Yönetilmesi* gerekenler genel, güçlü AI sistemleri olacaktır – bunlara zaten sahibiz ve çok sayıda gri alan, nüans ve karmaşıklık içerecekler. Bunlar için güçlü etkili kurumlar kritiktir.

Ayrıca uluslararası düzeyde (jeopolitik rakipler veya düşmanlar arasında da dahil olmak üzere) ele alınması gereken konular[^5] ile bireysel yargı yetkileri, ülkeler veya ülke gruplarının yönetebileceği konuları ayırmak da faydalı olabilir. Yasaklanan geliştirme büyük ölçüde "uluslararası" kategorisine girer, çünkü bir teknolojinin geliştirilmesi üzerindeki yerel bir yasak genellikle yer değiştirerek aşılabilir.[^6]

Son olarak, araç kutusundaki araçları düşünebiliriz. Teknik araçlar, yumuşak hukuk (standartlar, normlar vb.), sert hukuk (düzenlemeler ve gereklilikler), sorumluluk, piyasa teşvikleri ve benzerleri de dahil olmak üzere birçok araç vardır. AI'ya özgü olan bir tanesine özel dikkat verelim.

## İşlem gücü güvenliği ve yönetişimi

Yüksek güçlü AI'yı yönetmede temel bir araç, gerektirdiği donanım olacaktır. Yazılım kolayca yayılır, neredeyse sıfır marjinal üretim maliyetine sahiptir, sınırları önemsizce geçer ve anında değiştirilebilir; bunların hiçbiri donanım için geçerli değildir. Ancak tartıştığımız gibi, en yetenekli sistemlere ulaşmak için hem AI sistemlerinin eğitimi hem de çıkarım sırasında büyük miktarlarda bu "işlem gücü" gereklidir. İşlem gücü, bunu yapmak için iyi kurallar geliştirildikten sonra nispeten az belirsizlikle kolayca ölçülebilir, hesaplanabilir ve denetlenebilir. En kritik olanı, büyük miktarlarda hesaplama, zenginleştirilmiş uranyum gibi, çok nadir, pahalı ve üretimi zor bir kaynaktır. Bilgisayar çipleri her yerde bulunsa da, AI için gerekli donanım pahalı ve üretimi son derece zordur.[^7]

AI'ya özel çipleri uranyumdan çok *daha* yönetilebilir bir nadir kaynak yapan şey, donanım tabanlı güvenlik mekanizmaları içerebilmeleridir. Modern cep telefonlarının çoğu ve bazı dizüstü bilgisayarlar, yalnızca onaylanmış işletim sistemi yazılımı ve güncellemelerini yüklemelerini sağlayan, hassas biyometrik verileri cihazda koruyan ve saklayan, kaybolması veya çalınması durumunda sahibi dışında herkes için işe yaramaz hale getirilebilen özel çip üzerinde donanım özelliklerine sahiptir. Son birkaç yılda bu tür donanım güvenlik önlemleri iyi kurulmuş ve yaygın olarak benimsenmiş ve genellikle oldukça güvenli olduğu kanıtlanmıştır.

Bu özelliklerin temel yeniliği, kriptografi kullanarak donanım ve yazılımı birbirine bağlamalarıdır.[^8] Yani, belirli bir bilgisayar donanımına sahip olmak, kullanıcının farklı yazılımlar uygulayarak onunla istediği her şeyi yapabileceği anlamına gelmez. Bu bağlama aynı zamanda güçlü güvenlik de sağlar çünkü birçok saldırı yalnızca *yazılım* güvenliğinin değil *donanım* güvenliğinin de ihlal edilmesini gerektirir.

Son zamanlardaki birkaç rapor ([GovAI ve iş ortakları](https://www.governance.ai/post/computing-power-and-the-governance-of-ai), [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips) ve [RAND](https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf) gibi), AI ile ilgili son teknoloji hesaplama donanımına gömülü benzer donanım özelliklerinin AI güvenliği ve yönetişiminde son derece faydalı bir rol oynayabileceğine dikkat çekti. Bir "yöneticinin"[^9] mevcut veya hatta mümkün olduğunu tahmin edemeyeceği bir dizi işlevi mümkün kılarlar. Bazı temel örnekler olarak:

- *Coğrafi konum belirleme*: Sistemler çiplerin bilinen bir konuma sahip olacak şekilde kurulabilir ve konuma bağlı olarak farklı davranabilir (veya tamamen kapatılabilir).[^10]
- *İzin listeli bağlantılar*: Her çip, ağ kurabileceği belirli diğer çiplerin donanım tarafından uygulanan bir izin listesi ile yapılandırılabilir ve bu listede olmayan çiplerle bağlantı kuramaz.[^11] Bu, iletişim kuran çip kümelerinin boyutunu sınırlayabilir.[^12]
- *Ölçülü çıkarım veya eğitim (ve otomatik kapatma anahtarı)*: Bir yönetici, kullanıcı tarafından yalnızca belirli miktarda eğitim veya çıkarım (süre, FLOP veya belki token cinsinden) yapılmasına lisans verebilir, bundan sonra yeni izin gerekir. Artışlar küçükse, nispeten sürekli model yeniden lisanslaması gerekir. Model daha sonra bu lisans sinyalini vermeyerek basitçe "kapatılabilir".[^13]
- *Hız sınırı*: Bir modelin, yönetici veya başka bir yolla belirlenen bir sınırdan daha yüksek çıkarım hızında çalışması engellenir. Bu, sınırlı bir izin listeli bağlantı seti veya daha sofistike yöntemlerle uygulanabilir.
- *Kanıtlanmış eğitim*: Bir eğitim prosedürü, modelin oluşturulmasında belirli bir kod seti, veri ve işlem gücü kullanım miktarının kullanıldığına dair kriptografik olarak güvenli kanıt sağlayabilir.

## Süper zekayı nasıl inşa etmemeli: eğitim ve çıkarım işlem gücünde küresel sınırlar

Bu değerlendirmeler – özellikle hesaplama ile ilgili olanlar – göz önünde bulundurularak, yapay süper zekaya giden Kapıları nasıl kapatabileceğimizi tartışabiliriz; daha sonra tam YGZ'yi engellemeye ve AI modellerini farklı açılardan insan yeteneğine yaklaşıp onu aştıkları sırada yönetmeye döneceğiz.

İlk bileşen tabii ki süper zekanın kontrol edilemeyeceği ve sonuçlarının temelde öngörülemez olduğu anlayışıdır. En azından Çin ve ABD'nin bu veya başka amaçlar için bağımsız olarak süper zeka inşa etmemeye karar vermesi gerekir.[^14] Daha sonra aralarında ve diğerleri arasında güçlü bir doğrulama ve uygulama mekanizmasına sahip uluslararası bir anlaşma gereklidir, böylece tüm taraflar rakiplerinin anlaşmayı ihlal edip zar atmaya karar vermediğinden emin olabilir.

Doğrulanabilir ve uygulanabilir olması için sınırlar sert sınırlar olmalı ve mümkün olduğunca belirsizliğe yer vermemelidir. Bu neredeyse imkansız bir problem gibi görünüyor: öngörülemeyen özelliklere sahip karmaşık yazılımların yeteneklerini dünya çapında sınırlamak. Neyse ki durum bundan çok daha iyidir, çünkü gelişmiş AI'yı mümkün kılan şey – büyük miktarda işlem gücü – kontrol etmesi çok, çok daha kolaydır. Yine de güçlü ve tehlikeli bazı sistemlere izin verebilse de, *kontrolsüz süper zeka* muhtemelen bir sinir ağına giren hesaplama miktarına sert bir üst sınır ile birlikte bir AI sisteminin (bağlı sinir ağları ve diğer yazılımların) gerçekleştirebileceği çıkarım miktarına bir hız sınırı koyarak önlenebilir. Bunun spesifik bir versiyonu aşağıda önerilmektedir.

AI hesaplamasına sert küresel sınırlar koymanın büyük düzeyde uluslararası koordinasyon ve müdahaleci, mahremiyeti parçalayan gözetim gerektireceği görünebilir. Neyse ki gerektirmeyecektir. Son derece [sıkı ve darboğazlı tedarik zinciri](https://arxiv.org/abs/2402.08797), sınır yasal olarak belirlendikten sonra (kanunla veya yürütme emriyle), bu sınıra uyumun doğrulanmasının yalnızca bir avuç büyük şirketin katılımı ve iş birliğini gerektireceğini sağlamaktadır.[^15]

Böyle bir planın son derece arzu edilir bir dizi özelliği vardır. Yalnızca birkaç büyük şirkete gereklilikler getirmesi ve yalnızca oldukça önemli hesaplama kümelerinin yönetilmesi anlamında minimal müdahalecidir. İlgili çipler zaten ilk versiyon için gereken donanım yeteneklerini içermektedir.[^16] Hem uygulama hem de icra standart yasal kısıtlamalara dayanır. Ancak bunlar, donanımın kullanım şartları ve donanım kontrolleri ile desteklenir, icrayı büyük ölçüde basitleştirir ve şirketler, özel gruplar veya hatta ülkeler tarafından hilenin önüne geçer. Donanım şirketlerinin donanım kullanımına uzaktan kısıtlamalar getirmesi ve belirli yetenekleri dışarıdan kilitleme/kilit açması için bolca emsal vardır,[^17] hatta veri merkezlerindeki yüksek güçlü CPU'larda bile.[^18] Etkilenen oldukça küçük donanım ve kuruluş payı için bile gözetim telemetri ile sınırlı olabilir, veri veya modellere doğrudan erişim olmadan; ve bunun için yazılım, ek veri kaydedilmediğini göstermek üzere incelemeye açık olabilir. Şema uluslararası ve işbirlikçidir, oldukça esnek ve genişletilebilirdir. Sınır esas olarak yazılımdan ziyade donanım üzerinde olduğu için, AI yazılımının nasıl geliştirildiği ve dağıtıldığı konusunda nispeten agnostiktir ve AI kaynaklı güç yoğunlaşmasıyla mücadele etmeyi amaçlayan daha "merkezi olmayan" veya "kamusal" AI de dahil olmak üzere çeşitli paradigmalarla uyumludur.

Hesaplama tabanlı Kapı kapatmanın da dezavantajları vardır. İlk olarak, genel olarak AI yönetişimi sorununa tam bir çözüm olmaktan uzaktır. İkinci olarak, bilgisayar donanımı hızlandıkça, sistem daha küçük ve daha küçük kümelerdeki (hatta bireysel GPU'lardaki) daha fazla ve daha fazla donanımı "yakalayacaktır".[^19] Algoritma gelişmeleri nedeniyle zaman içinde daha da düşük bir hesaplama sınırının gerekli olması[^20] veya hesaplama miktarının büyük ölçüde alakasız hale gelmesi ve Kapıyı kapatmanın bunun yerine AI için daha ayrıntılı risk tabanlı veya yetenek tabanlı bir yönetişim rejimi gerektirmesi de mümkündür. Üçüncü olarak, garantiler ve etkilenen az sayıda varlık ne olursa olsun, böyle bir sistem mahremiyet ve gözetim ile ilgili diğer endişelerin yanı sıra tepkiler yaratmaya mahkumdur.[^21]

Tabii ki, kısa sürede işlem gücü sınırlaması yönetişim şeması geliştirmek ve uygulamak oldukça zorlu olacaktır. Ancak kesinlikle yapılabilirdir.

## Y-G-Z: Risk ve politikanın temeli olarak üçlü kesişim

Şimdi YGZ'ye dönelim. Burada sert çizgiler ve tanımlar daha zordur, çünkü kesinlikle yapay ve genel olan zekaya sahibiz ve mevcut hiçbir tanıma göre mevcut olup olmadığı veya ne zaman mevcut olacağı konusunda herkes hemfikir olmayacaktır. Üstelik işlem gücü veya çıkarım sınırı biraz kaba bir araçtır (işlem gücü yetenek için vekil, o da risk için vekil) ve – oldukça düşük olmadıkça – toplumsal veya uygarlıksal bozulma ya da akut risklere neden olacak kadar güçlü YGZ'yi engelleme olasılığı düşüktür.

En akut risklerin çok yüksek yetenek, yüksek özerklik ve büyük genelliğin üçlü kesişiminden ortaya çıktığını savundum. Bunlar – eğer geliştiriliyorlarsa – muazzam dikkatle yönetilmesi gereken sistemlerdir. Bu üç özelliği birleştiren sistemler için katı standartlar (sorumluluk ve düzenleme yoluyla) yaratarak AI geliştirmeyi daha güvenli alternatiflere yönlendirebiliriz.

Potansiyel olarak tüketicilere veya kamuya zarar verebilecek diğer endüstriler ve ürünler gibi, AI sistemleri de etkili ve yetkin devlet kurumları tarafından dikkatli düzenleme gerektirir. Bu düzenleme YGZ'nin doğasında var olan riskleri tanımalı ve kabul edilemez derecede riskli yüksek güçlü AI sistemlerinin geliştirilmesini engellemelidir.[^22]

Ancak büyük ölçekli düzenleme, özellikle endüstri tarafından karşıt çıkılması kesin olan gerçek diş[^23] ile zaman alır[^24] ve gerekli olduğuna dair politik kararlılık gerektirir.[^25] İlerleme hızı göz önünde bulundurulduğunda, bu elimizdeki zamandan daha fazla zaman alabilir.

Çok daha hızlı bir zaman çizelgesinde ve düzenleyici tedbirler geliştirilirken, en tehlikeli sistemler için sorumluluk seviyelerini netleştirerek ve artırarak şirketlere (a) çok yüksek riskli faaliyetlerden vazgeçme ve (b) riski değerlendirme ve azaltma için kapsamlı sistemler geliştirme konusunda gerekli teşvikleri verebiliriz. Fikir, yüksek özerklik-genellik-zeka üçlü kesişimindeki sistemler için en yüksek sorumluluk düzeylerini – katı ve bazı durumlarda kişisel cezai – dayatmak, ancak bu özelliklerden birinin eksik olduğu veya yönetilebileceği garanti edilen sistemler için daha tipik kusur tabanlı sorumluluğa "güvenli limanlar" sağlamaktır. Yani örneğin, genel ve otonom olan "zayıf" bir sistem (yetenekli ve güvenilir ama sınırlı kişisel asistan gibi) daha düşük sorumluluk seviyelerine tabi olacaktır. Benzer şekilde, kendi kendine giden araba gibi dar ve otonom bir sistem halihazırda tabi olduğu önemli düzenlemeye tabi olmaya devam edecek, ancak artırılmış sorumluluğa tabi olmayacaktır. Benzer şekilde, yüksek yetenekli ve genel ama "pasif" ve büyük ölçüde bağımsız eylem yeteneği olmayan bir sistem için. İki özellikten yoksun sistemler daha da yönetilebilir ve güvenli limanları talep etmek daha da kolay olacaktır. Bu yaklaşım diğer potansiyel tehlikeli teknolojileri nasıl ele aldığımızı yansıtır:[^26] daha tehlikeli konfigürasyonlar için daha yüksek sorumluluk, daha güvenli alternatiflere doğal teşvikler yaratır.

Böyle yüksek sorumluluk seviyelerinin varsayılan sonucu, YGZ riskini şirketlere kamuya yüklemek yerine *içselleştirmeye* yarar, muhtemelen (ve umarım!) şirketlerin risk altındaki taraflar *kendi liderlikleriyken* gerçekten güvenilir, güvenli ve kontrol edilebilir hale getirebilecekleri ana kadar tam YGZ geliştirmemesidir. (Bu yeterli değilse, sorumluluğu netleştiren mevzuat açıkça kesin olarak tehlike bölgesinde olan ve muhtemelen kamu riski oluşturan faaliyetler için yasaklama kararı, yani bir yargıcın durdurmayı emretmesine de izin vermelidir.) Düzenleme devreye girdikçe, düzenlemeye uymak güvenli liman haline gelebilir ve AI sistemlerinin düşük özerklik, darlık veya zayıflığından kaynaklanan güvenli limanlar nispeten daha hafif düzenleyici rejimlere dönüşebilir.

## Kapı kapatmanın temel hükümleri

Yukarıdaki tartışmalar göz önünde bulundurularak, bu bölüm tam YGZ ve süper zeka üzerindeki yasağın uygulanması ve sürdürülmesi ile tam YGZ eşiğine yakın insan rekabeti veya uzman rekabeti genel amaçlı AI'nın yönetilmesi için temel hükümleri önermektedir.[^27] Dört ana parçası vardır: 1) işlem gücü muhasebesi ve gözetimi, 2) AI'nın eğitimi ve işletilmesinde işlem gücü üst sınırları, 3) sorumluluk çerçevesi ve 4) sert düzenleyici gereklilikleri içeren kademelendirilmiş güvenlik ve emniyet standartları. Bunlar sonraki kısımda öz olarak açıklanmakta, daha fazla ayrıntı veya uygulama örnekleri üç ek tabloda verilmektedir. Önemli olarak, bunların gelişmiş AI sistemlerini yönetmek için gerekli olanların tümünden uzak olduğunu unutmayın; ek güvenlik ve emniyet faydalarına sahip olmalarına rağmen, zeka kaçışına giden Kapıyı kapatmayı ve AI geliştirmeyi daha iyi bir yöne yönlendirmeyi amaçlamaktadırlar.

### 1. İşlem gücü muhasebesi ve şeffaflık

- Bir standart kuruluşu (ABD'de NIST, ardından uluslararası olarak ISO/IEEE gibi), AI modellerinin eğitiminde ve işletilmesinde kullanılan toplam işlem gücünü FLOP cinsinden ve FLOP/s cinsinden çalıştıkları hız için ayrıntılı teknik standart kodlamalıdır. Bunun nasıl görünebileceğine dair ayrıntılar Ek A'da verilmiştir.[^28]
- Büyük ölçekli AI eğitiminin gerçekleştiği yargı yetkileri tarafından yeni mevzuat veya mevcut yetki altında[^29] 10^25 FLOP veya 10^18 FLOP/s eşiğinin üzerindeki tüm modellerin eğitimi ve işletilmesinde kullanılan toplam FLOP'u hesaplama ve düzenleyici organa veya diğer kuruma rapor etme gereksinimi konulmalıdır.[^30]
- Bu gereklilikler aşamalı olarak devreye alınmalı, başlangıçta üç ayda bir iyi belgelenmiş iyi niyetli tahminler gerektirmeli, sonraki aşamalar kademeli olarak daha yüksek standartlar gerektirmeli, her model *çıktısına* eklenen kriptografik olarak kanıtlanmış toplam FLOP ve FLOP/s'ye kadar çıkmalıdır.
- Bu raporlar, her AI çıktısının üretilmesinde kullanılan marjinal enerji ve finansal maliyetin iyi belgelenmiş tahminleri ile tamamlanmalıdır.

Gerekçe: Bu iyi hesaplanmış ve şeffaf olarak rapor edilen sayılar, eğitim ve işletim üst sınırları ile daha yüksek sorumluluk önlemlerinden güvenli liman için temel sağlayacaktır (Ek C ve D'ye bakın).

### 2. Eğitim ve işletim işlem gücü üst sınırları

- AI sistemlerini barındıran yargı yetkileri, herhangi bir AI modeli çıktısına giren toplam işlem gücüne 10^27 FLOP'tan[^31] başlayarak ve uygun şekilde ayarlanabilir şekilde sert bir sınır getirmelidir.
- AI sistemlerini barındıran yargı yetkileri, AI modeli çıktılarının işlem gücü hızına 10^20 FLOP/s'den başlayarak ve uygun şekilde ayarlanabilir şekilde sert bir sınır getirmelidir.

Gerekçe: Toplam hesaplama, çok kusurlu olsa da, somut olarak ölçülebilir ve doğrulanabilir AI yeteneği (ve risk) için bir vekil olduğundan, yetenekleri sınırlamak için sert bir destek sağlar. Somut bir uygulama önerisi Ek B'de verilmiştir.

### 3. Tehlikeli sistemler için artırılmış sorumluluk

- Oldukça genel, yetenekli ve otonom gelişmiş bir AI sisteminin yaratılması ve işletilmesi,[^32] mevzuat yoluyla hukuken tek-taraf kusur tabanlı değil katı, müşterek ve müteselsil sorumluluğa tabi olacağı netleştirilmelidir.[^33]
- Küçük (işlem gücü açısından), zayıf, dar, pasif olan veya yeterli güvenlik, emniyet ve kontrol edilebilirlik garantilerine sahip sistemler için katı sorumluluktan güvenli liman sağlayacak olumlu güvenlik davalarını oluşturacak yasal süreç mevcut olmalıdır.
- Kamu tehlikesi oluşturan AI eğitimi ve çıkarım faaliyetlerini durdurmaya yönelik kesin yolak kararları için açık bir yol ve koşullar dizisi özetlenmelidir.

Gerekçe: AI sistemleri sorumlu tutulamaz, bu nedenle neden oldukları zarar için insan bireylerini ve kuruluşları sorumlu tutmalıyız (sorumluluk).[^34] Kontrol edilemeyen YGZ toplum ve uygarlık için bir tehdit olup, güvenlik davası yokluğunda "anormal derecede tehlikeli" kabul edilmelidir. Güçlü modellerin "anormal derecede tehlikeli" sayılmayacak kadar güvenli olduğunu gösterme yükünü geliştiricilere yüklemek, bu güvenli limanları talep etmek için şeffaflık ve kayıt tutma ile birlikte güvenli geliştirmeyi teşvik eder. Düzenleme daha sonra sorumluluktan caydırmanın yetersiz olduğu yerde zararı önleyebilir. Son olarak, AI geliştiricileri zaten neden oldukları zararlardan sorumludur, bu nedenle sistemlerin en risklilerinin sorumluluğunu hukuken netleştirmek hemen yapılabilir, oldukça ayrıntılı standartlar geliştirilmesine gerek kalmadan; bunlar daha sonra zaman içinde gelişebilir. Ayrıntılar Ek C'de verilmiştir.

### 4. AI için güvenlik düzenlemesi

AI'nın büyük ölçekli akut risklerini ele alan bir düzenleyici sistem en azından şunları gerektirir:

- Uygun düzenleyici organların belirlenmesi veya yaratılması, muhtemelen yeni bir kurum;
- Kapsamlı risk değerlendirmesi çerçevesi;[^35]
- Kısmen risk değerlendirmesi çerçevesine dayanan geliştiriciler tarafından yapılacak ve *bağımsız* gruplar ve kurumlar tarafından denetlenecek olumlu güvenlik davaları çerçevesi;
- Yetenek seviyelerini takip eden katmanlarla kademelendirilmiş lisanslama sistemi.[^36] Lisanslar güvenlik davaları ve denetimler temelinde sistemlerin geliştirilmesi ve dağıtımı için verilecektir. Gereklilikler alt uçta bildirimden, üst uçta geliştirmeden önce kantitatif güvenlik, emniyet ve kontrol edilebilirlik garantilerine kadar uzanacaktır. Bunlar güvenli oldukları kanıtlanana kadar sistemlerin piyasaya sürülmesini engelleyecek ve doğası gereği güvensiz sistemlerin geliştirilmesini yasaklayacaktır. Ek D bu tür güvenlik ve emniyet standartlarının neleri içerebileceğine dair öneri sunmaktadır.
- Bu önlemleri uluslararası düzeye taşıma anlaşmaları, normları ve standartları uyumlaştırma konusunda uluslararası organlar ve potansiyel olarak güvenlik davalarını gözden geçirme konusunda uluslararası kurumlar da dahil olmak üzere.

Gerekçe: Nihayetinde, sorumluluk yeni bir teknolojiden kamuya yönelik büyük ölçekli riski önlemek için doğru mekanizma değildir. Yetkin düzenleyici organlarla kapsamlı düzenleme, kamuya risk oluşturan diğer her büyük endüstride olduğu gibi AI için de gerekli olacaktır.[^37]

Diğer yaygın ama daha az akut riskleri önlemeye yönelik düzenleme, yargı yetkisinden yargı yetkisine formu değişiklik gösterecektir. Kritik olan, bu risklerin yönetilemez olacağı kadar riskli AI sistemlerinin geliştirilmesinden kaçınmaktır.

## Sonra ne olacak?

Önümüzdeki on yılda, AI daha yaygın hale geldikçe ve temel teknoloji ilerledikçe, iki temel şeyin olması muhtemeldir. İlk olarak, mevcut güçlü AI sistemlerinin düzenlenmesi daha zor, ancak daha da gerekli hale gelecektir. En azından büyük ölçekli güvenlik risklerini ele alan bazı önlemlerin uluslararası düzeyde anlaşma gerektirmesi, bireysel yargı yetkililerinin uluslararası anlaşmalara dayalı kuralları uygulaması muhtemeldir.

İkinci olarak, donanım daha ucuz ve daha maliyet etkin hale geldikçe eğitim ve işletim işlem gücü üst sınırlarını korumak zorlaşacaktır; algoritmalar ve mimarilerdeki ilerlemelerle birlikte daha az alakalı hale gelebilir (veya daha da sıkı olması gerekebilir).

AI'yı kontrol etmenin zorlaşacak olması pes etmemiz gerektiği anlamına gelmez! Bu makalede özetlenen planı uygulamak bize hem değerli zaman hem de süreci üzerinde kritik kontrol verecek, bu da AI'nın toplumumuza, uygarlığımıza ve türümüze yönelik varoluşsal riskinden kaçınmak için çok, çok daha iyi bir konuma getirecektir.

Daha uzun vadede, neye izin vereceğimize dair seçimler yapmamız gerekecek. Yine de gerçekten kontrol edilebilir bir YGZ formu yaratmayı seçebiliriz, bunun mümkün olduğu ölçüde. Veya dünyayı yönetmeyi makinelere bırakmanın daha iyi olduğuna karar verebiliriz, eğer bunu daha iyi yapacaklarına ve bize iyi davranacaklarına kendimizi ikna edebilirsek. Ancak bunlar AI konusunda derin bilimsel anlayış ile ve anlamlı küresel kapsayıcı tartışmadan sonra verilmesi gereken kararlar olmalı, insanlığın çoğu tamamen dahil olmadan ve fark etmeden teknoloji mogulları arasındaki yarışta değil.

![](https://keepthefuturehuman.ai/essay/_next/image?url=https%3A%2F%2Fkeepthefuturehuman.ai%2Fwp-content%2Fuploads%2F2025%2F02%2FAGI-Venn-Diagram-Risk-Tiers-1024x1024.png&w=3840&q=75) Sorumluluk ve düzenleme yoluyla Y-G-Z ve süper zeka yönetişiminin özeti. Sorumluluk en yüksek, düzenleme en güçlü, Özerklik, Genellik ve Zeka üçlü kesişiminde. Katı sorumluluk ve güçlü düzenlemeden güvenli limanlar, bir sistemin zayıf ve/veya dar ve/veya pasif olduğunu gösteren olumlu güvenlik davaları yoluyla elde edilebilir. Yasal ve donanım ile kriptografik güvenlik önlemleri kullanılarak doğrulanan ve uygulanan toplam Eğitim İşlem Gücü ve Çıkarım İşlem Gücü Hızı üst sınırları, tam YGZ'den kaçınarak ve süper zekayı etkili bir şekilde yasaklayarak güvenliği destekler.

[^1]: Büyük olasılıkla, bu farkındalığın yayılması ya bu davayı ortaya koyan eğitim ve savunuculuk gruplarının yoğun çabası ya da oldukça önemli bir AI kaynaklı felaket gerektirecektir. Bunun ilki olmasını umalım.

[^2]: Paradoksal olarak, Doğa'nın teknolojimizi özellikle bilimsel olarak geliştirmeyi çok zor hale getirerek sınırlamaya alıştık. Ancak bu artık AI için geçerli değil: temel bilimsel problemler beklenenden daha kolay çıkıyor. Doğa'nın bizi kendimizden kurtarmasına güvenemeyiz – bunu kendimiz yapmak zorunda kalacağız.

[^3]: Tam olarak nerede durup yeni sistemler geliştirmeyi bırakıyoruz? Burada, ihtiyatlılık ilkesini benimsemeliyiz. Bir sistem dağıtıldıktan, özellikle de o sistem yetenek seviyesi yaygınlaştıktan sonra geri almak son derece zordur. Ve eğer bir sistem *geliştirilirse* (özellikle büyük maliyet ve çabayla), kullanmak veya dağıtmak için muazzam baskı ve sızma veya çalınma ayartısı olacaktır. Sistemleri geliştirip *sonra* derinden güvensiz olup olmadığına karar vermek tehlikeli bir yoldur.

[^4]: Özünde tehlikeli olan AI geliştirmesini yasaklamak da akıllıca olurdu, örneğin kendi kendini çoğaltan ve evrimleşen sistemler, çevreyi terk etmek için tasarlananlar, özerk olarak kendini geliştirebilecek olanlar, kasıtlı olarak aldatıcı ve kötü niyetli AI vb.

[^5]: Bunun mutlaka küresel bir organ tarafından uluslararası düzeyde *uygulandığı* anlamına gelmediğini unutmayın: bunun yerine egemen ülkeler birçok anlaşmada olduğu gibi üzerinde anlaşılan kuralları uygulayabilir.

[^6]: Aşağıda göreceğimiz gibi, AI hesaplamasının doğası bir tür hibrite izin verir; ancak yine de uluslararası işbirliği gerekecektir.

[^7]: Örneğin, AI ile ilgili çipleri kazımak için gereken makineler (diğerlerinin de denememelerine rağmen) yalnızca bir firma olan ASML tarafından yapılmakta, ilgili çiplerin büyük çoğunluğu (diğerlerinin rekabet etmeye çalışmasına rağmen) tek bir firma olan TSMC tarafından üretilmekte ve bu çiplerden donanım tasarımı ve yapımı NVIDIA, AMD ve Google da dahil olmak üzere sadece birkaç firma tarafından yapılmaktadır.

[^8]: En önemlisi, her çip bir şeyleri "imzalamak" için kullanabileceği benzersiz ve erişilemez kriptografik özel anahtar tutar.

[^9]: Varsayılan olarak bu çipleri satan şirket olacaktır, ancak başka modeller mümkün ve potansiyel olarak faydalıdır.

[^10]: Bir yönetici, çipte imzalı mesajların değiş tokuşunu zamanlayarak çipin konumunu belirleyebilir: ışık hızının sonlu hızı, çipin, *r* / *c* 'den (burada *c* ışık hızıdır) daha az bir sürede imzalı mesajı döndürebiliyorsa verilen *r* yarıçapı içinde bir "istasyon"dan olmasını gerektirir. Birden fazla istasyon ve ağ özelliklerinin biraz anlaşılması kullanarak çipin konumu belirlenebilir. Bu yöntemin güzelliği güvenliğinin çoğunun fizik yasaları tarafından sağlanmasıdır. Diğer yöntemler GPS, atalet takibi ve benzer teknolojileri kullanabilir.

[^11]: Alternatif olarak, çip çiftlerinin birbirleriyle yalnızca yöneticinin açık izni ile iletişim kurmasına izin verilebilir.

[^12]: Bu kritiktir çünkü en azından şu anda büyük AI modellerini eğitmek için çipler arasında çok yüksek bant genişliği bağlantısı gereklidir.

[^13]: Bu aynı zamanda *N* / *M* farklı yöneticiden imzalı mesajları gerektirrecek şekilde kurulabilir, birden fazla tarafın yönetişimi paylaşmasına izin verir.

[^14]: Bu emsalsiz olmaktan uzaktır – örneğin ordular klonlanmış veya genetik olarak geliştirilmiş süper askerlerden oluşan ordular geliştirmemiştir, oysa bu muhtemelen teknolojik olarak mümkündür. Ancak diğerleri tarafından engellenmektense bunu yapmamayı *seçmişlerdir*. Büyük dünya güçlerinin güçlü bir şekilde geliştirmek istedikleri bir teknolojinin geliştirilmesini engellemek konusundaki sicil pek iyi değildir.

[^15]: Birkaç önemli istisna ile (özellikle NVIDIA) AI'ya özel donanım bu şirketlerin genel iş ve gelir modellerinin nispeten küçük bir parçasıdır. Üstelik, gelişmiş AI'da kullanılan donanım ile "tüketici sınıfı" donanım arasındaki fark önemlidir, bu nedenle bilgisayar donanımı tüketicilerinin çoğu büyük ölçüde etkilenmeyecektir.

[^16]: Daha ayrıntılı analiz için [RAND](https://www.rand.org/pubs/working_papers/WRA3056-1.html) ve [CNAS](https://www.cnas.org/publications/reports/secure-governable-chips)'tan son raporlara bakın. Bunlar özellikle ABD ihracat kontrollerinin diğer ülkelerin yüksek uçlu hesaplamadaki kapasitesini kısıtlamaya çalışması bağlamında teknik fizibiliteye odaklanır; ancak bunun burada öngörülen küresel kısıtlamayla açık örtüşmesi vardır.

[^17]: Örneğin Apple cihazları, kayıp veya çalındı olarak bildirildiğinde uzaktan ve güvenli bir şekilde kilitlenir ve uzaktan yeniden etkinleştirilebilir. Bu, burada tartışılan aynı donanım güvenlik özelliklerine dayanır.

[^18]: Örneğin IBM'in [isteğe bağlı kapasite](https://www.ibm.com/docs/en/power9?topic=environment-capacity-demand) teklifine, Intel'in [Intel isteğe bağlı](https://www.intel.com/content/www/us/en/products/docs/ondemand/overview.html) hizmetine ve Apple'ın [özel bulut hesaplamaya](https://security.apple.com/blog/private-cloud-compute/) bakın.

[^19]: [Bu çalışma](https://epochai.org/trends#hardware-trends-section), tarihsel olarak aynı performansın yılda yaklaşık %30 daha az dolarla elde edildiğini gösterir. Bu eğilim devam ederse, AI ve "tüketici" çip kullanımı arasında önemli örtüşme olabilir ve genel olarak yüksek güçlü AI sistemleri için gereken donanım miktarı rahatsız edici derecede küçük hale gelebilir.

[^20]: [Aynı çalışmaya](https://epochai.org/trends#hardware-trends-section) göre, görüntü tanımada verilen performans her yıl 2,5 kat daha az hesaplama gerektirmiştir. Bu en yetenekli AI sistemleri için de geçerli olsaydı, hesaplama sınırı çok uzun süre faydalı olmayacaktı.

[^21]: Özellikle ülke düzeyinde bu, hükümetin hesaplama gücünün nasıl kullanıldığı üzerinde çok fazla kontrole sahip olacağı anlamında hesaplamanın millileştirilmesi gibi görünmektedir. Ancak hükümet katılımından endişe duyanlar için bu, en güçlü AI yazılımının *kendisinin* bazılarının savunmaya başladığı gibi büyük AI şirketleri ile ulusal hükümetler arasındaki birleşme yoluyla millileştirilmesinden çok daha güvenli ve tercih edilir görünmektedir.

[^22]: Avrupa'da 2024'te [AB AI Yasası](https://artificialintelligenceact.eu/) geçişiyle önemli bir düzenleyici adım atılmıştır. AI'yı riske göre sınıflandırır: kabul edilemez sistemleri yasaklar, yüksek riskli olanları düzenler, düşük riskli sistemlere şeffaflık kuralları veya hiç önlem uygulamaz. Bazı AI risklerini önemli ölçüde azaltacak ve ABD firmaları için bile AI şeffaflığını artıracak, ancak iki temel kusurunun vardır. İlki, sınırlı erişim: AB'de AI sağlayan herhangi bir şirkete uygulanırken, ABD merkezli firmalar üzerindeki icra zayıf ve askeri AI muaftır. İkinci olarak, GPAI'yı kapsarken, YGZ veya süper zekayı kabul edilemez risk olarak tanıyamaz veya gelişimlerini engelleyemez – yalnızca AB dağıtımlarını. Sonuç olarak, YGZ veya süper zeka risklerini freenleme konusunda çok az şey yapar.

[^23]: Şirketler genellikle makul düzenleme lehinde olduklarını beyan ederler. Ancak bir şekilde neredeyse her zaman herhangi *belirli* düzenlemeye karşı çıkarlar gibi görünürler; [çoğu AI şirketinin kamuya veya özel olarak karşı çıktığı](https://www.reuters.com/technology/artificial-intelligence/big-tech-wants-ai-be-regulated-why-do-they-oppose-california-ai-bill-2024-08-21/) oldukça hafif SB1047 üzerindeki mücadeleye tanık olun.

[^24]: AB AI yasasının önerildiği zamandan yürürlüğe girdiği zamana kadar yaklaşık 3,5 yıl geçti.

[^25]: Bazen AI'yı düzenlemeye başlamak için "çok erken" olduğu ifade edilir. Son not göz önünde bulundurulduğunda, bu pek olası görünmüyor. İfade edilen bir başka endişe ise düzenlemenin "inovasyona zarar vereceğidir." Ancak iyi düzenleme yalnızca yönü değiştirir, inovasyon miktarını değil.

[^26]: İlginç bir emsal, kaçıp zarar verebilecek tehlikeli malzemelerin taşınmasındadır. Burada [düzenleme](https://code.dccouncil.gov/us/dc/council/code/sections/8-1442) ve [içtihat](https://www.hoganlovells.com/~/media/hogan-lovells/pdf/publication/1478accasupplement_pdf.pdf) patlayıcılar, benzin, zehirler, bulaşıcı ajanlar ve radyoaktif atık gibi çok tehlikeli malzemeler için katı sorumluluk oluşturmuştur. Diğer örnekler arasında [farmasötiklerde uyarılar](https://www.medicalnewstoday.com/articles/boxed-warnings), [tıbbi cihaz sınıfları](https://www.fda.gov/about-fda/cdrh-transparency/overview-medical-device-classification-and-reclassification) vb. yer alır.

[^27]: ["Dar Yol"](https://www.narrowpath.co/) adlı başka bir kapsamlı öneri benzer amaçlarla daha merkezileşmiş, yasak tabanlı bir yaklaşım savunur ve tüm sınır AI geliştirmesini güçlü uluslararası kurumlar tarafından gözetlenen tek bir uluslararası varlık aracılığıyla yönlendirir, kademelendirilmiş kısıtlamalar yerine açık kategorik yasaklarla. Ben de bu planı desteklerdim; ancak burada önerilen plandan daha fazla politik irade ve koordinasyon gerektirecektir.

[^28]: Böyle bir standart için bazı kılavuzlar [Frontier Model Forum](https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/) tarafından yayınlandı. Buradaki öneriye göre, bunlar daha az kesinlik ve sayımda daha az hesaplama içerilmesi yönünde hata yaparlar.

[^29]: 2023 ABD AI yürütme emri (şimdi yürürlükten kaldırıldı) benzer ancak daha az ayrıntılı raporlama gerektiriyordu. Bu, yerine geçen emirle güçlendirilmelidir.

[^30]: Çok kabaca, şimdi yaygın olan H100 çipler için bu çıkarım yapan yaklaşık 1000'lik kümelere karşılık gelir; çıkarım yapan çok yeni son teknoloji NVIDIA B200 çiplerinin yaklaşık 100'üdür (yaklaşık 5 milyon USD değerinde). Her iki durumda da eğitim sayısı o kümenin birkaç ay hesaplama yapmasına karşılık gelir.

[^31]: Bu miktar şu anda eğitilmiş herhangi bir AI sisteminden daha büyüktür; AI yeteneğinin hesaplama ile nasıl ölçeklendiğini daha iyi anladıkça daha büyük veya daha küçük bir sayı haklı çıkabilir.

[^32]: Bu modelleri oluşturan ve sağlayan/barındıran taraflara uygulanır, son kullanıcılara değil.

[^33]: Kabaca, "katı" sorumluluk geliştiricilerin bir ürünün neden olduğu zararlardan *varsayılan olarak* sorumlu tutulması anlamına gelir ve "anormal derecede tehlikeli" ürünler için kullanılan bir standarttır ve (biraz eğlenceli ama uygun şekilde) vahşi hayvanlar. "Müşterek ve müteselsil" sorumluluk, sorumluluğun bir üründen sorumlu tüm taraflara atandığı ve bu tarafların kendi aralarında kimin ne sorumluluğu taşıyacağını çözmesi gerektiği anlamına gelir. Bu, uzun ve karmaşık değer zincirine sahip AI gibi sistemler için önemlidir.

[^34]: Standart kusur tabanlı tek taraf sorumluluk yeterli değildir: AI sistemleri karmaşık olduğu, işleyişi anlaşılmadığı ve tehlikeli bir sistem veya çıktının yaratılmasında birçok taraf yer alabileceği için kusuru hem izlemek hem de atamak zor olacaktır. Ayrıca davalar yargılanması yıllar alacak ve büyük olasılıkla bu şirketler için önemsiz olan para cezalarıyla sonuçlanacak, bu nedenle yöneticiler için kişisel sorumluluk da önemlidir.

[^35]: Açık ağırlık modelleri için güvenlik kriterlerinden muafiyet olmamalıdır. Ayrıca, riski değerlendirirken kaldırılabilecek koruyucuların yaygın olarak mevcut modellerden kaldırılacağı ve kapalı modellerin bile güvenli kalacaklarına dair çok yüksek güvence olmadıkça yayılacağı varsayılmalıdır.

[^36]: Burada önerilen şema genel yeteneğe göre tetiklenen düzenleyici incelemeye sahiptir; ancak bazı özellikle riskli kullanım durumlarının daha fazla incelemeyi tetiklemesi mantıklıdır – örneğin uzman bir viroloji AI sistemi, dar ve pasif olsa bile muhtemelen daha yüksek bir katmanda olmalıdır. Eski ABD yürütme emrinin biyolojik yetenekler için bu yapının bir kısmı vardı.

[^37]: İki açık örnek FAA ve FDA ile benzer kurumlar tarafından düzenlenen havacılık ve ilaçlar ile diğer ülkelerdeki benzer kurumlar. Bu kurumlar kusursuz değildir, ancak o endüstrilerin işleyişi ve başarısı için kesinlikle hayati olmuştur.